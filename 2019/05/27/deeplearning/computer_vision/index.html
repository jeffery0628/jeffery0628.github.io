<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/snoppy.jpeg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open Sans:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-big-counter.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jeffery.ink","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":270,"display":"hide","padding":15,"offset":12,"onmobile":true,"dimmer":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":true,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":true,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="计算机视觉基础">
<meta property="og:url" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/index.html">
<meta property="og:site_name" content="火种2号">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/1.jpeg">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_3_1.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_9_0.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_11_0.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_13_1.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_15_1.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_17_1.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/finetune.svg">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_2_1.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_8_0.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/chapter_computer-vision_anchor_9_0.svg">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/9.4_iou.svg">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/anchor-label.svg">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_38_0.svg">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_56_0.svg">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_63_0.svg">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_5_0.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_7_0.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_9_0-5863308.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/r-cnn.svg">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/fast-rcnn.svg">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/9.8_faster-rcnn.svg">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/mask-rcnn.svg">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/9.9_segmentation.svg">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_31_1.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/fcn.svg">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_23_2.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_24_2.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/style-transfer.svg">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/neural-style.svg">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_12_1.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_13_1-5863677.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_57_1.png">
<meta property="og:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/output_60_1.png">
<meta property="article:published_time" content="2019-05-27T14:50:23.000Z">
<meta property="article:modified_time" content="2020-08-22T05:32:11.128Z">
<meta property="article:author" content="Li Zhen">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/1.jpeg">

<link rel="canonical" href="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>计算机视觉基础 | 火种2号</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">火种2号</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">火种计划</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>简历</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>读书</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-video fa-fw"></i>电影</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>



</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jeffery.ink/2019/05/27/deeplearning/computer_vision/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/snoppy.jpeg">
      <meta itemprop="name" content="Li Zhen">
      <meta itemprop="description" content="他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="火种2号">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          计算机视觉基础
        </h1>

        <div class="post-meta">
          
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-08-22 13:32:11" itemprop="dateModified" datetime="2020-08-22T13:32:11+08:00">2020-08-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">技术</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          
            <span id="/2019/05/27/deeplearning/computer_vision/" class="post-meta-item leancloud_visitors" data-flag-title="计算机视觉基础" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论：</span>
    
    <a title="valine" href="/2019/05/27/deeplearning/computer_vision/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/05/27/deeplearning/computer_vision/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>46k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>42 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/2019/05/27/deeplearning/computer_vision/1.jpeg" alt></p>
<a id="more"></a>
<h1 id="tu-xiang-zeng-yan">图像增广</h1>
<p>大规模数据集是成功应用深度神经网络的前提。图像增广技术通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模。图像增广的另一种解释是，随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力。例如，可以对图像进行不同方式的裁剪，使感兴趣的物体出现在不同位置，从而减轻模型对物体出现位置的依赖性。也可以调整亮度、色彩等因素来降低模型对色彩的敏感度。可以说，在当年AlexNet的成功中，图像增广技术功不可没。</p>
<p>读取一张形状为\(400\times 500\)的图像作为实验的样例。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = plt.imread(<span class="string">'../img/cat1.jpg'</span>)</span><br><span class="line">plt.imshow(img)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_3_1.png" alt></p>
<p>定义绘图函数<code>show_images</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_images</span><span class="params">(imgs, num_rows, num_cols, scale=<span class="number">2</span>)</span>:</span></span><br><span class="line">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class="line">    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_rows):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_cols):</span><br><span class="line">            axes[i][j].imshow(imgs[i * num_cols + j])</span><br><span class="line">            axes[i][j].axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">            axes[i][j].axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> axes</span><br></pre></td></tr></table></figure>
<p>大部分图像增广方法都有一定的随机性。为了方便观察图像增广的效果，接下来定义一个辅助函数<code>apply</code>。这个函数对输入图像<code>img</code>多次运行图像增广方法<code>aug</code>并展示所有的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span><span class="params">(img, aug, num_rows=<span class="number">2</span>, num_cols=<span class="number">4</span>, scale=<span class="number">1.5</span>)</span>:</span></span><br><span class="line">    Y = [aug(img) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_rows * num_cols)]</span><br><span class="line">    show_images(Y, num_rows, num_cols, scale)</span><br></pre></td></tr></table></figure>
<h2 id="fan-zhuan-he-cai-jian">翻转和裁剪</h2>
<p>左右翻转图像通常不改变物体的类别。它是最早也是最广泛使用的一种图像增广方法。下面通过<code>tf.image.random_flip_left_right</code>来实现一半概率的图像左右翻转。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apply(img, tf.image.random_flip_left_right)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_9_0.png" alt></p>
<p>上下翻转不如左右翻转通用。但是至少对于样例图像，上下翻转不会造成识别障碍。下面创建<code>tf.image.random_flip_up_down</code>实例来实现一半概率的图像上下翻转。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apply(img, tf.image.random_flip_up_down)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_11_0.png" alt></p>
<p>在使用的样例图像里，猫在图像正中间，但一般情况下可能不是这样。池化层能降低卷积层对目标位置的敏感度,除此之外，还可以通过对图像随机裁剪来让物体以不同的比例出现在图像的不同位置，这同样能够降低模型对目标位置的敏感性。</p>
<p>在下面的代码里，每次随机裁剪出一块面积为原面积\(10\% \sim 100\%\)的区域，且该区域的宽和高之比随机取自\(0.5 \sim 2\)，然后再将该区域的宽和高分别缩放到200像素。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">aug=tf.image.random_crop</span><br><span class="line">num_rows=<span class="number">2</span></span><br><span class="line">num_cols=<span class="number">4</span></span><br><span class="line">scale=<span class="number">1.5</span></span><br><span class="line">crop_size=<span class="number">200</span></span><br><span class="line"></span><br><span class="line">Y = [aug(img, (crop_size, crop_size, <span class="number">3</span>)) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_rows * num_cols)]</span><br><span class="line">show_images(Y, num_rows, num_cols, scale)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_13_1.png" alt></p>
<h2 id="bian-hua-yan-se">变化颜色</h2>
<p>另一类增广方法是变化颜色。可以从4个方面改变图像的颜色：亮度、对比度、饱和度和色调。将图像的亮度随机变化为原图亮度的\(50\%\)（即\(1-0.5\)）\(\sim 150\%\)（即\(1+0.5\)）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">aug=tf.image.random_brightness</span><br><span class="line">num_rows=<span class="number">2</span></span><br><span class="line">num_cols=<span class="number">4</span></span><br><span class="line">scale=<span class="number">1.5</span></span><br><span class="line">max_delta=<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">Y = [aug(img, max_delta) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_rows * num_cols)]</span><br><span class="line">show_images(Y, num_rows, num_cols, scale)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_15_1.png" alt></p>
<p>类似地，也可以随机变化图像的色调。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">aug=tf.image.random_hue</span><br><span class="line">num_rows=<span class="number">2</span></span><br><span class="line">num_cols=<span class="number">4</span></span><br><span class="line">scale=<span class="number">1.5</span></span><br><span class="line">max_delta=<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">Y = [aug(img, max_delta) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_rows * num_cols)]</span><br><span class="line">show_images(Y, num_rows, num_cols, scale)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_17_1.png" alt></p>
<h1 id="wei-diao">微调</h1>
<p>ImageNet有超过1,000万的图像和1,000类的物体。然而，我们平常接触到数据集的规模通常小于1000万。</p>
<p>假设我们想从图像中识别出不同种类的椅子，然后将购买链接推荐给用户。一种可能的方法是先找出100种常见的椅子，为每种椅子拍摄1,000张不同角度的图像，然后在收集到的图像数据集上训练一个分类模型。这个椅子数据集虽然可能比Fashion-MNIST数据集要庞大，但样本数仍然不及ImageNet数据集中样本数的十分之一。这可能会导致适用于ImageNet数据集的复杂模型在这个椅子数据集上过拟合。同时，因为数据量有限，最终训练得到的模型的精度也可能达不到实用的要求。</p>
<p>为了应对上述问题，一个显而易见的解决办法是收集更多的数据。然而，收集和标注数据会花费大量的时间和资金。例如，为了收集ImageNet数据集，研究人员花费了数百万美元的研究经费。虽然目前的数据采集成本已降低了不少，但其成本仍然不可忽略。</p>
<p>另外一种解决办法是应用迁移学习，将从源数据集学到的知识迁移到目标数据集上。虽然ImageNet数据集的图像大多跟椅子无关，但在该数据集上训练的模型可以抽取较通用的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于识别椅子也可能同样有效。</p>
<p>迁移学习中的一种常用技术：微调。如下图所示，微调由以下4步构成。</p>
<ol>
<li>在源数据集上预训练一个神经网络模型，即源模型。</li>
<li>创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。还假设源模型的输出层跟源数据集的标签紧密相关，因此在目标模型中不予采用。</li>
<li>为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。</li>
<li>在目标数据集（如椅子数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。</li>
</ol>
<p><img src="/2019/05/27/deeplearning/computer_vision/finetune.svg" alt="微调"></p>
<p>当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。</p>
<h2 id="ding-yi-he-chu-shi-hua-mo-xing">定义和初始化模型</h2>
<p>这里使用在ImageNet数据集上预训练的ResNet-50作为源模型。这里指定<code>weights='imagenet'</code>来自动下载并加载预训练的模型参数。在第一次使用时需要联网下载模型参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ResNet50 = tf.keras.applications.resnet_v2.ResNet50V2(weights=<span class="string">'imagenet'</span>, input_shape=(<span class="number">224</span>,<span class="number">224</span>,<span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> ResNet50.layers:</span><br><span class="line">    layer.trainable = <span class="literal">False</span></span><br><span class="line">net = tf.keras.models.Sequential()</span><br><span class="line">net.add(ResNet50)</span><br><span class="line">net.add(tf.keras.layers.Flatten())</span><br><span class="line">net.add(tf.keras.layers.Dense(<span class="number">2</span>, activation=<span class="string">'softmax'</span>))</span><br></pre></td></tr></table></figure>
<h1 id="mu-biao-jian-ce-he-bian-jie-kuang">目标检测和边界框</h1>
<h2 id="mu-biao-jian-ce">目标检测</h2>
<p>在图像分类任务里，我们假设图像里只有一个主体目标，并关注如何识别该目标的类别。然而，很多时候图像里有多个我们感兴趣的目标，我们不仅想知道它们的类别，还想得到它们在图像中的具体位置。在计算机视觉里，将这类任务称为目标检测或物体检测。</p>
<p>目标检测在多个领域中被广泛使用。例如，在无人驾驶里，需要通过识别拍摄到的视频图像里的车辆、行人、道路和障碍的位置来规划行进线路。机器人也常通过该任务来检测感兴趣的目标。安防领域则需要检测异常目标，如歹徒或者炸弹。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img = plt.imread(<span class="string">'catdog.jpg'</span>)</span><br><span class="line">plt.imshow(img)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_2_1.png" alt></p>
<h2 id="bian-jie-kuang">边界框</h2>
<p>在目标检测里，通常使用边界框来描述目标位置。边界框是一个矩形框，可以由矩形左上角的\(x\)和\(y\)轴坐标与右下角的\(x\)和\(y\)轴坐标确定。根据上面的图的坐标信息来定义图中狗和猫的边界框。图中的坐标原点在图像的左上角，原点往右和往下分别为\(x\)轴和\(y\)轴的正方向。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bbox是bounding box的缩写</span></span><br><span class="line">dog_bbox, cat_bbox = [<span class="number">60</span>, <span class="number">45</span>, <span class="number">378</span>, <span class="number">516</span>], [<span class="number">400</span>, <span class="number">112</span>, <span class="number">655</span>, <span class="number">493</span>]</span><br></pre></td></tr></table></figure>
<p>可以在图中将边界框画出来，以检查其是否准确。画之前，定义一个辅助函数<code>bbox_to_rect</code>。它将边界框表示成matplotlib的边界框格式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox_to_rect</span><span class="params">(bbox, color)</span>:</span></span><br><span class="line">    <span class="comment"># 将边界框(左上x, 左上y, 右下x, 右下y)格式转换成matplotlib格式：</span></span><br><span class="line">    <span class="comment"># ((左上x, 左上y), 宽, 高)</span></span><br><span class="line">    <span class="keyword">return</span> plt.Rectangle(</span><br><span class="line">        xy=(bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), width=bbox[<span class="number">2</span>]-bbox[<span class="number">0</span>], height=bbox[<span class="number">3</span>]-bbox[<span class="number">1</span>],</span><br><span class="line">        fill=<span class="literal">False</span>, edgecolor=color, linewidth=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>将边界框加载在图像上，可以看到目标的主要轮廓基本在框内。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.imshow(img)</span><br><span class="line">fig.axes.add_patch(bbox_to_rect(dog_bbox, <span class="string">'blue'</span>))</span><br><span class="line">fig.axes.add_patch(bbox_to_rect(cat_bbox, <span class="string">'red'</span>));</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_8_0.png" alt></p>
<h1 id="mao-kuang">锚框</h1>
<p>目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边缘从而更准确地预测目标的真实边界框。不同的模型使用的区域采样方法可能不同。这里介绍其中的一种方法：它以每个像素为中心生成多个大小和宽高比不同的边界框。这些边界框被称为锚框。</p>
<h2 id="sheng-cheng-duo-ge-mao-kuang">生成多个锚框</h2>
<p>假设输入图像高为 h ，宽为 w 。分别以图像的每个像素为中心生成不同形状的锚框。设大小为 \(s \in (0,1]\) 且宽高比为 \(r>0\) ，那么锚框的宽和高将分别为\(ws\sqrt{r}\)和\(\frac{hs}{\sqrt{r}}\) 。当中心位置给定时，已知宽和高的锚框是确定的。</p>
<p>下面分别设定好一组大小 \(s_1,\ldots,s_n\) 和一组宽高比 \(r_1,\ldots,r_m\) 。如果以每个像素为中心时使用所有的大小与宽高比的组合，输入图像将一共得到 \(w \times h \times n \times m\) 个锚框。虽然这些锚框可能覆盖了所有的真实边界框，但计算复杂度容易过高。因此，通常只对包含 s1 或 r1 的大小与宽高比的组合感兴趣，即</p>
<p>\[
(s_1,r_1),(s_1,r_2),\ldots,(s_1,r_m),(s_2,r_1),(s_3,r_1),\ldots,(s_n,r_1)
\]</p>
<p>也就是说，以相同像素为中心的锚框的数量为 \(n+m−1\) 。对于整个输入图像，将一共生成 \(w \times h \times (n+m−1)\) 个锚框。</p>
<p>生成锚框的方法已实现在MultiBoxPrior函数中。指定输入、一组大小和一组宽高比，该函数将返回输入的所有锚框。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">img_raw = tf.io.read_file(<span class="string">'catdog.jpg'</span>)</span><br><span class="line">img = tf.image.decode_jpeg(img_raw).numpy()</span><br><span class="line">h, w = img.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">print(h, w) <span class="comment"># 252 322</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MultiBoxPrior</span><span class="params">(feature_map, sizes=[<span class="number">0.75</span>, <span class="number">0.5</span>, <span class="number">0.25</span>], ratios=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>])</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        feature_map: torch tensor, Shape: [N, C, H, W].</span></span><br><span class="line"><span class="string">        sizes: List of sizes (0~1) of generated MultiBoxPriores. </span></span><br><span class="line"><span class="string">        ratios: List of aspect ratios (non-negative) of generated MultiBoxPriores. </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        anchors of shape (1, num_anchors, 4). 由于batch里每个都一样, 所以第一维为1</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    pairs = [] <span class="comment"># pair of (size, sqrt(ratio))</span></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> ratios:</span><br><span class="line">        pairs.append([sizes[<span class="number">0</span>], np.sqrt(r)])</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> sizes[<span class="number">1</span>:]:</span><br><span class="line">        pairs.append([s, np.sqrt(ratios[<span class="number">0</span>])])</span><br><span class="line">    </span><br><span class="line">    pairs = np.array(pairs)</span><br><span class="line"></span><br><span class="line">    ss1 = pairs[:, <span class="number">0</span>] * pairs[:, <span class="number">1</span>] <span class="comment"># size * sqrt(ration)</span></span><br><span class="line">    ss2 = pairs[:, <span class="number">0</span>] / pairs[:, <span class="number">1</span>] <span class="comment"># size / sqrt(retion)</span></span><br><span class="line"></span><br><span class="line">    base_anchors = tf.stack([-ss1, -ss2, ss1, ss2], axis=<span class="number">1</span>) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    h, w = feature_map.shape[<span class="number">-2</span>:]</span><br><span class="line">    shifts_x = tf.divide(tf.range(<span class="number">0</span>, w), w)</span><br><span class="line">    shifts_y = tf.divide(tf.range(<span class="number">0</span>, h), h)</span><br><span class="line">    shift_x, shift_y = tf.meshgrid(shifts_x, shifts_y)</span><br><span class="line">    shift_x = tf.reshape(shift_x, (<span class="number">-1</span>,))</span><br><span class="line">    shift_y = tf.reshape(shift_y, (<span class="number">-1</span>,))</span><br><span class="line">    shifts = tf.stack((shift_x, shift_y, shift_x, shift_y), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    anchors = tf.add(tf.reshape(shifts, (<span class="number">-1</span>,<span class="number">1</span>,<span class="number">4</span>)), tf.reshape(base_anchors, (<span class="number">1</span>,<span class="number">-1</span>,<span class="number">4</span>)))</span><br><span class="line">    <span class="keyword">return</span> tf.cast(tf.reshape(anchors, (<span class="number">1</span>,<span class="number">-1</span>,<span class="number">4</span>)), tf.float32)</span><br><span class="line"></span><br><span class="line">x = tf.zeros((<span class="number">1</span>,<span class="number">3</span>,h,w))</span><br><span class="line">y = MultiBoxPrior(x)</span><br><span class="line">y.shape</span><br></pre></td></tr></table></figure>
<p>返回锚框变量y的形状为（1，锚框个数，4）。将锚框变量y的形状变为（图像高，图像宽，以相同像素为中心的锚框个数，4）后，就可以通过指定像素位置来获取所有以该像素为中心的锚框了。下面的例子里我们访问以（250，250）为中心的第一个锚框。它有4个元素，分别是锚框左上角的x和y轴坐标和右下角的x和y轴坐标，其中x和y轴的坐标值分别已除以图像的宽和高，因此值域均为0和1之间。</p>
<p>为了描绘图像中以某个像素为中心的所有锚框，先定义show_bboxes函数以便在图像上画出多个边界框。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox_to_rect</span><span class="params">(bbox, color)</span>:</span></span><br><span class="line">    <span class="comment"># 将边界框(左上x, 左上y, 右下x, 右下y)格式转换成matplotlib格式：</span></span><br><span class="line">    <span class="comment"># ((左上x, 左上y), 宽, 高)</span></span><br><span class="line">    <span class="keyword">return</span> plt.Rectangle(</span><br><span class="line">        xy=(bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), width=bbox[<span class="number">2</span>]-bbox[<span class="number">0</span>], height=bbox[<span class="number">3</span>]-bbox[<span class="number">1</span>],</span><br><span class="line">        fill=<span class="literal">False</span>, edgecolor=color, linewidth=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_bboxes</span><span class="params">(axes, bboxes, labels=None, colors=None)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_list</span><span class="params">(obj, default_values=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> obj <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            obj = default_values</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> isinstance(obj, (list, tuple)):</span><br><span class="line">            obj = [obj]</span><br><span class="line">        <span class="keyword">return</span> obj</span><br><span class="line">    </span><br><span class="line">    labels = _make_list(labels)</span><br><span class="line">    colors = _make_list(colors, [<span class="string">'b'</span>, <span class="string">'g'</span>, <span class="string">'r'</span>, <span class="string">'m'</span>, <span class="string">'c'</span>])</span><br><span class="line">    <span class="keyword">for</span> i, bbox <span class="keyword">in</span> enumerate(bboxes):</span><br><span class="line">        color = colors[i % len(colors)]</span><br><span class="line">        rect = bbox_to_rect(bbox.numpy(), color)</span><br><span class="line">        axes.add_patch(rect)</span><br><span class="line">        <span class="keyword">if</span> labels <span class="keyword">and</span> len(labels) &gt; i:</span><br><span class="line">            text_color = <span class="string">'k'</span> <span class="keyword">if</span> color == <span class="string">'w'</span> <span class="keyword">else</span> <span class="string">'w'</span></span><br><span class="line">            axes.text(rect.xy[<span class="number">0</span>], rect.xy[<span class="number">1</span>], labels[i],</span><br><span class="line">                va=<span class="string">'center'</span>, ha=<span class="string">'center'</span>, fontsize=<span class="number">6</span>,</span><br><span class="line">                color=text_color, bbox=dict(facecolor=color, lw=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<p>刚刚我们看到，变量boxes中x和y轴的坐标值分别已除以图像的宽和高。在绘图时，我们需要恢复锚框的原始坐标值，并因此定义了变量bbox_scale。现在可以画出图像中以(250, 250)为中心的所有锚框了。可以看到，大小为0.75且宽高比为1的锚框较好地覆盖了图像中的狗。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_svg_display</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""Use svg format to display plot in jupyter"""</span></span><br><span class="line">    display.set_matplotlib_formats(<span class="string">'svg'</span>)</span><br><span class="line"></span><br><span class="line">use_svg_display()</span><br><span class="line"><span class="comment"># 设置图的尺寸</span></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">3.5</span>, <span class="number">2.5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig = plt.imshow(img)</span><br><span class="line">bbox_scale = tf.constant([[w,h,w,h]], dtype=tf.float32)</span><br><span class="line">show_bboxes(fig.axes, tf.multiply(boxes[<span class="number">200</span>,<span class="number">250</span>,:,:], bbox_scale), </span><br><span class="line">    [<span class="string">'s=0.75, r=1'</span>, <span class="string">'s=0.75, r=2'</span>, <span class="string">'s=0.55, r=0.5'</span>, </span><br><span class="line">     <span class="string">'s=0.5, r=1'</span>, <span class="string">'s=0.25, r=1'</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/chapter_computer-vision_anchor_9_0.svg" alt></p>
<h2 id="jiao-bing-bi">交并比</h2>
<p>刚刚提到某个锚框较好地覆盖了图像中的狗。如果该目标的真实边界框已知，这里的“较好”该如何量化呢？一种直观的方法是衡量锚框和真实边界框之间的相似度。Jaccard系数可以衡量两个集合的相似度。给定集合A和B，它们的Jaccard系数即二者交集大小除以二者并集大小：<br>
\[
J(\mathcal{A}, \mathcal{B})=\frac{|\mathcal{A} \cap \mathcal{B}|}{|\mathcal{A} \cup \mathcal{B} \mid}
\]<br>
实际上，可以把边界框内的像素区域看成是像素的集合。如此一来，可以用两个边界框的像素集合的Jaccard系数衡量这两个边界框的相似度。当衡量两个边界框的相似度时，通常将Jaccard系数称为交并比，即两个边界框相交面积与相并面积之比，如下图所示。交并比的取值范围在0和1之间：0表示两个边界框无重合像素，1表示两个边界框相等。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/9.4_iou.svg" alt></p>
<p>下面对其进行实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">set_1 = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]]</span><br><span class="line">set_2 = [[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]]</span><br><span class="line">lower_bounds = tf.maximum(tf.expand_dims(set_1, axis=<span class="number">1</span>), tf.expand_dims(set_2, axis=<span class="number">0</span>)) <span class="comment"># (n1, n2, 2)</span></span><br><span class="line">upper_bounds = tf.minimum(tf.expand_dims(set_1, axis=<span class="number">1</span>), tf.expand_dims(set_2, axis=<span class="number">0</span>)) <span class="comment"># (n1, n2, 2)</span></span><br><span class="line"></span><br><span class="line">tf.expand_dims(set_1, axis=<span class="number">1</span>), tf.expand_dims(set_2, axis=<span class="number">0</span>), lower_bounds, tf.multiply(set_1, set_2), tf.subtract(set_1, set_2)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_intersection</span><span class="params">(set_1, set_2)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算anchor之间的交集</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)</span></span><br><span class="line"><span class="string">        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># tensorflow auto-broadcasts singleton dimensions</span></span><br><span class="line">    lower_bounds = tf.maximum(tf.expand_dims(set_1[:,:<span class="number">2</span>], axis=<span class="number">1</span>), tf.expand_dims(set_2[:,:<span class="number">2</span>], axis=<span class="number">0</span>)) <span class="comment"># (n1, n2, 2)</span></span><br><span class="line">    upper_bounds = tf.minimum(tf.expand_dims(set_1[:,<span class="number">2</span>:], axis=<span class="number">1</span>), tf.expand_dims(set_2[:,<span class="number">2</span>:], axis=<span class="number">0</span>)) <span class="comment"># (n1, n2, 2)</span></span><br><span class="line">    <span class="comment"># 设置最小值</span></span><br><span class="line">    intersection_dims = tf.clip_by_value(upper_bounds - lower_bounds, clip_value_min=<span class="number">0</span>, clip_value_max=<span class="number">3</span>) <span class="comment"># (n1, n2, 2)</span></span><br><span class="line">    <span class="keyword">return</span> tf.multiply(intersection_dims[:, :, <span class="number">0</span>], intersection_dims[:, :, <span class="number">1</span>]) <span class="comment"># (n1, n2)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_jaccard</span><span class="params">(set_1, set_2)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算anchor之间的Jaccard系数(IoU)</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)</span></span><br><span class="line"><span class="string">        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Find intersections</span></span><br><span class="line">    intersection = compute_intersection(set_1, set_2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Find areas of each box in both sets</span></span><br><span class="line">    areas_set_1 = tf.multiply(tf.subtract(set_1[:, <span class="number">2</span>], set_1[:, <span class="number">0</span>]), tf.subtract(set_1[:, <span class="number">3</span>], set_1[:, <span class="number">1</span>]))  <span class="comment"># (n1)</span></span><br><span class="line">    areas_set_2 = tf.multiply(tf.subtract(set_2[:, <span class="number">2</span>], set_2[:, <span class="number">0</span>]), tf.subtract(set_2[:, <span class="number">3</span>], set_2[:, <span class="number">1</span>]))  <span class="comment"># (n2)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Find the union</span></span><br><span class="line">    union = tf.add(tf.expand_dims(areas_set_1, axis=<span class="number">1</span>), tf.expand_dims(areas_set_2, axis=<span class="number">0</span>))  <span class="comment"># (n1, n2)</span></span><br><span class="line">    union = tf.subtract(union, intersection)  <span class="comment"># (n1, n2)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.divide(intersection, union) <span class="comment">#(n1, n2)</span></span><br></pre></td></tr></table></figure>
<p>使用交并比来衡量锚框与真实边界框以及锚框与锚框之间的相似度。</p>
<h2 id="biao-zhu-xun-lian-ji-de-mao-kuang">标注训练集的锚框</h2>
<p>在训练集中，将每个锚框视为一个训练样本。为了训练目标检测模型，需要为每个锚框标注两类标签：一是锚框所含目标的类别，简称类别；二是真实边界框相对锚框的偏移量，简称偏移量。在目标检测时，首先生成多个锚框，然后为每个锚框预测类别以及偏移量，接着根据预测的偏移量调整锚框位置从而得到预测边界框，最后筛选需要输出的预测边界框。</p>
<p>在目标检测的训练集中，每个图像已标注了真实边界框的位置以及所含目标的类别。在生成锚框之后，主要依据与锚框相似的真实边界框的位置和类别信息为锚框标注。那么，该如何为锚框分配与其相似的真实边界框呢？</p>
<p>假设图像中锚框分别为 \(A_1,A_2,\dots,A_{na}\) ，真实边界框分别为 \(B_1,B_2,\ldots,B_{nb}\) ，且 \(na\ge nb\) 。定义矩阵 \(X \in R_{na×nb}\) ，其中第 i 行第 j 列的元素 \(x_{ij}\) 为锚框 Ai 与真实边界框 Bj 的交并比。 首先，找出矩阵 \(X\) 中最大元素，并将该元素的行索引与列索引分别记为 \(i_1,j_1\) 。为锚框 \(A_{i_1}\) 分配真实边界框 \(B_{j_1}\) 。显然，锚框 \(A_{i_1}\) 和真实边界框 \(B_{j_1}\) 在所有的“锚框—真实边界框”的配对中相似度最高。接下来，将矩阵 \(X\) 中第 \(i_1\) 行和第 \(j_1\) 列上的所有元素丢弃。找出矩阵 X 中剩余的最大元素，并将该元素的行索引与列索引分别记为 \(i_2,j_2\) 。为锚框 \(A_{i_2}\) 分配真实边界框 \(B_{j_2}\) ，再将矩阵 X 中第 \(i_2\) 行和第 \(j_2\) 列上的所有元素丢弃。此时矩阵 X 中已有2行2列的元素被丢弃。 依此类推，直到矩阵 X 中所有 \(n_b\) 列元素全部被丢弃。这个时候，已为 \(n_b\) 个锚框各分配了一个真实边界框。 接下来，只遍历剩余的 \(n_a−n_b\) 个锚框：给定其中的锚框 \(A_i\) ，根据矩阵 X 的第 \(i\) 行找到与 \(A_i\) 交并比最大的真实边界框 \(B_j\) ，且只有当该交并比大于预先设定的阈值时，才为锚框 \(A_i\) 分配真实边界框 \(B_j\) 。</p>
<p>如下图所示，假设矩阵 X 中最大值为 \(x_{23}\) ，将为锚框 \(A_2\) 分配真实边界框 \(B_3\) 。然后，丢弃矩阵中第2行和第3列的所有元素，找出剩余阴影部分的最大元素 \(x_{71}\) ，为锚框 \(A_7\) 分配真实边界框 \(B_1\) 。接着如下图（中）所示，丢弃矩阵中第7行和第1列的所有元素，找出剩余阴影部分的最大元素 \(x_{54}\) ，为锚框 \(A_5\) 分配真实边界框 \(B_4\) 。最后如下图（右）所示，丢弃矩阵中第5行和第4列的所有元素，找出剩余阴影部分的最大元素 \(x_{92}\) ，为锚框 \(A_9\) 分配真实边界框 \(B_2\) 。之后，我们只需遍历除去 \(A_2,A_5,A_7,A_9\) 的剩余锚框，并根据阈值判断是否为剩余锚框分配真实边界框。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/anchor-label.svg" alt></p>
<p>现在可以标注锚框的类别和偏移量了。如果一个锚框 A 被分配了真实边界框 B ，将锚框 A 的类别设为 B 的类别，并根据 B 和 A 的中心坐标的相对位置以及两个框的相对大小为锚框 A 标注偏移量。由于数据集中各个框的位置和大小各异，因此这些相对位置和相对大小通常需要一些特殊变换，才能使偏移量的分布更均匀从而更容易拟合。设锚框 A 及其被分配的真实边界框 B 的中心坐标分别为 \((x_a,y_a)\) 和 \((x_b,y_b)\) ， A 和 B 的宽分别为 \(w_a\) 和 \(w_b\) ，高分别为 \(h_a\) 和 \(h_b\) ，一个常用的技巧是将 A 的偏移量标注为<br>
\[
\left(\frac{\frac{x_{b}-x_{a}}{w_{a}}-\mu_{x}}{\sigma_{x}} ,
\frac{\frac{y_{b}-y_{a}}{h_{a}}-\mu_{y}}{\sigma_{y}} ,
\frac{\log \frac{w_{b}}{w_{a}}-\mu_{w}}{\sigma_{w}},
\frac{\log \frac{h_{b}}{h_{a}}-\mu_{h}}{\sigma_{h}}
\right)
\]<br>
其中常数的默认值为 \(\mu_x=\mu_y=\mu_w=\mu_h=0\),\(\sigma_x=\sigma_y=0.1\),\(\sigma_w=\sigma_h=0.2\) 。如果一个锚框没有被分配真实边界框，只需将该锚框的类别设为背景。类别为背景的锚框通常被称为负类锚框，其余则被称为正类锚框。</p>
<p>下面演示一个具体的例子。为读取的图像中的猫和狗定义真实边界框，其中第一个元素为类别（0为狗，1为猫），剩余4个元素分别为左上角的 x 和 y 轴坐标以及右下角的 x 和 y 轴坐标（值域在0到1之间）。这里通过左上角和右下角的坐标构造了5个需要标注的锚框，分别记为 \(A_0,\ldots,A_4\) 。先画出这些锚框与真实边界框在图像中的位置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">bbox_scale = tf.constant([[w,h,w,h]], dtype=tf.float32)</span><br><span class="line">ground_truth = tf.constant([[<span class="number">0</span>, <span class="number">0.13</span>, <span class="number">0</span>, <span class="number">0.47</span>, <span class="number">1</span>],</span><br><span class="line">                [<span class="number">1</span>, <span class="number">0.47</span>, <span class="number">0.15</span>, <span class="number">0.84</span>, <span class="number">1</span>]])</span><br><span class="line">anchors = tf.constant([[<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>],</span><br><span class="line">            [<span class="number">0.15</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.4</span>],</span><br><span class="line">            [<span class="number">0.63</span>, <span class="number">0.05</span>, <span class="number">0.88</span>, <span class="number">0.98</span>],</span><br><span class="line">            [<span class="number">0.66</span>, <span class="number">0.45</span>, <span class="number">0.8</span>, <span class="number">0.8</span>],</span><br><span class="line">            [<span class="number">0.57</span>, <span class="number">0.3</span>,  <span class="number">0.92</span>, <span class="number">0.9</span>]])</span><br><span class="line"></span><br><span class="line">fig = plt.imshow(img)</span><br><span class="line">show_bboxes(fig.axes, tf.multiply(ground_truth[:, <span class="number">1</span>:], bbox_scale),</span><br><span class="line">        [<span class="string">'dog'</span>, <span class="string">'cat'</span>], <span class="string">'k'</span>)</span><br><span class="line">show_bboxes(fig.axes, tf.multiply(anchors, bbox_scale),</span><br><span class="line">        [<span class="string">'0'</span>, <span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>, <span class="string">'4'</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_38_0.svg" alt></p>
<p>下面MultiBoxTarget函数来为锚框标注类别和偏移量。该函数将背景类别设为0，并令从零开始的目标类别的整数索引自加1（1为狗，2为猫）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">assign_anchor</span><span class="params">(bb, anchor, jaccard_threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        bb: 真实边界框(bounding box), shape:（nb, 4）</span></span><br><span class="line"><span class="string">        anchor: 待分配的anchor, shape:（na, 4）</span></span><br><span class="line"><span class="string">        jaccard_threshold: 预先设定的阈值</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        assigned_idx: shape: (na, ), 每个anchor分配的真实bb对应的索引, 若未分配任何bb则为-1</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    na = anchor.shape[<span class="number">0</span>]</span><br><span class="line">    nb = bb.shape[<span class="number">0</span>]</span><br><span class="line">    jaccard = compute_jaccard(anchor, bb).numpy()   <span class="comment"># shape: (na, nb)</span></span><br><span class="line">    assigned_idx = np.ones(na) * <span class="number">-1</span> <span class="comment"># 初始全为-1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 先为每个bb分配一个anchor（不要求满足jaccard_threshold）</span></span><br><span class="line">    jaccard_cp = jaccard.copy()</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(nb):</span><br><span class="line">        i = np.argmax(jaccard_cp[:, j])</span><br><span class="line">        assigned_idx[i] = j</span><br><span class="line">        jaccard_cp[i, :] = float(<span class="string">"-inf"</span>)    <span class="comment"># 赋值为负无穷, 相当于去掉这一行</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 处理还未被分配的anchor， 要求满足jaccard_threshold</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(na):</span><br><span class="line">        <span class="keyword">if</span> assigned_idx[i] == <span class="number">-1</span>:</span><br><span class="line">            j = np.argmax(jaccard[i, :])</span><br><span class="line">            <span class="keyword">if</span> jaccard[i, j] &gt;= jaccard_threshold:</span><br><span class="line">                assigned_idx[i] = j</span><br><span class="line">    <span class="keyword">return</span> tf.cast(assigned_idx, tf.int32)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xy_to_cxcy</span><span class="params">(xy)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将(x_min, y_min, x_max, y_max)形式的anchor转换成(center_x, center_y, w, h)形式的.</span></span><br><span class="line"><span class="string">    https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        xy: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> tf.concat(((xy[:, <span class="number">2</span>:] + xy[:, :<span class="number">2</span>]) / <span class="number">2</span>,  <span class="comment">#c_x, c_y</span></span><br><span class="line">              xy[:, <span class="number">2</span>:] - xy[:, :<span class="number">2</span>]), axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MultiBoxTarget</span><span class="params">(anchor, label)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        anchor: torch tensor, 输入的锚框, 一般是通过MultiBoxPrior生成, shape:（1，锚框总数，4）</span></span><br><span class="line"><span class="string">        label: 真实标签, shape为(bn, 每张图片最多的真实锚框数, 5)</span></span><br><span class="line"><span class="string">               第二维中，如果给定图片没有这么多锚框, 可以先用-1填充空白, 最后一维中的元素为[类别标签, 四个坐标值]</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        列表, [bbox_offset, bbox_mask, cls_labels]</span></span><br><span class="line"><span class="string">        bbox_offset: 每个锚框的标注偏移量，形状为(bn，锚框总数*4)</span></span><br><span class="line"><span class="string">        bbox_mask: 形状同bbox_offset, 每个锚框的掩码, 一一对应上面的偏移量, 负类锚框(背景)对应的掩码均为0, 正类锚框的掩码均为1</span></span><br><span class="line"><span class="string">        cls_labels: 每个锚框的标注类别, 其中0表示为背景, 形状为(bn，锚框总数)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">assert</span> len(anchor.shape) == <span class="number">3</span> <span class="keyword">and</span> len(label.shape) == <span class="number">3</span></span><br><span class="line">    bn = label.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">MultiBoxTarget_one</span><span class="params">(anchor, label, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        MultiBoxTarget函数的辅助函数, 处理batch中的一个</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            anchor: shape of (锚框总数, 4)</span></span><br><span class="line"><span class="string">            label: shape of (真实锚框数, 5), 5代表[类别标签, 四个坐标值]</span></span><br><span class="line"><span class="string">            eps: 一个极小值, 防止log0</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            offset: (锚框总数*4, )</span></span><br><span class="line"><span class="string">            bbox_mask: (锚框总数*4, ), 0代表背景, 1代表非背景</span></span><br><span class="line"><span class="string">            cls_labels: (锚框总数, 4), 0代表背景</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        an = anchor.shape[<span class="number">0</span>]</span><br><span class="line">        assigned_idx = assign_anchor(label[:, <span class="number">1</span>:], anchor) <span class="comment">## (锚框总数, )</span></span><br><span class="line">        <span class="comment"># 决定anchor留下或者舍去</span></span><br><span class="line">        bbox_mask = tf.repeat(tf.expand_dims(tf.cast((assigned_idx &gt;= <span class="number">0</span>), dtype=tf.double), axis=<span class="number">-1</span>), repeats=<span class="number">4</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        cls_labels = np.zeros(an, dtype=int) <span class="comment"># 0表示背景</span></span><br><span class="line">        assigned_bb = np.zeros((an, <span class="number">4</span>), dtype=float) <span class="comment"># 所有anchor对应的bb坐标</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(an):</span><br><span class="line">            bb_idx = assigned_idx[i]</span><br><span class="line">            <span class="keyword">if</span> bb_idx &gt;= <span class="number">0</span>: <span class="comment"># 即非背景</span></span><br><span class="line">                cls_labels[i] = label.numpy()[bb_idx, <span class="number">0</span>] + <span class="number">1</span> <span class="comment"># 要注意加1</span></span><br><span class="line">                assigned_bb[i, :] = label.numpy()[bb_idx, <span class="number">1</span>:]</span><br><span class="line">        </span><br><span class="line">        center_anchor = tf.cast(xy_to_cxcy(anchor), dtype=tf.double)  <span class="comment"># (center_x, center_y, w, h)</span></span><br><span class="line">        center_assigned_bb = tf.cast(xy_to_cxcy(assigned_bb), dtype=tf.double) <span class="comment"># (center_x, center_y, w, h)</span></span><br><span class="line"></span><br><span class="line">        offset_xy = <span class="number">10.0</span> * (center_assigned_bb[:,:<span class="number">2</span>] - center_anchor[:,:<span class="number">2</span>]) / center_anchor[:,<span class="number">2</span>:]</span><br><span class="line">        offset_wh = <span class="number">5.0</span> * tf.math.log(eps + center_assigned_bb[:, <span class="number">2</span>:] / center_anchor[:, <span class="number">2</span>:])</span><br><span class="line">        offset = tf.multiply(tf.concat((offset_xy, offset_wh), axis=<span class="number">1</span>), bbox_mask)    <span class="comment"># (锚框总数, 4)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tf.reshape(offset, (<span class="number">-1</span>,)), tf.reshape(bbox_mask, (<span class="number">-1</span>,)), cls_labels</span><br><span class="line">    </span><br><span class="line">    batch_offset = []</span><br><span class="line">    batch_mask = []</span><br><span class="line">    batch_cls_labels = []</span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> range(bn):</span><br><span class="line">        offset, bbox_mask, cls_labels = MultiBoxTarget_one(anchor[<span class="number">0</span>, :, :], label[b,:,:])</span><br><span class="line"></span><br><span class="line">        batch_offset.append(offset)</span><br><span class="line">        batch_mask.append(bbox_mask)</span><br><span class="line">        batch_cls_labels.append(cls_labels)</span><br><span class="line">    </span><br><span class="line">    batch_offset = tf.convert_to_tensor(batch_offset)</span><br><span class="line">    batch_mask = tf.convert_to_tensor(batch_mask)</span><br><span class="line">    batch_cls_labels = tf.convert_to_tensor(batch_cls_labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [batch_offset, batch_mask, batch_cls_labels]</span><br></pre></td></tr></table></figure>
<p>我们通过tf.expand_dims函数为锚框和真实边界框添加样本维。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">labels = MultiBoxTarget(tf.expand_dims(anchors, axis=<span class="number">0</span>), </span><br><span class="line">            tf.expand_dims(ground_truth, axis=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<p>根据锚框与真实边界框在图像中的位置来分析这些标注的类别。首先，在所有的“锚框—真实边界框”的配对中，锚框 A4 与猫的真实边界框的交并比最大，因此锚框 A4 的类别标注为猫。不考虑锚框 A4 或猫的真实边界框，在剩余的“锚框—真实边界框”的配对中，最大交并比的配对为锚框 A1 和狗的真实边界框，因此锚框 A1 的类别标注为狗。接下来遍历未标注的剩余3个锚框：与锚框 A0 交并比最大的真实边界框的类别为狗，但交并比小于阈值（默认为0.5），因此类别标注为背景；与锚框 A2 交并比最大的真实边界框的类别为猫，且交并比大于阈值，因此类别标注为猫；与锚框 A3 交并比最大的真实边界框的类别为猫，但交并比小于阈值，因此类别标注为背景。</p>
<p>返回值的第二项为掩码（mask）变量，形状为(批量大小, 锚框个数的四倍)。掩码变量中的元素与每个锚框的4个偏移量一一对应。 由于不关心对背景的检测，有关负类的偏移量不应影响目标函数。通过按元素乘法，掩码变量中的0可以在计算目标函数之前过滤掉负类的偏移量。</p>
<h2 id="shu-chu-yu-ce-bian-jie-kuang">输出预测边界框</h2>
<p>在模型预测阶段，先为图像生成多个锚框，并为这些锚框一一预测类别和偏移量。随后，根据锚框及其预测偏移量得到预测边界框。当锚框数量较多时，同一个目标上可能会输出较多相似的预测边界框。为了使结果更加简洁，可以移除相似的预测边界框。常用的方法叫作非极大值抑制。</p>
<p>来描述一下非极大值抑制的工作原理。对于一个预测边界框 B ，模型会计算各个类别的预测概率。设其中最大的预测概率为 p ，该概率所对应的类别即 B 的预测类别。也将 p 称为预测边界框 B 的置信度。在同一图像上，将预测类别非背景的预测边界框按置信度从高到低排序，得到列表 L 。从 L 中选取置信度最高的预测边界框 B1 作为基准，将所有与 B1 的交并比大于某阈值的非基准预测边界框从 L 中移除。这里的阈值是预先设定的超参数。此时， L 保留了置信度最高的预测边界框并移除了与其相似的其他预测边界框。 接下来，从 L 中选取置信度第二高的预测边界框 B2 作为基准，将所有与 B2 的交并比大于某阈值的非基准预测边界框从 L 中移除。重复这一过程，直到 L 中所有的预测边界框都曾作为基准。此时 L 中任意一对预测边界框的交并比都小于阈值。最终，输出列表 L 中的所有预测边界框。</p>
<p>先构造4个锚框。简单起见，假设预测偏移量全是0：预测边界框即锚框。最后，我们构造每个类别的预测概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">anchors = tf.convert_to_tensor([[<span class="number">0.1</span>, <span class="number">0.08</span>, <span class="number">0.52</span>, <span class="number">0.92</span>],</span><br><span class="line">                [<span class="number">0.08</span>, <span class="number">0.2</span>, <span class="number">0.56</span>, <span class="number">0.95</span>],</span><br><span class="line">                [<span class="number">0.15</span>, <span class="number">0.3</span>, <span class="number">0.62</span>, <span class="number">0.91</span>],</span><br><span class="line">                [<span class="number">0.55</span>, <span class="number">0.2</span>, <span class="number">0.9</span>, <span class="number">0.88</span>]])</span><br><span class="line">offset_preds = tf.convert_to_tensor([<span class="number">0.0</span>] * (<span class="number">4</span> * len(anchors)))</span><br><span class="line">cls_probs = tf.convert_to_tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>], <span class="comment"># 背景的预测概率</span></span><br><span class="line">                [<span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.7</span>, <span class="number">0.1</span>],    <span class="comment"># 狗的预测概率</span></span><br><span class="line">                [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.9</span>]])   <span class="comment"># 猫的预测概率</span></span><br><span class="line">anchors, offset_preds, cls_probs</span><br><span class="line"></span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">(&lt;tf.Tensor: shape=(<span class="number">4</span>, <span class="number">4</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">0.1</span> , <span class="number">0.08</span>, <span class="number">0.52</span>, <span class="number">0.92</span>],</span><br><span class="line">        [<span class="number">0.08</span>, <span class="number">0.2</span> , <span class="number">0.56</span>, <span class="number">0.95</span>],</span><br><span class="line">        [<span class="number">0.</span>  , <span class="number">0.3</span> , <span class="number">0.62</span>, <span class="number">0.91</span>],</span><br><span class="line">        [<span class="number">0.55</span>, <span class="number">0.2</span> , <span class="number">0.9</span> , <span class="number">0.88</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">16</span>,), dtype=float32, numpy=</span><br><span class="line"> array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">4</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.</span> ],</span><br><span class="line">        [<span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.7</span>, <span class="number">0.1</span>],</span><br><span class="line">        [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.9</span>]], dtype=float32)&gt;)</span><br></pre></td></tr></table></figure>
<p>在图像上打印预测边界框和它们的置信度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.imshow(img)</span><br><span class="line">show_bboxes(fig.axes, anchors * bbox_scale,</span><br><span class="line">        [<span class="string">'dog=0.9'</span>, <span class="string">'dog=0.8'</span>, <span class="string">'dog=0.7'</span>, <span class="string">'cat=0.9'</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_56_0.svg" alt></p>
<p>下面实现MultiBoxDetection函数来执行非极大值抑制。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line">Pred_BB_Info = namedtuple(<span class="string">"Pred_BB_Info"</span>, </span><br><span class="line">        [<span class="string">"index"</span>, <span class="string">"class_id"</span>, <span class="string">"confidence"</span>, <span class="string">"xyxy"</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">non_max_suppression</span><span class="params">(bb_info_list, nms_threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    非极大抑制处理预测的边界框</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        bb_info_list: Pred_BB_Info的列表, 包含预测类别、置信度等信息</span></span><br><span class="line"><span class="string">        nms_threshold: 阈值</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        output: Pred_BB_Info的列表, 只保留过滤后的边界框信息</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    output = []</span><br><span class="line">    <span class="comment"># 现根据置信度从高到底排序</span></span><br><span class="line">    sorted_bb_info_list = sorted(bb_info_list,</span><br><span class="line">                    key = <span class="keyword">lambda</span> x: x.confidence, </span><br><span class="line">                    reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">while</span> len(sorted_bb_info_list) != <span class="number">0</span>:</span><br><span class="line">        best = sorted_bb_info_list.pop(<span class="number">0</span>)</span><br><span class="line">        output.append(best)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> len(sorted_bb_info_list) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        bb_xyxy = []</span><br><span class="line">        <span class="keyword">for</span> bb <span class="keyword">in</span> sorted_bb_info_list:</span><br><span class="line">            bb_xyxy.append(bb.xyxy)</span><br><span class="line">        </span><br><span class="line">        iou = compute_jaccard(tf.convert_to_tensor(best.xyxy),</span><br><span class="line">                    tf.squeeze(tf.convert_to_tensor(bb_xyxy), axis=<span class="number">1</span>))[<span class="number">0</span>] <span class="comment"># shape: (len(sorted_bb_info_list), )</span></span><br><span class="line">        n = len(sorted_bb_info_list)</span><br><span class="line">        sorted_bb_info_list = [</span><br><span class="line">                    sorted_bb_info_list[i] <span class="keyword">for</span> i <span class="keyword">in</span> </span><br><span class="line">                    range(n) <span class="keyword">if</span> iou[i] &lt;= nms_threshold]</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MultiBoxDetection</span><span class="params">(cls_prob, loc_pred, anchor, nms_threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        cls_prob: 经过softmax后得到的各个锚框的预测概率, shape:(bn, 预测总类别数+1, 锚框个数)</span></span><br><span class="line"><span class="string">        loc_pred: 预测的各个锚框的偏移量, shape:(bn, 锚框个数*4)</span></span><br><span class="line"><span class="string">        anchor: MultiBoxPrior输出的默认锚框, shape: (1, 锚框个数, 4)</span></span><br><span class="line"><span class="string">        nms_threshold: 非极大抑制中的阈值</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        所有锚框的信息, shape: (bn, 锚框个数, 6)</span></span><br><span class="line"><span class="string">        每个锚框信息由[class_id, confidence, xmin, ymin, xmax, ymax]表示</span></span><br><span class="line"><span class="string">        class_id=-1 表示背景或在非极大值抑制中被移除了</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">assert</span> len(cls_prob.shape) == <span class="number">3</span> <span class="keyword">and</span> len(loc_pred.shape) == <span class="number">2</span> <span class="keyword">and</span> len(anchor.shape) == <span class="number">3</span></span><br><span class="line">    bn = cls_prob.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">MultiBoxDetection_one</span><span class="params">(c_p, l_p, anc, nms_threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        MultiBoxDetection的辅助函数, 处理batch中的一个</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            c_p: (预测总类别数+1, 锚框个数)</span></span><br><span class="line"><span class="string">            l_p: (锚框个数*4, )</span></span><br><span class="line"><span class="string">            anc: (锚框个数, 4)</span></span><br><span class="line"><span class="string">            nms_threshold: 非极大抑制中的阈值</span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            output: (锚框个数, 6)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        pred_bb_num = c_p.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 加上偏移量</span></span><br><span class="line">        anc = tf.add(anc, tf.reshape(l_p, (pred_bb_num, <span class="number">4</span>))).numpy()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最大的概率</span></span><br><span class="line">        confidence = tf.reduce_max(c_p, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 最大概率对应的id</span></span><br><span class="line">        class_id = tf.argmax(c_p, axis=<span class="number">0</span>)</span><br><span class="line">        confidence = confidence.numpy()</span><br><span class="line">        class_id = class_id.numpy()</span><br><span class="line"></span><br><span class="line">        pred_bb_info = [Pred_BB_Info(index=i,</span><br><span class="line">                    class_id=class_id[i]<span class="number">-1</span>,</span><br><span class="line">                    confidence=confidence[i],</span><br><span class="line">                    xyxy=[anc[i]]) <span class="comment"># xyxy是个列表</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(pred_bb_num)]</span><br><span class="line">        <span class="comment"># 正类的index</span></span><br><span class="line">        obj_bb_idx = [bb.index <span class="keyword">for</span> bb </span><br><span class="line">                <span class="keyword">in</span> non_max_suppression(pred_bb_info,</span><br><span class="line">                            nms_threshold)]</span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> bb <span class="keyword">in</span> pred_bb_info:</span><br><span class="line">            output.append(np.append([</span><br><span class="line">                (bb.class_id <span class="keyword">if</span> bb.index <span class="keyword">in</span> obj_bb_idx </span><br><span class="line">                        <span class="keyword">else</span> <span class="number">-1.0</span>),</span><br><span class="line">                bb.confidence],</span><br><span class="line">                bb.xyxy))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> tf.convert_to_tensor(output) <span class="comment"># shape: (锚框个数， 6)</span></span><br><span class="line">    </span><br><span class="line">    batch_output = []</span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> range(bn):</span><br><span class="line">        batch_output.append(MultiBoxDetection_one(cls_prob[b],</span><br><span class="line">                        loc_pred[b], anchor[<span class="number">0</span>],</span><br><span class="line">                        nms_threshold))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tf.convert_to_tensor(batch_output)</span><br></pre></td></tr></table></figure>
<p>然后运行MultiBoxDetection函数并设阈值为0.5。这里为输入都增加了样本维。看到返回的结果的形状为(批量大小, 锚框个数, 6)。其中每一行的6个元素代表同一个预测边界框的输出信息。第一个元素是索引从0开始计数的预测类别（0为狗，1为猫），其中-1表示背景或在非极大值抑制中被移除。第二个元素是预测边界框的置信度。剩余的4个元素分别是预测边界框左上角的xx和yy轴坐标以及右下角的xx和yy轴坐标（值域在0到1之间）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">output = MultiBoxDetection(</span><br><span class="line">    tf.expand_dims(cls_probs, <span class="number">0</span>),</span><br><span class="line">    tf.expand_dims(offset_preds, <span class="number">0</span>),</span><br><span class="line">    tf.expand_dims(anchors, <span class="number">0</span>),</span><br><span class="line">    nms_threshold=<span class="number">0.5</span>)</span><br><span class="line">output</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">1</span>, <span class="number">4</span>, <span class="number">6</span>), dtype=float64, numpy=</span><br><span class="line">array([[[ <span class="number">0.</span>        ,  <span class="number">0.89999998</span>,  <span class="number">0.1</span>       ,  <span class="number">0.08</span>      ,</span><br><span class="line">          <span class="number">0.51999998</span>,  <span class="number">0.92000002</span>],</span><br><span class="line">        [<span class="number">-1.</span>        ,  <span class="number">0.80000001</span>,  <span class="number">0.08</span>      ,  <span class="number">0.2</span>       ,</span><br><span class="line">          <span class="number">0.56</span>      ,  <span class="number">0.94999999</span>],</span><br><span class="line">        [<span class="number">-1.</span>        ,  <span class="number">0.69999999</span>,  <span class="number">0.</span>        ,  <span class="number">0.30000001</span>,</span><br><span class="line">          <span class="number">0.62</span>      ,  <span class="number">0.91000003</span>],</span><br><span class="line">        [ <span class="number">1.</span>        ,  <span class="number">0.89999998</span>,  <span class="number">0.55000001</span>,  <span class="number">0.2</span>       ,</span><br><span class="line">          <span class="number">0.89999998</span>,  <span class="number">0.88</span>      ]]])&gt;</span><br></pre></td></tr></table></figure>
<p>移除掉类别为-1的预测边界框，并可视化非极大值抑制保留的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.imshow(img)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> output[<span class="number">0</span>].numpy():</span><br><span class="line">    <span class="keyword">if</span> i[<span class="number">0</span>] == <span class="number">-1</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    label = (<span class="string">'dog='</span>, <span class="string">'cat='</span>)[int(i[<span class="number">0</span>])] + str(i[<span class="number">1</span>])</span><br><span class="line">    show_bboxes(fig.axes, tf.multiply(i[<span class="number">2</span>:], bbox_scale), label)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_63_0.svg" alt></p>
<p>实践中，可以在执行非极大值抑制前将置信度较低的预测边界框移除，从而减小非极大值抑制的计算量。还可以筛选非极大值抑制的输出，例如，只保留其中置信度较高的结果作为最终输出。</p>
<h1 id="duo-chi-du-mu-biao-jian-ce">多尺度目标检测</h1>
<p>在实验中以输入图像的每个像素为中心生成多个锚框,这些锚框是对输入图像不同区域的采样。然而，如果以图像每个像素为中心都生成锚框，很容易生成过多锚框而造成计算量过大。举个例子，假设输入图像的高和宽分别为561像素和728像素，如果以每个像素为中心生成5个不同形状的锚框，那么一张图像上则需要标注并预测200多万个锚框（\(561 \times 728 \times 5\)）。</p>
<p>减少锚框个数并不难。一种简单的方法是在输入图像中均匀采样一小部分像素，并以采样的像素为中心生成锚框。此外，在不同尺度下，可以生成不同数量和不同大小的锚框。值得注意的是，较小目标比较大目标在图像上出现位置的可能性更多。举个简单的例子：形状为\(1 \times 1\)、\(1 \times 2\)和\(2 \times 2\)的目标在形状为\(2 \times 2\)的图像上可能出现的位置分别有4、2和1种。因此，当使用较小锚框来检测较小目标时，可以采样较多的区域；而当使用较大锚框来检测较大目标时，我们可以采样较少的区域。可以通过定义特征图的形状来确定任一图像上均匀采样的锚框中心。</p>
<p>下面定义<code>display_anchors</code>函数。在特征图<code>fmap</code>上以每个单元（像素）为中心生成锚框<code>anchors</code>。由于锚框<code>anchors</code>中\(x\)和\(y\)轴的坐标值分别已除以特征图<code>fmap</code>的宽和高，这些值域在0和1之间的值表达了锚框在特征图中的相对位置。由于锚框<code>anchors</code>的中心遍布特征图<code>fmap</code>上的所有单元，<code>anchors</code>的中心在任一图像的空间相对位置一定是均匀分布的。具体来说，当特征图的宽和高分别设为<code>fmap_w</code>和<code>fmap_h</code>时，该函数将在任一图像上均匀采样<code>fmap_h</code>行<code>fmap_w</code>列个像素，并分别以它们为中心生成大小为<code>s</code>（假设列表<code>s</code>长度为1）的不同宽高比（<code>ratios</code>）的锚框。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_anchors</span><span class="params">(fmap_w, fmap_h, s)</span>:</span></span><br><span class="line">    fmap = np.zeros((<span class="number">1</span>, <span class="number">10</span>, fmap_w, fmap_h))  <span class="comment"># 前两维的取值不影响输出结果</span></span><br><span class="line">    anchors = MultiBoxPrior(fmap, sizes=s, ratios=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>])</span><br><span class="line">    bbox_scale = np.array((w, h, w, h))</span><br><span class="line">    show_bboxes(plt.imshow(img).axes,</span><br><span class="line">                    anchors[<span class="number">0</span>] * bbox_scale)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MultiBoxPrior</span><span class="params">(feature_map, sizes=[<span class="number">0.75</span>, <span class="number">0.5</span>, <span class="number">0.25</span>], ratios=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>])</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        feature_map: torch tensor, Shape: [N, C, H, W].</span></span><br><span class="line"><span class="string">        sizes: List of sizes (0~1) of generated MultiBoxPriores. </span></span><br><span class="line"><span class="string">        ratios: List of aspect ratios (non-negative) of generated MultiBoxPriores. </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        anchors of shape (1, num_anchors, 4).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    pairs = [] <span class="comment"># pair of (size, sqrt(ratio))</span></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> ratios:</span><br><span class="line">        pairs.append([sizes[<span class="number">0</span>], np.sqrt(r)])</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> sizes[<span class="number">1</span>:]:</span><br><span class="line">        pairs.append([s, np.sqrt(ratios[<span class="number">0</span>])])</span><br><span class="line">    </span><br><span class="line">    pairs = np.array(pairs)</span><br><span class="line"></span><br><span class="line">    ss1 = pairs[:, <span class="number">0</span>] * pairs[:, <span class="number">1</span>] <span class="comment"># size * sqrt(ration)</span></span><br><span class="line">    ss2 = pairs[:, <span class="number">0</span>] / pairs[:, <span class="number">1</span>] <span class="comment"># size / sqrt(retion)</span></span><br><span class="line"></span><br><span class="line">    base_anchors = tf.stack([-ss1, -ss2, ss1, ss2], axis=<span class="number">1</span>) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    h, w = feature_map.shape[<span class="number">-2</span>:]</span><br><span class="line">    shifts_x = tf.divide(tf.range(<span class="number">0</span>, w), w)</span><br><span class="line">    shifts_y = tf.divide(tf.range(<span class="number">0</span>, h), h)</span><br><span class="line">    shift_x, shift_y = tf.meshgrid(shifts_x, shifts_y)</span><br><span class="line">    shift_x = tf.reshape(shift_x, (<span class="number">-1</span>,))</span><br><span class="line">    shift_y = tf.reshape(shift_y, (<span class="number">-1</span>,))</span><br><span class="line">    shifts = tf.stack((shift_x, shift_y, shift_x, shift_y), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    anchors = tf.add(tf.reshape(shifts, (<span class="number">-1</span>,<span class="number">1</span>,<span class="number">4</span>)), tf.reshape(base_anchors, (<span class="number">1</span>,<span class="number">-1</span>,<span class="number">4</span>)))</span><br><span class="line">    <span class="keyword">return</span> tf.cast(tf.reshape(anchors, (<span class="number">1</span>,<span class="number">-1</span>,<span class="number">4</span>)), tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_bboxes</span><span class="params">(axes, bboxes, labels=None, colors=None)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_list</span><span class="params">(obj, default_values=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> obj <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            obj = default_values</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> isinstance(obj, (list, tuple)):</span><br><span class="line">            obj = [obj]</span><br><span class="line">        <span class="keyword">return</span> obj</span><br><span class="line">    </span><br><span class="line">    labels = _make_list(labels)</span><br><span class="line">    colors = _make_list(colors, [<span class="string">'b'</span>, <span class="string">'g'</span>, <span class="string">'r'</span>, <span class="string">'m'</span>, <span class="string">'c'</span>])</span><br><span class="line">    <span class="keyword">for</span> i, bbox <span class="keyword">in</span> enumerate(bboxes):</span><br><span class="line">        color = colors[i % len(colors)]</span><br><span class="line">        rect = bbox_to_rect(bbox.numpy(), color)</span><br><span class="line">        axes.add_patch(rect)</span><br><span class="line">        <span class="keyword">if</span> labels <span class="keyword">and</span> len(labels) &gt; i:</span><br><span class="line">            text_color = <span class="string">'k'</span> <span class="keyword">if</span> color == <span class="string">'w'</span> <span class="keyword">else</span> <span class="string">'w'</span></span><br><span class="line">            axes.text(rect.xy[<span class="number">0</span>], rect.xy[<span class="number">1</span>], labels[i],</span><br><span class="line">                va=<span class="string">'center'</span>, ha=<span class="string">'center'</span>, fontsize=<span class="number">6</span>,</span><br><span class="line">                color=text_color, bbox=dict(facecolor=color, lw=<span class="number">0</span>))</span><br><span class="line">            </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox_to_rect</span><span class="params">(bbox, color)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> plt.Rectangle(</span><br><span class="line">        xy=(bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), width=bbox[<span class="number">2</span>]-bbox[<span class="number">0</span>], height=bbox[<span class="number">3</span>]-bbox[<span class="number">1</span>],</span><br><span class="line">        fill=<span class="literal">False</span>, edgecolor=color, linewidth=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">x = tf.zeros((<span class="number">1</span>,<span class="number">3</span>,h,w))</span><br><span class="line">y = MultiBoxPrior(x)</span><br><span class="line">y.shape <span class="comment"># TensorShape([1, 405720, 4])</span></span><br></pre></td></tr></table></figure>
<p>先关注小目标的检测。为了在显示时更容易分辨，这里令不同中心的锚框不重合：设锚框大小为0.15，特征图的高和宽分别为4。可以看出，图像上4行4列的锚框中心分布均匀。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_anchors(fmap_w=<span class="number">4</span>, fmap_h=<span class="number">4</span>, s=[<span class="number">0.15</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_5_0.png" alt></p>
<p>将特征图的高和宽分别减半，并用更大的锚框检测更大的目标。当锚框大小设0.4时，有些锚框的区域有重合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_anchors(fmap_w=<span class="number">2</span>, fmap_h=<span class="number">2</span>, s=[<span class="number">0.4</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_7_0.png" alt></p>
<p>最后，将特征图的高和宽进一步减半至1，并将锚框大小增至0.8。此时锚框中心即图像中心。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_anchors(fmap_w=<span class="number">1</span>, fmap_h=<span class="number">1</span>, s=[<span class="number">0.8</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_9_0-5863308.png" alt></p>
<p>既然已在多个尺度上生成了不同大小的锚框，相应地，需要在不同尺度下检测不同大小的目标。</p>
<p>下面介绍一种基于卷积神经网络的方法。</p>
<p>在某个尺度下，假设依据\(c_i\)张形状为\(h \times w\)的特征图生成\(h \times w\)组不同中心的锚框，且每组的锚框个数为\(a\)。例如，在刚才实验的第一个尺度下，依据10（通道数）张形状为\(4 \times 4\)的特征图生成了16组不同中心的锚框，且每组含3个锚框。<br>
接下来，依据真实边界框的类别和位置，每个锚框将被标注类别和偏移量。在当前的尺度下，目标检测模型需要根据输入图像预测\(h \times w\)组不同中心的锚框的类别和偏移量。</p>
<p>假设这里的\(c_i\)张特征图为卷积神经网络根据输入图像做前向计算所得的中间输出。既然每张特征图上都有\(h \times w\)个不同的空间位置，那么相同空间位置可以看作含有\(c_i\)个单元。<br>
根据二维卷积层中感受野的定义，特征图在相同空间位置的\(c_i\)个单元在输入图像上的感受野相同，并表征了同一感受野内的输入图像信息。<br>
因此，可以将特征图在相同空间位置的\(c_i\)个单元变换为以该位置为中心生成的\(a\)个锚框的类别和偏移量。<br>
本质上是用输入图像在某个感受野区域内的信息来预测输入图像上与该区域位置相近的锚框的类别和偏移量。</p>
<p>当不同层的特征图在输入图像上分别拥有不同大小的感受野时，它们将分别用来检测不同大小的目标。例如，可以通过设计网络，令较接近输出层的特征图中每个单元拥有更广阔的感受野，从而检测输入图像中更大尺寸的目标。</p>
<h1 id="r-cnn">R-CNN</h1>
<p>区域卷积神经网络（R-CNN）是将深度模型应用于目标检测的开创性工作之一 。下面介绍R-CNN和它的一系列改进方法：快速的R-CNN（Fast R-CNN）、更快的R-CNN（Faster R-CNN） 以及掩码R-CNN（Mask R-CNN）。</p>
<h2 id="r-cnn-1">R-CNN</h2>
<p>R-CNN首先对图像选取若干提议区域（如锚框也是一种选取方法）并标注它们的类别和边界框（如偏移量）。然后，用卷积神经网络对每个提议区域做前向计算抽取特征。之后，我们用每个提议区域的特征预测类别和边界框。下图描述了R-CNN模型。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/r-cnn.svg" alt></p>
<p>具体来说，R-CNN主要由以下4步构成。</p>
<ol>
<li>对输入图像使用选择性搜索来选取多个高质量的提议区域。这些提议区域通常是在多个尺度下选取的，并具有不同的形状和大小。每个提议区域将被标注类别和真实边界框。</li>
<li>选取一个预训练的卷积神经网络，并将其在输出层之前截断。将每个提议区域变形为网络需要的输入尺寸，并通过前向计算输出抽取的提议区域特征。</li>
<li>将每个提议区域的特征连同其标注的类别作为一个样本，训练多个支持向量机对目标分类。其中每个支持向量机用来判断样本是否属于某一个类别。</li>
<li>将每个提议区域的特征连同其标注的边界框作为一个样本，训练线性回归模型来预测真实边界框。</li>
</ol>
<p>R-CNN虽然通过预训练的卷积神经网络有效抽取了图像特征，但它的主要缺点是速度慢。想象一下，我们可能从一张图像中选出上千个提议区域，对该图像做目标检测将导致上千次的卷积神经网络的前向计算。这个巨大的计算量令R-CNN难以在实际应用中被广泛采用。</p>
<h2 id="fast-r-cnn">Fast R-CNN</h2>
<p>R-CNN的主要性能瓶颈在于需要对每个提议区域独立抽取特征。由于这些区域通常有大量重叠，独立的特征抽取会导致大量的重复计算。Fast R-CNN对R-CNN的一个主要改进在于只对整个图像做卷积神经网络的前向计算。</p>
<p>下图描述了Fast R-CNN模型。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/fast-rcnn.svg" alt></p>
<p>它的主要计算步骤如下。</p>
<ol>
<li>与R-CNN相比，Fast R-CNN用来提取特征的卷积神经网络的输入是整个图像，而不是各个提议区域。而且，这个网络通常会参与训练，即更新模型参数。设输入为一张图像，将卷积神经网络的输出的形状记为\(1×c×h_1×w_1\)。</li>
<li>假设选择性搜索生成n个提议区域。这些形状各异的提议区域在卷积神经网络的输出上分别标出形状各异的兴趣区域。这些兴趣区域需要抽取出形状相同的特征（假设高和宽均分别指定为h2和w2）以便于连结后输出。Fast R-CNN引入兴趣区域池化（region of interest pooling，RoI池化）层，将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征，形状为\(n×c×h_2×w_2\)。</li>
<li>通过全连接层将输出形状变换为\(n×d\)，其中超参数d取决于模型设计。</li>
<li>预测类别时，将全连接层的输出的形状再变换为\(n×q\)并使用softmax回归（q为类别个数）。预测边界框时，将全连接层的输出的形状变换为\(n×4\)。也就是说，我们为每个提议区域预测类别和边界框。</li>
</ol>
<p>Fast R-CNN中提出的兴趣区域池化层与池化层有所不同。在池化层中，通过设置池化窗口、填充和步幅来控制输出形状。而兴趣区域池化层对每个区域的输出形状是可以直接指定的，例如，指定每个区域输出的高和宽分别为\(h_2\)和\(w_2\)。假设某一兴趣区域窗口的高和宽分别为h和w，该窗口将被划分为形状为\(h_2×w_2\)的子窗口网格，且每个子窗口的大小大约为\((\frac{h}{h_2})×(\frac{w}{w_2})\)。任一子窗口的高和宽要取整，其中的最大元素作为该子窗口的输出。因此，兴趣区域池化层可从形状各异的兴趣区域中均抽取出形状相同的特征。</p>
<p>上图中，在4×4的输入上选取了左上角的3×3区域作为兴趣区域。对于该兴趣区域，通过2×2兴趣区域池化层得到一个2×2的输出。4个划分后的子窗口分别含有元素0、1、4、5（5最大），2、6（6最大），8、9（9最大），10。</p>
<p>使用ROIPooling函数来演示兴趣区域池化层的计算。假设卷积神经网络抽取的特征X的高和宽均为4且只有单通道。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = tf.convert_to_tensor(np.arange(<span class="number">16</span>).reshape((<span class="number">1</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>)))</span><br><span class="line">x</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>), dtype=int64, numpy=</span><br><span class="line">array([[[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">         [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">         [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">         [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]]]])&gt;</span><br></pre></td></tr></table></figure>
<p>假设图像的高和宽均为40像素。再假设选择性搜索在图像上生成了两个提议区域：每个区域由5个元素表示，分别为区域目标类别、左上角的x和y轴坐标以及右下角的x和y轴坐标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rois = tf.convert_to_tensor(np.array([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">20</span>,<span class="number">20</span>], [<span class="number">0</span>,<span class="number">0</span>,<span class="number">10</span>,<span class="number">30</span>,<span class="number">30</span>]]))</span><br><span class="line">rois</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">5</span>), dtype=int64, numpy=</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>, <span class="number">20</span>, <span class="number">20</span>],</span><br><span class="line">       [ <span class="number">0</span>,  <span class="number">0</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">30</span>]])&gt;</span><br></pre></td></tr></table></figure>
<p>由于X的高和宽是图像的高和宽的1/10，以上两个提议区域中的坐标先按spatial_scale自乘0.1，然后在X上分别标出兴趣区域X[:,:,0:3,0:3]和X[:,:,1:4,0:4]。最后对这两个兴趣区域分别划分子窗口网格并抽取高和宽为2的特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">roi_pool</span><span class="params">(x, rois, output_size, spatial_scale)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Shape Of x:</span></span><br><span class="line"><span class="string">        [batch_size, h, w, c]</span></span><br><span class="line"><span class="string">    Shape Of rois:</span></span><br><span class="line"><span class="string">        [num_rois, 4]</span></span><br><span class="line"><span class="string">    Shape Of output_size:</span></span><br><span class="line"><span class="string">        [2,]    #(h, w)</span></span><br><span class="line"><span class="string">    Type Of spatial_scale:</span></span><br><span class="line"><span class="string">        0-1</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.shape) == <span class="number">4</span> <span class="keyword">and</span> len(rois.shape) == <span class="number">2</span> <span class="keyword">and</span> len(output_size) == <span class="number">2</span></span><br><span class="line">    <span class="comment"># feature_map_height = int(x.shape[1])</span></span><br><span class="line">    <span class="comment"># feature_map_width = int(x.shape[2])</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_curried_pool_rois</span><span class="params">(x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> _pool_rois(x, rois)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_pool_rois</span><span class="params">(feature_map, rois)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Applies ROI pooling for a single image and varios ROIs</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">curried_pool_roi</span><span class="params">(roi)</span>:</span></span><br><span class="line">            <span class="comment"># print(feature_map.shape)</span></span><br><span class="line">            <span class="keyword">return</span> _pool_roi(feature_map, roi)</span><br><span class="line">        pooled_areas = tf.map_fn(curried_pool_roi, rois, dtype=tf.float32)</span><br><span class="line">        <span class="keyword">return</span> pooled_areas</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_pool_roi</span><span class="params">(x, roi)</span>:</span></span><br><span class="line">        <span class="comment"># Shape Of x is [h, w, c]</span></span><br><span class="line">        h_start = tf.cast(roi[<span class="number">1</span>] * spatial_scale, dtype=tf.int32)</span><br><span class="line">        w_start = tf.cast(roi[<span class="number">2</span>] * spatial_scale, dtype=tf.int32)</span><br><span class="line">        h_end = tf.cast(roi[<span class="number">3</span>] * spatial_scale, dtype=tf.int32)</span><br><span class="line">        w_end = tf.cast(roi[<span class="number">4</span>] * spatial_scale, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(x.shape, roi.shape)</span></span><br><span class="line">        region = x[h_start:h_end+<span class="number">1</span>, w_start:w_end+<span class="number">1</span>, :]</span><br><span class="line"></span><br><span class="line">        region_height = h_end - h_start + <span class="number">1</span></span><br><span class="line">        region_width = w_end - w_start + <span class="number">1</span></span><br><span class="line">        h_step = tf.cast((region_height+output_size[<span class="number">0</span>]<span class="number">-1</span>)/output_size[<span class="number">0</span>], dtype=tf.int32)</span><br><span class="line">        w_step = tf.cast((region_height+output_size[<span class="number">1</span>]<span class="number">-1</span>)/output_size[<span class="number">1</span>], dtype=tf.int32)</span><br><span class="line">        <span class="comment"># print(region_height, output_size[0])</span></span><br><span class="line"></span><br><span class="line">        areas = [[(</span><br><span class="line">            i * h_step,</span><br><span class="line">            j * w_step,</span><br><span class="line">            (i+<span class="number">1</span>) * h_step <span class="keyword">if</span> i+<span class="number">1</span> &lt; output_size[<span class="number">0</span>] <span class="keyword">else</span> region_height,</span><br><span class="line">            (j+<span class="number">1</span>) * w_step <span class="keyword">if</span> j+<span class="number">1</span> &lt; output_size[<span class="number">1</span>] <span class="keyword">else</span> region_width</span><br><span class="line">            ) </span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(output_size[<span class="number">1</span>])]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(output_size[<span class="number">0</span>])]</span><br><span class="line">        <span class="comment"># take the maximum of each area and stack the result</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">pool_area</span><span class="params">(x)</span>:</span></span><br><span class="line">          <span class="keyword">return</span> tf.math.reduce_max(region[x[<span class="number">0</span>]:x[<span class="number">2</span>], x[<span class="number">1</span>]:x[<span class="number">3</span>], :], axis=[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">        </span><br><span class="line">        pooled_features = tf.stack([[pool_area(item) <span class="keyword">for</span> item <span class="keyword">in</span> row] <span class="keyword">for</span> row <span class="keyword">in</span> areas])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tf.cast(pooled_features, dtype=tf.float32)</span><br><span class="line">    pooled_areas = tf.map_fn(_curried_pool_rois, x, dtype=tf.float32)</span><br><span class="line">    <span class="keyword">return</span> pooled_areas</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = tf.convert_to_tensor(np.arange(<span class="number">16</span>).reshape(<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>), dtype=tf.float32)</span><br><span class="line">rois = tf.convert_to_tensor(np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">20</span>, <span class="number">20</span>], [<span class="number">0</span>, <span class="number">10</span>, <span class="number">0</span>, <span class="number">30</span>, <span class="number">30</span>]]), dtype=tf.float32)</span><br><span class="line">output_size=(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">roi_pool(x, rois, output_size=output_size, spatial_scale=<span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>), dtype=float32, numpy=</span><br><span class="line">array([[[[[ <span class="number">5.</span>],</span><br><span class="line">          [ <span class="number">6.</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">9.</span>],</span><br><span class="line">          [<span class="number">10.</span>]]],</span><br><span class="line">         [[[ <span class="number">9.</span>],</span><br><span class="line">          [<span class="number">11.</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">13.</span>],</span><br><span class="line">          [<span class="number">15.</span>]]]]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="faster-r-cnn">Faster R-CNN</h2>
<p>Fast R-CNN通常需要在选择性搜索中生成较多的提议区域，以获得较精确的目标检测结果。Faster R-CNN提出将选择性搜索替换成区域提议网络（region proposal network），从而减少提议区域的生成数量，并保证目标检测的精度。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/9.8_faster-rcnn.svg" alt="Faster R-CNN模型"></p>
<p>与Fast R-CNN相比，只有生成提议区域的方法从选择性搜索变成了区域提议网络，而其他部分均保持不变。具体来说，区域提议网络的计算步骤如下。</p>
<ol>
<li>使用填充为1的3×33×3卷积层变换卷积神经网络的输出，并将输出通道数记为cc。这样，卷积神经网络为图像抽取的特征图中的每个单元均得到一个长度为cc的新特征。</li>
<li>以特征图每个单元为中心，生成多个不同大小和宽高比的锚框并标注它们。</li>
<li>用锚框中心单元长度为cc的特征分别预测该锚框的二元类别（含目标还是背景）和边界框。</li>
<li>使用非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测边界框即兴趣区域池化层所需要的提议区域。</li>
</ol>
<p>值得一提的是，区域提议网络作为Faster R-CNN的一部分，是和整个模型一起训练得到的。也就是说，Faster R-CNN的目标函数既包括目标检测中的类别和边界框预测，又包括区域提议网络中锚框的二元类别和边界框预测。最终，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少提议区域数量的情况下也能保证目标检测的精度。</p>
<h2 id="mask-r-cnn">Mask R-CNN</h2>
<p>如果训练数据还标注了每个目标在图像上的像素级位置，那么Mask R-CNN能有效利用这些详尽的标注信息进一步提升目标检测的精度。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/mask-rcnn.svg" alt="Mask R-CNN模型"></p>
<p>如上图所示，Mask R-CNN在Faster R-CNN的基础上做了修改。Mask R-CNN将兴趣区域池化层替换成了兴趣区域对齐层，即通过双线性插值（bilinear interpolation）来保留特征图上的空间信息，从而更适于像素级预测。兴趣区域对齐层的输出包含了所有兴趣区域的形状相同的特征图。它们既用来预测兴趣区域的类别和边界框，又通过额外的全卷积网络预测目标的像素级位置。我们将在9.10节（全卷积网络）介绍如何使用全卷积网络预测图像中像素级的语义。</p>
<h1 id="yu-yi-fen-ge-he-shu-ju-ji">语义分割和数据集</h1>
<p>语义分割问题关注如何将图像分割成属于不同语义类别的区域。值得一提的是，这些语义区域的标注和预测都是像素级的。下图展示了语义分割中图像有关狗、猫和背景的标签。可以看到，与目标检测相比，语义分割标注的像素级的边框显然更加精细。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/9.9_segmentation.svg" alt></p>
<p>语义分割中图像有关狗、猫和背景的标签</p>
<h2 id="tu-xiang-fen-ge-he-shi-li-fen-ge">图像分割和实例分割</h2>
<p>计算机视觉领域还有2个与语义分割相似的重要问题，即图像分割和实例分割。在这里将它们与语义分割简单区分一下。</p>
<ul>
<li>图像分割将图像分割成若干组成区域。这类问题的方法通常利用图像中像素之间的相关性。它在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义。以图9.10的图像为输入，图像分割可能将狗分割成两个区域：一个覆盖以黑色为主的嘴巴和眼睛，而另一个覆盖以黄色为主的其余部分身体。</li>
<li>实例分割又叫同时检测并分割。它研究如何识别图像中各个目标实例的像素级区域。与语义分割有所不同，实例分割不仅需要区分语义，还要区分不同的目标实例。如果图像中有两只狗，实例分割需要区分像素属于这两只狗中的哪一只。</li>
</ul>
<h2 id="yu-chu-li-shu-ju">预处理数据</h2>
<p>通常图像处理包括缩放图像使其符合模型的输入形状。然而在语义分割里，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像。这样的映射难以做到精确，尤其在不同语义的分割区域。为了避免这个问题，将图像裁剪成固定尺寸而不是缩放。具体来说，我们使用图像增广里的随机裁剪，并对输入图像和标签裁剪相同区域。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">voc_rand_crop</span><span class="params">(feature, label, height, width)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Random crop feature (tf image) and label (tf image).</span></span><br><span class="line"><span class="string">    先将channel合并，剪裁之后再分开</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    combined = tf.concat([feature, label], axis=<span class="number">2</span>)</span><br><span class="line">    last_label_dim = tf.shape(label)[<span class="number">-1</span>]</span><br><span class="line">    last_feature_dim = tf.shape(feature)[<span class="number">-1</span>]</span><br><span class="line">    combined_crop = tf.image.random_crop(combined,</span><br><span class="line">                        size=tf.concat([(height, width), [last_label_dim + last_feature_dim]],axis=<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">return</span> combined_crop[:, :, :last_feature_dim], combined_crop[:, :, last_feature_dim:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">imgs = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(n):</span><br><span class="line">    imgs += voc_rand_crop(train_features[<span class="number">0</span>], train_labels[<span class="number">0</span>], <span class="number">200</span>, <span class="number">300</span>)</span><br><span class="line"></span><br><span class="line">show_images(imgs[::<span class="number">2</span>] + imgs[<span class="number">1</span>::<span class="number">2</span>], <span class="number">2</span>, n)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_31_1.png" alt></p>
<h1 id="quan-juan-ji-wang-luo-fcn">全卷积网络（FCN）</h1>
<p>全卷积网络（FCN）采用卷积神经网络实现了从图像像素到像素类别的变换 [1]。全卷积网络通过转置卷积层将中间层特征图的高和宽变换回输入图像的尺寸，从而令预测结果与输入图像在空间维（高和宽）上一一对应：给定空间维上的位置，通道维的输出即该位置对应像素的类别预测。</p>
<h2 id="zhuan-zhi-juan-ji-ceng">转置卷积层</h2>
<p>顾名思义，转置卷积层得名于矩阵的转置操作。事实上，卷积运算还可以通过矩阵乘法来实现。在下面的例子中，定义一个高和宽分别为4的输入<code>X</code>，以及高和宽分别为3的卷积核<code>K</code>。打印二维卷积运算的输出以及卷积核。可以看到，输出的高和宽分别为2。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">X = np.arange(<span class="number">1</span>, <span class="number">17</span>).reshape((<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">X=tf.convert_to_tensor(X, dtype=tf.float32)</span><br><span class="line">K = np.arange(<span class="number">1</span>, <span class="number">10</span>).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">K = initializers.Constant(K)</span><br><span class="line">conv = layers.Conv2D(filters=<span class="number">1</span>, kernel_size=<span class="number">3</span>, kernel_initializer=K)</span><br><span class="line">conv(X),np.shape(K.value)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">(&lt;tf.Tensor: shape=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>), dtype=float32, numpy=</span><br><span class="line"> array([[[[<span class="number">348.</span>],</span><br><span class="line">          [<span class="number">393.</span>]],</span><br><span class="line"> </span><br><span class="line">         [[<span class="number">528.</span>],</span><br><span class="line">          [<span class="number">573.</span>]]]], dtype=float32)&gt;, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<p>下面将卷积核<code>K</code>改写成含有大量零元素的稀疏矩阵<code>W</code>，即权重矩阵。权重矩阵的形状为(4, 16)，其中的非零元素来自卷积核<code>K</code>中的元素。将输入<code>X</code>逐行连结，得到长度为16的向量。然后将<code>W</code>与向量化的<code>X</code>做矩阵乘法，得到长度为4的向量。对其变形后，可以得到和上面卷积运算相同的结果。可见，在这个例子中使用矩阵乘法实现了卷积运算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">W, k = np.zeros((<span class="number">4</span>, <span class="number">16</span>)), np.zeros(<span class="number">11</span>)</span><br><span class="line">k[:<span class="number">3</span>], k[<span class="number">4</span>:<span class="number">7</span>], k[<span class="number">8</span>:] = K.value[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, :], K.value[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, :], K.value[<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, :]</span><br><span class="line">W[<span class="number">0</span>, <span class="number">0</span>:<span class="number">11</span>], W[<span class="number">1</span>, <span class="number">1</span>:<span class="number">12</span>], W[<span class="number">2</span>, <span class="number">4</span>:<span class="number">15</span>], W[<span class="number">3</span>, <span class="number">5</span>:<span class="number">16</span>] = k, k, k, k</span><br><span class="line">tf.matmul(tf.convert_to_tensor(W, dtype=tf.float32), tf.reshape(X, (<span class="number">-1</span>, <span class="number">1</span>))), W</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">(&lt;tf.Tensor: shape=(<span class="number">4</span>, <span class="number">1</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">348.</span>],</span><br><span class="line">        [<span class="number">393.</span>],</span><br><span class="line">        [<span class="number">528.</span>],</span><br><span class="line">        [<span class="number">573.</span>]], dtype=float32)&gt;,</span><br><span class="line"> array([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">0.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">0.</span>, <span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">0.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">0.</span>, <span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">0.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">0.</span>, <span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">0.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">0.</span>, <span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>]]))</span><br></pre></td></tr></table></figure>
<p>现在从矩阵乘法的角度来描述卷积运算。设输入向量为\(\boldsymbol{x}\)，权重矩阵为\(\boldsymbol{W}\)，卷积的前向计算函数的实现可以看作将函数输入乘以权重矩阵，并输出向量\(\boldsymbol{y} = \boldsymbol{W}\boldsymbol{x}\)。反向传播需要依据链式法则。由于\(\nabla_{\boldsymbol{x}} \boldsymbol{y} = \boldsymbol{W}^\top\)，卷积的反向传播函数的实现可以看作将函数输入乘以转置后的权重矩阵\(\boldsymbol{W}^\top\)。而转置卷积层正好交换了卷积层的前向计算函数与反向传播函数：转置卷积层的这两个函数可以看作将函数输入向量分别乘以\(\boldsymbol{W}^\top\)和\(\boldsymbol{W}\)。</p>
<p>不难想象，转置卷积层可以用来交换卷积层输入和输出的形状。继续用矩阵乘法描述卷积。设权重矩阵是形状为\(4\times16\)的矩阵，对于长度为16的输入向量，卷积前向计算输出长度为4的向量。假如输入向量的长度为4，转置权重矩阵的形状为\(16\times4\)，那么转置卷积层将输出长度为16的向量。在模型设计中，转置卷积层常用于将较小的特征图变换为更大的特征图。在全卷积网络中，当输入是高和宽较小的特征图时，转置卷积层可以用来将高和宽放大到输入图像的尺寸。</p>
<p>来看一个例子。构造一个卷积层<code>conv</code>，并设输入<code>X</code>的形状为(1, 64, 64, 3)。卷积输出<code>Y</code>的通道数增加到10，但高和宽分别缩小了一半。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conv = layers.Conv2D(<span class="number">10</span>, <span class="number">4</span>, padding=<span class="string">'same'</span>, strides=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">x = np.random.uniform(size=(<span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>))</span><br><span class="line">x = tf.convert_to_tensor(x, dtype=tf.float32)</span><br><span class="line">y = conv(x)</span><br><span class="line">y.shape <span class="comment"># TensorShape([1, 32, 32, 10])</span></span><br></pre></td></tr></table></figure>
<p>下面通过创建<code>Conv2DTranspose</code>实例来构造转置卷积层<code>conv_trans</code>。这里设<code>conv_trans</code>的卷积核形状、填充以及步幅与<code>conv</code>中的相同，并设输出通道数为3。当输入为卷积层<code>conv</code>的输出<code>Y</code>时，转置卷积层输出与卷积层输入的高和宽相同：转置卷积层将特征图的高和宽分别放大了2倍。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conv_trans = layers.Convolution2DTranspose(filters=<span class="number">3</span>,</span><br><span class="line">                          kernel_size=<span class="number">4</span>,</span><br><span class="line">                          padding=<span class="string">'same'</span>,</span><br><span class="line">                          strides=<span class="number">2</span>)</span><br><span class="line">conv_trans(y).shape <span class="comment"># TensorShape([1, 64, 64, 3])</span></span><br></pre></td></tr></table></figure>
<p>在有些文献中，转置卷积也被称为分数步长卷积[2]。</p>
<h2 id="gou-zao-mo-xing">构造模型</h2>
<p>这里给出全卷积网络模型最基本的设计。如下图所示，全卷积网络先使用卷积神经网络抽取图像特征，然后通过\(1\times 1\)卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。模型输出与输入图像的高和宽相同，并在空间位置上一一对应：最终输出的通道包含了该空间位置像素的类别预测。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/fcn.svg" alt="全卷积网络"></p>
<p>下面使用一个基于ImageNet数据集预训练的ResNet-101模型来抽取图像特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pretrained_net = applications.ResNet101V2(include_top=<span class="literal">False</span>, weights=<span class="string">'imagenet'</span>, input_shape=(<span class="number">320</span>, <span class="number">480</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建全卷积网络实例net，它复制了pretrained_net实例成员变量features里除去最后两层的所有层以及预训练得到的模型参数。</span></span><br><span class="line">net = keras.Sequential()</span><br><span class="line">net.add(pretrained_net)</span><br></pre></td></tr></table></figure>
<p>给定高和宽分别为320和480的输入，net的前向计算将输入的高和宽减小至原来的 1/32 ，即10和15。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.uniform(size=(<span class="number">1</span>, <span class="number">320</span>, <span class="number">480</span>, <span class="number">3</span>))</span><br><span class="line">x = tf.convert_to_tensor(x, dtype=tf.float32)</span><br><span class="line">net(x).shape <span class="comment"># TensorShape([1, 10, 15, 2048])</span></span><br></pre></td></tr></table></figure>
<p>接下来，通过\(1\times 1\)卷积层将输出通道数变换为Pascal VOC2012数据集的类别个数21。最后，将特征图的高和宽放大32倍，从而变回输入图像的高和宽。由于\((320-64+16\times2+32)/32=10\)且\((480-64+16\times2+32)/32=15\)，构造一个步幅为32的转置卷积层，并将卷积核的高和宽设为64、填充设为16。不难发现，如果步幅为\(s\)、填充为\(s/2\)（假设\(s/2\)为整数）、卷积核的高和宽为\(2s\)，转置卷积核将输入的高和宽分别放大\(s\)倍。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_classes = <span class="number">21</span></span><br><span class="line">net.add(keras.layers.Conv2D(num_classes, kernel_size=<span class="number">1</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">print(net(x).shape) <span class="comment"># (1, 10, 15, 21)</span></span><br><span class="line">net.add(keras.layers.Conv2DTranspose(num_classes,</span><br><span class="line">                kernel_size=<span class="number">64</span>,</span><br><span class="line">                padding=<span class="string">'same'</span>,</span><br><span class="line">                strides=<span class="number">32</span>))</span><br><span class="line">print(net(x).shape) <span class="comment"># (1, 320, 480, 21)</span></span><br><span class="line">net.add(keras.layers.Softmax(axis=<span class="number">-1</span>))</span><br></pre></td></tr></table></figure>
<h2 id="chu-shi-hua-zhuan-zhi-juan-ji-ceng">初始化转置卷积层</h2>
<p>转置卷积层可以放大特征图，在图像处理中，有时需要将图像放大，即上采样（upsample）。上采样的方法有很多，常用的有双线性插值。简单来说，为了得到输出图像在坐标\((x,y)\)上的像素，先将该坐标映射到输入图像的坐标\((x',y')\)，例如，根据输入与输出的尺寸之比来映射。映射后的\(x'\)和\(y'\)通常是实数。然后，在输入图像上找到与坐标\((x',y')\)最近的4个像素。最后，输出图像在坐标\((x,y)\)上的像素依据输入图像上这4个像素及其与\((x',y')\)的相对距离来计算。双线性插值的上采样可以通过由以下<code>bilinear_kernel</code>函数构造的卷积核的转置卷积层来实现。这里给出<code>bilinear_kernel</code>函数的实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 目的就是找出 kernel_size*kernel_size 的框，框内的权值中间大外面小</span></span><br><span class="line"><span class="comment"># 然后每个通道只乘以自己的权值 就像上面图一样（kernel_size=2）</span></span><br><span class="line"><span class="comment"># 每个channel乘以对应的权值</span></span><br><span class="line"><span class="comment"># in_channels, out_channels 应该相同</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bilinear_kernel</span><span class="params">(in_channels, out_channels, kernel_size)</span>:</span></span><br><span class="line">    factor = (kernel_size + <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> kernel_size % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">        center = factor - <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        center = factor - <span class="number">0.5</span></span><br><span class="line">    og = np.ogrid[:kernel_size, :kernel_size]</span><br><span class="line">    filt = (<span class="number">1</span>-abs(og[<span class="number">0</span>]-center)/factor) * (<span class="number">1</span>-abs(og[<span class="number">1</span>]-center)/factor)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># kernel_size = (kernel_size, kernel_size, in_channels, out_channels)</span></span><br><span class="line">    weight = np.zeros((kernel_size, kernel_size, in_channels, out_channels), dtype=<span class="string">'float32'</span>)</span><br><span class="line">    weight[:, :, range(in_channels), range(out_channels)] = filt.reshape((kernel_size,kernel_size,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> weight</span><br></pre></td></tr></table></figure>
<p>用转置卷积层实现的双线性插值的上采样。构造一个将输入的高和宽放大2倍的转置卷积层，并将其卷积核用<code>bilinear_kernel</code>函数初始化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">init_param = initializers.Constant(bilinear_kernel(<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">conv_trans = layers.Conv2DTranspose(<span class="number">3</span>,</span><br><span class="line">                kernel_size=<span class="number">4</span>,</span><br><span class="line">                padding=<span class="string">'same'</span>,</span><br><span class="line">                strides=<span class="number">2</span>,</span><br><span class="line">                kernel_initializer=init_param,)</span><br></pre></td></tr></table></figure>
<p>读取图像<code>X</code>，将上采样的结果记作<code>Y</code>。为了打印图像，我们需要调整通道维的位置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">img = tf.io.read_file(<span class="string">'catdog.jpg'</span>)</span><br><span class="line">img_org = tf.image.decode_jpeg(img, channels=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">img = tf.cast(img_org, tf.float32)/<span class="number">255.</span></span><br><span class="line">print(img.shape) <span class="comment"># (561, 728, 3)</span></span><br><span class="line">plt.imshow(img.numpy())</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_23_2.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加维度</span></span><br><span class="line">y = conv_trans(tf.expand_dims(img, axis=<span class="number">0</span>))</span><br><span class="line">print(y.shape) <span class="comment"># (1, 1122, 1456, 3)</span></span><br><span class="line"><span class="comment"># 颜色会变浅</span></span><br><span class="line">plt.imshow(y[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_24_2.png" alt></p>
<p>在全卷积网络中，将转置卷积层初始化为双线性插值的上采样。对于 1×1 卷积层，采用Xavier随机初始化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">-2</span>, <span class="number">-1</span>):</span><br><span class="line">    bias_weight = net.layers[i].get_weights()[<span class="number">1</span>]</span><br><span class="line">    kernel_size = net.layers[i].get_config()[<span class="string">'kernel_size'</span>][<span class="number">0</span>]</span><br><span class="line">    net.layers[i].set_weights([bilinear_kernel(num_classes, num_classes, kernel_size), </span><br><span class="line">                bias_weight])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    print(net.layers[i].trainable)</span><br><span class="line"><span class="comment"># net[-3] 不需要动了，tf默认Xavier初始化</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h1 id="yang-shi-qian-yi">样式迁移</h1>
<p>滤镜能改变照片的颜色样式，从而使风景照更加锐利或者令人像更加美白。但一个滤镜通常只能改变照片的某个方面。如果要照片达到理想中的样式，经常需要尝试大量不同的组合，其复杂程度不亚于模型调参。</p>
<p>如何使用卷积神经网络自动将某图像中的样式应用在另一图像之上，即样式迁移[1]。这里需要两张输入图像，一张是内容图像，另一张是样式图像，使用神经网络修改内容图像使其在样式上接近样式图像。下图中的内容图像为西雅图郊区的雷尼尔山国家公园的风景照，而样式图像则是一幅主题为秋天橡树的油画。最终输出的合成图像在保留了内容图像中物体主体形状的情况下应用了样式图像的油画笔触，同时也让整体颜色更加鲜艳。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/style-transfer.svg" alt="输入内容图像和样式图像，输出样式迁移后的合成图像"></p>
<h2 id="yang-shi-qian-yi-fang-fa">样式迁移方法</h2>
<p>上图用一个例子来阐述基于卷积神经网络的样式迁移方法。首先，初始化合成图像，例如将其初始化成内容图像。该合成图像是样式迁移过程中唯一需要更新的变量，即样式迁移所需迭代的模型参数。然后，选择一个预训练的卷积神经网络来抽取图像的特征，其中的模型参数在训练中无须更新。深度卷积神经网络凭借多个层逐级抽取图像的特征。可以选择其中某些层的输出作为内容特征或样式特征。以上图为例，这里选取的预训练的神经网络含有3个卷积层，其中第二层输出图像的内容特征，而第一层和第三层的输出被作为图像的样式特征。接下来，通过正向传播（实线箭头方向）计算样式迁移的损失函数，并通过反向传播（虚线箭头方向）迭代模型参数，即不断更新合成图像。样式迁移常用的损失函数由3部分组成：内容损失使合成图像与内容图像在内容特征上接近，样式损失令合成图像与样式图像在样式特征上接近，而总变差损失则有助于减少合成图像中的噪点。最后，当模型训练结束时，输出样式迁移的模型参数，即得到最终的合成图像。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/neural-style.svg" alt="基于卷积神经网络的样式迁移。实线箭头和虚线箭头分别表示正向传播和反向传播"></p>
<h2 id="du-qu-nei-rong-tu-xiang-he-yang-shi-tu-xiang">读取内容图像和样式图像</h2>
<p>首先，分别读取内容图像和样式图像。从打印出的图像坐标轴可以看出，它们的尺寸并不一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">content_img = tf.io.read_file(<span class="string">'rainier.jpg'</span>)</span><br><span class="line">content_img = tf.image.decode_jpeg(content_img)</span><br><span class="line">plt.imshow(content_img)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_12_1.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">style_img = tf.io.read_file(<span class="string">'../../data/autumn_oak.jpg'</span>)</span><br><span class="line">style_img = tf.image.decode_jpeg(style_img)</span><br><span class="line">plt.imshow(style_img)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_13_1-5863677.png" alt></p>
<h2 id="yu-chu-li-he-hou-chu-li-tu-xiang">预处理和后处理图像</h2>
<p>下面定义图像的预处理函数和后处理函数。预处理函数preprocess对先对更改输入图像的尺寸，然后再将PIL图片转成卷积神经网络接受的输入格式，再在RGB三个通道分别做标准化，由于预训练模型是在均值为[0.485, 0.456, 0.406]标准差为[0.229, 0.224, 0.225]的图片数据上预训练的，所以要将图片标准化保持相同的均值和标准差。后处理函数postprocess则将输出图像中的像素值还原回标准化之前的值。由于图像每个像素的浮点数值在0到1之间，可以使用clamp函数对小于0和大于1的值分别取0和1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">rgb_mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">rgb_std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(img_tensor, image_shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    image_shape = (h, w)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    img = tf.image.resize(img_tensor, image_shape[<span class="number">0</span>:<span class="number">2</span>])</span><br><span class="line">    img = tf.divide(img, <span class="number">255.</span>)</span><br><span class="line">    img = tf.divide(tf.subtract(img, rgb_mean), rgb_std)</span><br><span class="line">    img = tf.expand_dims(img, axis=<span class="number">0</span>) <span class="comment"># (batch_size, h, w, 3)</span></span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">postprocess</span><span class="params">(img_tensor)</span>:</span></span><br><span class="line">    mean = -rgb_mean / rgb_std</span><br><span class="line">    std= <span class="number">1</span> / rgb_std</span><br><span class="line">    img = tf.divide(tf.subtract(img_tensor, mean), std)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure>
<h2 id="chou-qu-te-zheng">抽取特征</h2>
<p>使用基于ImageNet数据集预训练的VGG-19模型来抽取图像特征 [1]。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pretrained_net = keras.applications.vgg19.VGG19(weights=<span class="string">"imagenet"</span>)</span><br></pre></td></tr></table></figure>
<p>为了抽取图像的内容特征和样式特征，可以选择VGG网络中某些层的输出。一般来说，越靠近输入层的输出越容易抽取图像的细节信息，反之则越容易抽取图像的全局信息。为了避免合成图像过多保留内容图像的细节，选择VGG较靠近输出的层，也称内容层，来输出图像的内容特征。另外，还从VGG中选择不同层的输出来匹配局部和全局的样式，这些层也叫样式层。VGG网络使用了5个卷积块。实验中，选择第四卷积块的最后一个卷积层作为内容层，以及每个卷积块的第一个卷积层作为样式层。这些层的索引可以通过打印pretrained_net实例来获取</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">need = [<span class="string">'name'</span>, <span class="string">'filters'</span>, <span class="string">'kernel_size'</span>, <span class="string">'strides'</span>, <span class="string">'pool_size'</span>, <span class="string">'padding'</span>, <span class="string">'batch_input_shape'</span>]</span><br><span class="line">print(<span class="string">"&#123;&#125;"</span>.format(<span class="string">"\t"</span>), end=<span class="string">""</span>)</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> need:</span><br><span class="line">    print(n.ljust(<span class="number">13</span>,<span class="string">' '</span>), end=<span class="string">'\t'</span>)</span><br><span class="line">print()</span><br><span class="line"><span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(pretrained_net.layers[:]):</span><br><span class="line">    config = layer.get_config()</span><br><span class="line">    params = [config.get(key) <span class="keyword">for</span> key <span class="keyword">in</span> need]</span><br><span class="line">    print(<span class="string">"(&#123;&#125;)"</span>.format(i), end=<span class="string">""</span>)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> params:</span><br><span class="line">        print(str(p).ljust(<span class="number">13</span>,<span class="string">' '</span>), end=<span class="string">'\t'</span>)</span><br><span class="line">    print()</span><br><span class="line">   </span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">  name         	filters      	kernel_size  	strides      	pool_size    	padding      	batch_input_shape	</span><br><span class="line">(<span class="number">0</span>)input_1      	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	(<span class="literal">None</span>, <span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>)	</span><br><span class="line">(<span class="number">1</span>)block1_conv1 	<span class="number">64</span>           	(<span class="number">3</span>, <span class="number">3</span>)       	(<span class="number">1</span>, <span class="number">1</span>)       	<span class="literal">None</span>         	same         	<span class="literal">None</span>         	</span><br><span class="line">(<span class="number">2</span>)block1_conv2 	<span class="number">64</span>           	(<span class="number">3</span>, <span class="number">3</span>)       	(<span class="number">1</span>, <span class="number">1</span>)       	<span class="literal">None</span>         	same         	<span class="literal">None</span>         	</span><br><span class="line">...</span><br><span class="line">(<span class="number">24</span>)fc2          	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	</span><br><span class="line">(<span class="number">25</span>)predictions  	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># style_layers = [block1_conv1, block2_conv1, block3_conv1, block3_conv4, block4_conv1, block5_conv1]</span></span><br><span class="line"><span class="comment"># content_layers = [block3_conv1]</span></span><br><span class="line">style_layers, content_layers = [<span class="number">1</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">17</span>], [<span class="number">7</span>]</span><br></pre></td></tr></table></figure>
<p>在抽取特征时，只需要用到VGG从输入层到最靠近输出层的内容层或样式层之间的所有层。下面构建一个新的网络net，它只保留需要用到的VGG的所有层。我们将使用net来抽取特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net_list = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(max(content_layers + style_layers) + <span class="number">1</span>):</span><br><span class="line">    net_list.append(pretrained_net.layers[i])</span><br><span class="line">net = keras.Sequential()</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net_list:</span><br><span class="line">        net.add(layer)</span><br></pre></td></tr></table></figure>
<p>给定输入X，如果简单调用前向计算net(X)，只能获得最后一层的输出。由于还需要中间层的输出，因此这里逐层计算，并保留内容层和样式层的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_feature</span><span class="params">(x, content_layers, style_layers)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.shape) == <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    contents = []</span><br><span class="line">    styles = []</span><br><span class="line">    <span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(net.layers):</span><br><span class="line">        x = layer(x)</span><br><span class="line">        <span class="keyword">if</span> i+<span class="number">1</span> <span class="keyword">in</span> style_layers:</span><br><span class="line">            styles.append(x)</span><br><span class="line">        <span class="keyword">if</span> i+<span class="number">1</span> <span class="keyword">in</span> content_layers:</span><br><span class="line">            contents.append(x)</span><br><span class="line">    <span class="keyword">return</span> contents, styles</span><br></pre></td></tr></table></figure>
<p>下面定义两个函数，其中get_contents函数对内容图像抽取内容特征，而get_styles函数则对样式图像抽取样式特征。因为在训练时无须改变预训练的VGG的模型参数，所以可以在训练开始之前就提取出内容图像的内容特征，以及样式图像的样式特征。由于合成图像是样式迁移所需迭代的模型参数，只能在训练过程中通过调用extract_features函数来抽取合成图像的内容特征和样式特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_contents</span><span class="params">(image_shape)</span>:</span></span><br><span class="line">    <span class="comment"># 统一尺寸</span></span><br><span class="line">    content_x = preprocess(content_img, image_shape)</span><br><span class="line">    <span class="comment"># 获得内容特征</span></span><br><span class="line">    contents_y, _ = extract_feature(content_x, content_layers, style_layers)</span><br><span class="line">    <span class="keyword">return</span> content_x, contents_y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_styles</span><span class="params">(image_shape)</span>:</span></span><br><span class="line">    <span class="comment"># 统一尺寸</span></span><br><span class="line">    style_x = preprocess(style_img, image_shape)</span><br><span class="line">    <span class="comment"># 获得样式特征</span></span><br><span class="line">    _, styles_y = extract_feature(style_x, content_layers, style_layers)</span><br><span class="line">    <span class="keyword">return</span> style_x, styles_y</span><br></pre></td></tr></table></figure>
<h2 id="ding-yi-sun-shi-han-shu">定义损失函数</h2>
<p>下面描述样式迁移的损失函数。它由内容损失、样式损失和总变差损失3部分组成。</p>
<h3 id="nei-rong-sun-shi">内容损失</h3>
<p>与线性回归中的损失函数类似，内容损失通过平方误差函数衡量合成图像与内容图像在内容特征上的差异。平方误差函数的两个输入均为extract_features函数计算所得到的内容层的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">content_loss</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    c_l = keras.losses.mean_squared_error(y, y_hat)</span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(c_l)</span><br></pre></td></tr></table></figure>
<h3 id="yang-shi-sun-shi">样式损失</h3>
<p>样式损失也一样通过平方误差函数衡量合成图像与样式图像在样式上的差异。为了表达样式层输出的样式，先通过extract_features函数计算样式层的输出。假设该输出的样本数为1，通道数为 c ，高和宽分别为 h 和 w ，可以把输出变换成 c 行 \((h\times w)\) 列的矩阵 X 。矩阵 X 可以看作由 c 个长度为 \((h \times w)\) 的向量 x1,…,xc 组成的。其中向量 \(x_i\) 代表了通道 \(i\) 上的样式特征。这些向量的格拉姆矩阵（Gram matrix） \(XX^⊤ \in R_{c \times c}\) 中 i 行 j 列的元素 \(x_{ij}\) 即向量 \(x_i\) 与 \(x_j\) 的内积，它表达了通道 i 和通道 j 上样式特征的相关性。用这样的格拉姆矩阵表达样式层输出的样式。需要注意的是，当 hw 的值较大时，格拉姆矩阵中的元素容易出现较大的值。此外，格拉姆矩阵的高和宽皆为通道数 c 。为了让样式损失不受这些值的大小影响，下面定义的gram函数将格拉姆矩阵除以了矩阵中元素的个数，即 chw 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># x.shape=(batch_size, h, w, c)</span></span><br><span class="line">    num_channels, n = x.shape[<span class="number">3</span>], x.shape[<span class="number">1</span>] * x.shape[<span class="number">2</span>]</span><br><span class="line">    x = tf.reshape(x, shape=(num_channels, n))</span><br><span class="line">    x_big = tf.matmul(x, tf.transpose(x))</span><br><span class="line">    x_big = tf.divide(x_big, num_channels * n)</span><br><span class="line">    <span class="keyword">return</span> x_big</span><br></pre></td></tr></table></figure>
<p>自然地，样式损失的平方误差函数的两个格拉姆矩阵输入分别基于合成图像与样式图像的样式层输出。这里假设基于样式图像的格拉姆矩阵gram_Y已经预先计算好了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">style_loss</span><span class="params">(y_hat, gram_y)</span>:</span></span><br><span class="line">    s_l = keras.losses.mean_squared_error(gram_y, gram(y_hat))</span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(s_l)</span><br></pre></td></tr></table></figure>
<h3 id="zong-bian-chai-sun-shi">总变差损失</h3>
<p>学到的合成图像里面有大量高频噪点，即有特别亮或者特别暗的颗粒像素。一种常用的降噪方法是总变差降噪。假设 \(x_{i,j}\) 表示坐标为 (i,j) 的像素值，降低总变差损失</p>
<p>\[
\sum_{i,j} \left|x_{i,j} - x_{i+1,j}\right| + \left|x_{i,j} - x_{i,j+1}\right|
\]</p>
<p>能够尽可能使邻近的像素值相似。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tv_loss</span><span class="params">(y_hat)</span>:</span></span><br><span class="line">    t1 = keras.losses.mean_absolute_error(y_hat[:, <span class="number">1</span>:, :, :], </span><br><span class="line">                        y_hat[:, :<span class="number">-1</span>, :, :])</span><br><span class="line">    t1 = tf.reduce_mean(t1)</span><br><span class="line">    t2 = keras.losses.mean_absolute_error(y_hat[:, :, <span class="number">1</span>:, :], </span><br><span class="line">                        y_hat[:, :, :<span class="number">-1</span>, :])</span><br><span class="line">    t2 = tf.reduce_mean(t2)</span><br><span class="line">    </span><br><span class="line">    tv = tf.add(t1, t2)</span><br><span class="line">    <span class="keyword">return</span> tf.multiply(<span class="number">0.5</span>, tv)</span><br></pre></td></tr></table></figure>
<h2 id="sun-shi-han-shu">损失函数</h2>
<p>样式迁移的损失函数即内容损失、样式损失和总变差损失的加权和。通过调节这些权值超参数，可以权衡合成图像在保留内容、迁移样式以及降噪三方面的相对重要性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">content_weight, style_weight, tv_weight = <span class="number">1</span>, <span class="number">1e3</span>, <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span><span class="params">(x, contents_y_hat, styles_y_hat, contents_y, styles_y_gram)</span>:</span></span><br><span class="line">    <span class="comment"># 分别计算内容损失、样式损失和总变差损失</span></span><br><span class="line">    contents_l = [content_loss(y_hat, y) * content_weight <span class="keyword">for</span> y_hat, y <span class="keyword">in</span> zip(</span><br><span class="line">        contents_y_hat, contents_y)]</span><br><span class="line">    styles_l = [style_loss(y_hat, y) * style_weight <span class="keyword">for</span> y_hat, y <span class="keyword">in</span> zip(</span><br><span class="line">        styles_y_hat, styles_y_gram)]</span><br><span class="line">    tv_l = tv_loss(x) * tv_weight</span><br><span class="line">    <span class="comment"># 对所有损失求和</span></span><br><span class="line">    l = tf.reduce_sum(tf.reshape(contents_l,(<span class="number">-1</span>,))) + tf.reduce_sum(tf.reshape(styles_l,(<span class="number">-1</span>,))) + tv_l</span><br><span class="line">    <span class="keyword">return</span> contents_l, styles_l, tv_l, l</span><br></pre></td></tr></table></figure>
<h3 id="chuang-jian-he-chu-shi-hua-he-cheng-tu-xiang">创建和初始化合成图像</h3>
<p>在样式迁移中，合成图像是唯一需要更新的变量。因此，可以定义一个简单的模型GeneratedImage，并将合成图像视为模型参数。模型的前向计算只需返回模型参数即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GeneratedImage</span><span class="params">(keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, img)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weight = tf.Variable(img, dtype=tf.float32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.weight</span><br></pre></td></tr></table></figure>
<p>下面，定义get_inits函数。该函数创建了合成图像的模型实例，并将其初始化为图像X。样式图像在各个样式层的格拉姆矩阵<code>styles_Y_gram</code>将在训练前预先计算好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_inits</span><span class="params">(x, lr, styles_y)</span>:</span></span><br><span class="line">    gen_img = GeneratedImage(x)</span><br><span class="line">    <span class="comment"># 这里如果像pytorch那样直接赋值会报错，所以我在上面那个代码赋值的</span></span><br><span class="line">    <span class="comment"># gen_img.weight = x</span></span><br><span class="line">    optimizer = keras.optimizers.Adam(lr)</span><br><span class="line">    styles_y_gram = [gram(y) <span class="keyword">for</span> y <span class="keyword">in</span> styles_y]</span><br><span class="line">    <span class="keyword">return</span> gen_img(), styles_y_gram, optimizer</span><br></pre></td></tr></table></figure>
<h2 id="xun-lian">训练</h2>
<p>在训练模型时，不断抽取合成图像的内容特征和样式特征，并计算损失函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(x, contents_y, styles_y, lr, max_epochs, lr_decay_epoch)</span>:</span></span><br><span class="line">    x, styles_y_gram, optimizer = get_inits(x, lr, styles_y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_epochs):</span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> t:</span><br><span class="line">            t.watch(x)</span><br><span class="line">            contents_y_hat, styles_y_hat = extract_feature(x, content_layers,</span><br><span class="line">                                style_layers)</span><br><span class="line">            contents_l, styles_l, tv_l, l = compute_loss(x,</span><br><span class="line">                                contents_y_hat,</span><br><span class="line">                                styles_y_hat,</span><br><span class="line">                                contents_y,</span><br><span class="line">                                styles_y_gram)</span><br><span class="line">            </span><br><span class="line">        grads = t.gradient(l, x)</span><br><span class="line">        optimizer.apply_gradients(zip([grads], [x]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span> <span class="keyword">and</span> i != <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %3d, content loss %.2f, style loss %.2f, '</span></span><br><span class="line">                  <span class="string">'TV loss %.2f, %.2f sec'</span></span><br><span class="line">                  % (i, tf.reduce_sum(contents_l), </span><br><span class="line">                     tf.reduce_sum(styles_l), </span><br><span class="line">                     tf.reduce_sum(tv_l),</span><br><span class="line">                     time.time() - start))</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>下面开始训练模型。首先将内容图像和样式图像的高和宽分别调整为150和225像素。合成图像将由内容图像来初始化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">image_shape =  (<span class="number">150</span>, <span class="number">225</span>, <span class="number">3</span>)</span><br><span class="line">content_x, contents_y = get_contents(image_shape)</span><br><span class="line">style_x, styles_y = get_styles(image_shape)</span><br><span class="line">output = train(content_x, contents_y, styles_y, <span class="number">0.1</span>, <span class="number">501</span>, <span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img = postprocess(output)[<span class="number">0</span>]</span><br><span class="line">img = tf.clip_by_value(img, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">plt.imsave(<span class="string">'1.jpg'</span>, img.numpy())</span><br><span class="line">plt.imshow(img)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_57_1.png" alt></p>
<p>为了得到更加清晰的合成图像，下面在更大的300×450尺寸上训练。将上图的高和宽放大2倍，以初始化更大尺寸的合成图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">image_shape =  (<span class="number">300</span>, <span class="number">450</span>, <span class="number">3</span>)</span><br><span class="line">content_x, contents_y = get_contents(image_shape)</span><br><span class="line">style_x, styles_y = get_styles(image_shape)</span><br><span class="line">big_output = train(content_x, contents_y, styles_y, <span class="number">0.01</span>, <span class="number">1001</span>, <span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img=postprocess(big_output)[<span class="number">0</span>]</span><br><span class="line">img = tf.clip_by_value(img, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">plt.imsave(<span class="string">'2.jpg'</span>, img.numpy())</span><br><span class="line">plt.imshow(img)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_60_1.png" alt></p>
<h1 id="zong-jie">总结</h1>
<ol>
<li>迁移学习将从源数据集学到的知识迁移到目标数据集上。微调是迁移学习的一种常用技术。</li>
<li>目标模型复制了源模型上除了输出层外的所有模型设计及其参数，并基于目标数据集微调这些参数。而目标模型的输出层需要从头训练。</li>
<li>一般来说，微调参数会使用较小的学习率，而从头训练输出层可以使用较大的学习率。</li>
<li>在目标检测里不仅需要找出图像里面所有感兴趣的目标，而且要知道它们的位置。位置一般由矩形边界框来表示。</li>
<li>以每个像素为中心，生成多个大小和宽高比不同的锚框。</li>
<li>交并比是两个边界框相交面积与相并面积之比。</li>
<li>在训练集中，为每个锚框标注两类标签：一是锚框所含目标的类别；二是真实边界框相对锚框的偏移量。</li>
<li>预测时，可以使用非极大值抑制来移除相似的预测边界框，从而令结果简洁。</li>
<li>可以在多个尺度下生成不同数量和不同大小的锚框，从而在多个尺度下检测不同大小的目标。</li>
<li>特征图的形状能确定任一图像上均匀采样的锚框中心。</li>
<li>用输入图像在某个感受野区域内的信息来预测输入图像上与该区域相近的锚框的类别和偏移量。</li>
<li>R-CNN对图像选取若干提议区域，然后用卷积神经网络对每个提议区域做前向计算抽取特征，再用这些特征预测提议区域的类别和边界框。</li>
<li>Fast R-CNN对R-CNN的一个主要改进在于只对整个图像做卷积神经网络的前向计算。它引入了兴趣区域池化层，从而令兴趣区域能够抽取出形状相同的特征。</li>
<li>Faster R-CNN将Fast R-CNN中的选择性搜索替换成区域提议网络，从而减少提议区域的生成数量，并保证目标检测的精度。</li>
<li>Mask R-CNN在Faster R-CNN基础上引入一个全卷积网络，从而借助目标的像素级位置进一步提升目标检测的精度。</li>
<li>语义分割关注如何将图像分割成属于不同语义类别的区域。</li>
<li>语义分割的一个重要数据集叫作Pascal VOC2012。</li>
<li>由于语义分割的输入图像和标签在像素上一一对应，所以将图像随机裁剪成固定尺寸而不是缩放。</li>
<li>可以通过矩阵乘法来实现卷积运算。</li>
<li>全卷积网络先使用卷积神经网络抽取图像特征，然后通过\(1\times 1\)卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸，从而输出每个像素的类别。</li>
<li>在全卷积网络中，可以将转置卷积层初始化为双线性插值的上采样。</li>
<li>样式迁移常用的损失函数由3部分组成：内容损失使合成图像与内容图像在内容特征上接近，样式损失令合成图像与样式图像在样式特征上接近，而总变差损失则有助于减少合成图像中的噪点。</li>
<li>可以通过预训练的卷积神经网络来抽取图像的特征，并通过最小化损失函数来不断更新合成图像。</li>
<li>用格拉姆矩阵表达样式层输出的样式。</li>
</ol>
<h1 id="can-kao-wen-xian">参考文献</h1>
<p>[1] Long, J., Shelhamer, E., &amp; Darrell, T. (2015). Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3431-3440).</p>
<p>[2] Dumoulin, V., &amp; Visin, F. (2016). A guide to convolution arithmetic for deep learning. arXiv preprint arXiv:1603.07285.</p>

    </div>

    
    
    <div>
  
    <div style="text-align:center;color:#bfbfbf;font-size:16px;">
      <span>-------- 本文结束 </span>
      <i class="fa fa-coffee"></i>
      <span> 感谢阅读 --------</span>
    </div>
  
</div>
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    鼓励一下
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat.png" alt="Li Zhen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/ali.png" alt="Li Zhen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/04/17/deeplearning/cnn/" rel="prev" title="卷积神经网络">
      <i class="fa fa-chevron-left"></i> 卷积神经网络
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/07/18/deeplearning/optimization/" rel="next" title="优化算法">
      优化算法 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#tu-xiang-zeng-yan"><span class="nav-number">1.</span> <span class="nav-text">图像增广</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#fan-zhuan-he-cai-jian"><span class="nav-number">1.1.</span> <span class="nav-text">翻转和裁剪</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bian-hua-yan-se"><span class="nav-number">1.2.</span> <span class="nav-text">变化颜色</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#wei-diao"><span class="nav-number">2.</span> <span class="nav-text">微调</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ding-yi-he-chu-shi-hua-mo-xing"><span class="nav-number">2.1.</span> <span class="nav-text">定义和初始化模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#mu-biao-jian-ce-he-bian-jie-kuang"><span class="nav-number">3.</span> <span class="nav-text">目标检测和边界框</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#mu-biao-jian-ce"><span class="nav-number">3.1.</span> <span class="nav-text">目标检测</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bian-jie-kuang"><span class="nav-number">3.2.</span> <span class="nav-text">边界框</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#mao-kuang"><span class="nav-number">4.</span> <span class="nav-text">锚框</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#sheng-cheng-duo-ge-mao-kuang"><span class="nav-number">4.1.</span> <span class="nav-text">生成多个锚框</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#jiao-bing-bi"><span class="nav-number">4.2.</span> <span class="nav-text">交并比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#biao-zhu-xun-lian-ji-de-mao-kuang"><span class="nav-number">4.3.</span> <span class="nav-text">标注训练集的锚框</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shu-chu-yu-ce-bian-jie-kuang"><span class="nav-number">4.4.</span> <span class="nav-text">输出预测边界框</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#duo-chi-du-mu-biao-jian-ce"><span class="nav-number">5.</span> <span class="nav-text">多尺度目标检测</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#r-cnn"><span class="nav-number">6.</span> <span class="nav-text">R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#r-cnn-1"><span class="nav-number">6.1.</span> <span class="nav-text">R-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fast-r-cnn"><span class="nav-number">6.2.</span> <span class="nav-text">Fast R-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#faster-r-cnn"><span class="nav-number">6.3.</span> <span class="nav-text">Faster R-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mask-r-cnn"><span class="nav-number">6.4.</span> <span class="nav-text">Mask R-CNN</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#yu-yi-fen-ge-he-shu-ju-ji"><span class="nav-number">7.</span> <span class="nav-text">语义分割和数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#tu-xiang-fen-ge-he-shi-li-fen-ge"><span class="nav-number">7.1.</span> <span class="nav-text">图像分割和实例分割</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#yu-chu-li-shu-ju"><span class="nav-number">7.2.</span> <span class="nav-text">预处理数据</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#quan-juan-ji-wang-luo-fcn"><span class="nav-number">8.</span> <span class="nav-text">全卷积网络（FCN）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#zhuan-zhi-juan-ji-ceng"><span class="nav-number">8.1.</span> <span class="nav-text">转置卷积层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gou-zao-mo-xing"><span class="nav-number">8.2.</span> <span class="nav-text">构造模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#chu-shi-hua-zhuan-zhi-juan-ji-ceng"><span class="nav-number">8.3.</span> <span class="nav-text">初始化转置卷积层</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#yang-shi-qian-yi"><span class="nav-number">9.</span> <span class="nav-text">样式迁移</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#yang-shi-qian-yi-fang-fa"><span class="nav-number">9.1.</span> <span class="nav-text">样式迁移方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#du-qu-nei-rong-tu-xiang-he-yang-shi-tu-xiang"><span class="nav-number">9.2.</span> <span class="nav-text">读取内容图像和样式图像</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#yu-chu-li-he-hou-chu-li-tu-xiang"><span class="nav-number">9.3.</span> <span class="nav-text">预处理和后处理图像</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#chou-qu-te-zheng"><span class="nav-number">9.4.</span> <span class="nav-text">抽取特征</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ding-yi-sun-shi-han-shu"><span class="nav-number">9.5.</span> <span class="nav-text">定义损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#nei-rong-sun-shi"><span class="nav-number">9.5.1.</span> <span class="nav-text">内容损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#yang-shi-sun-shi"><span class="nav-number">9.5.2.</span> <span class="nav-text">样式损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zong-bian-chai-sun-shi"><span class="nav-number">9.5.3.</span> <span class="nav-text">总变差损失</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sun-shi-han-shu"><span class="nav-number">9.6.</span> <span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#chuang-jian-he-chu-shi-hua-he-cheng-tu-xiang"><span class="nav-number">9.6.1.</span> <span class="nav-text">创建和初始化合成图像</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xun-lian"><span class="nav-number">9.7.</span> <span class="nav-text">训练</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#zong-jie"><span class="nav-number">10.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#can-kao-wen-xian"><span class="nav-number">11.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <a href='/'>
    <img class="site-author-image" itemprop="image" alt="Li Zhen"
      src="/images/snoppy.jpeg">
  </a>
  <p class="site-author-name" itemprop="name">Li Zhen</p>
  <div class="site-description" itemprop="description">他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">90</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">66</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jeffery0628" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jeffery0628" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jeffery.lee.0628@gmail.com" title="邮箱 → mailto:jeffery.lee.0628@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮箱</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Zhen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">1.2m</span>
</div>

        
<div class="busuanzi-count">
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.1.0/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  






  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>


 

<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180101,"YYYYMMDD"));
    ages = ages.replace(/years?/, "年");
    ages = ages.replace(/months?/, "月");
    ages = ages.replace(/days?/, "天");
    ages = ages.replace(/hours?/, "小时");
    ages = ages.replace(/minutes?/, "分");
    ages = ages.replace(/seconds?/, "秒");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `我已在此等候你 ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


<script src="https://cdn.jsdelivr.net/npm/sweetalert@2.1.2/dist/sweetalert.min.js"></script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'tChTbEIggDSVcdhPBUf0MFxW-gzGzoHsz',
      appKey     : 'sJ6xUiFnifEDIIfnj3Ut9zy1',
      placeholder: "留下你的小脚印吧！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '8' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
