<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/snoppy.jpeg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open Sans:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css">
  <script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jeffery.ink","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":15,"offset":12,"onmobile":true,"dimmer":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":true,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":true,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="优化算法">
<meta property="og:url" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/index.html">
<meta property="og:site_name" content="火种2号">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.1_output1.svg">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.1_output1.svg">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.1_output2.svg">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.1_output3.svg">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.2_output1.svg">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.2_output2.svg">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.2_output3.svg">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.2_output4.svg">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.2_output5.svg">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.3_output1.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.3_output2.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.3_output3.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.3_output4.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.4_output1.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.4_output2.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.4_output3.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.4_output4.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.4_output5.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.4_output6.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.4_output7.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.4_output8.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.5_output1.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.5_output2.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.5_output3.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.5_output4.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.6_output1.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.6_output2.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.6_output3.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.7_output1.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.7_output2.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.8_output1.png">
<meta property="og:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.8_output2.png">
<meta property="article:published_time" content="2019-07-18T09:43:21.000Z">
<meta property="article:modified_time" content="2020-07-19T01:59:43.611Z">
<meta property="article:author" content="Li Zhen">
<meta property="article:tag" content="optimization">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jeffery.ink/2019/07/18/deeplearning/optimization/7.1_output1.svg">

<link rel="canonical" href="https://jeffery.ink/2019/07/18/deeplearning/optimization/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>优化算法 | 火种2号</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">火种2号</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">火种计划</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>简历</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>读书</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-video fa-fw"></i>电影</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>



</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jeffery.ink/2019/07/18/deeplearning/optimization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/snoppy.jpeg">
      <meta itemprop="name" content="Li Zhen">
      <meta itemprop="description" content="他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="火种2号">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          优化算法
        </h1>

        <div class="post-meta">
          
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-07-19 09:59:43" itemprop="dateModified" datetime="2020-07-19T09:59:43+08:00">2020-07-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">技术/深度学习</span></a>
                </span>
            </span>

          
            <span id="/2019/07/18/deeplearning/optimization/" class="post-meta-item leancloud_visitors" data-flag-title="优化算法" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论：</span>
    
    <a title="valine" href="/2019/07/18/deeplearning/optimization/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/07/18/deeplearning/optimization/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>21k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>19 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/2019/07/18/deeplearning/optimization/7.1_output1.svg" alt></p>
<a id="more"></a>
<h1 id="you-hua-yu-shen-du-xue-xi">优化与深度学习</h1>
<p>在一个深度学习问题中，通常会预先定义一个损失函数。有了损失函数以后，就可以使用优化算法试图将其最小化。在优化中，这样的损失函数通常被称作优化问题的目标函数。依据惯例，优化算法通常只考虑最小化目标函数。其实，任何最大化问题都可以很容易地转化为最小化问题，只需令目标函数的相反数为新的目标函数即可。</p>
<h2 id="you-hua-yu-shen-du-xue-xi-de-guan-xi">优化与深度学习的关系</h2>
<p>虽然优化为深度学习提供了最小化损失函数的方法，但本质上，优化与深度学习的目标是有区别的。由于优化算法的目标函数通常是一个基于训练数据集的损失函数，优化的目标在于降低训练误差。而深度学习的目标在于降低泛化误差。为了降低泛化误差，除了使用优化算法降低训练误差以外，还需要注意应对过拟合。</p>
<h2 id="you-hua-zai-shen-du-xue-xi-zhong-de-tiao-zhan">优化在深度学习中的挑战</h2>
<p>深度学习中绝大多数目标函数都很复杂。因此，很多优化问题并不存在解析解，而需要使用基于数值方法的优化算法找到近似解，即数值解。为了求得最小化目标函数的数值解，将通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。</p>
<p>优化在深度学习中有很多挑战。下面描述了其中的两个挑战，即局部最小值和鞍点。</p>
<h3 id="ju-bu-zui-xiao-zhi">局部最小值</h3>
<p>对于目标函数\(f(x)\)，如果\(f(x)\)在\(x\)上的值比在\(x\)邻近的其他点的值更小，那么\(f(x)\)可能是一个局部最小值（local minimum）。如果\(f(x)\)在\(x\)上的值是目标函数在整个定义域上的最小值，那么\(f(x)\)是全局最小值（global minimum）。</p>
<p>举个例子，给定函数</p>
<p>\[
f(x) = x \cdot \text{cos}(\pi x), \qquad -1.0 \leq x \leq 2.0,
\]</p>
<p>可以大致找出该函数的局部最小值和全局最小值的位置。需要注意的是，图中箭头所指示的只是大致位置。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.1_output1.svg" alt></p>
<p>深度学习模型的目标函数可能有若干局部最优值。当一个优化问题的数值解在局部最优解附近时，由于目标函数有关解的梯度接近或变成零，最终迭代求得的数值解可能只令目标函数局部最小化而非全局最小化。</p>
<h3 id="an-dian">鞍点</h3>
<p>梯度接近或变成零可能是由于当前解在局部最优解附近造成的。事实上，另一种可能性是当前解在鞍点（saddle point）附近。举个例子，给定函数</p>
<p>\[
f(x) = x^3
\]</p>
<p>可以找出该函数的鞍点位置。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.1_output2.svg" alt></p>
<p>再举个定义在二维空间的函数的例子，例如：</p>
<p>\[f(x, y) = x^2 - y^2.\]</p>
<p>可以找出该函数的鞍点位置。该函数看起来像一个马鞍，而鞍点恰好是马鞍上可坐区域的中心。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.1_output3.svg" alt></p>
<p>在图的鞍点位置，目标函数在\(x\)轴方向上是局部最小值，但在\(y\)轴方向上是局部最大值。</p>
<p>假设一个函数的输入为\(k\)维向量，输出为标量，那么它的海森矩阵（Hessian matrix）有\(k\)个特征值。该函数在梯度为0的位置上可能是局部最小值、局部最大值或者鞍点。</p>
<ul>
<li>当函数的海森矩阵在梯度为零的位置上的特征值全为正时，该函数得到局部最小值。</li>
<li>当函数的海森矩阵在梯度为零的位置上的特征值全为负时，该函数得到局部最大值。</li>
<li>当函数的海森矩阵在梯度为零的位置上的特征值有正有负时，该函数得到鞍点。</li>
</ul>
<p>随机矩阵理论告诉我们，对于一个大的高斯随机矩阵来说，任一特征值是正或者是负的概率都是0.5 。那么，以上第一种情况的概率为 \(0.5^k\)。由于深度学习模型参数通常都是高维的（\(k\)很大），目标函数的鞍点通常比局部最小值更常见。</p>
<h1 id="ti-du-xia-jiang-he-sui-ji-ti-du-xia-jiang">梯度下降和随机梯度下降</h1>
<p>虽然梯度下降在深度学习中很少被直接使用，但理解梯度的意义以及沿着梯度反方向更新自变量可能降低目标函数值的原因是学习后续优化算法的基础。</p>
<h2 id="yi-wei-ti-du-xia-jiang">一维梯度下降</h2>
<p>先以简单的一维梯度下降为例，解释梯度下降算法可能降低目标函数值的原因。假设连续可导的函数\(f: \mathbb{R} \rightarrow \mathbb{R}\)的输入和输出都是标量。给定绝对值足够小的数\(\epsilon\)，根据泰勒展开公式，我们得到以下的近似：</p>
<p>\[
f(x + \epsilon) \approx f(x) + \epsilon f'(x)
\]</p>
<p>这里\(f'(x)\)是函数\(f\)在\(x\)处的梯度。一维函数的梯度是一个标量，也称导数。接下来，找到一个常数\(\eta > 0\)，使得\(\left|\eta f'(x)\right|\)足够小，那么可以将\(\epsilon\)替换为\(-\eta f'(x)\)并得到</p>
<p>\[
f(x - \eta f'(x)) \approx f(x) -  \eta f'(x)^2
\]</p>
<p>如果导数\(f'(x) \neq 0\)，那么\(\eta f'(x)^2>0\)，所以</p>
<p>\[
f(x - \eta f'(x)) \lesssim f(x)
\]</p>
<p>这意味着，如果通过</p>
<p>\[
x \leftarrow x - \eta f'(x)
\]</p>
<p>来迭代\(x\)，函数\(f(x)\)的值可能会降低。因此在梯度下降中，我们先选取一个初始值\(x\)和常数\(\eta > 0\)，然后不断通过上式来迭代\(x\)，直到达到停止条件，例如\(f'(x)^2\)的值已足够小或迭代次数已达到某个值。</p>
<p>下面以目标函数\(f(x)=x^2\)为例来看一看梯度下降是如何工作的。虽然知道最小化\(f(x)\)的解为\(x=0\)，这里依然使用这个简单函数来观察\(x\)是如何被迭代的。</p>
<p>接下来使用\(x=10\)作为初始值，并设\(\eta=0.2\)。使用梯度下降对\(x\)迭代10次，可见最终\(x\)的值较接近最优解。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gd</span><span class="params">(eta)</span>:</span></span><br><span class="line">    x = <span class="number">10</span></span><br><span class="line">    results = [x]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        x -= eta * <span class="number">2</span> * x  <span class="comment"># f(x) = x * x的导数为f'(x) = 2 * x</span></span><br><span class="line">        results.append(x)</span><br><span class="line">    print(<span class="string">'epoch 10, x:'</span>, x)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">res = gd(<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.2_output1.svg" alt></p>
<h2 id="xue-xi-lu">学习率</h2>
<p>上述梯度下降算法中的正数\(\eta\)通常叫作学习率。这是一个超参数，需要人工设定。如果使用过小的学习率，会导致\(x\)更新缓慢从而需要更多的迭代才能得到较好的解。</p>
<p>下面展示使用学习率\(\eta=0.05\)时自变量\(x\)的迭代轨迹。可见，同样迭代10次后，当学习率过小时，最终\(x\)的值依然与最优解存在较大偏差。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.2_output2.svg" alt></p>
<p>如果使用过大的学习率，\(\left|\eta f'(x)\right|\)可能会过大从而使前面提到的一阶泰勒展开公式不再成立：这时无法保证迭代\(x\)会降低\(f(x)\)的值。</p>
<p>举个例子，当设学习率\(\eta=1.1\)时，可以看到\(x\)不断越过（overshoot）最优解\(x=0\)并逐渐发散。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.2_output3.svg" alt></p>
<h2 id="duo-wei-ti-du-xia-jiang">多维梯度下降</h2>
<p>在了解了一维梯度下降之后，再考虑一种更广义的情况：目标函数的输入为向量，输出为标量。假设目标函数\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)的输入是一个\(d\)维向量\(\boldsymbol{x} = [x_1, x_2, \ldots, x_d]^\top\)。目标函数\(f(\boldsymbol{x})\)有关\(\boldsymbol{x}\)的梯度是一个由\(d\)个偏导数组成的向量：</p>
<p>\[
\nabla_{\boldsymbol{x}} f(\boldsymbol{x}) = \bigg[\frac{\partial f(\boldsymbol{x})}{\partial x_1}, \frac{\partial f(\boldsymbol{x})}{\partial x_2}, \ldots, \frac{\partial f(\boldsymbol{x})}{\partial x_d}\bigg]^\top
\]</p>
<p>为表示简洁，用\(\nabla f(\boldsymbol{x})\)代替\(\nabla_{\boldsymbol{x}} f(\boldsymbol{x})\)。梯度中每个偏导数元素\(\partial f(\boldsymbol{x})/\partial x_i\)代表着\(f\)在\(\boldsymbol{x}\)有关输入\(x_i\)的变化率。为了测量\(f\)沿着单位向量\(\boldsymbol{u}\)（即\(\|\boldsymbol{u}\|=1\)）方向上的变化率，在多元微积分中，定义\(f\)在\(\boldsymbol{x}\)上沿着\(\boldsymbol{u}\)方向的方向导数为</p>
<p>\[
\text{D}_{\boldsymbol{u}} f(\boldsymbol{x}) = \lim_{h \rightarrow 0}  \frac{f(\boldsymbol{x} + h \boldsymbol{u}) - f(\boldsymbol{x})}{h}
\]</p>
<p>依据方向导数性质，以上方向导数可以改写为</p>
<p>\[
\text{D}_{\boldsymbol{u}} f(\boldsymbol{x}) = \nabla f(\boldsymbol{x}) \cdot \boldsymbol{u}
\]</p>
<p>方向导数\(\text{D}_{\boldsymbol{u}} f(\boldsymbol{x})\)给出了\(f\)在\(\boldsymbol{x}\)上沿着所有可能方向的变化率。为了最小化\(f\)，希望找到\(f\)能被降低最快的方向。因此，可以通过单位向量\(\boldsymbol{u}\)来最小化方向导数\(\text{D}_{\boldsymbol{u}} f(\boldsymbol{x})\)。</p>
<p>由于<br>
\[
\text{D}_{\boldsymbol{u}} f(\boldsymbol{x}) = \|\nabla f(\boldsymbol{x})\| \cdot \|\boldsymbol{u}\|  \cdot \text{cos} (\theta) = \|\nabla f(\boldsymbol{x})\|  \cdot \text{cos} (\theta)
\]<br>
其中\(\theta\)为梯度\(\nabla f(\boldsymbol{x})\)和单位向量\(\boldsymbol{u}\)之间的夹角，当\(\theta = \pi\)时，\(\text{cos}(\theta)\)取得最小值\(-1\)。因此，当\(\boldsymbol{u}\)在梯度方向\(\nabla f(\boldsymbol{x})\)的相反方向时，方向导数\(\text{D}_{\boldsymbol{u}} f(\boldsymbol{x})\)被最小化。因此，我们可能通过梯度下降算法来不断降低目标函数\(f\)的值：</p>
<p>\[
\boldsymbol{x} \leftarrow \boldsymbol{x} - \eta \nabla f(\boldsymbol{x})
\]</p>
<p>同样，其中\(\eta\)（取正数）称作学习率。</p>
<p>下面构造一个输入为二维向量\(\boldsymbol{x} = [x_1, x_2]^\top\)和输出为标量的目标函数\(f(\boldsymbol{x})=x_1^2+2x_2^2\)。那么，梯度\(\nabla f(\boldsymbol{x}) = [2x_1, 4x_2]^\top\)。观察梯度下降从初始位置\([-5,-2]\)开始对自变量\(\boldsymbol{x}\)的迭代轨迹。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.2_output4.svg" alt></p>
<h2 id="sui-ji-ti-du-xia-jiang">随机梯度下降</h2>
<p>在深度学习里，目标函数通常是训练数据集中有关各个样本的损失函数的平均。设\(f_i(\boldsymbol{x})\)是有关索引为\(i\)的训练数据样本的损失函数，\(n\)是训练数据样本数，\(\boldsymbol{x}\)是模型的参数向量，那么目标函数定义为</p>
<p>\[
f(\boldsymbol{x}) = \frac{1}{n} \sum_{i = 1}^n f_i(\boldsymbol{x})
\]</p>
<p>目标函数在\(\boldsymbol{x}\)处的梯度计算为</p>
<p>\[
\nabla f(\boldsymbol{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\boldsymbol{x})
\]</p>
<p>如果使用梯度下降，每次自变量迭代的计算开销为\(\mathcal{O}(n)\)，它随着\(n\)线性增长。因此，当训练数据样本数很大时，梯度下降每次迭代的计算开销很高。</p>
<p>随机梯度下降减少了每次迭代的计算开销。在随机梯度下降的每次迭代中，我们随机均匀采样的一个样本索引\(i\in\{1,\ldots,n\}\)，并计算梯度\(\nabla f_i(\boldsymbol{x})\)来迭代\(\boldsymbol{x}\)：</p>
<p>\[
\boldsymbol{x} \leftarrow \boldsymbol{x} - \eta \nabla f_i(\boldsymbol{x})
\]</p>
<p>这里\(\eta\)同样是学习率。可以看到每次迭代的计算开销从梯度下降的\(\mathcal{O}(n)\)降到了常数\(\mathcal{O}(1)\)。值得强调的是，随机梯度\(\nabla f_i(\boldsymbol{x})\)是对梯度\(\nabla f(\boldsymbol{x})\)的无偏估计：</p>
<p>\[
E_i \nabla f_i(\boldsymbol{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\boldsymbol{x}) = \nabla f(\boldsymbol{x})
\]</p>
<p>这意味着，平均来说，随机梯度是对梯度的一个良好的估计。</p>
<p>下面我们通过在梯度中添加均值为0的随机噪声来模拟随机梯度下降，以此来比较它与梯度下降的区别。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.2_output5.svg" alt></p>
<p>可以看到，随机梯度下降中自变量的迭代轨迹相对于梯度下降中的来说更为曲折。这是由于实验所添加的噪声使模拟的随机梯度的准确度下降。在实际中，这些噪声通常指训练数据集中的无意义的干扰。</p>
<h2 id="xiao-pi-liang-sui-ji-ti-du-xia-jiang">小批量随机梯度下降</h2>
<p>在每一次迭代中，梯度下降使用整个训练数据集来计算梯度，因此它有时也被称为批量梯度下降（batch gradient descent）。而随机梯度下降在每次迭代中只随机采样一个样本来计算梯度。还可以在每轮迭代中随机均匀采样多个样本来组成一个小批量，然后使用这个小批量来计算梯度。</p>
<p>设目标函数\(f(\boldsymbol{x}): \mathbb{R}^d \rightarrow \mathbb{R}\)。在迭代开始前的时间步设为0。该时间步的自变量记为\(\boldsymbol{x}_0\in \mathbb{R}^d\)，通常由随机初始化得到。在接下来的每一个时间步\(t>0\)中，小批量随机梯度下降随机均匀采样一个由训练数据样本索引组成的小批量\(\mathcal{B}_t\)。我们可以通过重复采样（sampling with replacement）或者不重复采样（sampling without replacement）得到一个小批量中的各个样本。前者允许同一个小批量中出现重复的样本，后者则不允许如此，且更常见。对于这两者间的任一种方式，都可以使用</p>
<p>\[
\boldsymbol{g}_t \leftarrow \nabla f_{\mathcal{B}_t}(\boldsymbol{x}_{t-1}) = \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t}\nabla f_i(\boldsymbol{x}_{t-1})
\]</p>
<p>来计算时间步\(t\)的小批量\(\mathcal{B}_t\)上目标函数位于\(\boldsymbol{x}_{t-1}\)处的梯度\(\boldsymbol{g}_t\)。这里\(|\mathcal{B}|\)代表批量大小，即小批量中样本的个数，是一个超参数。同随机梯度一样，重复采样所得的小批量随机梯度\(\boldsymbol{g}_t\)也是对梯度\(\nabla f(\boldsymbol{x}_{t-1})\)的无偏估计。给定学习率\(\eta_t\)（取正数），小批量随机梯度下降对自变量的迭代如下：</p>
<p>\[
\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \eta_t \boldsymbol{g}_t
\]</p>
<p>基于随机采样得到的梯度的方差在迭代过程中无法减小，因此在实际中，（小批量）随机梯度下降的学习率可以在迭代过程中自我衰减，例如\(\eta_t=\eta t^\alpha\)（通常\(\alpha=-1\)或者\(-0.5\)）、\(\eta_t = \eta \alpha^t\)（如\(\alpha=0.95\)）或者每迭代若干次后将学习率衰减一次。如此一来，学习率和（小批量）随机梯度<strong>乘积</strong>的方差会减小。而梯度下降在迭代过程中一直使用目标函数的真实梯度，无须自我衰减学习率。</p>
<p>小批量随机梯度下降中每次迭代的计算开销为\(\mathcal{O}(|\mathcal{B}|)\)。当批量大小为1时，该算法即为随机梯度下降；当批量大小等于训练数据样本数时，该算法即为梯度下降。当批量较小时，每次迭代中使用的样本少，这会导致并行处理和内存使用效率变低。这使得在计算同样数目样本的情况下比使用更大批量时所花时间更多。当批量较大时，每个小批量梯度里可能含有更多的冗余信息。为了得到较好的解，批量较大时比批量较小时需要计算的样本数目可能更多，例如增大迭代周期数。</p>
<h3 id="dai-ma-shi-xian">代码实现</h3>
<p>向小批量随机梯度下降算法添加了一个状态输入<code>states</code>并将超参数放在字典<code>hyperparams</code>里。此外，将在训练函数里对各个小批量样本的损失求平均，因此优化算法里的梯度不需要除以批量大小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, states,hyperparams,grads)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i,p <span class="keyword">in</span> enumerate(params):</span><br><span class="line">        p.assign_sub(hyperparams[<span class="string">'lr'</span>] * grads[i])</span><br></pre></td></tr></table></figure>
<p>当批量大小为样本总数1,500时，优化使用的是梯度下降。梯度下降的1个迭代周期对模型参数只迭代1次。可以看到6次迭代后目标函数值的下降趋向了平稳。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_sgd(<span class="number">1</span>, <span class="number">1500</span>, <span class="number">6</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.3_output1.png" alt></p>
<p>当批量大小为1时，优化使用的是随机梯度下降。为了简化实现，有关（小批量）随机梯度下降的实验中，未对学习率进行自我衰减，而是直接采用较小的常数学习率。随机梯度下降中，每处理一个样本会更新一次自变量（模型参数），一个迭代周期里会对自变量进行1,500次更新。可以看到，目标函数值的下降在1个迭代周期后就变得较为平缓。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_sgd(<span class="number">0.005</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.3_output2.png" alt></p>
<p>虽然随机梯度下降和梯度下降在一个迭代周期里都处理了1,500个样本，但实验中随机梯度下降的一个迭代周期耗时更多。这是因为随机梯度下降在一个迭代周期里做了更多次的自变量迭代，而且单样本的梯度计算难以有效利用矢量计算。</p>
<p>当批量大小为10时，优化使用的是小批量随机梯度下降。它在每个迭代周期的耗时介于梯度下降和随机梯度下降的耗时之间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_sgd(<span class="number">0.05</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.3_output3.png" alt></p>
<p>无须自己实现小批量随机梯度下降算法。tensorflow.keras.optimizers 模块提供了很多常用的优化算法比如SGD、Adam和RMSProp等。下面创建一个用于优化model 所有参数的优化器实例，并指定学习率为0.05的小批量随机梯度下降（SGD）为优化算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line">trainer = optimizers.SGD(learning_rate=<span class="number">0.05</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.3_output4.png" alt></p>
<h1 id="dong-liang-fa">动量法</h1>
<p>梯度下降和随机梯度下降中我们提到，目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫作最陡下降。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。</p>
<h2 id="ti-du-xia-jiang-de-wen-ti">梯度下降的问题</h2>
<p>考虑一个输入和输出分别为二维向量\(\boldsymbol{x} = [x_1, x_2]^\top\)和标量的目标函数\(f(\boldsymbol{x})=0.1x_1^2+2x_2^2\)。这里将\(x_1^2\)系数从\(1\)减小到了\(0.1\)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">eta = <span class="number">0.4</span> <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gd_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x1 - eta * <span class="number">0.2</span> * x1, x2 - eta * <span class="number">4</span> * x2, <span class="number">0</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.4_output1.png" alt></p>
<p>可以看到，同一位置上，目标函数在竖直方向（\(x_2\)轴方向）比在水平方向（\(x_1\)轴方向）的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。那么，就需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这会造成自变量在水平方向上朝最优解移动变慢。</p>
<p>下面我们试着将学习率调得稍大一点，此时自变量在竖直方向不断越过最优解并逐渐发散。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.4_output2.png" alt></p>
<h2 id="dong-liang-fa-1">动量法</h2>
<p>动量法的提出是为了解决梯度下降的上述问题。由于小批量随机梯度下降比梯度下降更为广义，后续讨论将沿用小批量随机梯度下降中时间步\(t\)的小批量随机梯度\(\boldsymbol{g}_t\)的定义。设时间步\(t\)的自变量为\(\boldsymbol{x}_t\)，学习率为\(\eta_t\)。<br>
在时间步\(0\)，动量法创建速度变量\(\boldsymbol{v}_0\)，并将其元素初始化成0。在时间步\(t>0\)，动量法对每次迭代的步骤做如下修改：<br>
\[
\begin{aligned}
\boldsymbol{v}_t &amp;\leftarrow \gamma \boldsymbol{v}_{t-1} + \eta_t \boldsymbol{g}_t, \\
\boldsymbol{x}_t &amp;\leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{v}_t,
\end{aligned}
\]</p>
<p>其中，动量超参数\(\gamma\)满足\(0 \leq \gamma &lt; 1\)。当\(\gamma=0\)时，动量法等价于小批量随机梯度下降。</p>
<p>在解释动量法的数学原理前，让我们先从实验中观察梯度下降在使用动量法后的迭代轨迹。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">momentum_2d</span><span class="params">(x1, x2, v1, v2)</span>:</span></span><br><span class="line">    v1 = gamma * v1 + eta * <span class="number">0.2</span> * x1</span><br><span class="line">    v2 = gamma * v2 + eta * <span class="number">4</span> * x2</span><br><span class="line">    <span class="keyword">return</span> x1 - v1, x2 - v2, v1, v2</span><br><span class="line"></span><br><span class="line">eta, gamma = <span class="number">0.4</span>, <span class="number">0.5</span></span><br><span class="line">show_trace_2d(f_2d, d2l.train_2d(momentum_2d))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.4_output3.png" alt></p>
<p>可以看到使用较小的学习率\(\eta=0.4\)和动量超参数\(\gamma=0.5\)时，动量法在竖直方向上的移动更加平滑，且在水平方向上更快逼近最优解。下面使用较大的学习率\(\eta=0.6\)，此时自变量也不再发散。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">eta = <span class="number">0.6</span></span><br><span class="line">show_trace_2d(f_2d, d2l.train_2d(momentum_2d))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.4_output4.png" alt></p>
<h3 id="zhi-shu-jia-quan-yi-dong-ping-jun">指数加权移动平均</h3>
<p>为了从数学上理解动量法，先解释一下指数加权移动平均（exponentially weighted moving average）。给定超参数\(0 \leq \gamma &lt; 1\)，当前时间步\(t\)的变量\(y_t\)是上一时间步\(t-1\)的变量\(y_{t-1}\)和当前时间步另一变量\(x_t\)的线性组合：</p>
<p>\[
y_t = \gamma y_{t-1} + (1-\gamma) x_t
\]</p>
<p>可以对\(y_t\)展开：</p>
<p>\[
\begin{aligned}
y_t  &amp;= (1-\gamma) x_t + \gamma y_{t-1}\\
         &amp;= (1-\gamma)x_t + (1-\gamma) \cdot \gamma x_{t-1} + \gamma^2y_{t-2}\\
         &amp;= (1-\gamma)x_t + (1-\gamma) \cdot \gamma x_{t-1} + (1-\gamma) \cdot \gamma^2x_{t-2} + \gamma^3y_{t-3}\\
         &amp;\ldots
\end{aligned}
\]</p>
<p>令\(n = 1/(1-\gamma)\)，那么 \(\left(1-1/n\right)^n = \gamma^{1/(1-\gamma)}\)。因为</p>
<p>\[
\lim_{n \rightarrow \infty}  \left(1-\frac{1}{n}\right)^n = \exp(-1) \approx 0.3679
\]</p>
<p>所以当\(\gamma \rightarrow 1\)时，\(\gamma^{1/(1-\gamma)}=\exp(-1)\approx 0.3679\)，如\(0.95^{20} \approx \exp(-1)\)。如果把\(\exp(-1)\)当作一个比较小的数，我们可以在近似中忽略所有含\(\gamma^{1/(1-\gamma)}\)和比\(\gamma^{1/(1-\gamma)}\)更高阶的系数的项。例如，当\(\gamma=0.95\)时，</p>
<p>\[
y_t \approx 0.05 \sum_{i=0}^{19} 0.95^i x_{t-i}
\]</p>
<p>因此，在实际中，常常将\(y_t\)看作是对最近\(1/(1-\gamma)\)个时间步的\(x_t\)值的加权平均。例如，当\(\gamma = 0.95\)时，\(y_t\)可以被看作对最近20个时间步的\(x_t\)值的加权平均；当\(\gamma = 0.9\)时，\(y_t\)可以看作是对最近10个时间步的\(x_t\)值的加权平均。而且，离当前时间步\(t\)越近的\(x_t\)值获得的权重越大（越接近1）。</p>
<h3 id="you-zhi-shu-jia-quan-yi-dong-ping-jun-li-jie-dong-liang-fa">由指数加权移动平均理解动量法</h3>
<p>现在，对动量法的速度变量做变形：</p>
<p>\[
\boldsymbol{v}_t \leftarrow \gamma \boldsymbol{v}_{t-1} + (1 - \gamma) \left(\frac{\eta_t}{1 - \gamma} \boldsymbol{g}_t\right)
\]</p>
<p>由指数加权移动平均的形式可得，速度变量\(\boldsymbol{v}_t\)实际上对序列\(\{\eta_{t-i}\boldsymbol{g}_{t-i} /(1-\gamma):i=0,\ldots,1/(1-\gamma)-1\}\)做了指数加权移动平均。换句话说，相比于小批量随机梯度下降，<strong>动量法在每个时间步的自变量更新量近似于将最近\(1/(1-\gamma)\)个时间步的普通更新量（即学习率乘以梯度）做了指数加权移动平均后再除以\(1-\gamma\)</strong>。所以，在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。在本节之前示例的优化问题中，所有梯度在水平方向上为正（向右），而在竖直方向上时正（向上）时负（向下）。这样，我们就可以使用较大的学习率，从而使自变量向最优解更快移动。</p>
<h2 id="dai-ma-shi-xian-1">代码实现</h2>
<p>相对于小批量随机梯度下降，动量法需要对每一个自变量维护一个同它一样形状的速度变量，且超参数里多了动量超参数。实现中，将速度变量用更广义的状态变量<code>states</code>表示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_momentum_states</span><span class="params">()</span>:</span></span><br><span class="line">    v_w = tf.zeros((features.shape[<span class="number">1</span>],<span class="number">1</span>))</span><br><span class="line">    v_b = tf.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (v_w, v_b)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_momentum</span><span class="params">(params, states, hyperparams,grads)</span>:</span></span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p,v <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        v=hyperparams[<span class="string">'momentum'</span>] * v + hyperparams[<span class="string">'lr'</span>] * grads[i]</span><br><span class="line">        p.assign_sub(v)</span><br><span class="line">        i+=<span class="number">1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_momentum_states</span><span class="params">()</span>:</span></span><br><span class="line">    v_w = tf.zeros((features.shape[<span class="number">1</span>],<span class="number">1</span>))</span><br><span class="line">    v_b = tf.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (v_w, v_b)</span><br></pre></td></tr></table></figure>
<p>先将动量超参数<code>momentum</code>设0.5，这时可以看成是特殊的小批量随机梯度下降：其小批量随机梯度为最近2个时间步的2倍小批量梯度的加权平均。</p>
<blockquote>
<p>注：这里不应该是“加权平均”而应该是“加权和”，因为加权平均最后除以了\(1-\gamma\)，所以就相当于没有进行平均。</p>
</blockquote>
<p><img src="/2019/07/18/deeplearning/optimization/7.4_output5.png" alt></p>
<p>将动量超参数<code>momentum</code>增大到0.9，这时依然可以看成是特殊的小批量随机梯度下降：其小批量随机梯度为最近10个时间步的10倍小批量梯度的加权平均。先保持学习率0.02不变。</p>
<blockquote>
<p>同理，这里不应该是“加权平均”而应该是“加权和”。</p>
</blockquote>
<p><img src="/2019/07/18/deeplearning/optimization/7.4_output6.png" alt></p>
<p>可见目标函数值在后期迭代过程中的变化不够平滑。直觉上，10倍小批量梯度比2倍小批量梯度大了5倍，可以试着将学习率减小到原来的1/5。此时目标函数值在下降了一段时间后变化更加平滑。</p>
<blockquote>
<p>这也印证了刚刚的观点。</p>
</blockquote>
<p><img src="/2019/07/18/deeplearning/optimization/7.4_output7.png" alt></p>
<p>在Tensorflow中，只需要通过参数<code>momentum</code>来指定动量超参数即可使用动量法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line">trainer = optimizers.SGD(learning_rate=<span class="number">0.004</span>,momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.4_output8.png" alt></p>
<h1 id="ada-grad-suan-fa">AdaGrad算法</h1>
<p>目标函数自变量的每一个元素在相同时间步都使用同一个学习率来自我迭代。举个例子，假设目标函数为\(f\)，自变量为一个二维向量\([x_1, x_2]^\top\)，该向量中每一个元素在迭代时都使用相同的学习率。例如，在学习率为\(\eta\)的梯度下降中，元素\(x_1\)和\(x_2\)都使用相同的学习率\(\eta\)来自我迭代：</p>
<p>\[
x_1 \leftarrow x_1 - \eta \frac{\partial{f}}{\partial{x_1}}, \quad
x_2 \leftarrow x_2 - \eta \frac{\partial{f}}{\partial{x_2}}.
\]</p>
<p>当\(x_1\)和\(x_2\)的梯度值有较大差别时，需要选择足够小的学习率使得自变量在梯度值较大的维度上不发散。但这样会导致自变量在梯度值较小的维度上迭代过慢。动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能。<strong>AdaGrad算法，它根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题</strong>。</p>
<h2 id="suan-fa">算法</h2>
<p>AdaGrad算法会使用一个小批量随机梯度\(\boldsymbol{g}_t\)按元素平方的累加变量\(\boldsymbol{s}_t\)。在时间步0，AdaGrad将\(\boldsymbol{s}_0\)中每个元素初始化为0。在时间步\(t\)，首先将小批量随机梯度\(\boldsymbol{g}_t\)按元素平方后累加到变量\(\boldsymbol{s}_t\)：</p>
<p>\[
\boldsymbol{s}_t \leftarrow \boldsymbol{s}_{t-1} + \boldsymbol{g}_t \odot \boldsymbol{g}_t
\]</p>
<p>其中\(\odot\)是按元素相乘。接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整一下：</p>
<p>\[
\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t
\]</p>
<p>其中\(\eta\)是学习率，\(\epsilon\)是为了维持数值稳定性而添加的常数，如\(10^{-6}\)。这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。</p>
<h2 id="te-dian">特点</h2>
<p>需要强调的是，小批量随机梯度按元素平方的累加变量\(\boldsymbol{s}_t\)出现在学习率的分母项中。因此，如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快；反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。然而，由于\(\boldsymbol{s}_t\)一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。所以，<strong>当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解</strong>。</p>
<p>下面以目标函数\(f(\boldsymbol{x})=0.1x_1^2+2x_2^2\)为例观察AdaGrad算法对自变量的迭代轨迹。实现AdaGrad算法并使用学习率0.4。可以看到，自变量的迭代轨迹较平滑。但由于\(\boldsymbol{s}_t\)的累加效果使学习率不断衰减，自变量在迭代后期的移动幅度较小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adagrad_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    g1, g2, eps = <span class="number">0.2</span> * x1, <span class="number">4</span> * x2, <span class="number">1e-6</span>  <span class="comment"># 前两项为自变量梯度</span></span><br><span class="line">    s1 += g1 ** <span class="number">2</span></span><br><span class="line">    s2 += g2 ** <span class="number">2</span></span><br><span class="line">    x1 -= eta / math.sqrt(s1 + eps) * g1</span><br><span class="line">    x2 -= eta / math.sqrt(s2 + eps) * g2</span><br><span class="line">    <span class="keyword">return</span> x1, x2, s1, s2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.4</span></span><br><span class="line">show_trace_2d(f_2d, train_2d(adagrad_2d))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.5_output1.png" alt></p>
<p>下面将学习率增大到2。可以看到自变量更为迅速地逼近了最优解。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">eta = <span class="number">2</span></span><br><span class="line">show_trace_2d(f_2d, train_2d(adagrad_2d))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.5_output2.png" alt></p>
<h2 id="dai-ma-shi-xian-2">代码实现</h2>
<p>同动量法一样，AdaGrad算法需要对每个自变量维护同它一样形状的状态变量。根据AdaGrad算法中的公式实现该算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adagrad_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w = tf.zeros((features.shape[<span class="number">1</span>],<span class="number">1</span>),dtype=tf.float32)</span><br><span class="line">    s_b = tf.zeros(<span class="number">1</span>,dtype=tf.float32)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adagrad</span><span class="params">(params, states, hyperparams,grads)</span>:</span></span><br><span class="line">    eps = <span class="number">1e-6</span></span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s += (grads[i]**<span class="number">2</span>)</span><br><span class="line">        p.assign_sub(hyperparams[<span class="string">'lr'</span>]*grads[i]/tf.sqrt(s+eps))</span><br><span class="line">        i+=<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>这里使用更大的学习率来训练模型。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.5_output3.png" alt></p>
<p>使用Tensorflow2提供的AdaGrad算法来训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line">trainer = optimizers.Adagrad(learning_rate=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.5_output4.png" alt></p>
<h1 id="rms-prop-suan-fa">RMSProp算法</h1>
<p>因为调整学习率时分母上的变量\(\boldsymbol{s}_t\)一直在累加按元素平方的小批量随机梯度，所以目标函数自变量每个元素的学习率在迭代过程中一直在降低（或不变）。因此，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSProp算法对AdaGrad算法做了一点小小的修改。该算法源自Coursera上的一门课程，即“机器学习的神经网络” 。</p>
<h2 id="suan-fa-1">算法</h2>
<p>动量法里介绍过指数加权移动平均。不同于AdaGrad算法里状态变量\(\boldsymbol{s}_t\)是截至时间步\(t\)所有小批量随机梯度\(\boldsymbol{g}_t\)按元素平方和，RMSProp算法将这些梯度按元素平方做指数加权移动平均。具体来说，给定超参数\(0 \leq \gamma &lt; 1\)，RMSProp算法在时间步\(t>0\)计算</p>
<p>\[
\boldsymbol{s}_t \leftarrow \gamma \boldsymbol{s}_{t-1} + (1 - \gamma) \boldsymbol{g}_t \odot \boldsymbol{g}_t
\]</p>
<p>和AdaGrad算法一样，RMSProp算法将目标函数自变量中每个元素的学习率通过按元素运算重新调整，然后更新自变量</p>
<p>\[
\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t
\]</p>
<p>其中\(\eta\)是学习率，\(\epsilon\)是为了维持数值稳定性而添加的常数，如\(10^{-6}\)。因为RMSProp算法的状态变量\(\boldsymbol{s}_t\)是对平方项\(\boldsymbol{g}_t \odot \boldsymbol{g}_t\)的指数加权移动平均，所以可以看作是最近\(1/(1-\gamma)\)个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。</p>
<p>先观察RMSProp算法对目标函数\(f(\boldsymbol{x})=0.1x_1^2+2x_2^2\)中自变量的迭代轨迹。使用的学习率为0.4的AdaGrad算法，自变量在迭代后期的移动幅度较小。但在同样的学习率下，RMSProp算法可以更快逼近最优解。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    g1, g2, eps = <span class="number">0.2</span> * x1, <span class="number">4</span> * x2, <span class="number">1e-6</span></span><br><span class="line">    s1 = gamma * s1 + (<span class="number">1</span> - gamma) * g1 ** <span class="number">2</span></span><br><span class="line">    s2 = gamma * s2 + (<span class="number">1</span> - gamma) * g2 ** <span class="number">2</span></span><br><span class="line">    x1 -= eta / math.sqrt(s1 + eps) * g1</span><br><span class="line">    x2 -= eta / math.sqrt(s2 + eps) * g2</span><br><span class="line">    <span class="keyword">return</span> x1, x2, s1, s2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">eta, gamma = <span class="number">0.4</span>, <span class="number">0.9</span></span><br><span class="line">show_trace_2d(f_2d, train_2d(rmsprop_2d))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.6_output1.png" alt></p>
<h2 id="dai-ma-shi-xian-3">代码实现</h2>
<p>接下来按照RMSProp算法中的公式实现该算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rmsprop_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w = tf.zeros((features.shape[<span class="number">1</span>],<span class="number">1</span>),dtype=tf.float32)</span><br><span class="line">    s_b = tf.zeros(<span class="number">1</span>,dtype=tf.float32)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop</span><span class="params">(params, states, hyperparams,grads)</span>:</span></span><br><span class="line">    gamma, eps, i = hyperparams[<span class="string">'gamma'</span>], <span class="number">1e-6</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s=gamma*s+(<span class="number">1</span>-gamma)*(grads[i])**<span class="number">2</span></span><br><span class="line">        p.assign_sub(hyperparams[<span class="string">'lr'</span>]*grads[i]/tf.sqrt(s+eps))</span><br><span class="line">        i+=<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>将初始学习率设为0.01，并将超参数\(\gamma\)设为0.9。此时，变量\(\boldsymbol{s}_t\)可看作是最近\(1/(1-0.9) = 10\)个时间步的平方项\(\boldsymbol{g}_t \odot \boldsymbol{g}_t\)的加权平均。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_ch7(rmsprop, init_rmsprop_states(), &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>, <span class="string">'gamma'</span>: <span class="number">0.9</span>&#125;,</span><br><span class="line">              features, labels)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.6_output2.png" alt></p>
<p>通过名称为<code>RMSprop</code>的优化器方法，便可使用Tensorflow2中提供的RMSProp算法来训练模型。注意，超参数\(\gamma\)通过<code>alpha</code>指定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line">trainer = optimizers.RMSprop(learning_rate=<span class="number">0.01</span>,rho=<span class="number">0.9</span>)</span><br><span class="line">d2l.train_tensorflow2_ch7(trainer, &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>&#125;,</span><br><span class="line">                    features, labels)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.6_output3.png" alt></p>
<p>RMSProp算法和AdaGrad算法的不同在于，RMSProp算法使用了小批量随机梯度按元素平方的指数加权移动平均来调整学习率。</p>
<h1 id="ada-delta-suan-fa">AdaDelta算法</h1>
<p>除了RMSProp算法以外，另一个常用优化算法AdaDelta算法也针对AdaGrad算法在迭代后期可能较难找到有用解的问题做了改进 。有意思的是，<strong>AdaDelta算法没有学习率这一超参数</strong>。</p>
<h2 id="suan-fa-2">算法</h2>
<p>AdaDelta算法也像RMSProp算法一样，使用了小批量随机梯度\(\boldsymbol{g}_t\)按元素平方的指数加权移动平均变量\(\boldsymbol{s}_t\)。在时间步0，它的所有元素被初始化为0。给定超参数\(0 \leq \rho &lt; 1\)（对应RMSProp算法中的\(\gamma\)），在时间步\(t>0\)，同RMSProp算法一样计算</p>
<p>\[
\boldsymbol{s}_t \leftarrow \rho \boldsymbol{s}_{t-1} + (1 - \rho) \boldsymbol{g}_t \odot \boldsymbol{g}_t
\]</p>
<p>与RMSProp算法不同的是，AdaDelta算法还维护一个额外的状态变量\(\Delta\boldsymbol{x}_t\)，其元素同样在时间步0时被初始化为0。使用\(\Delta\boldsymbol{x}_{t-1}\)来计算自变量的变化量：</p>
<p>\[
\boldsymbol{g}_t' \leftarrow \sqrt{\frac{\Delta\boldsymbol{x}_{t-1} + \epsilon}{\boldsymbol{s}_t + \epsilon}}   \odot \boldsymbol{g}_t
\]</p>
<p>其中\(\epsilon\)是为了维持数值稳定性而添加的常数，如\(10^{-5}\)。接着更新自变量：</p>
<p>\[
\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}'_t
\]</p>
<p>最后，使用\(\Delta\boldsymbol{x}_t\)来记录自变量变化量\(\boldsymbol{g}'_t\)按元素平方的指数加权移动平均：</p>
<p>\[
\Delta\boldsymbol{x}_t \leftarrow \rho \Delta\boldsymbol{x}_{t-1} + (1 - \rho) \boldsymbol{g}'_t \odot \boldsymbol{g}'_t
\]</p>
<p>可以看到，如不考虑\(\epsilon\)的影响，<strong>AdaDelta算法跟RMSProp算法的不同之处在于使用\(\sqrt{\Delta\boldsymbol{x}_{t-1}}\)来替代学习率\(\eta\)</strong>。</p>
<h2 id="dai-ma-shi-xian-4">代码实现</h2>
<p>AdaDelta算法需要对每个自变量维护两个状态变量，即\(\boldsymbol{s}_t\)和\(\Delta\boldsymbol{x}_t\)。我们按AdaDelta算法中的公式实现该算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adadelta_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w, s_b = np.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=float), np.zeros(<span class="number">1</span>, dtype=float)</span><br><span class="line">    delta_w, delta_b = np.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=float), np.zeros(<span class="number">1</span>, dtype=float)</span><br><span class="line">    <span class="keyword">return</span> ((s_w, delta_w), (s_b, delta_b))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adadelta</span><span class="params">(params, states, hyperparams,grads)</span>:</span></span><br><span class="line">    rho, eps,i = hyperparams[<span class="string">'rho'</span>], <span class="number">1e-5</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p, (s, delta) <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s[:] = rho * s + (<span class="number">1</span> - rho) * (grads[i]**<span class="number">2</span>)</span><br><span class="line">        g =  grads[i] * np.sqrt((delta + eps) / (s + eps))</span><br><span class="line">        p.assign_sub(g)</span><br><span class="line">        delta[:] = rho * delta + (<span class="number">1</span> - rho) * g * g</span><br><span class="line">        i+=<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>使用超参数\(\rho=0.9\)来训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(adadelta, init_adadelta_states(), &#123;<span class="string">'rho'</span>: <span class="number">0.9</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.7_output1.png" alt></p>
<p>通过名称为<code>Adadelta</code>的优化器方法，便可使用Tensorflow2提供的AdaDelta算法。它的超参数可以通过<code>rho</code>来指定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line">trainer = keras.optimizers.Adadelta(learning_rate=<span class="number">0.01</span>,rho=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.7_output2.png" alt></p>
<p>AdaDelta算法没有学习率超参数，它通过使用有关自变量更新量平方的指数加权移动平均的项来替代RMSProp算法中的学习率。</p>
<h1 id="adam-suan-fa">Adam算法</h1>
<p>Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均 。</p>
<blockquote>
<p>所以Adam算法可以看做是RMSProp算法与动量法的结合。</p>
</blockquote>
<h2 id="suan-fa-3">算法</h2>
<p>Adam算法使用了动量变量\(\boldsymbol{v}_t\)和RMSProp算法中小批量随机梯度按元素平方的指数加权移动平均变量\(\boldsymbol{s}_t\)，并在时间步0将它们中每个元素初始化为0。给定超参数\(0 \leq \beta_1 &lt; 1\)（算法作者建议设为0.9），时间步\(t\)的动量变量\(\boldsymbol{v}_t\)即小批量随机梯度\(\boldsymbol{g}_t\)的指数加权移动平均：</p>
<p>\[
\boldsymbol{v}_t \leftarrow \beta_1 \boldsymbol{v}_{t-1} + (1 - \beta_1) \boldsymbol{g}_t
\]</p>
<p>和RMSProp算法中一样，给定超参数\(0 \leq \beta_2 &lt; 1\)（算法作者建议设为0.999），<br>
将小批量随机梯度按元素平方后的项\(\boldsymbol{g}_t \odot \boldsymbol{g}_t\)做指数加权移动平均得到\(\boldsymbol{s}_t\)：</p>
<p>\[
\boldsymbol{s}_t \leftarrow \beta_2 \boldsymbol{s}_{t-1} + (1 - \beta_2) \boldsymbol{g}_t \odot \boldsymbol{g}_t
\]</p>
<p>由于将\(\boldsymbol{v}_0\)和\(\boldsymbol{s}_0\)中的元素都初始化为0，在时间步\(t\)得到\(\boldsymbol{v}_t =  (1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} \boldsymbol{g}_i\)。将过去各时间步小批量随机梯度的权值相加，得到 \((1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} = 1 - \beta_1^t\)。需要注意的是，当\(t\)较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当\(\beta_1 = 0.9\)时，\(\boldsymbol{v}_1 = 0.1\boldsymbol{g}_1\)。为了消除这样的影响，对于任意时间步\(t\)，可以将\(\boldsymbol{v}_t\)再除以\(1 - \beta_1^t\)，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作偏差修正。在Adam算法中，对变量\(\boldsymbol{v}_t\)和\(\boldsymbol{s}_t\)均作偏差修正：</p>
<p>\[
\hat{\boldsymbol{v}}_t \leftarrow \frac{\boldsymbol{v}_t}{1 - \beta_1^t}
\]</p>
<p>\[
\hat{\boldsymbol{s}}_t \leftarrow \frac{\boldsymbol{s}_t}{1 - \beta_2^t}
\]</p>
<p>接下来，Adam算法使用以上偏差修正后的变量\(\hat{\boldsymbol{v}}_t\)和\(\hat{\boldsymbol{s}}_t\)，将模型参数中每个元素的学习率通过按元素运算重新调整：</p>
<p>\[
\boldsymbol{g}_t' \leftarrow \frac{\eta \hat{\boldsymbol{v}}_t}{\sqrt{\hat{\boldsymbol{s}}_t} + \epsilon}
\]</p>
<p>其中\(\eta\)是学习率，\(\epsilon\)是为了维持数值稳定性而添加的常数，如\(10^{-8}\)。和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用\(\boldsymbol{g}_t'\)迭代自变量：</p>
<p>\[
\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}_t'
\]</p>
<h2 id="dai-ma-shi-xian-5">代码实现</h2>
<p>按照Adam算法中的公式实现该算法。其中时间步\(t\)通过<code>hyperparams</code>参数传入<code>adam</code>函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adam_states</span><span class="params">()</span>:</span></span><br><span class="line">    v_w, v_b = np.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=float), np.zeros(<span class="number">1</span>, dtype=float)</span><br><span class="line">    s_w, s_b = np.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=float), np.zeros(<span class="number">1</span>, dtype=float)</span><br><span class="line">    <span class="keyword">return</span> ((v_w, s_w), (v_b, s_b))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span><span class="params">(params, states, hyperparams, grads)</span>:</span></span><br><span class="line">    beta1, beta2, eps, i = <span class="number">0.9</span>, <span class="number">0.999</span>, <span class="number">1e-6</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p, (v, s) <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        v[:] = beta1 * v + (<span class="number">1</span> - beta1) * grads[i]</span><br><span class="line">        s[:] = beta2 * s + (<span class="number">1</span> - beta2) * grads[i]**<span class="number">2</span></span><br><span class="line">        v_bias_corr = v / (<span class="number">1</span> - beta1 ** hyperparams[<span class="string">'t'</span>])</span><br><span class="line">        s_bias_corr = s / (<span class="number">1</span> - beta2 ** hyperparams[<span class="string">'t'</span>])</span><br><span class="line">        p.assign_sub(hyperparams[<span class="string">'lr'</span>]*v_bias_corr/(np.sqrt(s_bias_corr) + eps))</span><br><span class="line">        i+=<span class="number">1</span></span><br><span class="line">    hyperparams[<span class="string">'t'</span>] += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>使用学习率为0.01的Adam算法来训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(adam, init_adam_states(), &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>, <span class="string">'t'</span>: <span class="number">1</span>&#125;, features,labels)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.8_output1.png" alt></p>
<p>通过名称为“adam”的<code>Trainer</code>实例，我们便可使用Tensorflow2提供的Adam算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line">trainer = keras.optimizers.Adam(learning_rate=<span class="number">0.01</span>)</span><br><span class="line">d2l.train_tensorflow2_ch7(trainer, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.01</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.8_output2.png" alt></p>
<p>Adam算法在RMSProp算法的基础上对小批量随机梯度也做了指数加权移动平均。<br>
Adam算法使用了偏差修正。</p>

    </div>

    
    
    <div>
  
    <div style="text-align:center;color:#bfbfbf;font-size:16px;">
      <span>-------- 本文结束 </span>
      <i class="fa fa-coffee"></i>
      <span> 感谢阅读 --------</span>
    </div>
  
</div>
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    鼓励一下
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat.png" alt="Li Zhen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/ali.png" alt="Li Zhen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/optimization/" rel="tag"><i class="fa fa-tag"></i> optimization</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/05/27/deeplearning/computer_vision/" rel="prev" title="计算机视觉基础">
      <i class="fa fa-chevron-left"></i> 计算机视觉基础
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/07/19/requests/" rel="next" title="requests">
      requests <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#you-hua-yu-shen-du-xue-xi"><span class="nav-number">1.</span> <span class="nav-text">优化与深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#you-hua-yu-shen-du-xue-xi-de-guan-xi"><span class="nav-number">1.1.</span> <span class="nav-text">优化与深度学习的关系</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#you-hua-zai-shen-du-xue-xi-zhong-de-tiao-zhan"><span class="nav-number">1.2.</span> <span class="nav-text">优化在深度学习中的挑战</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ju-bu-zui-xiao-zhi"><span class="nav-number">1.2.1.</span> <span class="nav-text">局部最小值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#an-dian"><span class="nav-number">1.2.2.</span> <span class="nav-text">鞍点</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ti-du-xia-jiang-he-sui-ji-ti-du-xia-jiang"><span class="nav-number">2.</span> <span class="nav-text">梯度下降和随机梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#yi-wei-ti-du-xia-jiang"><span class="nav-number">2.1.</span> <span class="nav-text">一维梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xue-xi-lu"><span class="nav-number">2.2.</span> <span class="nav-text">学习率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#duo-wei-ti-du-xia-jiang"><span class="nav-number">2.3.</span> <span class="nav-text">多维梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sui-ji-ti-du-xia-jiang"><span class="nav-number">2.4.</span> <span class="nav-text">随机梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xiao-pi-liang-sui-ji-ti-du-xia-jiang"><span class="nav-number">2.5.</span> <span class="nav-text">小批量随机梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dai-ma-shi-xian"><span class="nav-number">2.5.1.</span> <span class="nav-text">代码实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dong-liang-fa"><span class="nav-number">3.</span> <span class="nav-text">动量法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ti-du-xia-jiang-de-wen-ti"><span class="nav-number">3.1.</span> <span class="nav-text">梯度下降的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dong-liang-fa-1"><span class="nav-number">3.2.</span> <span class="nav-text">动量法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#zhi-shu-jia-quan-yi-dong-ping-jun"><span class="nav-number">3.2.1.</span> <span class="nav-text">指数加权移动平均</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#you-zhi-shu-jia-quan-yi-dong-ping-jun-li-jie-dong-liang-fa"><span class="nav-number">3.2.2.</span> <span class="nav-text">由指数加权移动平均理解动量法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dai-ma-shi-xian-1"><span class="nav-number">3.3.</span> <span class="nav-text">代码实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ada-grad-suan-fa"><span class="nav-number">4.</span> <span class="nav-text">AdaGrad算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#suan-fa"><span class="nav-number">4.1.</span> <span class="nav-text">算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#te-dian"><span class="nav-number">4.2.</span> <span class="nav-text">特点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dai-ma-shi-xian-2"><span class="nav-number">4.3.</span> <span class="nav-text">代码实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#rms-prop-suan-fa"><span class="nav-number">5.</span> <span class="nav-text">RMSProp算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#suan-fa-1"><span class="nav-number">5.1.</span> <span class="nav-text">算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dai-ma-shi-xian-3"><span class="nav-number">5.2.</span> <span class="nav-text">代码实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ada-delta-suan-fa"><span class="nav-number">6.</span> <span class="nav-text">AdaDelta算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#suan-fa-2"><span class="nav-number">6.1.</span> <span class="nav-text">算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dai-ma-shi-xian-4"><span class="nav-number">6.2.</span> <span class="nav-text">代码实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#adam-suan-fa"><span class="nav-number">7.</span> <span class="nav-text">Adam算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#suan-fa-3"><span class="nav-number">7.1.</span> <span class="nav-text">算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dai-ma-shi-xian-5"><span class="nav-number">7.2.</span> <span class="nav-text">代码实现</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <a href='/'>
    <img class="site-author-image" itemprop="image" alt="Li Zhen"
      src="/images/snoppy.jpeg">
  </a>
  <p class="site-author-name" itemprop="name">Li Zhen</p>
  <div class="site-description" itemprop="description">他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">89</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">67</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jeffery0628" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jeffery0628" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jeffery.lee.0628@gmail.com" title="邮箱 → mailto:jeffery.lee.0628@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮箱</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Zhen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">1.1m</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  






  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>


 

<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180101,"YYYYMMDD"));
    ages = ages.replace(/years?/, "年");
    ages = ages.replace(/months?/, "月");
    ages = ages.replace(/days?/, "天");
    ages = ages.replace(/hours?/, "小时");
    ages = ages.replace(/minutes?/, "分");
    ages = ages.replace(/seconds?/, "秒");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `我已在此等候你 ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


<script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>

<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'tChTbEIggDSVcdhPBUf0MFxW-gzGzoHsz',
      appKey     : 'sJ6xUiFnifEDIIfnj3Ut9zy1',
      placeholder: "留下你的小脚印吧！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '8' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
