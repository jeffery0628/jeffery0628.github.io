<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/snoppy.jpeg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open Sans:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-big-counter.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jeffery.ink","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":270,"display":"hide","padding":15,"offset":12,"onmobile":true,"dimmer":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":true,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":true,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="卷积神经网络">
<meta property="og:url" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/index.html">
<meta property="og:site_name" content="火种2号">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/5.1_correlation.svg">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/5.1_correlation-4944107.svg">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/5.2_conv_pad.svg">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/5.2_conv_stride.svg">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/5.3_conv_multi_in.svg">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/5.3_conv_1x1.svg">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/5.4_pooling.svg">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/2019-8-26_9-35-48.png">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/2019-8-26_9-38-34.png">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/2019-8-26_9-52-37.png">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/1_1TI1aGBZ4dybR6__DI9dzA.png">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/1_qyc21qM0oxWEuRaj-XJKcw.png">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/14596362-cf346be06be6a51c.jpg.png">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/2.png">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/5.8_nin-5032783.svg">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/v2-406a9c2c203abc36d55af1bdbb7002f7_r.jpg">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/5.9_inception-5033243.svg">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/5.11_residual-block.svg">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/5.12_densenet.svg">
<meta property="og:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/v2-acd8346dc4de74caac921b2b88088f1c_1440w.jpg">
<meta property="article:published_time" content="2019-04-16T23:30:24.000Z">
<meta property="article:modified_time" content="2020-08-22T05:32:01.449Z">
<meta property="article:author" content="Li Zhen">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jeffery.ink/2019/04/17/deeplearning/cnn/5.1_correlation.svg">

<link rel="canonical" href="https://jeffery.ink/2019/04/17/deeplearning/cnn/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>卷积神经网络 | 火种2号</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">火种2号</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">火种计划</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>简历</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>读书</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-video fa-fw"></i>电影</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>



</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jeffery.ink/2019/04/17/deeplearning/cnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/snoppy.jpeg">
      <meta itemprop="name" content="Li Zhen">
      <meta itemprop="description" content="他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="火种2号">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          卷积神经网络
        </h1>

        <div class="post-meta">
          
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-08-22 13:32:01" itemprop="dateModified" datetime="2020-08-22T13:32:01+08:00">2020-08-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">技术</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          
            <span id="/2019/04/17/deeplearning/cnn/" class="post-meta-item leancloud_visitors" data-flag-title="卷积神经网络" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论：</span>
    
    <a title="valine" href="/2019/04/17/deeplearning/cnn/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/04/17/deeplearning/cnn/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>33k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>30 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/2019/04/17/deeplearning/cnn/5.1_correlation.svg" alt></p>
<a id="more"></a>
<h1 id="er-wei-juan-ji-ceng">二维卷积层</h1>
<p>卷积神经网络是含有卷积层的神经网络，它有高和宽两个空间维度，常用来处理图像数据。</p>
<h2 id="two-dimentional-cross-correlation">two dimentional cross-correlation</h2>
<p>虽然卷积层得名于卷积运算，但我们通常在卷积层中使用更加直观的互相关运算。在二维卷积层中，一个二维输入数组和一个二维核数组通过互相关运算输出一个二维数组。<br>
用一个具体例子来解释二维互相关运算的含义。如下图所示，输入是一个高和宽均为3的二维数组。我们将该数组的形状记为\(3 \times 3\)。核数组的高和宽分别为2。该数组在卷积计算中又称卷积核或过滤器。卷积核窗口的形状取决于卷积核的高和宽，即\(2 \times 2\)。下图中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：\(0\times0+1\times1+3\times2+4\times3=19\)。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.1_correlation-4944107.svg" alt="二维互相关运算"></p>
<p>在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。图中的输出数组高和宽分别为2，其中的4个元素由二维互相关运算得出：</p>
<p>\[
0\times0+1\times1+3\times2+4\times3=19,\\
1\times0+2\times1+4\times2+5\times3=25,\\
3\times0+4\times1+6\times2+7\times3=37,\\
4\times0+5\times1+7\times2+8\times3=43.\\
\]</p>
<p>下面将上述过程实现在<code>corr2d</code>函数里。它接受输入数组<code>X</code>与核数组<code>K</code>，并输出数组<code>Y</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = tf.Variable(tf.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w +<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i,j].assign(tf.cast(tf.reduce_sum(X[i:i+h, j:j+w] * K), dtype=tf.float32))</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<h2 id="conv-2-d">Conv2d</h2>
<p>二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。</p>
<p>下面基于<code>corr2d</code>函数来实现一个自定义的二维卷积层。在构造函数<code>__init__</code>里声明<code>weight</code>和<code>bias</code>这两个模型参数。前向计算函数<code>forward</code>则是直接调用<code>corr2d</code>函数再加上偏差。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.units = units</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, kernel_size)</span>:</span></span><br><span class="line">        self.w = self.add_weight(name=<span class="string">'w'</span>,</span><br><span class="line">                                shape=kernel_size,</span><br><span class="line">                                initializer=tf.random_normal_initializer())</span><br><span class="line">        self.b = self.add_weight(name=<span class="string">'b'</span>,</span><br><span class="line">                                shape=(<span class="number">1</span>,),</span><br><span class="line">                                initializer=tf.random_normal_initializer())</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(inputs, self.w) + self.b</span><br></pre></td></tr></table></figure>
<p>卷积窗口形状为\(p \times q\)的卷积层称为\(p \times q\)卷积层。同样，\(p \times q\)卷积或\(p \times q\)卷积核说明卷积核的高和宽分别为\(p\)和\(q\)。</p>
<h2 id="hu-xiang-guan-yun-suan-he-juan-ji-yun-suan">互相关运算和卷积运算</h2>
<p>实际上，卷积运算与互相关运算类似。为了得到卷积运算的输出，只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算。可见，卷积运算和互相关运算虽然类似，但如果它们使用相同的核数组，对于同一个输入，输出往往并不相同。</p>
<p>那么，卷积层为何能使用互相关运算替代卷积运算？</p>
<p>在深度学习中核数组都是学出来的：卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出。为了解释这一点，假设卷积层使用互相关运算学出上图的核数组。设其他条件不变，使用卷积运算学出的核数组即上图中的核数组按上下、左右翻转。也就是说，图中的输入与学出的已翻转的核数组再做卷积运算时，依然得到图中的输出。</p>
<h2 id="te-zheng-tu-he-gan-shou-ye">特征图和感受野</h2>
<p>二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图。影响元素\(x\)的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做\(x\)的感受野。以上图为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图中形状为\(2 \times 2\)的输出记为\(Y\)，并考虑一个更深的卷积神经网络：将\(Y\)与另一个形状为\(2 \times 2\)的核数组做互相关运算，输出单个元素\(z\)。那么，\(z\)在\(Y\)上的感受野包括\(Y\)的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。</p>
<h2 id="tian-chong-he-bu-fu">填充和步幅</h2>
<p>一般来说，假设输入形状是\(n_h\times n_w\)，卷积核窗口形状是\(k_h\times k_w\)，那么输出形状将会是</p>
<p>\[
(n_h-k_h+1) \times (n_w-k_w+1)
\]</p>
<p>所以卷积层的输出形状由输入形状和卷积核窗口形状决定。而填充和步幅可以对给定形状的输入和卷积核改变输出形状。</p>
<h3 id="padding">padding</h3>
<p>填充是指在输入高和宽的两侧填充元素（通常是0元素）。如下图所示，在原输入高和宽的两侧分别添加了值为0的元素，使得输入高和宽从3变成了5，并导致输出高和宽由2增加到4。图5.2中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：\(0\times0+0\times1+0\times2+0\times3=0\)。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.2_conv_pad.svg" alt="在输入的高和宽两侧分别填充了0元素的二维互相关计算"></p>
<p>一般来说，如果在高的两侧一共填充\(p_h\)行，在宽的两侧一共填充\(p_w\)列，那么输出形状将会是</p>
<p>\[
(n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1)
\]</p>
<p>也就是说，输出的高和宽会分别增加\(p_h\)和\(p_w\)。</p>
<p>在很多情况下，我们会设置\(p_h=k_h-1\)和\(p_w=k_w-1\)来使输入和输出具有相同的高和宽。这样会方便在构造网络时推测每个层的输出形状。假设这里\(k_h\)是奇数，我们会在高的两侧分别填充\(p_h/2\)行。如果\(k_h\)是偶数，一种可能是在输入的顶端一侧填充\(\lceil p_h/2\rceil\)行，而在底端一侧填充\(\lfloor p_h/2\rfloor\)行。在宽的两侧填充同理。</p>
<p>卷积神经网络经常使用奇数高宽的卷积核，如1、3、5和7，所以两端上的填充个数相等。对任意的二维数组<code>X</code>，设它的第<code>i</code>行第<code>j</code>列的元素为<code>X[i,j]</code>。当两端上的填充个数相等，并使输入和输出具有相同的高和宽时，就可以知道输出<code>Y[i,j]</code>是由输入以<code>X[i,j]</code>为中心的窗口同卷积核进行互相关计算得到的。</p>
<h3 id="stride">stride</h3>
<p>卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。每次滑动的行数和列数称为步幅。</p>
<p>以使用更大步幅。如下图展示了在高上步幅为3、在宽上步幅为2的二维互相关运算。可以看到，输出第一列第二个元素时，卷积窗口向下滑动了3行，而在输出第一行第二个元素时卷积窗口向右滑动了2列。当卷积窗口在输入上再向右滑动2列时，由于输入元素无法填满窗口，无结果输出。图5.3中的阴影部分为输出元素及其计算所使用的输入和核数组元素：\(0\times0+0\times1+1\times2+2\times3=8\)、\(0\times0+6\times1+0\times2+0\times3=6\)。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.2_conv_stride.svg" alt="高和宽上步幅分别为3和2的二维互相关运算"></p>
<p>一般来说，当高上步幅为\(s_h\)，宽上步幅为\(s_w\)时，输出形状为</p>
<p>\[\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.\]</p>
<p>如果设置\(p_h=k_h-1\)和\(p_w=k_w-1\)，那么输出形状将简化为\(\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor\)。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是\((n_h/s_h) \times (n_w/s_w)\)。</p>
<h2 id="duo-shu-ru-tong-dao-he-duo-shu-chu-tong-dao">多输入通道和多输出通道</h2>
<p>真实数据的维度通常很高。如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是\(h\)和\(w\)（像素），那么它可以表示为一个\(3\times h\times w\)的多维数组。我们将大小为3的这一维称为通道维。</p>
<h3 id="muti-channels-in">muti-channels in</h3>
<p>当输入数据含多个通道时，需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。假设输入数据的通道数为\(c_i\)，那么卷积核的输入通道数同样为\(c_i\)。设卷积核窗口形状为\(k_h\times k_w\)。当\(c_i=1\)时，我们知道卷积核只包含一个形状为\(k_h\times k_w\)的二维数组。当\(c_i > 1\)时，我们将会为每个输入通道各分配一个形状为\(k_h\times k_w\)的核数组。把这\(c_i\)个数组在输入通道维上连结，即得到一个形状为\(c_i\times k_h\times k_w\)的卷积核。由于输入和卷积核各有\(c_i\)个通道，可以在各个通道上对输入的二维数组和卷积核的二维核数组做互相关运算，再将这\(c_i\)个互相关运算的二维输出按通道相加，得到一个二维数组。这就是含多个通道的输入数据与多输入通道的卷积核做二维互相关运算的输出。</p>
<p>下图展示了含2个输入通道的二维互相关计算的例子。在每个通道上，二维输入数组与二维核数组做互相关运算，再按通道相加即得到输出。下图中阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：\((1\times1+2\times2+4\times3+5\times4)+(0\times0+1\times1+3\times2+4\times3)=56\)。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.3_conv_multi_in.svg" alt="含2个输入通道的互相关计算"></p>
<p>接下来实现含多个输入通道的互相关运算。只需要对每个通道做互相关运算，然后进行累加。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d_multi_in</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_sum([corr2d(X[i], K[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">0</span>])],axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="multi-channels-out">multi-channels out</h3>
<p>当输入通道有多个时，因为对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为1。设卷积核输入通道数和输出通道数分别为\(c_i\)和\(c_o\)，高和宽分别为\(k_h\)和\(k_w\)。如果希望得到含多个通道的输出，可以为每个输出通道分别创建形状为\(c_i\times k_h\times k_w\)的核数组。将它们在输出通道维上连结，卷积核的形状即\(c_o\times c_i\times k_h\times k_w\)。在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与整个输入数组计算而来。</p>
<p>下面我们实现一个互相关运算函数来计算多个通道的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d_multi_in_out</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K],axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="convolution">convolution</h3>
<p>卷积窗口形状为\(1\times 1\)（\(k_h=k_w=1\)）的多通道卷积层。通常称之为\(1\times 1\)卷积层，并将其中的卷积运算称为\(1\times 1\)卷积。因为使用了最小窗口，\(1\times 1\)卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，\(1\times 1\)卷积的主要计算发生在通道维上。展示了使用输入通道数为3、输出通道数为2的\(1\times 1\)卷积核的互相关计算。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么\(1\times 1\)卷积层的作用与全连接层等价。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.3_conv_1x1.svg" alt="使用输入通道数为3、输出通道数为2的1 x 1卷积核的互相关计算。输入和输出具有相同的高和宽"></p>
<p>使用全连接层中的矩阵乘法来实现\(1\times 1\)卷积。这里需要在矩阵乘法运算前后对数据形状做一些调整。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d_multi_in_out_1x1</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line">    X = tf.reshape(X,(c_i, h * w))</span><br><span class="line">    K = tf.reshape(K,(c_o, c_i))</span><br><span class="line">    Y = tf.matmul(K, X)</span><br><span class="line">    <span class="keyword">return</span> tf.reshape(Y, (c_o, h, w))</span><br></pre></td></tr></table></figure>
<p>在之后的模型里我们将会看到\(1\times 1\)卷积层被当作保持高和宽维度形状不变的全连接层使用。于是，可以通过调整网络层之间的通道数来控制模型复杂度。</p>
<h3 id="xiao-jie">小结</h3>
<ol>
<li>二维卷积层的核心计算是二维互相关运算。在最简单的形式下，它对二维输入数据和卷积核做互相关运算然后加上偏差。</li>
<li>可以设计卷积核来检测图像中的边缘。</li>
<li>可以通过数据来学习卷积核。</li>
<li>使用多通道可以拓展卷积层的模型参数。</li>
<li>假设将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么\(1\times 1\)卷积层的作用与全连接层等价。</li>
<li>\(1\times 1\)卷积层通常用来调整网络层之间的通道数，并控制模型复杂度。</li>
</ol>
<h1 id="juan-ji-he-de-xuan-ze">卷积核的选择</h1>
<p>大卷积核虽然可以获取更大的感受域，但是大卷积核反而会导致计算量大幅增加，不利于训练更深层的模型，相应的计算能力也会降低。</p>
<p>堆叠2个 <strong>3×3</strong> 卷积核（二通道）可以获得与 <strong>5×5卷积核</strong> 相同的感受视野，同时参数量会更少（3×3×2+1&lt;5×5×1+1）,这也是 <strong>3×3卷积</strong> 应用更广泛的原因，在大多数情况下通过堆叠较小的卷积核比直接采用单个更大的卷积核会更加有效。</p>
<h1 id="chi-hua-ceng">池化层</h1>
<p>在图像物体边缘检测应用中，构造卷积核从而精确地找到了像素变化的位置。设任意二维数组<code>X</code>的<code>i</code>行<code>j</code>列的元素为<code>X[i, j]</code>。如果我们构造的卷积核输出<code>Y[i, j]=1</code>，那么说明输入中<code>X[i, j]</code>和<code>X[i, j+1]</code>数值不一样。这可能意味着物体边缘通过这两个元素之间。但实际图像里，我们感兴趣的物体不会总出现在固定位置：即使我们连续拍摄同一个物体也极有可能出现像素位置上的偏移。这会导致同一个边缘对应的输出可能出现在卷积输出<code>Y</code>中的不同位置，进而对后面的模式识别造成不便。</p>
<p>池化层的提出是为了缓解卷积层对位置的过度敏感性。</p>
<h2 id="2-dimentional-max-pooling-and-avg-pooling">2-dimentional max_pooling and avg_pooling</h2>
<p>同卷积层一样，池化层每次对输入数据的一个固定形状窗口中的元素计算输出。不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。在二维最大池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当池化窗口滑动到某一位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.4_pooling.svg" alt="池化窗口形状为2 x 2的最大池化"></p>
<p>上图展示了池化窗口形状为\(2\times 2\)的最大池化，阴影部分为第一个输出元素及其计算所使用的输入元素。输出数组的高和宽分别为2，其中的4个元素由取最大值运算\(\text{max}\)得出：</p>
<p>\[
\max(0,1,3,4)=4,\\
\max(1,2,4,5)=5,\\
\max(3,4,6,7)=7,\\
\max(4,5,7,8)=8.\\
\]</p>
<p>二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。池化窗口形状为\(p \times q\)的池化层称为\(p \times q\)池化层，其中的池化运算叫作\(p \times q\)池化。</p>
<p>在物体边缘检测的例子中，将卷积层的输出作为\(2\times 2\)最大池化的输入。设该卷积层输入是<code>X</code>、池化层输出为<code>Y</code>。无论是<code>X[i, j]</code>和<code>X[i, j+1]</code>值不同，还是<code>X[i, j+1]</code>和<code>X[i, j+2]</code>不同，池化层输出均有<code>Y[i, j]=1</code>。也就是说，使用\(2\times 2\)最大池化层时，只要卷积层识别的模式在高和宽上移动不超过一个元素，我们依然可以将它检测出来。</p>
<p>下面把池化层的前向计算实现在<code>pool2d</code>函数里。它跟二维卷积层里<code>corr2d</code>函数非常类似，唯一的区别在计算输出<code>Y</code>上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool2d</span><span class="params">(X, pool_size, mode=<span class="string">'max'</span>)</span>:</span></span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = tf.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w +<span class="number">1</span>))</span><br><span class="line">    Y = tf.Variable(Y)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">'max'</span>:</span><br><span class="line">                Y[i,j].assign(tf.reduce_max(X[i:i+p_h, j:j+p_w]))</span><br><span class="line">            <span class="keyword">elif</span> mode ==<span class="string">'avg'</span>:</span><br><span class="line">                Y[i,j].assign(tf.reduce_mean(X[i:i+p_h, j:j+p_w]))</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<h2 id="padding-and-stride">padding and stride</h2>
<p>同卷积层一样，池化层也可以在输入的高和宽两侧的填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。下面将通过<code>nn</code>模块里的二维最大池化层MaxPool2D来演示池化层填充和步幅的工作机制。我们先构造一个形状为(1, 1, 4, 4)的输入数据，前两个维度分别是批量和通道。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#tensorflow default data_format == 'channels_last'</span></span><br><span class="line"><span class="comment">#so (1,4,4,1) instead of (1,1,4,4)</span></span><br><span class="line">X = tf.reshape(tf.constant(range(<span class="number">16</span>)), (<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>默认情况下，<code>MaxPool2D</code>实例里步幅和池化窗口形状相同。下面使用形状为(3, 3)的池化窗口，默认获得形状为(3, 3)的步幅。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pool2d = tf.keras.layers.MaxPool2D(pool_size=[<span class="number">3</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#I guess no custom padding settings in keras.layers?</span></span><br><span class="line">pool2d = tf.keras.layers.MaxPool2D(pool_size=[<span class="number">3</span>,<span class="number">3</span>],padding=<span class="string">'same'</span>,strides=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="multi-channels">multi-channels</h2>
<p>在处理多通道输入数据时，池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加。这意味着池化层的输出通道数与输入通道数相等。</p>
<h2 id="xiao-jie-1">小结</h2>
<ol>
<li>最大池化和平均池化分别取池化窗口中输入元素的最大值和平均值作为输出。</li>
<li>池化层的一个主要作用是缓解卷积层对位置的过度敏感性。</li>
<li>可以指定池化层的填充和步幅。</li>
<li>池化层的输出通道数跟输入通道数相同。</li>
</ol>
<h1 id="yi-wei-juan-ji">一维卷积</h1>
<p>与⼆维卷积层⼀样，⼀维卷积层使⽤⼀维的互相关运算。在⼀维互相关运算中，卷积窗口从输⼊数组的最左⽅开始，按从左往右的顺序，依次在输⼊数组上滑动。当卷积窗口滑动到某⼀位置时，窗口中的输⼊⼦数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。如下图所⽰，输⼊是⼀个宽为7的⼀维数组，核数组的宽为2。可以看到输出的宽度为 7 - 2 + 1 = 6，且第⼀个元素是由输⼊的最左边的宽为2的⼦数组与核数组按元素相乘后再相加得到的：0 <em>×</em> 1 + 1 × 2 = 2。</p>
<p><img src="/2019/04/17/deeplearning/cnn/2019-8-26_9-35-48.png" alt></p>
<p>多输⼊通道的⼀维互相关运算也与多输⼊通道的⼆维互相关运算类似：在每个通道上，将核与相应的输⼊做⼀维互相关运算，并将通道之间的结果相加得到输出结果。下图展⽰了含3个输⼊ 通道的⼀维互相关运算，其中阴影部分为第⼀个输出元素及其计算所使⽤的输⼊和核数组元素： 0 <em>×</em> 1 + 1 <em>×</em> 2 + 1 <em>×</em> 3 + 2 <em>×</em> 4 + 2 <em>×</em> (-1) + 3 <em>×</em> (-3) = 2。</p>
<p><img src="/2019/04/17/deeplearning/cnn/2019-8-26_9-38-34.png" alt></p>
<p>由⼆维互相关运算的定义可知，多输⼊通道的⼀维互相关运算可以看作单输⼊通道的⼆维互相关运算。如下图所⽰，也可以将上图中多输⼊通道的⼀维互相关运算以等价的单输⼊通道的⼆维互相关运算呈现。这⾥核的⾼等于输⼊的⾼。下图的阴影部分为第⼀个输出元素及其计算所使⽤的输⼊和核数组元素：2 <em>×</em> (-1) + 3 <em>×</em> (-3) + 1 <em>×</em> 3 + 2 <em>×</em> 4 + 0 <em>×</em> 1 + 1 <em>×</em> 2 = 2。</p>
<p><img src="/2019/04/17/deeplearning/cnn/2019-8-26_9-52-37.png" alt></p>
<h1 id="wei-chi-hua">⼀维池化</h1>
<p>⼀维池化层也叫时序最⼤池化层。textCNN中使⽤的时序最⼤池化（max-over-time pooling）层实际上对应⼀维全局最⼤池化层：假设输⼊包含多个通道，各通道由不同时间步上的数值组成，各通道的输出即该通道所有时间步中最⼤的数值。因此，时序最⼤池化层的输⼊在各个通道上的时间步数可以不同。为提升计算性能，我们常常将不同⻓度的时序样本组成⼀个小批量，并通过在较短序列后附加特殊字符（如0）令批量中各时序样本⻓度相同。当然这些⼈为添加的特殊字符是⽆意义的。由于时序最⼤池化的主要⽬的是抓取时序中最重要的特征，它通常能使模型不受⼈为添加字符的影响。</p>
<h1 id="juan-ji-ceng-yu-chi-hua-ceng-bi-jiao">卷积层与池化层比较</h1>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">卷积层</th>
<th style="text-align:center">池化层</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">结构</td>
<td style="text-align:center">零填充时输出维度不变，而通道数改变</td>
<td style="text-align:center">通常特征维度会降低，通道数不变</td>
</tr>
<tr>
<td style="text-align:center">稳定性</td>
<td style="text-align:center">输入特征发生细微改变时，输出结果会改变</td>
<td style="text-align:center">感受域内的细微变化不影响输出结果</td>
</tr>
<tr>
<td style="text-align:center">作用</td>
<td style="text-align:center">感受域内提取局部关联特征</td>
<td style="text-align:center">感受域内提取泛化特征，降低维度</td>
</tr>
<tr>
<td style="text-align:center">参数量</td>
<td style="text-align:center">与卷积核尺寸、卷积核个数相关</td>
<td style="text-align:center">不引入额外参数</td>
</tr>
</tbody>
</table>
<h1 id="juan-ji-shen-jing-wang-luo">卷积神经网络</h1>
<h2 id="le-net-juan-ji-shen-jing-wang-luo">LeNet:卷积神经网络</h2>
<p>卷积层尝试解决这两个问题。</p>
<ol>
<li>一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；</li>
<li>另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</li>
</ol>
<p>卷积神经网络就是含卷积层的网络。早期用来识别手写数字图像的卷积神经网络：LeNet [1]。这个名字来源于LeNet论文的第一作者Yann LeCun。LeNet展示了通过梯度下降训练卷积神经网络可以达到手写数字识别在当时最先进的结果。这个奠基性的工作第一次将卷积神经网络推上舞台，为世人所知。</p>
<h3 id="le-net-mo-xing">LeNet模型</h3>
<p><img src="/2019/04/17/deeplearning/cnn/1_1TI1aGBZ4dybR6__DI9dzA.png" alt="LeNet"></p>
<p>LeNet分为卷积层块和全连接层块两个部分。</p>
<p>卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用\(5\times 5\)的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为\(2\times 2\)，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。</p>
<p>卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。</p>
<p>通过<code>Sequential</code>类来实现LeNet模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">net = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Conv2D(filters=<span class="number">6</span>,kernel_size=<span class="number">5</span>,activation=<span class="string">'sigmoid'</span>,input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)),</span><br><span class="line">    tf.keras.layers.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(filters=<span class="number">16</span>,kernel_size=<span class="number">5</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">    tf.keras.layers.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">120</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">84</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>,activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>可以看到，在卷积层块中输入的高和宽在逐层减小。卷积层由于使用高和宽均为5的卷积核，从而将高和宽分别减小4，而池化层则将高和宽减半，但通道数则从1增加到16。全连接层则逐层减少输出个数，直到变成图像的类别数10。</p>
<h3 id="xiao-jie-2">小结</h3>
<ol>
<li>卷积神经网络就是含卷积层的网络。</li>
<li>LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。</li>
</ol>
<h2 id="alex-net-shen-du-juan-ji-wang-luo">AlexNet:深度卷积网络</h2>
<p>在LeNet提出后的将近20年里，神经网络一度被其他机器学习方法超越，如支持向量机。虽然LeNet可以在早期的小数据集上取得好的成绩，但是在更大的真实数据集上的表现并不尽如人意。一方面，神经网络计算复杂。虽然20世纪90年代也有过一些针对神经网络的加速硬件，但并没有像之后GPU那样大量普及。因此，训练一个多通道、多层和有大量参数的卷积神经网络在当年很难完成。另一方面，当年研究者还没有大量深入研究参数初始化和非凸优化算法等诸多领域，导致复杂的神经网络的训练通常较困难。</p>
<p>神经网络可以直接基于图像的原始像素进行分类。这种称为端到端（end-to-end）的方法节省了很多中间步骤。然而，在很长一段时间里更流行的是研究者通过勤劳与智慧所设计并生成的手工特征。这类图像分类研究的主要流程是：</p>
<ol>
<li>获取图像数据集；</li>
<li>使用已有的特征提取函数生成图像的特征；</li>
<li>使用机器学习模型对图像的特征分类。</li>
</ol>
<p>当时认为的机器学习部分仅限最后这一步。如果那时候跟机器学习研究者交谈，他们会认为机器学习既重要又优美。优雅的定理证明了许多分类器的性质。机器学习领域生机勃勃、严谨而且极其有用。然而，如果跟计算机视觉研究者交谈，则是另外一幅景象。他们会告诉你图像识别里“不可告人”的现实是：计算机视觉流程中真正重要的是数据和特征。也就是说，使用较干净的数据集和较有效的特征甚至比机器学习模型的选择对图像分类结果的影响更大。</p>
<h3 id="xue-xi-te-zheng-biao-shi">学习特征表示</h3>
<p>既然特征如此重要，它该如何表示呢？</p>
<p>在相当长的时间里，特征都是基于各式各样手工设计的函数从数据中提取的。事实上，不少研究者通过提出新的特征提取函数不断改进图像分类结果。这一度为计算机视觉的发展做出了重要贡献。</p>
<p>然而，另一些研究者则持异议。他们认为特征本身也应该由学习得来。他们还相信，为了表征足够复杂的输入，特征本身应该分级表示。持这一想法的研究者相信，多层神经网络可能可以学得数据的多级表征，并逐级表示越来越抽象的概念或模式。在多层神经网络中，图像的第一级的表示可以是在特定的位置和⻆度是否出现边缘；而第二级的表示说不定能够将这些边缘组合出有趣的模式，如花纹；在第三级的表示中，也许上一级的花纹能进一步汇合成对应物体特定部位的模式。这样逐级表示下去，最终，模型能够较容易根据最后一级的表示完成分类任务。需要强调的是，输入的逐级表示由多层模型中的参数决定，而这些参数都是学出来的。</p>
<p>尽管一直有一群执着的研究者不断钻研，试图学习视觉数据的逐级表征，然而很长一段时间里这些野心都未能实现。这其中有诸多因素值得我们一一分析。</p>
<h4 id="que-shi-yao-su-yi-shu-ju">缺失要素一：数据</h4>
<p>包含许多特征的深度模型需要大量的有标签的数据才能表现得比其他经典方法更好。限于早期计算机有限的存储和90年代有限的研究预算，大部分研究只基于小的公开数据集。例如，不少研究论文基于加州大学欧文分校（UCI）提供的若干个公开数据集，其中许多数据集只有几百至几千张图像。这一状况在2010年前后兴起的大数据浪潮中得到改善。特别是，2009年诞生的ImageNet数据集包含了1,000大类物体，每类有多达数千张不同的图像。这一规模是当时其他公开数据集无法与之相提并论的。ImageNet数据集同时推动计算机视觉和机器学习研究进入新的阶段，使此前的传统方法不再有优势。</p>
<h4 id="que-shi-yao-su-er-ying-jian">缺失要素二：硬件</h4>
<p>深度学习对计算资源要求很高。早期的硬件计算能力有限，这使训练较复杂的神经网络变得很困难。然而，通用GPU的到来改变了这一格局。很久以来，GPU都是为图像处理和计算机游戏设计的，尤其是针对大吞吐量的矩阵和向量乘法从而服务于基本的图形变换。值得庆幸的是，这其中的数学表达与深度网络中的卷积层的表达类似。通用GPU这个概念在2001年开始兴起，涌现出诸如OpenCL和CUDA之类的编程框架。这使得GPU也在2010年前后开始被机器学习社区使用。</p>
<h3 id="alex-net">AlexNet</h3>
<p>2012年，AlexNet横空出世。这个模型的名字来源于论文第一作者的姓名Alex Krizhevsky [2]。AlexNet使用了8层卷积神经网络，并以很大的优势赢得了ImageNet 2012图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。</p>
<p>AlexNet与LeNet的设计理念非常相似，但也有显著的区别。</p>
<p><strong>第一</strong>，与相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。下面来详细描述这些层的设计。</p>
<p>AlexNet第一层中的卷积窗口形状是\(11\times11\)。因为ImageNet中绝大多数图像的高和宽均比MNIST图像的高和宽大10倍以上，ImageNet图像的物体占用更多的像素，所以需要更大的卷积窗口来捕获物体。第二层中的卷积窗口形状减小到\(5\times5\)，之后全采用\(3\times3\)。此外，第一、第二和第五个卷积层之后都使用了窗口形状为\(3\times3\)、步幅为2的最大池化层。而且，AlexNet使用的卷积通道数也大于LeNet中的卷积通道数数十倍。</p>
<p>紧接着最后一个卷积层的是两个输出个数为4096的全连接层。这两个巨大的全连接层带来将近1 GB的模型参数。由于早期显存的限制，最早的AlexNet使用双数据流的设计使一个GPU只需要处理一半模型。幸运的是，显存在过去几年得到了长足的发展，因此通常我们不再需要这样的特别设计了。</p>
<p><strong>第二</strong>，AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。一方面，ReLU激活函数的计算更简单，例如它并没有sigmoid激活函数中的求幂运算。另一方面，ReLU激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度几乎为0，从而造成反向传播无法继续更新部分模型参数；而ReLU激活函数在正区间的梯度恒为1。因此，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，从而令模型无法得到有效训练。</p>
<p><strong>第三</strong>，AlexNet通过丢弃法来控制全连接层的模型复杂度。而LeNet并没有使用丢弃法。</p>
<p><strong>第四</strong>，AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。</p>
<p>下面实现稍微简化过的AlexNet。</p>
<p><img src="/2019/04/17/deeplearning/cnn/1_qyc21qM0oxWEuRaj-XJKcw.png" alt></p>
<p>建议采用GPU进行训练，需要使用tensorflow-gpu-2.0并设置memory_growth:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> gpu <span class="keyword">in</span> tf.config.experimental.list_physical_devices(<span class="string">'GPU'</span>):</span><br><span class="line">    tf.config.experimental.set_memory_growth(gpu, <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">net = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Conv2D(filters=<span class="number">96</span>,kernel_size=<span class="number">11</span>,strides=<span class="number">4</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(filters=<span class="number">256</span>,kernel_size=<span class="number">5</span>,padding=<span class="string">'same'</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(filters=<span class="number">384</span>,kernel_size=<span class="number">3</span>,padding=<span class="string">'same'</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(filters=<span class="number">384</span>,kernel_size=<span class="number">3</span>,padding=<span class="string">'same'</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(filters=<span class="number">256</span>,kernel_size=<span class="number">3</span>,padding=<span class="string">'same'</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">4096</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">4096</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>,activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h3 id="xiao-jie-3">小结</h3>
<ul>
<li>AlexNet跟LeNet结构类似，但使用了更多的卷积层和更大的参数空间来拟合大规模数据集ImageNet。它是浅层神经网络和深度神经网络的分界线。</li>
<li>虽然看上去AlexNet的实现比LeNet的实现也就多了几行代码而已，但这个观念上的转变和真正优秀实验结果的产生令学术界付出了很多年。</li>
</ul>
<h2 id="vgg-shi-yong-zhong-fu-yuan-su-de-wang-luo">VGG:使用重复元素的网络</h2>
<p>AlexNet在LeNet的基础上增加了3个卷积层。但AlexNet作者对它们的卷积窗口、输出通道数和构造顺序均做了大量的调整。虽然AlexNet指明了深度卷积神经网络可以取得出色的结果，但并没有提供简单的规则以指导后来的研究者如何设计新的网络。</p>
<p>VGG的名字来源于论文作者所在的实验室Visual Geometry Group。VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路。</p>
<h3 id="vgg-kuai">VGG块</h3>
<p>VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为\(3\times 3\)的卷积层后接上一个步幅为2、窗口形状为\(2\times 2\)的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用<code>vgg_block</code>函数来实现这个基础的VGG块，它可以指定卷积层的数量<code>num_convs</code>和输出通道数<code>num_channels</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_block</span><span class="params">(num_convs, num_channels)</span>:</span></span><br><span class="line">    blk = tf.keras.models.Sequential()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_convs):</span><br><span class="line">        blk.add(tf.keras.layers.Conv2D(num_channels,kernel_size=<span class="number">3</span>,</span><br><span class="line">                                    padding=<span class="string">'same'</span>,activation=<span class="string">'relu'</span>))</span><br><span class="line">    </span><br><span class="line">    blk.add(tf.keras.layers.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>
<h3 id="vgg-wang-luo">VGG网络</h3>
<p>与AlexNet和LeNet一样，VGG网络由卷积层模块后接全连接层模块构成。卷积层模块串联数个<code>vgg_block</code>，其超参数由变量<code>conv_arch</code>定义。该变量指定了每个VGG块里卷积层个数和输出通道数。全连接模块则跟AlexNet中的一样。</p>
<p><img src="/2019/04/17/deeplearning/cnn/14596362-cf346be06be6a51c.jpg.png" alt="VGG"></p>
<p>现在构造一个VGG网络。它有5个卷积块，前2块使用单卷积层，而后3块使用双卷积层。第一块的输出通道是64，之后每次对输出通道数翻倍，直到变为512。因为这个网络使用了8个卷积层和3个全连接层，所以经常被称为VGG-11。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br></pre></td></tr></table></figure>
<p>下面实现VGG-11:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg</span><span class="params">(conv_arch)</span>:</span></span><br><span class="line">    net = tf.keras.models.Sequential()</span><br><span class="line">    <span class="keyword">for</span> (num_convs, num_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        net.add(vgg_block(num_convs,num_channels))</span><br><span class="line">    net.add(tf.keras.models.Sequential([tf.keras.layers.Flatten(),</span><br><span class="line">             tf.keras.layers.Dense(<span class="number">4096</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">             tf.keras.layers.Dropout(<span class="number">0.5</span>),</span><br><span class="line">             tf.keras.layers.Dense(<span class="number">4096</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">             tf.keras.layers.Dropout(<span class="number">0.5</span>),</span><br><span class="line">             tf.keras.layers.Dense(<span class="number">10</span>,activation=<span class="string">'sigmoid'</span>)]))</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/deeplearning/cnn/2.png" alt></p>
<p>可以看到，每次我们将输入的高和宽减半，直到最终高和宽变成7后传入全连接层。与此同时，输出通道数每次翻倍，直到变成512。因为每个卷积层的窗口大小一样，所以每层的模型参数尺寸和计算复杂度与输入高、输入宽、输入通道数和输出通道数的乘积成正比。VGG这种高和宽减半以及通道翻倍的设计使得多数卷积层都有相同的模型参数尺寸和计算复杂度。</p>
<h3 id="xiao-jie-4">小结</h3>
<ol>
<li>VGG-11通过5个可以重复使用的卷积块来构造网络。根据每块里卷积层个数和输出通道数的不同可以定义出不同的VGG模型。</li>
</ol>
<h2 id="ni-n-wang-luo-zhong-de-wang-luo">NiN:网络中的网络</h2>
<p>LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深。网络中的网络（NiN）。它提出了另外一个思路，即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。</p>
<h3 id="ni-n-kuai">NiN块</h3>
<p>卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维。\(1\times 1\)卷积层可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。因此，NiN使用\(1\times 1\)卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。下图对比了NiN同AlexNet和VGG等网络在结构上的主要区别。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.8_nin-5032783.svg" alt="左图是AlexNet和VGG的网络结构局部，右图是NiN的网络结构局部"></p>
<p>NiN块是NiN中的基础块。它由一个卷积层加两个充当全连接层的\(1\times 1\)卷积层串联而成。其中第一个卷积层的超参数可以自行设置，而第二和第三个卷积层的超参数一般是固定的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nin_block</span><span class="params">(num_channels, kernel_size, strides, padding)</span>:</span></span><br><span class="line">    blk = tf.keras.models.Sequential()</span><br><span class="line">    blk.add(tf.keras.layers.Conv2D(num_channels, kernel_size,</span><br><span class="line">                                   strides=strides, padding=padding, activation=<span class="string">'relu'</span>)) </span><br><span class="line">    blk.add(tf.keras.layers.Conv2D(num_channels, kernel_size=<span class="number">1</span>,activation=<span class="string">'relu'</span>)) </span><br><span class="line">    blk.add(tf.keras.layers.Conv2D(num_channels, kernel_size=<span class="number">1</span>,activation=<span class="string">'relu'</span>))    </span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>
<h3 id="ni-n-mo-xing">NiN模型</h3>
<p>NiN是在AlexNet问世不久后提出的。它们的卷积层设定有类似之处。NiN使用卷积窗口形状分别为\(11\times 11\)、\(5\times 5\)和\(3\times 3\)的卷积层，相应的输出通道数也与AlexNet中的一致。每个NiN块后接一个步幅为2、窗口形状为\(3\times 3\)的最大池化层。</p>
<p>除使用NiN块以外，NiN还有一个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使用了输出通道数等于标签类别数的NiN块，然后使用全局平均池化层对<strong>每个通道</strong>中所有元素求平均并直接用于分类。这里的全局平均池化层即窗口形状等于输入空间维形状的平均池化层。NiN的这个设计的好处是可以显著减小模型参数尺寸，从而缓解过拟合。然而，该设计有时会造成获得有效模型的训练时间的增加。</p>
<p><img src="/2019/04/17/deeplearning/cnn/v2-406a9c2c203abc36d55af1bdbb7002f7_r.jpg" alt="nin"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">net = tf.keras.models.Sequential()</span><br><span class="line">net.add(nin_block(<span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, padding=<span class="string">'valid'</span>))</span><br><span class="line">net.add(tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>))</span><br><span class="line">net.add(nin_block(<span class="number">256</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="string">'same'</span>))</span><br><span class="line">net.add(tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>))</span><br><span class="line">net.add(nin_block(<span class="number">384</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="string">'same'</span>))</span><br><span class="line">net.add(tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>))</span><br><span class="line">net.add(tf.keras.layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">net.add(nin_block(<span class="number">10</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="string">'same'</span>))</span><br><span class="line">net.add(tf.keras.layers.GlobalAveragePooling2D())</span><br><span class="line">net.add(tf.keras.layers.Flatten())</span><br></pre></td></tr></table></figure>
<h3 id="xiao-jie-5">小结</h3>
<ol>
<li>NiN重复使用由卷积层和代替全连接层的\(1\times 1\)卷积层构成的NiN块来构建深层网络。</li>
<li>NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块和全局平均池化层。</li>
<li>NiN的以上设计思想影响了后面一系列卷积神经网络的设计。</li>
</ol>
<h2 id="goog-le-net-han-bing-xing-lian-jie-de-wang-luo">GoogLeNet:含并行连接的网络</h2>
<p>在2014年的ImageNet图像识别挑战赛中，一个名叫GoogLeNet的网络结构大放异彩 。它虽然在名字上向LeNet致敬，但在网络结构上已经很难看到LeNet的影子。GoogLeNet吸收了NiN中网络串联网络的思想，并在此基础上做了很大改进。</p>
<h3 id="inception-kuai">Inception 块</h3>
<p>GoogLeNet中的基础卷积块叫作Inception块，得名于同名电影《盗梦空间》（Inception）。与NiN块相比，这个基础块在结构上更加复杂，如图所示。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.9_inception-5033243.svg" alt="Inception块的结构"></p>
<p>由上图可以看出，Inception块里有4条并行的线路。前3条线路使用窗口大小分别是\(1\times 1\)、\(3\times 3\)和\(5\times 5\)的卷积层来抽取不同空间尺寸下的信息，其中中间2个线路会对输入先做\(1\times 1\)卷积来减少输入通道数，以降低模型复杂度。第四条线路则使用\(3\times 3\)最大池化层，后接\(1\times 1\)卷积层来改变通道数。4条线路都使用了合适的填充来使输入与输出的高和宽一致。最后我们将每条线路的输出在通道维上连结，并输入接下来的层中去。</p>
<p>Inception块中可以自定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,c1, c2, c3, c4)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="comment"># 线路1，单1 x 1卷积层</span></span><br><span class="line">        self.p1_1 = tf.keras.layers.Conv2D(c1, kernel_size=<span class="number">1</span>, activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>)</span><br><span class="line">        <span class="comment"># 线路2，1 x 1卷积层后接3 x 3卷积层</span></span><br><span class="line">        self.p2_1 = tf.keras.layers.Conv2D(c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.p2_2 = tf.keras.layers.Conv2D(c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="string">'same'</span>,</span><br><span class="line">                              activation=<span class="string">'relu'</span>)</span><br><span class="line">        <span class="comment"># 线路3，1 x 1卷积层后接5 x 5卷积层</span></span><br><span class="line">        self.p3_1 = tf.keras.layers.Conv2D(c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.p3_2 = tf.keras.layers.Conv2D(c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="string">'same'</span>,</span><br><span class="line">                              activation=<span class="string">'relu'</span>)</span><br><span class="line">        <span class="comment"># 线路4，3 x 3最大池化层后接1 x 1卷积层</span></span><br><span class="line">        self.p4_1 = tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, padding=<span class="string">'same'</span>, strides=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = tf.keras.layers.Conv2D(c4, kernel_size=<span class="number">1</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        p1 = self.p1_1(x)</span><br><span class="line">        p2 = self.p2_2(self.p2_1(x))</span><br><span class="line">        p3 = self.p3_2(self.p3_1(x))</span><br><span class="line">        p4 = self.p4_2(self.p4_1(x))</span><br><span class="line">        <span class="keyword">return</span> tf.concat([p1, p2, p3, p4], axis=<span class="number">-1</span>)  <span class="comment"># 在通道维上连结输出</span></span><br></pre></td></tr></table></figure>
<h3 id="goog-le-net-mo-xing">GoogLeNet模型</h3>
<p>GoogLeNet跟VGG一样，在主体卷积部分中使用5个模块，每个模块之间使用步幅为2的\(3\times 3\)最大池化层来减小输出高宽。第一模块使用一个64通道的\(7\times 7\)卷积层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b1 = tf.keras.models.Sequential()</span><br><span class="line">b1.add(tf.keras.layers.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">b1.add(tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>))</span><br></pre></td></tr></table></figure>
<p>第二模块使用2个卷积层：首先是64通道的\(1\times 1\)卷积层，然后是将通道增大3倍的\(3\times 3\)卷积层。它对应Inception块中的第二条线路。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b2 = tf.keras.models.Sequential()</span><br><span class="line">b2.add(tf.keras.layers.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">1</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">b2.add(tf.keras.layers.Conv2D(<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">b2.add(tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>))</span><br></pre></td></tr></table></figure>
<p>第三模块串联2个完整的Inception块。第一个Inception块的输出通道数为\(64+128+32+32=256\)，其中4条线路的输出通道数比例为\(64:128:32:32=2:4:1:1\)。其中第二、第三条线路先分别将输入通道数减小至\(96/192=1/2\)和\(16/192=1/12\)后，再接上第二层卷积层。第二个Inception块输出通道数增至\(128+192+96+64=480\)，每条线路的输出通道数之比为\(128:192:96:64 = 4:6:3:2\)。其中第二、第三条线路先分别将输入通道数减小至\(128/256=1/2\)和\(32/256=1/8\)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b3 = tf.keras.models.Sequential()</span><br><span class="line">b3.add(Inception(<span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>))</span><br><span class="line">b3.add(Inception(<span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>))</span><br><span class="line">b3.add(tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>))</span><br></pre></td></tr></table></figure>
<p>第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是\(192+208+48+64=512\)、\(160+224+64+64=512\)、\(128+256+64+64=512\)、\(112+288+64+64=528\)和\(256+320+128+128=832\)。这些线路的通道数分配和第三模块中的类似，首先含\(3\times 3\)卷积层的第二条线路输出最多通道，其次是仅含\(1\times 1\)卷积层的第一条线路，之后是含\(5\times 5\)卷积层的第三条线路和含\(3\times 3\)最大池化层的第四条线路。其中第二、第三条线路都会先按比例减小通道数。这些比例在各个Inception块中都略有不同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">b4 = tf.keras.models.Sequential()</span><br><span class="line">b4.add(Inception(<span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>))</span><br><span class="line">b4.add(Inception(<span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>))</span><br><span class="line">b4.add(Inception(<span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>))</span><br><span class="line">b4.add(Inception(<span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>))</span><br><span class="line">b4.add(Inception(<span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>))</span><br><span class="line">b4.add(tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>))</span><br></pre></td></tr></table></figure>
<p>第五模块有输出通道数为\(256+320+128+128=832\)和\(384+384+128+128=1024\)的两个Inception块。其中每条线路的通道数的分配思路和第三、第四模块中的一致，只是在具体数值上有所不同。需要注意的是，第五模块的后面紧跟输出层，该模块同NiN一样使用全局平均池化层来将每个通道的高和宽变成1。最后我们将输出变成二维数组后接上一个输出个数为标签类别数的全连接层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">b5 = tf.keras.models.Sequential()</span><br><span class="line">b5.add(Inception(<span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>))</span><br><span class="line">b5.add(Inception(<span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>))</span><br><span class="line">b5.add(tf.keras.layers.GlobalAvgPool2D())</span><br><span class="line"></span><br><span class="line">net = tf.keras.models.Sequential([b1, b2, b3, b4, b5, tf.keras.layers.Dense(<span class="number">10</span>)])</span><br></pre></td></tr></table></figure>
<p>GoogLeNet模型的计算复杂，而且不如VGG那样便于修改通道数。</p>
<h3 id="xiao-jie-6">小结</h3>
<ol>
<li>Inception块相当于一个有4条线路的子网络。它通过不同窗口形状的卷积层和最大池化层来并行抽取信息，并使用\(1\times 1\)卷积层减少通道数从而降低模型复杂度。</li>
<li>GoogLeNet将多个设计精细的Inception块和其他层串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。</li>
<li>GoogLeNet和它的后继者们一度是ImageNet上最高效的模型之一：在类似的测试精度下，它们的计算复杂度往往更低。</li>
</ol>
<h2 id="res-net-can-chai-wang-luo">ResNet:残差网络</h2>
<p>先思考一个问题：对神经网络模型添加新的层，充分训练后的模型是否只可能更有效地降低训练误差？理论上，原模型解的空间只是新模型解的空间的子空间。也就是说，如果能将新添加的层训练成恒等映射\(f(x) = x\)，新模型和原模型将同样有效。由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。然而在实践中，添加过多的层后训练误差往往不降反升。即使利用批量归一化带来的数值稳定性使训练深层模型更加容易，该问题仍然存在。针对这一问题，何恺明等人提出了残差网络（ResNet）。它在2015年的ImageNet图像识别挑战赛夺魁，并深刻影响了后来的深度神经网络的设计。</p>
<h3 id="can-chai-kuai">残差块</h3>
<p>让我们聚焦于神经网络局部。如下图所示，设输入为\(\boldsymbol{x}\)。假设希望学出的理想映射为\(f(\boldsymbol{x})\)，从而作为图上方激活函数的输入。左图虚线框中的部分需要直接拟合出该映射\(f(\boldsymbol{x})\)，而右图虚线框中的部分则需要拟合出有关恒等映射的残差映射\(f(\boldsymbol{x})-\boldsymbol{x}\)。残差映射在实际中往往更容易优化。以恒等映射作为希望学出的理想映射\(f(\boldsymbol{x})\)。只需将图中右图虚线框内上方的加权运算（如仿射）的权重和偏差参数学成0，那么\(f(\boldsymbol{x})\)即为恒等映射。实际中，当理想映射\(f(\boldsymbol{x})\)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。右图也是ResNet的基础块，即残差块。在残差块中，输入可通过跨层的数据线路更快地向前传播。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.11_residual-block.svg" alt="普通的网络结构（左）与加入残差连接的网络结构（右）"></p>
<p>ResNet沿用了VGG全\(3\times 3\)卷积层的设计。残差块里首先有2个有相同输出通道数的\(3\times 3\)卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。然后将输入跳过这两个卷积运算后直接加在最后的ReLU激活函数前。这样的设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的\(1\times 1\)卷积层来将输入变换成需要的形状后再做相加运算。</p>
<p>残差块的实现如下。它可以设定输出通道数、是否使用额外的\(1\times 1\)卷积层来修改通道数以及卷积层的步幅。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,activations</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_channels, use_1x1conv=False, strides=<span class="number">1</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Residual, self).__init__(**kwargs)</span><br><span class="line">        self.conv1 = layers.Conv2D(num_channels,</span><br><span class="line">                                   padding=<span class="string">'same'</span>,</span><br><span class="line">                                   kernel_size=<span class="number">3</span>,</span><br><span class="line">                                   strides=strides)</span><br><span class="line">        self.conv2 = layers.Conv2D(num_channels, kernel_size=<span class="number">3</span>,padding=<span class="string">'same'</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = layers.Conv2D(num_channels,</span><br><span class="line">                                       kernel_size=<span class="number">1</span>,</span><br><span class="line">                                       strides=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = layers.BatchNormalization()</span><br><span class="line">        self.bn2 = layers.BatchNormalization()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        Y = activations.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        <span class="keyword">return</span> activations.relu(Y + X)</span><br></pre></td></tr></table></figure>
<h3 id="res-net-mo-xing">ResNet模型</h3>
<p>ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的\(7\times 7\)卷积层后接步幅为2的\(3\times 3\)的最大池化层。不同之处在于ResNet每个卷积层后增加的批量归一化层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net = tf.keras.models.Sequential(</span><br><span class="line">    [layers.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>),</span><br><span class="line">    layers.BatchNormalization(), layers.Activation(<span class="string">'relu'</span>),</span><br><span class="line">    layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>)])</span><br></pre></td></tr></table></figure>
<p>一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResnetBlock</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,num_channels, num_residuals, first_block=False,**kwargs)</span>:</span></span><br><span class="line">        super(ResnetBlock, self).__init__(**kwargs)</span><br><span class="line">        self.listLayers=[]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_residuals):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">                self.listLayers.append(Residual(num_channels, use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.listLayers.append(Residual(num_channels))      </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.listLayers.layers:</span><br><span class="line">            X = layer(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<p>接着为ResNet加入所有残差块。这里每个模块使用两个残差块。</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">class ResNet(tf.keras.Model):</span><br><span class="line">    def __init__(self,num_blocks,**kwargs):</span><br><span class="line">        super(ResNet, self).__init__(**kwargs)</span><br><span class="line">        self.<span class="attribute">conv</span>=layers.Conv2D(64, <span class="attribute">kernel_size</span>=7, <span class="attribute">strides</span>=2, <span class="attribute">padding</span>=<span class="string">'same'</span>)</span><br><span class="line">        self.<span class="attribute">bn</span>=layers.BatchNormalization()</span><br><span class="line">        self.<span class="attribute">relu</span>=layers.Activation('relu')</span><br><span class="line">        self.<span class="attribute">mp</span>=layers.MaxPool2D(pool_size=3, <span class="attribute">strides</span>=2, <span class="attribute">padding</span>=<span class="string">'same'</span>)</span><br><span class="line">        self.<span class="attribute">resnet_block1</span>=ResnetBlock(64,num_blocks[0], <span class="attribute">first_block</span>=<span class="literal">True</span>)</span><br><span class="line">        self.<span class="attribute">resnet_block2</span>=ResnetBlock(128,num_blocks[1])</span><br><span class="line">        self.<span class="attribute">resnet_block3</span>=ResnetBlock(256,num_blocks[2])</span><br><span class="line">        self.<span class="attribute">resnet_block4</span>=ResnetBlock(512,num_blocks[3])</span><br><span class="line">        self.<span class="attribute">gap</span>=layers.GlobalAvgPool2D()</span><br><span class="line">        self.<span class="attribute">fc</span>=layers.Dense(units=10,activation=tf.keras.activations.softmax)</span><br><span class="line"></span><br><span class="line">    def call(self, x):</span><br><span class="line">        <span class="attribute">x</span>=self.conv(x)</span><br><span class="line">        <span class="attribute">x</span>=self.bn(x)</span><br><span class="line">        <span class="attribute">x</span>=self.relu(x)</span><br><span class="line">        <span class="attribute">x</span>=self.mp(x)</span><br><span class="line">        <span class="attribute">x</span>=self.resnet_block1(x)</span><br><span class="line">        <span class="attribute">x</span>=self.resnet_block2(x)</span><br><span class="line">        <span class="attribute">x</span>=self.resnet_block3(x)</span><br><span class="line">        <span class="attribute">x</span>=self.resnet_block4(x)</span><br><span class="line">        <span class="attribute">x</span>=self.gap(x)</span><br><span class="line">        <span class="attribute">x</span>=self.fc(x)</span><br><span class="line">        return x</span><br><span class="line">    </span><br><span class="line"><span class="attribute">mynet</span>=ResNet([2,2,2,2])</span><br></pre></td></tr></table></figure>
<p>最后，与GoogLeNet一样，加入全局平均池化层后接上全连接层输出。</p>
<p>这里每个模块里有4个卷积层（不计算 1×1卷积层），加上最开始的卷积层和最后的全连接层，共计18层。这个模型通常也被称为ResNet-18。通过配置不同的通道数和模块里的残差块数可以得到不同的ResNet模型，例如更深的含152层的ResNet-152。虽然ResNet的主体架构跟GoogLeNet的类似，但ResNet结构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用。</p>
<h3 id="xiao-jie-7">小结</h3>
<ol>
<li>残差块通过跨层的数据通道从而能够训练出有效的深度神经网络。</li>
<li>ResNet深刻影响了后来的深度神经网络的设计。</li>
</ol>
<h2 id="dense-net-chou-mi-lian-jie-wang-luo">DenseNet:稠密连接网络</h2>
<p>ResNet中的跨层连接设计引申出了数个后续工作。DenseNet是其中的一个：稠密连接网络（DenseNet）。 它与ResNet的主要区别如图5.10所示。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.12_densenet.svg" alt="ResNet（左）与DenseNet（右）在跨层连接上的主要区别：使用相加和使用连结"></p>
<p>上图中将部分前后相邻的运算抽象为模块\(A\)和模块\(B\)。与ResNet的主要区别在于，DenseNet里模块\(B\)的输出不是像ResNet那样和模块\(A\)的输出相加，而是在通道维上连结。这样模块\(A\)的输出可以直接传入模块\(B\)后面的层。在这个设计里，模块\(A\)直接跟模块\(B\)后面的所有层连接在了一起。这也是它被称为“稠密连接”的原因。</p>
<p>DenseNet的主要构建模块是稠密块和过渡层。前者定义了输入和输出是如何连结的，后者则用来控制通道数，使之不过大。</p>
<h3 id="chou-mi-kuai">稠密块</h3>
<p>DenseNet使用了ResNet改良版的“批量归一化、激活和卷积”结构，首先在<code>BottleNeck</code>函数里实现这个结构。在前向计算时，将每块的输入和输出在通道维上连结。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BottleNeck</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, growth_rate, drop_rate)</span>:</span></span><br><span class="line">        super(BottleNeck, self).__init__()</span><br><span class="line">        self.bn1 = tf.keras.layers.BatchNormalization()</span><br><span class="line">        self.conv1 = tf.keras.layers.Conv2D(filters=<span class="number">4</span> * growth_rate,</span><br><span class="line">                                            kernel_size=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                                            strides=<span class="number">1</span>,</span><br><span class="line">                                            padding=<span class="string">"same"</span>)</span><br><span class="line">        self.bn2 = tf.keras.layers.BatchNormalization()</span><br><span class="line">        self.conv2 = tf.keras.layers.Conv2D(filters=growth_rate,</span><br><span class="line">                                            kernel_size=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">                                            strides=<span class="number">1</span>,</span><br><span class="line">                                            padding=<span class="string">"same"</span>)</span><br><span class="line">        self.dropout = tf.keras.layers.Dropout(rate=drop_rate)</span><br><span class="line">        </span><br><span class="line">        self.listLayers = [self.bn1,</span><br><span class="line">                           tf.keras.layers.Activation(<span class="string">"relu"</span>),</span><br><span class="line">                           self.conv1,</span><br><span class="line">                           self.bn2,</span><br><span class="line">                           tf.keras.layers.Activation(<span class="string">"relu"</span>),</span><br><span class="line">                           self.conv2,</span><br><span class="line">                           self.dropout]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        y = x</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.listLayers.layers:</span><br><span class="line">            y = layer(y)</span><br><span class="line">        y = tf.keras.layers.concatenate([x,y], axis=<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<p>稠密块由多个<code>BottleNeck</code>组成，每块使用相同的输出通道数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseBlock</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_layers, growth_rate, drop_rate=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        super(DenseBlock, self).__init__()</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.growth_rate = growth_rate</span><br><span class="line">        self.drop_rate = drop_rate</span><br><span class="line">        self.listLayers = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers):</span><br><span class="line">            self.listLayers.append(BottleNeck(growth_rate=self.growth_rate, drop_rate=self.drop_rate))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.listLayers.layers:</span><br><span class="line">            x = layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率（growth rate）。</p>
<h3 id="guo-du-ceng">过渡层</h3>
<p>由于每个稠密块都会带来通道数的增加，使用过多则会带来过于复杂的模型。过渡层用来控制模型复杂度。它通过\(1\times1\)卷积层来减小通道数，并使用步幅为2的平均池化层减半高和宽，从而进一步降低模型复杂度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransitionLayer</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, out_channels)</span>:</span></span><br><span class="line">        super(TransitionLayer, self).__init__()</span><br><span class="line">        self.bn = tf.keras.layers.BatchNormalization()</span><br><span class="line">        self.conv = tf.keras.layers.Conv2D(filters=out_channels,</span><br><span class="line">                                           kernel_size=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                                           strides=<span class="number">1</span>,</span><br><span class="line">                                           padding=<span class="string">"same"</span>)</span><br><span class="line">        self.pool = tf.keras.layers.MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">                                              strides=<span class="number">2</span>,</span><br><span class="line">                                              padding=<span class="string">"same"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = self.bn(inputs)</span><br><span class="line">        x = tf.keras.activations.relu(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="dense-net-mo-xing">DenseNet模型</h3>
<p><img src="/2019/04/17/deeplearning/cnn/v2-acd8346dc4de74caac921b2b88088f1c_1440w.jpg" alt="Densenet"></p>
<p>DenseNet首先使用同ResNet一样的单卷积层和最大池化层。类似于ResNet接下来使用的4个残差块，DenseNet使用的是4个稠密块。同ResNet一样，可以设置每个稠密块使用多少个卷积层。这里设成4，从而与ResNet-18保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。</p>
<p>ResNet里通过步幅为2的残差块在每个模块之间减小高和宽。这里则使用过渡层来减半高和宽，并减半通道数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseNet</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_init_features, growth_rate, block_layers, compression_rate, drop_rate)</span>:</span></span><br><span class="line">        super(DenseNet, self).__init__()</span><br><span class="line">        self.conv = tf.keras.layers.Conv2D(filters=num_init_features,</span><br><span class="line">                                           kernel_size=(<span class="number">7</span>, <span class="number">7</span>),</span><br><span class="line">                                           strides=<span class="number">2</span>,</span><br><span class="line">                                           padding=<span class="string">"same"</span>)</span><br><span class="line">        self.bn = tf.keras.layers.BatchNormalization()</span><br><span class="line">        self.pool = tf.keras.layers.MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">                                              strides=<span class="number">2</span>,</span><br><span class="line">                                              padding=<span class="string">"same"</span>)</span><br><span class="line">        self.num_channels = num_init_features</span><br><span class="line">        self.dense_block_1 = DenseBlock(num_layers=block_layers[<span class="number">0</span>], growth_rate=growth_rate, drop_rate=drop_rate)</span><br><span class="line">        self.num_channels += growth_rate * block_layers[<span class="number">0</span>]</span><br><span class="line">        self.num_channels = compression_rate * self.num_channels</span><br><span class="line">        self.transition_1 = TransitionLayer(out_channels=int(self.num_channels))</span><br><span class="line">        self.dense_block_2 = DenseBlock(num_layers=block_layers[<span class="number">1</span>], growth_rate=growth_rate, drop_rate=drop_rate)</span><br><span class="line">        self.num_channels += growth_rate * block_layers[<span class="number">1</span>]</span><br><span class="line">        self.num_channels = compression_rate * self.num_channels</span><br><span class="line">        self.transition_2 = TransitionLayer(out_channels=int(self.num_channels))</span><br><span class="line">        self.dense_block_3 = DenseBlock(num_layers=block_layers[<span class="number">2</span>], growth_rate=growth_rate, drop_rate=drop_rate)</span><br><span class="line">        self.num_channels += growth_rate * block_layers[<span class="number">2</span>]</span><br><span class="line">        self.num_channels = compression_rate * self.num_channels</span><br><span class="line">        self.transition_3 = TransitionLayer(out_channels=int(self.num_channels))</span><br><span class="line">        self.dense_block_4 = DenseBlock(num_layers=block_layers[<span class="number">3</span>], growth_rate=growth_rate, drop_rate=drop_rate)</span><br><span class="line"></span><br><span class="line">        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()</span><br><span class="line">        self.fc = tf.keras.layers.Dense(units=<span class="number">10</span>,</span><br><span class="line">                                        activation=tf.keras.activations.softmax)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = self.conv(inputs)</span><br><span class="line">        x = self.bn(x)</span><br><span class="line">        x = tf.keras.activations.relu(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.dense_block_1(x)</span><br><span class="line">        x = self.transition_1(x)</span><br><span class="line">        x = self.dense_block_2(x)</span><br><span class="line">        x = self.transition_2(x)</span><br><span class="line">        x = self.dense_block_3(x)</span><br><span class="line">        x = self.transition_3(x,)</span><br><span class="line">        x = self.dense_block_4(x)</span><br><span class="line"></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">densenet</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> DenseNet(num_init_features=<span class="number">64</span>, growth_rate=<span class="number">32</span>, block_layers=[<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>], compression_rate=<span class="number">0.5</span>, drop_rate=<span class="number">0.5</span>)</span><br><span class="line">mynet=densenet()</span><br></pre></td></tr></table></figure>
<h3 id="xiao-jie-8">小结</h3>
<ol>
<li>在跨层连接上，不同于ResNet中将输入与输出相加，DenseNet在通道维上连结输入与输出。</li>
<li>DenseNet的主要构建模块是稠密块和过渡层。</li>
</ol>
<h1 id="chang-jian-wen-ti">常见问题</h1>
<h2 id="wei-shi-yao-xu-yao-padding">为什么需要 Padding ？</h2>
<p>图像是 <code>5 × 5</code>的矩阵，卷积核是 <code> 3 × 3</code>的，最终得出的feature map是 <code>3 × 3 </code>的矩阵（<code>n -f + 1</code>) 。</p>
<p>这样会带来两个问题：</p>
<ul>
<li>每一次做卷积操作时，你的图像就会缩小，如果这种情况发生多次，你的图像就会变得很小。</li>
<li>边缘的像素点所受到的关注点比中心的关注点少很多。比如上述例子边缘的像素点只进行了一次卷积计算，而中心点 <code>3 * 3</code> 却进行了9次卷积计算，这明显是不公平的。这意味着图像边缘的信息大多都丢失了。</li>
</ul>
<p>如果加上 paddding 之后，我们的 feature-map 就变为 <code>(n + 2p - f + 1) × (n + 2p - f + 1)</code>的矩阵。</p>
<p>Padding存在的意义在于：</p>
<ul>
<li>
<p>为了不丢弃原图信息</p>
</li>
<li>
<p>为了保持feature map 的大小与原图一致</p>
</li>
<li>
<p>为了让更深层的layer的 input 依旧保持有足够大的信息量</p>
</li>
<li>
<p>为了实现上述目的，且不做多余的事情，padding出来的pixel的值都是0，不存在噪音问题。</p>
</li>
</ul>
<h2 id="wei-shi-yao-juan-ji-he-she-ji-chi-cun-du-shi-qi-shu">为什么卷积核设计尺寸都是奇数</h2>
<ul>
<li>保证像素点中心位置，避免位置信息偏移</li>
<li>填充边缘时能保证两边都能填充，原矩阵依然对称</li>
</ul>
<h2 id="juan-ji-cao-zuo-de-te-dian">卷积操作的特点</h2>
<ul>
<li>
<p>**稀疏交互：**卷积神经网络中，卷积核尺度远小于输入的尺度，这样每个输出神经网仅与前一层区域内的神经元存在连接权重，称此为稀疏交互。</p>
</li>
<li>
<p>提高了模型的统计效率：原本一幅图像只能提供少量特征，现在每一块像素区域都可以提供一部分特征</p>
<ul>
<li>使得参数大量减少，优化的时间复杂度也会减小几个数量级，过拟合情况也得到改善。</li>
<li>稀疏交互的意义在于，<strong>先从局部的特征入手，再将局部特征组合起来形成更复杂和抽象的特征</strong>。</li>
</ul>
</li>
<li>
<p><strong>参数共享：</strong> 参数共享指的是<strong>同一个模型的不同模块中使用相同的参数</strong>。参数共享的意义在于使得卷积层具有<strong>平移等特性</strong>。</p>
<ul>
<li>权重共享一定程度上能增强参数之间的联系，获得更好的<strong>共性特征</strong>。</li>
<li>很大程度上降低了网络的参数，<strong>节省计算量和计算所需内存</strong>。</li>
<li>权重共享能起到<strong>很好正则的作用</strong>。正则化的目的是为了降低模型复杂度，防止过拟合，而权重共享则正好降低了模型的参数和复杂度。</li>
</ul>
</li>
<li>
<p><strong>平移不变性：</strong>（局部）平移不变性是一个很有用的性质，尤其是当我们关心某个特征<strong>是否出现</strong>而不关心它出现的具体位置时。平移不变性是由于<strong>参数共享和池化</strong> 所带来的。</p>
</li>
</ul>
<h2 id="cnn-you-shi-yao-bu-zu">CNN有什么不足？</h2>
<ul>
<li>
<p><strong>信息损失问题。</strong> CNN在Pooling的时候会丢失大量的有价值信息，以及忽略局部与整体之间的关联性比如得分最高的特征只出现了一次，而得分第二高的特征出现了很多次，得分第二高的特征可能比最高的特征还要重要，却被丢弃了，自然造成了不小的信息损失</p>
</li>
<li>
<p><strong>忽略了位置信息</strong>：一个区域有用的特征极有可能和另一个区域的信息有联系，如TextCNN：对于一些粒度较粗的分类问题如话题分类，位置信息可能不大，但对于如情感分析这种粒度较细的分类问题，位置信息不足便会导致一些问题，如&quot;虽然他长的很帅，但是人品不好&quot;和&quot;虽然他人品不好，但他长得帅啊&quot;，在情感倾向上区别还是比较明显的。</p>
</li>
</ul>
<h2 id="cnn-yu-rnn-de-you-lie">CNN 与 RNN 的优劣</h2>
<ul>
<li>并行能力， 训练时间很漫长</li>
<li>RNN 容易发生<strong>梯度消失</strong>，包括 LSTM</li>
<li>CNN 的感受视野受限于卷积核，需要深层的 CNN 网络来获得更大的感受视野</li>
</ul>
<h2 id="juan-ji-chi-hua-de-yi-yi">卷积，池化的意义</h2>
<ul>
<li>卷积和池化可能导致<strong>欠拟合</strong>
<ul>
<li>如果一项任务涉及到要<strong>对输入中相隔较远的信息进行合并</strong>时，那么卷积可能就不正确了。</li>
<li>如果一项任务依赖于保存<strong>精确的空间信息</strong>，那么在所有的特征上使用池化将会增大训练误差。</li>
</ul>
</li>
<li>当我们比较卷积模型的统计学习表现时，只能以基准中的其他卷积模型作为比较的对象</li>
</ul>
<h2 id="juan-ji-zhong-bu-tong-ling-tian-chong-de-ying-xiang">卷积中不同零填充的影响</h2>
<p>假定 <code>m， k</code> 分别代表图像的宽度和卷积核的宽度：</p>
<ul>
<li><strong>Valid 卷积（有效卷积）</strong>：不使用零填充，卷积核只允许访问那些图像中能够<strong>完全包含整个核</strong>的位置，输出的宽度为 <code>m − k + 1</code>
<ul>
<li>在这种情况下，输出的所有像素都是输入中相同数量像素的函数，这使得输出像素的表示更加规范。</li>
<li>然而，输出的大小在每一层都会缩减，这限制了网络中能够包含的卷积层的层数。（一般情况下，影响不大，除非是上百层的网络）</li>
</ul>
</li>
<li><strong>Same 卷积（相同卷积）：<strong>只进行足够的零填充来</strong>保持输出和输入具有相同的大小</strong>，即输出的宽度为 <code>m</code>.
<ul>
<li>在这种情况下，只要硬件支持，网络就能包含任意多的卷积层。</li>
<li>然而，输入像素中靠近边界的部分相比于中间部分对于输出像素的影响更小。这可能会导致边界像素存在一定程度的欠表示。</li>
</ul>
</li>
<li>**Full 卷积（全卷积）：**进行足够多的零填充使得每个像素都能被访问 k 次（非全卷积只有中间的像素能被访问 k 次），最终输出图像的宽度为 <code>m + k − 1</code>
<ul>
<li>因为 same 卷积可能导致边界像素欠表示，从而出现了 Full 卷积；</li>
<li>但是在这种情况下，输出像素中靠近边界的部分相比于中间部分是更少像素的函数。这将导致<strong>学得的卷积核不能再所有所有位置表现一致</strong>。</li>
<li>事实上，很少使用 Full 卷积</li>
</ul>
</li>
</ul>
<p>通常<strong>零填充的最优数量</strong>处于 “有效卷积”和 “相同卷积” 之间。</p>
<h2 id="1-1-juan-ji-de-zuo-yong">1 * 1 卷积的作用？</h2>
<ul>
<li>实现信息的跨通道交互和整合。</li>
<li>对卷积核通道数进行降维和升维，减小参数量。</li>
</ul>
<h2 id="juan-ji-he-shi-fou-yue-da-yue-hao">卷积核是否越大越好？</h2>
<p>卷积核越大，参数量越多。 前期无法使用较小卷积核是因为，前期的模型无法做的很深，这样限制了卷积核的感受视野。但其实，通过堆叠2 个 3 * 3 卷积核可以获得与 5 * 5 卷积核相同的感受视野，同时参数量特更少。 因此，大多数情况下，通过堆叠较小的卷积核比直接采用单个较大的卷积核更加有效。</p>
<p>自然语言中， TextCNN 就采用单层的卷积核，此时选择合适的，较大的卷积核相对比较重要， 而DPCNN 中，因为能够将卷积做的很深，那么就可以采用3 * 3 的卷积核来做了。</p>
<h2 id="ru-he-jian-shao-juan-ji-ceng-can-shu-liang">如何减少卷积层参数量？</h2>
<ul>
<li>用深层小卷积代替浅层大卷积</li>
<li>使用分离卷积操作：将原本\(K\times K\times C\)的卷积操作分离为\(K\times K\times 1\)和\(1\times1\times C\)的两部分操作</li>
<li>添加 \(1 \times 1\) 卷积</li>
<li>在卷积层前使用池化操作</li>
</ul>
<h2 id="cnn-te-dian">CNN 特点</h2>
<ul>
<li>
<p><strong>区域不变性：</strong> filter 在每层的输入向量(图像)上滑动，检测的是局部信息，然后通过pooling取最大值或均值。pooling这步综合了局部特征，失去了每个特征的位置信息。</p>
<p>这很适合基于图像的任务，比如要判断一幅图里有没有猫这种生物，你可能不会去关心这只猫出现在图像的哪个区域。但是在NLP里，词语在句子或是段落里出现的位置，顺序，都是很重要的信息。</p>
</li>
<li>
<p><strong>局部组合性：</strong> CNN中，每个滤波器都把较低层的局部特征组合生成较高层的更全局化的特征。</p>
<p>这在CV里很好理解，像素组合成边缘，边缘生成形状，最后把各种形状组合起来得到复杂的物体表达。在NLP里，当然也有类似的组合关系，但是远不如图像来的直接。而且在图像里，相邻像素必须是相关的，相邻的词语却未必相关。</p>
</li>
</ul>
<h2 id="wei-he-jiao-da-de-batch-size-neng-gou-ti-gao-cnn-de-fan-hua-neng-li">为何较大的batch size 能够提高 CNN 的泛化能力？</h2>
<p>在相同迭代次数和学习率的条件下，每批次采用更多的数据将有助于模型更好的学习到正确的模式，模型输出结果也会更加稳定</p>
<h2 id="same-yu-valid-de-qu-bie">SAME 与 VALID 的区别</h2>
<ul>
<li>SAME： 宽卷积，通常采用零填充的方式对卷积核不满足整除条件的输入特征进行补全，以使卷积层的输出维度保持与输入特征维度一致。</li>
<li>VALID：窄卷积，不进行任何填充，在输入特征边缘位置若不足以进行卷积操作，则对边缘信息进行舍弃，因此在步长为1的情况下该填充方式的卷积层输出特征维度可能会略小于输入特征的维度。</li>
</ul>
<h2 id="cnn-you-que-dian">CNN 优缺点</h2>
<p><strong>优点：</strong></p>
<ul>
<li>共享卷积核，优化计算量。</li>
<li>无需手动选取特征，训练好权重，即得特征。</li>
<li>深层次的网络抽取图像信息丰富，表达效果好。</li>
<li>保持了层级网络结构。</li>
<li>不同层次有不同形式与功能。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>需要调参，需要大样本量，GPU等硬件依赖。</li>
<li>物理含义不明确。</li>
</ul>

    </div>

    
    
    <div>
  
    <div style="text-align:center;color:#bfbfbf;font-size:16px;">
      <span>-------- 本文结束 </span>
      <i class="fa fa-coffee"></i>
      <span> 感谢阅读 --------</span>
    </div>
  
</div>
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    鼓励一下
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat.png" alt="Li Zhen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/ali.png" alt="Li Zhen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/07/19/math/information_theory/" rel="prev" title="信息论">
      <i class="fa fa-chevron-left"></i> 信息论
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/05/27/deeplearning/computer_vision/" rel="next" title="计算机视觉基础">
      计算机视觉基础 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#er-wei-juan-ji-ceng"><span class="nav-number">1.</span> <span class="nav-text">二维卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#two-dimentional-cross-correlation"><span class="nav-number">1.1.</span> <span class="nav-text">two dimentional cross-correlation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#conv-2-d"><span class="nav-number">1.2.</span> <span class="nav-text">Conv2d</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hu-xiang-guan-yun-suan-he-juan-ji-yun-suan"><span class="nav-number">1.3.</span> <span class="nav-text">互相关运算和卷积运算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#te-zheng-tu-he-gan-shou-ye"><span class="nav-number">1.4.</span> <span class="nav-text">特征图和感受野</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tian-chong-he-bu-fu"><span class="nav-number">1.5.</span> <span class="nav-text">填充和步幅</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#padding"><span class="nav-number">1.5.1.</span> <span class="nav-text">padding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#stride"><span class="nav-number">1.5.2.</span> <span class="nav-text">stride</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#duo-shu-ru-tong-dao-he-duo-shu-chu-tong-dao"><span class="nav-number">1.6.</span> <span class="nav-text">多输入通道和多输出通道</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#muti-channels-in"><span class="nav-number">1.6.1.</span> <span class="nav-text">muti-channels in</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multi-channels-out"><span class="nav-number">1.6.2.</span> <span class="nav-text">multi-channels out</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#convolution"><span class="nav-number">1.6.3.</span> <span class="nav-text">convolution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xiao-jie"><span class="nav-number">1.6.4.</span> <span class="nav-text">小结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#juan-ji-he-de-xuan-ze"><span class="nav-number">2.</span> <span class="nav-text">卷积核的选择</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#chi-hua-ceng"><span class="nav-number">3.</span> <span class="nav-text">池化层</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-dimentional-max-pooling-and-avg-pooling"><span class="nav-number">3.1.</span> <span class="nav-text">2-dimentional max_pooling and avg_pooling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#padding-and-stride"><span class="nav-number">3.2.</span> <span class="nav-text">padding and stride</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multi-channels"><span class="nav-number">3.3.</span> <span class="nav-text">multi-channels</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xiao-jie-1"><span class="nav-number">3.4.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#yi-wei-juan-ji"><span class="nav-number">4.</span> <span class="nav-text">一维卷积</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#wei-chi-hua"><span class="nav-number">5.</span> <span class="nav-text">⼀维池化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#juan-ji-ceng-yu-chi-hua-ceng-bi-jiao"><span class="nav-number">6.</span> <span class="nav-text">卷积层与池化层比较</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#juan-ji-shen-jing-wang-luo"><span class="nav-number">7.</span> <span class="nav-text">卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#le-net-juan-ji-shen-jing-wang-luo"><span class="nav-number">7.1.</span> <span class="nav-text">LeNet:卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#le-net-mo-xing"><span class="nav-number">7.1.1.</span> <span class="nav-text">LeNet模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xiao-jie-2"><span class="nav-number">7.1.2.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#alex-net-shen-du-juan-ji-wang-luo"><span class="nav-number">7.2.</span> <span class="nav-text">AlexNet:深度卷积网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#xue-xi-te-zheng-biao-shi"><span class="nav-number">7.2.1.</span> <span class="nav-text">学习特征表示</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#que-shi-yao-su-yi-shu-ju"><span class="nav-number">7.2.1.1.</span> <span class="nav-text">缺失要素一：数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#que-shi-yao-su-er-ying-jian"><span class="nav-number">7.2.1.2.</span> <span class="nav-text">缺失要素二：硬件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#alex-net"><span class="nav-number">7.2.2.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xiao-jie-3"><span class="nav-number">7.2.3.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#vgg-shi-yong-zhong-fu-yuan-su-de-wang-luo"><span class="nav-number">7.3.</span> <span class="nav-text">VGG:使用重复元素的网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#vgg-kuai"><span class="nav-number">7.3.1.</span> <span class="nav-text">VGG块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vgg-wang-luo"><span class="nav-number">7.3.2.</span> <span class="nav-text">VGG网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xiao-jie-4"><span class="nav-number">7.3.3.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ni-n-wang-luo-zhong-de-wang-luo"><span class="nav-number">7.4.</span> <span class="nav-text">NiN:网络中的网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ni-n-kuai"><span class="nav-number">7.4.1.</span> <span class="nav-text">NiN块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ni-n-mo-xing"><span class="nav-number">7.4.2.</span> <span class="nav-text">NiN模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xiao-jie-5"><span class="nav-number">7.4.3.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#goog-le-net-han-bing-xing-lian-jie-de-wang-luo"><span class="nav-number">7.5.</span> <span class="nav-text">GoogLeNet:含并行连接的网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#inception-kuai"><span class="nav-number">7.5.1.</span> <span class="nav-text">Inception 块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#goog-le-net-mo-xing"><span class="nav-number">7.5.2.</span> <span class="nav-text">GoogLeNet模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xiao-jie-6"><span class="nav-number">7.5.3.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#res-net-can-chai-wang-luo"><span class="nav-number">7.6.</span> <span class="nav-text">ResNet:残差网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#can-chai-kuai"><span class="nav-number">7.6.1.</span> <span class="nav-text">残差块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#res-net-mo-xing"><span class="nav-number">7.6.2.</span> <span class="nav-text">ResNet模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xiao-jie-7"><span class="nav-number">7.6.3.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dense-net-chou-mi-lian-jie-wang-luo"><span class="nav-number">7.7.</span> <span class="nav-text">DenseNet:稠密连接网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#chou-mi-kuai"><span class="nav-number">7.7.1.</span> <span class="nav-text">稠密块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#guo-du-ceng"><span class="nav-number">7.7.2.</span> <span class="nav-text">过渡层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dense-net-mo-xing"><span class="nav-number">7.7.3.</span> <span class="nav-text">DenseNet模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xiao-jie-8"><span class="nav-number">7.7.4.</span> <span class="nav-text">小结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#chang-jian-wen-ti"><span class="nav-number">8.</span> <span class="nav-text">常见问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#wei-shi-yao-xu-yao-padding"><span class="nav-number">8.1.</span> <span class="nav-text">为什么需要 Padding ？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#wei-shi-yao-juan-ji-he-she-ji-chi-cun-du-shi-qi-shu"><span class="nav-number">8.2.</span> <span class="nav-text">为什么卷积核设计尺寸都是奇数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#juan-ji-cao-zuo-de-te-dian"><span class="nav-number">8.3.</span> <span class="nav-text">卷积操作的特点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cnn-you-shi-yao-bu-zu"><span class="nav-number">8.4.</span> <span class="nav-text">CNN有什么不足？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cnn-yu-rnn-de-you-lie"><span class="nav-number">8.5.</span> <span class="nav-text">CNN 与 RNN 的优劣</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#juan-ji-chi-hua-de-yi-yi"><span class="nav-number">8.6.</span> <span class="nav-text">卷积，池化的意义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#juan-ji-zhong-bu-tong-ling-tian-chong-de-ying-xiang"><span class="nav-number">8.7.</span> <span class="nav-text">卷积中不同零填充的影响</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-juan-ji-de-zuo-yong"><span class="nav-number">8.8.</span> <span class="nav-text">1 * 1 卷积的作用？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#juan-ji-he-shi-fou-yue-da-yue-hao"><span class="nav-number">8.9.</span> <span class="nav-text">卷积核是否越大越好？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ru-he-jian-shao-juan-ji-ceng-can-shu-liang"><span class="nav-number">8.10.</span> <span class="nav-text">如何减少卷积层参数量？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cnn-te-dian"><span class="nav-number">8.11.</span> <span class="nav-text">CNN 特点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#wei-he-jiao-da-de-batch-size-neng-gou-ti-gao-cnn-de-fan-hua-neng-li"><span class="nav-number">8.12.</span> <span class="nav-text">为何较大的batch size 能够提高 CNN 的泛化能力？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#same-yu-valid-de-qu-bie"><span class="nav-number">8.13.</span> <span class="nav-text">SAME 与 VALID 的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cnn-you-que-dian"><span class="nav-number">8.14.</span> <span class="nav-text">CNN 优缺点</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <a href='/'>
    <img class="site-author-image" itemprop="image" alt="Li Zhen"
      src="/images/snoppy.jpeg">
  </a>
  <p class="site-author-name" itemprop="name">Li Zhen</p>
  <div class="site-description" itemprop="description">他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">90</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">66</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jeffery0628" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jeffery0628" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jeffery.lee.0628@gmail.com" title="邮箱 → mailto:jeffery.lee.0628@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮箱</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Zhen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">1.2m</span>
</div>

        
<div class="busuanzi-count">
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.1.0/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  






  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>


 

<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180101,"YYYYMMDD"));
    ages = ages.replace(/years?/, "年");
    ages = ages.replace(/months?/, "月");
    ages = ages.replace(/days?/, "天");
    ages = ages.replace(/hours?/, "小时");
    ages = ages.replace(/minutes?/, "分");
    ages = ages.replace(/seconds?/, "秒");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `我已在此等候你 ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


<script src="https://cdn.jsdelivr.net/npm/sweetalert@2.1.2/dist/sweetalert.min.js"></script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'tChTbEIggDSVcdhPBUf0MFxW-gzGzoHsz',
      appKey     : 'sJ6xUiFnifEDIIfnj3Ut9zy1',
      placeholder: "留下你的小脚印吧！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '8' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
