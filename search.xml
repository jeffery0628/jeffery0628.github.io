<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BatchNormalization</title>
    <url>/2020/03/28/BatchNormalization/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>机器学习领域有个很重要的假设：<strong>IID独立同分布假设</strong>，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。</p>
<p>那BatchNorm的作用是什么呢？<strong>BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。</strong></p>
<a id="more"></a>
<h3 id="Internal-Covariate-Shift-问题"><a href="#Internal-Covariate-Shift-问题" class="headerlink" title="Internal Covariate Shift 问题"></a>Internal Covariate Shift 问题</h3><p><strong>如果ML系统实例集合中的输入值X的分布老是变，这不符合IID假设</strong>，网络模型很难<strong>稳定的学规律</strong>,对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就是<strong>在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层</strong>。</p>
<p>然后提出了BatchNorm的基本思想：能不能<strong>让每个隐层节点的激活输入分布固定下来呢</strong>？这样就避免了“Internal Covariate Shift”问题了。</p>
<p>BN不是凭空拍脑袋拍出来的好点子，它是有启发来源的：之前的研究表明如果在图像处理中对输入图像进行白化（Whiten）操作的话——所谓<strong>白化</strong>，<strong>就是对输入数据分布变换到0均值，单位方差的正态分布</strong>——那么神经网络会较快收敛，那么BN作者就开始推论了：图像是深度神经网络的输入层，做白化能加快收敛，那么其实对于深度网络来说，其中某个隐层的神经元是下一层的输入，意思是其实深度神经网络的每一个隐层都是输入层，不过是相对下一层来说而已，那么能不能对每个隐层都做白化呢？这就是启发BN产生的原初想法，而BN也确实就是这么做的，<strong>可以理解为对深层神经网络每个隐层神经元的激活值做简化版本的白化操作。</strong></p>
<h3 id="BatchNorm的本质思想"><a href="#BatchNorm的本质思想" class="headerlink" title="BatchNorm的本质思想"></a>BatchNorm的本质思想</h3><p>BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的<strong>激活输入值</strong>（就是那个x=WU+B，U是输入）<strong>随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近</strong>（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这<strong>导致反向传播时低层神经网络的梯度消失</strong>，这是训练深层神经网络收敛越来越慢的<strong>本质原因</strong>，<strong>而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布</strong>，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是<strong>这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</strong></p>
<p><strong>对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题</strong>。因为梯度一直都能保持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也就是说收敛地快。BN说到底就是这么个机制，方法很简单，道理很深刻。</p>
<h3 id="训练阶段如何做BatchNorm"><a href="#训练阶段如何做BatchNorm" class="headerlink" title="训练阶段如何做BatchNorm"></a>训练阶段如何做BatchNorm</h3><p>假设对于一个深层神经网络来说，其中两层结构如下：</p>
<p><img src="/2020/03/28/BatchNormalization/bn1.png" alt="avatar"></p>
<p>要对每个隐层神经元的激活值做BN，可以想象成每个隐层又加上了一层BN操作层，它位于X=WU+B激活值获得之后，非线性函数变换之前，其图示如下：</p>
<p><img src="/2020/03/28/BatchNormalization/bn2.png" alt="avatar"></p>
<p>对于Mini-Batch SGD来说，一次训练过程里面包含m个训练实例，其具体BN操作就是对于隐层内每个神经元的激活值来说，进行如下变换：</p>
<script type="math/tex; mode=display">
\hat{x}^{(k)}=\frac{x^{(k)}-E\left[x^{(k)}\right]}{\sqrt{\operatorname{Var}\left[x^{(k)}\right]}}</script><p>某个神经元对应的原始的激活x通过减去mini-Batch内m个实例获得的m个激活x求得的均值E(x)并除以求得的方差Var(x)来进行转换。</p>
<p>经过这个<strong>变换后某个神经元的激活x形成了均值为0，方差为1的正态分布，目的是把值往后续要进行的非线性变换的线性区拉动，增大导数值，增强反向传播信息流动性，加快训练收敛速度。 但是这样会导致网络表达能力下降，为了防止这一点，每个神经元增加两个调节参数（scale和shift），这两个参数是通过训练来学习到的，用来对变换后的激活反变换，使得网络表达能力增强，即对变换后的激活进行如下的scale和shift操作，这其实是变换的反操作：</strong></p>
<script type="math/tex; mode=display">
y^{(k)}=\gamma^{(k)} \hat{x}^{(k)}+\beta^{(k)}</script><p>算法描述：</p>
<p><img src="/2020/03/28/BatchNormalization/bn3.png" alt="avatar"></p>
<h3 id="推理阶段如何做BatchNorm"><a href="#推理阶段如何做BatchNorm" class="headerlink" title="推理阶段如何做BatchNorm"></a>推理阶段如何做BatchNorm</h3><p>BN在训练的时候可以根据Mini-Batch里的若干训练实例进行激活数值调整，但是在推理（inference）的过程中，很明显输入就只有一个实例，看不到Mini-Batch其它实例，那么这时候怎么对输入做BN呢？因为很明显一个实例是没法算实例集合求出的均值和方差的。这可如何是好？</p>
<p>既然没有从Mini-Batch数据里可以得到的统计量，那就想其它办法来获得这个统计量，就是均值和方差。可以用从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的均值和方差统计量，因为本来就打算用全局的统计量，只是因为计算量等太大所以才会用Mini-Batch这种简化方式的，那么在推理的时候直接用全局统计量即可。</p>
<p>决定了获得统计量的数据范围，那么接下来的问题是如何获得均值和方差的问题。很简单，因为每次做Mini-Batch训练时，都会有那个Mini-Batch里m个训练实例获得的均值和方差，现在要全局统计量，只要把每个Mini-Batch的均值和方差统计量记住，然后对这些均值和方差求其对应的数学期望即可得出全局统计量</p>
<script type="math/tex; mode=display">
\begin{aligned}
&E[x] \leftarrow E_{\mathrm{B}}\left[\mu_{\mathrm{B}}\right] \\
&\operatorname{Var}[x] \leftarrow \frac{m}{m-1} E_{\mathrm{B}}\left[\sigma_{\mathrm{B}}^{2}\right]
\end{aligned}</script><p>有了均值和方差，每个隐层神经元也已经有对应训练好的Scaling参数和Shift参数，就可以在推导的时候对每个神经元的激活数据计算NB进行变换了，在推理过程中进行BN采取如下方式：</p>
<script type="math/tex; mode=display">
y=\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\varepsilon}} \cdot x+\left(\beta-\frac{\gamma \cdot E[x]}{\sqrt{\operatorname{Var}[x]+\varepsilon})}\right)</script><p>这个公式其实和训练时</p>
<script type="math/tex; mode=display">
y^{(k)}=\gamma^{(k)} \hat{x}^{(k)}+\beta^{(k)}</script><p>是等价的，通过简单的合并计算推导就可以得出这个结论。那么为啥要写成这个变换形式呢？作者这么写的意思是：在实际运行的时候，按照这种变体形式可以减少计算量，为啥呢？因为对于每个隐层节点来说：</p>
<script type="math/tex; mode=display">
\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\varepsilon}} \frac{\gamma \cdot E[x]}{\sqrt{\operatorname{Var}[x]+\varepsilon}}</script><p>都是固定值，这样这两个值可以事先算好存起来，在推理的时候直接用就行了，这样比原始的公式每一步骤都现算少了除法的运算过程，乍一看也没少多少计算量，但是如果隐层节点个数多的话节省的计算量就比较多了。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol>
<li><strong>不仅仅极大提升了训练速度，收敛过程大大加快</strong></li>
<li><strong>还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果</strong></li>
<li>另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等。总而言之，经过这么简单的变换，带来的好处多得很，这也是为何现在BN这么快流行起来的原因。</li>
</ol>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a href="https://cloud.tencent.com/developer/article/1157136" target="_blank" rel="noopener">深度学习】深入理解Batch Normalization批标准化</a></li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>batch normalization</tag>
      </tags>
  </entry>
  <entry>
    <title>Vim速查</title>
    <url>/2018/05/29/Vim%E9%80%9F%E6%9F%A5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="vim（Visual-Interface）"><a href="#vim（Visual-Interface）" class="headerlink" title="vim（Visual Interface）"></a>vim（Visual Interface）</h2><p><img src="/2018/05/29/Vim%E9%80%9F%E6%9F%A5/NSFileHandle.png" alt="avatar"></p>
<a id="more"></a>
<h3 id="命令模式"><a href="#命令模式" class="headerlink" title="命令模式"></a>命令模式</h3><ol>
<li>h：左移</li>
<li>j：下移</li>
<li>k：上移</li>
<li>l(L)：右移</li>
<li><code>^</code> 或 <code>0</code>：光标移动到当前行的行首。</li>
<li><code>1$</code>：光标移动到当前行的行末，<code>2$</code>:光标移动到下一行的行末,…。</li>
<li>M：光标移动到中间行</li>
<li>L：光标移动到屏幕最后一行行首</li>
<li>G：移动到指定行，行号 -G，只有G的话，到文件最末尾</li>
<li>gg：文件第一个字符</li>
<li>w:向后一次移动一个字</li>
<li>b：向前一次移动一个字</li>
<li>{：按段移动，上移</li>
<li>}：按段下移，下移</li>
<li>ctrl+d：向下翻半屏</li>
<li>ctrl+u：向上翻半屏</li>
<li>ctrl+f：向下翻一屏</li>
<li>ctrl+b：向上翻一屏</li>
</ol>
<h4 id="可视模式"><a href="#可视模式" class="headerlink" title="可视模式"></a>可视模式</h4><ol>
<li>v:按字符移动，结合h,j,k,l选中文本内容。</li>
<li>V：按行移动，选中文本可视模式，可以配合d，y，&gt;&gt;,&lt;&lt;实现对文本块的删除，复制，左右移动。</li>
</ol>
<h4 id="删除命令"><a href="#删除命令" class="headerlink" title="删除命令"></a>删除命令</h4><ol>
<li>x：删除光标后一个字符，n x 删除光标后的n个字符</li>
<li>X：删除光标前一个字符，相当于Backspace</li>
<li>dd：删除光标所在行，n dd删除指定的n行</li>
<li>D：删除光标后本行所有内容，包含光标所在字符</li>
<li>d0：删除光标前本行所有内容，不包含光标所在字符</li>
<li>dw：删除光标开始位置的字，包含光标所在字符</li>
</ol>
<h4 id="撤销命令"><a href="#撤销命令" class="headerlink" title="撤销命令"></a>撤销命令</h4><ol>
<li>u：一步一步撤销</li>
<li>ctrl+r：反撤销</li>
</ol>
<h4 id="重复命令"><a href="#重复命令" class="headerlink" title="重复命令"></a>重复命令</h4><ol>
<li>. ：重复上一次操作的命令</li>
</ol>
<h4 id="文本移动"><a href="#文本移动" class="headerlink" title="文本移动"></a>文本移动</h4><ol>
<li>>&gt;:文本行右移</li>
<li>\&lt;&lt;:文本行左移</li>
</ol>
<h4 id="复制粘贴"><a href="#复制粘贴" class="headerlink" title="复制粘贴"></a>复制粘贴</h4><ol>
<li>yy：复制当前行，n yy复制n行</li>
<li>在末行模式，输入：a,by 复制从第a行开始，到第b行结束的内容</li>
<li>p：在光标坐在位置向下新开辟一行，粘贴</li>
<li>d：剪切选中内容</li>
</ol>
<h4 id="查找命令"><a href="#查找命令" class="headerlink" title="查找命令"></a>查找命令</h4><ol>
<li>/str : 查找str，从光标所在行往下查找</li>
<li>？str：查找str，从光标所在行往上查找</li>
<li>n：查找下一个</li>
<li>N：查找上一个 </li>
</ol>
<h4 id="替换操作"><a href="#替换操作" class="headerlink" title="替换操作"></a>替换操作</h4><ol>
<li>r：替换当前字符</li>
<li>R替换当前行光标后的字符</li>
</ol>
<h4 id="替换命令"><a href="#替换命令" class="headerlink" title="替换命令"></a>替换命令</h4><ol>
<li>末行模式下，将光标所在行的abc替换成123，：s/abc/123/g</li>
<li>末行模式下，将第一行到第10行之间的abc替换成123，:1,10s/abc/123/g</li>
<li>末行模式下，把文件中的abc全部替换成123，:%s/abc/123</li>
</ol>
<h3 id="输入模式"><a href="#输入模式" class="headerlink" title="输入模式"></a>输入模式</h3><ol>
<li>i：插入光标前一个字符</li>
<li>I：插入行首</li>
<li>a：插入光标后一个字符</li>
<li>A: 插入行末</li>
<li>o：向下新开一行，插入行首</li>
<li><p>O：向上新开一行，插入行首</p>
<h3 id="末行模式"><a href="#末行模式" class="headerlink" title="末行模式"></a>末行模式</h3></li>
<li><p>：set nu 显示行号</p>
</li>
<li>! shell命令</li>
<li>wq：保存退出</li>
<li>ZZ： 保存退出</li>
<li>q！：不保存退出</li>
</ol>
<h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p>修改用户目录下的vimrc（~/.vimrc）文件，修改vim配置</p>
<ol>
<li>set nu :设置每次vim 打开文件显示行号。</li>
<li>set ts=4:设置tab键每次4个空格</li>
</ol>
]]></content>
      <categories>
        <category>soft skill</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Vim</tag>
        <tag>速查</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo基础建站</title>
    <url>/2018/02/09/hexo-get-start/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">请输入密码以阅读这篇私密文章。</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="3bc6c6d2345e006bbeac2935935a354b0b85957348c84ccf26e08bf8c3560205">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>技术/博客</category>
      </categories>
  </entry>
  <entry>
    <title>Hexo写作技巧</title>
    <url>/2018/02/09/hexo-writing-skills/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">请输入密码以阅读这篇私密文章。</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="25696393d6f71d079639de0fa8172f1bbe6fbd714ac0a326d0903292ca1500e2">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>技术/博客</category>
      </categories>
  </entry>
  <entry>
    <title>latex公式速查</title>
    <url>/2018/05/10/latex%E5%85%AC%E5%BC%8F%E9%80%9F%E6%9F%A5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="上下标"><a href="#上下标" class="headerlink" title="上下标"></a>上下标</h3><div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>x_{2}</td>
<td>$x_{2}$</td>
</tr>
<tr>
<td>x^2</td>
<td>$x^{2}$</td>
</tr>
</tbody>
</table>
</div>
<a id="more"></a>
<h3 id="公式序号"><a href="#公式序号" class="headerlink" title="公式序号"></a>公式序号</h3><script type="math/tex; mode=display">
y = wx+b \tag{1.1}</script><h3 id="括号"><a href="#括号" class="headerlink" title="括号"></a>括号</h3><ol>
<li><p>() [] 直接写就行，而 {} 则需要转义</p>
<ol>
<li>y \in \{3, 4, 5\} : $y \in \{3, 4, 5\}$</li>
</ol>
</li>
<li><p>有时候括号需要大号的，此时需要使用\left和\right加大括号的大小。</p>
<ol>
<li>\left(\frac {x} {y} \right)^2 : $\left(\frac {x} {y} \right)^2$</li>
</ol>
</li>
<li><p>\left 和 \right必须成对出现，对于不显示的一边可以使用 . 代替</p>
</li>
<li><p>\left. \frac{du}{dx} \right| _{x=0} : $\left. \frac{du}{dx} \right| _{x=0}$</p>
</li>
<li><p>双括号$\left|g_{k}\right|$</p>
</li>
<li><p>左大括号：</p>
<p> <code>\left\{\begin{array}{ll}
 x=\frac{3 \pi}{2}(1+2 t) \cos \left(\frac{3 \pi}{2}(1+2 t)\right) &amp; \\
 y=s, &amp; 0 \leq s \leq L,|t| \leq 1 \\
 z=\frac{3 \pi}{2}(1+2 t) \sin \left(\frac{3 \pi}{2}(1+2 t)\right)
 \end{array}\right.</code></p>
<script type="math/tex; mode=display">
 \left\{\begin{array}{ll}
 x=\frac{3 \pi}{2}(1+2 t) \cos \left(\frac{3 \pi}{2}(1+2 t)\right) & \\
 y=s, & 0 \leq s \leq L,|t| \leq 1 \\
 z=\frac{3 \pi}{2}(1+2 t) \sin \left(\frac{3 \pi}{2}(1+2 t)\right)
 \end{array}\right.</script></li>
</ol>
<h3 id="分数"><a href="#分数" class="headerlink" title="分数"></a>分数</h3><p>两种写法：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">命令</th>
<th style="text-align:center">显示</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">\frac{1}{2x+1}</td>
<td style="text-align:center">$\frac{1}{2x+1}$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="开方"><a href="#开方" class="headerlink" title="开方"></a>开方</h3><div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\sqrt[n]{a}</td>
<td>$\sqrt[n]{a}$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="对数"><a href="#对数" class="headerlink" title="对数"></a>对数</h3><div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\log</td>
<td>$\log$</td>
</tr>
<tr>
<td>\lg</td>
<td>$\lg$</td>
</tr>
<tr>
<td>\ln</td>
<td>$\ln$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h3><div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\vec{a}</td>
<td>$\vec{a}$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="三角运算"><a href="#三角运算" class="headerlink" title="三角运算"></a>三角运算</h3><div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\bot</td>
<td>$\bot$</td>
<td></td>
<td>\angle</td>
<td>$\angle$</td>
</tr>
<tr>
<td>\sin</td>
<td>$\sin$</td>
<td></td>
<td>\cos</td>
<td>$\cos$</td>
</tr>
<tr>
<td>\tan</td>
<td>$\tan$</td>
<td></td>
<td>\cot</td>
<td>$\cot$</td>
</tr>
<tr>
<td>\sec</td>
<td>$\sec$</td>
<td></td>
<td>\csc</td>
<td>$\csc$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="省略号"><a href="#省略号" class="headerlink" title="省略号"></a>省略号</h3><div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\ldots</td>
<td>$\ldots$</td>
</tr>
<tr>
<td>\cdots</td>
<td>$\cdots$</td>
</tr>
<tr>
<td>\cdot</td>
<td>$\cdot$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="需要转义的字符"><a href="#需要转义的字符" class="headerlink" title="需要转义的字符"></a>需要转义的字符</h3><div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\#</td>
<td>$#$</td>
<td></td>
<td>\_</td>
<td>$_$</td>
</tr>
<tr>
<td>\$</td>
<td>$$$</td>
<td></td>
<td>\\%</td>
<td>$\%$</td>
</tr>
<tr>
<td>\\&amp;</td>
<td>$\&amp;$</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\\{</td>
<td>$\{$</td>
<td></td>
<td>\\}</td>
<td>$\}$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="普通符号"><a href="#普通符号" class="headerlink" title="普通符号"></a>普通符号</h3><div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\pm</td>
<td>$\pm$</td>
<td></td>
<td>\times</td>
<td>$\times$</td>
</tr>
<tr>
<td>\div</td>
<td>$\div$</td>
<td></td>
<td>\mid</td>
<td>$\mid$</td>
</tr>
<tr>
<td>\cdot</td>
<td>$\cdot$</td>
<td></td>
<td>\circ</td>
<td>$\circ$</td>
</tr>
<tr>
<td>\ast</td>
<td>$\ast$</td>
<td></td>
<td>\bigodot</td>
<td>$\bigodot$</td>
</tr>
<tr>
<td>\bigotimes</td>
<td>$\bigotimes$</td>
<td></td>
<td>\equiv</td>
<td>$\equiv$</td>
</tr>
<tr>
<td>\leq or \le</td>
<td>$\leq$</td>
<td></td>
<td>\geq or \ge</td>
<td>$\geq$</td>
</tr>
<tr>
<td>\ll</td>
<td>$\ll$</td>
<td></td>
<td>\gg</td>
<td>$\gg$</td>
</tr>
<tr>
<td>\neq or  \ne</td>
<td>$\neq$</td>
<td></td>
<td>\approx</td>
<td>$\approx$</td>
</tr>
<tr>
<td>\succeq</td>
<td>$\succeq$</td>
<td></td>
<td>\preceq</td>
<td>$\preceq$</td>
</tr>
<tr>
<td>\doteq</td>
<td>$\doteq$</td>
<td></td>
<td>\cong</td>
<td>$\cong$</td>
</tr>
<tr>
<td>\sim</td>
<td>$\sim$</td>
<td></td>
<td>\simeq</td>
<td>$\simeq$</td>
</tr>
<tr>
<td>\vdash</td>
<td>$\vdash$</td>
<td></td>
<td>\dashv</td>
<td>$\dashv$</td>
</tr>
<tr>
<td>\models</td>
<td>$\models$</td>
<td></td>
<td>\perp</td>
<td>$\perp$</td>
</tr>
<tr>
<td>\smile</td>
<td>$\smile$</td>
<td></td>
<td>\frown</td>
<td>$\frown$</td>
</tr>
<tr>
<td>\saymp</td>
<td>$\asymp$</td>
<td></td>
<td>\:</td>
<td>$\:$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h3><p>大写：开头字母大写<br>斜体：命令前面加var</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th>大写</th>
<th>斜体</th>
<th></th>
<th>命令</th>
<th>显示</th>
<th>大写</th>
<th>斜体</th>
</tr>
</thead>
<tbody>
<tr>
<td>\alpha</td>
<td>$\alpha$</td>
<td>A</td>
<td></td>
<td></td>
<td>\beta</td>
<td>$\beta$</td>
<td>B</td>
<td></td>
</tr>
<tr>
<td>\gamma</td>
<td>$\gamma$</td>
<td>$\Gamma$</td>
<td>$\varGamma$</td>
<td></td>
<td>\delta</td>
<td>$\delta$</td>
<td>$\Delta$</td>
<td>$\varDelta$</td>
</tr>
<tr>
<td>\epsilon</td>
<td>$\epsilon$</td>
<td>E</td>
<td>$\varepsilon$</td>
<td></td>
<td>\zeta</td>
<td>$\zeta$</td>
<td>Z</td>
<td></td>
</tr>
<tr>
<td>\eta</td>
<td>$\eta$</td>
<td>H</td>
<td></td>
<td></td>
<td>\theta</td>
<td>$\theta$</td>
<td>$\Theta$</td>
<td>$\varTheta$</td>
</tr>
<tr>
<td>\iota</td>
<td>$\iota$</td>
<td>I</td>
<td></td>
<td></td>
<td>\kappa</td>
<td>$\kappa$</td>
<td>K</td>
<td></td>
</tr>
<tr>
<td>\lambda</td>
<td>$\lambda$</td>
<td>$\Lambda$</td>
<td>$\varLambda$</td>
<td></td>
<td>\mu</td>
<td>$\mu$</td>
<td>M</td>
<td></td>
</tr>
<tr>
<td>\xi</td>
<td>$\xi$</td>
<td>$\Xi$</td>
<td>$\varXi$</td>
<td></td>
<td>\nu</td>
<td>$\nu$</td>
<td>N</td>
<td></td>
</tr>
<tr>
<td>\pi</td>
<td>$\pi$</td>
<td>$\Pi$</td>
<td>$\varPi$</td>
<td></td>
<td>\rho</td>
<td>$\rho$</td>
<td>P</td>
<td>$\varrho$</td>
</tr>
<tr>
<td>\sigma</td>
<td>$\sigma$</td>
<td>$\Sigma$</td>
<td>$\varSigma$</td>
<td></td>
<td>\tau</td>
<td>$\tau$</td>
<td>T</td>
<td></td>
</tr>
<tr>
<td>\upsilon</td>
<td>$\upsilon$</td>
<td>$\Upsilon$</td>
<td>$\varUpsilon$</td>
<td></td>
<td>\phi</td>
<td>$\phi$</td>
<td>$\Phi$</td>
<td>$\varPhi$</td>
</tr>
<tr>
<td>\chi</td>
<td>$\chi$</td>
<td>X</td>
<td></td>
<td></td>
<td>\psi</td>
<td>$\psi$</td>
<td>$\Psi$</td>
<td>$\varPsi$</td>
</tr>
<tr>
<td>\omega</td>
<td>$\omega$</td>
<td>$\Omega$</td>
<td>$\varOmega$</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p><img src="/2018/05/10/latex%E5%85%AC%E5%BC%8F%E9%80%9F%E6%9F%A5/NSFileHandle.png" alt="5f979c551b937e468a1ed99ff62134d4"></p>
<h3 id="集合运算"><a href="#集合运算" class="headerlink" title="集合运算"></a>集合运算</h3><div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\emptyset</td>
<td>$\emptyset$</td>
<td></td>
<td>\propto</td>
<td>$\propto$</td>
</tr>
<tr>
<td>\notin</td>
<td>$\notin$</td>
<td></td>
<td>\Join or \bowtie</td>
<td>$\Join$</td>
</tr>
<tr>
<td>\supset</td>
<td>$\supset$</td>
<td></td>
<td>\subseteq</td>
<td>$\subseteq$</td>
</tr>
<tr>
<td>\subset</td>
<td>$\subset$</td>
<td></td>
<td>\supseteq</td>
<td>$\supseteq$</td>
</tr>
<tr>
<td>\sqsubset</td>
<td>$\sqsubset$</td>
<td></td>
<td>\sqsupseteq</td>
<td>$\sqsupseteq$</td>
</tr>
<tr>
<td>\bigcap</td>
<td>$\bigcap$</td>
<td></td>
<td>\bigcup</td>
<td>$\bigcup$</td>
</tr>
<tr>
<td>\bigvee</td>
<td>$\bigvee$</td>
<td></td>
<td>\bigwedge</td>
<td>$\bigwedge$</td>
</tr>
<tr>
<td>\biguplus</td>
<td>$\biguplus$</td>
<td></td>
<td>\bigsqcup</td>
<td>$\bigsqcup$</td>
</tr>
<tr>
<td>\in</td>
<td>$\in$</td>
<td></td>
<td>\ni or \owns</td>
<td>$\owns$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="逻辑运算"><a href="#逻辑运算" class="headerlink" title="逻辑运算"></a>逻辑运算</h3><div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\because</td>
<td>$\because$</td>
<td></td>
<td>\therefore</td>
<td>$\therefore$</td>
</tr>
<tr>
<td>\forall</td>
<td>$\forall$</td>
<td></td>
<td>\exists</td>
<td>$\exists$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="微分"><a href="#微分" class="headerlink" title="微分"></a>微分</h3><div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>y{\prime}x</td>
<td>$y{\prime}x$</td>
<td></td>
<td>\int</td>
<td>$\int$</td>
</tr>
<tr>
<td>\iint</td>
<td>$\iint$</td>
<td></td>
<td>\iiint</td>
<td>$\iiint$</td>
</tr>
<tr>
<td>\oint</td>
<td>$oint$</td>
<td></td>
<td>\lim</td>
<td>$lim$</td>
</tr>
<tr>
<td>\infty</td>
<td>$\infty$</td>
<td></td>
<td>\nabla</td>
<td>$\nabla$</td>
</tr>
<tr>
<td>\partial</td>
<td>$\partial$</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="箭头"><a href="#箭头" class="headerlink" title="箭头"></a>箭头</h3><div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\uparrow</td>
<td>$\uparrow$</td>
<td></td>
<td>\downarrow</td>
<td>$\downarrow$</td>
</tr>
<tr>
<td>\Uparrow</td>
<td>$\Uparrow$</td>
<td></td>
<td>\Downarrow</td>
<td>$\Downarrow$</td>
</tr>
<tr>
<td>\leftarrow</td>
<td>$\leftarrow$</td>
<td></td>
<td>\rightarrow</td>
<td>$\rightarrow$</td>
</tr>
<tr>
<td>\Leftarrow</td>
<td>$\Leftarrow$</td>
<td></td>
<td>\Rightarrow</td>
<td>$\Rightarrow$</td>
</tr>
<tr>
<td>\longleftarrow</td>
<td>$\longleftarrow$</td>
<td></td>
<td>\longrightarrow</td>
<td>$\longrightarrow$</td>
</tr>
<tr>
<td>\Longrightarrow</td>
<td>$\Longrightarrow$</td>
<td></td>
<td>\Longrightarrow</td>
<td>$\Longrightarrow$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="求和、连乘和积分号"><a href="#求和、连乘和积分号" class="headerlink" title="求和、连乘和积分号"></a>求和、连乘和积分号</h3><div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th>使用</th>
<th>例子</th>
</tr>
</thead>
<tbody>
<tr>
<td>\sum</td>
<td>$\sum$</td>
<td>\sum_{i=1}^{N}</td>
<td>$\sum_{i=1}^{N}$</td>
</tr>
<tr>
<td>\prod</td>
<td>$\prod$</td>
<td>\prod_{i=1}^{N}</td>
<td>$\prod_{i=1}^{N}$</td>
</tr>
<tr>
<td>\coprod</td>
<td>$\coprod$</td>
<td>\coprod_{i=1}^{N}</td>
<td>$\coprod_{i=1}^{N}$</td>
</tr>
<tr>
<td>\int</td>
<td>$\int$</td>
<td>\int_{a}^{b}</td>
<td>$\int_{a}^{b}$</td>
</tr>
<tr>
<td>\iint</td>
<td>$\iint$</td>
<td>\iint_{a}^{b}</td>
<td>$\iint_{a}^{b}$</td>
</tr>
<tr>
<td>\bigcup</td>
<td>$\bigcup$</td>
<td>\bigcup_{i=1}^{N}</td>
<td>$\bigcup_{i=1}^{N}$</td>
</tr>
<tr>
<td>\bigcap</td>
<td>$\bigcap$</td>
<td>\bigcap_{i=1}^{N}</td>
<td>$\bigcap_{i=1}^{N}$</td>
</tr>
<tr>
<td>\lim_{n\rightarrow+\infty}</td>
<td>$lim_{n\rightarrow+\infty}$</td>
<td>\lim_{n\rightarrow+\infty}\frac{1}{n(n+1)}</td>
<td>$\lim_{n\rightarrow+\infty}\frac{1}{n(n+1)}$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="数字模式重音符"><a href="#数字模式重音符" class="headerlink" title="数字模式重音符"></a>数字模式重音符</h3><div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\hat{a}</td>
<td>$\hat{a}$</td>
<td></td>
<td>\dot{a}</td>
<td>$\dot{a}$</td>
</tr>
<tr>
<td>\grave{a}</td>
<td>$\grave{a}$</td>
<td></td>
<td>\acute{a}</td>
<td>$\acute{a}$</td>
</tr>
<tr>
<td>\bar{a}</td>
<td>$\bar{a}$</td>
<td></td>
<td>\tilde{a}</td>
<td>$\tilde{a}$</td>
</tr>
<tr>
<td>\check{a}</td>
<td>$\check{a}$</td>
<td></td>
<td>\ddot{a}</td>
<td>$\ddot{a}$</td>
</tr>
<tr>
<td>\widehat{A}</td>
<td>$\widehat{A}$</td>
<td></td>
<td>\widetilde{A}</td>
<td>$\widetilde{A}$</td>
</tr>
<tr>
<td>\vec{a}</td>
<td>$\vec{a}$</td>
<td></td>
<td>\breve{a}</td>
<td>$\breve{a}$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="连线符号"><a href="#连线符号" class="headerlink" title="连线符号"></a>连线符号</h3><div class="table-container">
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\overline{a+b+c+d}</td>
<td>$\overline{a+b+c+d}$</td>
</tr>
<tr>
<td>\underline{a+b+c+d}</td>
<td>$\underline{a+b+c+d}$</td>
</tr>
<tr>
<td>\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}</td>
<td>$\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><p>命令</p>
<p>\left|\begin{array}{cccc}<br>     1 &amp; 6 &amp; 9 \\\<br>     7 &amp; 9 &amp; 0 \\\<br>     9 &amp; 5 &amp; 0<br>\end{array}\right|</p>
<p>显示：</p>
<script type="math/tex; mode=display">
\left|\begin{array}{cccc}
     1 & 6 & 9 \\ 
     7 & 9 & 0 \\
     9 & 5 & 0
\end{array}\right|</script><h3 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h3><p><a href="https://mathpix.com/" target="_blank" rel="noopener">https://mathpix.com/</a></p>
<p><a href="https://houmin.cc/posts/fedfc052/" target="_blank" rel="noopener">https://houmin.cc/posts/fedfc052/</a></p>
]]></content>
      <categories>
        <category>soft skill</category>
      </categories>
      <tags>
        <tag>速查</tag>
        <tag>latex</tag>
      </tags>
  </entry>
  <entry>
    <title>bert系列</title>
    <url>/2020/04/13/bert%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="BERT-及其后续模型"><a href="#BERT-及其后续模型" class="headerlink" title="BERT 及其后续模型"></a>BERT 及其后续模型</h2><p>了解这些后续的模型，方便在后续的研究和应用中来更好的选择模型。</p>
<h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><p>BERT模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/bert.jpg" alt="avatar"></p>
<p>对比OpenAI GPT(Generative pre-trained transformer)，BERT是双向的Transformer block连接；就像单向rnn和双向rnn的区别，直觉上来讲效果会好一些。</p>
<p>对比ELMo，虽然都是“双向”，但目标函数其实是不同的。ELMo是分别以 $P(w_i|w_1,\cdots,w_{i-1})$ 和$P(w_i|w_{i+1},\cdots,w_n)$作为目标函数，独立训练处两个representation然后拼接，而BERT则是以$P(w_i|w_1,\cdots,w_{i-1},w_{i+1}\cdots,w_n)$  作为目标函数训练LM。</p>
<a id="more"></a>
<h4 id="Task-1-MLM"><a href="#Task-1-MLM" class="headerlink" title="Task 1: MLM"></a>Task 1: MLM</h4><p>由于BERT需要通过上下文信息，来预测中心词的信息，同时又不希望模型提前看见中心词的信息，因此提出了一种 Masked Language Model 的预训练方式，即随机从输入预料上 mask 掉一些单词，然后通过的上下文预测该单词，类似于一个完形填空任务。</p>
<p>在预训练任务中，15%的 Word Piece 会被mask，这15%的 Word Piece 中，80%的时候会直接替换为 [Mask] ，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token</p>
<ul>
<li>没有100%mask的原因<ul>
<li>如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词</li>
</ul>
</li>
<li>加入10%随机token的原因<ul>
<li>Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘</li>
<li>另外编码器不知道哪些词需要预测的，哪些词是错误的，因此被迫需要学习每一个token的表示向量</li>
</ul>
</li>
<li>另外，每个batchsize只有15%的单词被mask的原因，是因为性能开销的问题，双向编码器比单项编码器训练要更慢</li>
</ul>
<h4 id="Task-2-NSP"><a href="#Task-2-NSP" class="headerlink" title="Task 2: NSP"></a>Task 2: NSP</h4><p>仅仅一个MLM任务是不足以让 BERT 解决阅读理解等句子关系判断任务的，因此添加了额外的一个预训练任务，即 Next Sequence Prediction。</p>
<p>具体任务即为一个句子关系判断任务，即判断句子B是否是句子A的下文，如果是的话输出’IsNext‘，否则输出’NotNext‘。</p>
<p>训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在图4中的[CLS]符号中.</p>
<h4 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h4><p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/bert_input.jpg" alt="avatar"></p>
<ul>
<li>Token Embeddings：即传统的词向量层，每个输入样本的首字符需要设置为[CLS]，可以用于之后的分类任务，若有两个不同的句子，需要用[SEP]分隔，且最后一个字符需要用[SEP]表示终止</li>
<li>Segment Embeddings：为[0,1]序列，用来在NSP任务中区别两个句子，便于做句子关系判断任务</li>
<li>Position Embeddings：与Transformer中的位置向量不同，BERT中的位置向量是直接训练出来的</li>
</ul>
<h4 id="Fine-tunninng"><a href="#Fine-tunninng" class="headerlink" title="Fine-tunninng"></a>Fine-tunninng</h4><p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/bert_finetune.jpg" alt="avatar"></p>
<p>对于不同的下游任务，我们仅需要对BERT不同位置的输出进行处理即可，或者直接将BERT不同位置的输出直接输入到下游模型当中。具体的如下所示：</p>
<ul>
<li>对于情感分析等单句分类任务，可以直接输入单个句子（不需要[SEP]分隔双句），将[CLS]的输出直接输入到分类器进行分类</li>
<li>对于句子对任务（句子关系判断任务），需要用[SEP]分隔两个句子输入到模型中，然后同样仅须将[CLS]的输出送到分类器进行分类</li>
<li>对于问答任务，将问题与答案拼接输入到BERT模型中，然后将答案位置的输出向量进行二分类并在句子方向上进行softmax（只需预测开始和结束位置即可）</li>
<li>对于命名实体识别任务，对每个位置的输出进行分类即可，如果将每个位置的输出作为特征输入到CRF将取得更好的效果。</li>
</ul>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>考虑双向信息（上下文信息）</li>
<li>不用考虑很长的时序问题（梯度弥散，梯度爆炸）：long-term dependency</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>BERT的预训练任务MLM使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务(训练数据缺乏mask)</li>
<li>缺乏生成能力</li>
<li>另外，BERT没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计。（朴素贝叶斯，naive）</li>
<li>由于最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）</li>
<li>适合处理自然语义理解类任务(NLU)，而不适合自然语言生成类任务(NLG)</li>
</ul>
<h4 id="torch使用"><a href="#torch使用" class="headerlink" title="torch使用"></a>torch使用</h4><ol>
<li>导入相关库</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> transformers <span class="keyword">as</span> ppb <span class="comment"># pytorch transformers</span></span><br></pre></td></tr></table></figure>
<ol>
<li>导入预训练好的 DistilBERT 模型与分词器</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, <span class="string">'distilbert-base-uncased'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Want BERT instead of distilBERT? Uncomment the following line:</span></span><br><span class="line"><span class="comment">#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')# </span></span><br><span class="line"></span><br><span class="line">Load pretrained model/tokenizertokenizer = tokenizer_class.from_pretrained(pretrained_weights)</span><br><span class="line">model = model_class.from_pretrained(pretrained_weights)</span><br></pre></td></tr></table></figure>
<ol>
<li>tokenize：分词+[cls]+[sep]<br><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/bert_tokenize.png" alt="avatar"></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.encode(<span class="string">'a visually stunning rumination on love'</span>, add_special_tokens=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>padding</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_len = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tokenized.values:</span><br><span class="line"> <span class="keyword">if</span> len(i) &gt; max_len:</span><br><span class="line">     max_len = len(i)</span><br><span class="line">padded = np.array([i + [<span class="number">0</span>]*(max_len-len(i)) <span class="keyword">for</span> i <span class="keyword">in</span> tokenized.values])</span><br></pre></td></tr></table></figure>
<ol>
<li>masking</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">attention_mask = np.where(padded != <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>使用bert进行编码</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># last_hidden_states = [batch_size, max_sentence_len, 768]</span></span><br><span class="line">    last_hidden_states = model(input_ids, attention_mask=attention_mask)</span><br></pre></td></tr></table></figure>
<p><a href="http://fancyerii.github.io/2019/03/09/bert-codes/#dataprocessor" target="_blank" rel="noopener">bert代码阅读</a></p>
<hr>
<p>谷歌对BERT进行了改版，下面将对改版后的bert进行学习，对比改版前后主要的相似点和不同点，以便可以选择在研究或应用中使用哪一种。提出的几种方法改进BERT的预测指标或计算速度，但是始终达不到两者兼顾。<strong>XLNet和RoBERTa改善了性能，而DistilBERT提高了推理速度</strong>。</p>
<h3 id="BERT-WWM"><a href="#BERT-WWM" class="headerlink" title="BERT-WWM"></a><strong>BERT-WWM</strong></h3><p><a href>论文</a></p>
<h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>wwm 即 Whole Word Masking（对全词进行Mask）。相比于bert的改进是用Mask标签替换一个完整的词而不是字，中文和英文不同，英文最小的token是一个单词，而中文中最小的token却是字，词是由一个或多个字组成，且每个词之间没有明显的分割，包含更多信息的是词，对全词mask就是对整个词都通过mask进行掩码。<br><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/bert_wwm_exam.png" alt="avatar"></p>
<h4 id="Bert-wwm-ext"><a href="#Bert-wwm-ext" class="headerlink" title="Bert-wwm-ext"></a><strong>Bert-wwm-ext</strong></h4><p>它是BERT-wwm的一个升级版，相比于BERT-wwm的改进是增加了训练数据集同时也增加了训练步数。<br>BERT-wwm-ext主要是有两点改进：<br>1）预训练数据集做了增加，达到5.4B；<br>2）训练步数增大，训练第一阶段1M步，训练第二阶段400K步。</p>
<p><a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">模型下载</a></p>
<h3 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h3><p>XLNet作为bert的升级模型，主要在以下三个方面进行了优化:</p>
<ul>
<li>采用AR模型替代AE模型，解决mask带来的负面影响</li>
<li>双流注意力机制</li>
<li>引入transformer-xl</li>
</ul>
<h4 id="AR与AE语言模型"><a href="#AR与AE语言模型" class="headerlink" title="AR与AE语言模型"></a>AR与AE语言模型</h4><p>目前主流的nlp预训练模型包括两类 Auto Regressive (<strong>AR</strong>) Language Model 与Auto Encoding (AE) Language Model。</p>
<h5 id="AR模型"><a href="#AR模型" class="headerlink" title="AR模型"></a>AR模型</h5><p>AR模型的主要任务在于评估语料的概率分布，例如，给定一个序列 $X=(x_1, \cdots ,x_T)$ ，AR模型就是在计算其极大似然估计$p(X)=\prod_{t=1}^Tp(x_t∣x_{<t})$即已知$x_t$之前的序列，预测$x_t$ 的值，当然也可以反着来$p(x)="\prod_{t=1}^Tp(x_t∣x_{">t})$即已知$x_t$之后的序列，预测$x_t$的值。AR模型一个很明显的缺点就是：模型是单向的，我们更希望的是根据上下文来预测目标，而不单是上文或者下文，之前open AI提出的GPT就是采用的AR模式，包括GPT2.0也是该模式。</t})$即已知$x_t$之前的序列，预测$x_t$></p>
<p>优点：</p>
<ol>
<li>具备生成能力</li>
<li>考虑了相关性</li>
<li>无监督</li>
<li>严格的数学表达</li>
</ol>
<p>缺点：</p>
<ol>
<li>单向的</li>
<li>（难考虑 long term dependency）</li>
</ol>
<h5 id="AE模型"><a href="#AE模型" class="headerlink" title="AE模型"></a>AE模型</h5><p>AE模型采用的就是以上下文的方式，最典型的成功案例就是bert。简单回顾下bert的预训练阶段，预训练包括了两个任务，Masked Language Model与Next Sentence Prediction，Next Sentence Prediction即判断两个序列的推断关系，Masked Language Model采用了一个标志位[MASK]来随机替换一些词，再用[MASK]的上下文来预测[MASK]的真实值，bert的最大问题也是处在这个MASK的点，因为在微调阶段，没有MASK这就导致预训练和微调数据的不统一，从而引入了一些人为误差。</p>
<p>在xlnet中，采用了<strong>AR模型</strong>，但是怎么解决这个上下文的问题呢？</p>
<h4 id="排列语言模型-为了考虑上下文信息"><a href="#排列语言模型-为了考虑上下文信息" class="headerlink" title="排列语言模型(为了考虑上下文信息)"></a>排列语言模型(为了考虑上下文信息)</h4><p>为了解决上文提到的问题，作者提出了排列语言模型，该模型不再对传统的AR模型的序列的值按顺序进行建模，而是最大化所有可能的序列的因式分解顺序的期望对数似然，这句话可能有点不好理解，举个栗子，假如我们有一个序列$[1,2,3,4]$，如果我们的预测目标是3，对于传统的AR模型来说，结果是$p(3)=\prod^3_{t=1}p(3|x_{&lt;t})$，如果采用本文的方法，先对该序列进行因式分解，最终会有24种排列方式，下图是其中可能的四种情况，对于第一种情况因为3的左边没有其他的值，所以该情况无需做对应的计算，第二种情况3的左边还包括了2与4，所以得到的结果是$p(3)=p(3|2)p(3|2,4)$，后续的情况类似，这样处理过后不但保留了序列的上下文信息，也<strong>避免了采用mask标记位，巧妙的改进了bert与传统AR模型的缺点</strong>。<br><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/xlnet.png" alt="avatar"></p>
<h5 id="如何训练？"><a href="#如何训练？" class="headerlink" title="如何训练？"></a>如何训练？</h5><ol>
<li>假如句子长度为20，则会产生$20$!种排列，在这$20$!种排列中随机抽样，得到训练样本。</li>
<li>为获得丰富的上下文语义信息，对抽取的样本最后的几个单词进行预测。</li>
</ol>
<h4 id="基于目标感知表征的双流自注意力-为了解决位置信息"><a href="#基于目标感知表征的双流自注意力-为了解决位置信息" class="headerlink" title="基于目标感知表征的双流自注意力(为了解决位置信息)"></a>基于目标感知表征的<strong>双流自注意力</strong>(为了解决位置信息)</h4><p>虽然排列语言模型能满足目前的目标，但是对于普通的transformer结构来说是存在一定的问题的，为什么这么说呢，假设我们要求这样的一个对数似然，$p_\theta(X_{z_t}|x_{z_{&lt;t}})$，如果采用标准的softmax的话，那么</p>
<script type="math/tex; mode=display">
p_{\theta}\left(X_{z_t} | x_{z_{<t}}\right)=\frac{\exp \left(e(x)^{T} h_{\theta}\left(x_{z_{<t}}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{T} h_{\theta}\left(x_{z_{<t}}\right)\right)}</script><p>其中$h_\theta (x_{z_{&lt;t}})$表示的是添加了mask后的transformer的输出值，可以发现$h_\theta (x_{z_{&lt;t}})$并不依赖于其要预测的内容的位置信息，因为无论预测目标的位置在哪里，因式分解后得到的所有情况都是一样的，并且transformer的权重对于不同的情况是一样的，因此无论目标位置怎么变都能得到相同的分布结果，如下图所示，假如我们的序列index表示为[1,2,3]，对于目标2与3来说，其因式分解后的结果是一样的，那么经过transformer之后得到的结果肯定也是一样的。<br><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/123.png" alt="avatar"></p>
<p>这就导致模型没法得到正确的表述，为了解决这个问题，论文中提出来新的分布计算方法，来实现目标位置感知</p>
<script type="math/tex; mode=display">
p_{\theta}\left(X_{zt}=x | x_{z<t}\right)=\frac{\exp \left(e(x)^{T} g_{\theta}\left(x_{z<t}, z_{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{T} g_{\theta}\left(x_{z<t}, z_{t}\right)\right)}</script><p>其中$g_{\theta}\left(x_{z_{&lt;t}}, z_{t}\right)$是新的表示形式，并且把位置信息$z_t$作为了其输入。<br>这个新的表示形式，论文把该方法称为Two-Stream Self-Attention，双流自注意力，该机制需要解决了两个问题:</p>
<ul>
<li>如果目标是预测$x_{z_t}$，$g_\theta (x_{z&lt;t},z_t)$那么只能有其位置信息$z_t$ 而不能包含内容信息$x_{z_t}$</li>
<li>如果目标是预测其他tokens即$x_{z_j}$那么应该包含$x_{z_t}$的内容信息这样才有完整的上下文信息</li>
</ul>
<p>传统的transformer并不满足这样的需求，因此作者采用了两种表述来代替原来的表述，这也是为什么称为<strong>双流</strong>的原因:</p>
<ul>
<li><p>content representation内容表述，即$h_\theta (x_{z_{\leq t}})$下文用$h_{z_t}$表示，该表述和传统的transformer一样，同时编码了上下文和$x_{z_t}$自身</p>
<script type="math/tex; mode=display">
h_{z_{t}}^{(m)}=\text {Attention}\left(Q=h_{z_{t}}^{(m-1)}, K V=h_{z \leq t}^{(m-1)} ; \theta\right)</script><p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/content_representation.png" alt="avatar"></p>
</li>
<li><p>query representation查询表述，即$g_\theta (x_{z_{&lt;t}})$，下文用$g_{z_t}$ 表示，该表述包含上下文的内容信息$x_{z_{&lt;t}}$ 和目标的位置信息$z_t$，但是不包括目标的内容信息$x_{z_{t}}$ ，从图中可以看到，K与V的计算并没有包括Q，自然也就无法获取到目标的内容信息，但是目标的位置信息在计算Q的时候保留了下来，</p>
<script type="math/tex; mode=display">
g_{z_{t}}^{(m)}=\text {Attention}\left(Q=g_{z_{t}}^{(m-1)}, K V=h_{z<t}^{(m-1)} ; \theta\right)</script></li>
</ul>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/query_representation.png" alt="avatar"></p>
<p>总的计算过程，首先，第一层的查询流是随机初始化了一个向量即$g_i^{0}=w$，内容流是采用的词向量即$h_i^{0}=e(x_i)$，self-attention的计算过程中两个流的网络权重是共享的，最后在微调阶段，只需要简单的把query stream移除，只采用content stream即可。</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/two_stream.png" alt="avatar"></p>
<h4 id="引入transformer-XL"><a href="#引入transformer-XL" class="headerlink" title="引入transformer XL"></a>引入transformer XL</h4><p>作者还将transformer-xl的两个最重要的技术点应用了进来，即片段循环机制与相对位置编码。</p>
<h5 id="片段循环机制"><a href="#片段循环机制" class="headerlink" title="片段循环机制"></a>片段循环机制</h5><p>ransformer-xl的提出主要是为了解决超长序列的依赖问题，对于普通的transformer由于有一个最长序列的超参数控制其长度（如果不对文本控制，则会极大消耗内存，因为在计算score矩阵的时候，会开辟$n^2$大小的矩阵，对于一篇文本，长度为$10^5$,则需要开辟$10^5 * 10^5 = 10^10$大小的score矩阵，极其消耗内存。），对于特别长的序列就会导致丢失一些信息，transformer-xl就能解决这个问题。我们看个例子，假设我们有一个长度为1000的序列，如果我们设置transformer的最大序列长度是100，那么这个1000长度的序列需要计算十次，并且每一次的计算都没法考虑到每一个段之间的关系，如果采用transformer-xl，首先取第一个段进行计算，然后把得到的结果的隐藏层的值进行缓存，第二个段计算的过程中，把缓存的值拼接起来再进行计算（RNN）。该机制不但能保留长依赖关系还能加快训练，因为每一个前置片段都保留了下来，不需要再重新计算，在transformer-xl的论文中，经过试验其速度比transformer快了1800倍。<br>在xlnet中引入片段循环机制其实也很简单，只需要在计算KV的时候做简单的修改，其中$\tilde{h}^{m-1}$表示的是缓存值。</p>
<script type="math/tex; mode=display">
h_{z_{t}}^{(m)}=\text {Attention}\left(Q=h_{z_{t}}^{(m-1)}, K V=\left[\tilde{h}^{(m-1)}, h_{z \leq t}^{(m-1)}\right] ; \theta\right)</script><h5 id="相对位置编码"><a href="#相对位置编码" class="headerlink" title="相对位置编码"></a>相对位置编码</h5><p>bert的position embedding采用的是绝对位置编码，但是绝对位置编码在transformer-xl中有一个致命的问题，因为没法区分到底是哪一个片段里的，这就导致了一些位置信息的损失，这里被替换为了transformer-xl中的相对位置编码。假设给定一对位置$i$与$j$，如果$i$与$j$是同一个片段里的那么我们令这个片段编码$s_{ij}=s_+$，如果不在一个片段里则令这个片段编码为$s_{ij}=s_-$，这个值是在训练的过程中得到的，也是用来计算attention weight时候用到的，在传统的transformer中$\text {attention weight}= softmax(\frac{Q⋅K}{d}V)$，在引入相对位置编码后，首先要计算出$a_{ij}=(q_i+b)^T_{s_{sj}}$，其中$b$也是一个需要训练得到的偏执量，最后把得到的$a_ij$与传统的transformer的weight相加从而得到最终的attention weight。</p>
<p><a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_xlnet.py" target="_blank" rel="noopener">源码</a></p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>在实验中发现，xlnet更多还是在长文本的阅读理解类的任务上提升更明显一些，这也很符合上文中介绍的这些优化点，在实际工业场景中，机器翻译、本文摘要类的任务应该会有更好的效果，当然在其他的文本分类、自然语言推理等任务上xlnet也有一定的效果提升。nlp领域的模型目前已经完全采用了pretrain+fine tuning的模式，GPT2.0的单向模式在增大训练语料的情况下效果就已经超越了bert，可想而知该领域的研究还有很大的上升空间</p>
<h3 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h3><h4 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h4><p>在XLNet全面超越Bert后没多久，Facebook提出了RoBERTa（a Robustly Optimized BERT Pretraining Approach）。再度在多个任务上达到SOTA。</p>
<p>它在模型层面没有改变Google的Bert，<strong>改变的只是预训练的方法</strong>。</p>
<h4 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h4><p>Yinhan Liu等人[6]认为超参数的选择对最终结果有重大影响，为此他们提出了BERT预训练的重复研究，其中包括对超参数调整和训练集大小的影响的仔细评估。<strong>最终，他们发现了BERT的训练不足</strong>，并提出了一种改进的模型来训练BERT模型。</p>
<h4 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h4><h5 id="静态Masking-vs-动态Masking"><a href="#静态Masking-vs-动态Masking" class="headerlink" title="静态Masking vs 动态Masking"></a>静态Masking vs 动态Masking</h5><p>原来Bert对每一个序列随机选择15%的Tokens替换成[MASK]，为了消除与下游任务的不匹配，还对这15%的Tokens进行（1）80%的概率替换成[MASK]；（2）10%的概率不变；（3）10%的概率替换成其他词。但整个训练过程，这15%的Tokens一旦被选择就不再改变，也就是说从一开始随机选择了这15%的Tokens，之后的N个epoch里都不再改变了。这就叫做静态Masking。</p>
<p>而RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。这就叫做动态Masking。作者在只将静态Masking改成动态Masking，其他参数不变的情况下做了实验，动态Masking确实能提高性能。</p>
<h5 id="with-NSP-vs-without-NSP"><a href="#with-NSP-vs-without-NSP" class="headerlink" title="with NSP vs without NSP"></a>with NSP vs without NSP</h5><p>原本的Bert为了捕捉句子之间的关系，使用了NSP任务进行预训练，就是输入一对句子A和B，判断这两个句子是否是连续的。在训练的数据中，50%的B是A的下一个句子，50%的B是随机抽取的。</p>
<p>而RoBERTa去除了NSP，而是每次输入连续的多个句子，直到最大长度512（可以跨文章）。这种训练方式叫做（FULL - SENTENCES），而原来的Bert每次只输入两个句子。实验表明在MNLI这种推断句子关系的任务上RoBERTa也能有更好性能。（？？？？）</p>
<h5 id="更大的mini-batch"><a href="#更大的mini-batch" class="headerlink" title="更大的mini-batch"></a>更大的mini-batch</h5><p>原本的BERTbase 的batch size是256，训练1M个steps。RoBERTa的batch size为8k。为什么要用更大的batch size呢？作者借鉴了在机器翻译中，用更大的batch size配合更大学习率能提升模型优化速率和模型性能的现象，并且也用实验证明了确实Bert还能用更大的batch size。</p>
<h5 id="更多的数据，更长的训练时间"><a href="#更多的数据，更长的训练时间" class="headerlink" title="更多的数据，更长的训练时间"></a>更多的数据，更长的训练时间</h5><p>借鉴XLNet用了比Bert多10倍的数据，RoBERTa也用了更多的数据。性能确实再次彪升。当然，也需要配合更长时间的训练。</p>
<h5 id="Byte-Pair-Encoding-BPE-字符编码"><a href="#Byte-Pair-Encoding-BPE-字符编码" class="headerlink" title="Byte-Pair Encoding (BPE)字符编码"></a>Byte-Pair Encoding (BPE)字符编码</h5><p>使用Sennrich[8]等人提出的Byte-Pair Encoding (BPE)字符编码，它是字符级和单词级表示之间的混合体，可以处理自然语言语料库中常见的大词汇，避免训练数据出现更多的“[UNK]”标志符号，从而影响预训练模型的性能。其中，“[UNK]”标记符表示当在BERT自带字典vocab.txt找不到某个字或者英文单词时，则用“[UNK]”表示。</p>
<h3 id="SpanBERT"><a href="#SpanBERT" class="headerlink" title="SpanBERT"></a>SpanBERT</h3><p><a href="https://arxiv.org/pdf/1907.10529.pdf" target="_blank" rel="noopener">论文</a></p>
<h4 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h4><p>在许多 NLP 任务中都涉及对多个文本分词间关系的推理。例如，在抽取式问答任务中，在回答问题“Which NFL team won Super Bown 50?”时，判断“Denver Broncos” 是否属于“NFL team”是非常重要的步骤。相比于在已知“Broncos”预测“Denver”的情况，直接预测“Denver Broncos”难度更大，这意味着这类分词对自监督任务提出了更多的挑战。</p>
<p>SpanBert对 BERT 模型进行了如下改进：</p>
<ol>
<li>没有segment embedding，只有一个长的句子，类似RoBERTa。</li>
<li><strong>Span Masking</strong>:对随机的邻接分词（span）而非随机的单个词语（token）添加mask；</li>
<li>MLM+SBO:通过使用分词边界的表示来预测被添加mask的分词的内容，不再依赖分词内单个 token 的表示。</li>
</ol>
<p>SpanBERT 能够对分词进行更好地表示和预测。该模型和 BERT 在mask机制和训练目标上存在差别。首先，SpanBERT 不再对随机的单个 token 添加mask，而是对随机对邻接分词添加mask。其次，SpanBert提出了一个新的训练目标 span-boundary objective (SBO) 进行模型训练。通过对分词添加mask，作者能够使模型依据其所在语境预测整个分词。另外，SBO 能使模型在边界词中存储其分词级别的信息，使得模型的调优更佳容易。</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/spanbert.png" alt="avatar"></p>
<p>模型使用边界词 was和 to来预测分词中的每个单词。</p>
<p>为了搭建 SpanBERT ，作者首先构建了一个 BERT 模型的并进行了微调，SpanBERT的表现优于原始 BERT 模型。在搭建baseline的时候，作者发现对单个部分进行预训练的效果，比使用 next sentence prediction (NSP) 目标对两个长度为一半的部分进行训练的效果更优，在下游任务中表现尤其明显。因此，作者在经过调优的 BERT 模型的顶端对模型进行了改进。</p>
<h4 id="Span-Masking"><a href="#Span-Masking" class="headerlink" title="Span Masking(????)"></a>Span Masking(????)</h4><p>对于每一个单词序列 $X = (x_1, \ldots , x_n )$，作者通过迭代地采样文本的分词选择单词，直到达到掩膜要求的大小（例如 X 的 15%），并形成 X 的子集 Y。在每次迭代中，作者首先从几何分布 $l \sim Geo(p)$中采样得到分词的长度，该几何分布是偏态分布，偏向于较短的分词。之后，作者随机（均匀地）选择分词的起点。</p>
<p>根据几何分布，先随机选择一段（span）的长度，之后再根据均匀分布随机选择这一段的起始位置，最后按照长度遮盖。作者设定几何分布取 p=0.2，并裁剪最大长度只能是 10（不应当是长度 10 以上修剪，而应当为丢弃），利用此方案获得平均采样长度分布。因此分词的平均长度为 3.8 。作者还测量了词语（word）中的分词程度，使得添加掩膜的分词更长。下图展示了分词掩膜长度的分布情况。</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/spanbert_fenci.png" alt="avatar"></p>
<p>和在 BERT 中一样，作者将 Y 的规模设定为 X 的15%，其中 80% 使用 [MASK] 进行替换，10% 使用随机单词替换，10%保持不变。与之不同的是，作者是在分词级别进行的这一替换，而非将每个单词单独替换。</p>
<h4 id="分词边界目标-SBO"><a href="#分词边界目标-SBO" class="headerlink" title="分词边界目标(SBO)???"></a>分词边界目标(SBO)???</h4><p>分词选择模型一般使用其边界词创建一个固定长度的分词表示。为了于该模型相适应，作者希望结尾分词的表示的总和与中间分词的内容尽量相同。为此，作者引入了 SBO ，其仅使用观测到的边界词来预测带掩膜的分词的内容。</p>
<p>具体做法是，在训练时取 Span 前后边界的两个词，值得指出，这两个词不在 Span 内，然后<strong>用这两个词向量加上 Span 中被遮盖掉词的位置向量，来预测原词</strong>。</p>
<script type="math/tex; mode=display">
\mathbf{y}_{i}=f\left(\mathbf{x}_{s-1}, \mathbf{x}_{e+1}, \mathbf{p}_{i}\right)</script><p>详细做法是将词向量和位置向量拼接起来，作者使用一个两层的前馈神经网络作为表示函数，该网络使用 GeLu 激活函数，并使用层正则化：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{h} &=\text { LayerNorm }\left(\operatorname{GeLU}\left(W_{1} \cdot\left[\mathbf{x}_{s-1} ; \mathbf{x}_{e+1} ; \mathbf{p}_{i}\right]\right)\right) \\
f(\cdot) &=\text { LayerNorm }\left(\operatorname{GeLU}\left(W_{2} \cdot \mathbf{h}\right)\right)
\end{aligned}</script><p>作者使用向量表示$y_i$来预测$x_i$，并和 MLM 一样使用交叉熵作为损失函数，就是 SBO 目标的损失，之后将这个损失和 BERT 的 <strong>Mased Language Model （MLM）</strong>的损失加起来，一起用于训练模型。</p>
<script type="math/tex; mode=display">
\mathcal{L}(\text { football })=\mathcal{L}_{\mathrm{MLM}}\left(\mathbf{x}_{7}\right)+\mathcal{L}_{\mathrm{SBO}}\left(\mathbf{x}_{4}, \mathbf{x}_{9}, \mathbf{p}_{7}\right)</script><h4 id="单序列训练"><a href="#单序列训练" class="headerlink" title="单序列训练"></a>单序列训练</h4><p>SpanBERT 没用 Next Sentence Prediction (NSP) 任务，而是直接用 Single-Sequence Training，也就是不加入 NSP 任务来判断是否两句是上下句，直接用一句来训练，片段长度最多为512个单词。</p>
<h4 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h4><ol>
<li>训练时用了 <strong>Dynamic Masking</strong> 而不是像 BERT 在预处理时做 静态Mask。</li>
<li><strong>取消 BERT 中随机采样短句的策略</strong></li>
<li>对Adam优化器中的一些参数做了改变。</li>
</ol>
<h5 id="mask机制"><a href="#mask机制" class="headerlink" title="mask机制"></a>mask机制</h5><p>作者在子单词、完整词语、命名实体、名词短语和随机分词方面进行了比较：发现使用随机分词掩膜机制效果更优。</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/spanbert_token.png" alt="avatar"></p>
<h5 id="辅助目标"><a href="#辅助目标" class="headerlink" title="辅助目标"></a>辅助目标</h5><p>使用 SBO 替换 NSP 并使用单序列进行预测的效果更优。</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/spanbert_a_o.png" alt="avatar"></p>
<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><p>SpanBERT是基于分词的预训练模型，在多个评测任务中的得分都超越了 BERT 且在分词选择类任务中的提升尤其明显。</p>
<h3 id="ERNIE"><a href="#ERNIE" class="headerlink" title="ERNIE"></a>ERNIE</h3><p><a href="https://arxiv.org/pdf/1905.07129" target="_blank" rel="noopener">论文</a></p>
<h4 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h4><p>无论是Elmo、GPT, 还是能力更强的 BERT 模型，其建模对象主要聚焦在原始语言信号上，较少利用语义知识单元建模。这个问题在中文方面尤为明显，例如，BERT 在处理中文语言时，通过预测汉字进行建模，模型很难学出更大语义单元的完整语义表示。例如，对于乒 [mask] 球，清明上 [mask] 图，[mask] 颜六色这些词，BERT 模型通过字的搭配，很容易推测出掩码的字信息，但没有显式地对语义概念单元 (如乒乓球、清明上河图) 以及其对应的语义关系进行建模。如果能够让模型学习到海量文本中蕴含的潜在知识，势必会进一步提升各个 NLP 任务效果。</p>
<p>ERNIE 模型通过建模海量数据中的实体概念等先验语义知识，学习真实世界的语义关系。具体来说，ERNIE 模型通过对词、实体等语义单元的掩码，使得模型学习完整概念的语义表示。相较于 BERT 学习原始语言信号，ERNIE 直接对先验语义知识单元进行建模，增强了模型语义表示能力。</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/ernie.png" alt="avatar"></p>
<p>在 BERT 模型中，通过『哈』与『滨』的局部共现，即可判断出『尔』字，模型没有学习与『哈尔滨』相关的知识。而 ERNIE 通过学习词与实体的表达，使模型能够建模出『哈尔滨』与『黑龙江』的关系，学到『哈尔滨』是『黑龙江』的省会以及『哈尔滨』是个冰雪城市。</p>
<h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/ernie_ar.png" alt="avatar"></p>
<p>模型上主要的改进是在bert的后段加入了实体向量和经过bert编码后的向量拼接，另外在输出时多加了实体自编码的任务，从而帮助模型注入实体知识信息。</p>
<h5 id="T-Encoder"><a href="#T-Encoder" class="headerlink" title="T-Encoder"></a>T-Encoder</h5><p>这部分就是纯粹的bert结构，在该部分模型中主要负责对输入句子（token embedding, segment embedding和positional embedding）进行编码.</p>
<h5 id="K-Encoder"><a href="#K-Encoder" class="headerlink" title="K-Encoder"></a>K-Encoder</h5><p>引入实体信息，使用了TransE训练实体向量，再通过多头Attention进行编码，然后通过实体对齐技术，将实体向量加入到对应实体的第一个token编码后的向量上。（例如姚明是一个实体，在输入时会被分割成姚、明，最后在这部分引入的姚明这个实体的向量会被加入到“姚”这个字经过bert之后的向量上去）</p>
<script type="math/tex; mode=display">
\begin{aligned}
\left\{\tilde{\boldsymbol{w}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{w}}_{n}^{(i)}\right\} &=\mathrm{MH}-\mathrm{ATT}\left(\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}\right) \\
\left\{\tilde{\boldsymbol{e}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{e}}_{m}^{(i)}\right\} &=\mathrm{MH}-\mathrm{ATT}\left(\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}\right)
\end{aligned}</script><p>由于直接拼接后向量维度会与其他未拼接的向量维度不同，所以加入information fusion layer，另外考虑到后面的实体自编码任务，所以这里在融合信息之后，有实体向量加入的部分需要另外多输出一个实体向量。</p>
<p>加入实体信息之后的融合输出过程：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{h}_{j} &=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{W}}_{e}^{(i)} \tilde{\boldsymbol{e}}_{k}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\
\boldsymbol{w}_{j}^{(i)} &=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \\
\boldsymbol{e}_{k}^{(i)} &=\sigma\left(\boldsymbol{W}_{e}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{e}^{(i)}\right)
\end{aligned}</script><p>未加入实体信息之后的融合输出过程：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{h}_{j} &=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\
\boldsymbol{w}_{j}^{(i)} &=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right)
\end{aligned}</script><h5 id="实体自编码"><a href="#实体自编码" class="headerlink" title="实体自编码????"></a>实体自编码????</h5><p>为了更好地使用实体信息，作者在这里多加入了一个预训练任务 entity auto-encoder(dEA)。</p>
<script type="math/tex; mode=display">
p\left(e_{j} | w_{i}\right)=\frac{\exp \left(\operatorname{linear}\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{j}\right)}{\sum_{k=1}^{m} \exp \left(\operatorname{linear}\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{k}\right)}</script><p>该预训练任务和bert相似，按以下方式进行训练：</p>
<ol>
<li>15% 的情况下：用 [MASK] 替换被选择的单词，例如，my dog is hairy → my dog is [MASK]</li>
<li>5% 的情况下：用一个随机单词替换被选择的单词，例如，my dog is hairy → my dog is apple</li>
<li>80% 的情况下：保持被选择的单词不变，例如，my dog is hairy → my dog is hairy</li>
</ol>
<h5 id="Extra-task"><a href="#Extra-task" class="headerlink" title="Extra-task"></a>Extra-task</h5><p>除了正常的任务之外，ERNIE引入了两个新任务Entity Typing和Relation Classification</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/ernie_extra_task.png" alt="avatar"></p>
<h5 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h5><p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/ernie_experiment.png" alt="avatar"></p>
<p>ERNIE在通用任务其实相比bert优势不大，尽管文章提到了ERNIE更鲁棒以及GLUE的评估对ERNIE不是很友好，在增加复杂度的同时，并没有取得期待的效果。ERNIE提供了一种很好的实体信息引入思路，并且其新提出的预训练方法也给希望将bert这一模型引入关系抽取领域提供了很好的例子。</p>
<h3 id="MT-DNN（预训练-多任务）"><a href="#MT-DNN（预训练-多任务）" class="headerlink" title="MT-DNN（预训练+多任务）"></a>MT-DNN（预训练+多任务）</h3><p><a href="https://arxiv.org/pdf/1901.11504.pdf" target="_blank" rel="noopener">论文</a>,<a href="https://github.com/namisan/mt-dnn" target="_blank" rel="noopener">源码</a></p>
<h4 id="简介-4"><a href="#简介-4" class="headerlink" title="简介"></a>简介</h4><p>学习文本的向量空间表示，是许多自然语言理解(NLU)任务的基础。两种流行的方法是<strong>多任务学习</strong>和<strong>语言模型的预训练</strong>。MT-DNN通过提出一种新的多任务深度神经网络（MT-DNN）来综合两种方法的优点。</p>
<p><strong>预训练</strong>，如BERT、GPT等，就不多说了。</p>
<p><strong>多任务</strong>学习( MTL )的灵感来自于人的学习活动。在人类的学习活动中，人们经常应用从以前的任务中学到的知识来帮助学习新的任务。例如，在学习滑冰这件事情上，一个知道如何滑雪的人比什么都不知道的人容易。同样，<strong>联合学习多个(相关)</strong>任务也很有用，这样在一个任务中学习到的知识有益于其他任务。对于MTL（Multi-task Learning，多任务学习）来说，其优点有两个：1）弥补了有些任务的数据不足问题；2）有正则的作用，防止模型过拟合。</p>
<p><strong>两者结合</strong>（强强联手！）：MT-DNN认为，MTL和pretrain有很好的互补作用，那么是不是可以结合一下，发挥两者的作用。更具体的就是，先用BERT进行pretrain，然后用MTL进行finetune，这就形成了MT-DNN。可见，与BERT的不同在于finetune的过程，这里用MTL作为目标。</p>
<h4 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h4><p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/mtdnn.png" alt="avatar"></p>
<p>表征学习 MT-DNN 模型的架构。下面的网络层在所有任务中都共享，上面的两层是针对特定任务。输入$X$(一句话或句子对)首先表征为一个序列的嵌入向量，在$l_1$  中每个词对应一个向量。然后 Transformer 编码器捕捉每个单词的语境信息并在$l_2$中生成共享的语境嵌入向量。最后，针对每个任务，特定任务层生成特定任务的表征，而后是分类、相似性打分、关联排序等必需的操作。</p>
<h5 id="多任务"><a href="#多任务" class="headerlink" title="多任务"></a>多任务</h5><p>MT-DNN是结合了4种类型的NLU任务：单句分类、句子对分类、文本相似度打分和相关度排序。</p>
<ul>
<li>单句分类：CoLA任务是预测英语句子是否合乎语法，SST-2任务预测电影评论是正向还是负向。</li>
<li>文本相似度：这是一个回归任务。对于给定的句子对，模型计算二者之间的相似度。在GLUE中只有STS-B这个任务是处理文本相似度。</li>
<li>成对文本分类（文本蕴含）：对于给定的句子对，推理两个句子之间的关系。RET和MNLI是语言推理任务，推理句子之间是否存在蕴含关系、矛盾的关系或者中立关系。QQP和MRPC是预测句子是否语义等价。</li>
<li>相关性排序：给定一个问题和一系列候选答案，模型根据问题对所有候选答案进行排序。QNLI是斯坦福问答数据集的一个版本，任务是预测候选答案中是否包含对问题的正确答案。尽管这是一个二分类任务，但我们依旧把它当作排序任务，因为模型重排了候选答案，将正确答案排在更前。</li>
</ul>
<h6 id="单句分类"><a href="#单句分类" class="headerlink" title="单句分类"></a>单句分类</h6><p>用[CLS]的表征作为特征，设为$x$，则对于单句的分类任务，直接在后面接入一个分类层即可.</p>
<script type="math/tex; mode=display">
P_{r}(c | X)=\operatorname{softmax}\left(W^{T} \cdot x\right)</script><p>loss是交叉熵损失：</p>
<script type="math/tex; mode=display">
-\sum_{c} I(X, c) \log \left(P_{r}(c | X)\right)</script><h6 id="句子相似度"><a href="#句子相似度" class="headerlink" title="句子相似度"></a>句子相似度</h6><p>将两句话pack后送进去，得到的[CLS]的表征，可拿出来计算分数：</p>
<script type="math/tex; mode=display">
\operatorname{sim}\left(X_{1}, X_{2}\right)=\operatorname{sigmoid}\left(w^{T} \cdot x\right)</script><p>loss是MSE：</p>
<script type="math/tex; mode=display">
\left(y-\operatorname{Sim}\left(X_{1}, X_{2}\right)\right)^{2}</script><h6 id="句子对分类"><a href="#句子对分类" class="headerlink" title="句子对分类"></a>句子对分类</h6><p>对这个任务不熟悉。。。</p>
<h6 id="相关性排序"><a href="#相关性排序" class="headerlink" title="相关性排序"></a>相关性排序</h6><p>先计算两个句子之间的相似度，输入两个句子pack，采用[CLS]的输出作为表征。</p>
<script type="math/tex; mode=display">
\operatorname{Rel}(Q, A)=g\left(w^{T} \cdot x\right)</script><p>loss采用排序损失：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&-\sum_{Q, A^{+}} P_{r}\left(A^{+} | Q\right) \\
P_{r}\left(A^{+} | Q\right) &=\frac{\exp \left(\gamma \operatorname{Rel}\left(Q, A^{+}\right)\right)}{\sum_{A^{\prime} \in A} \exp \left(\gamma \operatorname{Rel}\left(Q, A^{\prime}\right)\right)}
\end{aligned}</script><h5 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h5><p>MT-DNN 的训练程序包含两个阶段：预训练和多任务微调。</p>
<ol>
<li>预训练阶段遵循 BERT 模型的方式。lexicon encoder和Transformer encoder参数的学习是通过两个无监督预测任务：掩码语言建模(masked language modeling)和下一句预测(next sentence pre-<br>diction)。</li>
<li>在多任务微调阶段，使用基于minibatch的随机梯度下降（SGD）来学习模型参数（也就是，所有共享层和任务特定层的参数），算法流程如下图所示。每个epoch，选择一个mini-batch $b_t$(在9个GLUE任务中)，再对特定任务$k$进行模型参数的更新。这近似地优化所有多任务的和。</li>
</ol>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/mtdnn_al.png" alt="avatar"></p>
<h3 id="DistilBERT"><a href="#DistilBERT" class="headerlink" title="DistilBERT"></a>DistilBERT</h3><p><a href="https://arxiv.org/pdf/1910.01108" target="_blank" rel="noopener">论文</a></p>
<h4 id="动机-1"><a href="#动机-1" class="headerlink" title="动机"></a>动机</h4><p>预训练的语言模型正变得日益庞大，这些庞大的模型尽管能够带来准确率的提升，但是在这些预训练模型投入使用的过程中，往往需要对模型进行微调，而这需要大量的资源，而且模型投入使用的时候，由于计算量巨大，模型处理数据的时延过长。一种比较好的解决方案是对模型进行压缩。</p>
<h4 id="模型压缩的方法"><a href="#模型压缩的方法" class="headerlink" title="模型压缩的方法"></a>模型压缩的方法</h4><ol>
<li><p>量化</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/quantization.jpg" alt="avatar"></p>
</li>
</ol>
<p>量化方法意味着降低模型权重的精度。比如K-means 量化方法：对模型权重矩阵W进行分组，分成n簇，然后把权重矩阵里的值转化成$1,\cdots,n$的正数。通过这种方式把矩阵中的32位的float型转化成只有8位（或者1位，binarizing matrix）的整数。</p>
<h5 id="裁剪（Pruning）"><a href="#裁剪（Pruning）" class="headerlink" title="裁剪（Pruning）"></a>裁剪（Pruning）</h5><p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/Pruning.png" alt="avatar"></p>
<h6 id="weight-pruning"><a href="#weight-pruning" class="headerlink" title="weight pruning"></a>weight pruning</h6><p>   把权重矩阵中数值较大的值设置为0，从而构造稀疏矩阵。（理论上稀疏矩阵乘法速度快于normal(danse)矩阵乘法。）</p>
<h6 id="Removing-neurons"><a href="#Removing-neurons" class="headerlink" title="Removing neurons"></a>Removing neurons</h6><p>   直接删除权重矩阵中不重要的的某一行或某一列。（删除和fine-tune交替进行，这样剩下的神经元在一定程度上可以弥补被删除的神经元）</p>
<h6 id="Removing-weight-matrices"><a href="#Removing-weight-matrices" class="headerlink" title="Removing weight matrices"></a>Removing weight matrices</h6><p>   [10]从big transformers中直接删除对accuracy贡献不大的整个attention head。但是这种方式不能保证能够并行加速，因为相邻的权重矩阵的size变的不同了。</p>
<h5 id="知识蒸馏"><a href="#知识蒸馏" class="headerlink" title="知识蒸馏"></a>知识蒸馏</h5><p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/Knowledge-distillation.jpg" alt="avatar"></p>
<p>DistilBert使用模型蒸馏技术：这是一种能够将大型模型（被称为「老师」）压缩为较小模型（即「学生」）的技术。</p>
<p><a href="https://blog.csdn.net/nature553863/article/details/80568658" target="_blank" rel="noopener">知识蒸馏方法</a></p>
<h6 id="知识蒸馏：迁移泛化能力"><a href="#知识蒸馏：迁移泛化能力" class="headerlink" title="知识蒸馏：迁移泛化能力"></a>知识蒸馏：迁移泛化能力</h6><p>知识蒸馏是一种压缩技术，要求对小型模型进行训练，以使其拥有类似于大型模型的行为特征。在监督学习领域，我们在训练分类模型时往往会利用对数似然信号实现概率最大化（logits 的 softmax），进而预测出正确类。在大多数情况下，性能良好的模型能够利用具有高概率的正确类预测输出分布，同时其它类的发生概率则接近于零。</p>
<p><strong>但是，某些“接近于零”的概率要比其它概率更大，这在一定程度上反映出模型的泛化能力</strong>。例如，把普通椅子误认为扶手椅虽然属于错误，但这种错误远比将其误认为蘑菇来得轻微。这种不确定性，有时被称为“暗知识”。也可以从另一个角度来理解蒸馏——用于防止模型对预测结果太过确定（类似于标签平滑）。</p>
<p>在语言建模当中，可以通过查看词汇表中的分布轻松观察到这种不确定性。下图为 BERT 对《卡萨布兰卡》电影当中经典台词下一句用词的猜测：</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/next_word.png" alt="avatar"></p>
<p>BERT 提出的 20 大高概率用词猜测结果。语言模型确定了两个可能性最高的选项（day 与 life），接下来的词汇相比之下概率要低得多。</p>
<h6 id="如何复制这些“暗知识”"><a href="#如何复制这些“暗知识”" class="headerlink" title="如何复制这些“暗知识”"></a>如何复制这些“暗知识”</h6><p>在训练当中，通过训练学生网络，来模拟老师网络的全部输出分布（也就是知识），也就是通过匹配输出分布的方式训练学生网络，从而实现与老师网络相同的泛化方式，即通过软目标（老师概率）将交叉熵从老师处传递给学生。</p>
<script type="math/tex; mode=display">
L=-\sum_{i} t_{i} * \log \left(s_{i}\right)</script><p>其中 $t_i$为来自老师的 $logit$，$s_i$ 为学生的 $logit$。以老师的行为$t_i$作为标签，让学生去学习老师的行为。<strong>这个损失函数属于更丰富的训练信号，因为单一示例要比单一硬目标拥有更高的强制约束效果。</strong>为了进一步揭示分类结果的质量，Hinton 等人提出了 softmax 温度的概念：</p>
<script type="math/tex; mode=display">
p_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}</script><p>T 为温度参数。当$T \rightarrow 0$时，分布变为 Kronecker（相当于独热目标矢量）；当 $T \rightarrow +\infty$时，则变为均匀分布。在训练过程中，将相同的温度参数应用于学生与老师网络，即可进一步为每个训练示例揭示更多信号。T 被设置为 1 即为标准 Softmax。</p>
<p>在蒸馏方面，使用 Kullback-Leibler 损失函数，因为其拥有相同的优化效果：</p>
<script type="math/tex; mode=display">
K L(p \| q)=\mathbb{E}_{p}\left(\log \left(\frac{p}{q}\right)\right)=\sum_{i} p_{i} * \log \left(p_{i}\right)-\sum_{i} p_{i} * \log \left(q_{i}\right)</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line">KD_loss = nn.KLDivLoss(reduction=<span class="string">'batchmean'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kd_step</span><span class="params">(teacher: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">            student: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">            temperature: float,</span></span></span><br><span class="line"><span class="function"><span class="params">            inputs: torch.tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            optimizer: Optimizer)</span>:</span></span><br><span class="line">    teacher.eval()</span><br><span class="line">    student.train()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        logits_t = teacher(inputs=inputs) <span class="comment"># 老师的输出</span></span><br><span class="line">    logits_s = student(inputs=inputs) <span class="comment"># 学生的输出</span></span><br><span class="line">    <span class="comment"># 计算kd_loss</span></span><br><span class="line">    loss = KD_loss(input=F.log_softmax(logits_s/temperature, dim=<span class="number">-1</span>),</span><br><span class="line">                   target=F.softmax(logits_t/temperature, dim=<span class="number">-1</span>))</span><br><span class="line">    </span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<p>利用老师信号，能够训练出一套较小的语言模型 DistilBERT，属于 BERT 的监督产物。</p>
<h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>DistilBERT想较于BERT删除了 token-type 嵌入与 pooler（用于下一句分类任务），但其余部分架构保持不变，而层数也减少至原本的二分之一。总体而言，蒸馏模型 DistilBERT 在总体参数数量上约为 BERT 的一半，但在 GLUE 语言理解基准测试中能够保留 95% 的 BERT 性能表现。</p>
<ol>
<li><strong>为什么不降低隐藏层的大小？</strong><br>将 768 减至 512 ，意味着总参数量约下降至原本的二分之一。在现代框架当中，大多数运算都经过高度优化，而且张量的最终维度（隐藏维度）的变化会对 Transformer 架构（线性分层与层规范化）中的大部分运算产生小幅影响。实验中发现，层数对于推理时间的影响要远高于隐藏层的大小。因此，更小并不代表着一定更快。</li>
</ol>
<p>训练子网络的核心不只是建立架构，还要求为子网络找到正确的初始化方式以实现收敛。 DistilBERT 使用bert的参数进行初始化，将层数削减一半，并采用相同的隐藏大小。DistilBERT还用到了最近 RoBERTa 论文当中提到的一些训练技巧，这也再次证明 BERT 模型的训练方式对其最终表现有着至关重要的影响。与 RoBERTa 类似，对 DIstilBERT 进行大批次训练，使用梯度累积（每批最多 4000 个例子）、配合动态遮挡并删除了下一句预测目标。</p>
<h4 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h4><p>DistilBERT 的表现：保留了BERT  95% 以上的性能，同时将参数减少了 40%。推理时间方面，DistilBERT 比 BERT 快 60%，体积比 BERT 小 60%。</p>
<h3 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h3><p><a href="[PDF](https://arxiv.org/pdf/1909.11942">论文</a>)</p>
<h4 id="简介-5"><a href="#简介-5" class="headerlink" title="简介"></a>简介</h4><p>自BERT的成功以来，预训练模型都采用了很大的参数量以取得更好的模型表现。但是模型参数量越来越大也带来了很多问题，比如对算力要求越来越高、模型需要更长的时间去训练、甚至有些情况下参数量更大的模型表现却更差。为了解决目前预训练模型参数量过大的问题，albert提出了两种能够大幅减少预训练模型参数量的方法，此外还提出用Sentence-order prediction（SOP）任务代替BERT中的Next-sentence prediction（NSP）任务。</p>
<h4 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h4><h5 id="嵌入向量因式分解（Factorized-embedding-parameterization）"><a href="#嵌入向量因式分解（Factorized-embedding-parameterization）" class="headerlink" title="嵌入向量因式分解（Factorized embedding parameterization）"></a>嵌入向量因式分解（Factorized embedding parameterization）</h5><p>在BERT、XLNet、RoBERTa等模型中，由于模型结构的限制，WordePiece embedding的大小$E$ 总是与隐层大小$H$相同，即 $E \equiv H$ 。从建模的角度考虑，词嵌入学习的是单词与上下文无关的表示，而隐层则是学习与上下文相关的表示。显然后者更加复杂，需要更多的参数，也就是说模型应当增大隐层大小$H$，或者说满足 $H \gg E$。但实际上词汇表的大小$V$通常非常大，如果$E=H$的话，增加隐层大小$H$后将会使embedding matrix的维度$V \times E$非常巨大。</p>
<p>因此ALBERT想要打破$E$与$H$ 之间的绑定关系，从而减小模型的参数量，同时提升模型表现。具体做法是将embedding matrix分解为两个大小分别为$V \times  E$ 和$E \times H$ 矩阵，也就是说先将单词投影到一个低维的embedding空间 $E$ ，再将其投影到高维的隐藏空间$H$  。这使得embedding matrix的维度从$O(V \times H)$  减小到$O(v \times E + E \times H)$ 。当 $H \gg E$时，参数量减少非常明显。在实现时，随机初始化$V\times E$和$E\times H$的矩阵，计算某个单词的表示需用一个单词的one-hot向量乘以$V\times E$维的矩阵（也就是lookup），再用得到的结果乘$E\times H$维的矩阵即可。两个矩阵的参数通过模型学习。</p>
<p>从下图实验结果可见，对于不共享参数的情况，$E$几乎是与大越好；而共享参数之后， $E$太大反而会使模型表现变差，$E=128$模型表现最好，因此ALBERT的默认参数设置中$E=128$.</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/albert_embedding.png" alt="avatar"></p>
<p>考虑到ALBERT-base的 $h=768$，那么 $E=768$时，模型应该可以看作没有减少embedding参数量的情况。而不共享参数的实验结果表明此时模型表现更好，那么似乎说明了Factorized embedding在一定程度上降低了模型的表现。</p>
<h5 id="跨层参数共享（参数量减少主要共享）"><a href="#跨层参数共享（参数量减少主要共享）" class="headerlink" title="跨层参数共享（参数量减少主要共享）"></a>跨层参数共享（参数量减少主要共享）</h5><p>另一个减少参数量的方法就是层之间的参数共享，即多个层使用相同的参数。参数共享有三种方式：只共享feed-forward network的参数、只共享attention的参数、共享全部参数。ALBERT默认是共享全部参数的。实验表明加入参数共享之后，每一层的输入embedding和输出embedding的L2距离和余弦相似度都比BERT稳定了很多。这证明参数共享能够使模型参数更加稳定。</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/cross.png" alt="avatar"></p>
<p>如下图所示，可以看出参数共享几乎也是对模型结果有负面影响的。但是考虑到其能够大幅削减参数量，并且对结果影响不是特别大，因此权衡之下选择了参数共享。</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/albert_crosslayer.png" alt="avatar"></p>
<h5 id="句间连贯性"><a href="#句间连贯性" class="headerlink" title="句间连贯性"></a>句间连贯性</h5><ul>
<li><strong>NSP</strong>：下一句预测， 正样本=上下相邻的2个句子，负样本=随机2个句子</li>
<li><strong>SOP</strong>：句子顺序预测，正样本=正常顺序的2个相邻句子，负样本=调换顺序的2个相邻句子</li>
</ul>
<p>NSP任务过于简单，只要模型发现两个句子的主题不一样就行了，所以SOP预测任务能够让模型学习到更多的信息。</p>
<p>除了减少模型参数外，还对BERT的预训练任务Next-sentence prediction (NSP)进行了改进。在BERT中，NSP任务的正例是文章中连续的两个句子，而负例则是从两篇文档中各选一个句子构造而成。在先前的研究中，已经证明NSP是并不是一个合适的预训练任务。作者推测其原因是模型在判断两个句子的关系时不仅考虑了两个句子之间的连贯性（coherence），还会考虑到两个句子的话题（topic）。而两篇文档的话题通常不同，模型会更多的通过话题去分析两个句子的关系，而不是句子间的连贯性，这使得NSP任务变成了一个相对简单的任务。</p>
<p>因此本文提出了Sentence-order prediction (SOP)来取代NSP。具体来说，其正例与NSP相同，但负例是通过选择一篇文档中的两个连续的句子并将它们的顺序交换构造的。这样两个句子就会有相同的话题，模型学习到的就更多是句子间的连贯性。</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/albert_sop.png" alt="avatar"></p>
<p>如上实验结果所示，如果不使用NSP或SOP作为预训练任务的话，模型在NSP和SOP两个任务上表现都很差；如果使用NSP作为预训练任务的话，模型确实能很好的解决NSP问题，但是在SOP问题上表现却很差，几乎相当于随机猜，因此说明NSP任务确实很难学到句子间的连贯性；而如果用SOP作为预训练任务，则模型也可以较好的解决NSP问题，同时模型在下游任务上表现也更好。说明SOP确实是更好的预训练任务。</p>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><h5 id="额外的训练数据和dropout"><a href="#额外的训练数据和dropout" class="headerlink" title="额外的训练数据和dropout"></a>额外的训练数据和dropout</h5><p>ALBERT训练时还加入了XLNet和RoBERTa训练时用的额外数据，实验表明加入额外数据（W additional data）确实会提升模型表现。此外，作者还观察到模型似乎一直没有过拟合数据，因此去除了Dropout，从对比试验可以看出，去除Dropout（W/O Dropout）后模型表现确实更好。</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/albert_dropout.png" alt="avatar"></p>
<h4 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a>总结</h4><p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/albert_bert.png" alt="avatar"></p>
<p>ALBERT的训练速度明显比BERT快，ALBERT-xxlarge的表现更是全方面超过了BERT。</p>
<p>本文有两个小细节可以学习，一个是在模型不会过拟合的情况下不使用dropout；另一个则是warm-start，即在训练深层网络（例如12层）时，可以先训练浅层网络（例如6层），再在其基础上做fine-tune，这样可以加快深层模型的收敛。</p>
<h3 id="TinyBERT"><a href="#TinyBERT" class="headerlink" title="TinyBERT"></a>TinyBERT</h3><p><a href="https://arxiv.org/abs/1909.10351" target="_blank" rel="noopener">论文</a></p>
<h4 id="简介-6"><a href="#简介-6" class="headerlink" title="简介"></a>简介</h4><p>在 NLP 领域，BERT 由于模型过于庞大，单个样本计算一次的开销动辄上百毫秒，很难应用到实际生产中。中科技大学、华为诺亚方舟实验室的研究者提出了 TinyBERT，这是一种为基于 transformer 的模型专门设计的知识蒸馏方法，模型大小为 BERT 的 13.3%，推理速度是 BERT 的 9.4 倍，而且性能没有出现明显下降。</p>
<p>目前主流的几种蒸馏方法大概分成 1. 利用 transformer 结构蒸馏, 2. 利用其它简单的结构比如 BiLSTM 等蒸馏。由于 BiLSTM 等结构简单，且一般是用 BERT 最后一层的输出结果进行蒸馏，不能学到 transformer 中间层的信息，对于复杂的语义匹配任务，效果有点不尽人意。</p>
<p>基于 transformer 结构的蒸馏方法目前比较出名的有微软的 BERT-PKD (Patient Knowledge Distillation for BERT)，huggingface 的 DistilBERT，以及华为的 TinyBERT。他们的基本思路都是减少 transformer encoding 的层数和 hidden size 大小，实现细节上各有不同，主要差异体现在 loss 的设计上。</p>
<h4 id="模型-3"><a href="#模型-3" class="headerlink" title="模型"></a>模型</h4><p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/tinyBert.jpg" alt="avatar"></p>
<p>整个 TinyBERT 的 loss 设计分为三部分：</p>
<h5 id="Embedding-layer-Distillation"><a href="#Embedding-layer-Distillation" class="headerlink" title="Embedding-layer Distillation"></a>Embedding-layer Distillation</h5><script type="math/tex; mode=display">
\mathcal{L}_{\mathrm{embd}}=\operatorname{MSE}\left(\boldsymbol{E}^{S} \boldsymbol{W}_{e}, \boldsymbol{E}^{T}\right)</script><p>其中，$E^s \in R^{l \times ds}$,$E^T \ in R^{l \times dt}$分别表示 student 网络的 embedding 和 teacher 网络的 embedding. 其中 l 代表 sequence length, ds 代表 student embedding 维度， dt 代表 teacher embedding 维度。由于 student 网络的 embedding 层通常较 teacher 会变小以获得更小的模型和加速，所以 $W_e$ 是一个 $d_s \times d_t$维的可训练的线性变换矩阵，把 student 的 embedding 投影到 teacher embedding 所在的空间。最后再算 MSE，得到 embedding loss.</p>
<h5 id="Transformer-layer-Distillation"><a href="#Transformer-layer-Distillation" class="headerlink" title="Transformer-layer Distillation"></a>Transformer-layer Distillation</h5><p>TinyBERT 的 transformer 蒸馏采用隔 k 层蒸馏的方式。举个例子，teacher BERT 一共有 12 层，若是设置 student BERT 为 4 层，就是每隔 3 层计算一个 transformer loss. 映射函数为 g(m) = 3 * m(???), m 为 student encoder 层数。具体对应为 student 第 1 层 transformer 对应 teacher 第 3 层，第 2 层对应第 6 层，第 3 层对应第 9 层，第 4 层对应第 12 层。每一层的 transformer loss 又分为两部分组成，attention based distillation 和 hidden states based distillation.</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/transormers_distillation.jpg" alt="avatar"></p>
<h6 id="Attention-based-loss"><a href="#Attention-based-loss" class="headerlink" title="Attention based loss"></a>Attention based loss</h6><script type="math/tex; mode=display">
\mathcal{L}_{\mathrm{attn}}=\frac{1}{h} \sum_{i=1}^{h} \operatorname{MSE}\left(\boldsymbol{A}_{i}^{S}, \boldsymbol{A}_{i}^{T}\right)</script><p>其中，$A_i \in R^{l \times l}$ $h$代表attention的头数，$l$代表输入长度，$A_i^S$代表student网络第i个attention头的attention score矩阵。$A_i^T$代表teacher网络的第i个attention头的attention score 矩阵。为什么要做这个损失？在What Does BERT Look At? An Analysis of BERT’s Attention [12]中研究了attention 权重到底学到了什么，实验发现与语义还有语法相关的词比如第一个动词宾语，第一个介词宾语，以及[CLS], [SEP], 逗号等 token，有很高的注意力权重。为了确保这部分信息能被 student 网络学到，TinyBERT 在 loss 设计中加上了 student 和 teacher 的 attention matrix 的 MSE。这样语言知识可以很好的从 teacher BERT 转移到 student BERT.</p>
<h6 id="hidden-states-based-distillation"><a href="#hidden-states-based-distillation" class="headerlink" title="hidden states based distillation"></a>hidden states based distillation</h6><script type="math/tex; mode=display">
\mathcal{L}_{\mathrm{hidn}}=\mathrm{MSE}\left(\boldsymbol{H}^{S} \boldsymbol{W}_{h}, \boldsymbol{H}^{T}\right)</script><p>其中，$H^S \in R^{l \times ds}$,$H^T \in R^{l \times dt}$分别是 student transformer 和 teacher transformer 的隐层输出。和 embedding loss 同理，$W_h$把$H^S$投影到$H^T$所在的空间。</p>
<h6 id="Prediction-Layer-Distillation"><a href="#Prediction-Layer-Distillation" class="headerlink" title="Prediction-Layer Distillation"></a>Prediction-Layer Distillation</h6><script type="math/tex; mode=display">
\mathcal{L}_{\text {pred }}=-\operatorname{softmax}\left(\boldsymbol{z}^{T}\right) \cdot \log \_ \left(\boldsymbol{z}^{S} / t\right)</script><p>其中 t 是 temperature value，暂时设为 1.除了模仿中间层的行为外，这一层用来模拟 teacher 网络在 predict 层的表现。具体来说，这一层计算了 teacher 输出的概率分布和 student 输出的概率分布的 softmax 交叉熵。这一层的实现和具体任务相关.</p>
<p> prediction loss 有很多变化。</p>
<ol>
<li>在 TinyBERT 中，这个 loss 是 teacher BERT 预测的概率和 student BERT 预测概率的 softmax 交叉熵.</li>
<li>在 BERT-PKD 模型中，这个 loss 是 teacher BERT 和 student BERT 的交叉熵和 student BERT 和 hard target( one-hot)的交叉熵的加权平均。</li>
</ol>
<p>直接用 hard target loss，效果比使用 teacher student softmax 交叉熵下降 5-6 个点。因为 softmax 比 one-hot 编码了更多概率分布的信息。并且softmax cross-entropy loss 容易发生不收敛的情况，把 softmax 交叉熵改成 MSE, 收敛效果变好，但泛化效果变差。这是因为使用 softmax cross-entropy 需要学到整个概率分布，更难收敛，因为拟合了 teacher BERT 的概率分布，有更强的泛化性。MSE 对极值敏感，收敛的更快，但泛化效果不如前者。</p>
<p>所以总的loss：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\text {model }}=\sum_{m=0}^{M+1} \lambda_{m} \mathcal{L}_{\text {layer }}\left(S_{m}, T_{g(m)}\right)</script><p>其中</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\text {layer }}\left(S_{m}, T_{g(m)}\right)=\left\{\begin{array}{ll}
\mathcal{L}_{\text {embd }}\left(S_{0}, T_{0}\right), & m=0 \\
\mathcal{L}_{\text {hidn }}\left(S_{m}, T_{g(m)}\right)+\mathcal{L}_{\text {attn }}\left(S_{m}, T_{g(m)}\right), & M \geq m>0 \\
\mathcal{L}_{\text {pred }}\left(S_{M+1}, T_{N+1}\right), & m=M+1
\end{array}\right.</script><h4 id="两阶段学习框架"><a href="#两阶段学习框架" class="headerlink" title="两阶段学习框架"></a>两阶段学习框架</h4><p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/two_step.jpg" alt="avatar"></p>
<p>类似于原生的 BERT 先 pre-train, 根据具体任务再 fine-tine。TinyBERT 先在 general domain 数据集上用未经微调的 BERT 充当教师蒸馏出一个 base 模型，在此基础上，具体任务通过数据增强，利用微调后的 BERT 再进行重新执行蒸馏。这种两阶段的方法给 TinyBERT 提供了像 BERT 一样的泛化能力。 </p>
<h4 id="总结-5"><a href="#总结-5" class="headerlink" title="总结"></a>总结</h4><p> TinyBERT 作为一种蒸馏方法，能有效的提取 BERT transformer 结构中丰富的语意信息，在不牺牲性能的情况下，速度能获得 8 到 9 倍的提升。</p>
<h3 id="FastBert"><a href="#FastBert" class="headerlink" title="FastBert"></a>FastBert</h3><h4 id="简介-7"><a href="#简介-7" class="headerlink" title="简介"></a>简介</h4><p>FastBERT的创新点很容易理解，就是在每层Transformer后都去预测样本标签，如果某样本预测结果的置信度很高，就不用继续计算了。论文把这个逻辑称为样本自适应机制（Sample-wise adaptive mechanism），就是自适应调整每个样本的计算量，容易的样本通过一两层就可以预测出来，较难的样本则需要走完全程。</p>
<p>举个例子，比如 text_a = ‘北京鲜花快递’ text_b = ‘北京鲜花速递’ 这种case可能浅层的bert就已经能够很好的算出其相关性打分，所以算完两层bert的打分和12层的结果基本一致，而text_a = ‘北京鲜花快递’ text_b = ‘有没有哪个地方卖花卉，而且包送，位置北京’ 这种case可能需要深层的bert去抽取两边的语义特征，并计算其是否匹配，所以在inference阶段就需要算完12层。</p>
<p><img src="/2020/04/13/bert%E7%B3%BB%E5%88%97/fastbert.jpg" alt="avatar"></p>
<p>作者将原BERT模型称为主干（Backbone），每个分类器称为分支（Branch）。这里的分支Classifier都是最后一层的分类器蒸馏来的，作者将这称为自蒸馏（Self-distillation）。就是在预训练和精调阶段都只更新主干参数，精调完后<strong>freeze主干参数，用分支分类器（图中的student）蒸馏主干分类器（图中的teacher）的概率分布</strong>。之所以叫自蒸馏，是因为之前的蒸馏都是用两个模型去做，一个模型学习另一个模型的知识，而FastBERT是自己（分支）蒸馏自己（主干）的知识。值得注意的是，蒸馏时需要freeze主干部分，保证pretrain和finetune阶段学习的知识不被影响，仅用brach 来尽可能的拟合teacher的分布。同时，使用自蒸馏还有一点重要的好处，就是<strong>不再依赖于标注数据</strong>。蒸馏的效果可以通过源源不断的无标签数据来提升。</p>
<h4 id="总结-6"><a href="#总结-6" class="headerlink" title="总结"></a>总结</h4><p>FastBERT通过提前输出简单样本的预测结果，减少模型的计算负担，从而提高推理速度。虽然每层都多了一个分类器，但分类器的计算量也比Transformer小了两个数量级，对速度影响较小。后续的分支自蒸馏也设计的比较巧妙，可以利用无监督数据不断提升分支分类器的效果。</p>
<h3 id="总结-7"><a href="#总结-7" class="headerlink" title="总结"></a>总结</h3><p>性能：</p>
<ol>
<li>BERT-WWM</li>
<li>XLNet（长文本表现更好一些）</li>
<li>RoBERTa（动态mask、without NSP、更大batch，更多数据，更长训练时间）</li>
<li>SpanBERT(基于分词的预训练模型)</li>
<li>MT-DNN(预训练+多任务)</li>
<li>ERNIR（引入实体信息）</li>
</ol>
<p>效率：</p>
<ol>
<li>DistilBert（知识蒸馏）</li>
<li>ALBert（因式分解、跨层权重共享）</li>
<li>TinyBert（知识蒸馏）</li>
</ol>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a href="https://zhuanlan.zhihu.com/p/46652512" target="_blank" rel="noopener">【NLP】Google BERT详解</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/81157740" target="_blank" rel="noopener">带你读论文丨8篇论文梳理BERT相关模型进展与反思</a></li>
<li><a href="https://blog.csdn.net/u012526436/article/details/93196139" target="_blank" rel="noopener">最通俗易懂的XLNET详解</a></li>
<li><a href="https://www.jianshu.com/p/eddf04ba8545" target="_blank" rel="noopener">改进版的RoBERTa到底改进了什么？</a></li>
<li><a href="https://www.bilibili.com/video/av73657563/" target="_blank" rel="noopener">【AI模型】最通俗易懂的XLNet详解</a></li>
<li>Liu Y, Ott M, Goyal N, et al. Roberta: A robustly optimized bert pretraining approach[J]. arXiv preprint arXiv:1907.11692, 2019</li>
<li>Sennrich R, Haddow B, Birch A. Neural machine translation of rare words with subword units[J]. arXiv preprint arXiv:1508.07909, 2015.</li>
<li><a href="http://fancyerii.github.io/2019/03/09/bert-codes/#dataprocessor" target="_blank" rel="noopener">bert代码阅读</a></li>
<li><a href="https://www.infoq.cn/article/STabowUeFupgc4gRqRQj" target="_blank" rel="noopener">更小、更快、更便宜、更轻量：开源 DistilBERT，BERT 的精简版本</a></li>
<li>Michel, P., Levy, O., &amp; Neubig, G. (2019). Are Sixteen Heads Really Better than One? Retrieved from <a href="https://arxiv.org/abs/1905.10650" target="_blank" rel="noopener">https://arxiv.org/abs/1905.10650</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/94359189" target="_blank" rel="noopener">比 Bert 体积更小速度更快的 TinyBERT</a></li>
<li><a href="https://arxiv.org/abs/1906.04341" target="_blank" rel="noopener">https://arxiv.org/abs/1906.04341</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/87562926" target="_blank" rel="noopener">【论文阅读】ALBERT</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/59436589" target="_blank" rel="noopener">中文任务全面超越BERT：百度正式发布NLP预训练模型ERNIE</a></li>
<li><a href="https://www.jianshu.com/p/5e12e6edbd59" target="_blank" rel="noopener">BERT泛读系列（三）—— ERNIE: Enhanced Language Representation with Informative Entities论文笔记</a></li>
<li><a href="https://blog.csdn.net/ljp1919/article/details/90269059" target="_blank" rel="noopener">文献阅读：MT-DNN模型</a></li>
<li><a href="https://blog.csdn.net/Magical_Bubble/article/details/89517709" target="_blank" rel="noopener">MT-DNN解读(论文 + PyTorch源码)</a></li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>BERT-WWM</tag>
        <tag>XLNet</tag>
        <tag>RoBERTa</tag>
        <tag>SpanBERT</tag>
        <tag>ernie</tag>
        <tag>MT-DNN</tag>
        <tag>DistillBERT</tag>
        <tag>ALBERT</tag>
        <tag>TinyBERT</tag>
      </tags>
  </entry>
  <entry>
    <title>linux命令速查</title>
    <url>/2018/06/25/linux%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="linux的文件系统"><a href="#linux的文件系统" class="headerlink" title="linux的文件系统"></a>linux的文件系统</h2><ol>
<li>/boot:系统启动相关的文件，如内核、initrd、以及grub（bootloader）</li>
<li>/dev:设备文件：<ol>
<li>块设备：随机访问，数据块</li>
<li>字符设备：线性访问，按字符为单位</li>
<li>设备号：主设备号（major）和次设备号（minor）</li>
</ol>
</li>
<li>/etc：配置文件</li>
<li>/home：用户的家目录，每一个用户的家目录通常默认为/home/username</li>
<li>/root：管理员的家目录；</li>
<li>/lib:库文件<ol>
<li>静态库：.a</li>
<li>动态库：.dll, .so(shared object)</li>
<li>/lib/moudules:内核模块文件</li>
</ol>
</li>
<li>/lib64</li>
<li>/media:挂载点目录，移动设备</li>
<li>/mnt:挂载点目录，额外的临时文件系统</li>
<li>/opt:可选目录，第三方程序的安装目录</li>
<li>/proc：伪文件系统，内核映射文件</li>
<li>/sys：伪文件系统，跟硬件设备相关的属性映射文件</li>
<li>/tmp:临时文件，/var/tmp</li>
<li>/var:可变化的文件</li>
<li>/bin:可执行文件，用户命令</li>
<li>/sbin：管理命令</li>
</ol>
<a id="more"></a>
<h2 id="terminal-命令"><a href="#terminal-命令" class="headerlink" title="terminal 命令"></a>terminal 命令</h2><ol>
<li>printenv：打印系统的环境变量</li>
<li>whereis: 查找文件or命令</li>
<li>查看已经存在的ssh key : cat ~/.ssh/id_rsa.pub</li>
</ol>
<h3 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h3><ol>
<li>tar  <ol>
<li>解包：tar zxvf filename.tar  </li>
<li>打包：tar czvf filename.tar dirname</li>
</ol>
</li>
<li>gz命令  <ol>
<li>解压1：gunzip filename.gz  </li>
<li>解压2：gzip -d filename.gz  </li>
<li>压缩：gzip filename<ol>
<li>.tar.gz 和  .tgz      </li>
<li>解压：tar zxvf filename.tar.gz      </li>
<li>压缩：tar zcvf filename.tar.gz dirname      </li>
<li>压缩多个文件：tar zcvf filename.tar.gz dirname1 dirname2 dirname3…..</li>
</ol>
</li>
</ol>
</li>
<li>bz2命令  <ol>
<li>解压1：bzip2 -d filename.bz2  </li>
<li>解压2：bunzip2 filename.bz2  </li>
<li>压缩：bzip2 -z filename       </li>
<li>.tar.bz2       <ol>
<li>解压：tar jxvf filename.tar.bz2       </li>
<li>压缩：tar jcvf filename.tar.bz2 dirname</li>
</ol>
</li>
</ol>
</li>
<li>bz命令    <ol>
<li>解压1：bzip2 -d filename.bz    </li>
<li>解压2：bunzip2 filename.bz         </li>
<li>.tar.bz       <ol>
<li>解压：tar jxvf filename.tar.bz</li>
</ol>
</li>
<li>z命令    <ol>
<li>解压：uncompress filename.z    </li>
<li>压缩：compress filename       </li>
<li>.tar.z          <ol>
<li>解压：tar zxvf filename.tar.z          </li>
<li>压缩：tar zcvf filename.tar.z dirname</li>
</ol>
</li>
</ol>
</li>
<li>zip命令        <ol>
<li>解压：unzip filename.zip    </li>
<li>压缩：zip filename.zip dirname</li>
</ol>
</li>
</ol>
</li>
<li><p>总结：z 以gzip格式压缩，c表示create，v显示压缩的详细情况 f file</p>
<ol>
<li>压缩<ol>
<li>tar –cvf jpg.tar *.jpg //将目录里所有jpg文件打包成tar.jpg </li>
<li>tar –czf jpg.tar.gz *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gz </li>
<li>tar –cjf jpg.tar.bz2 *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用bzip2压缩，生成一个bzip2压缩过的包，命名为jpg.tar.bz2 </li>
<li>tar –cZf jpg.tar.Z *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用compress压缩，生成一个umcompress压缩过的包，命名为jpg.tar.Z </li>
<li>rar a jpg.rar *.jpg //rar格式的压缩，需要先下载rar for linux </li>
<li>zip jpg.zip *.jpg //zip格式的压缩，需要先下载zip for linux </li>
<li>解压</li>
<li>tar –xvf file.tar //解压 tar包 </li>
<li>tar -xzvf file.tar.gz //解压tar.gz </li>
<li>tar -xjvf file.tar.bz2 //解压 </li>
<li>tar.bz2 tar –xZvf file.tar.Z //解压tar.Z </li>
<li>unrar e file.rar //解压rar </li>
<li>unzip file.zip //解压zip </li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="screen"><a href="#screen" class="headerlink" title="screen"></a>screen</h3><ol>
<li>新建一个叫session_name的session：screen -S session_name</li>
<li>列出当前所有的session：screen -ls</li>
<li>回到session_name这个session：screen -r session_name</li>
<li>远程detach某个session：screen -d session_name</li>
<li>结束当前session并回到session_name这个session：screen -d -r session_name        </li>
<li>利用exit退出并kill掉session</li>
</ol>
<h3 id="查看显卡和内存使用情况"><a href="#查看显卡和内存使用情况" class="headerlink" title="查看显卡和内存使用情况"></a>查看显卡和内存使用情况</h3><ol>
<li>查看显卡使用情况：watch -n 5 nvidia-smi</li>
<li>查看磁盘使用：df -h</li>
</ol>
<h3 id="移动重命名"><a href="#移动重命名" class="headerlink" title="移动重命名"></a>移动重命名</h3><ol>
<li>将/usr/udt中的所有文件移到当前目录(用”.”表示)中：$ mv /usr/udt/* .</li>
<li>将文件test.txt重命名为wbk.txt：$ mv test.txt wbk.txt</li>
</ol>
<h3 id="目录管理"><a href="#目录管理" class="headerlink" title="目录管理"></a>目录管理</h3><ol>
<li>ls</li>
<li>cd</li>
<li>pwd</li>
<li>mkdir</li>
<li>rmdir</li>
<li>tree</li>
</ol>
<h3 id="文件管理"><a href="#文件管理" class="headerlink" title="文件管理"></a>文件管理</h3><ol>
<li>touch</li>
<li>stat</li>
<li>file</li>
<li>rm<ol>
<li>-i: 若指定目录已有同名文件，则先询问是否覆盖旧文件;</li>
<li>-f: 在mv操作要覆盖某已有的目标文件时不给任何指示;</li>
</ol>
</li>
<li>cp [options] source dest<ol>
<li>-a:此选项通常在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容。其作用等于dpr参数组合。</li>
<li>-d：复制时保留链接。这里所说的链接相当于Windows系统中的快捷方式。</li>
<li>-f：覆盖已经存在的目标文件而不给出提示。</li>
<li>-i：与-f选项相反，在覆盖目标文件之前给出提示，要求用户确认是否覆盖，回答”y”时目标文件将被覆盖。</li>
<li>-p：除复制文件的内容外，还把修改时间和访问权限也复制到新文件中。</li>
<li>-r：若给出的源文件是一个目录文件，此时将复制该目录下所有的子目录和文件。</li>
<li>-l：不复制文件，只是生成链接文件。</li>
</ol>
</li>
<li>mv</li>
<li>nano</li>
<li>vi</li>
<li>vim</li>
</ol>
<h3 id="日期时间"><a href="#日期时间" class="headerlink" title="日期时间"></a>日期时间</h3><ol>
<li>data</li>
<li>clock</li>
<li>hwclock</li>
<li>cal</li>
<li>ntpdate</li>
</ol>
<h3 id="查看文本"><a href="#查看文本" class="headerlink" title="查看文本"></a>查看文本</h3><ol>
<li>cat</li>
<li>tac</li>
<li>more</li>
<li>less</li>
<li>head</li>
<li>tail</li>
</ol>
<h3 id="文本查找"><a href="#文本查找" class="headerlink" title="文本查找"></a>文本查找</h3><ol>
<li>find：查找文件，eg：find /etc/pass* 在/etc目录下查找以pass开头的文件</li>
<li>grep：查找文本内容，eg: grep ‘^root’ /etc/passwd 在/etc/passwd文件中查找以root开头的内容<h3 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h3></li>
<li>| 表示管道，表示管道左边命令的结果传给管道右边。eg: ls -l | more、 ps aux | grep xxx</li>
<li>命令1 | 命令2 | 命令3</li>
</ol>
<h3 id="文本处理"><a href="#文本处理" class="headerlink" title="文本处理"></a>文本处理</h3><ol>
<li>cut<ol>
<li>-d：指定字段分割符，默认是空格<ol>
<li>-d”,”</li>
</ol>
</li>
<li>-f： 指定要显示的字段<ol>
<li>-f 1,3</li>
<li>-f 1-3</li>
</ol>
</li>
</ol>
</li>
<li>sort：文本排序<ol>
<li>-n：数值排序</li>
<li>-r：降序</li>
<li>-t：字段分隔符</li>
<li>-k：以哪个字段为关键字进行排序</li>
<li>-u：排序后相同的行只显示一次</li>
<li>-f：排序时忽略字符大小写</li>
</ol>
</li>
<li>join</li>
<li>sed</li>
<li>awk</li>
</ol>
<h3 id="系统管理命令"><a href="#系统管理命令" class="headerlink" title="系统管理命令"></a>系统管理命令</h3><ol>
<li>ps 命令可以查看进程的详细状况<ol>
<li>-a 显示终端上所有进程，包括其他用户的进程</li>
<li>-u 显示进程的详细状态</li>
<li>-x 显示没有控制终端的进程</li>
<li>-w 显示加宽，以便显示更多的信息</li>
<li>-r 只显示正在运行的进程</li>
</ol>
</li>
<li>top 命令用来动态显示运行中的进程，可以在使用top命令时加上-d来指定显示信息更新的时间间隔。在top命令下，通过按下下列字母，来对显示结果进行排序<ol>
<li>m 根据内存使用量来排序</li>
<li>p  根据cpu占有率来排序</li>
<li>t   根据进程运行时间的长短来排序</li>
<li>u  可以根据后面输入的用户名来筛选进程</li>
<li>k  可以根据后面输入的pid来杀死进程</li>
<li>q  退出</li>
<li>h  获得帮助</li>
</ol>
</li>
<li>kill ，killall<ol>
<li>kill -9 pid</li>
<li>killall 进程名</li>
</ol>
</li>
</ol>
<h3 id="磁盘空间命令"><a href="#磁盘空间命令" class="headerlink" title="磁盘空间命令"></a>磁盘空间命令</h3><ol>
<li>df 命令用于检测文件系统的磁盘空间占用和剩余情况，可以显示所有文件系统对节点和磁盘块的使用情况<ol>
<li>a 显示所有文件系统的磁盘使用情况</li>
<li>m 以1024字节为单位显示</li>
<li>t 显示各指定文件系统的磁盘空间使用情况</li>
<li>T 显示文件系统</li>
</ol>
</li>
<li>du 命令用于统计目录或文件所占磁盘空间的大小，该命令的执行结果与df类似，du更侧重于磁盘的使用情况。 格式： du [选项] 目录或文件名<ol>
<li>a 递归显示指定目录中各文件和子目录中文件占用的数据块</li>
<li>s 显示指定文件或目录占用的数据块</li>
<li>b 以字节为单位显示磁盘占用情况</li>
<li>l 计算所有文件大小，对硬链接文件计算多次</li>
</ol>
</li>
</ol>
<h3 id="查看或配置网卡信息：ifconfig"><a href="#查看或配置网卡信息：ifconfig" class="headerlink" title="查看或配置网卡信息：ifconfig"></a>查看或配置网卡信息：ifconfig</h3><h3 id="测试远程主机连通性：ping"><a href="#测试远程主机连通性：ping" class="headerlink" title="测试远程主机连通性：ping"></a>测试远程主机连通性：ping</h3><ol>
<li>ping -c4 www.baidu.com<h3 id="查看网络情况：-netstat-ntpl"><a href="#查看网络情况：-netstat-ntpl" class="headerlink" title="查看网络情况： netstat -ntpl"></a>查看网络情况： netstat -ntpl</h3></li>
</ol>
<h3 id="linux引号"><a href="#linux引号" class="headerlink" title="linux引号"></a>linux引号</h3><ol>
<li>反引号：` ， 命令替换</li>
<li>单引号：’’,字符串</li>
<li>双引号：””,变量替换</li>
</ol>
<h2 id="软件安装和管理"><a href="#软件安装和管理" class="headerlink" title="软件安装和管理"></a>软件安装和管理</h2><h3 id="软件包"><a href="#软件包" class="headerlink" title="软件包"></a>软件包</h3><ol>
<li>bin文件（适合所有Linux发行版），是可执行的文件</li>
<li>rpm包，yum（redhat系列）<ol>
<li>rpm命令：安装过程中不需要指定安装路径，rpm文件在制作的时候已经确定了安装路径</li>
<li>查询软件安装路径：rpm -ql xxx</li>
</ol>
</li>
<li>源码压缩包（适合所有的Linux发行版）</li>
<li>官方已经编译好的，下载软件包直接可以使用</li>
<li>安装步骤：<ol>
<li>检查是否已经安装：rpm -qa | grep jdk</li>
<li>下载软件包</li>
<li>安装依赖<h3 id="yum"><a href="#yum" class="headerlink" title="yum"></a>yum</h3></li>
</ol>
</li>
<li>解决rpm下载问题</li>
<li>解决rpm文件的查询问题</li>
<li>解决rpm安装问题</li>
<li>解决rpm的依赖问题</li>
</ol>
]]></content>
      <categories>
        <category>速查</category>
      </categories>
      <tags>
        <tag>速查</tag>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title>python高级</title>
    <url>/2018/06/05/python%E9%AB%98%E7%BA%A7/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="range-start-stop-step"><a href="#range-start-stop-step" class="headerlink" title="range(start, stop[, step])"></a>range(start, stop[, step])</h3><ul>
<li>start: 计数从 start 开始。默认是从 0 开始。</li>
<li>stop: 计数到 stop 结束，</li>
<li>step：步长，默认为1。</li>
</ul>
<a id="more"></a>
<h3 id="lambda-map-filter-reduce-yield函数的应用"><a href="#lambda-map-filter-reduce-yield函数的应用" class="headerlink" title="lambda,map,filter,reduce,yield函数的应用"></a>lambda,map,filter,reduce,yield函数的应用</h3><h4 id="匿名函数lambda"><a href="#匿名函数lambda" class="headerlink" title="匿名函数lambda"></a><strong>匿名函数lambda</strong></h4><p>冒号左边放原函数的参数，可以有多个参数，用逗号(,)隔开，冒号右边是返回值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">lambda</span> x,y:x+y</span><br></pre></td></tr></table></figure>
<h4 id="遍历函数-map"><a href="#遍历函数-map" class="headerlink" title="遍历函数:map"></a><strong>遍历函数</strong>:map</h4><p>map函数用于遍历序列</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">li = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">lo = [<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]</span><br><span class="line">list(map(<span class="keyword">lambda</span> x, y: x**<span class="number">2</span> + y**<span class="number">2</span>, li, lo))</span><br><span class="line"><span class="comment"># [26, 40, 58, 80]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiply</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x*x)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x+x)</span><br><span class="line"></span><br><span class="line">funcs = [multiply, add]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    value = map(<span class="keyword">lambda</span> x: x(i), funcs)</span><br><span class="line">print(list(value))</span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"><span class="comment"># [0, 0]</span></span><br><span class="line"><span class="comment"># [1, 2]</span></span><br><span class="line"><span class="comment"># [4, 4]</span></span><br><span class="line"><span class="comment"># [9, 6]</span></span><br></pre></td></tr></table></figure>
<h4 id="筛选函数-filter"><a href="#筛选函数-filter" class="headerlink" title="筛选函数:filter"></a><strong>筛选函数</strong>:filter</h4><p>筛选函数用于对序列中的元素进行筛选，最终获取符合条件的序列。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">li = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]</span><br><span class="line">list(filter(<span class="keyword">lambda</span> x: x&gt;<span class="number">5</span>, li))</span><br><span class="line"><span class="comment">#Output</span></span><br><span class="line"><span class="comment"># [6, 7, 8, 9]</span></span><br></pre></td></tr></table></figure>
<h4 id="累计函数-reduce"><a href="#累计函数-reduce" class="headerlink" title="累计函数:reduce"></a><strong>累计函数</strong>:reduce</h4><p>对序列中的所有函数进行累积操作</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">li = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">reduce(<span class="keyword">lambda</span> a, b: a+b, li) <span class="comment"># 结果为15</span></span><br><span class="line">reduce(<span class="keyword">lambda</span> a, b: a+b, li, <span class="number">100</span>)<span class="comment">#结果为115，第三个参数为初始值，在此值之进行累计</span></span><br></pre></td></tr></table></figure>
<h4 id="关键字-yield"><a href="#关键字-yield" class="headerlink" title="关键字: yield"></a><strong>关键字</strong>: yield</h4><p>yield可以将函数执行的中间结果返回但是不结束程序。把一个函数变成一个生成器.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(x)</span>:</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i&lt;x:</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line">        i+=<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="args-和-kwargs"><a href="#args-和-kwargs" class="headerlink" title="args 和 *kwargs"></a>args 和 *kwargs</h3><h4 id="args：预先并不知道-函数使用者会传递多少个参数"><a href="#args：预先并不知道-函数使用者会传递多少个参数" class="headerlink" title="args：预先并不知道, 函数使用者会传递多少个参数"></a>args：预先并不知道, 函数使用者会传递多少个参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_var_args</span><span class="params">(f_arg, *argv)</span>:</span></span><br><span class="line">print(<span class="string">"first normal arg:"</span>, f_arg)</span><br><span class="line"><span class="keyword">for</span> arg <span class="keyword">in</span> argv:</span><br><span class="line">    print(<span class="string">"another arg through *argv:"</span>, arg)</span><br><span class="line"></span><br><span class="line">test_var_args(<span class="string">'yasoob'</span>, <span class="string">'python'</span>, <span class="string">'eggs'</span>, <span class="string">'test'</span>)</span><br><span class="line"></span><br><span class="line">first normal arg: yasoob</span><br><span class="line">another arg through *argv: python</span><br><span class="line">another arg through *argv: eggs</span><br><span class="line">another arg through *argv: test</span><br></pre></td></tr></table></figure>
<h4 id="args：将不定长度的键值对-作为参数传递给一个函数。"><a href="#args：将不定长度的键值对-作为参数传递给一个函数。" class="headerlink" title="**args：将不定长度的键值对, 作为参数传递给一个函数。"></a>**args：将不定长度的键值对, 作为参数传递给一个函数。</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greet_me</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> kwargs.items():</span><br><span class="line">    print(<span class="string">"&#123;0&#125; == &#123;1&#125;"</span>.format(key, value))</span><br><span class="line"></span><br><span class="line">greet_me(name=<span class="string">"yasoob"</span>)</span><br><span class="line">name == yasoob</span><br></pre></td></tr></table></figure>
<h3 id="生成器-Generators"><a href="#生成器-Generators" class="headerlink" title="生成器 Generators"></a>生成器 Generators</h3><p>生成器并不把所有的值存在内存中，而是在运行时生成值。通过遍历来使用，要么用一个“for”循环，要么将它传递给任意可以进行迭代的函数和结构。大多数时候生成器是以函数来实现的。然而，它们并不返回一个值，而是yield(暂且译作“生出”)一个值。</p>
<h4 id="可迭代对象-Iterable"><a href="#可迭代对象-Iterable" class="headerlink" title="可迭代对象(Iterable)"></a>可迭代对象(Iterable)</h4><p> Python中任意的对象，只要它定义了可以返回一个迭代器的<strong>iter</strong>方法，或者定义了可以支持下标索引的<strong>getitem</strong>方法，那么它就是一个可迭代对象。</p>
<h4 id="迭代器-Iterator"><a href="#迭代器-Iterator" class="headerlink" title="迭代器(Iterator)"></a>迭代器(Iterator)</h4><p>任意对象，只要定义了next(Python2) 或者<strong>next</strong>方法，它就是一个迭代器。对于python中的字符串如：p_s = ‘python’来说，p_s是一个可迭代对象，但不是一个迭代器，不可以使用next(p_s),但可以使用iter()函数，使得一个可迭代对象变成一个迭代器：iter(p_s)</p>
<h4 id="迭代-Iteration"><a href="#迭代-Iteration" class="headerlink" title="迭代(Iteration)"></a>迭代(Iteration)</h4><p>用一个循环来遍历某个东西时，这个过程本身就叫迭代。</p>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Python</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_function</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> generator_function():</span><br><span class="line">  print(item)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: 0</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"><span class="comment"># 4</span></span><br><span class="line"><span class="comment"># 5</span></span><br><span class="line"><span class="comment"># 6</span></span><br><span class="line"><span class="comment"># 7</span></span><br><span class="line"><span class="comment"># 8</span></span><br><span class="line"><span class="comment"># 9</span></span><br></pre></td></tr></table></figure>
<h3 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h3><p>装饰器是：是修改其他函数的功能的函数</p>
<h4 id="在函数中定义函数"><a href="#在函数中定义函数" class="headerlink" title="在函数中定义函数"></a><strong>在函数中定义函数</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hi</span><span class="params">(name=<span class="string">"yasoob"</span>)</span>:</span></span><br><span class="line">     print(<span class="string">"now you are inside the hi() function"</span>)</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">greet</span><span class="params">()</span>:</span></span><br><span class="line">     <span class="keyword">return</span> <span class="string">"now you are in the greet() function"</span></span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">welcome</span><span class="params">()</span>:</span></span><br><span class="line">     <span class="keyword">return</span> <span class="string">"now you are in the welcome() function"</span></span><br><span class="line"> </span><br><span class="line"> print(greet())</span><br><span class="line"> print(welcome())</span><br><span class="line"> print(<span class="string">"now you are back in the hi() function"</span>)</span><br><span class="line"> </span><br><span class="line"> hi()</span><br><span class="line"> <span class="comment">#output:now you are inside the hi() function</span></span><br><span class="line"> <span class="comment">#       now you are in the greet() function</span></span><br><span class="line"> <span class="comment">#       now you are in the welcome() function</span></span><br><span class="line"> <span class="comment">#       now you are back in the hi() function</span></span><br><span class="line"> </span><br><span class="line"> <span class="comment"># 上面展示了无论何时你调用hi(), greet()和welcome()将会同时被调用。</span></span><br><span class="line"> <span class="comment"># 然后greet()和welcome()函数在hi()函数之外是不能访问的，比如：</span></span><br><span class="line"> </span><br><span class="line"> greet()</span><br><span class="line"> <span class="comment">#outputs: NameError: name 'greet' is not defined</span></span><br></pre></td></tr></table></figure>
<h4 id="从函数中返回函数"><a href="#从函数中返回函数" class="headerlink" title="从函数中返回函数"></a><strong>从函数中返回函数</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hi</span><span class="params">(name=<span class="string">"yasoob"</span>)</span>:</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">greet</span><span class="params">()</span>:</span></span><br><span class="line">     <span class="keyword">return</span> <span class="string">"now you are in the greet() function"</span></span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">welcome</span><span class="params">()</span>:</span></span><br><span class="line">     <span class="keyword">return</span> <span class="string">"now you are in the welcome() function"</span></span><br><span class="line"> </span><br><span class="line"> <span class="keyword">if</span> name == <span class="string">"yasoob"</span>:</span><br><span class="line">     <span class="keyword">return</span> greet</span><br><span class="line"> <span class="keyword">else</span>:</span><br><span class="line">     <span class="keyword">return</span> welcome</span><br><span class="line"> </span><br><span class="line"> a = hi()</span><br><span class="line"> print(a)</span><br><span class="line"> <span class="comment">#outputs: &lt;function greet at 0x7f2143c01500&gt;</span></span><br><span class="line"> <span class="comment">#上面清晰地展示了`a`现在指向到hi()函数中的greet()函数</span></span><br><span class="line"> <span class="comment">#现在试试这个</span></span><br><span class="line"> print(a())</span><br><span class="line"> <span class="comment">#outputs: now you are in the greet() function</span></span><br></pre></td></tr></table></figure>
<h4 id="将函数作为参数传给另一个函数"><a href="#将函数作为参数传给另一个函数" class="headerlink" title="将函数作为参数传给另一个函数"></a><strong>将函数作为参数传给另一个函数</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hi</span><span class="params">()</span>:</span></span><br><span class="line"><span class="keyword">return</span> <span class="string">"hi yasoob!"</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doSomethingBeforeHi</span><span class="params">(func)</span>:</span></span><br><span class="line">print(<span class="string">"I am doing some boring work before executing hi()"</span>)</span><br><span class="line">print(func())</span><br><span class="line">doSomethingBeforeHi(hi)</span><br><span class="line"><span class="comment">#outputs:I am doing some boring work before executing hi()</span></span><br><span class="line"><span class="comment">#        hi yasoob!</span></span><br></pre></td></tr></table></figure>
<h4 id="装饰器-1"><a href="#装饰器-1" class="headerlink" title="装饰器"></a><strong>装饰器</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_new_decorator</span><span class="params">(a_func)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapTheFunction</span><span class="params">()</span>:</span></span><br><span class="line">        print(<span class="string">"I am doing some boring work before executing a_func()"</span>)</span><br><span class="line"></span><br><span class="line">        a_func()</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"I am doing some boring work after executing a_func()"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> wrapTheFunction</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_function_requiring_decoration</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"I am the function which needs some decoration to remove my foul smell"</span>)</span><br><span class="line"></span><br><span class="line">a_function_requiring_decoration()</span><br><span class="line"><span class="comment">#outputs: "I am the function which needs some decoration to remove my foul smell"</span></span><br><span class="line"></span><br><span class="line">a_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration)</span><br><span class="line"><span class="comment">#now a_function_requiring_decoration is wrapped by wrapTheFunction()</span></span><br><span class="line"></span><br><span class="line">a_function_requiring_decoration()</span><br><span class="line"><span class="comment">#outputs:I am doing some boring work before executing a_func()</span></span><br><span class="line"><span class="comment">#        I am the function which needs some decoration to remove my foul smell</span></span><br><span class="line"><span class="comment">#        I am doing some boring work after executing a_func()</span></span><br></pre></td></tr></table></figure>
<h4 id="语法糖"><a href="#语法糖" class="headerlink" title="@语法糖"></a><strong>@语法糖</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@a_new_decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_function_requiring_decoration</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""Hey you! Decorate me!"""</span></span><br><span class="line">    print(<span class="string">"I am the function which needs some decoration to "</span></span><br><span class="line">        <span class="string">"remove my foul smell"</span>)</span><br><span class="line"></span><br><span class="line">a_function_requiring_decoration()</span><br><span class="line"><span class="comment">#outputs: I am doing some boring work before executing a_func()</span></span><br><span class="line"><span class="comment">#         I am the function which needs some decoration to remove my foul smell</span></span><br><span class="line"><span class="comment">#         I am doing some boring work after executing a_func()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#the @a_new_decorator is just a short way of saying:</span></span><br><span class="line">a_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration)</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>functools.wraps</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(a_function_requiring_decoration.__name__)</span><br><span class="line"><span class="comment"># Output: wrapTheFunction</span></span><br></pre></td></tr></table></figure>
<p>我们期待的输出应该是“a_function_requiring_decoration”，这里的函数被warpTheFunction替代了。Python提供一个简单的函数来解决这个问题：functools.wrap。</p>
<p>@wraps接受一个函数来进行装饰，并加入了复制函数名称、注释文档、参数列表等等的功能。这可以让我们在装饰器里面访问在装饰之前的函数的属性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_new_decorator</span><span class="params">(a_func)</span>:</span></span><br><span class="line"><span class="meta">    @wraps(a_func)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapTheFunction</span><span class="params">()</span>:</span></span><br><span class="line">        print(<span class="string">"I am doing some boring work before executing a_func()"</span>)</span><br><span class="line">        a_func()</span><br><span class="line">        print(<span class="string">"I am doing some boring work after executing a_func()"</span>)</span><br><span class="line">    <span class="keyword">return</span> wrapTheFunction</span><br><span class="line"></span><br><span class="line"><span class="meta">@a_new_decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_function_requiring_decoration</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""Hey yo! Decorate me!"""</span></span><br><span class="line">    print(<span class="string">"I am the function which needs some decoration to "</span></span><br><span class="line">        <span class="string">"remove my foul smell"</span>)</span><br><span class="line"></span><br><span class="line">print(a_function_requiring_decoration.__name__)</span><br><span class="line"><span class="comment"># Output: a_function_requiring_decoration</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="应用1-日志"><a href="#应用1-日志" class="headerlink" title="应用1:日志"></a>应用1:日志</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">logit</span><span class="params">(logfile=<span class="string">'out.log'</span>)</span>:</span></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">logging_decorator</span><span class="params">(func)</span>:</span></span><br><span class="line"><span class="meta">         @wraps(func)</span></span><br><span class="line">         <span class="function"><span class="keyword">def</span> <span class="title">wrapped_function</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">             log_string = func.__name__ + <span class="string">" was called"</span></span><br><span class="line">             print(log_string)</span><br><span class="line">             <span class="comment"># 打开logfile，并写入内容</span></span><br><span class="line">             <span class="keyword">with</span> open(logfile, <span class="string">'a'</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">                 <span class="comment"># 现在将日志打到指定的logfile</span></span><br><span class="line">                 opened_file.write(log_string + <span class="string">'\n'</span>)</span><br><span class="line">             <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">         <span class="keyword">return</span> wrapped_function</span><br><span class="line">     <span class="keyword">return</span> logging_decorator</span><br><span class="line"> </span><br><span class="line"><span class="meta"> @logit()</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">myfunc1</span><span class="params">()</span>:</span></span><br><span class="line">     <span class="keyword">pass</span></span><br><span class="line"> </span><br><span class="line"> myfunc1()</span><br><span class="line"> <span class="comment"># Output: myfunc1 was called</span></span><br><span class="line"> <span class="comment"># 现在一个叫做 out.log 的文件出现了，里面的内容就是上面的字符串</span></span><br><span class="line"> </span><br><span class="line"><span class="meta"> @logit(logfile='func2.log')</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">myfunc2</span><span class="params">()</span>:</span></span><br><span class="line">     <span class="keyword">pass</span></span><br><span class="line"> </span><br><span class="line"> myfunc2()</span><br><span class="line"> <span class="comment"># Output: myfunc2 was called</span></span><br><span class="line"> <span class="comment"># 现在一个叫做 func2.log 的文件出现了，里面的内容就是上面的字符串</span></span><br></pre></td></tr></table></figure>
<h5 id="应用2：计算函数运行时间"><a href="#应用2：计算函数运行时间" class="headerlink" title="应用2：计算函数运行时间"></a>应用2：计算函数运行时间</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timethis</span><span class="params">(func)</span>:</span></span><br><span class="line"><span class="string">"""Decorator that reports the execution time."""</span></span><br><span class="line"><span class="meta">@wraps(func)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    result = func(*args, **kwargs)</span><br><span class="line">        end = time.time()</span><br><span class="line">        print(func.__name__, end - start, <span class="string">'s'</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    eturn wrapper</span><br></pre></td></tr></table></figure>
<h4 id="装饰器类"><a href="#装饰器类" class="headerlink" title="装饰器类"></a>装饰器类</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">logit</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, logfile=<span class="string">'out.log'</span>)</span>:</span></span><br><span class="line">        self.logfile = logfile</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, func)</span>:</span></span><br><span class="line"><span class="meta">        @wraps(func)</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">wrapped_function</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">            log_string = func.__name__ + <span class="string">" was called"</span></span><br><span class="line">            print(log_string)</span><br><span class="line">            <span class="comment"># 打开logfile并写入</span></span><br><span class="line">            <span class="keyword">with</span> open(self.logfile, <span class="string">'a'</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">                <span class="comment"># 现在将日志打到指定的文件</span></span><br><span class="line">                opened_file.write(log_string + <span class="string">'\n'</span>)</span><br><span class="line">            <span class="comment"># 现在，发送一个通知</span></span><br><span class="line">            self.notify()</span><br><span class="line">            <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> wrapped_function</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">notify</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># logit只打日志，不做别的</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h3 id="对象自省"><a href="#对象自省" class="headerlink" title="对象自省"></a>对象自省</h3><h4 id="dir"><a href="#dir" class="headerlink" title="dir"></a>dir</h4><p>用于自省的最重要的函数之一。它返回一个列表，列出了一个对象所拥有的属性和方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_list = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">dir(my_list)</span><br><span class="line"><span class="comment"># Output: ['__add__', '__class__', '__contains__', '__delattr__', '__delitem__',</span></span><br><span class="line"><span class="comment"># '__delslice__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__',</span></span><br><span class="line"><span class="comment"># '__getitem__', '__getslice__', '__gt__', '__hash__', '__iadd__', '__imul__',</span></span><br><span class="line"><span class="comment"># '__init__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__',</span></span><br><span class="line"><span class="comment"># '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__',</span></span><br><span class="line"><span class="comment"># '__setattr__', '__setitem__', '__setslice__', '__sizeof__', '__str__',</span></span><br><span class="line"><span class="comment"># '__subclasshook__', 'append', 'count', 'extend', 'index', 'insert', 'pop',</span></span><br><span class="line"><span class="comment"># 'remove', 'reverse', 'sort']</span></span><br></pre></td></tr></table></figure>
<h4 id="type-和-id"><a href="#type-和-id" class="headerlink" title="type  和 id"></a>type  和 id</h4><h4 id="inspect"><a href="#inspect" class="headerlink" title="inspect"></a>inspect</h4><p>inspect模块可以查看一个对象的成员</p>
<h3 id="推导式"><a href="#推导式" class="headerlink" title="推导式"></a>推导式</h3><h4 id="列表推导式"><a href="#列表推导式" class="headerlink" title="列表推导式"></a>列表推导式</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">variable = [out_exp <span class="keyword">for</span> out_exp <span class="keyword">in</span> input_list <span class="keyword">if</span> out_exp == <span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<h4 id="字典推导式"><a href="#字典推导式" class="headerlink" title="字典推导式"></a>字典推导式</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> some_dict.items()&#125;</span><br></pre></td></tr></table></figure>
<h3 id="For-else"><a href="#For-else" class="headerlink" title="For - else"></a>For - else</h3><p>   for循环还有一个else从句,这个else从句会在循环正常结束时执行。</p>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">10</span>):</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">    <span class="keyword">if</span> n % x == <span class="number">0</span>:</span><br><span class="line">        print(n, <span class="string">'equals'</span>, x, <span class="string">'*'</span>, n / x)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># loop fell through without finding a factor</span></span><br><span class="line">    print(n, <span class="string">'is a prime number'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="函数缓存"><a href="#函数缓存" class="headerlink" title="函数缓存"></a>函数缓存</h3><p>函数缓存允许我们将一个函数对于给定参数的返回值缓存起来。当一个I/O密集的函数被频繁使用相同的参数调用的时候，函数缓存可以节约时间。</p>
<p>Python3.2+</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"><span class="meta">@lru_cache(maxsize=32)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">return</span> fib(n<span class="number">-1</span>) + fib(n<span class="number">-2</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print([fib(n) <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">10</span>)])</span><br><span class="line"><span class="comment"># Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对返回值清空缓存</span></span><br><span class="line">fib.cache_clear()</span><br></pre></td></tr></table></figure>
<p>python2</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">memoize</span><span class="params">(function)</span>:</span></span><br><span class="line">    memo = &#123;&#125;</span><br><span class="line"><span class="meta">    @wraps(function)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> args <span class="keyword">in</span> memo:</span><br><span class="line">            <span class="keyword">return</span> memo[args]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            rv = function(*args)</span><br><span class="line">            memo[args] = rv</span><br><span class="line">            <span class="keyword">return</span> rv</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@memoize</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n &lt; <span class="number">2</span>: <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">return</span> fibonacci(n - <span class="number">1</span>) + fibonacci(n - <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">fibonacci(<span class="number">25</span>)</span><br></pre></td></tr></table></figure>
<h3 id="With-上下文管理器"><a href="#With-上下文管理器" class="headerlink" title="With 上下文管理器"></a>With 上下文管理器</h3><p>可以用来管理数据库连接关闭，文件打开关闭一个上下文管理器的类，起码要定义<strong>enter</strong>和<strong>exit</strong>方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">File</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, file_name, method)</span>:</span></span><br><span class="line">        self.file_obj = open(file_name, method)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.file_obj</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果发生异常，Python会将异常的type,value和traceback传递给__exit__方法。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, type, value, traceback)</span>:</span></span><br><span class="line">        print(<span class="string">"Exception has been handled"</span>)</span><br><span class="line">        self.file_obj.close()</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> File(<span class="string">'demo.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">    opened_file.undefined_function()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: Exception has been handled</span></span><br></pre></td></tr></table></figure>
<h3 id="Collections"><a href="#Collections" class="headerlink" title="Collections"></a>Collections</h3><h4 id="Defaultdict"><a href="#Defaultdict" class="headerlink" title="Defaultdict"></a>Defaultdict</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">dict_list = defaultdict(list)</span><br><span class="line">dict_int = defaultdict(int) <span class="comment">#0</span></span><br></pre></td></tr></table></figure>
<h4 id="Counter"><a href="#Counter" class="headerlink" title="Counter"></a>Counter</h4><p>Counter是一个计数器，它可以针对某项数据进行计数。</p>
<h4 id="Deque"><a href="#Deque" class="headerlink" title="Deque"></a>Deque</h4><p>deque提供了一个双端队列，可以从头/尾两端添加或删除元素。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = deque(range(<span class="number">5</span>))</span><br><span class="line">print(len(d))</span><br><span class="line"><span class="comment">## 输出: 5</span></span><br><span class="line">d.popleft()</span><br><span class="line"><span class="comment">## 输出: 0</span></span><br><span class="line">d.pop()</span><br><span class="line"><span class="comment">## 输出: 4</span></span><br><span class="line">print(d)</span><br><span class="line"><span class="comment">## 输出: deque([1, 2, 3])</span></span><br></pre></td></tr></table></figure>
<h4 id="Namedtuple"><a href="#Namedtuple" class="headerlink" title="Namedtuple"></a>Namedtuple</h4><p>可以像字典(dict)一样访问namedtuples，但namedtuples是不可变的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line">Animal = namedtuple(<span class="string">'Animal'</span>, <span class="string">'name age type'</span>)</span><br><span class="line">perry = Animal(name=<span class="string">"perry"</span>, age=<span class="number">31</span>, type=<span class="string">"cat"</span>)</span><br><span class="line"></span><br><span class="line">print(perry)</span><br><span class="line"><span class="comment">## 输出: Animal(name='perry', age=31, type='cat')</span></span><br><span class="line"></span><br><span class="line">print(perry.name)</span><br><span class="line"><span class="comment">## 输出: 'perry'</span></span><br></pre></td></tr></table></figure>
<h4 id="enum-Enum"><a href="#enum-Enum" class="headerlink" title="enum.Enum"></a>enum.Enum</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">from</span> enum <span class="keyword">import</span> Enum</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Species</span><span class="params">(Enum)</span>:</span></span><br><span class="line">    cat = <span class="number">1</span></span><br><span class="line">    dog = <span class="number">2</span></span><br><span class="line">    horse = <span class="number">3</span></span><br><span class="line">    aardvark = <span class="number">4</span></span><br><span class="line">    butterfly = <span class="number">5</span></span><br><span class="line">    owl = <span class="number">6</span></span><br><span class="line">    platypus = <span class="number">7</span></span><br><span class="line">    dragon = <span class="number">8</span></span><br><span class="line">    unicorn = <span class="number">9</span></span><br><span class="line">    <span class="comment"># 依次类推</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 但我们并不想关心同一物种的年龄，所以我们可以使用一个别名</span></span><br><span class="line">    kitten = <span class="number">1</span>  <span class="comment"># (译者注：幼小的猫咪)</span></span><br><span class="line">    puppy = <span class="number">2</span>   <span class="comment"># (译者注：幼小的狗狗)</span></span><br><span class="line"></span><br><span class="line">Animal = namedtuple(<span class="string">'Animal'</span>, <span class="string">'name age type'</span>)</span><br><span class="line">perry = Animal(name=<span class="string">"Perry"</span>, age=<span class="number">31</span>, type=Species.cat)</span><br><span class="line">drogon = Animal(name=<span class="string">"Drogon"</span>, age=<span class="number">4</span>, type=Species.dragon)</span><br><span class="line">tom = Animal(name=<span class="string">"Tom"</span>, age=<span class="number">75</span>, type=Species.cat)</span><br><span class="line">charlie = Animal(name=<span class="string">"Charlie"</span>, age=<span class="number">2</span>, type=Species.kitten)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>charlie.type == tom.type</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>charlie.type</span><br><span class="line">&lt;Species.cat: <span class="number">1</span>&gt;</span><br></pre></td></tr></table></figure>
<h3 id="itertools"><a href="#itertools" class="headerlink" title="itertools"></a>itertools</h3><p>itertools提供的工具相当高效且节省内存。使用这些工具，你将能够创建自己定制的迭代器用于高效率的循环。itertools主要来分为三类函数，分别为无限迭代器、输入序列迭代器、组合生成器</p>
<h4 id="无限迭代器"><a href="#无限迭代器" class="headerlink" title="无限迭代器"></a>无限迭代器</h4><h5 id="Itertools-count-start-0-step-1"><a href="#Itertools-count-start-0-step-1" class="headerlink" title="Itertools.count(start=0, step=1)"></a><strong>Itertools.count(start=0, step=1)</strong></h5><p>创建一个迭代对象，生成从start开始的连续整数，步长为step。如果省略了start则默认从0开始，步长默认为1。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> itertools.count():</span><br><span class="line">  <span class="keyword">print</span> i</span><br></pre></td></tr></table></figure>
<h5 id="Itertools-cycle-iterable"><a href="#Itertools-cycle-iterable" class="headerlink" title="Itertools.cycle(iterable)"></a><strong>Itertools.cycle(iterable)</strong></h5><p>创建一个迭代对象，对于输入的iterable的元素反复执行循环操作，内部生成iterable中的元素的一个副本，这个副本用来返回循环中的重复项。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> itertools.cycle([<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]):</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">10</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">print</span> (i, item)</span><br></pre></td></tr></table></figure>
<h5 id="Itertools-repeat-object-times"><a href="#Itertools-repeat-object-times" class="headerlink" title="Itertools.repeat(object[, times])"></a><strong>Itertools.repeat(object[, times])</strong></h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> itertools.repeat(<span class="string">'kivinsae'</span>, <span class="number">5</span>):</span><br><span class="line">    <span class="keyword">print</span> i</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># kivinsae</span></span><br><span class="line"><span class="comment"># kivinsae</span></span><br><span class="line"><span class="comment"># kivinsae</span></span><br><span class="line"><span class="comment"># kivinsae</span></span><br><span class="line"><span class="comment"># kivinsae</span></span><br></pre></td></tr></table></figure>
<h4 id="输入序列迭代器"><a href="#输入序列迭代器" class="headerlink" title="输入序列迭代器"></a>输入序列迭代器</h4><h5 id="itertools-accumulate-iterables"><a href="#itertools-accumulate-iterables" class="headerlink" title="itertools.accumulate(*iterables)"></a>itertools.accumulate(*iterables)</h5><p>这个函数简单来说就是一个累加器，不停对列表或者迭代器进行累加操作（这里指每项累加）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = itertools.accumulate(range(<span class="number">10</span>))</span><br><span class="line">print(list(x))</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># [0, 1, 3, 6, 10, 15, 21, 28, 36, 45]</span></span><br></pre></td></tr></table></figure>
<h5 id="itertools-chain-iterables"><a href="#itertools-chain-iterables" class="headerlink" title="itertools.chain(*iterables)"></a><strong>itertools.chain(*iterables)</strong></h5><p>把多个迭代器作为参数，但是只会返回单个迭代器。产生所有参数迭代器的内容，却好似来自于一个单一的序列。简单了讲就是连接多个 列表 或者 迭代器。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> itertools.chain([<span class="string">'p'</span>,<span class="string">'x'</span>,<span class="string">'e'</span>], [<span class="string">'scp'</span>, <span class="string">'nmb'</span>, <span class="string">'balenciaga'</span>]):</span><br><span class="line">    <span class="keyword">print</span> i</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># p</span></span><br><span class="line"><span class="comment"># x</span></span><br><span class="line"><span class="comment"># e</span></span><br><span class="line"><span class="comment"># scp</span></span><br><span class="line"><span class="comment"># nmb</span></span><br><span class="line"><span class="comment"># balenciaga</span></span><br></pre></td></tr></table></figure>
<h5 id="itertools-compress-data-selectors"><a href="#itertools-compress-data-selectors" class="headerlink" title="itertools.compress(data,selectors)"></a><strong>itertools.compress(data,selectors)</strong></h5><p>compress提供了一个对于原始数据的筛选功能，具体条件可以设置的非常复杂，下面列出相关的定义代码来解释。简单来理解，就是按照真值表进行元素筛选。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#  例：</span></span><br><span class="line">list(itertools.compress(<span class="string">'ABCDEF'</span>, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]))</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># ['A', 'B', 'D', 'F']</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 具体实现：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compress</span><span class="params">(data, selectors)</span>:</span></span><br><span class="line">    <span class="comment"># compress('ABCDEF', [1,0,1,0,1,1]) --&gt; A C E F</span></span><br><span class="line">    <span class="keyword">return</span> (d <span class="keyword">for</span> d, s <span class="keyword">in</span> izip(data, selectors) <span class="keyword">if</span> s)</span><br></pre></td></tr></table></figure>
<h5 id="itertools-dropwhile-predicate-iterable"><a href="#itertools-dropwhile-predicate-iterable" class="headerlink" title="itertools.dropwhile(predicate,iterable)"></a><strong>itertools.dropwhile(predicate,iterable)</strong></h5><p>dropwhile作用是创建一个迭代器，只要是函数predicate(item)为True，则丢掉iterable中的项，但是如果predicate返回的是False，则生成iterable中的项和所有的后续项。<br>具体来说就是，在条件为False之后的第一次，就返回迭代器中剩余的所有项。在这个函数表达式里面iterable的值会按索引一个个作为predicate的参数进行计算。简单来说，就是按照真值函数丢弃掉列表和迭代器前面的元素。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">should_drop</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Testing:'</span>, x</span><br><span class="line">    <span class="keyword">return</span> (x &lt; <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> itertools.dropwhile(should_drop, [<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">-2</span>]):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Yielding:'</span>, i</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># Testing: -1</span></span><br><span class="line"><span class="comment"># Testing: 0</span></span><br><span class="line"><span class="comment"># Testing: 1</span></span><br><span class="line"><span class="comment"># Yielding: 1</span></span><br><span class="line"><span class="comment"># Yielding: 2</span></span><br><span class="line"><span class="comment"># Yielding: 3</span></span><br><span class="line"><span class="comment"># Yielding: 4</span></span><br><span class="line"><span class="comment"># Yielding: 1</span></span><br><span class="line"><span class="comment"># Yielding: -2</span></span><br></pre></td></tr></table></figure>
<h5 id="itertools-groupby-iterable-key"><a href="#itertools-groupby-iterable-key" class="headerlink" title="itertools.groupby(iterable[,key])"></a><strong>itertools.groupby(iterable[,key])</strong></h5><p>返回一个集合的迭代器，集合内是按照key进行分组后的值。<br>如果iterable在多次连续的迭代中生成了同一项，则会定义一个组，如果对这个函数应用一个分类列表，那么分组会定义这个列表中所有的唯一项，key是一个函数并应用于每一项。如果这个函数有返回值，则这个值会用于后续的项，而不是和该项本身进行比较。这个函数返回的迭代器生成元素(key,group)，key是分组的键值，group是迭代器，从而生成组成这个组的所有项目。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="string">'aa'</span>, <span class="string">'ab'</span>, <span class="string">'abc'</span>, <span class="string">'bcd'</span>, <span class="string">'abcde'</span>]</span><br><span class="line"><span class="keyword">for</span> i, k <span class="keyword">in</span> itertools.groupby(a, len):</span><br><span class="line">    <span class="keyword">print</span> i, list(k)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># 2 ['aa', 'ab']</span></span><br><span class="line"><span class="comment"># 3 ['abc', 'bcd']</span></span><br><span class="line"><span class="comment"># 5 ['abcde']</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 具体实现</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">groupby</span><span class="params">(object)</span>:</span></span><br><span class="line">        <span class="comment"># [k for k, g in groupby('AAAABBBCCDAABBB')] --&gt; A B C D A B</span></span><br><span class="line">        <span class="comment"># [list(g) for k, g in groupby('AAAABBBCCD')] --&gt; AAAA BBB CC D</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, iterable, key=None)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> key <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                key = <span class="keyword">lambda</span> x: x</span><br><span class="line">            self.keyfunc = key</span><br><span class="line">            self.it = iter(iterable)</span><br><span class="line">            self.tgtkey = self.currkey = self.currvalue = object()</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> self</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">next</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="keyword">while</span> self.currkey == self.tgtkey:</span><br><span class="line">                self.currvalue = next(self.it)    <span class="comment"># Exit on StopIteration</span></span><br><span class="line">                self.currkey = self.keyfunc(self.currvalue)</span><br><span class="line">            self.tgtkey = self.currkey</span><br><span class="line">            <span class="keyword">return</span> (self.currkey, self._grouper(self.tgtkey))</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_grouper</span><span class="params">(self, tgtkey)</span>:</span></span><br><span class="line">            <span class="keyword">while</span> self.currkey == tgtkey:</span><br><span class="line">                <span class="keyword">yield</span> self.currvalue</span><br><span class="line">                self.currvalue = next(self.it)    <span class="comment"># Exit on StopIteration</span></span><br><span class="line">                self.currkey = self.keyfunc(self.currvalue)</span><br></pre></td></tr></table></figure>
<h5 id="itertools-ifilter-predicate-iterable"><a href="#itertools-ifilter-predicate-iterable" class="headerlink" title="itertools.ifilter(predicate,iterable)"></a><strong>itertools.ifilter(predicate,iterable)</strong></h5><p>函数返回一个迭代器，类似于针对于列表的函数filter()，但是只包括测试函数返回True时候的值。和dropwhile()作用不同。<br>函数创建一个迭代器，只生成predicate(iterable)为True的项，简单来说就是返回iterable中所有计算后为True的项。如果是非True则进行之后的其他操作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_item</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Testing:'</span>, x</span><br><span class="line">    <span class="keyword">return</span> (x&lt;<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> itertools.ifilter(check_item, [ <span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">-2</span> ]):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Yielding:'</span>, i</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># Testing: -1</span></span><br><span class="line"><span class="comment"># Yielding: -1</span></span><br><span class="line"><span class="comment"># Testing: 0</span></span><br><span class="line"><span class="comment"># Yielding: 0</span></span><br><span class="line"><span class="comment"># Testing: 1</span></span><br><span class="line"><span class="comment"># Testing: 2</span></span><br><span class="line"><span class="comment"># Testing: 3</span></span><br><span class="line"><span class="comment"># Testing: 4</span></span><br><span class="line"><span class="comment"># Testing: 1</span></span><br><span class="line"><span class="comment"># Testing: -2</span></span><br><span class="line"><span class="comment"># Yielding: -2</span></span><br></pre></td></tr></table></figure>
<h5 id="itertools-ifilterfalse-predicate-iterable"><a href="#itertools-ifilterfalse-predicate-iterable" class="headerlink" title="itertools.ifilterfalse(predicate,iterable)"></a><strong>itertools.ifilterfalse(predicate,iterable)</strong></h5><p>本函数和上面的ifilter一样，唯一的区别是只有当predicate(iterable)为False时候才进行predicate的输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_item</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Testing:'</span>, x</span><br><span class="line">    <span class="keyword">return</span> (x &lt; <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> itertools.ifilterfalse(check_item, [<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">-2</span>]):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Yielding:'</span>, i</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># Testing: -1</span></span><br><span class="line"><span class="comment"># Testing: 0</span></span><br><span class="line"><span class="comment"># Testing: 1</span></span><br><span class="line"><span class="comment"># Yielding: 1</span></span><br><span class="line"><span class="comment"># Testing: 2</span></span><br><span class="line"><span class="comment"># Yielding: 2</span></span><br><span class="line"><span class="comment"># Testing: 3</span></span><br><span class="line"><span class="comment"># Yielding: 3</span></span><br><span class="line"><span class="comment"># Testing: 4</span></span><br><span class="line"><span class="comment"># Yielding: 4</span></span><br><span class="line"><span class="comment"># Testing: 1</span></span><br><span class="line"><span class="comment"># Yielding: 1</span></span><br><span class="line"><span class="comment"># Testing: -2</span></span><br></pre></td></tr></table></figure>
<h5 id="itertools-islice-iterable-stop"><a href="#itertools-islice-iterable-stop" class="headerlink" title="itertools.islice(iterable,stop)"></a><strong>itertools.islice(iterable,stop)</strong></h5><p>简单来说这个函数，就是对于一个迭代对象iterable，设定一个特定的切片/选取/截取规则，然后最后输出一个特定的新的迭代对象的过程。这个stop实际上代表一个三元数组，也就是start，stop，step。如果start省略，默认从索引0开始；如果step被省略，则默认步长为1；stop不能被省略。本质就是一个切片工具。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'Stop at 5:'</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> itertools.islice(itertools.count(), <span class="number">5</span>):</span><br><span class="line">    <span class="keyword">print</span> i</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'Start at 5, Stop at 10:'</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> itertools.islice(itertools.count(), <span class="number">5</span>, <span class="number">10</span>):</span><br><span class="line">    <span class="keyword">print</span> i</span><br><span class="line"><span class="keyword">print</span> <span class="string">'By tens to 100:'</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> itertools.islice(itertools.count(), <span class="number">0</span>, <span class="number">100</span>, <span class="number">10</span>):</span><br><span class="line">    <span class="keyword">print</span> i</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># Stop</span></span><br><span class="line"><span class="comment"># at</span></span><br><span class="line"><span class="comment"># 5:</span></span><br><span class="line"><span class="comment"># 0</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"><span class="comment"># 4</span></span><br><span class="line"><span class="comment"># Start</span></span><br><span class="line"><span class="comment"># at</span></span><br><span class="line"><span class="comment"># 5, Stop</span></span><br><span class="line"><span class="comment"># at</span></span><br><span class="line"><span class="comment"># 10:</span></span><br><span class="line"><span class="comment"># 5</span></span><br><span class="line"><span class="comment"># 6</span></span><br><span class="line"><span class="comment"># 7</span></span><br><span class="line"><span class="comment"># 8</span></span><br><span class="line"><span class="comment"># 9</span></span><br><span class="line"><span class="comment"># By</span></span><br><span class="line"><span class="comment"># tens</span></span><br><span class="line"><span class="comment"># to</span></span><br><span class="line"><span class="comment"># 100:</span></span><br><span class="line"><span class="comment"># 0</span></span><br><span class="line"><span class="comment"># 10</span></span><br><span class="line"><span class="comment"># 20</span></span><br><span class="line"><span class="comment"># 30</span></span><br><span class="line"><span class="comment"># 40</span></span><br><span class="line"><span class="comment"># 50</span></span><br><span class="line"><span class="comment"># 60</span></span><br><span class="line"><span class="comment"># 70</span></span><br><span class="line"><span class="comment"># 80</span></span><br><span class="line"><span class="comment"># 90</span></span><br></pre></td></tr></table></figure>
<h5 id="itertools-imap-function-iterable"><a href="#itertools-imap-function-iterable" class="headerlink" title="itertools.imap(function,*iterable)"></a><strong>itertools.imap(function,*iterable)</strong></h5><p>本函数创建一个迭代器，作用函数为function1，function2，function3…，对应的变量来自迭代器iterable1，iterable2，iterable3…。然后返回一个(f1,f2,f3…)形式的元组。只要其中一个迭代器不再生成值，这个函数就会停止。所以要处理好None的情况，用一下替代输出之类的方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">'Doubles:'</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> itertools.imap(<span class="keyword">lambda</span> x:<span class="number">2</span>*x, xrange(<span class="number">5</span>)):</span><br><span class="line">    <span class="keyword">print</span> i</span><br><span class="line"><span class="keyword">print</span> <span class="string">'Multiples:'</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> itertools.imap(<span class="keyword">lambda</span> x,y:(x, y, x*y), xrange(<span class="number">5</span>), xrange(<span class="number">5</span>,<span class="number">10</span>)):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'%d * %d = %d'</span> % i</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># Doubles:</span></span><br><span class="line"><span class="comment"># 0</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 4</span></span><br><span class="line"><span class="comment"># 6</span></span><br><span class="line"><span class="comment"># 8</span></span><br><span class="line"><span class="comment"># Multiples:</span></span><br><span class="line"><span class="comment"># 0 * 5 = 0</span></span><br><span class="line"><span class="comment"># 1 * 6 = 6</span></span><br><span class="line"><span class="comment"># 2 * 7 = 14</span></span><br><span class="line"><span class="comment"># 3 * 8 = 24</span></span><br><span class="line"><span class="comment"># 4 * 9 = 36</span></span><br></pre></td></tr></table></figure>
<h5 id="itertools-starmap-function-iterable"><a href="#itertools-starmap-function-iterable" class="headerlink" title="itertools.starmap(function,iterable)"></a><strong>itertools.starmap(function,iterable)</strong></h5><p>函数创建一个函数，其中内调用的function(*item)，item来自于iterable。只有当迭代对象iterable生成的项适合这个函数的调用形式的时候，starmap才会有效。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> itertools.starmap(<span class="keyword">lambda</span> x,y:(x, y, x*y), [(<span class="number">0</span>, <span class="number">5</span>), (<span class="number">1</span>, <span class="number">6</span>), (<span class="number">2</span>, <span class="number">7</span>), (<span class="number">3</span>, <span class="number">8</span>), (<span class="number">4</span>, <span class="number">9</span>)]):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'%d * %d = %d'</span> % i</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># 0 * 5 = 0</span></span><br><span class="line"><span class="comment"># 1 * 6 = 6</span></span><br><span class="line"><span class="comment"># 2 * 7 = 14</span></span><br><span class="line"><span class="comment"># 3 * 8 = 24</span></span><br><span class="line"><span class="comment"># 4 * 9 = 36</span></span><br></pre></td></tr></table></figure>
<h5 id="itertools-tee-iterable-n-2"><a href="#itertools-tee-iterable-n-2" class="headerlink" title="itertools.tee(iterable[,n=2])"></a><strong>itertools.tee(iterable[,n=2])</strong></h5><p>函数会返回若干个基于某个原始输入的独立迭代器。类似于Linux系统上的tee指令。如果不特地制定n的话，函数会默认是2。tee括号里面最好使用标准输入，而不是原始迭代器。不然会在某些缓存过程中出现异常。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r = itertools.islice(itertools.count(), <span class="number">5</span>)</span><br><span class="line">i1, i2 = itertools.tee(r)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> i1:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'i1:'</span>, i</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> i2:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'i2:'</span>, i</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># i1: 0</span></span><br><span class="line"><span class="comment"># i1: 1</span></span><br><span class="line"><span class="comment"># i1: 2</span></span><br><span class="line"><span class="comment"># i1: 3</span></span><br><span class="line"><span class="comment"># i1: 4</span></span><br><span class="line"><span class="comment"># i2: 0</span></span><br><span class="line"><span class="comment"># i2: 1</span></span><br><span class="line"><span class="comment"># i2: 2</span></span><br><span class="line"><span class="comment"># i2: 3</span></span><br><span class="line"><span class="comment"># i2: 4</span></span><br></pre></td></tr></table></figure>
<h5 id="itertools-takewhile-predicate-iterable"><a href="#itertools-takewhile-predicate-iterable" class="headerlink" title="itertools.takewhile(predicate,iterable)"></a><strong>itertools.takewhile(predicate,iterable)</strong></h5><p>函数和dropwhile刚好相反，只要predicate计算后为False，迭代过程立刻停止。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">should_take</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Testing:'</span>, x</span><br><span class="line">    <span class="keyword">return</span> (x&lt;<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> itertools.takewhile(should_take, [ <span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">-2</span> ]):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Yielding:'</span>, i</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># Testing: -1</span></span><br><span class="line"><span class="comment"># Yielding: -1</span></span><br><span class="line"><span class="comment"># Testing: 0</span></span><br><span class="line"><span class="comment"># Yielding: 0</span></span><br><span class="line"><span class="comment"># Testing: 1</span></span><br><span class="line"><span class="comment"># Yielding: 1</span></span><br><span class="line"><span class="comment"># Testing: 2</span></span><br></pre></td></tr></table></figure>
<h5 id="itertools-izip-iterables"><a href="#itertools-izip-iterables" class="headerlink" title="itertools.izip( *iterables)"></a><strong>itertools.izip( *iterables)</strong></h5><p>这个函数返回一个合并多个迭代器，成为一个元组的迭代对象。类似于内置函数zip，但返回的是迭代对象而非列表。<br>创建一个迭代对象，生成元组(i1,i2,i3…)分别来自于i1,i2,i3…，只要提供的某个迭代器不在生成值，函数就会立刻停止。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> itertools.izip([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]):</span><br><span class="line">    <span class="keyword">print</span> i</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># (1, 'a')</span></span><br><span class="line"><span class="comment"># (2, 'b')</span></span><br><span class="line"><span class="comment"># (3, 'c')</span></span><br></pre></td></tr></table></figure>
<h5 id="itertools-izip-longest-iterable-fillvalue"><a href="#itertools-izip-longest-iterable-fillvalue" class="headerlink" title="itertools.izip_longest(*iterable[,fillvalue])"></a><strong>itertools.izip_longest(*iterable[,fillvalue])</strong></h5><p>函数和izip雷同，但是区别在于不会停止，会把所有输入的迭代对象全部耗尽为止，对于参数不匹配的项，会用None代替。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> itertools.izip_longest([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="string">'a'</span>, <span class="string">'b'</span>]):</span><br><span class="line">    <span class="keyword">print</span> i</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># (1, 'a')</span></span><br><span class="line"><span class="comment"># (2, 'b')</span></span><br><span class="line"><span class="comment"># (3, None)</span></span><br></pre></td></tr></table></figure>
<h4 id="组合生成器"><a href="#组合生成器" class="headerlink" title="组合生成器"></a>组合生成器</h4><h5 id="itertools-product-iterable-repeat"><a href="#itertools-product-iterable-repeat" class="headerlink" title="itertools.product(*iterable[,repeat])"></a><strong>itertools.product(*iterable[,repeat])</strong></h5><p>这个工具就是产生多个列表或者迭代器的n维积。如果没有特别指定repeat默认为列表和迭代器的数量。（笛卡尔积）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = (<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>)</span><br><span class="line">c = itertools.product(a,b)</span><br><span class="line"><span class="keyword">for</span> elem <span class="keyword">in</span> c:</span><br><span class="line">    <span class="keyword">print</span> elem</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># (1, 'A')</span></span><br><span class="line"><span class="comment"># (1, 'B')</span></span><br><span class="line"><span class="comment"># (1, 'C')</span></span><br><span class="line"><span class="comment"># (2, 'A')</span></span><br><span class="line"><span class="comment"># (2, 'B')</span></span><br><span class="line"><span class="comment"># (2, 'C')</span></span><br><span class="line"><span class="comment"># (3, 'A')</span></span><br><span class="line"><span class="comment"># (3, 'B')</span></span><br><span class="line"><span class="comment"># (3, 'C')</span></span><br></pre></td></tr></table></figure>
<h5 id="itertools-permutations-iterable-r"><a href="#itertools-permutations-iterable-r" class="headerlink" title="itertools.permutations(iterable[,r])"></a><strong>itertools.permutations(iterable[,r])</strong></h5><p>函数作用其实就是产生指定数目repeat的元素的所有排列，且顺序有关，但是遇到原列表或者迭代器有重复元素的现象的时候，也会对应的产生重复项。这个时候最好用groupby或者其他filter去一下重，如果有需要的话。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = itertools.permutations(range(<span class="number">4</span>), <span class="number">3</span>)</span><br><span class="line">print(list(x))</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># [(0, 1, 2), </span></span><br><span class="line"><span class="comment"># (0, 1, 3), </span></span><br><span class="line"><span class="comment"># (0, 2, 1), </span></span><br><span class="line"><span class="comment"># (0, 2, 3), </span></span><br><span class="line"><span class="comment"># (0, 3, 1), </span></span><br><span class="line"><span class="comment"># (0, 3, 2), </span></span><br><span class="line"><span class="comment"># (1, 0, 2), </span></span><br><span class="line"><span class="comment"># (1, 0, 3), </span></span><br><span class="line"><span class="comment"># (1, 2, 0), </span></span><br><span class="line"><span class="comment"># (1, 2, 3), </span></span><br><span class="line"><span class="comment"># (1, 3, 0), </span></span><br><span class="line"><span class="comment"># (1, 3, 2), </span></span><br><span class="line"><span class="comment"># (2, 0, 1), </span></span><br><span class="line"><span class="comment"># (2, 0, 3), </span></span><br><span class="line"><span class="comment"># (2, 1, 0), </span></span><br><span class="line"><span class="comment"># (2, 1, 3), </span></span><br><span class="line"><span class="comment"># (2, 3, 0), </span></span><br><span class="line"><span class="comment"># (2, 3, 1), </span></span><br><span class="line"><span class="comment"># (3, 0, 1),</span></span><br><span class="line"><span class="comment"># (3, 0, 2), </span></span><br><span class="line"><span class="comment"># (3, 1, 0), </span></span><br><span class="line"><span class="comment"># (3, 1, 2), </span></span><br><span class="line"><span class="comment"># (3, 2, 0), </span></span><br><span class="line"><span class="comment"># (3, 2, 1)</span></span><br><span class="line"><span class="comment"># ]</span></span><br></pre></td></tr></table></figure>
<h5 id="itertools-combinations-iterable-r"><a href="#itertools-combinations-iterable-r" class="headerlink" title="itertools.combinations(iterable,r)"></a><strong>itertools.combinations(iterable,r)</strong></h5><p>这个函数用来生成指定数目r的元素不重复的所有组合。注意和permutation的区分，以及这个组合是无序的，只考虑元素本身的unique性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = itertools.combinations(range(<span class="number">4</span>), <span class="number">3</span>)</span><br><span class="line">print(list(x))</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># [(0, 1, 2), (0, 1, 3), (0, 2, 3), (1, 2, 3)]</span></span><br></pre></td></tr></table></figure>
<h5 id="itertools-combinations-with-replacement-iterable-r"><a href="#itertools-combinations-with-replacement-iterable-r" class="headerlink" title="itertools.combinations_with_replacement(iterable,r)"></a><strong>itertools.combinations_with_replacement(iterable,r)</strong></h5><p>这个函数用来生成指定数目r的元素可重复的所有组合。然而这个函数依然要保证元素组合的unique性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = itertools.combinations_with_replacement(<span class="string">'ABC'</span>, <span class="number">2</span>)</span><br><span class="line">print(list(x))</span><br><span class="line"><span class="comment"># 输出为：</span></span><br><span class="line"><span class="comment"># [('A', 'A'), </span></span><br><span class="line"><span class="comment"># ('A', 'B'), </span></span><br><span class="line"><span class="comment"># ('A', 'C'), </span></span><br><span class="line"><span class="comment"># ('B', 'B'), </span></span><br><span class="line"><span class="comment"># ('B', 'C'), </span></span><br><span class="line"><span class="comment"># ('C', 'C’)</span></span><br><span class="line"><span class="comment"># ]</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title>requests</title>
    <url>/2019/07/19/requests/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><ol>
<li>爬虫的基本流程<ul>
<li><strong>发起请求</strong><br>通过HTTP库向目标站点发起请求，也就是发送一个Request，请求可以包含额外的header等信息，等待服务器响应</li>
<li><strong>获取响应内容</strong><br>如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能是HTML,Json字符串，二进制数据（图片或者视频）等类型</li>
<li><strong>解析内容</strong><br>得到的内容可能是HTML,可以用正则表达式，页面解析库进行解析，可能是Json,可以直接转换为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理</li>
<li><strong>保存数据</strong><br>保存形式多样，可以存为文本，也可以保存到数据库，或者保存特定格式的文件</li>
</ul>
</li>
</ol>
<a id="more"></a>
<ol>
<li><p><strong>Request中包含什么？</strong></p>
<ul>
<li><p><strong>请求方式</strong></p>
<p>主要有：GET/POST两种类型常用，另外还有HEAD/PUT/DELETE/OPTIONS</p>
<ol>
<li><p>GET和POST的区别就是：请求的数据GET是在url中，POST则是存放在头部</p>
<p>GET:向指定的资源发出“显示”请求。使用GET方法应该只用在读取数据，而不应当被用于产生“副作用”的操作中，例如在Web Application中。其中一个原因是GET可能会被网络蜘蛛等随意访问</p>
<p>POST:向指定资源提交数据，请求服务器进行处理（例如提交表单或者上传文件）。数据被包含在请求本文中。这个请求可能会创建新的资源或修改现有资源，或二者皆有。</p>
</li>
<li><p>HEAD：与GET方法一样，都是向服务器发出指定资源的请求。只不过服务器将不传回资源的本文部分。它的好处在于，使用这个方法可以在不必传输全部内容的情况下，就可以获取其中“关于该资源的信息”（元信息或称元数据）。</p>
</li>
<li><p>PUT：向指定资源位置上传其最新内容。</p>
</li>
<li><p>OPTIONS：这个方法可使服务器传回该资源所支持的所有HTTP请求方法。用’*’来代替资源名称，向Web服务器发送OPTIONS请求，可以测试服务器功能是否正常运作。</p>
</li>
<li><p>DELETE：请求服务器删除Request-URI所标识的资源。</p>
</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>请求URL</strong></p>
<p>URL，即统一资源定位符，也就是我们说的网址，统一资源定位符是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址。互联网上的每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。</p>
<p>URL的格式由三个部分组成：</p>
<ul>
<li>第一部分是协议(或称为服务方式)。</li>
<li>第二部分是存有该资源的主机IP地址(有时也包括端口号)。</li>
<li>第三部分是主机资源的具体地址，如目录和文件名等。</li>
</ul>
<p>爬虫爬取数据时必须要有一个目标的URL才可以获取数据，因此，它是爬虫获取数据的基本依据。</p>
</li>
<li><p><strong>请求头</strong></p>
<p>包含请求时的头部信息，如User-Agent,Host,Cookies等信息.</p>
</li>
<li><p><strong>请求体</strong></p>
<p>请求是携带的数据，如提交表单数据时候的表单数据（POST）</p>
</li>
<li><p>能爬取什么样的数据？</p>
<ul>
<li>网页文本：如HTML文档，Json格式化文本等</li>
<li>图片：获取到的是二进制文件，保存为图片格式</li>
<li>视频：同样是二进制文件</li>
<li>其他：只要请求到的，都可以获取</li>
</ul>
</li>
<li><p><strong>如何解析数据？</strong></p>
<ol>
<li>直接处理</li>
<li>Json解析</li>
<li>正则表达式处理</li>
<li>BeautifulSoup解析处理</li>
<li>PyQuery解析处理</li>
<li>XPath解析处理</li>
</ol>
</li>
<li><p><strong>关于抓取的页面数据和浏览器里看到的不一样的问题？</strong></p>
<p>出现这种情况是因为，很多网站中的数据都是通过js，ajax动态加载的，所以直接通过get请求获取的页面和浏览器显示的不同。</p>
</li>
<li><p><strong>如何解决js渲染的问题？</strong></p>
<ul>
<li>分析ajax</li>
<li>Selenium/webdriver</li>
<li>Splash</li>
</ul>
</li>
<li><p><strong>怎样保存数据？</strong></p>
<ul>
<li>文本：纯文本，Json,Xml等</li>
<li>关系型数据库：如mysql,oracle,sql server等结构化数据库</li>
<li>非关系型数据库：MongoDB,Redis等key-value形式存储</li>
</ul>
</li>
<li><p><strong>request的应用</strong></p>
<ol>
<li><p>Requests是用python语言基于urllib编写的，采用的是Apache2 Licensed开源协议的HTTP库。一句话，requests是python实现的最简单易用的HTTP库，建议爬虫使用requests库。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response  = requests.get(<span class="string">"https://www.baidu.com"</span>)</span><br><span class="line">print(type(response))</span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.text) </span><br><span class="line">print(response.cookies)</span><br><span class="line">print(response.content) <span class="comment">#这样获取的数据是二进制数据</span></span><br><span class="line">print(response.content.decode(<span class="string">"utf-8"</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>很多情况下的网站如果直接response.text会出现乱码的问题，所以这个使用response.content这样返回的数据格式其实是二进制格式，然后通过decode()转换为utf-8，这样就解决了通过response.text直接返回显示乱码的问题.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response =requests.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line">response.encoding=<span class="string">"utf-8"</span></span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>
<p>不管是通过response.content.decode(“utf-8)的方式还是通过response.encoding=”utf-8”的方式都可以避免乱码的问题发生</p>
</li>
<li><p><strong>requests包中的请求方式</strong></p>
<p>requests里提供个各种请求方式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">requests.post(<span class="string">"http://httpbin.org/post"</span>)</span><br><span class="line">requests.put(<span class="string">"http://httpbin.org/put"</span>)</span><br><span class="line">requests.delete(<span class="string">"http://httpbin.org/delete"</span>)</span><br><span class="line">requests.head(<span class="string">"http://httpbin.org/get"</span>)</span><br><span class="line">requests.options(<span class="string">"http://httpbin.org/get"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基本GET请求</span></span><br><span class="line">requests.get(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line"><span class="comment"># 带参数的GET请求</span></span><br><span class="line">requests.get(<span class="string">"http://httpbin.org/get?name=zhaofan&amp;age=23"</span>)</span><br><span class="line"><span class="comment"># 或者 使用params关键字传递参数，如果字典中的参数为None则不会添加到url上</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">"name"</span>:<span class="string">"zhaofan"</span>,</span><br><span class="line">    <span class="string">"age"</span>:<span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">"http://httpbin.org/get"</span>,params=data)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>response的主要属性</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response = requests.get(<span class="string">'http://www.jianshu.com'</span>)</span><br><span class="line">print(type(response.status_code), response.status_code)</span><br><span class="line">print(type(response.headers), response.headers)</span><br><span class="line">print(type(response.cookies), response.cookies)</span><br><span class="line">print(type(response.url), response.url)</span><br><span class="line">print(type(response.history), response.history)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>解析json</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response = requests.get(<span class="string">"http://httpbin.org/get"</span>)</span><br><span class="line"><span class="comment"># requests里面集成的json其实就是执行了json.loads()方法，两者的结果是一样的</span></span><br><span class="line">print(response.json())</span><br><span class="line">print(json.loads(response.text))</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>添加headers</strong></p>
<p>可以定制headers的信息，如当我们直接通过requests请求知乎网站的时候，默认是无法访问的,因为访问知乎需要头部信息，这个时候我们在谷歌浏览器里输入chrome://version,就可以看到用户代理，将用户代理添加到头部信息.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="string">"User-Agent"</span>:<span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"</span></span><br><span class="line">&#125;</span><br><span class="line">response =requests.get(<span class="string">"https://www.zhihu.com"</span>,headers=headers)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>基本POST请求</strong></p>
<p>通过在发送post请求时添加一个data参数，这个data参数可以通过字典构造成，这样对于发送post请求就非常方便.同样的在发送post请求的时候也可以和发送get请求一样通过headers参数传递一个字典类型的数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = &#123;</span><br><span class="line">    <span class="string">"name"</span>:<span class="string">"zhaofan"</span>,</span><br><span class="line">    <span class="string">"age"</span>:<span class="number">23</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.post(<span class="string">"http://httpbin.org/post"</span>,data=data)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>关于请求状态</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response= requests.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"><span class="keyword">if</span> response.status_code == requests.codes.ok:</span><br><span class="line">    print(<span class="string">"访问成功"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>文件上传</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">files= &#123;<span class="string">"files"</span>:open(<span class="string">"git.jpeg"</span>,<span class="string">"rb"</span>)&#125;</span><br><span class="line">response = requests.post(<span class="string">"http://httpbin.org/post"</span>,files=files)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>获取cookie</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response = requests.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line">print(response.cookies)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key,value <span class="keyword">in</span> response.cookies.items():</span><br><span class="line">    print(key+<span class="string">"="</span>+value)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>会话维持</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = requests.Session()</span><br><span class="line">s.get(<span class="string">"http://httpbin.org/cookies/set/number/123456"</span>)</span><br><span class="line">response = s.get(<span class="string">"http://httpbin.org/cookies"</span>)</span><br><span class="line">print(response.text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面是错误示范</span></span><br><span class="line">requests.get(<span class="string">"http://httpbin.org/cookies/set/number/123456"</span>)</span><br><span class="line">response = requests.get(<span class="string">"http://httpbin.org/cookies"</span>)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>证书验证</strong></p>
<p>现在很多网站都需要证书的验证，没有证书会出现“你访问的不是一个私密链接”之类的错误。对于Https协议，直首先会检查证书是否合法，如果证书不合法，则会抛出：SSLError。针对这一点有两种措施：<br>下面这种方法：在访问的时候，设置不进行证书的验证，此时返回状态码200，但是依旧会有警告。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.packages <span class="keyword">import</span> urllib3</span><br><span class="line">urllib3.disable_warnings()</span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, verify=<span class="literal">False</span>)</span><br><span class="line">print(response.status_code)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动指定证书</span></span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, cert=(<span class="string">'/path/server.crt'</span>, <span class="string">'/path/key'</span>))</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>代理的设置</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">proxies = &#123;</span><br><span class="line">  <span class="string">"http"</span>: <span class="string">"http://127.0.0.1:9743"</span>,</span><br><span class="line">  <span class="string">"https"</span>: <span class="string">"https://127.0.0.1:9743"</span>,</span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">"https://www.taobao.com"</span>, proxies=proxies)</span><br><span class="line">print(response.status_code)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于需要用户名和密码的代理</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">"http"</span>: <span class="string">"http://user:password@127.0.0.1:9743/"</span>,<span class="comment">#指定好用户名和密码</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">"https://www.taobao.com"</span>, proxies=proxies)</span><br><span class="line">print(response.status_code)</span><br><span class="line"><span class="comment"># socks代理：先安装该模块   pip3 install 'requests[socks]'</span></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'socks5://127.0.0.1:9742'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'socks5://127.0.0.1:9742'</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">"https://www.taobao.com"</span>, proxies=proxies)</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>超时设置</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> ReadTimeout</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = requests.get(<span class="string">"http://httpbin.org/get"</span>, timeout = <span class="number">0.5</span>)</span><br><span class="line">    print(response.status_code)</span><br><span class="line"><span class="keyword">except</span> ReadTimeout:</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>需要登录认证才能访问的网站</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.auth <span class="keyword">import</span> HTTPBasicAuth</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'http://120.27.34.24:9001'</span>, auth=HTTPBasicAuth(<span class="string">'user'</span>, <span class="string">'123'</span>))</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">r = requests.get(<span class="string">'http://120.27.34.24:9001'</span>, auth=(<span class="string">'user'</span>, <span class="string">'123'</span>))</span><br><span class="line">print(r.status_code)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>异常处理</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> ReadTimeout, ConnectionError, RequestException</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = requests.get(<span class="string">"http://httpbin.org/get"</span>, timeout = <span class="number">0.5</span>)</span><br><span class="line">    print(response.status_code)</span><br><span class="line"><span class="keyword">except</span> ReadTimeout:</span><br><span class="line">    print(<span class="string">'Timeout'</span>)</span><br><span class="line"><span class="keyword">except</span> ConnectionError:</span><br><span class="line">    print(<span class="string">'Connection error'</span>)</span><br><span class="line"><span class="keyword">except</span> RequestException:</span><br><span class="line">    print(<span class="string">'Error'</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>专业技能</tag>
      </tags>
  </entry>
  <entry>
    <title>scrapy</title>
    <url>/2019/08/31/scrapy/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id><a href="#" class="headerlink" title=" "></a> </h3><p><img src="/2019/08/31/scrapy/scrapy_framework.png" alt="avatar"></p>
<a id="more"></a>
<p><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/" target="_blank" rel="noopener">scrapy中文文档</a></p>
<h3 id="1-编写scrapy爬虫步骤"><a href="#1-编写scrapy爬虫步骤" class="headerlink" title="1. 编写scrapy爬虫步骤"></a>1. 编写scrapy爬虫步骤</h3><ol>
<li><p>新建项目：（scrapy startproject projectname）:新建爬虫项目</p>
</li>
<li><p>创建爬虫：scrapy genspider spidername “<a href="http://www.xxx.cn/" target="_blank" rel="noopener">http://www.xxx.cn/</a>“</p>
</li>
<li><p>明确目标：（编写items.py）：明确想要抓取的目标</p>
</li>
<li><p>制作爬虫：（spiders/xxspider.py）:制作爬虫开始爬取的网页</p>
</li>
<li><p>编写pipeline.py，处理spider返回的item数据。写Pipeline函数</p>
</li>
<li><p>编写settings.py,启动管道组件ITEM_PIPELINES={}，以及其他相关设置USER_AGENT,DEFAULT_REQUEST_HEADERS</p>
</li>
<li><p>执行爬虫</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> cmdline</span><br><span class="line">cmdline.execute([<span class="string">'scrapy'</span>,<span class="string">'crawl'</span>,<span class="string">'cib'</span>])</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="2-创建爬虫"><a href="#2-创建爬虫" class="headerlink" title="2. 创建爬虫"></a>2. 创建爬虫</h3><ol>
<li>scrapy genspider spidername “<a href="http://www.xxx.cn/" target="_blank" rel="noopener">http://www.xxx.cn/</a>“<ul>
<li>genspider:表示生成一个爬虫（默认是scrapy.Spider类）</li>
<li>spidername：表示爬虫名（对应爬虫代码里的name参数）</li>
<li>“<a href="http://www.xxx.cn/" target="_blank" rel="noopener">http://www.xxx.cn/</a>“ ：表示允许爬虫爬取的域范围</li>
</ul>
</li>
<li>spidername.py<ul>
<li>name= ‘’:爬虫的识别名称，唯一</li>
<li>allow_domains=[] ：搜索的域名范围，爬虫的约束区域，规定爬虫只爬取这个域名下的网页，不存在的url会被忽略</li>
<li>start_urls=():爬取的url列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成</li>
<li>parse(self,response):解析的方法，每个初始URL完成下载后将被调用，调用的时候传入每一个URL传回的Response对象来作为唯一参数，主要作用如下：</li>
<li>负责解析返回的网页数据（response.body），提取结构化数据（生成item）</li>
<li>生成需要下一页的URL请求</li>
<li>start_requests(self):这个方法必须返回一个可迭代对象。该对象包含spider用于爬取（默认实现是使用start_urls的url）的第一个Request。当spider启动爬取并且为指定start_urls时，调用该方法</li>
<li>log（self,message[,level,component]）:使用scrapy.log.msg()方法记录（log）message</li>
</ul>
</li>
</ol>
<h3 id="3-执行爬虫"><a href="#3-执行爬虫" class="headerlink" title="3. 执行爬虫"></a>3. 执行爬虫</h3><ol>
<li>scrapy crawl spidername -o save_filename<ul>
<li>crawl：表示启动一个scrapy爬虫</li>
<li>spidername：表示需要启动的爬虫名（对应爬虫代码里的name参数）</li>
<li>-0 :表示输出到文件</li>
<li>save_filename:表示保存文件的名称,，默认4种输出文件格式：json，jsonl，csv，xml</li>
</ul>
</li>
</ol>
<h3 id="4-查看当前项目下的爬虫"><a href="#4-查看当前项目下的爬虫" class="headerlink" title="4. 查看当前项目下的爬虫"></a>4. 查看当前项目下的爬虫</h3><ol>
<li>scrapy list</li>
</ol>
<h3 id="5-pipeline-的一些典型应用"><a href="#5-pipeline-的一些典型应用" class="headerlink" title="5. pipeline 的一些典型应用"></a>5. pipeline 的一些典型应用</h3><ol>
<li>验证爬取的数据（检查item包含某些字段，比如说name）</li>
<li>数据查重（并丢弃）</li>
<li>将爬取结果保存到文件或者数据库中</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SomethingPiple</span><span class="params">(object)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="comment"># 可选实现，做参数初始化，比如打开文件操作f.open('xxx','w',edcoding='utf-8')</span></span><br><span class="line">            <span class="comment"># doing something</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">            <span class="comment"># spider(Spider 对象) - 被关闭的spider</span></span><br><span class="line">            <span class="comment"># 可选实现，当spider被开启时，这个方法被调用。</span></span><br><span class="line">            <span class="comment"># 该方法和__init__方法功能基本相同。</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self,item,spider)</span>:</span></span><br><span class="line">            <span class="comment"># item(Item对象) - 被爬取的item</span></span><br><span class="line">            <span class="comment"># spider （Spider对象） - 爬取该item的spider</span></span><br><span class="line">            <span class="comment"># 这个方法必须实现，每个item pipeline 组件都需要调用该方法</span></span><br><span class="line">            <span class="comment"># 这个方法必须返回一个Item对象，被丢弃的item将不会被之后的pipeline组件处理</span></span><br><span class="line">            <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">            <span class="comment"># spider(Spider 对象) - 被关闭的spider</span></span><br><span class="line">            <span class="comment"># 可选实现，当spider被关闭时，这个方法被调用。</span></span><br><span class="line">            <span class="comment"># 比如关闭初始化打开的文件f.close()</span></span><br></pre></td></tr></table></figure>
<h3 id="6-启动Scrapy-Shell"><a href="#6-启动Scrapy-Shell" class="headerlink" title="6. 启动Scrapy Shell"></a>6. 启动Scrapy Shell</h3><p>命令：scrapy shell “www.baidu.com”</p>
<h3 id="7-selector-选择器"><a href="#7-selector-选择器" class="headerlink" title="7. selector 选择器"></a>7. selector 选择器</h3><ol>
<li><p>Selector有四个基本方法，最常用的是xpath：</p>
<ol>
<li><p>xpath（）：传入xpath表达式，返回该表达式所对应的所有节点的selector list列表</p>
<p>XPath表达式的例子及对应含义：</p>
<ul>
<li>/html/head/title:选择<html>文档中<head>标签内的<title>元素</title></head></html></li>
<li>/html/head/title/text():选择<html>文档中<head>标签内的<title>元素的文字</title></head></html></li>
<li>//td：选择所有的<td>元素</td></li>
<li>//div[@class=”mine”]:选择所有具有class=”mine”属性的div元素</li>
</ul>
</li>
<li><p>extract（）：序列化该结点为Unicode字符串，并返回list</p>
</li>
<li><p>css()：传入css表达式，返回该表达式所对应的所有节点的selector list列表，语法同BeautifulSoup4</p>
</li>
<li><p>re()：根据传入的正则表达式对数据进行提取，返回Unicode字符串list列表</p>
</li>
</ol>
</li>
<li><p>注意</p>
<ol>
<li><p>xpath 返回的是一个列表</p>
</li>
<li><p>xpath.extract():将xpath对象转换成Unicode字符串</p>
</li>
<li><p>settings设置</p>
<ul>
<li><p>HTTPERROR_ALLOWED_CODES = [403, 500, 404]</p>
</li>
<li><p>ROBOTSTXT_OBEY = False</p>
</li>
<li><p>下载中间件：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DOWNLOAD_DELAY = <span class="number">2</span></span><br><span class="line">        RANDOMIZE_DOWNLOAD_DELAY = <span class="literal">True</span></span><br><span class="line">        COOKIES_ENABLED = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">        DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">        <span class="string">'bank_info.middlewares.MyUserAgentMiddleware'</span>: <span class="number">300</span>,</span><br><span class="line">        <span class="string">'bank_info.middlewares.BankInfoDownloaderMiddleware'</span>: <span class="number">543</span>, <span class="comment"># 值越小优先级越高</span></span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        ITEM_PIPELINES = &#123;</span><br><span class="line">        <span class="string">'bank_info.pipelines.BankInfoPipeline'</span>: <span class="number">300</span>, <span class="comment"># 值越小优先级越高</span></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="8-scrapy高级"><a href="#8-scrapy高级" class="headerlink" title="8. scrapy高级"></a>8. scrapy高级</h3><ol>
<li><p>翻页功能：scrapy.follow(next_page,callback=self.parse)   会自动拼接url和next_page</p>
</li>
<li><p>抽取response中满足xpath规则的链接：LinkExtractor(restrict_xpath=’xxxx’), links = link.extract_links(response)</p>
</li>
<li><p>要防止scrapy被ban，主要有以下几个策略：</p>
<ul>
<li><p>动态设置user agent（ 在middleware.py中随机选取user-agent,并把它赋值给request）</p>
<ol>
<li><p>在settings开启UAMiddleware这个中间件：DOWNLOADER_MIDDLEWARES</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UAMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 定义一个User-Agent的List</span></span><br><span class="line">    ua_list = [</span><br><span class="line">    <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 '</span>,</span><br><span class="line">    <span class="string">'(KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)'</span>,</span><br><span class="line">    <span class="string">'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)'</span>,</span><br><span class="line">    <span class="string">'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'</span>,</span><br><span class="line">    <span class="string">'Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)'</span>,</span><br><span class="line">    ]</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span> <span class="comment"># 对request进行拦截</span></span><br><span class="line">        ua = random.choices(self.ua_list) <span class="comment"># 使用random模块，随机在ua_list中选取User-Agent</span></span><br><span class="line">        request.headers[<span class="string">'User-Agent'</span>] = ua <span class="comment"># 把选取出来的User-Agent赋给request</span></span><br><span class="line">        print(request.url) <span class="comment"># 打印出request的url</span></span><br><span class="line">        print(request.headers[<span class="string">'User-Agent'</span>]) <span class="comment"># 打印出request的headers</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span><span class="params">(self, request, response, spider)</span>:</span> <span class="comment"># 对response进行拦截</span></span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_exception</span><span class="params">(self, request, exception, spider)</span>:</span> <span class="comment"># 对process_request方法传出来的异常进行处理</span></span><br><span class="line">       <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>禁用cookies ： COOKIES_ENABLED=False</p>
</li>
<li><p>设置延迟下载 ： DOWNLOAD_DELAY=2</p>
</li>
<li><p>使用Google cache</p>
</li>
<li><p>使用IP地址池（Tor project、VPN和代理IP）</p>
</li>
<li><p>使用Crawlera</p>
</li>
</ul>
</li>
<li><p>scrapy中间件的分类：</p>
<ul>
<li>scrapy的中间件理论上有三种(Schduler Middleware,Spider Middleware,Downloader Middleware),在应用上一般有以下两种：<ol>
<li>爬虫中间件Spider Middleware：可以添加代码来处理发送给 Spiders 的response及spider产生的item和request.<ul>
<li>当蜘蛛传递请求和items给引擎的过程中，蜘蛛中间件可以对其进行处理（过滤出 URL 长度比 URLLENGTH_LIMIT 的 request。）</li>
<li>当引擎传递响应给蜘蛛的过程中，蜘蛛中间件可以对响应进行过滤（例如过滤出所有失败(错误)的 HTTP response）</li>
</ul>
</li>
<li>下载器中间件Downloader Middleware：主要功能在请求到网页后,页面被下载时进行一些处理.（反爬策略都是部署在下载中间件的）<ul>
<li>当引擎传递请求给下载器的过程中，下载中间件可以对请求进行处理 （例如增加http header信息，增加proxy信息等）</li>
<li>在下载器完成http请求，传递响应给引擎的过程中， 下载中间件可以对响应进行处理（例如进行gzip的解压等）</li>
<li>下载中间件三大函数：<ol>
<li>process_request(request, spider)——主要函数<ul>
<li>process_request() 必须返回其中之一: 返回 None 、返回一个 Response 对象、返回一个 Request 对象或raise IgnoreRequest</li>
<li>如果其返回 None： Scrapy将继续处理该request，执行其他的中间件的相应方法，直到合适的下载器处理函数(download handler)被调用， 该request被执行(其response被下载)</li>
<li>如果其返回Response 对象： Scrapy将不会调用任何其他的process_request()或 process_exception()方法，或相应的下载函数。其将返回该response，已安装的中间件的 process_response() 方法则会在每个response返回时被调用</li>
<li>如果其返回 Request对象 ： Scrapy则会停止调用 process_request方法并重新调度返回的request，也就是把request重新返回，进入调度器重新入队列</li>
<li>如果其返回raise IgnoreRequest异常 ： 则安装的下载中间件的 process_exception()方法 会被调用。如果没有任何一个方法处理该异常， 则request的errback(Request.errback)方法会被调用。如果没有代码处理抛出的异常， 则该异常被忽略且不记录(不同于其他异常那样)</li>
</ul>
</li>
<li>process_response(request, response, spider)——主要函数<ul>
<li>process_response() 必须返回以下之一：返回一个Response 对象、 返回一个Request 对象或raise IgnoreRequest 异常</li>
<li>如果其返回一个 Response对象： (可以与传入的response相同，也可以是全新的对象)， 该response会被在链中的其他中间件的 process_response() 方法处理</li>
<li>如果其返回一个 Request对象： 则中间件链停止， 返回的request会被重新调度下载。处理类似于 process_request() 返回request所做的那样</li>
<li>如果其抛出一个IgnoreRequest异常 ：则调用request的errback(Request.errback)。</li>
<li>如果没有代码处理抛出的异常，则该异常被忽略且不记录(不同于其他异常那样)</li>
</ul>
</li>
<li>process_exception(request, exception, spider)<ul>
<li>如果其返回 None ： Scrapy将会继续处理该异常，接着调用已安装的其他中间件的 process_exception()方法，直到所有中间件都被调用完毕，则调用默认的异常处理</li>
<li>如果其返回一个 Response 对象： 相当于异常被纠正了，则已安装的中间件链的 process_response()方法被调用。Scrapy将不会调用任何其他中间件的 process_exception()方法</li>
<li>如果其返回一个 Request 对象： 则返回的request将会被重新调用下载。这将停止中间件的 process_exception() 方法执行，就如返回一个response的那样</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>其他内置downloader middleware</p>
<p>| item | value |<br>| —- | —- |<br>| DefaultHeadersMiddleware | 将所有request的头设置为默认模式 |<br>| DownloadTimeoutMiddleware | 设置request的timeout |<br>| HttpAuthMiddleware | 对来自特定spider的request授权 |<br>| HttpCacheMiddleware | 给request&amp;response设置缓存策略 |<br>| HttpProxyMiddleware | 给所有request设置http代理 |<br>| RedirectMiddleware | 处理request的重定向 |<br>| MetaRefreshMiddleware | 根据meta-refresh html tag处理重定向 |<br>| RetryMiddleware | 失败重试策略 |<br>| RobotsTxtMiddleware | robots封禁处理 |<br>| UserAgentMiddleware | 支持user agent重写 |</p>
</li>
<li><p>把数据保存到json文件</p>
<p>下面这个例子将会把所有爬虫所爬取到的数据保存到 items.jl 文件中，.jl既是表示 JSON Lines 格式，既是每一行存储一个 item；</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWriterPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file = open(<span class="string">'items.jl'</span>, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        line = json.dumps(dict(item)) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(line)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
</li>
<li><p>把数据写到MongoDB</p>
<p>MongoDB address 以及 database name 是通过 Scrapy settings 配置的；下面这个用例主要用来展示如何使用 from_crawler 的用法以及如何正确的清理掉这些相关的 resources</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    collection_name = <span class="string">'scrapy_items'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DATABASE'</span>, <span class="string">'items'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.db[self.collection_name].insert_one(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="9-爬取动态页面"><a href="#9-爬取动态页面" class="headerlink" title="9. 爬取动态页面"></a>9. 爬取动态页面</h3><p>在scrapy中使用splash</p>
<ol>
<li><p>安装docker ：</p>
</li>
<li><p>安装splash：docker pull scrapinghub/splash</p>
</li>
<li><p>开启端口:docker run -p 5023:5023 -p 8050:8050 -p 8051:8051 scrapinghub/splash</p>
</li>
<li><p>安装 scrapy-splash: pip install scrapy-splash</p>
</li>
<li><p>修改setting.py文件对scrapy-splash进行配置</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Splash服务器地址</span></span><br><span class="line"><span class="string">SPLASH_URL</span> <span class="string">=</span> <span class="string">'http://localhost:8050'</span></span><br><span class="line"><span class="comment">#开启Splash的两个下载中间件并调整HttpCompressionMiddleware的次序</span></span><br><span class="line"> <span class="string">DOWNLOADER_MIDDLEWARES</span> <span class="string">=</span> <span class="string">&#123;</span></span><br><span class="line">    <span class="attr">'scrapy_splash.SplashCookiesMiddleware':</span> <span class="number">723</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy_splash.SplashMiddleware':</span> <span class="number">725</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware':</span> <span class="number">810</span><span class="string">,</span></span><br><span class="line"> <span class="string">&#125;</span></span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line"> <span class="comment">#设置去重过滤器</span></span><br><span class="line"> <span class="string">DUPEFILTER_CLASS</span> <span class="string">=</span> <span class="string">'scrapy_splash.SplashAwareDupeFilter'</span></span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line"> <span class="comment">#用来支持cache_args（可选）</span></span><br><span class="line"> <span class="string">SPIDER_MIDDLEWARES</span> <span class="string">=</span> <span class="string">&#123;</span></span><br><span class="line">    <span class="attr">'scrapy_splash.SplashDeduplicateArgsMiddleware':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>专业技能</tag>
      </tags>
  </entry>
  <entry>
    <title>selenium</title>
    <url>/2019/08/09/selenium/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><ol>
<li><p>安装：pip install selenium</p>
<p>因为selenium是配合浏览器一起使用，所以需要下载浏览器的驱动(webdriver)，以chrome为例：chrome的webdriver： <a href="http://chromedriver.storage.googleapis.com/index.html" target="_blank" rel="noopener">chromedriver</a> 不同的Chrome的版本对应的chromedriver.exe 版本也不一样。如果是最新的Chrome, 下载最新的chromedriver.exe 就可以。把chromedriver的路径也加到<strong>环境变量</strong>里</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver      <span class="comment"># 引入webdriver api</span></span><br><span class="line"></span><br><span class="line">driver = webdriver.Chrome()     <span class="comment"># 使用chrome浏览器声明一个webdriver对象</span></span><br><span class="line"><span class="comment"># driver = webdriver.Chrome('/your path /webdriver')</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  driver.get(<span class="string">'http://www.baidu.com/'</span>) <span class="comment"># 表示使用chrome以get的方式请求百度的url</span></span><br><span class="line">  driver.find_element_by_id(<span class="string">"kw"</span>).send_keys(<span class="string">"selenium"</span>)   <span class="comment"># 检索到百度的输入框，输入selenium</span></span><br><span class="line">  driver.find_element_by_id(<span class="string">"su"</span>).click() <span class="comment"># 检索到百度的搜索按钮并点击</span></span><br><span class="line">  wait = WebDriverWait(driever,<span class="number">10</span>) <span class="comment"># 等待加载</span></span><br><span class="line">  wait.until(EC.presence_of_element_located((By.ID,<span class="string">'content_left'</span>)))</span><br><span class="line">  print(driver.current_url)  <span class="comment"># 输出当前页面url</span></span><br><span class="line">  print(driver.get_cookies) <span class="comment"># 输出cookies</span></span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">  driver.close()</span><br></pre></td></tr></table></figure>
<a id="more"></a>
</li>
<li><p>元素选取</p>
<ol>
<li><p>单元素选取</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_element_by_id      <span class="comment"># 通过元素id</span></span><br><span class="line">find_element_by_name    <span class="comment"># 通过name属性</span></span><br><span class="line">find_element_by_xpath   <span class="comment"># 通过xpath</span></span><br><span class="line">find_element_by_link_text   <span class="comment"># 通过链接文本</span></span><br><span class="line">find_element_by_partial_link_text</span><br><span class="line">find_element_by_tag_name    <span class="comment"># 通过标签名</span></span><br><span class="line">find_element_by_class_name      <span class="comment"># 通过class名称定位</span></span><br><span class="line">find_element_by_css_selector    <span class="comment"># 通过css选择器定位</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>多元素选取</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_elements_by_name</span><br><span class="line">find_elements_by_xpath</span><br><span class="line">find_elements_by_link_text</span><br><span class="line">find_elements_by_partial_link_text</span><br><span class="line">find_elements_by_tag_name</span><br><span class="line">find_elements_by_class_name</span><br><span class="line">find_elements_by_css_selector</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>元素操作</p>
<ul>
<li>clear 清除元素的内容：clear(self)</li>
<li>send_keys 模拟按键输入：send_keys(self, *value)</li>
<li>click 点击元素：click(self)</li>
<li>submit 提交表单：submit(self)</li>
<li>获取元素属性：get_attribute(self, name)</li>
<li>获取元素文本：text</li>
</ul>
</li>
</ol>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver.find_element_by_id(<span class="string">"kw"</span>).clear()</span><br></pre></td></tr></table></figure>
<ol>
<li><p><strong>页面操作方法</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打开浏览器</span></span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line"><span class="comment"># 请求一个url</span></span><br><span class="line">driver.get(<span class="string">"www.baidu.com"</span>)</span><br><span class="line"><span class="comment"># 返回当前页面的title</span></span><br><span class="line">title = driver.title</span><br><span class="line"><span class="comment"># 返回当前页面的url</span></span><br><span class="line">url = driver.current_url</span><br><span class="line"><span class="comment"># 返回当前页面的源码</span></span><br><span class="line">source = driver.page_source</span><br><span class="line"><span class="comment"># 关闭当前页面</span></span><br><span class="line">driver.close()</span><br><span class="line"><span class="comment"># 注销并关闭浏览器</span></span><br><span class="line">driver.quit()</span><br><span class="line"><span class="comment"># 浏览器前进</span></span><br><span class="line">driver.forward()</span><br><span class="line"><span class="comment"># 浏览器后退</span></span><br><span class="line">driver.back()</span><br><span class="line"><span class="comment"># 刷新当前页面</span></span><br><span class="line">driver.refresh()</span><br><span class="line"><span class="comment"># 获取当前session中的全部cookie</span></span><br><span class="line">get_cookies(self)</span><br><span class="line"><span class="comment"># 获取当前会中中的指定cookie</span></span><br><span class="line">get_cookie(self, name)</span><br><span class="line"><span class="comment"># 在当前会话中添加cookie</span></span><br><span class="line">add_cookie(self, cookie_dict)</span><br><span class="line"><span class="comment"># 添加浏览器User-Agent：</span></span><br><span class="line">options.add_argument(<span class="string">'User-Agent=Mozilla/5.0 (Linux; U; Android 4.0.2; en-us; Galaxy Nexus Build/ICL53F) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30'</span>)</span><br><span class="line"><span class="comment"># 添加设置项Chrome Options：</span></span><br><span class="line">options = webdriver.ChromeOptions()</span><br><span class="line">options.add_argument(<span class="string">'xxxx'</span>)</span><br><span class="line">driver = webdriver.Chrome(chrome_options=options)</span><br></pre></td></tr></table></figure>
</li>
<li><p>页面等待</p>
<ol>
<li><p>隐式等待：简单的设置等待时间，单位为：秒</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">driver.implicitly_wait(<span class="number">10</span>) <span class="comment"># seconds</span></span><br><span class="line">driver.get(<span class="string">"http://somedomain/url_that_delays_loading"</span>)</span><br><span class="line">myDynamicElement = driver.find_element_by_id(<span class="string">"myDynamicElement"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>显式等待：指定某个条件，然后设置最长等待时间。如果在这个时间还没有找到元素，便会抛出异常。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">driver.get(<span class="string">"http://somedomain/url_that_delays_loading"</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line"> 	<span class="comment"># 这里需要特别注意的是until或until_not中的可执行方法method参数，很多人传入了WebElement对象</span></span><br><span class="line">  <span class="comment"># WebDriverWait(driver, 10).until(driver.find_element_by_id('kw'))  错误</span></span><br><span class="line">  element = WebDriverWait(driver, <span class="number">10</span>).until(</span><br><span class="line">            EC.presence_of_element_located((By.ID, <span class="string">"myDynamicElement"</span>))</span><br><span class="line">            )</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">driver.quit()</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>expected_conditions(17个条件)：expected_conditions是selenium的一个模块，其中包含一系列可用于判断的条件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">selenium.webdriver.support.expected_conditions</span><br><span class="line"></span><br><span class="line">这两个条件类验证title，验证传入的参数title是否等于或包含于driver.title</span><br><span class="line">title_is</span><br><span class="line">title_contains</span><br><span class="line"></span><br><span class="line">一个只要一个符合条件的元素加载出来就通过；另一个必须所有符合条件的元素都加载出来才行</span><br><span class="line">presence_of_element_located</span><br><span class="line">presence_of_all_elements_located</span><br><span class="line"></span><br><span class="line">这三个条件验证元素是否可见，前两个传入参数是元组类型的locator，第三个传入WebElement</span><br><span class="line">第一个和第三个其实质是一样的</span><br><span class="line">visibility_of_element_located</span><br><span class="line">invisibility_of_element_located</span><br><span class="line">visibility_of</span><br><span class="line"></span><br><span class="line">这两个人条件判断某段文本是否出现在某元素中，一个判断元素的text，一个判断元素的value</span><br><span class="line">text_to_be_present_in_element</span><br><span class="line">text_to_be_present_in_element_value</span><br><span class="line"></span><br><span class="line">这个条件判断frame是否可切入，可传入locator元组或者直接传入定位方式：id、name、index或WebElement</span><br><span class="line">frame_to_be_available_and_switch_to_it</span><br><span class="line"></span><br><span class="line">这个条件判断是否有alert出现</span><br><span class="line">alert_is_present</span><br><span class="line"></span><br><span class="line">这个条件判断元素是否可点击，传入locator</span><br><span class="line">element_to_be_clickable</span><br><span class="line"></span><br><span class="line">这四个条件判断元素是否被选中，第一个条件传入WebElement对象，第二个传入locator元组</span><br><span class="line">第三个传入WebElement对象以及状态，相等返回<span class="literal">True</span>，否则返回<span class="literal">False</span></span><br><span class="line">第四个传入locator以及状态，相等返回<span class="literal">True</span>，否则返回<span class="literal">False</span></span><br><span class="line">element_to_be_selected</span><br><span class="line">element_located_to_be_selected</span><br><span class="line">element_selection_state_to_be</span><br><span class="line">element_located_selection_state_to_be</span><br><span class="line"></span><br><span class="line">最后一个条件判断一个元素是否仍在DOM中，传入WebElement对象，可以判断页面是否刷新了</span><br><span class="line">staleness_of</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>鼠标操作</strong></p>
<ul>
<li>context_click(elem) 右击鼠标点击元素elem，另存为等行为</li>
<li>double_click(elem) 双击鼠标点击元素elem，地图web可实现放大功能</li>
<li>drag_and_drop(source,target) 拖动鼠标，源元素按下左键移动至目标元素释放</li>
<li>move_to_element(elem) 鼠标移动到一个元素上</li>
<li>click_and_hold(elem) 按下鼠标左键在一个元素上</li>
<li>perform() 在通过调用该函数执行ActionChains中存储行为</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取通过鼠标右键另存为百度图片logo的例子</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.action_chains <span class="keyword">import</span> ActionChains</span><br><span class="line"> </span><br><span class="line">driver = webdriver.Firefox()</span><br><span class="line">driver.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#鼠标移动至图片上 右键保存图片</span></span><br><span class="line">elem_pic = driver.find_element_by_xpath(<span class="string">"//div[@id='lg']/img"</span>)</span><br><span class="line"><span class="keyword">print</span> elem_pic.get_attribute(<span class="string">"src"</span>)</span><br><span class="line">action = ActionChains(driver).move_to_element(elem_pic)</span><br><span class="line">action.context_click(elem_pic)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#重点:当右键鼠标点击键盘光标向下则移动至右键菜单第一个选项</span></span><br><span class="line">action.send_keys(Keys.ARROW_DOWN)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line">action.send_keys(<span class="string">'v'</span>) <span class="comment">#另存为</span></span><br><span class="line">action.perform()</span><br><span class="line"> </span><br><span class="line"><span class="comment">#获取另存为对话框(失败)</span></span><br><span class="line">alert.switch_to_alert()</span><br><span class="line">alert.accept()</span><br><span class="line"><span class="comment"># driver.switch_to_alert().accept()  # 点击弹出里面的确定按钮</span></span><br><span class="line"><span class="comment"># driver.switch_to_alert().dismiss() # 点击弹出上面的X按钮</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>键盘操作</strong></p>
<ul>
<li>send_keys(Keys.ENTER) 按下回车键</li>
<li>send_keys(Keys.TAB) 按下Tab制表键</li>
<li>send_keys(Keys.SPACE) 按下空格键space</li>
<li>send_keys(Kyes.ESCAPE) 按下回退键Esc</li>
<li>send_keys(Keys.BACK_SPACE) 按下删除键BackSpace</li>
<li>send_keys(Keys.SHIFT) 按下shift键</li>
<li>send_keys(Keys.CONTROL) 按下Ctrl键</li>
<li>send_keys(Keys.ARROW_DOWN) 按下鼠标光标向下按键</li>
<li>send_keys(Keys.CONTROL,’a’) 组合键全选Ctrl+A</li>
<li>send_keys(Keys.CONTROL,’c’) 组合键复制Ctrl+C</li>
<li>send_keys(Keys.CONTROL,’x’) 组合键剪切Ctrl+X</li>
<li>send_keys(Keys.CONTROL,’v’) 组合键粘贴Ctrl+V</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"> </span><br><span class="line">driver = webdriver.Firefox()</span><br><span class="line">driver.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#输入框输入内容</span></span><br><span class="line">elem = driver.find_element_by_id(<span class="string">"kw"</span>)</span><br><span class="line">elem.send_keys(<span class="string">"Eastmount CSDN"</span>)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#删除一个字符CSDN 回退键</span></span><br><span class="line">elem.send_keys(Keys.BACK_SPACE)</span><br><span class="line">elem.send_keys(Keys.BACK_SPACE)</span><br><span class="line">elem.send_keys(Keys.BACK_SPACE)</span><br><span class="line">elem.send_keys(Keys.BACK_SPACE)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#输入空格+"博客"</span></span><br><span class="line">elem.send_keys(Keys.SPACE)</span><br><span class="line">elem.send_keys(<span class="string">u"博客"</span>)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#ctrl+a 全选输入框内容</span></span><br><span class="line">elem.send_keys(Keys.CONTROL,<span class="string">'a'</span>)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#ctrl+x 剪切输入框内容</span></span><br><span class="line">elem.send_keys(Keys.CONTROL,<span class="string">'x'</span>)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#输入框重新输入搜索</span></span><br><span class="line">elem.send_keys(Keys.CONTROL,<span class="string">'v'</span>)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#通过回车键替代点击操作</span></span><br><span class="line">driver.find_element_by_id(<span class="string">"su"</span>).send_keys(Keys.ENTER)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">driver.quit()</span><br></pre></td></tr></table></figure>
</li>
<li><p>截图保存：当爬虫出错时可以截图保存当时页面，以便复现bug，进行分析。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver.maximize_window()</span><br><span class="line">driver.get_screenshot_as_file(<span class="string">"screen shot saved path/pic_&#123;&#125;.png"</span>.format(xx))</span><br></pre></td></tr></table></figure>
</li>
<li><p>对selenium二次封装</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasePage</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    主要是把常用的几个Selenium方法封装到BasePage这个类，我们这里演示以下几个方法</span></span><br><span class="line"><span class="string">    back()</span></span><br><span class="line"><span class="string">    forward()</span></span><br><span class="line"><span class="string">    get()</span></span><br><span class="line"><span class="string">    quit()</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, driver)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        写一个构造函数，有一个参数driver</span></span><br><span class="line"><span class="string">        :param driver:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.driver = driver</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">back</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        浏览器后退按钮</span></span><br><span class="line"><span class="string">        :param none:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.driver.back()</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        浏览器前进按钮</span></span><br><span class="line"><span class="string">        :param none:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.driver.forward()</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_url</span><span class="params">(self, url)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        打开url站点</span></span><br><span class="line"><span class="string">        :param url:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.driver.get(url)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">quit_browser</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        关闭并停止浏览器服务</span></span><br><span class="line"><span class="string">        :param none:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.driver.quit()</span><br><span class="line">       </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BrowserEngine</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    定义一个浏览器引擎类，根据browser_type的值去，控制启动不同的浏览器，这里主要是IE，Firefox, Chrome</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, driver)</span>:</span></span><br><span class="line">        self.driver = driver</span><br><span class="line"> </span><br><span class="line">    browser_type = <span class="string">"IE"</span>   <span class="comment"># maybe Firefox, Chrome, IE</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_browser</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        通过if语句，来控制初始化不同浏览器的启动，默认是启动Chrome</span></span><br><span class="line"><span class="string">        :return: driver</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> self.browser_type == <span class="string">'Firefox'</span>:</span><br><span class="line">            driver = webdriver.Firefox()</span><br><span class="line">        <span class="keyword">elif</span> self.browser_type == <span class="string">'Chrome'</span>:</span><br><span class="line">            driver = webdriver.Chrome()</span><br><span class="line">        <span class="keyword">elif</span> self.browser_type == <span class="string">'IE'</span>:</span><br><span class="line">            driver = webdriver.Ie()</span><br><span class="line">        <span class="keyword">else</span>: driver = webdriver.Chrome()</span><br><span class="line"> </span><br><span class="line">        driver.maximize_window()</span><br><span class="line">        driver.implicitly_wait(<span class="number">10</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> driver</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>专业技能</tag>
      </tags>
  </entry>
  <entry>
    <title>优化算法</title>
    <url>/2018/07/11/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h2><p>梯度下降法(Gradient descent)或最速下降法(steepest descent)是求解无约束最优化问题的一种常用的、实现简单的方法。<br>假设$f(x)$是$R^n$上具有一阶连续偏导数的函数。求解</p>
<script type="math/tex; mode=display">
   min_{x \in R^n}f(x)</script><p>无约束最优化问题。 $f^\ast$表示目标函数$f(x)$的极小点。<br>梯度下降法是一种迭代算法，选取适当的初值$x^0$,不断迭代，更新x的值，进行目标函数的极小化，直到收敛。由于负梯度的方向是使函数下降最快的方向，在迭代的每一步，以负梯度方向更新x的值，从而达到减少函数值的目的。<br>第k+1次迭代值：</p>
<script type="math/tex; mode=display">
x^{(k+1)} \leftarrow x^{(k)}+\lambda_{k} p_{k}</script><p>其中，$p_k$是搜索方向，取负梯度方向$p_k = - \nabla f(x^k)$,$\lambda_k$使得</p>
<script type="math/tex; mode=display">
f\left(x^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geq 0} f\left(x^{(k)}+\lambda p_{k}\right)</script><a id="more"></a>
<h3 id="算法描述："><a href="#算法描述：" class="headerlink" title="算法描述："></a>算法描述：</h3><p>输入:目标函数$f(x)$，梯度函数$g(x^k)=\nabla f(x^k)$,计算$\epsilon$.<br>输出：$f(x)$的极小值点$x^\star$</p>
<ol>
<li>取初值$x^0 \in R^n$,置k=0</li>
<li>计算$f(x^k)$</li>
<li>计算梯度$g_k = g(x^k)$,当$\left|g_{k}\right|&lt;\varepsilon$时，停止迭代，令$p_k = -g(x^k)$求$\lambda_k$,使<script type="math/tex; mode=display">
f\left(x^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geq 0} f\left(x^{(k)}+\lambda p_{k}\right)</script></li>
<li>置$x^{k+1}=x^k+\lambda_k p_k$,计算$f(x^{k+1})$当$<br>\left|f\left(x^{(k+1)}\right)-f\left(x^{(k)}\right)\right|&lt;\varepsilon<br>$或$\left|x^{(k+1)}-x^{(k)}\right|&lt;\varepsilon$停止迭代。令$x^\star = x^{k+1}$</li>
<li>否则，置$k=k+1$,转3.</li>
</ol>
<h3 id="梯度下降调优"><a href="#梯度下降调优" class="headerlink" title="梯度下降调优"></a>梯度下降调优</h3><ol>
<li><strong>算法的步长选择</strong>。步长取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。</li>
<li><strong>算法参数的初始值选择</strong>。初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</li>
<li><strong>归一化</strong>。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化。</li>
</ol>
<h3 id="梯度下降法大家族（BGD，SGD，MBGD）"><a href="#梯度下降法大家族（BGD，SGD，MBGD）" class="headerlink" title="梯度下降法大家族（BGD，SGD，MBGD）"></a>梯度下降法大家族（BGD，SGD，MBGD）</h3><ol>
<li><strong>批量梯度下降法（Batch Gradient Descent）</strong> 批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新。</li>
<li><strong>随机梯度下降法（Stochastic Gradient Descent）</strong> 其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的样本的数据，而是仅仅选取一个样本来求梯度。随机梯度下降法，和批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。</li>
<li><strong>小批量梯度下降法（Mini-batch Gradient Descent）</strong> 小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于$m$个样本，我们采用$x$个样本来迭代，$1&lt;x&lt;m$。一般可以取$x=16,32,64…$，当然根据样本的数据，可以调整这个$x$ 的值。</li>
</ol>
<p>上述三种方法得到局部最优解的过程：<br><img src="/2018/07/11/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/C80A2811-1678-4371-BCCD-8AA51A777820.jpg" alt="avatar"></p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(X, y, W, B, alpha, max_iters)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    使用了所有的样本进行梯度下降</span></span><br><span class="line"><span class="string">    X: 训练集,</span></span><br><span class="line"><span class="string">    y: 标签,</span></span><br><span class="line"><span class="string">    W: 权重向量,</span></span><br><span class="line"><span class="string">    B: bias,</span></span><br><span class="line"><span class="string">    alpha: 学习率,</span></span><br><span class="line"><span class="string">    max_iters: 最大迭代次数.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    dW = <span class="number">0</span> <span class="comment"># 权重梯度收集器</span></span><br><span class="line">    dB = <span class="number">0</span> <span class="comment"># Bias梯度的收集器</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>] <span class="comment"># 样本数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_iters):</span><br><span class="line">        dW = <span class="number">0</span> <span class="comment"># 每次迭代重置</span></span><br><span class="line">        dB = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment"># 1. 迭代所有的样本</span></span><br><span class="line">            <span class="comment"># 2. 计算权重和bias的梯度保存在w_grad和b_grad,</span></span><br><span class="line">            <span class="comment"># 3. 通过增加w_grad和b_grad来更新dW和dB</span></span><br><span class="line">            W = W - alpha * (dW / m) <span class="comment"># 更新权重</span></span><br><span class="line">            B = B - alpha * (dB / m) <span class="comment"># 更新bias</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> W, B</span><br></pre></td></tr></table></figure>
<h3 id="梯度下降和最小二乘法"><a href="#梯度下降和最小二乘法" class="headerlink" title="梯度下降和最小二乘法"></a>梯度下降和最小二乘法</h3><p>梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。</p>
<h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><p>一般来说，牛顿法主要应用在两个方面，1：求方程的根；2：最优化。</p>
<h3 id="求解过程"><a href="#求解过程" class="headerlink" title="求解过程"></a>求解过程</h3><p>并不是所有的方程都有求根公式，或者求根公式很复杂，导致求解困难。利用牛顿法，可以迭代求解。</p>
<p>原理是利用泰勒公式，在 $x_0$处展开，且展开到一阶, 即 $f(x)=f(x_0)+f^\prime{x_0}(x-x_0)$ 。求解方程$f(x)=0$，等价于$f(x_0)+f^\prime(x_0)(x-x_0)=0$，求解 $x=x_1=x_0-\frac{f(x_0)}{f^\prime(x_0)}$。因为这是利用泰勒公式的一阶展开,$f(x)=f(x_0)+f^\prime(x_0)(x-x_0)$ 处并不是完全相等，而是近似相等，这里求得的的$x_1$并不能让 $f(x)=0$，只能说 $f(x_1)$ 的值比$f(x_9)$的值更接近于0，于是迭代的想法就很自然了，可以进而推出 $x_{n+1}=x_n-\frac{f(x_n)}{f\prime(x_n)}$，通过迭代，这个式子必然在 $f(x^\star)=0$ 的时候收敛。整个过程如下：<br><img src="/2018/07/11/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/B03C2A8B-AF9F-4871-94F6-0E7F6D6EC890.png" alt="avatar"></p>
<h3 id="最优化"><a href="#最优化" class="headerlink" title="最优化"></a>最优化</h3><p>无约束最优化问题</p>
<script type="math/tex; mode=display">
min_{x \in R^N}f(x)</script><p>其中$s^\star$为目标函数的极小点</p>
<p>设$f(x)$具有二阶连续偏导，若第$k$次迭代值为$x^k$,则可将$f(x)$在$x^k$附近进行二阶泰勒展开：</p>
<script type="math/tex; mode=display">
f(x)=f\left(x^{(k)}\right)+g_{k}^{T}\left(x-x^{(k)}\right)+\frac{1}{2}\left(x-x^{(k)}\right)^{T} H\left(x^{(k)}\right)\left(x-x^{(x)}\right)</script><p>其中，$g_k=g(x^k)=\nabla f(x^k)$是$f(x)$的梯度向量在点$x^k$的值，$H(x^k)$是$f(x)$的海塞矩阵</p>
<script type="math/tex; mode=display">
H(x)=\left[\frac{\partial^{2} f}{\partial x_{i} \partial x_{j}}\right]_{n \times n}</script><p>在点$x^k$的值。</p>
<p>函数$f(x)$有极值的必要条件是在极值点处一阶导数为0，即梯度向量为0。特别的当$H(x^k)$是正定矩阵时，函数$f(x)$的极值为最小值。</p>
<p>为了的得到一阶导数为0的点，可以用到<strong>求解方程</strong>部分的方法。根据二阶泰勒展开，对$\nabla f(x)$在$x^k$进行展开得（也可以对泰勒公式再进行求导）：</p>
<script type="math/tex; mode=display">
\nabla f(x)=g_{k}+H_{k}\left(x-x^{(k)}\right)</script><p>其中，$H_k=H(x^k)$则：</p>
<script type="math/tex; mode=display">
\begin{aligned}&g_{k}+H_{k}\left(x^{(k+1)}-x^{(k)}\right)=0\\&x^{(k+1)}=x^{(k)}-H_{k}^{-1} g_{k}\end{aligned}</script><p>令</p>
<script type="math/tex; mode=display">
H_kp_k = -g_k</script><p>得到迭代公式：</p>
<script type="math/tex; mode=display">
x^{k+1} = x^k+p_k</script><p>最终在$\nabla f(x^\star)=0$收敛</p>
<h3 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h3><p>输入：目标函数$f(x)$，梯度$g(x) = \nabla f(x)$,海塞矩阵$H(x)$,精度(阈值)$\epsilon$</p>
<p>输出：$f(x)$的极小值点$x^\star$</p>
<ol>
<li><p>取初始点$x^0$,置k=0</p>
</li>
<li><p>计算$g_k = g(x^k)$</p>
</li>
<li><p>若$\left|g_k \right| &lt; \epsilon$，则停止计算，求得近似解:$x^\star = x^k$</p>
</li>
<li><p>计算$H_k = H(x^k)$,并求$p_k$。</p>
<script type="math/tex; mode=display">
H_kp_k = -g_k</script></li>
<li><p>置 $x(k+1) = x^k + p_k$</p>
</li>
<li>置 k=k+1, 转2.</li>
</ol>
<h3 id="牛顿法和梯度下降"><a href="#牛顿法和梯度下降" class="headerlink" title="牛顿法和梯度下降"></a>牛顿法和梯度下降</h3><p>梯度下降法和牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法是用二阶的海森矩阵的逆矩阵求解。相对而言，使用牛顿法收敛更快（迭代更少次数）。但是每次迭代的时间比梯度下降法长。</p>
<p>梯度下降法： </p>
<script type="math/tex; mode=display">
x^{k+1} = x^k - \lambda \nabla f(x^k)</script><p>牛顿法：</p>
<script type="math/tex; mode=display">
x^{k+1} = x^k-\lambda(H^k)^{-1}\nabla f(x^k)</script><p>至于为什么牛顿法收敛更快，通俗来说梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。<br><img src="/2018/07/11/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/84835FEB-0EA4-41E5-B07D-41BBEB49AE9B.jpg" alt="9c6e73f0200cbe504e0370a18ca3a95a"><br>红色的牛顿法的迭代路径，绿色的是梯度下降法的迭代路径。<br>牛顿法是梯度下降法的进一步发展，梯度法利用了目标函数的一阶偏导信息、一负梯度方向作为搜索方向，只考虑目标函数在迭代点的局部性质；而牛顿法不仅使用目标函数的一阶偏导数，还进一步利用了目标函数的二阶偏导数，这样就考虑了梯度变化的趋势，因而能更全面地确定合适的搜索方向以加快收敛，它具有二阶收敛速度，但是牛顿法也存在两个<strong>缺点</strong> ：</p>
<ul>
<li>对目标函数有较严格的要求，函数必须具有连续的一、二阶偏导，海塞矩阵必须正定。</li>
<li>计算相当复杂，除了需要计算梯度外，还需计算二阶偏导矩阵和它的逆矩阵。计算量、存储量都很大。且均以维数N的平方比增加，当N很大时，这个问题就更加突出。</li>
</ul>
<h2 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h2><p>在牛顿法的迭代计算中，需要计算海塞矩阵的逆矩阵$H^-1$，这一计算比较复杂，考虑用一个n阶矩阵$G_k=G(x^k)$来近似替代$H^{-1}_{k}=H^{-1}(x^k)$。这就是拟牛顿的基本想法。</p>
<p>要找到近似的替代矩阵，必定要和$H_k$有类似的性质。先看下牛顿法迭代中海塞矩阵$H_k$满足的条件。首先，$H_k$满足以下关系：</p>
<p>取$x=x^{k-1}$，由</p>
<script type="math/tex; mode=display">
\nabla f(x)=g_k+H_k(x-x^k)</script><p>得：</p>
<script type="math/tex; mode=display">
g_{k-1}-g_k = H_k(x^{k-1}-x^k)</script><p>记$y_k=g_k-g_{k-1},\delta_k = x^k-x^{k-1}$,则：</p>
<script type="math/tex; mode=display">
y_{k-1}=H_k\delta_{k-1}H^{-1}_ky_{k-1}=\delta_{k-1}</script><p>称为拟牛顿条件。</p>
<p>其次，如果$H_k$是正定的（$H_k^{-1}$也是正定的），那么保证牛顿法的搜索方向$p_k$是下降方向。这是因为搜索方向是$p_k = -H_k^{-1}g_k$,</p>
<p>由</p>
<script type="math/tex; mode=display">
x^{k+1} = x^k -H^{-1}_kg_k</script><p>有</p>
<script type="math/tex; mode=display">
x = x^k - \lambda H_k^{-1}g_k = x^k+\lambda p_k</script><p>则$f(x)$在$x^k$的泰勒展开可近似为：</p>
<script type="math/tex; mode=display">
f(x) = f(x^k) - \lambda g^T_kH_k^{-1}g_k</script><p>由于$H_k^{-1}$正定，故$g_k^TH_k^{-1}g_k&gt;0$。当$\lambda$为一个充分小的正数时，有$f(x)&lt;f(x^k)$,即搜索方向$p_k$是下降方向。</p>
<p>因此拟牛顿法将$G_k$作为$H^{-1}_k$近似。要求$G_k$满足同样的条件。首先，每次迭代矩阵$G_k$是正定的。同时，$G_k满足下面的拟牛顿条件</p>
<script type="math/tex; mode=display">
G_{k+1}y_k = \delta_k</script><p>按照拟牛顿条件，在每次迭代中可以选择更新矩阵$G_{k+1}$:</p>
<script type="math/tex; mode=display">
G_{k+1} = G_k + \nabla G_k</script><h3 id="DFP"><a href="#DFP" class="headerlink" title="DFP"></a>DFP</h3><h3 id="BFGS"><a href="#BFGS" class="headerlink" title="BFGS"></a>BFGS</h3><h3 id="Broyden"><a href="#Broyden" class="headerlink" title="Broyden"></a>Broyden</h3>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title>动态规划</title>
    <url>/2018/03/09/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2018/03/09/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/dynamic_programming.png" alt="avatar"></p>
<a id="more"></a>
<h2 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h2><h3 id="思考状态"><a href="#思考状态" class="headerlink" title="思考状态"></a>思考状态</h3><p>状态先尝试“题目问什么，就把什么设置为状态”。然后考虑“状态如何转移”，如果“状态转移方程”不容易得到，尝试修改定义，目的仍然是为了方便得到“状态转移方程”。</p>
<h3 id="思考状态转移方程（核心、难点）"><a href="#思考状态转移方程（核心、难点）" class="headerlink" title="思考状态转移方程（核心、难点）"></a>思考状态转移方程（核心、难点）</h3><p>状态转移方程是非常重要的，是动态规划的核心，也是难点，起到承上启下的作用。</p>
<blockquote>
<p>技巧是分类讨论。对状态空间进行分类，思考最优子结构到底是什么。即大问题的最优解如何由小问题的最优解得到。</p>
</blockquote>
<p>归纳“状态转移方程”是一个很灵活的事情，得具体问题具体分析，除了掌握经典的动态规划问题以外，还需要多做题。如果是针对面试，请自行把握难度，我个人觉得掌握常见问题的动态规划解法，明白动态规划的本质就是打表格，从一个小规模问题出发，逐步得到大问题的解，并记录过程。动态规划依然是“空间换时间”思想的体现。</p>
<h3 id="思考初始化"><a href="#思考初始化" class="headerlink" title="思考初始化"></a>思考初始化</h3><p>初始化是非常重要的，一步错，步步错，初始化状态一定要设置对，才可能得到正确的结果。</p>
<p>角度 1：直接从状态的语义出发；</p>
<p>角度 2：如果状态的语义不好思考，就考虑“状态转移方程”的边界需要什么样初始化的条件；</p>
<p>角度 3：从“状态转移方程”方程的下标看是否需要多设置一行、一列表示“哨兵”，这样可以避免一些边界的讨论，使得代码变得比较短。</p>
<h3 id="思考输出"><a href="#思考输出" class="headerlink" title="思考输出"></a>思考输出</h3><p>有些时候是最后一个状态，有些时候可能会综合所有计算过的状态。</p>
<h3 id="思考状态压缩"><a href="#思考状态压缩" class="headerlink" title="思考状态压缩"></a>思考状态压缩</h3><p>  “状态压缩”会使得代码难于理解，初学的时候可以不一步到位。先把代码写正确，然后再思考状态压缩。</p>
<p>  状态压缩在有一种情况下是很有必要的，那就是状态空间非常庞大的时候（处理海量数据），此时空间不够用，就必须状态压缩。</p>
<hr>
<h2 id="题目（LeetCode）"><a href="#题目（LeetCode）" class="headerlink" title="题目（LeetCode）"></a>题目（LeetCode）</h2><h3 id="5-最长回文子串"><a href="#5-最长回文子串" class="headerlink" title="5. 最长回文子串"></a>5. <a href="https://leetcode-cn.com/problems/longest-palindromic-substring/" target="_blank" rel="noopener">最长回文子串</a></h3><p>描述：给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。</p>
<p>示例 1：</p>
<p>输入: “babad”</p>
<p>输出: “bab”</p>
<p>注意: “aba” 也是一个有效答案。</p>
<p>示例 2：</p>
<p>输入: “cbbd”</p>
<p>输出: “bb”</p>
<p>“动态规划”最关键的步骤是想清楚“状态如何转移”，事实上，“回文”是天然具有“状态转移”性质的：</p>
<blockquote>
<p>一个回文去掉两头以后，剩下的部分依然是回文（这里暂不讨论边界）。</p>
</blockquote>
<p>依然从回文串的定义展开讨论：</p>
<ol>
<li><p>如果一个字符串的头尾两个字符都不相等，那么这个字符串一定不是回文串；</p>
</li>
<li><p>如果一个字符串的头尾两个字符相等，才有必要继续判断下去。</p>
</li>
<li><p>如果里面的子串是回文，整体就是回文串；</p>
</li>
<li><p>如果里面的子串不是回文串，整体就不是回文串。</p>
</li>
</ol>
<p>即在头尾字符相等的情况下，里面子串的回文性质据定了整个子串的回文性质，这就是状态转移。因此可以把“状态”定义为原字符串的一个子串是否为回文子串。</p>
<h4 id="第-1-步：定义状态"><a href="#第-1-步：定义状态" class="headerlink" title="第 1 步：定义状态"></a>第 1 步：定义状态</h4><p><code>dp[i][j]</code> 表示子串 <code>s[i,j]</code> 是否为回文子串。</p>
<h4 id="第-2-步：思考状态转移方程"><a href="#第-2-步：思考状态转移方程" class="headerlink" title="第 2 步：思考状态转移方程"></a>第 2 步：思考状态转移方程</h4><p>这一步在做分类讨论（根据头尾字符是否相等），根据上面的分析得到：</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">dp[<span class="string">i</span>][<span class="symbol">j</span>] = (s[<span class="string">i</span>] == s[<span class="string">j</span>]) and dp[<span class="string">i + 1</span>][<span class="symbol">j - 1</span>]</span><br></pre></td></tr></table></figure>
<p>分析：</p>
<ol>
<li><p><code>i</code> 和 <code>j</code> 的关系是 <code>i &lt;= j</code>，因此，只需要填这张表的上半部分；</p>
</li>
<li><p>看到 <code>dp[i + 1][j - 1]</code> 就得考虑边界情况。边界条件是：表达式 <code>[i + 1, j - 1]</code>不构成区间，即长度严格小于 2，即<code>j - 1 - (i + 1) + 1 &lt; 2</code> ，整理得 <code>j - i &lt; 3</code>。</p>
</li>
</ol>
<p>这个结论很显然：当子串 <code>s[i, j]</code> 的长度等于 2 或者等于 3 的时候，我其实只需要判断一下头尾两个字符是否相等就可以直接下结论了。</p>
<ul>
<li><p>如果子串 <code>s[i + 1, j - 1]</code> 只有 1 个字符，即去掉两头，剩下中间部分只有 1 个字符，当然是回文；</p>
</li>
<li><p>如果子串 <code>s[i + 1, j - 1]</code> 为空串，那么子串 <code>s[i, j]</code> 一定是回文子串。</p>
</li>
</ul>
<p>因此，在 <code>s[i] == s[j]</code> 成立和 <code>j - i &lt; 3</code> 的前提下，直接可以下结论，<code>dp[i][j] = true</code>，否则才执行状态转移。</p>
<h4 id="第-3-步-考虑初始化"><a href="#第-3-步-考虑初始化" class="headerlink" title="第 3 步:考虑初始化"></a>第 3 步:考虑初始化</h4><p>初始化的时候，单个字符一定是回文串，因此把对角线先初始化为 1，即 <code>dp[i][i] = 1</code>。</p>
<h4 id="第-4-步-考虑输出"><a href="#第-4-步-考虑输出" class="headerlink" title="第 4 步:考虑输出"></a>第 4 步:考虑输出</h4><p>只要一得到 <code>dp[i][j] = true</code>，就记录子串的长度和起始位置，没有必要截取，因为截取字符串也要消耗性能，记录此时的回文子串的“起始位置”和“回文长度”即可。</p>
<h4 id="第-5-步-考虑状态是否可以压缩"><a href="#第-5-步-考虑状态是否可以压缩" class="headerlink" title="第 5 步:考虑状态是否可以压缩"></a>第 5 步:考虑状态是否可以压缩</h4><p>在填表的过程中，只参考了左下方的数值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">longestPalindrome</span><span class="params">(self,s)</span>:</span></span><br><span class="line"></span><br><span class="line">size = len(s)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> size &lt; <span class="number">2</span>:</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line"></span><br><span class="line">dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(size)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(size)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(size):</span><br><span class="line"></span><br><span class="line">dp[_][_] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">start_idx = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">max_len = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,size):</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(j):</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> s[i] == s[j]:</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> j - i &lt; <span class="number">3</span>: <span class="comment"># i,j 之间只有一个字符或者没有字符</span></span><br><span class="line"></span><br><span class="line">dp[i][j] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">dp[i][j] = dp[i+<span class="number">1</span>][j<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> dp[i][j] == <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">cur_len = j - i + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> cur_len &gt; max_len:</span><br><span class="line"></span><br><span class="line">max_len = cur_len</span><br><span class="line"></span><br><span class="line">start_idx = i</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> s[start_idx:start_idx+max_len]</span><br></pre></td></tr></table></figure>
<h3 id="1143-最长公共子序列"><a href="#1143-最长公共子序列" class="headerlink" title="1143. 最长公共子序列"></a>1143. <a href="https://leetcode-cn.com/problems/longest-common-subsequence/" target="_blank" rel="noopener">最长公共子序列</a></h3><p>描述：给定两个字符串 text1 和 text2，返回这两个字符串的最长公共子序列。一个字符串的 子序列是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。</p>
<p>例如，”ace” 是 “abcde” 的子序列，但 “aec” 不是 “abcde” 的子序列。两个字符串的「公共子序列」是这两个字符串所共同拥有的子序列。若这两个字符串没有公共子序列，则返回 0。</p>
<p>示例 1:</p>
<p>输入：text1 = “abcde”, text2 = “ace”</p>
<p>输出：3</p>
<p>解释：最长公共子序列是 “ace”，它的长度为 3。</p>
<p>示例 2:</p>
<p>输入：text1 = “abc”, text2 = “abc”</p>
<p>输出：3</p>
<p>解释：最长公共子序列是 “abc”，它的长度为 3。</p>
<p>示例 3:</p>
<p>输入：text1 = “abc”, text2 = “def”</p>
<p>输出：0</p>
<p>解释：两个字符串没有公共子序列，返回 0。</p>
<p>提示:</p>
<p>1 &lt;= text1.length &lt;= 1000</p>
<p>1 &lt;= text2.length &lt;= 1000</p>
<p>输入的字符串只含有小写英文字符。</p>
<h4 id="思路："><a href="#思路：" class="headerlink" title="思路："></a>思路：</h4><ol>
<li><p>状态：问题要求最长公共子序列，那就让dp表存储长度。</p>
</li>
<li><p>状态转移：</p>
</li>
<li><p>当 <code>S1i==S2j</code> 时，那么就能在 <code>S1</code> 的前 <code>i-1</code> 个字符与 <code>S2</code> 的前 <code>j-1</code> 个字符最长公共子序列的基础上再加上 <code>S1i</code> 这个值，最长公共子序列长度加 <code>1</code>，即 <code>dp[i][j] = dp[i-1][j-1] + 1</code>。</p>
</li>
<li><p>当 <code>S1i != S2j</code> 时，此时最长公共子序列为 <code>S1</code> 的前 <code>i-1</code> 个字符和 <code>S2</code> 的前 <code>j</code> 个字符最长公共子序列，或者 <code>S1</code> 的前 i 个字符和 <code>S2</code> 的前 <code>j-1</code> 个字符最长公共子序列，取它们的最大者，即 <code>dp[i][j] = max{ dp[i-1][j], dp[i][j-1] }</code>。</p>
</li>
</ol>
<p>综上，状态转移方程：</p>
<p><img src="/2018/03/09/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/dp_1.png" alt="avatar"></p>
<ol>
<li>解的形式：解在dp表的右下角</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">longestCommonSubsequence</span><span class="params">(self, text1, text2)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:type text1: str</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:type text2: str</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:rtype: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> text1 <span class="keyword">or</span> <span class="keyword">not</span> text2:</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">size_1 = len(text1)</span><br><span class="line"></span><br><span class="line">size_2 = len(text2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化表格</span></span><br><span class="line"></span><br><span class="line">dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(size_2+<span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(size_1+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(size_1):</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(size_2):</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> text1[i] == text2[j]:</span><br><span class="line"></span><br><span class="line">dp[i+<span class="number">1</span>][j+<span class="number">1</span>] = dp[i][j] + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">dp[i+<span class="number">1</span>][j+<span class="number">1</span>] = max(dp[i][j+<span class="number">1</span>],dp[i+<span class="number">1</span>][j])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> dp[size_1][size_2]</span><br></pre></td></tr></table></figure>
<h3 id="53-最大序列和"><a href="#53-最大序列和" class="headerlink" title="53. 最大序列和"></a>53. <a href="https://leetcode-cn.com/problems/maximum-subarray/" target="_blank" rel="noopener">最大序列和</a></h3><p>给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。</p>
<p>示例:</p>
<p>输入: [-2,1,-3,4,-1,2,1,-5,4],</p>
<p>输出: 6</p>
<p>解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。</p>
<p>进阶:</p>
<p>如果你已经实现复杂度为 O(n) 的解法，尝试使用更为精妙的分治法求解。</p>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>法一：暴力动态规划：超时</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxSubArray</span><span class="params">(self, nums)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:type nums: List[int]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:rtype: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">size = len(nums)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> size == <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> nums[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(size)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(size)]</span><br><span class="line"></span><br><span class="line">start = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">end = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">max_val = float(<span class="string">'-inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(size):</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(j):</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> j - i == <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">dp[i][j] = nums[i] + nums[j]</span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">dp[i][j] = dp[i][j<span class="number">-1</span>] + nums[j]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> max_val &lt; dp[i][j]:</span><br><span class="line"></span><br><span class="line">start = i</span><br><span class="line"></span><br><span class="line">end = j</span><br><span class="line"></span><br><span class="line">max_val = dp[i][j]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> max(max_val, max(nums))</span><br></pre></td></tr></table></figure>
<p>法二：DP法，用了些灵巧的思路</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxSubArray</span><span class="params">(self, nums)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:type nums: List[int]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:rtype: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">max_, sum_ = nums[<span class="number">0</span>],nums[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> nums[<span class="number">1</span>:]:</span><br><span class="line"></span><br><span class="line">sum_ = sum_ + num <span class="keyword">if</span> sum_ &gt; <span class="number">0</span> <span class="keyword">else</span> num</span><br><span class="line"></span><br><span class="line">max_ = max_ <span class="keyword">if</span> max_ &gt; sum_ <span class="keyword">else</span> sum_</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> max_</span><br></pre></td></tr></table></figure>
<h3 id="300-最长上升子序列"><a href="#300-最长上升子序列" class="headerlink" title="300. 最长上升子序列"></a>300. <a href="https://leetcode-cn.com/problems/longest-increasing-subsequence/" target="_blank" rel="noopener">最长上升子序列</a></h3><p>给定一个无序的整数数组，找到其中最长上升子序列的长度。</p>
<p>示例:</p>
<p>输入: [10,9,2,5,3,7,101,18]</p>
<p>输出: 4</p>
<p>解释: 最长的上升子序列是 [2,3,7,101]，它的长度是 4。</p>
<p>说明:可能会有多种最长上升子序列的组合，你只需要输出对应的长度即可。你算法的时间复杂度应该为 $O(n^2)$ 。</p>
<p>进阶: 你能将算法的时间复杂度降低到 <code>O(n log n)</code> 吗?</p>
<h4 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h4><p>状态：定义 dp[i]为考虑前 i个元素，以第 i 个数字结尾的最长上升子序列的长度，注意 nums[i]必须被选取。</p>
<p>状态转移：从小到大计算 dp[] 数组的值，在计算 dp[i] 之前，我们已经计算出 dp[0…i−1] 的值，则状态转移方程为：</p>
<p><img src="/2018/03/09/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/dp_2.png" alt="avatar"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lengthOfLIS</span><span class="params">(self, nums)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:type nums: List[int]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:rtype: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">dp = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line"></span><br><span class="line">dp.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> nums[i] &gt; nums[j]:</span><br><span class="line"></span><br><span class="line">dp[i] = max(dp[i], dp[j] + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> max(dp)</span><br></pre></td></tr></table></figure>
<h3 id="10-正则表达式匹配"><a href="#10-正则表达式匹配" class="headerlink" title="10. 正则表达式匹配"></a><a href="https://leetcode-cn.com/problems/regular-expression-matching/" target="_blank" rel="noopener">10. 正则表达式匹配</a></h3><p><strong>状态</strong>：问题要求解的是正则串是否可以匹配，则把状态设为是否可匹配。<code>dp[i][j]</code> 表示 <code>s</code> 的前 <code>i</code>个是否能被 <code>p</code> 的前 <code>j</code> 个匹配</p>
<p><strong>转移方程</strong> ： </p>
<ol>
<li><p><code>p[j] == s[i] or p[j] == &quot;.&quot; : dp[i][j] = dp[i-1][j-1]</code> </p>
</li>
<li><p><code>p[j] ==&quot; * &quot;</code>:     # 比较难想</p>
<ol>
<li><code>p[j-1] != s[i]: dp[i][j] = dp[i][j-2]</code> # <code>*</code> 前的字符和原字符串不匹配,相当于该字符匹配0次。</li>
<li><code>p[j-1] == s[i] or p[j-1] == &quot;.&quot;：</code> # 匹配<ol>
<li><code>dp[i][j] = dp[i-1][j]</code> # * 匹配多次</li>
<li><code>dp[i][j] = dp[i][j-1]</code> # * 匹配一次</li>
<li><code>dp[i][j] = dp[i][j-2]</code> # * 匹配0次</li>
</ol>
</li>
</ol>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span><span class="params">(self, s, p)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> p:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">not</span> s</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> s <span class="keyword">and</span> len(p) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        nrow = len(s) + <span class="number">1</span></span><br><span class="line">        ncol = len(p) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        dp = [[<span class="literal">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(ncol)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(nrow)]</span><br><span class="line"></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">1</span>] = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">2</span>, ncol):</span><br><span class="line">            j = c - <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> p[j] == <span class="string">'*'</span>:</span><br><span class="line">                dp[<span class="number">0</span>][c] = dp[<span class="number">0</span>][c - <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">1</span>, nrow): <span class="comment"># r 原始串</span></span><br><span class="line">            i = r - <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">1</span>, ncol): <span class="comment"># c 正则串</span></span><br><span class="line">                j = c - <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> s[i] == p[j] <span class="keyword">or</span> p[j] == <span class="string">'.'</span>:</span><br><span class="line">                    dp[r][c] = dp[r - <span class="number">1</span>][c - <span class="number">1</span>]</span><br><span class="line">                <span class="keyword">elif</span> p[j] == <span class="string">'*'</span>:</span><br><span class="line">                    <span class="keyword">if</span> p[j - <span class="number">1</span>] == s[i] <span class="keyword">or</span> p[j - <span class="number">1</span>] == <span class="string">'.'</span>:</span><br><span class="line">                        dp[r][c] = dp[r - <span class="number">1</span>][c] <span class="keyword">or</span> dp[r][c - <span class="number">2</span>]</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        dp[r][c] = dp[r][c - <span class="number">2</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[r][c] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[nrow - <span class="number">1</span>][ncol - <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>专业技能</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>关键词抽取</title>
    <url>/2020/04/21/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="TF-IDF-提取关键词"><a href="#TF-IDF-提取关键词" class="headerlink" title="TF-IDF 提取关键词"></a>TF-IDF 提取关键词</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率).TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一份文件在一个语料库中的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。换句话说就是：<strong>一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章。</strong></p>
<blockquote>
<p>TF-IDF 算法主要适用于英文，中文首先要分词，分词后要解决多词一义，以及一词多义问题，这两个问题通过简单的tf-idf方法不能很好的解决。于是就有了后来的词嵌入方法，用向量来表征一个词。</p>
</blockquote>
<a id="more"></a>
<h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><h4 id="TF"><a href="#TF" class="headerlink" title="TF"></a>TF</h4><p>表示词条（关键字）在文本中出现的次数（frequency）(一般不用)。TF 背后的隐含的假设是，查询关键字中的单词应该相对于其他单词更加重要，而文档的重要程度，也就是相关度，与单词在文档中出现的次数成正比。比如，“Car” 这个单词在文档 A 里出现了 5 次，而在文档 B 里出现了 20 次，那么 TF 计算就认为文档 B 可能更相关。</p>
<h5 id="变种一-通过对数函数避免-TF-线性增长"><a href="#变种一-通过对数函数避免-TF-线性增长" class="headerlink" title="变种一:通过对数函数避免 TF 线性增长"></a>变种一:通过对数函数避免 TF 线性增长</h5><p>理由：虽然我们一般认为一个文档包含查询关键词多次相对来说表达了某种相关度，但这样的关系很难说是线性的。以 “Car Insurance” 为例，文档 A 可能包含 “Car” 这个词 100 次，而文档 B 可能包含 200 次，是不是说文档 B 的相关度就是文档 A 的 2 倍呢？其实，当这种频次超过了某个阈值之后，这个 TF 也就没那么有区分度了。</p>
<p><strong>用 Log，也就是对数函数，对 TF 进行变换，就是一个不让 TF 线性增长的技巧</strong>。具体来说，人们常常用 1+Log(TF) 这个值来代替原来的 TF 取值。在这样新的计算下，假设 “Car” 出现一次，新的值是 1，出现 100 次，新的值是 $log 100=5.6$，而出现 200 次，新的值是$log 200 = 6.3$。很明显，这样的计算保持了一个平衡，既有区分度，但也不至于完全线性增长。</p>
<h5 id="变种二：标准化解决长文档、短文档问题"><a href="#变种二：标准化解决长文档、短文档问题" class="headerlink" title="变种二：标准化解决长文档、短文档问题"></a>变种二：标准化解决长文档、短文档问题</h5><p>经典的计算并没有考虑 “长文档” 和“短文档”的区别。一个文档 A 有 3,000 个单词，一个文档 B 有 250 个单词，很明显，即便 “Car” 在这两个文档中都同样出现过 20 次，也不能说这两个文档都同等相关。<strong>对 TF 进行 “标准化”（Normalization），特别是根据文档的最大 TF 值进行的标准化，成了另外一个比较常用的技巧</strong>。</p>
<script type="math/tex; mode=display">
tf_{ij} = \frac{n_{i,j}}{\sum_k n_{k,j}}</script><p>其中$n_{i,j}$是词$w_i$在文档$d_j$中出现的次数，分母是文档$d_j$中所有词汇出现的次数总和。</p>
<h4 id="IDF"><a href="#IDF" class="headerlink" title="IDF"></a>IDF</h4><p>仅有 TF 不能比较完整地描述文档的相关度。因为语言的因素，有一些单词可能会比较自然地在很多文档中反复出现，比如英语中的 “The”、“An”、“But” 等等。这些词大多起到了链接语句的作用，是保持语言连贯不可或缺的部分。然而，如果我们要搜索 “How to Build A Car” 这个关键词，其中的 “How”、“To” 以及 “A” 都极可能在绝大多数的文档中出现，这个时候 TF 就无法帮助我们区分文档的相关度了。</p>
<p>逆文档频率的思路很简单，就是我们需要去 “惩罚”（Penalize）那些出现在太多文档中的单词。</p>
<h5 id="变种三：对数函数处理-IDF"><a href="#变种三：对数函数处理-IDF" class="headerlink" title="变种三：对数函数处理 IDF"></a>变种三：对数函数处理 IDF</h5><p>某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。</p>
<script type="math/tex; mode=display">
idf_w = log{\frac{|D|}{|j:t_i \in d_j|}}</script><p>其中，$|D|$表示语料库中文档总数，分母为包含词w的文档数+1，分母加1是为了避免出现分母为零的情况。</p>
<p>样做的好处就是，第一，使用了文档总数来做标准化，很类似上面提到的标准化的思路；第二，利用对数来达到非线性增长的目的。</p>
<h5 id="变种四：查询词及文档向量标准化"><a href="#变种四：查询词及文档向量标准化" class="headerlink" title="变种四：查询词及文档向量标准化"></a>变种四：查询词及文档向量标准化</h5><p>对查询关键字向量，以及文档向量进行标准化，使得这些向量能够不受向量里有效元素多少的影响，也就是不同的文档可能有不同的长度。在线性代数里，可以把向量都标准化为一个单位向量的长度。这个时候再进行点积运算，就相当于在原来的向量上进行余弦相似度的运算。所以，另外一个角度利用这个规则就是直接在多数时候进行余弦相似度运算，以代替点积运算。</p>
<h4 id="TF-IDF-1"><a href="#TF-IDF-1" class="headerlink" title="TF-IDF"></a>TF-IDF</h4><p>某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语(关键词)。</p>
<script type="math/tex; mode=display">
TF-IDF = TF * IDF</script><h3 id="TF-IDF-的应用"><a href="#TF-IDF-的应用" class="headerlink" title="TF-IDF 的应用"></a>TF-IDF 的应用</h3><ol>
<li>搜索引擎</li>
<li>关键词提取</li>
<li>文本相似性</li>
<li>文本摘要</li>
</ol>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>具体实现，移步<a href="https://github.com/jeffery0628/keyword_extraction" target="_blank" rel="noopener">GitHub</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf_idf</span><span class="params">(document, corpus)</span>:</span>  <span class="comment"># 计算TF-IDF,并返回字典</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param document: 计算document里面每个词的tfidf值，document为文本分词后的形式，</span></span><br><span class="line"><span class="string">    如:[6 月 19 日 2012 年度 中国 爱心 城市 公益活动 新闻 发布会 在京举行]</span></span><br><span class="line"><span class="string">    如果是对一篇文档进行关键词提取，则需要对文档进行分句，把每句话看成一个document，corpus则存放的是整篇文档分词后的所有句子（句子为分词后的结果）。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param corpus:  corpus为所有问当分词后的列表：[document1,document2,document3,...]</span></span><br><span class="line"><span class="string">    :return:dict类型，按照tfidf值从大到小排序： orderdict[word] = tfidf_value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    word_tfidf = &#123;&#125;</span><br><span class="line">    <span class="comment"># 计算词频</span></span><br><span class="line">    freq_words = Counter(document)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> freq_words:</span><br><span class="line">        <span class="comment"># 计算TF：某个词在文章中出现的次数/文章总词数</span></span><br><span class="line">        tf = freq_words[word] / len(document)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算IDF：log(语料库的文档总数/(包含该词的文档数+1))</span></span><br><span class="line">        idf = math.log(len(corpus) / (wordinfilecount(word, corpus) + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算每个词的TFIDF值</span></span><br><span class="line">        tfidf = tf * idf  <span class="comment"># 计算TF-IDF</span></span><br><span class="line">        word_tfidf[word] = tfidf</span><br><span class="line"></span><br><span class="line">    orderdic = sorted(word_tfidf.items(), key=<span class="keyword">lambda</span> item: item[<span class="number">1</span>], reverse=<span class="literal">True</span>)  <span class="comment"># 给字典排序</span></span><br><span class="line">    <span class="keyword">return</span> orderdic</span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    stop_words_path = <span class="string">r'stop_words.txt'</span>  <span class="comment"># 停用词表路径</span></span><br><span class="line">    stop_words = get_stopwords(stop_words_path)  <span class="comment"># 获取停用词表列表</span></span><br><span class="line"></span><br><span class="line">    documents_dir = <span class="string">'data'</span></span><br><span class="line">    filelist = get_documents(documents_dir)  <span class="comment"># 获取文件列表</span></span><br><span class="line"></span><br><span class="line">    corpus = get_corpus(filelist, stop_words)  <span class="comment"># 建立语料库</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx,document <span class="keyword">in</span> enumerate(corpus):</span><br><span class="line">        word_tfidf = tf_idf(document, corpus)  <span class="comment"># 计算TF-IDF</span></span><br><span class="line">        <span class="comment"># 输出前十关键词</span></span><br><span class="line">        print(<span class="string">'document &#123;&#125;,top 10 key words&#123;&#125;'</span>.format(idx+<span class="number">1</span>,word_tfidf[:<span class="number">10</span>]))</span><br></pre></td></tr></table></figure>
<p>使用scikit-learn 计算tfidf<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer, TfidfVectorizer</span><br><span class="line"><span class="comment"># 对语料进行稍微处理</span></span><br><span class="line">corpus = [*map(<span class="keyword">lambda</span> x:<span class="string">" "</span>.join(x), corpus)]</span><br><span class="line">tfidf_model = TfidfVectorizer()</span><br><span class="line">tfidf_matrix = tfidf_model.fit_transform(corpus)  <span class="comment"># 计算每个词的tfidf值</span></span><br><span class="line">words = tfidf_model.get_feature_names()<span class="comment"># 所有词的集合</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(corpus)):</span><br><span class="line">  	word_tfidf = &#123;&#125;</span><br><span class="line">  	<span class="keyword">for</span> j <span class="keyword">in</span> range(len(words)):</span><br><span class="line">        word_tfidf[words[j]] = tfidf_matrix[i, j]</span><br><span class="line">        word_tfidf = sorted(word_tfidf.items(),key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>],reverse=<span class="literal">True</span>)</span><br><span class="line">        print(<span class="string">'document &#123;&#125;,top 10 key words&#123;&#125;'</span>.format(i+<span class="number">1</span>, word_tfidf[:<span class="number">10</span>]))</span><br></pre></td></tr></table></figure></p>
<h2 id="TextRank-提取关键词"><a href="#TextRank-提取关键词" class="headerlink" title="TextRank 提取关键词"></a>TextRank 提取关键词</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>TF-IDF 算法对有多段文本的关键词提取非常有效，但是对于单篇或者文档分割较少的文本表现得不是特别好。如果需要提取关键词的语句只有一句话，那么基于TF-IDF可以知道，所有关键词的重要度都为0(因为IDF值为0)，这种情况下使用TextRank是比较好的选择。</p>
<p>TextRank是一种基于图排序的算法，基本思想是(来源于PageRank)：通过把文本分割成若干组成单元(单词、句子)并建立图模型，利用投票机制对文本中的重要成分进行排序，<strong>仅利用单篇文档本身的信息就可以实现关键词提取、文本摘要</strong>。和 LDA、HMM 等模型不同, TextRank不需要事先对多篇文档进行学习训练, 因其简洁有效而得到广泛应用。</p>
<h3 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h3><h4 id="PageRank-的简化模型"><a href="#PageRank-的简化模型" class="headerlink" title="PageRank 的简化模型"></a>PageRank 的简化模型</h4><p>假设一共有 4 个网页 A、B、C、D。它们之间的链接信息如图所示：</p>
<p><img src="/2020/04/21/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96/pagerank.png" alt="avatar"></p>
<p>出链指的是链接出去的链接。入链指的是链接进来的链接。比如图中 A 有 2 个入链，3 个出链。简单来说，一个网页的影响力 = 所有入链集合的页面的加权影响力之和，用公式表示为：</p>
<script type="math/tex; mode=display">
P R(u)=\sum_{v \in B_{u}} \frac{P R(v)}{L(v)}</script><p>u 为待评估的页面， $B_{u}$ 为页面 $u$ 的入链集合。针对入链集合中的任意页面 $v$，它能给 $u$ 带来的影响力是其自身的影响力 $PR(v)$ 除以 $v$ 页面的出链数量，即页面 $v$ 把影响力 $PR(v)$ 平均分配给了它的出链，这样统计所有能给 $u$ 带来链接的页面 $v$，得到的总和就是网页 $u$ 的影响力，即为 $PR(u)$。所以出链会给被链接的页面赋予影响力，当我们统计了一个网页链出去的数量，也就是统计了这个网页的跳转概率。</p>
<p>在例子中，A 有三个出链分别链接到了 B、C、D 上。那么当用户访问 A 的时候，就有跳转到 B、C 或者 D 的可能性，跳转概率均为 1/3。B 有两个出链，链接到了 A 和 D 上，跳转概率为 1/2。这样，我们可以得到 A、B、C、D 这四个网页的转移矩阵 M：</p>
<script type="math/tex; mode=display">
M=\left[\begin{array}{cccc}
0 & 1 / 2 & 1 & 0 \\
1 / 3 & 0 & 0 & 1 / 2 \\
1 / 3 & 0 & 0 & 1 / 2 \\
1 / 3 & 1 / 2 & 0 & 0
\end{array}\right]</script><p>假设 A、B、C、D 四个页面的初始影响力都是相同的，即：</p>
<script type="math/tex; mode=display">
w_{0}=\left[\begin{array}{l}
1 / 4 \\
1 / 4 \\
1 / 4 \\
1 / 4
\end{array}\right]</script><p>当进行第一次转移之后，各页面的影响力 $w_{1}$ 变为：</p>
<script type="math/tex; mode=display">
w_1=Mw_0=\left[\begin{array}{cccc}
0 & 1 / 2 & 1 & 0 \\
1 / 3 & 0 & 0 & 1 / 2 \\
1 / 3 & 0 & 0 & 1 / 2 \\
1 / 3 & 1 / 2 & 0 & 0
\end{array}\right]\left[\begin{array}{c}
1 / 4 \\
1 / 4 \\
1 / 4 \\
1 / 4
\end{array}\right]=\left[\begin{array}{c}
9 / 24 \\
5 / 24 \\
5 / 24 \\
5 / 24
\end{array}\right]</script><p>然后再用转移矩阵乘以 $w_{1}$ 得到 $w_{2}$ 结果，直到第 $n$ 次迭代后 $w_{n}$ 影响力不再发生变化，可以收敛到 $(0.3333,0.2222,0.2222,0.2222$，也就是对应着 A、B、C、D 四个页面最终平衡状态下的影响力。能看出 A 页面相比于其他页面来说权重更大，也就是 PR 值更高。而 B、C、D 页面的 PR 值相等。</p>
<h4 id="等级泄露（Rank-Leak）"><a href="#等级泄露（Rank-Leak）" class="headerlink" title="等级泄露（Rank Leak）"></a>等级泄露（Rank Leak）</h4><p>如果一个网页没有出链，就像是一个黑洞一样，吸收了其他网页的影响力而不释放，最终会导致其他网页的 PR 值为 0。</p>
<p><img src="/2020/04/21/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96/rank_leak.png" alt="avatar"></p>
<h4 id="等级沉没（Rank-Sink）"><a href="#等级沉没（Rank-Sink）" class="headerlink" title="等级沉没（Rank Sink）"></a>等级沉没（Rank Sink）</h4><p>如果一个网页只有出链，没有入链，计算的过程迭代下来，会导致<strong>这个网页</strong>的 PR 值为 0（也就是不存在公式中的 V）。</p>
<p><img src="/2020/04/21/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96/rank_sink.png" alt="avatar"></p>
<h4 id="解决方案：随机浏览"><a href="#解决方案：随机浏览" class="headerlink" title="解决方案：随机浏览"></a>解决方案：随机浏览</h4><p>为了解决简化模型中存在的等级泄露和等级沉没的问题，拉里·佩奇提出了 PageRank 的随机浏览模型。他假设了这样一个场景：用户并不都是按照跳转链接的方式来上网，还有一种可能是不论当前处于哪个页面，都有概率访问到其他任意的页面，比如说用户就是要直接输入网址访问其他页面，虽然这个概率比较小。</p>
<p>所以他定义了阻尼因子 d，这个因子代表了用户按照跳转链接来上网的概率，通常可以取一个固定值 0.85，而 1-d=0.15 则代表了用户不是通过跳转链接的方式来访问网页的，比如直接输入网址。</p>
<script type="math/tex; mode=display">
P R(u)=\frac{1-d}{N}+d \sum_{v \in B_{u}} \frac{P R(v)}{L(v)}</script><p>其中 $N$ 为网页总数，这样我们又可以重新迭代网页的权重计算了，加入了阻尼因子 $d$，一定程度上解决了等级泄露和等级沉没的问题。通过数学定理也可以证明，最终 PageRank 随机浏览模型是可以收敛的，也就是可以得到一个稳定正常的 PR 值。</p>
<h3 id="TextRank"><a href="#TextRank" class="headerlink" title="TextRank"></a>TextRank</h3><p>TextRank通过词之间的相邻关系构建网络，然后用PageRank迭代计算每个节点的rank值，排序rank值即可得到关键词。PageRank迭代计算公式如下：</p>
<script type="math/tex; mode=display">
P R\left(V_{i}\right)=(1-d)+d * \sum_{j \in \operatorname{In}\left(V_{i}\right)} \frac{1}{\left|O u t\left(V_{j}\right)\right|} P R\left(V_{j}\right)</script><p>其中，d:表示阻尼系数，一般为0.85。$V_i$：表示图中任一节点。$In(V_i)$:表示指向顶点$V_i$的所有顶点的集合。$|Out(V_j)|$：表示由顶点$V_j$连接出去的所有顶点集合个数。$PR(V_i)$：表示顶点$V_i$的最终排序权重。(与pagerank公式基本一致。)</p>
<p>网页之间的链接关系可以用图表示，那么怎么把一个句子（可以看作词的序列）构建成图呢？TextRank将某一个词与其前面的N个词、以及后面的N个词均具有图相邻关系（类似于N-gram语法模型）。具体实现：设置一个长度为N的滑动窗口，所有在这个窗口之内的词都视作词结点的相邻结点；则TextRank构建的词图为无向图。下图给出了由一个文档构建的词图（去掉了停用词并按词性做了筛选）：</p>
<p><img src="/2020/04/21/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96/textrank.png" alt="avatar"></p>
<p>考虑到不同词对可能有不同的共现（co-occurrence），TextRank将共现作为无向图边的权值。那么，TextRank的迭代计算公式如下：</p>
<script type="math/tex; mode=display">
W S\left(V_{i}\right)=(1-d)+d * \sum_{j \in \operatorname{In}\left(V_{i}\right)} \frac{w_{ij}}{\left|O u t\left(V_{j}\right)\right|} W S\left(V_{j}\right)</script><p>该公式仅仅比PageRank多了一个权重项$w_{ij}$，用来表示两个节点之间的边连接有不同的重要程度。</p>
<h4 id="TextRank-关键词-短语-提取算法"><a href="#TextRank-关键词-短语-提取算法" class="headerlink" title="TextRank 关键词(短语)提取算法"></a>TextRank 关键词(短语)提取算法</h4><ol>
<li>把给定的文本$T$按照完整句子进行分割，即$T = [S_1,S_2,\ldots,S_n]$.</li>
<li>文本$T$中每个句子$S_i$，进行分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词，如名词、动词、形容词，即$S_i = [t_{i,1},t_{i,2},\ldots,t_{i,\pi}]$,其中$t_{ij}$是保留后的候选关键词。</li>
<li>构建候选关键词图$G = (V,E)$，其中$V$为节点集，由步骤2生成的候选关键词组成，然后采用共现关系（co-occurrence）构造任两点之间的边，两个节点之间存在边仅当它们对应的词汇在长度为K的窗口中共现，K表示窗口大小，即最多共现K个单词。</li>
<li>根据上面公式，迭代传播各节点的权重，直至收敛。</li>
<li>对节点权重进行倒序排序，从而得到最重要的T个单词，作为候选关键词。</li>
<li>由步骤5得到最重要的T个单词，在原始文本中进行标记，若形成相邻词组，则组合成多词关键词。</li>
</ol>
<h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">textrank_graph</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.graph = defaultdict(list) <span class="comment"># key:[(),(),(),...] 如：是 [('是', '全国', 1), ('是', '调查', 1), ('是', '失业率', 1), ('是', '城镇', 1)]</span></span><br><span class="line">        self.d = <span class="number">0.85</span>  <span class="comment"># d是阻尼系数，一般设置为0.85</span></span><br><span class="line">        self.min_diff = <span class="number">1e-5</span>  <span class="comment"># 设定收敛阈值</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加节点之间的边</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addEdge</span><span class="params">(self, start, end, weight)</span>:</span></span><br><span class="line">        self.graph[start].append((start, end, weight))</span><br><span class="line">        self.graph[end].append((end, start, weight))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 节点排序</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rank</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 一共有14个节点</span></span><br><span class="line">        print(len(self.graph))</span><br><span class="line">        <span class="comment"># 默认初始化权重</span></span><br><span class="line">        weight_deault = <span class="number">1.0</span> / (len(self.graph) <span class="keyword">or</span> <span class="number">1.0</span>)</span><br><span class="line">        <span class="comment"># nodeweight_dict, 存储节点的权重</span></span><br><span class="line">        nodeweight_dict = defaultdict(float)</span><br><span class="line">        <span class="comment"># outsum，存储节点的出度权重</span></span><br><span class="line">        outsum_node_dict = defaultdict(float)</span><br><span class="line">        <span class="comment"># 根据图中的边，更新节点权重</span></span><br><span class="line">        <span class="keyword">for</span> node, out_edge <span class="keyword">in</span> self.graph.items():</span><br><span class="line">            <span class="comment"># 是 [('是', '全国', 1), ('是', '调查', 1), ('是', '失业率', 1), ('是', '城镇', 1)]</span></span><br><span class="line">            nodeweight_dict[node] = weight_deault <span class="comment"># 初始化节点权重</span></span><br><span class="line">            outsum_node_dict[node] = sum((edge[<span class="number">2</span>] <span class="keyword">for</span> edge <span class="keyword">in</span> out_edge), <span class="number">0.0</span>) <span class="comment"># 统计node节点的出度</span></span><br><span class="line">        <span class="comment"># 初始状态下的textrank重要性权重</span></span><br><span class="line">        sorted_keys = sorted(self.graph.keys())</span><br><span class="line">        <span class="comment"># 设定迭代次数，</span></span><br><span class="line">        step_dict = [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">1000</span>):</span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> sorted_keys:</span><br><span class="line">                s = <span class="number">0</span></span><br><span class="line">                <span class="comment"># 计算公式：(edge_weight/outsum_node_dict[edge_node])*node_weight[edge_node]</span></span><br><span class="line">                <span class="keyword">for</span> e <span class="keyword">in</span> self.graph[node]:</span><br><span class="line">                    s += e[<span class="number">2</span>] / outsum_node_dict[e[<span class="number">1</span>]] * nodeweight_dict[e[<span class="number">1</span>]]</span><br><span class="line">                <span class="comment"># 计算公式：(1-d) + d*s</span></span><br><span class="line">                nodeweight_dict[node] = (<span class="number">1</span> - self.d) + self.d * s</span><br><span class="line">            step_dict.append(sum(nodeweight_dict.values()))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> abs(step_dict[step] - step_dict[step - <span class="number">1</span>]) &lt;= self.min_diff:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 利用Z-score进行权重归一化，也称为离差标准化，是对原始数据的线性变换，使结果值映射到[0 - 1]之间。</span></span><br><span class="line">        <span class="comment"># 先设定最大值与最小值均为系统存储的最大值和最小值</span></span><br><span class="line">        (min_rank, max_rank) = (sys.float_info[<span class="number">0</span>], sys.float_info[<span class="number">3</span>])</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> nodeweight_dict.values():</span><br><span class="line">            <span class="keyword">if</span> w &lt; min_rank:</span><br><span class="line">                min_rank = w</span><br><span class="line">            <span class="keyword">if</span> w &gt; max_rank:</span><br><span class="line">                max_rank = w</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> n, w <span class="keyword">in</span> nodeweight_dict.items(): <span class="comment"># 归一化</span></span><br><span class="line">            nodeweight_dict[n] = (w - min_rank / <span class="number">10.0</span>) / (max_rank - min_rank / <span class="number">10.0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nodeweight_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextRank</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.candi_pos = [<span class="string">'n'</span>, <span class="string">'v'</span>, <span class="string">'a'</span>] <span class="comment"># 关键词的词性：名词，动词，形容词</span></span><br><span class="line">        self.span = <span class="number">5</span> <span class="comment"># 窗口大小</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract_keywords</span><span class="params">(self, text, num_keywords)</span>:</span></span><br><span class="line">        g = textrank_graph()</span><br><span class="line">        cm = defaultdict(int)</span><br><span class="line">        word_list = [[word.word, word.flag] <span class="keyword">for</span> word <span class="keyword">in</span> pseg.cut(text)] <span class="comment"># 使用jieba分词并且对词性进行标注</span></span><br><span class="line">        <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(word_list): <span class="comment"># 该循环用于统计在窗口范围内，词的共现次数</span></span><br><span class="line">            <span class="keyword">if</span> word[<span class="number">1</span>][<span class="number">0</span>] <span class="keyword">in</span> self.candi_pos <span class="keyword">and</span> len(word[<span class="number">0</span>]) &gt; <span class="number">1</span>: <span class="comment">#</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, i + self.span):</span><br><span class="line">                    <span class="keyword">if</span> j &gt;= len(word_list):<span class="comment"># 防止下标越界</span></span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    <span class="keyword">if</span> word_list[j][<span class="number">1</span>][<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> self.candi_pos <span class="keyword">or</span> len(word_list[j][<span class="number">0</span>]) &lt; <span class="number">2</span>: <span class="comment"># 排除词性不在关键词词性列表中的词或者词长度小于2的词</span></span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    pair = tuple((word[<span class="number">0</span>], word_list[j][<span class="number">0</span>]))</span><br><span class="line">                    cm[(pair)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> terms, w <span class="keyword">in</span> cm.items():</span><br><span class="line">            g.addEdge(terms[<span class="number">0</span>], terms[<span class="number">1</span>], w)</span><br><span class="line">        nodes_rank = g.rank()</span><br><span class="line">        nodes_rank = sorted(nodes_rank.items(), key=<span class="keyword">lambda</span> asd:asd[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nodes_rank[:num_keywords]</span><br></pre></td></tr></table></figure>
<h2 id="LDA-Latent-Dirichlet-Allocation-提取关键词"><a href="#LDA-Latent-Dirichlet-Allocation-提取关键词" class="headerlink" title="LDA (Latent Dirichlet Allocation) 提取关键词"></a>LDA (Latent Dirichlet Allocation) 提取关键词</h2><h3 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h3><p>TF-IDF 和 TextRank 两种算法更多反映的是文本的统计信息，对于文本之间的语义关系考虑得比较少。LDA是一种能够体文本语义关系的关键词提取方法。</p>
<blockquote>
<p>二项分布（Binomial Distribution），即重复n次的伯努利试验（Bernoulli Experiment），用$\xi$表示随机试验的结果。如果事件发生的概率是P,则不发生的概率$q=1-p$，$N$次独立重复试验中发生K次的概率是$P(\xi=K)= C(n,k) <em> p^k </em> (1-p)^{n-k}$，其中$C(n, k) =\frac{n!}{(k!(n-k)!)}$. 期望：$E(ξ)=np$,方差：$D(ξ)=npq$其中$q=1-p$</p>
<p>多项分布（Multinomial Distribution）：多项式分布是二项式分布的推广。二项分布的典型例子是扔硬币，硬币正面朝上概率为p, 重复扔n次硬币，k次为正面的概率即为一个二项分布概率。把二项分布公式推广至多种状态，就得到了多项分布。<br>某随机实验如果有k个可能结局$A_1,A_2,\ldots,A_k$，分别将他们的出现次数记为随机变量$X_1,X_2,\ldots,X_k$，它们的概率分布分别是$p_1,p_2,\ldots,p_k$那么在$n$次采样的总结果中，$A_1$出现$n_1$次、$A_2$出现$n_2$次,$\dots$,$A_k$出现$n_k$次的这种事件的出现概率$P$有下面公式：</p>
<script type="math/tex; mode=display">
P\left(X_{1}=n_{1}, \cdots, X_{k}=n_{k}\right)=\left\{\begin{array}{ll}
\frac{n !}{n_{1} ! \cdots n_{k} !} p_{1}^{n_{1}} \cdots p_{k}^{n_{k}} & , \sum_{i=1}^{k} n_{i}=n \\
0 & , \text { otherwise }
\end{array}\right.</script><p>Beta分布与Dirichlet分布的定义域均为[0,1]，在实际使用中，通常将两者作为概率的分布，Beta分布描述的是单变量分布，Dirichlet分布描述的是多变量分布，因此，Beta分布可作为二项分布的先验概率，Dirichlet分布可作为多项分布的先验概率。</p>
</blockquote>
<p>在主题模型中，主题表示一个概念，表现为一系列相关的单词，是这些单词的条件概率。形象来说，主题就是一个桶，里面装了出现概率较高的单词，这些单词与这个主题有很强的相关性。</p>
<h3 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h3><p>怎样才能生成主题？对文章的主题应该怎么分析？这是主题模型要解决的问题。</p>
<p> 首先，可以用生成模型来看文档和主题这两件事。所谓生成模型，就是说，我们认为<strong>一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”</strong>这样一个过程得到的。那么，如果我们要生成一篇文档，它里面的每个词语出现的概率为：</p>
<script type="math/tex; mode=display">
p(W|D) = \sum_T P(W|T)P(T|D)</script><p>其中，$W$表示词，$T$表示主题，$D$表示文档。</p>
<p><img src="/2020/04/21/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96/lda.png" alt="avatar"></p>
<p>其中”文档-词语”矩阵表示每个单词在每个文档中的词频，即出现的概率；”主题-词语”矩阵表示每个主题中每个单词的出现概率；”文档-主题”矩阵表示每个主题在每个文档中出现的概率。</p>
<p>单词对于主题的概率和主题对于文档的概率，可以通过Gibbs采样法（？？？）来进行概率的计算。</p>
<p>主题$T_k$下各个词$W_i$的权重计算公式：</p>
<script type="math/tex; mode=display">
P(W_i|T_k) = \frac{C_{ik}+\beta}{\sum_{i=1}^N{C_{ik}+N*\beta}} = \phi_i^{t=k}</script><p>其中，$w_i$：表示单词集合中的任一单词。$T_k$:表示主题集合中任一主题。$P(w_i|T_k)$:表示在主题为$k$时，单词$i$出现的概率，其简记为$\phi_i^{t=k}$，$C_{ik}$:表示语料库中单词$i$被赋予主题$k$的次数。$N$:表示词汇表的大小。$\beta$：表示超参数。</p>
<p>文档$D_m$下各个词$T_k$的权重计算公式：</p>
<script type="math/tex; mode=display">
P(T_k|D_m) = \frac{C_{km}+\alpha}{\sum^K_{k=1}C_{km}+K*\alpha}=\theta^m_{t=k}</script><p>其中，$D_m$:表示文档集合中任一文档。$T_k$:表示主题集合中任一主题。$P(T_k|D_m)$:表示语料库中文档m中单词被赋予主题$k$的次数。$K$：表示主题的数量。$\alpha$表示超参数。</p>
<p>得到了指定文档下某主题出现的概率，以及指定主题下、某单词出现的概率。那么由联合概率分布可以知道，对于指定文档某单词出现的概率：</p>
<script type="math/tex; mode=display">
P(W_i|D_m) = \sum_{k=1}^K{\phi_i^{t=k}*\theta_{t=k}^m}</script><p>基于上述公式，可以计算出单词$i$对于文档$m$的主题重要性。</p>
<p>但是由于在LDA主题概率模型中，所有的词汇都会以一定的概率出现在每个主题，所以这样会导致最终计算的单词对于文档的主题重要性区分度受影响。为了避免这种情况，一般会将单词相对于主题概率小于一定阈值的概率置为0.</p>
<h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 主题模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TopicModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 三个传入参数：处理后的数据集，关键词数量，具体模型（LSI、LDA），主题数量</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, doc_list, keyword_num, model=<span class="string">'LDA'</span>, num_topics=<span class="number">4</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 使用gensim的接口，将文本转为向量化表示</span></span><br><span class="line">        <span class="comment"># 先构建词空间</span></span><br><span class="line">        self.dictionary = corpora.Dictionary(doc_list)</span><br><span class="line">        <span class="comment"># 使用BOW模型向量化 (token_id,freq)</span></span><br><span class="line">        corpus = [self.dictionary.doc2bow(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> doc_list]  <span class="comment"># (token_id,freq)</span></span><br><span class="line">        <span class="comment"># 对每个词，根据tf-idf进行加权，得到加权后的向量表示</span></span><br><span class="line">        self.tfidf_model = models.TfidfModel(corpus)</span><br><span class="line">        self.tfidf_corpus = self.tfidf_model[corpus]</span><br><span class="line"></span><br><span class="line">        self.keyword_num = keyword_num</span><br><span class="line">        self.num_topics = num_topics</span><br><span class="line">        <span class="comment"># 选择加载胡模型</span></span><br><span class="line">        <span class="keyword">if</span> model == <span class="string">'LSI'</span>:</span><br><span class="line">            self.model = self.train_lsi()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.model = self.train_lda()</span><br><span class="line">        <span class="comment"># 得到数据集的主题-词分布</span></span><br><span class="line">        word_dic = self.word_dictionary(doc_list)</span><br><span class="line">        self.wordtopic_dic = self.get_wordtopic(word_dic)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 向量化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">doc2bowvec</span><span class="params">(self, word_list)</span>:</span></span><br><span class="line">        vec_list = [<span class="number">1</span> <span class="keyword">if</span> word <span class="keyword">in</span> word_list <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> word <span class="keyword">in</span> self.dictionary]</span><br><span class="line">        print(<span class="string">"vec_list"</span>, vec_list)</span><br><span class="line">        <span class="keyword">return</span> vec_list</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 词空间构建方法和向量化方法，在没有gensim接口时的一般处理方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word_dictionary</span><span class="params">(self, doc_list)</span>:</span></span><br><span class="line">        dictionary = []</span><br><span class="line">        <span class="comment"># 2及变1及结构</span></span><br><span class="line">        <span class="keyword">for</span> doc <span class="keyword">in</span> doc_list:</span><br><span class="line">            <span class="comment"># extend he append 方法有何异同 容易出错</span></span><br><span class="line">            dictionary.extend(doc)</span><br><span class="line"></span><br><span class="line">        dictionary = list(set(dictionary))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dictionary</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到数据集的主题 - 词分布</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_wordtopic</span><span class="params">(self, word_dic)</span>:</span></span><br><span class="line">        wordtopic_dic = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> word_dic:</span><br><span class="line">            singlist = [word]</span><br><span class="line">            <span class="comment"># 计算每个词胡加权向量</span></span><br><span class="line">            word_corpus = self.tfidf_model[self.dictionary.doc2bow(singlist)]</span><br><span class="line">            <span class="comment"># 计算每个词de主题向量</span></span><br><span class="line">            word_topic = self.model[word_corpus]</span><br><span class="line">            wordtopic_dic[word] = word_topic</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> wordtopic_dic</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_lsi</span><span class="params">(self)</span>:</span></span><br><span class="line">        lsi = models.LsiModel(self.tfidf_corpus, id2word=self.dictionary, num_topics=self.num_topics)</span><br><span class="line">        <span class="keyword">return</span> lsi</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_lda</span><span class="params">(self)</span>:</span></span><br><span class="line">        lda = models.LdaModel(self.tfidf_corpus, id2word=self.dictionary, num_topics=self.num_topics)</span><br><span class="line">        <span class="keyword">return</span> lda</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算词的分布和文档的分布的相似度，取相似度最高的keyword_num个词作为关键词</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_simword</span><span class="params">(self, word_list)</span>:</span></span><br><span class="line">        <span class="comment"># 文档的加权向量</span></span><br><span class="line">        sentcorpus = self.tfidf_model[self.dictionary.doc2bow(word_list)]</span><br><span class="line">        <span class="comment"># 文档主题 向量</span></span><br><span class="line">        senttopic = self.model[sentcorpus]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># senttopic [(0, 0.03457821), (1, 0.034260772), (2, 0.8970413), (3, 0.034119748)]</span></span><br><span class="line">        <span class="comment"># 余弦相似度计算</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">calsim</span><span class="params">(l1, l2)</span>:</span></span><br><span class="line">            a, b, c = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">            <span class="keyword">for</span> t1, t2 <span class="keyword">in</span> zip(l1, l2):</span><br><span class="line">                x1 = t1[<span class="number">1</span>]</span><br><span class="line">                x2 = t2[<span class="number">1</span>]</span><br><span class="line">                a += x1 * x1</span><br><span class="line">                b += x1 * x1</span><br><span class="line">                c += x2 * x2</span><br><span class="line">            sim = a / math.sqrt(b * c) <span class="keyword">if</span> <span class="keyword">not</span> (b * c) == <span class="number">0.0</span> <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line">            <span class="keyword">return</span> sim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算输入文本和每个词的主题分布相似度</span></span><br><span class="line">        sim_dic = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> self.wordtopic_dic.items():</span><br><span class="line">            <span class="comment"># 还是计算每个再本文档中的词  和文档的相识度</span></span><br><span class="line">            <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> word_list:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment">#</span></span><br><span class="line">            sim = calsim(v, senttopic)</span><br><span class="line">            sim_dic[k] = sim</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> sorted(sim_dic.items(), key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:self.keyword_num]:</span><br><span class="line">            print(k, v)</span><br><span class="line">        print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topic_extract</span><span class="params">(word_list, model, pos=False, keyword_num=<span class="number">10</span>)</span>:</span></span><br><span class="line">    doc_list = load_data(pos)</span><br><span class="line">    topic_model = TopicModel(doc_list, keyword_num, model=model)</span><br><span class="line">    topic_model.get_simword(word_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textrank_extract</span><span class="params">(text, pos=False, keyword_num=<span class="number">10</span>)</span>:</span></span><br><span class="line">    textrank = analyse.textrank</span><br><span class="line">    keywords = textrank(text, keyword_num)</span><br><span class="line">    <span class="comment"># 输出抽取出的关键词</span></span><br><span class="line">    <span class="keyword">for</span> keyword <span class="keyword">in</span> keywords:</span><br><span class="line">        print(keyword + <span class="string">"/ "</span>, end=<span class="string">''</span>)</span><br><span class="line">    print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># test</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    text = <span class="string">'6月19日,《2012年度“中国爱心城市”公益活动新闻发布会》在京举行。'</span> + \</span><br><span class="line">           <span class="string">'中华社会救助基金会理事长许嘉璐到会讲话。基金会高级顾问朱发忠,全国老龄'</span> + \</span><br><span class="line">           <span class="string">'办副主任朱勇,民政部社会救助司助理巡视员周萍,中华社会救助基金会副理事长耿志远,'</span> + \</span><br><span class="line">           <span class="string">'重庆市民政局巡视员谭明政。晋江市人大常委会主任陈健倩,以及10余个省、市、自治区民政局'</span> + \</span><br><span class="line">           <span class="string">'领导及四十多家媒体参加了发布会。中华社会救助基金会秘书长时正新介绍本年度“中国爱心城'</span> + \</span><br><span class="line">           <span class="string">'市”公益活动将以“爱心城市宣传、孤老关爱救助项目及第二届中国爱心城市大会”为主要内容,重庆市'</span> + \</span><br><span class="line">           <span class="string">'、呼和浩特市、长沙市、太原市、蚌埠市、南昌市、汕头市、沧州市、晋江市及遵化市将会积极参加'</span> + \</span><br><span class="line">           <span class="string">'这一公益活动。中国雅虎副总编张银生和凤凰网城市频道总监赵耀分别以各自媒体优势介绍了活动'</span> + \</span><br><span class="line">           <span class="string">'的宣传方案。会上,中华社会救助基金会与“第二届中国爱心城市大会”承办方晋江市签约,许嘉璐理'</span> + \</span><br><span class="line">           <span class="string">'事长接受晋江市参与“百万孤老关爱行动”向国家重点扶贫地区捐赠的价值400万元的款物。晋江市人大'</span> + \</span><br><span class="line">           <span class="string">'常委会主任陈健倩介绍了大会的筹备情况。'</span></span><br><span class="line"></span><br><span class="line">    pos = <span class="literal">False</span></span><br><span class="line">    seg_list = seg_to_list(text, pos)</span><br><span class="line">    filter_list = word_filter(seg_list, pos)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'LDA模型结果：'</span>)</span><br><span class="line">    topic_extract(filter_list, <span class="string">'LDA'</span>, pos)</span><br></pre></td></tr></table></figure>
<h4 id="LDA-步骤总结"><a href="#LDA-步骤总结" class="headerlink" title="LDA 步骤总结"></a>LDA 步骤总结</h4><p>数据集处理</p>
<ol>
<li>先构建词空间  Dictionary(4064 unique tokens: [‘上将’, ‘专门’, ‘乘客’, ‘仪式’, ‘体验’]…)</li>
<li>使用BOW模型向量化   corpus [[(0, 1), (1, 1), (2, 2), (3, 1),。。</li>
<li>对每个词，根据tf-idf进行加权，得到加权后的向量表示</li>
</ol>
<p>根据数据集获得模型</p>
<ol>
<li>得到数据集的主题-词分布  model (得到每个词的向量）（文档转列表 再转集合去重，再转列表）{‘白血病’: [(0, 0.1273009), (1, 0.6181468), (2, 0.12732704), (3, 0.12722531)], ‘婴儿’: [。。。</li>
<li>求文档的分布:词》向量》tf/idf加权》同第4步得到文档的分布向量 [(0, 0.033984687), (1, 0.033736005), (2, 0.8978361), (3, 0.03444325)]</li>
<li>计算余弦距离得到结果</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol>
<li><p>为什么要用浅层语义分析？</p>
<p>我觉着一方面是考虑文本的语义信息。还有就是用词袋模型所表示的单词-文本矩阵一方面存在数据稀疏的问题，另一方面就是词本身一词多义和多词一义现象在进行文本相似度计算的时候未必能够准确的表达两个文本的语义相似度。</p>
</li>
<li><p>潜在语义分析算法</p>
<ol>
<li>矩阵奇异值分解算法</li>
<li>非负矩阵分解算法</li>
</ol>
</li>
</ol>
<p><a href="https://github.com/jeffery0628/keyword_extraction" target="_blank" rel="noopener">github 代码</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="https://www.cnblogs.com/jpcflyer/p/11180263.html" target="_blank" rel="noopener">机器学习经典算法之PageRank</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/41091116" target="_blank" rel="noopener">通俗易懂理解——TF-IDF与TextRank</a></li>
</ol>
]]></content>
      <categories>
        <category>自然语言处理基础</category>
      </categories>
  </entry>
  <entry>
    <title>图论</title>
    <url>/2018/03/21/%E5%9B%BE%E8%AE%BA/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="无向图"><a href="#无向图" class="headerlink" title="无向图"></a>无向图</h3><h4 id="定义无向图数据结构"><a href="#定义无向图数据结构" class="headerlink" title="定义无向图数据结构"></a>定义无向图数据结构</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Graph</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        self.n = n</span><br><span class="line">        self.e = <span class="number">0</span></span><br><span class="line">        self.adj = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_edge</span><span class="params">(self, v, w)</span>:</span></span><br><span class="line">        self.adj[v].append(w)</span><br><span class="line">        self.adj[w].append(v)</span><br><span class="line">        self.e += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h4 id="无向图判环"><a href="#无向图判环" class="headerlink" title="无向图判环"></a>无向图判环</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cycle</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, G)</span>:</span></span><br><span class="line">        self.vis = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(G.n)]</span><br><span class="line">        self.has_cycle = <span class="literal">False</span></span><br><span class="line">        self.G = G</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> range(G.n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.vis[s]:</span><br><span class="line">                self.dfs(s, s)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self, v, parent)</span>:</span></span><br><span class="line">        self.vis[v] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> self.G.adj[v]:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.vis[w]:</span><br><span class="line">                self.dfs(w, v)</span><br><span class="line">            <span class="comment"># 相邻节点已被访问过，同时还不是parent节点，则存在环</span></span><br><span class="line">            <span class="keyword">elif</span> w != parent:</span><br><span class="line">                self.has_cycle = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h4 id="无向图是否为二分图"><a href="#无向图是否为二分图" class="headerlink" title="无向图是否为二分图"></a>无向图是否为二分图</h4><p>二分图又叫二部图，是图论中的一种特殊模型。设G=(V,E)是一个无向图，如果顶点V可分割为两个互不相交的子集<code>(A,B)</code>，并且图中的每条边<code>（i，j）</code>所关联的两个顶点i和j分别属于这两个不同的顶点集<code>(i in A,j in B)</code>，则称图G为一个二分图。</p>
<p><img src="/2018/03/21/%E5%9B%BE%E8%AE%BA/二分图.png" alt="avatar"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoColor</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, G)</span>:</span></span><br><span class="line">        self.vis = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(G.n)]</span><br><span class="line">        self.color = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(G.n)]</span><br><span class="line">        self.G = G</span><br><span class="line">        self.is_two_colorable = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> range(G.n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.vis[s]:</span><br><span class="line">                self.dfs(s)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self, v)</span>:</span></span><br><span class="line">        self.vis[v] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> self.G.adj[v]:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.vis[w]:</span><br><span class="line">                self.color[w] = <span class="keyword">not</span> self.color[v]</span><br><span class="line">                self.dfs(w)</span><br><span class="line">            <span class="keyword">elif</span> self.color[w] == self.color[v]:</span><br><span class="line">                self.is_two_colorable = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="最小生成树"><a href="#最小生成树" class="headerlink" title="最小生成树"></a>最小生成树</h3><h4 id="prime-算法"><a href="#prime-算法" class="headerlink" title="prime 算法"></a>prime 算法</h4><p><img src="/2018/03/21/%E5%9B%BE%E8%AE%BA/prime.gif" alt="avatar"></p>
<p>每次向最小生成树中加入权重最小的横切边，横切边是连接树与非树顶点的边。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prim</span><span class="params">(adj_matrix)</span>:</span></span><br><span class="line">    <span class="string">"""给定邻接矩阵，返回MST权值，返回-1表示图不连通"""</span></span><br><span class="line">    n = len(adj_matrix)  <span class="comment"># 顶点0~n-1</span></span><br><span class="line">    vis = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">    low_cut = [float(<span class="string">'inf'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]  <span class="comment"># 横切边的权重</span></span><br><span class="line"></span><br><span class="line">    ans = <span class="number">0</span></span><br><span class="line">    vis[<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        low_cut[i] = adj_matrix[<span class="number">0</span>][i]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        min_cut = float(<span class="string">'inf'</span>)</span><br><span class="line">        p = <span class="number">-1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[j] <span class="keyword">and</span> min_cut &gt; low_cut[j]:</span><br><span class="line">                min_cut = low_cut[j]</span><br><span class="line">                p = j</span><br><span class="line">        <span class="keyword">if</span> min_cut == float(<span class="string">'inf'</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>  <span class="comment"># 原图不连通</span></span><br><span class="line">        ans += min_cut</span><br><span class="line">        vis[p] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment"># 横切边更新为更小的权重</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[j] <span class="keyword">and</span> low_cut[j] &gt; adj_matrix[p][j]:</span><br><span class="line">                low_cut[j] = adj_matrix[p][j]</span><br><span class="line">    <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4 id="Kuskal-算法"><a href="#Kuskal-算法" class="headerlink" title="Kuskal 算法"></a>Kuskal 算法</h4><p><img src="/2018/03/21/%E5%9B%BE%E8%AE%BA/Kuskal.png" alt="avatar"></p>
<p>将图中所有边按照从小到大的顺序加入最小生成树，加入的边不会与已加入的边构成换。(利用并查集UF判断连通性)，直到树中含有n-1条边为止。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kruskal</span><span class="params">(edges, n)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    求最小生成树权值</span></span><br><span class="line"><span class="string">    :param edges: [(u, v, w), ...]，三元组含义(顶点u，顶点v，边权重w)</span></span><br><span class="line"><span class="string">    :param n: 顶点数，顶点范围0~n-1</span></span><br><span class="line"><span class="string">    :return: 最小生成树权值（图不连通，返回-1）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    uf = UnionFind(n)  <span class="comment"># 并查集</span></span><br><span class="line">    edges.sort(key=<span class="keyword">lambda</span> item: item[<span class="number">2</span>])</span><br><span class="line">    edge_cnt = <span class="number">0</span></span><br><span class="line">    ans = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(edges)):</span><br><span class="line">        u, v, w = edges[i]</span><br><span class="line">        root_u, root_v = uf.find(u), uf.find(v)</span><br><span class="line">        <span class="keyword">if</span> root_u != root_v:</span><br><span class="line">            ans += w</span><br><span class="line">            uf.union(u, v)</span><br><span class="line">            edge_cnt += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> edge_cnt == n<span class="number">-1</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> edge_cnt &lt; n<span class="number">-1</span>:  <span class="comment"># 不连通</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">return</span> ans</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UnionFind</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""并查集类"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="string">"""长度为n的并查集"""</span></span><br><span class="line">        self.uf = [<span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n + <span class="number">1</span>)]    <span class="comment"># 列表0位置空出</span></span><br><span class="line">        self.sets_count = n                     <span class="comment"># 判断并查集里共有几个集合, 初始化默认互相独立</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># def find(self, p):</span></span><br><span class="line">    <span class="comment">#     """查找p的根结点(祖先)"""</span></span><br><span class="line">    <span class="comment">#     r = p                                   # 初始p</span></span><br><span class="line">    <span class="comment">#     while self.uf[p] &gt; 0:</span></span><br><span class="line">    <span class="comment">#         p = self.uf[p]</span></span><br><span class="line">    <span class="comment">#     while r != p:                           # 路径压缩, 把搜索下来的结点祖先全指向根结点</span></span><br><span class="line">    <span class="comment">#         self.uf[r], r = p, self.uf[r]</span></span><br><span class="line">    <span class="comment">#     return p</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># def find(self, p):</span></span><br><span class="line">    <span class="comment">#     while self.uf[p] &gt;= 0:</span></span><br><span class="line">    <span class="comment">#         p = self.uf[p]</span></span><br><span class="line">    <span class="comment">#     return p</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">(self, p)</span>:</span></span><br><span class="line">        <span class="string">"""尾递归"""</span></span><br><span class="line">        <span class="keyword">if</span> self.uf[p] &lt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> p</span><br><span class="line">        self.uf[p] = self.find(self.uf[p])</span><br><span class="line">        <span class="keyword">return</span> self.uf[p]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">union</span><span class="params">(self, p, q)</span>:</span></span><br><span class="line">        <span class="string">"""连通p,q 让q指向p"""</span></span><br><span class="line">        proot = self.find(p)</span><br><span class="line">        qroot = self.find(q)</span><br><span class="line">        <span class="keyword">if</span> proot == qroot:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">elif</span> self.uf[proot] &gt; self.uf[qroot]:   <span class="comment"># 负数比较, 左边规模更小</span></span><br><span class="line">            self.uf[qroot] += self.uf[proot]</span><br><span class="line">            self.uf[proot] = qroot</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.uf[proot] += self.uf[qroot]  <span class="comment"># 规模相加</span></span><br><span class="line">            self.uf[qroot] = proot</span><br><span class="line">        self.sets_count -= <span class="number">1</span>                    <span class="comment"># 连通后集合总数减一</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_connected</span><span class="params">(self, p, q)</span>:</span></span><br><span class="line">        <span class="string">"""判断pq是否已经连通"""</span></span><br><span class="line">        <span class="keyword">return</span> self.find(p) == self.find(q)     <span class="comment"># 即判断两个结点是否是属于同一个祖先</span></span><br></pre></td></tr></table></figure>
<h3 id="有向图"><a href="#有向图" class="headerlink" title="有向图"></a>有向图</h3><h4 id="定义无向图的数据结构"><a href="#定义无向图的数据结构" class="headerlink" title="定义无向图的数据结构"></a>定义无向图的数据结构</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiGraph</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        self.n = n</span><br><span class="line">        self.e = <span class="number">0</span></span><br><span class="line">        self.adj = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_edge</span><span class="params">(self, v, w)</span>:</span></span><br><span class="line">        self.adj[v].append(w)</span><br><span class="line">        self.e += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h4 id="有向图判环"><a href="#有向图判环" class="headerlink" title="有向图判环"></a>有向图判环</h4><p>对有向图使用dfs搜索，系统调用栈表示了当前遍历了的有向路径</p>
<p>若递归过程访问到出现在栈中的节点，则图中存在了环。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DirectedCycle</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, G)</span>:</span></span><br><span class="line">        self.on_stack = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(G.n)]</span><br><span class="line">        self.vis = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(G.n)]</span><br><span class="line">        self.has_cycle = <span class="literal">False</span></span><br><span class="line">        self.G = G</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> range(G.n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.vis[s]:</span><br><span class="line">                self.dfs(s)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self, v)</span>:</span></span><br><span class="line">        self.on_stack[v] = <span class="literal">True</span></span><br><span class="line">        self.vis[v] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> self.G.adj[v]:</span><br><span class="line">            <span class="keyword">if</span> self.has_cycle:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="keyword">not</span> self.vis[w]:</span><br><span class="line">                self.dfs(w)</span><br><span class="line">            <span class="keyword">elif</span> self.on_stack[w]:</span><br><span class="line">                self.has_cycle = <span class="literal">True</span></span><br><span class="line">        self.on_stack[v] = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>一个有向五环图的拓扑排序即为所有顶点dfs的逆后序排列</p>
<p>强连通性：有向图任意两点互相可达，则为强连通图。</p>
<p><strong>Kosaraju 算法</strong>：</p>
<ol>
<li>有向图G翻转得到$G^R$</li>
<li>dfs得到$G^R$中顶点的逆后序排列</li>
<li>按照这个你后序排列在G中执行标准dfs，每次递归调用所标记的顶点在同一强连通分量重</li>
</ol>
<p><strong>最短路径比较、总结</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>条件</th>
<th>平均</th>
<th>最坏</th>
<th>空间复杂度</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Dijkstra</td>
<td>边权为正</td>
<td>$ElogV$</td>
<td>$ElogV$</td>
<td>$V$</td>
<td></td>
</tr>
<tr>
<td>拓扑排序</td>
<td>无环</td>
<td>$E$</td>
<td>$E+V$</td>
<td>$V$</td>
<td>无环图最优算法</td>
</tr>
<tr>
<td>Bellman-Ford（基于队列）</td>
<td>无负权重</td>
<td>$E+V$</td>
<td>$VE$</td>
<td>$V$</td>
</tr>
</tbody>
</table>
</div>
<p>Dijkstra 算法：每次添加离起点最近的非树节点</p>
<p>prim：每次添加离树最近的非树节点</p>
<h4 id="单源最短路径，邻接矩阵形式"><a href="#单源最短路径，邻接矩阵形式" class="headerlink" title="单源最短路径，邻接矩阵形式"></a>单源最短路径，邻接矩阵形式</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dijkstra</span><span class="params">(adj_matrix, s)</span>:</span></span><br><span class="line">    <span class="string">"""给定邻接矩阵和起点s，返回s到所有点的最短路径"""</span></span><br><span class="line">    n = len(adj_matrix)  <span class="comment"># 顶点0~n-1</span></span><br><span class="line">    vis = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">    edge_to = [<span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]  <span class="comment"># 路径父节点</span></span><br><span class="line">    dist = [float(<span class="string">'inf'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]  <span class="comment"># 最短路径</span></span><br><span class="line"></span><br><span class="line">    dist[s] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        min_dist = float(<span class="string">'inf'</span>)</span><br><span class="line">        p = <span class="number">-1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[j] <span class="keyword">and</span> dist[j] &lt; min_dist:</span><br><span class="line">                min_dist = dist[j]</span><br><span class="line">                p = j</span><br><span class="line">        <span class="keyword">if</span> p == <span class="number">-1</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        vis[p] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[j] <span class="keyword">and</span> dist[p] + adj_matrix[p][j] &lt; dist[j]:</span><br><span class="line">                dist[j] = dist[p] + adj_matrix[p][j]</span><br><span class="line">                edge_to[j] = p</span><br><span class="line">    <span class="keyword">return</span> dist, edge_to</span><br></pre></td></tr></table></figure>
<p>单元最短路径，邻接表形式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dijkstra</span><span class="params">(adj, s)</span>:</span></span><br><span class="line">    n = len(adj)</span><br><span class="line">    dist = [float(<span class="string">'inf'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line"></span><br><span class="line">    dist[s] = <span class="number">0</span></span><br><span class="line">    pq = [(<span class="number">0</span>, s)]</span><br><span class="line">    <span class="keyword">while</span> pq:</span><br><span class="line">        <span class="comment"># u, v是顶点，d是距离，w是边权重</span></span><br><span class="line">        d, u = heappop(pq)</span><br><span class="line">        <span class="keyword">if</span> d &gt; dist[u]:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> v, w <span class="keyword">in</span> adj[u]:</span><br><span class="line">            <span class="keyword">if</span> d + w &lt; dist[v]:</span><br><span class="line">                dist[v] = d + w</span><br><span class="line">                heappush(pq, (dist[v], v))</span><br><span class="line">    <span class="keyword">return</span> dist</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>图论</tag>
      </tags>
  </entry>
  <entry>
    <title>字符串匹配问题</title>
    <url>/2018/03/14/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="字符串匹配问题"><a href="#字符串匹配问题" class="headerlink" title="字符串匹配问题"></a>字符串匹配问题</h2><p>给定两个字符串<code>S,P</code>,其中<code>S</code>串长度为<code>n</code>，<code>P</code>串长度为<code>m</code>, <code>(m&lt;=n)</code>，判断字符串<code>P</code>是否是<code>S</code>的子串</p>
<h4 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h4><p>容易计算复杂度：<code>O(m*n)</code></p>
<h3 id="Brute-Force-简单匹配"><a href="#Brute-Force-简单匹配" class="headerlink" title="Brute-Force(简单匹配)"></a>Brute-Force(简单匹配)</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_match</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    m, n = len(s), len(p)</span><br><span class="line">    <span class="keyword">if</span> m-n &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m-n+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> s[i:i+n] == p:</span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="Rabin-Karp算法"><a href="#Rabin-Karp算法" class="headerlink" title="Rabin-Karp算法"></a>Rabin-Karp算法</h3><h4 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h4><p>基本思想和暴力破解算法是一样的。也需要一个大小为<strong>m</strong>的窗口，但是不一样的是，不是直接比较两个长度为<strong>m</strong>的字符串，而是比较他们的哈希值。</p>
<h4 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h4><p>一共会有(<strong>n-m+1</strong>)个窗口滑动，这一步的复杂度是<strong>O(n)</strong>。</p>
<p>计算哈希值:</p>
<p>假设现在窗口的起点在<strong>j</strong>这个位置，此时窗口内的字符串哈希值为，</p>
<script type="math/tex; mode=display">
H(S, j)=\sum_{i=0}^{m-1} \alpha^{m-(i+1)} \times \operatorname{char}\left(s_{i}\right)</script><p>那么，当计算下一个窗口的哈希值时，也就是当窗口的起点为<strong>j+1</strong>时，哈希函数值可由如下方法计算：</p>
<script type="math/tex; mode=display">
H(S, j+1)=\alpha\left(H(S, j)-\alpha^{m-1} \operatorname{char}\left(s_{j}\right)\right)+\operatorname{char}\left(s_{j+m}\right)</script><p>这样看来，在计算出第一个窗口的函数值之后，后面的每一个窗口哈希值都可以根据上述公式计算，只需要做一次减法，一次乘法，一次加法。之后的每一次哈希值计算都是<strong>O(1)</strong>的复杂度。所以计算第一个窗口的复杂度，<strong>O(m)</strong>，此后计算每一个窗口的复杂度<strong>O(1)</strong>，总的时间复杂度：<code>O(n+m)</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rabin_karp_match</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    n = len(s)</span><br><span class="line">    m = len(p)</span><br><span class="line">    h1 = hash(p)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, n-m+<span class="number">1</span>):</span><br><span class="line">        h2 = hash(s[i:i+m])</span><br><span class="line">        <span class="keyword">if</span> h1 != h2:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> s[i:i+m] == p:</span><br><span class="line">                <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<h3 id="KMP算法"><a href="#KMP算法" class="headerlink" title="KMP算法"></a>KMP算法</h3><p>有点复杂，有时间再写上。</p>
<h4 id="思想-1"><a href="#思想-1" class="headerlink" title="思想"></a>思想</h4><h4 id="复杂度分析-1"><a href="#复杂度分析-1" class="headerlink" title="复杂度分析"></a>复杂度分析</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># KMP算法：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmp_match</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    m, n = len(s), len(p)</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span> <span class="keyword">or</span> m - n &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    table = partial_table(p)</span><br><span class="line">    i, cur = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> i &lt; m-n+<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> p[cur] == s[i+cur]:</span><br><span class="line">            cur += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> cur == <span class="number">0</span>:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i += cur - table[cur<span class="number">-1</span>]  <span class="comment"># 移动位数=已匹配的字符数-对应的部分匹配值</span></span><br><span class="line">                cur = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> cur == n:</span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">partial_table</span><span class="params">(p)</span>:</span></span><br><span class="line">    m = len(p)</span><br><span class="line">    table = [<span class="number">0</span>] * m</span><br><span class="line">    k = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> q <span class="keyword">in</span> range(<span class="number">1</span>, m):</span><br><span class="line">        <span class="keyword">if</span> p[k] == p[q]:</span><br><span class="line">            k = k + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            k = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> p[k] == p[q]:</span><br><span class="line">                k = k + <span class="number">1</span></span><br><span class="line">        table[q] = k</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> table</span><br></pre></td></tr></table></figure>
<h3 id="Horspool算法"><a href="#Horspool算法" class="headerlink" title="Horspool算法"></a>Horspool算法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shift_table</span><span class="params">(p)</span>:</span></span><br><span class="line">    <span class="comment"># 生成 Horspool 算法的移动表</span></span><br><span class="line">    <span class="comment"># 当前检测字符为c，模式长度为m</span></span><br><span class="line">    <span class="comment"># 如果当前c不包含在模式的前m-1个字符中，移动模式的长度m</span></span><br><span class="line">    <span class="comment"># 其他情况下移动最右边的的c到模式最后一个字符的距离</span></span><br><span class="line">    <span class="comment"># from collections import defaultdict</span></span><br><span class="line">    <span class="comment"># table = defaultdict(lambda: len(p))</span></span><br><span class="line">    table = dict()</span><br><span class="line">    <span class="comment"># print table</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, len(p) - <span class="number">1</span>):</span><br><span class="line">        table[p[index]] = len(p) - <span class="number">1</span> - index</span><br><span class="line">    <span class="comment"># print table</span></span><br><span class="line">    <span class="keyword">return</span> table</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">horspool_match</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    m, n = len(s), len(p)</span><br><span class="line">    table = shift_table(p)</span><br><span class="line">    index = len(p) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> index &lt;= m - <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># print("start matching at", index)</span></span><br><span class="line">        match_count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> match_count &lt; n <span class="keyword">and</span> p[n - <span class="number">1</span> - match_count] == s[index - match_count]:</span><br><span class="line">            match_count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> match_count == len(p):</span><br><span class="line">            <span class="keyword">return</span> index - match_count + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># print s[index], table.get(s[index])</span></span><br><span class="line">            <span class="keyword">if</span> table.get(s[index]) <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                table[s[index]] = n</span><br><span class="line">            index += table[s[index]]</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<h3 id="Sunday-算法"><a href="#Sunday-算法" class="headerlink" title="Sunday 算法"></a>Sunday 算法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sunday_match</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    char_pos = dict()</span><br><span class="line">    <span class="keyword">for</span> i, ch <span class="keyword">in</span> enumerate(p):</span><br><span class="line">        char_pos[ch] = i</span><br><span class="line">    <span class="comment"># print "char_pos:", char_pos</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    m, n = len(s), len(p)</span><br><span class="line">    <span class="keyword">while</span> i &lt;= m - n:</span><br><span class="line">        found = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> j, ch <span class="keyword">in</span> enumerate(p):</span><br><span class="line">            <span class="keyword">if</span> s[i + j] != ch:</span><br><span class="line">                found = <span class="literal">False</span></span><br><span class="line">                <span class="keyword">if</span> (i + n) &lt; m:</span><br><span class="line">                    <span class="keyword">if</span> s[i + n] <span class="keyword">not</span> <span class="keyword">in</span> char_pos:</span><br><span class="line">                        i += (n + <span class="number">1</span>)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        i += (n - char_pos[s[i + n]])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> found:</span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<h3 id="Boyer-Moore算法"><a href="#Boyer-Moore算法" class="headerlink" title="Boyer_Moore算法"></a>Boyer_Moore算法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">boyer_moore_search</span><span class="params">(string, des)</span>:</span></span><br><span class="line">    l = len(des) - <span class="number">1</span></span><br><span class="line">    strlen = len(string) - <span class="number">1</span></span><br><span class="line">    start, end = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> strlen &gt;= <span class="number">0</span>:</span><br><span class="line">        end = start + len(des)</span><br><span class="line">        <span class="comment"># print string[start:end]</span></span><br><span class="line">        cr = compare(string[start:end], des)</span><br><span class="line">        <span class="keyword">if</span> cr[<span class="number">0</span>] == <span class="number">-2</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'not found'</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> cr[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># return end - l</span></span><br><span class="line">            <span class="keyword">return</span> start</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pos = is_character_in(des, cr[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> cr[<span class="number">0</span>] == (len(des) - <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> pos != <span class="number">-1</span>:</span><br><span class="line">                    start += len(des) - <span class="number">1</span> - pos</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    start += len(des)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> pos == <span class="number">-1</span>:</span><br><span class="line">                    <span class="comment">#  have good  string</span></span><br><span class="line">                    goodPos = is_character_in(des, des[l])</span><br><span class="line">                    <span class="keyword">if</span> goodPos == l:</span><br><span class="line">                        start += l + <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        start += l - goodPos</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_character_in</span><span class="params">(s, c)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> pos <span class="keyword">in</span> range(len(s)):</span><br><span class="line">        <span class="keyword">if</span> s[pos] == c:</span><br><span class="line">            <span class="keyword">return</span> pos</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compare</span><span class="params">(str1, str2)</span>:</span></span><br><span class="line">    l1 = len(str1) - <span class="number">1</span></span><br><span class="line">    l2 = len(str2) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> l1 != l2:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-2</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> l1 &gt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> str1[l1] != str2[l1]:</span><br><span class="line">            <span class="keyword">return</span> l1, str1[l1]</span><br><span class="line">        l1 -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>, <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h3 id="BMHBNFS算法（快速匹配算法）"><a href="#BMHBNFS算法（快速匹配算法）" class="headerlink" title="BMHBNFS算法（快速匹配算法）"></a>BMHBNFS算法（快速匹配算法）</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_skip</span><span class="params">(p)</span>:</span></span><br><span class="line">    m = len(p)</span><br><span class="line">    a = p[m<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m<span class="number">-2</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">if</span> p[i] == a:</span><br><span class="line">            <span class="keyword">return</span> m<span class="number">-2</span>-i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bmhbnfs_find</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    <span class="comment"># find first occurrence of p in s</span></span><br><span class="line">    m = len(s)</span><br><span class="line">    n = len(p)</span><br><span class="line">    <span class="comment"># skip是把离p[m-1]最近且字符相同的字符移到m-1位需要跳过的步数-1</span></span><br><span class="line">    <span class="comment"># skip = delta1(p)[p[m - 1]]</span></span><br><span class="line">    skip = compute_skip(p)</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt;= m - n:</span><br><span class="line">        <span class="keyword">if</span> s[i + n - <span class="number">1</span>] == p[n - <span class="number">1</span>]:  <span class="comment"># (boyer-moore)</span></span><br><span class="line">            <span class="comment"># potential match</span></span><br><span class="line">            <span class="keyword">if</span> s[i:i + n - <span class="number">1</span>] == p[:n - <span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">return</span> i</span><br><span class="line">            <span class="keyword">if</span> s[i + n] <span class="keyword">not</span> <span class="keyword">in</span> p:</span><br><span class="line">                i = i + n + <span class="number">1</span>  <span class="comment"># (sunday)</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i = i + skip  <span class="comment"># (horspool)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># skip</span></span><br><span class="line">            <span class="keyword">if</span> s[i + n] <span class="keyword">not</span> <span class="keyword">in</span> p:</span><br><span class="line">                i = i + n + <span class="number">1</span>  <span class="comment"># (sunday)</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i = i + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>  <span class="comment"># not found</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>字符串匹配</tag>
      </tags>
  </entry>
  <entry>
    <title>平滑方法</title>
    <url>/2020/05/08/%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在一个概率模型中，也就是 p(e)在 event space E 下的概率分布，模型很可能会用最大似然估计(MLE)：</p>
<script type="math/tex; mode=display">
P_{M L E}=\frac{c(x)}{\sum_{e} c(e)}</script><p>然而，由于并没有足够的数据，很多事件 $x$ 并没有在训练数据中出现，也就是 $c(x)=0$，$P_{MLE}=0$这是有问题的，没有在训练数据中出现的数据，并不代表不会在测试数据中出现，如果没有考虑到数据稀疏性，模型就显得太简单了。</p>
<p>Data sparsity 是 smoothing 的最大原因。Chen &amp; Goodman 在1998 年提到过，几乎所有数据稀疏的场景下，smoothing 都可以帮助提高 performance，而数据稀疏性几乎是所有统计模型都会遇到的问题。而如果你有足够多的训练数据，所有的 parameters 都可以在没有 smoothing 的情况下被准确的估计，那么你总是可以扩展模型，如原来是 bigram，没有数据稀疏，完全可以扩展到 trigram 来提高 performance，如果还没有出现稀疏，就再往高层推，当 parameters 越来越多的时候，数据稀疏再次成为了问题，这时候，用合适的平滑方法可以得到更准确的模型。实际上，无论有多少的数据，平滑几乎总是可以以很小的代价来提高 performance。</p>
<a id="more"></a>
<h3 id="平滑方法"><a href="#平滑方法" class="headerlink" title="平滑方法"></a>平滑方法</h3><h4 id="add-one-smoothing"><a href="#add-one-smoothing" class="headerlink" title="add-one smoothing"></a>add-one smoothing</h4><p>(也叫laplace smoothing),以bigram为例：</p>
<p>MLE estimate：</p>
<script type="math/tex; mode=display">
P_{M L E}\left(w_{i} | w_{i-1}\right)=\frac{c\left(w_{i-1} w_{i}\right)}{c\left(w_{i-1}\right)}</script><p>Add-one estimate:</p>
<script type="math/tex; mode=display">
P_{A d d-1}\left(w_{i} | w_{i-1}\right)=\frac{c\left(w_{i-1} w_{i}\right)+1}{c\left(w_{i-1}\right)+V}</script><p>其中，$V$表示词典大小。</p>
<p>假设语料库为：</p>
<blockquote>
<ol>
<li>john read moby dick</li>
<li>mary read a different book</li>
<li>she read a book by cher</li>
</ol>
</blockquote>
<p>那么，john read a book 这个句子的概率为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(\text { john read a book })&=p(\text { john } |<s>)  p(\text { read } | \text { john })  p(\text { a } | \text { read })  p(\text { book } | a )  p(<e> | \text {book}) \\
&=\frac{c(<s>,john)}{\sum_{w} c(<s>,w)} \frac{c(john,read)}{\sum_{w} c(john,w)} \frac{c(read,a)}{\sum_{w} c(read,w)} \frac{c(a,book)}{\sum_{w} c(a,w)} \frac{c(book,<e>)}{\sum_{w} c(book,w)}\\
&= \frac{1}{3} * \frac{1}{1} * \frac{2}{3} * \frac{1}{2} * \frac{1}{2} \\
&\approx 0.006
\end{aligned}</script><p>而，cher read a book 这个句子出现的概率是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(\text { cher read a book })&=p(\text { cher } |<s>)  p(\text { read } | \text { cher })  p(\text { a } | \text { read })  p(\text { book } | a )  p(<e> | \text {book}) \\
&=\frac{c(<s>,cher)}{\sum_{w} c(<s>,w)} \frac{c(cher,read)}{\sum_{w} c(cher,w)} \frac{c(read,a)}{\sum_{w} c(read,w)} \frac{c(a,book)}{\sum_{w} c(a,w)} \frac{c(book,<e>)}{\sum_{w} c(book,w)}\\
&= \frac{0}{3} * \frac{0}{1} * \frac{2}{3} * \frac{1}{2} * \frac{1}{2} \\
&\approx 0
\end{aligned}</script><p>如果使用add-one smoothing：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(\text { john read a book })&=p(\text { john } |<s>)  p(\text { read } | \text { john })  p(\text { a } | \text { read })  p(\text { book } | a )  p(<e> | \text {book}) \\
&= \frac{1+1}{3+11} * \frac{1+1}{1+11} * \frac{2+1}{3+11} * \frac{1+1}{2+11} * \frac{1+1}{2+11} \\
&\approx 0.0001
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
p(\text { cher read a book })&=p(\text { cher } |<s>)  p(\text { read } | \text { cher })  p(\text { a } | \text { read })  p(\text { book } | a )  p(<e> | \text {book}) \\
&= \frac{0+1}{3+11} * \frac{0+1}{1+11} * \frac{2+1}{3+11} * \frac{1+1}{2+11} * \frac{1+1}{2+11} \\
&\approx 0.00003
\end{aligned}</script><p>加 1 平滑通常情况下是一种很糟糕的算法，与其他平滑方法相比显得非常差，可以把加 1 平滑用在其他任务中，如文本分类，或者非零计数没那么多的情况下。</p>
<h4 id="Additive-smoothing"><a href="#Additive-smoothing" class="headerlink" title="Additive smoothing"></a>Additive smoothing</h4><p>对加 1 平滑的改进就是把 1 改成 $\delta$，且 $0&lt;\delta$</p>
<script type="math/tex; mode=display">
P_{A d d-\delta}\left(w_{i} | w_{i-1}\right)=\frac{c\left(w_{i-1} w_{i}\right)+\delta}{c\left(w_{i-1}\right)+\delta V}</script><p>可以把$\delta$看作是一个可以调节的超参数。</p>
<h4 id="Good-Turing-smoothing"><a href="#Good-Turing-smoothing" class="headerlink" title="Good-Turing smoothing"></a>Good-Turing smoothing</h4><p><strong>基本思想:</strong> 用观察计数较高的 N-gram 数量来重新估计概率量大小，并把它指派给那些具有零计数或较低计数的 N-gram.</p>
<p>举个例子：假设你在钓鱼，然后抓到了 18 条鱼，种类如下：10条鲤鱼, 3条黑鱼, 2条刀鱼, 1条鲨鱼, 1条草鱼, 1条鳗鱼，那么</p>
<ol>
<li><p>下一个钓到的鱼是鲨鱼的概率是多少？</p>
<p>1/18</p>
</li>
<li><p>下一条与是新鱼种（之前没有出现过）的概率是多少？</p>
<p>3/18(没有出现过的按出现过一次算)</p>
</li>
<li><p>下一条抓到的鱼是鲨鱼的概率是多少？</p>
<p>首先肯定的是概率小于1/18  —&gt; good turing</p>
</li>
</ol>
<p>在good turing 下,对参数的估计分两种情况：</p>
<ol>
<li>没有出现过的：$P_{GT}=\frac{N_1}{N}$</li>
<li>出现过的：$P_{GT}=\frac{(c+1)N_{c+1}}{N_c*N}$</li>
</ol>
<p>其中，$N_c$表示出现c次的单词个数,$N$代表样本总数。<br>比如：</p>
<blockquote>
<p>sam i am i am sam i do not eat </p>
<p>sam：2次</p>
<p>i：3次</p>
<p>am：2次</p>
<p>do：1次</p>
<p>not：1次</p>
<p>eat：1次</p>
</blockquote>
<p>有：$N_3=1,N_2=2,N_1=3$</p>
<p>对于第3个问题：$P_{GT}=\frac{(1+1)<em>1}{3</em>18}=\frac{1}{27}$.</p>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><p>由公式：$P_{GT}=\frac{(c+1)N_{c+1}}{N_c*N}$在计算当前项$c$ 的时候，需要计算到$c+1$的$N_{c+1}$,若$N_{c+1}=0$，则会导致概率为0.</p>
<p>解决方法：可以利用机器学习中回归算法对$N_1,N_2,\ldots,N_c,N_{c+1}$进行拟合，进而得出$N_{c+1}$的值。</p>
<h3 id="Interpolation"><a href="#Interpolation" class="headerlink" title="Interpolation"></a>Interpolation</h3><p><strong>基本思想</strong>：在计算Trigram概率的同时，考虑Unigram，Bigram，Trigram出现的次数。</p>
<p>举个例子：有</p>
<p><code>C(in the kitchen)=0</code>,<code>C(the kitchen)=3</code>,<code>C(kitchen)=4</code>,<code>C(arboretum)=0</code>.在计算<code>p(kitchen|in the)</code>和<code>p(arboretum|in the)</code>的时候，从语料计算得出：$p(kitchen|in,the)=p(arboretum|in,the)=0$,但是从经验上来讲$p(kitchen|in,the)&gt;p(arboretum|in,the)$，因为kitchen要比arboretum常见的多。要实现这个，我们就希望把 bigram 和 unigram 结合起来，interpolate 就是这样一种方法。用线性差值把不同阶的 N-gram 结合起来，这里结合了 trigram，bigram 和 unigram。用 lambda 进行加权</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathrm{p}\left(\mathrm{w}_{\mathrm{n}} | \mathrm{w}_{\mathrm{n}-1}, \mathrm{w}_{\mathrm{n}-2}\right)=& \lambda_{1} \mathrm{p}\left(\mathrm{w}_{\mathrm{n}} | \mathrm{w}_{\mathrm{n}-1}, \mathrm{w}_{\mathrm{n}-2}\right) \\
&+\lambda_{2} \mathrm{p}\left(\mathrm{w}_{\mathrm{n}} | \mathrm{w}_{\mathrm{n}-1}\right) \\
&+\lambda_{3} \mathrm{p}\left(\mathrm{w}_{\mathrm{n}}\right)\\
\sum_i\lambda_i=1
\end{aligned}</script><h5 id="怎样设置-lambdas？？？？"><a href="#怎样设置-lambdas？？？？" class="headerlink" title="怎样设置 lambdas？？？？"></a>怎样设置 lambdas？？？？</h5>]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理基础</tag>
      </tags>
  </entry>
  <entry>
    <title>排序算法</title>
    <url>/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h2><p><img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/排序.png" alt="avatar"></p>
<a id="more"></a>
<h3 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h3><h4 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h4><p>是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。</p>
<h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><ol>
<li>比较相邻的元素。如果第一个比第二个大，就交换它们两个；</li>
<li>对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数；</li>
<li>针对所有的元素重复以上的步骤，除了最后一个；</li>
<li><p>重复步骤1~3，直到排序完成。</p>
<h4 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h4></li>
<li><p>时间复杂度：<code>O(1)</code></p>
</li>
<li>空间复杂度：$O(n^2)$<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubble_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="comment"># 外层循环: 走访数据的次数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 设置是否交换标志位</span></span><br><span class="line">        flag = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 内层循环: 每次走访数据时, 相邻对比次数</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(nums) - i - <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 要求从低到高</span></span><br><span class="line">            <span class="comment"># 如次序有误就交换</span></span><br><span class="line">            <span class="keyword">if</span> nums[j] &gt; nums[j + <span class="number">1</span>]:</span><br><span class="line">                nums[j], nums[j + <span class="number">1</span>] = nums[j + <span class="number">1</span>], nums[j]</span><br><span class="line">                <span class="comment"># 发生了数据交换</span></span><br><span class="line">                flag = <span class="literal">True</span></span><br><span class="line">        <span class="comment"># 如果未发生交换数据, 则说明后续数据均有序</span></span><br><span class="line">        <span class="keyword">if</span> flag == <span class="literal">False</span>:</span><br><span class="line">            <span class="keyword">break</span>  <span class="comment"># 跳出数据走访</span></span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<h3 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h3><h4 id="思想-1"><a href="#思想-1" class="headerlink" title="思想"></a>思想</h4>它的工作原理：首先在未排序序列中找到最小元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。<h4 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h4>n个记录的直接选择排序可经过n-1趟直接选择排序得到有序结果。</li>
<li>初始状态：无序区为<code>R[1…n]</code>，有序区为空；</li>
<li>第i趟排序<code>(i=1,2,3…n-1)</code>开始时，当前有序区和无序区分别为<code>R[1…i-1]和R(i…n）</code>。该趟排序从当前无序区中-选出关键字最小的记录 <code>R[k]</code>，将它与无序区的第1个记录R交换，使<code>R[1…i]</code>和<code>R[i+1…n)</code>分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区；</li>
<li>n-1趟结束，数组有序化了。<h4 id="复杂度-1"><a href="#复杂度-1" class="headerlink" title="复杂度"></a>复杂度</h4></li>
<li>时间复杂度：$O(n^2)$</li>
<li>空间复杂度：O(1)<h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selection_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line">        <span class="keyword">return</span> nums</span><br><span class="line">    size = len(nums)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(size):</span><br><span class="line">        min_idx = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>,size):</span><br><span class="line">            <span class="keyword">if</span> nums[j] &lt; nums[min_idx]:</span><br><span class="line">                min_idx = j</span><br><span class="line">        temp = nums[min_idx]</span><br><span class="line">        nums[min_idx] = nums[i]</span><br><span class="line">        nums[i] = temp</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<h3 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h3><h4 id="思想-2"><a href="#思想-2" class="headerlink" title="思想"></a>思想</h4>和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是O(n log n）的时间复杂度。代价是需要额外的内存空间。</li>
</ol>
<h4 id="步骤-2"><a href="#步骤-2" class="headerlink" title="步骤"></a>步骤</h4><p><strong>归并排序</strong>是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。归并排序是一种稳定的排序方法。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。</p>
<ol>
<li>把长度为n的输入序列分成两个长度为n/2的子序列；</li>
<li>对这两个子序列分别采用归并排序；</li>
<li>将两个排序好的子序列合并成一个最终的排序序列。</li>
</ol>
<h4 id="复杂度-2"><a href="#复杂度-2" class="headerlink" title="复杂度"></a>复杂度</h4><ol>
<li>时间复杂度$O(nlog_2 n)$</li>
</ol>
<h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(nums) &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> nums</span><br><span class="line"></span><br><span class="line">    mid_idx = len(nums) / <span class="number">2</span></span><br><span class="line">    left_array = nums[<span class="number">0</span>:mid_idx]</span><br><span class="line">    right_array = nums[mid_idx:len(nums)]</span><br><span class="line">    <span class="keyword">return</span> merge(merge_sort(left_array),merge_sort(right_array))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span><span class="params">(left_array,right_array)</span>:</span></span><br><span class="line">    <span class="comment"># 合并两个有序数组</span></span><br><span class="line">    res = []</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    j = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; len(left_array) <span class="keyword">and</span> j &lt; len(right_array):</span><br><span class="line">        <span class="keyword">if</span> left_array[i] &lt; right_array[j]:</span><br><span class="line">            res.append(left_array[i])</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            res.append(right_array[j])</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i &lt; len(left_array):</span><br><span class="line">        res.extend(left_array[i:])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        res.extend(right_array[j:])</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h3><h4 id="思想-3"><a href="#思想-3" class="headerlink" title="思想"></a>思想</h4><p>通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。</p>
<h4 id="步骤-3"><a href="#步骤-3" class="headerlink" title="步骤"></a>步骤</h4><p>一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下：</p>
<ol>
<li>从第一个元素开始，该元素可以认为已经被排序；</li>
<li>取出下一个元素，在已经排序的元素序列中从后向前扫描；</li>
<li>如果该元素（已排序）大于新元素，将该元素移到下一位置；</li>
<li>重复步骤3，直到找到已排序的元素小于或者等于新元素的位置；</li>
<li>将新元素插入到该位置后；</li>
<li><p>重复步骤2~5。</p>
<h4 id="复杂度-3"><a href="#复杂度-3" class="headerlink" title="复杂度"></a>复杂度</h4></li>
<li><p>时间复杂度：<code>O(1)</code></p>
</li>
<li>空间复杂度：$O(n^2)$</li>
</ol>
<h4 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(nums)):</span><br><span class="line">        temp = nums[i]</span><br><span class="line">        pos = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> temp &lt; nums[j]:</span><br><span class="line">                nums[j + <span class="number">1</span>] = nums[j]</span><br><span class="line">                pos = j</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pos = j + <span class="number">1</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        nums[pos] = temp</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<h3 id="希尔排序"><a href="#希尔排序" class="headerlink" title="希尔排序"></a>希尔排序</h3><p>希尔排序是希尔（Donald Shell） 于1959年提出的一种排序算法。希尔排序也是一种插入排序，它是简单插入排序经过改进之后的一个更高效的版本，也称为缩小增量排序，同时该算法是冲破O(n2）的第一批算法之一。它与插入排序的不同之处在于，它会优先比较距离较远的元素。希尔排序又叫缩小增量排序。</p>
<h4 id="思想-4"><a href="#思想-4" class="headerlink" title="思想"></a>思想</h4><p>希尔排序是把记录按下表的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。</p>
<h4 id="步骤-4"><a href="#步骤-4" class="headerlink" title="步骤:"></a>步骤:</h4><p>先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序,具体步骤如下：</p>
<ol>
<li>选择一个增量序列<code>t1，t2，…，tk</code>，其中<code>ti&gt;tj，tk=1</code>；</li>
<li>按增量序列个数k，对序列进行 k 趟排序；</li>
<li>每趟排序，根据对应的增量ti，将待排序列分割成若干长度为 m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。<br><img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/aHR0cHM6Ly9pbWFnZXMyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvMTE5MjY5OS8yMDE4MDMvMTE5MjY5OS0yMDE4MDMxOTA5NDExNjA0MC0xNjM4NzY2MjcxLnBuZw.png" alt="avat"><h4 id="复杂度-4"><a href="#复杂度-4" class="headerlink" title="复杂度"></a>复杂度</h4></li>
<li>时间复杂度$O(nlog_2 n)$<h4 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shell_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    size = len(nums)</span><br><span class="line">    gap = size / <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> gap:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(gap, size):</span><br><span class="line">            temp = nums[i]</span><br><span class="line">            pre_index = i - gap</span><br><span class="line">            <span class="comment"># 对于划分到一组内的数进行排序</span></span><br><span class="line">            <span class="keyword">while</span> pre_index &gt;= <span class="number">0</span> <span class="keyword">and</span> nums[pre_index] &gt; temp:</span><br><span class="line">                nums[pre_index + gap] = nums[pre_index]</span><br><span class="line">                pre_index -= gap</span><br><span class="line">            <span class="comment"># 插入</span></span><br><span class="line">            nums[pre_index+gap] = temp</span><br><span class="line">        </span><br><span class="line">        gap /= <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<h3 id="快排"><a href="#快排" class="headerlink" title="快排"></a>快排</h3><img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/1450319-20190525163106857-545290968.png" alt="avatar"><h4 id="步骤-5"><a href="#步骤-5" class="headerlink" title="步骤"></a>步骤</h4>快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。</li>
<li>从数列中挑出一个元素，称为 “基准”（pivot ）</li>
<li>重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作</li>
<li><p>递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。<br>复杂度:</p>
</li>
<li><p>时间复杂度：$O(nlogn)$</p>
</li>
<li>空间复杂度：$O(logn)$</li>
<li>稳定性：不稳定<h4 id="代码-5"><a href="#代码-5" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="comment"># 递归退出条件</span></span><br><span class="line">    <span class="comment"># 仅剩一个元素无需继续分组</span></span><br><span class="line">    <span class="keyword">if</span> len(nums) &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> nums</span><br><span class="line">        <span class="comment"># 设置关键数据</span></span><br><span class="line">    a = nums[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 找出所有比 a 大的数据</span></span><br><span class="line">    big = [x <span class="keyword">for</span> x <span class="keyword">in</span> nums <span class="keyword">if</span> x &gt; a]</span><br><span class="line">    <span class="comment"># 找出所有比 a 小的数据</span></span><br><span class="line">    small = [x <span class="keyword">for</span> x <span class="keyword">in</span> nums <span class="keyword">if</span> x &lt; a]</span><br><span class="line">    <span class="comment"># 找出所有与 a 相等的数据</span></span><br><span class="line">    same = [x <span class="keyword">for</span> x <span class="keyword">in</span> nums <span class="keyword">if</span> x == a]</span><br><span class="line">    <span class="comment"># 拼接数据排序的结果</span></span><br><span class="line">    <span class="keyword">return</span> quick_sort(small) + same + quick_sort(big)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h3><p>堆排序（Heapsort） 是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。</p>
<h4 id="步骤-6"><a href="#步骤-6" class="headerlink" title="步骤"></a>步骤</h4><ol>
<li>将初始待排序关键字序列(R1,R2….Rn)构建成大顶堆，此堆为初始的无序区；</li>
<li>将堆顶元素R[1]与最后一个元素R[n]交换，此时得到新的无序区<code>(R1,R2,……Rn-1)</code>和新的有序区(Rn),且满足<code>R[1,2…n-1]&lt;=R[n]</code>；</li>
<li>由于交换后新的堆顶R[1]可能违反堆的性质，因此需要对当前无序区<code>(R1,R2,……Rn-1)</code>调整为新堆，然后再次将R[1]与无序区最后一个元素交换，得到新的无序区<code>(R1,R2….Rn-2)</code>和新的有序区<code>(Rn-1,Rn)</code>。不断重复此过程直到有序区的元素个数为n-1，则整个排序过程完成。</li>
</ol>
<p><img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/CA6B626B-98BA-4AA5-BC01-FDAFE554BB87.gif" alt="avatar"></p>
<h4 id="复杂度-5"><a href="#复杂度-5" class="headerlink" title="复杂度"></a>复杂度</h4><ol>
<li>时间复杂度：O(nlogn)<h4 id="代码-6"><a href="#代码-6" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">heap_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    size = len(nums)</span><br><span class="line">    <span class="keyword">if</span> size &lt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> nums</span><br><span class="line">    <span class="comment"># 构建一个最大堆</span></span><br><span class="line">    build_max_heap(nums)</span><br><span class="line">    <span class="comment"># 循环将根和最后一个元素交换，然后重新调整最大堆。</span></span><br><span class="line">    <span class="keyword">while</span> size &gt; <span class="number">0</span>:</span><br><span class="line">        temp = nums[<span class="number">0</span>]</span><br><span class="line">        nums[<span class="number">0</span>] = nums[size - <span class="number">1</span>]</span><br><span class="line">        nums[size - <span class="number">1</span>] = temp</span><br><span class="line"></span><br><span class="line">        size -= <span class="number">1</span></span><br><span class="line">        adjust_heap(nums, <span class="number">0</span>,size)</span><br><span class="line">    <span class="keyword">return</span> nums</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_max_heap</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="comment"># 从最后一个非叶子节点开始向上构造最大堆</span></span><br><span class="line">    <span class="comment"># i 的左子树和右子树分别为2i+1 和 2(i+1)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums) / <span class="number">2</span><span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        adjust_heap(nums, i,len(nums))</span><br><span class="line">    <span class="comment"># return nums</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adjust_heap</span><span class="params">(nums, i,size)</span>:</span></span><br><span class="line">    max_index = i</span><br><span class="line">    <span class="comment"># 如果有左子树，且左子树大于父节点，则将最大指针指向左子树</span></span><br><span class="line">    <span class="keyword">if</span> <span class="number">2</span> * i + <span class="number">1</span> &lt; size <span class="keyword">and</span> nums[<span class="number">2</span> * i + <span class="number">1</span>] &gt; nums[max_index]:</span><br><span class="line">        max_index = <span class="number">2</span> * i + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> <span class="number">2</span> * (i + <span class="number">1</span>) &lt; size <span class="keyword">and</span> nums[<span class="number">2</span> * (i + <span class="number">1</span>)] &gt; nums[max_index]:</span><br><span class="line">        max_index = <span class="number">2</span> * (i + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> max_index == i:</span><br><span class="line">        temp = nums[max_index]</span><br><span class="line">        nums[max_index] = nums[i]</span><br><span class="line">        nums[i] = temp</span><br><span class="line">        adjust_heap(nums, max_index,size)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="计数排序"><a href="#计数排序" class="headerlink" title="计数排序"></a>计数排序</h3><p><strong>计数排序</strong>的核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。</p>
<h4 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h4><p>计数排序(Counting sort) 是一种稳定的排序算法。计数排序使用一个额外的数组C，其中第i个元素是待排序数组A中值等于i的元素的个数。然后根据数组C来将A中的元素排到正确的位置。它<strong>只能对整数进行排序</strong>。<br><img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/E4559E2C-6389-4EAB-976A-95289A079EB8.gif" alt="avatar"></p>
<h4 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h4><ol>
<li>找出待排序的数组中最大和最小的元素</li>
<li>统计数组中每个值为i的元素出现的次数，存入数组C的第i项</li>
<li>对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）</li>
<li>反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。<h4 id="复杂度-6"><a href="#复杂度-6" class="headerlink" title="复杂度"></a>复杂度</h4></li>
<li>时间复杂度：O(n+k)<h4 id="代码-7"><a href="#代码-7" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_sort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="comment"># 获取arr中的最大值和最小值</span></span><br><span class="line">    max_num = max(arr)</span><br><span class="line">    min_num = min(arr)</span><br><span class="line">    <span class="comment"># 以最大值和最小值的差作为中间数组的长度,并构建中间数组，初始化为0</span></span><br><span class="line">    length = max_num - min_num + <span class="number">1</span></span><br><span class="line">    temp_arr = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(length)]</span><br><span class="line">    <span class="comment"># 创建结果List，存放排序完成的结果</span></span><br><span class="line">    res_arr = list(range(len(arr)))</span><br><span class="line">    <span class="comment"># 第一次循环遍历</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> arr:</span><br><span class="line">        temp_arr[num - min_num] += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 第二次循环遍历</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, length):</span><br><span class="line">        temp_arr[i] = temp_arr[i] + temp_arr[i - <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 第三次循环遍历</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr) - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        res_arr[temp_arr[arr[i] - min_num] - <span class="number">1</span>] = arr[i]</span><br><span class="line">        temp_arr[arr[i] - min_num] -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res_arr</span><br></pre></td></tr></table></figure>
<h3 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h3>桶排序 是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。</li>
</ol>
<h4 id="基本思想-1"><a href="#基本思想-1" class="headerlink" title="基本思想"></a>基本思想</h4><p>假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排</p>
<p><img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/D7FA465B-93A5-493B-BE2B-15375A28FAE2.gif" alt="avatar"></p>
<h4 id="算法步骤-1"><a href="#算法步骤-1" class="headerlink" title="算法步骤"></a>算法步骤</h4><ol>
<li>根据数据分桶</li>
<li>遍历数组，把数据一个一个放到对应的桶中。</li>
<li>对每个不是空的桶进行排序，可以使用其他排序方法，也可以递归使用桶排序。</li>
<li>从不是空的桶里把排好序的数据拼接起来。<h4 id="复杂度-7"><a href="#复杂度-7" class="headerlink" title="复杂度"></a>复杂度</h4></li>
<li>时间复杂度：最好情况下使用线性时间O(n)，桶排序的时间复杂度，取决与对各个桶之间数据进行排序的时间复杂度，因为其它部分的时间复杂度都为O(n)。很显然，桶划分的越小，各个桶之间的数据越少，排序所用的时间也会越少。<h4 id="代码-8"><a href="#代码-8" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k)</span>:</span></span><br><span class="line">        self.key = k;</span><br><span class="line">        self.next = <span class="literal">None</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bucket_sort</span><span class="params">(arr,bucket_num)</span>:</span></span><br><span class="line">    h = [];</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, bucket_num):</span><br><span class="line">        h.append(node(<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(arr)):</span><br><span class="line">        tmp = node(arr[i])</span><br><span class="line">        map = arr[i] / bucket_num</span><br><span class="line">        p = h[map]</span><br><span class="line">        <span class="keyword">if</span> p.key <span class="keyword">is</span> <span class="number">0</span>:</span><br><span class="line">            h[map].next = tmp</span><br><span class="line">            h[map].key = h[map].key + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">while</span> (p.next != <span class="literal">None</span> <span class="keyword">and</span> p.next.key &lt;= tmp.key):</span><br><span class="line">                p = p.next</span><br><span class="line">            tmp.next = p.next</span><br><span class="line">            p.next = tmp</span><br><span class="line">            h[map].key = h[map].key + <span class="number">1</span></span><br><span class="line">    k = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">10</span>):</span><br><span class="line">        q = h[i].next</span><br><span class="line">        <span class="keyword">while</span> (q != <span class="literal">None</span>):</span><br><span class="line">            arr[k] = q.key</span><br><span class="line">            k = k + <span class="number">1</span></span><br><span class="line">            q = q.next</span><br><span class="line">    <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a>基数排序</h3><p>基数排序也是非比较的排序算法，对每一位进行排序，从最低位开始排序，复杂度为O(kn),为数组长度，k为数组中的数的最大的位数；</p>
<h4 id="基本思想-2"><a href="#基本思想-2" class="headerlink" title="基本思想"></a>基本思想</h4><p>基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。基数排序基于分别排序，分别收集，所以是稳定的。<br><img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/2BA80F27-FCDC-4BEA-BD0B-2AAB59242752.gif" alt="avatar"></p>
<h4 id="算法步骤-2"><a href="#算法步骤-2" class="headerlink" title="算法步骤"></a>算法步骤</h4><ol>
<li>取得数组中的最大数，并取得位数；</li>
<li>arr为原始数组，从最低位开始取每个位组成radix数组；</li>
<li>对radix进行计数排序（利用计数排序适用于小范围数的特点）；</li>
</ol>
<h4 id="复杂度-8"><a href="#复杂度-8" class="headerlink" title="复杂度"></a>复杂度</h4><ol>
<li>时间复杂度：O（kn）</li>
</ol>
<h4 id="代码-9"><a href="#代码-9" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">radix_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    k = len(str(max(nums)))  <span class="comment"># 最大的数的位数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        tong = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            tong[int(num / (<span class="number">10</span> ** i)) % <span class="number">10</span>].append(num)  <span class="comment"># 获取当前排序位数上的数字</span></span><br><span class="line">        <span class="comment">#print(tong)</span></span><br><span class="line">        nums = []</span><br><span class="line">        <span class="keyword">for</span> zitong <span class="keyword">in</span> tong:</span><br><span class="line">            nums = nums + zitong</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>专业技能</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>文本多标签分类</title>
    <url>/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>对于事件抽取中单句多事件的问题，打算用文本多标签分类来进行解决。</p>
<h3 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h3><ol>
<li>类标数量不确定，有些样本可能只有一个类标，有些样本的类标可能高达几十甚至上百个。 </li>
<li>类标之间相互依赖，例如包含蓝天类标的样本很大概率上包含白云，如何解决类标之间的依赖性问题也是一大难点。</li>
<li>多标签的训练集比较难以获取。</li>
</ol>
<a id="more"></a>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>前有很多关于多标签的学习算法，依据解决问题的角度，这些算法可以分为两大类:一是基于问题转化的方法，二是基于算法适用的方法。基于问题转化的方法是转化问题数据，使之使用现有算法；基于算法适用的方法是指针对某一特定的算法进行扩展，从而能够处理多标记数据，改进算法，适用数据。</p>
<h4 id="基于问题转化的方法"><a href="#基于问题转化的方法" class="headerlink" title="基于问题转化的方法"></a><strong>基于问题转化的方法</strong></h4><p>基于问题转化的方法中有的考虑标签之间的关联性，有的不考虑标签的关联性。最简单的不考虑关联性的算法将多标签中的每一个标签当成是单标签，对每一个标签实施常见的分类算法。具体而言，在传统机器学习的模型中对每一类标签做二分类，可以使用SVM、DT、Naïve Bayes、DT、Xgboost等算法；在深度学习中，对每一类训练一个文本分类模型(如：textCNN、textRNN等)。考虑多标签的相关性时候可以将上一个输出的标签当成是下一个标签分类器的输入。在传统机器学习模型中可以使用<strong>分类器链</strong>，在这种情况下，第一个分类器只在输入数据上进行训练，然后每个分类器都在输入空间和链上的所有之前的分类器上进行训练。让我们试着通过一个例子来理解这个问题。在下面给出的数据集里，我们将X作为输入空间，而Y作为标签。在分类器链中，这个问题将被转换成4个不同的标签问题，就像下面所示。黄色部分是输入空间，白色部分代表目标变量。</p>
<p><img src="/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/1.png" alt="avatar"></p>
<p>在深度学习中，于输出层加上一个时序模型，将每一时刻输入的数据序列中加入上一时刻输出的结果值。在获得文章的整体语义(Text feature vector)后，将Text feature vector输入到一个RNN的序列中作为初始值，每一时刻输入是上一时刻的输出。从某种程度上来说，下图所示的模型将多标签任务当成了序列生成任务来处理。</p>
<p><img src="/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/cnn_rnn.png" alt="avatar"></p>
<p>除了将标签分开看之外，还有将标签统一来看(Label Powerset)。在这方面，我们将问题转化为一个多类问题，一个多类分类器在训练数据中发现的所有唯一的标签组合上被训练。让我们通过一个例子来理解它。</p>
<p><img src="/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/multi_one.png" alt="avatar"></p>
<p>在这一点上，我们发现x1和x4有相同的标签。同样的，x3和x6有相同的标签。因此，标签powerset将这个问题转换为一个单一的多类问题，如下所示。</p>
<p><img src="/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/multi_two.png" alt="avatar"></p>
<p>因此，标签powerset给训练集中的每一个可能的标签组合提供了一个独特的类。转化为单标签后就可以使用SVM、textCNN、textRNN等分类算法训练模型了。</p>
<p>感觉Label Powerset只适合标签数少的数据，一旦标签数目太多(假设有n个)，使用Label Powerset后可能的数据集将分布在[0,$2^{n-1}$]空间内，数据会很稀疏。</p>
<h4 id="基于算法适用的方法"><a href="#基于算法适用的方法" class="headerlink" title="基于算法适用的方法"></a><strong>基于算法适用的方法</strong></h4><p>改变算法来直接执行多标签分类，而不是将问题转化为不同的问题子集。在传统机器学习模型中适用于的多标签分类模型有:kNN多标签版本MLkNN，SVM的多标签版本Rank-SVM等。在深度学习中常常是修改多分类模型的输出层，使其适用于多标签的分类，如：在输出层对每一个标签的输出值使用sigmod函数进行2分类(标签之间无关联信息)。或者使用一般的CNN提取句子的语义信息，但考虑到句子表示的不同部分在句子分类的过程中会起不同的作用，故在进行分类时候使用了Attention机制(见图)，使得在预测每一类时候句子的不同部分的表示起不同的作用。</p>
<p><img src="/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/3.png" alt="avatar"></p>
<h3 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h3><ol>
<li><a href="https://ai.baidu.com/tech/nlp/doctagger" target="_blank" rel="noopener">百度AI开放平台(文章标签)</a></li>
<li><a href="https://github.com/inspirehep/magpie" target="_blank" rel="noopener">Magpie(多标签分类工具)</a></li>
<li><a href="https://biendata.com/competition/zhihu/" target="_blank" rel="noopener">2017知乎“看山杯”比赛</a></li>
<li>[英文新闻多 标签分类数据集](<a href="https://drive.google.com/file/d/18-JOCIj9v5bZCrn9CIsk23W4wyhroCp_/view?usp=sharing" target="_blank" rel="noopener">https://drive.google.com/file/d/18-JOCIj9v5bZCrn9CIsk23W4wyhroCp_/view?usp=sharing</a></li>
<li>[网上可采集的、质量较高的多标签分类数据集(<a href="https://movie.douban.com/" target="_blank" rel="noopener">豆瓣电影评论</a>)</li>
<li>GitHub上比较完善的<a href="https://github.com/brightmart/text_classification" target="_blank" rel="noopener">文本多标签分类项目</a></li>
<li>NLPCC2018，task6和tesk8均可以当成是多标签的分类任务；</li>
</ol>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a href="https://www.cnblogs.com/cxf-zzj/p/10049613.html" target="_blank" rel="noopener">多标签分类(multi-label classification)综述</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>序列标注</title>
    <url>/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>后面需要补充序列标注在分词、词性标注、命名实体识别等相关任务的代码实践(走过的坑。。)及相关理论知识。</p>
<h3 id="条件随机场-CRF"><a href="#条件随机场-CRF" class="headerlink" title="条件随机场(CRF)"></a>条件随机场(CRF)</h3><h4 id="crf-用于序列标注问题"><a href="#crf-用于序列标注问题" class="headerlink" title="crf 用于序列标注问题"></a>crf 用于序列标注问题</h4><p>在 CRF 的序列标注问题中，要计算的是条件概率：</p>
<script type="math/tex; mode=display">
P\left(y_{1}, \ldots, y_{n} | x_{1}, \ldots, x_{n}\right)=P\left(y_{1}, \ldots, y_{n} | \boldsymbol{x}\right), \quad \boldsymbol{x}=\left(x_{1}, \ldots, x_{n}\right) \tag{1}</script><a id="more"></a>
<p>为了得到这个概率的估计，CRF 做了两个假设：</p>
<h5 id="假设一：该分布是指数族分布。"><a href="#假设一：该分布是指数族分布。" class="headerlink" title="假设一：该分布是指数族分布。"></a><strong>假设一：该分布是指数族分布。</strong></h5><p>这个假设意味着存在函数 $f(y_1,…,y_n;x)$，使得：</p>
<script type="math/tex; mode=display">
P\left(y_{1}, \ldots, y_{n} | \boldsymbol{x}\right)=\frac{1}{Z(\boldsymbol{x})} \exp \left(f\left(y_{1}, \ldots, y_{n} ; \boldsymbol{x}\right)\right) \tag{2}</script><p>其中 Z(x) 是归一化因子，因为这个是条件分布，所以归一化因子跟 x 有关。这个 f 函数可以视为一个打分函数，打分函数取指数并归一化后就得到概率分布。 </p>
<h5 id="假设二：输出之间的关联仅发生在相邻位置，并且关联是指数加性的。"><a href="#假设二：输出之间的关联仅发生在相邻位置，并且关联是指数加性的。" class="headerlink" title="假设二：输出之间的关联仅发生在相邻位置，并且关联是指数加性的。"></a><strong>假设二：输出之间的关联仅发生在相邻位置，并且关联是指数加性的。</strong></h5><p>这个假设意味着 f(y1,…,yn;x) 可以更进一步简化为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
f\left(y_{1}, \ldots, y_{n} ; \boldsymbol{x}\right)=h\left(y_{1} ; \boldsymbol{x}\right) &+g\left(y_{1}, y_{2} ; \boldsymbol{x}\right)+h\left(y_{2} ; \boldsymbol{x}\right)+\ldots \\
&+g\left(y_{n-1}, y_{n} ; \boldsymbol{x}\right)+h\left(y_{n} ; \boldsymbol{x}\right)
\end{aligned} \tag{3}</script><p>这也就是说，现在我们只需要对每一个标签和每一个相邻标签对分别打分，然后将所有打分结果求和得到总分。</p>
<h5 id="线性链crf"><a href="#线性链crf" class="headerlink" title="线性链crf"></a>线性链crf</h5><p>尽管已经做了大量简化，但一般来说，(3)式所表示的概率模型还是过于复杂，难以求解。<strong>于是考虑到当前深度学习模型中，RNN 或者层叠 CNN 等模型已经能够比较充分捕捉各个 y 与输出 x 的联系</strong>，因此，不妨考虑函数 g 跟 x 无关，那么：</p>
<script type="math/tex; mode=display">
\begin{aligned}
f\left(y_{1}, \ldots, y_{n} ; \boldsymbol{x}\right)=h\left(y_{1} ; \boldsymbol{x}\right) &+g\left(y_{1}, y_{2}\right)+h\left(y_{2} ; \boldsymbol{x}\right)+\ldots \\
&+g\left(y_{n-1}, y_{n}\right)+h\left(y_{n} ; \boldsymbol{x}\right)
\end{aligned}</script><p>这时候 g 实际上就是一个有限的、待训练的参数矩阵而已，而单标签的打分函数 $h(y_i;x$) 可以通过 RNN 或者 CNN 来建模。因此，该模型是可以建立的，其中概率分布变为：</p>
<script type="math/tex; mode=display">
P\left(y_{1}, \ldots, y_{n} | x\right)=\frac{1}{Z(x)} \exp \left(h\left(y_{1} ; x\right)+\sum_{k=1}^{n-1} g\left(y_{k}, y_{k+1}\right)+h\left(y_{k+1} ; x\right)\right)</script><h5 id="归一化因子"><a href="#归一化因子" class="headerlink" title="归一化因子"></a>归一化因子</h5><p>为了训练 CRF 模型，用最大似然方法，也就是用：$-\log P\left(y_{1}, \ldots, y_{n} | x\right)$，作为损失函数，可以算出它等于：</p>
<script type="math/tex; mode=display">
-\left(h\left(y_{1} ; \boldsymbol{x}\right)+\sum_{k=1}^{n-1} g\left(y_{k}, y_{k+1}\right)+h\left(y_{k+1} ; \boldsymbol{x}\right)\right)+\log Z(\boldsymbol{x})</script><p>其中第一项是原来概率式的<strong>分子</strong>的对数，它是对目标的序列的打分，虽然它看上去挺迂回的，但是并不难计算。真正的难度在于<strong>分母</strong>的对数 $logZ(x)$ 这一项。</p>
<p>归一化因子，在物理上也叫配分函数，在这里它需要我们对所有可能的路径的打分进行指数求和，而我们前面已经说到，这样的路径数是指数量级的（k^n），因此直接来算几乎是不可能的。</p>
<p>事实上，<strong>归一化因子难算，几乎是所有概率图模型的公共难题</strong>。幸运的是，在 CRF 模型中，由于我们只考虑了临近标签的联系（马尔可夫假设），因此我们可以递归地算出归一化因子，这使得原来是指数级的计算量降低为线性级别。</p>
<p>具体来说，我们将计算到时刻 $t$ 的归一化因子记为$Z_t$，并将它分为 $k$ 个部分：</p>
<script type="math/tex; mode=display">
Z_{t}=Z_{t}^{(1)}+Z_{t}^{(2)}+\cdots+Z_{t}^{(k)}</script><p>分别是截止到当前时刻 $t$ 中、以标签 $1,…,k$ 为终点的所有路径的得分指数和。那么，我们可以递归地计算：</p>
<script type="math/tex; mode=display">
\begin{aligned}
Z_{t+1}^{(1)} &=\left(Z_{t}^{(1)} G_{11}+Z_{t}^{(2)} G_{21}+\cdots+Z_{t}^{(k)} G_{k 1}\right) h_{t+1}(1 | x) \\
Z_{t+1}^{(2)} &=\left(Z_{t}^{(1)} G_{12}+Z_{t}^{(2)} G_{22}+\cdots+Z_{t}^{(k)} G_{k 2}\right) h_{t+1}(2 | x) \\
& \vdots \\
Z_{t+1}^{(k)} &=\left(Z_{t}^{(1)} G_{1 k}+Z_{t}^{(2)} G_{2 k}+\cdots+Z_{t}^{(k)} G_{k k}\right) h_{t+1}(k | x)
\end{aligned}</script><p>它可以简单写为矩阵形式：</p>
<script type="math/tex; mode=display">
\mathbf{Z}_{t+1}=\mathbf{Z}_{t} \boldsymbol{G} \otimes H\left(y_{t+1} | \boldsymbol{x}\right)</script><p>其中,$Z_{t}=\left[Z_{t}^{(1)}, \ldots, Z_{t}^{(k)}\right]$,而 G 是对 g(yi,yj) (状态转移)各个元素取指数后的矩阵，即$G=e^{g(y i, y j)}$,而$H(y_{t+1}|x)$,是编码模型$h(y_{t+1}|x)$,（RNN、CNN等）对位置 t+1 的各个标签的打分的指数，即$H\left(y_{t+1} | x\right)=e^{h\left(y_{t+1} | x\right)}$，也是一个向量。$Z_tG$ 这一步是矩阵乘法，得到一个向量，而 $⊗$ 是两个向量的逐位对应相乘。</p>
<p><img src="/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/decode-crf.png" alt="avatar"></p>
<p>从t到t+1时刻的计算，包括转移概率和j+1节点本身的概率</p>
<h5 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h5><p>写出损失函数 $-logP(y_1,…,y_n|x)$ 后，就可以完成模型的训练了，因为目前的深度学习框架都已经带有自动求导的功能，只要我们能写出可导的 loss，就可以帮我们完成优化过程了。 </p>
<p>那么剩下的最后一步，就是模型训练完成后，如何根据输入找出最优路径来。跟前面一样，这也是一个从 $k^n$ 条路径中选最优的问题，而同样地，因为马尔可夫假设的存在，它可以转化为一个动态规划问题，用 viterbi 算法解决，计算量正比于 n。 </p>
<p>动态规划在本博客已经出现了多次了，<strong>它的递归思想就是：一条最优路径切成两段，那么每一段都是一条（局部）最优路径</strong>。</p>
<h5 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List, Optional</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Conditional random field.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This module implements a conditional random field [LMP01]_. The forward computation</span></span><br><span class="line"><span class="string">    of this class computes the log likelihood of the given sequence of tags and</span></span><br><span class="line"><span class="string">    emission score tensor. This class also has `~CRF.decode` method which finds</span></span><br><span class="line"><span class="string">    the best tag sequence given an emission score tensor using `Viterbi algorithm`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_tags: Number of tags.</span></span><br><span class="line"><span class="string">        batch_first: Whether the first dimension corresponds to the size of a minibatch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        start_transitions (`~torch.nn.Parameter`): Start transition score tensor of size</span></span><br><span class="line"><span class="string">            ``(num_tags,)``.</span></span><br><span class="line"><span class="string">        end_transitions (`~torch.nn.Parameter`): End transition score tensor of size</span></span><br><span class="line"><span class="string">            ``(num_tags,)``.</span></span><br><span class="line"><span class="string">        transitions (`~torch.nn.Parameter`): Transition score tensor of size</span></span><br><span class="line"><span class="string">            ``(num_tags, num_tags)``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. [LMP01] Lafferty, J., McCallum, A., Pereira, F. (2001).</span></span><br><span class="line"><span class="string">       "Conditional random fields: Probabilistic models for segmenting and</span></span><br><span class="line"><span class="string">       labeling sequence data". *Proc. 18th International Conf. on Machine</span></span><br><span class="line"><span class="string">       Learning*. Morgan Kaufmann. pp. 282–289.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_tags: int, batch_first: bool = False)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="keyword">if</span> num_tags &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f'invalid number of tags: <span class="subst">&#123;num_tags&#125;</span>'</span>)</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.num_tags = num_tags</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.start_transitions = nn.Parameter(torch.empty(num_tags))</span><br><span class="line">        self.end_transitions = nn.Parameter(torch.empty(num_tags))</span><br><span class="line">        self.transitions = nn.Parameter(torch.empty(num_tags, num_tags))</span><br><span class="line"></span><br><span class="line">        self.reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span><span class="params">(self)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""Initialize the transition parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The parameters will be initialized randomly from a uniform distribution</span></span><br><span class="line"><span class="string">        between -0.1 and 0.1.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        nn.init.uniform_(self.start_transitions, <span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">        nn.init.uniform_(self.end_transitions, <span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">        nn.init.uniform_(self.transitions, <span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span> -&gt; str:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f'<span class="subst">&#123;self.__class__.__name__&#125;</span>(num_tags=<span class="subst">&#123;self.num_tags&#125;</span>)'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            self,</span></span></span><br><span class="line"><span class="function"><span class="params">            emissions: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            tags: torch.LongTensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            mask: Optional[torch.ByteTensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">            reduction: str = <span class="string">'sum'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">"""Compute the conditional log likelihood of a sequence of tags given emission scores.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            emissions (`~torch.Tensor`): Emission score tensor of size</span></span><br><span class="line"><span class="string">                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,</span></span><br><span class="line"><span class="string">                ``(batch_size, seq_length, num_tags)`` otherwise.</span></span><br><span class="line"><span class="string">            tags (`~torch.LongTensor`): Sequence of tags tensor of size</span></span><br><span class="line"><span class="string">                ``(seq_length, batch_size)`` if ``batch_first`` is ``False``,</span></span><br><span class="line"><span class="string">                ``(batch_size, seq_length)`` otherwise.</span></span><br><span class="line"><span class="string">            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``</span></span><br><span class="line"><span class="string">                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.</span></span><br><span class="line"><span class="string">            reduction: Specifies  the reduction to apply to the output:</span></span><br><span class="line"><span class="string">                ``none|sum|mean|token_mean``. ``none``: no reduction will be applied.</span></span><br><span class="line"><span class="string">                ``sum``: the output will be summed over batches. ``mean``: the output will be</span></span><br><span class="line"><span class="string">                averaged over batches. ``token_mean``: the output will be averaged over tokens.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            `~torch.Tensor`: The log likelihood. This will have size ``(batch_size,)`` if</span></span><br><span class="line"><span class="string">            reduction is ``none``, ``()`` otherwise.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self._validate(emissions, tags=tags, mask=mask)</span><br><span class="line">        <span class="keyword">if</span> reduction <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">'none'</span>, <span class="string">'sum'</span>, <span class="string">'mean'</span>, <span class="string">'token_mean'</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f'invalid reduction: <span class="subst">&#123;reduction&#125;</span>'</span>)</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            mask = torch.ones_like(tags, dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            emissions = emissions.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            tags = tags.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            mask = mask.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        numerator = self._compute_score(emissions, tags, mask)</span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        denominator = self._compute_normalizer(emissions, mask)</span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        llh = numerator - denominator</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> reduction == <span class="string">'none'</span>:</span><br><span class="line">            <span class="keyword">return</span> llh</span><br><span class="line">        <span class="keyword">if</span> reduction == <span class="string">'sum'</span>:</span><br><span class="line">            <span class="keyword">return</span> llh.sum()</span><br><span class="line">        <span class="keyword">if</span> reduction == <span class="string">'mean'</span>:</span><br><span class="line">            <span class="keyword">return</span> llh.mean()</span><br><span class="line">        <span class="keyword">assert</span> reduction == <span class="string">'token_mean'</span></span><br><span class="line">        <span class="keyword">return</span> llh.sum() / mask.type_as(emissions).sum()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, emissions: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">               mask: Optional[torch.ByteTensor] = None)</span> -&gt; List[List[int]]:</span></span><br><span class="line">        <span class="string">"""Find the most likely tag sequence using Viterbi algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            emissions (`~torch.Tensor`): Emission score tensor of size</span></span><br><span class="line"><span class="string">                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,</span></span><br><span class="line"><span class="string">                ``(batch_size, seq_length, num_tags)`` otherwise.</span></span><br><span class="line"><span class="string">            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``</span></span><br><span class="line"><span class="string">                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            List of list containing the best tag sequence for each batch.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self._validate(emissions, mask=mask)</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            mask = emissions.new_ones(emissions.shape[:<span class="number">2</span>], dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            emissions = emissions.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            mask = mask.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self._viterbi_decode(emissions, mask)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_validate</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            self,</span></span></span><br><span class="line"><span class="function"><span class="params">            emissions: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            tags: Optional[torch.LongTensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">            mask: Optional[torch.ByteTensor] = None)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="keyword">if</span> emissions.dim() != <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f'emissions must have dimension of 3, got <span class="subst">&#123;emissions.dim()&#125;</span>'</span>)</span><br><span class="line">        <span class="keyword">if</span> emissions.size(<span class="number">2</span>) != self.num_tags:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f'expected last dimension of emissions is <span class="subst">&#123;self.num_tags&#125;</span>, '</span></span><br><span class="line">                <span class="string">f'got <span class="subst">&#123;emissions.size(<span class="number">2</span>)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> tags <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> emissions.shape[:<span class="number">2</span>] != tags.shape:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">'the first two dimensions of emissions and tags must match, '</span></span><br><span class="line">                    <span class="string">f'got <span class="subst">&#123;tuple(emissions.shape[:<span class="number">2</span>])&#125;</span> and <span class="subst">&#123;tuple(tags.shape)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> emissions.shape[:<span class="number">2</span>] != mask.shape:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">'the first two dimensions of emissions and mask must match, '</span></span><br><span class="line">                    <span class="string">f'got <span class="subst">&#123;tuple(emissions.shape[:<span class="number">2</span>])&#125;</span> and <span class="subst">&#123;tuple(mask.shape)&#125;</span>'</span>)</span><br><span class="line">            no_empty_seq = <span class="keyword">not</span> self.batch_first <span class="keyword">and</span> mask[<span class="number">0</span>].all()</span><br><span class="line">            no_empty_seq_bf = self.batch_first <span class="keyword">and</span> mask[:, <span class="number">0</span>].all()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> no_empty_seq <span class="keyword">and</span> <span class="keyword">not</span> no_empty_seq_bf:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">'mask of the first timestep must all be on'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_compute_score</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            self, emissions: torch.Tensor, tags: torch.LongTensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            mask: torch.ByteTensor)</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="comment"># emissions: (seq_length, batch_size, num_tags)</span></span><br><span class="line">        <span class="comment"># tags: (seq_length, batch_size)</span></span><br><span class="line">        <span class="comment"># mask: (seq_length, batch_size)</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.dim() == <span class="number">3</span> <span class="keyword">and</span> tags.dim() == <span class="number">2</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.shape[:<span class="number">2</span>] == tags.shape</span><br><span class="line">        <span class="keyword">assert</span> emissions.size(<span class="number">2</span>) == self.num_tags</span><br><span class="line">        <span class="keyword">assert</span> mask.shape == tags.shape</span><br><span class="line">        <span class="keyword">assert</span> mask[<span class="number">0</span>].all()</span><br><span class="line"></span><br><span class="line">        seq_length, batch_size = tags.shape</span><br><span class="line">        mask = mask.type_as(emissions)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Start transition score and first emission</span></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        score = self.start_transitions[tags[<span class="number">0</span>]]</span><br><span class="line">        score += emissions[<span class="number">0</span>, torch.arange(batch_size), tags[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, seq_length):</span><br><span class="line">            <span class="comment"># Transition score to next tag, only added if next timestep is valid (mask == 1)</span></span><br><span class="line">            <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">            score += self.transitions[tags[i - <span class="number">1</span>], tags[i]] * mask[i]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Emission score for next tag, only added if next timestep is valid (mask == 1)</span></span><br><span class="line">            <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># End transition score</span></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        seq_ends = mask.long().sum(dim=<span class="number">0</span>) - <span class="number">1</span></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        last_tags = tags[seq_ends, torch.arange(batch_size)]</span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        score += self.end_transitions[last_tags]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_compute_normalizer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            self, emissions: torch.Tensor, mask: torch.ByteTensor)</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="comment"># emissions: (seq_length, batch_size, num_tags)</span></span><br><span class="line">        <span class="comment"># mask: (seq_length, batch_size)</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.dim() == <span class="number">3</span> <span class="keyword">and</span> mask.dim() == <span class="number">2</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.shape[:<span class="number">2</span>] == mask.shape</span><br><span class="line">        <span class="keyword">assert</span> emissions.size(<span class="number">2</span>) == self.num_tags</span><br><span class="line">        <span class="keyword">assert</span> mask[<span class="number">0</span>].all()</span><br><span class="line"></span><br><span class="line">        seq_length = emissions.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Start transition score and first emission; score has size of</span></span><br><span class="line">        <span class="comment"># (batch_size, num_tags) where for each batch, the j-th column stores</span></span><br><span class="line">        <span class="comment"># the score that the first timestep has tag j</span></span><br><span class="line">        <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">        score = self.start_transitions + emissions[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, seq_length):</span><br><span class="line">            <span class="comment"># Broadcast score for every possible next tag</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags, 1)</span></span><br><span class="line">            broadcast_score = score.unsqueeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Broadcast emission score for every possible current tag</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, 1, num_tags)</span></span><br><span class="line">            broadcast_emissions = emissions[i].unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute the score tensor of size (batch_size, num_tags, num_tags) where</span></span><br><span class="line">            <span class="comment"># for each sample, entry at row i and column j stores the sum of scores of all</span></span><br><span class="line">            <span class="comment"># possible tag sequences so far that end with transitioning from tag i to tag j</span></span><br><span class="line">            <span class="comment"># and emitting</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags, num_tags)</span></span><br><span class="line">            next_score = broadcast_score + self.transitions + broadcast_emissions</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Sum over all possible current tags, but we're in score space, so a sum</span></span><br><span class="line">            <span class="comment"># becomes a log-sum-exp: for each sample, entry i stores the sum of scores of</span></span><br><span class="line">            <span class="comment"># all possible tag sequences so far, that end in tag i</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">            next_score = torch.logsumexp(next_score, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Set score to the next score if this timestep is valid (mask == 1)</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">            score = torch.where(mask[i].unsqueeze(<span class="number">1</span>), next_score, score)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># End transition score</span></span><br><span class="line">        <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">        score += self.end_transitions</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Sum (log-sum-exp) over all possible tags</span></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        <span class="keyword">return</span> torch.logsumexp(score, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_viterbi_decode</span><span class="params">(self, emissions: torch.FloatTensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                        mask: torch.ByteTensor)</span> -&gt; List[List[int]]:</span></span><br><span class="line">        <span class="comment"># emissions: (seq_length, batch_size, num_tags)</span></span><br><span class="line">        <span class="comment"># mask: (seq_length, batch_size)</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.dim() == <span class="number">3</span> <span class="keyword">and</span> mask.dim() == <span class="number">2</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.shape[:<span class="number">2</span>] == mask.shape</span><br><span class="line">        <span class="keyword">assert</span> emissions.size(<span class="number">2</span>) == self.num_tags</span><br><span class="line">        <span class="keyword">assert</span> mask[<span class="number">0</span>].all()</span><br><span class="line"></span><br><span class="line">        seq_length, batch_size = mask.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Start transition and first emission</span></span><br><span class="line">        <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">        score = self.start_transitions + emissions[<span class="number">0</span>]</span><br><span class="line">        history = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># score is a tensor of size (batch_size, num_tags) where for every batch,</span></span><br><span class="line">        <span class="comment"># value at column j stores the score of the best tag sequence so far that ends</span></span><br><span class="line">        <span class="comment"># with tag j</span></span><br><span class="line">        <span class="comment"># history saves where the best tags candidate transitioned from; this is used</span></span><br><span class="line">        <span class="comment"># when we trace back the best tag sequence</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Viterbi algorithm recursive case: we compute the score of the best tag sequence</span></span><br><span class="line">        <span class="comment"># for every possible next tag</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, seq_length):</span><br><span class="line">            <span class="comment"># Broadcast viterbi score for every possible next tag</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags, 1)</span></span><br><span class="line">            broadcast_score = score.unsqueeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Broadcast emission score for every possible current tag</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, 1, num_tags)</span></span><br><span class="line">            broadcast_emission = emissions[i].unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute the score tensor of size (batch_size, num_tags, num_tags) where</span></span><br><span class="line">            <span class="comment"># for each sample, entry at row i and column j stores the score of the best</span></span><br><span class="line">            <span class="comment"># tag sequence so far that ends with transitioning from tag i to tag j and emitting</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags, num_tags)</span></span><br><span class="line">            next_score = broadcast_score + self.transitions + broadcast_emission</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Find the maximum score over all possible current tag</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">            next_score, indices = next_score.max(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Set score to the next score if this timestep is valid (mask == 1)</span></span><br><span class="line">            <span class="comment"># and save the index that produces the next score</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">            score = torch.where(mask[i].unsqueeze(<span class="number">1</span>), next_score, score)</span><br><span class="line">            history.append(indices)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># End transition score</span></span><br><span class="line">        <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">        score += self.end_transitions</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Now, compute the best path for each sample</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        seq_ends = mask.long().sum(dim=<span class="number">0</span>) - <span class="number">1</span></span><br><span class="line">        best_tags_list = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(batch_size):</span><br><span class="line">            <span class="comment"># Find the tag which maximizes the score at the last timestep; this is our best tag</span></span><br><span class="line">            <span class="comment"># for the last timestep</span></span><br><span class="line">            _, best_last_tag = score[idx].max(dim=<span class="number">0</span>)</span><br><span class="line">            best_tags = [best_last_tag.item()]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># We trace back where the best last tag comes from, append that to our best tag</span></span><br><span class="line">            <span class="comment"># sequence, and trace it back again, and so on</span></span><br><span class="line">            <span class="keyword">for</span> hist <span class="keyword">in</span> reversed(history[:seq_ends[idx]]):</span><br><span class="line">                best_last_tag = hist[idx][best_tags[<span class="number">-1</span>]]</span><br><span class="line">                best_tags.append(best_last_tag.item())</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Reverse the order because we start from the last timestep</span></span><br><span class="line">            best_tags.reverse()</span><br><span class="line">            best_tags_list.append(best_tags)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> best_tags_list</span><br></pre></td></tr></table></figure>
<h4 id="逐帧-softmax-和-CRF-的异同"><a href="#逐帧-softmax-和-CRF-的异同" class="headerlink" title="逐帧 softmax 和 CRF 的异同?"></a>逐帧 softmax 和 CRF 的异同?</h4><p><strong>逐帧softmax</strong></p>
<p>CRF 主要用于序列标注问题，可以简单理解为是<strong>给序列中的每一帧都进行分类</strong>，既然是分类，很自然想到将这个序列用 CNN 或者 RNN 进行编码后，接一个全连接层用 softmax 激活，如下图所示：</p>
<p><img src="/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/encoder.png" alt="avatar"></p>
<p>然而，<strong>逐帧softmax并没有直接考虑输出的上下文关联</strong>。比如：当我们设计标签时，比如用 s、b、m、e 的 4 个标签来做字标注法的分词，目标输出序列本身会带有一些上下文关联，如 s 后面就不能接 m 和 e，等等。逐标签 softmax 并没有考虑这种输出层面的上下文关联，所以它意味着把这些关联放到了编码层面，希望模型能自己学到这些内容，但有时候会“强模型所难”。 </p>
<p>而 CRF 则更直接一点，它<strong>将输出层面的关联分离了出来</strong>，这使得模型在学习上更为“从容”,CRF在输出端显式地考虑了上下文关联.</p>
<p><img src="/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/encoder-crf.png" alt="avatar"></p>
<p>如果仅仅是引入输出的关联，还不仅仅是 CRF 的全部，CRF 的真正精巧的地方，是它以路径为单位，考虑的是路径的概率。 </p>
<p>假如一个输入有 n 帧，每一帧的标签有 k 中可能性，那么理论上就有$k^n$中不同的输入。我们将它用如下的网络图进行简单的可视化。在下图中，每个点代表一个标签的可能性，点之间的连线表示标签之间的关联，而每一种标注结果，都对应着图上的一条完整的路径。</p>
<p><img src="/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/crf-v.png" alt="avatar"></p>
<p>而在序列标注任务中，我们的正确答案是一般是唯一的。比如“今天天气不错”，如果对应的分词结果是“今天/天气/不/错”，那么目标输出序列就是 bebess，除此之外别的路径都不符合要求。</p>
<p>换言之，在序列标注任务中，我们的研究的基本单位应该是路径，我们要做的事情，是从 $k^n$ 条路径选出正确的一条，那就意味着，如果将它视为一个分类问题，那么将是 $k^n$ 类中选一类的分类问题。</p>
<p>这就是逐帧 softmax 和 CRF 的根本不同了：<strong>前者将序列标注看成是 n 个 k 分类问题，后者将序列标注看成是 1 个$ k^n$ 分类问题</strong>。</p>
<h4 id="CRF"><a href="#CRF" class="headerlink" title="CRF++"></a>CRF++</h4><h5 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h5><p>CRF++ 是C++ 实现的CRF 工具。</p>
<p><a href="https://taku910.github.io/crfpp/" target="_blank" rel="noopener">文档</a></p>
<h5 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/taku910/crfpp.git</span><br><span class="line">cd crfpp</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>如果make的时候发生找不到winmain.h的错误: 可以下面这种方式修复：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sed -i <span class="string">'/#include "winmain.h"/d'</span> crf_test.cpp</span><br><span class="line">sed -i <span class="string">'/#include "winmain.h"/d'</span> crf_learn.cpp</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>后面需要用到Python 使用训练好的模型所以也一起安装CRFPP, cd python 目录下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cd python</span><br><span class="line">python setup.py build</span><br><span class="line">sudo python setup.py install</span><br></pre></td></tr></table></figure>
<p>然后在Python 或者Ipython 里输入 <code>import CRFPP</code> 如果发生如下错误</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ImportError: libcrfpp.so<span class="number">.0</span>: cannot open shared object file: No such file <span class="keyword">or</span> directory</span><br></pre></td></tr></table></figure>
<p>可用下面的方法解决</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/ld.so.conf</span><br><span class="line"><span class="comment"># 添加</span></span><br><span class="line">include /usr/local/lib</span><br><span class="line"><span class="comment"># 保存后加载一下</span></span><br><span class="line">sudo /sbin/ldconfig -v</span><br></pre></td></tr></table></figure>
<h5 id="训练数据格式"><a href="#训练数据格式" class="headerlink" title="训练数据格式"></a>训练数据格式</h5><p>对于训练数据，首先需要多列，但不能不一致，既在一个文件里有的行是两列，有的行是三列（这种格式是错误的）；其次第一列代表的是需要标注的<strong>字或词</strong>，最后一列是输出位标记tag，\如果有额外的特征，例如词性什么的，可以加到中间列里，所以训练集或者测试集的文件最少要有两列。以分词举例，标签为BMES</p>
<p>东    N    B<br>南    N    M<br>大    N    M<br>学    N    E<br>欢    V    B<br>迎    V    E<br>您    N    S</p>
<p>其中第3列是标签，也是测试文件中需要预测的结果，有BMES 4种状态。第二列是词性，不是必须的。</p>
<h5 id="特征模板"><a href="#特征模板" class="headerlink" title="特征模板"></a>特征模板</h5><p>每一行模板生成一组状态特征函数，数量是$L<em>N$ 个，$L$是标签状态数是4个。$N$是此行模板在训练集上展开后的唯一样本数，在这个例子中，第一列的唯一字数是7个，所以有$L</em>N = 4*7=28$。例如：U01:%x[0,0]，生成如下28个函数：</p>
<figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line">func1 = <span class="keyword">if</span> (output = B and feature=U01:<span class="string">"东"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func2 = <span class="keyword">if</span> (output = M and feature=U01:<span class="string">"东"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func3 = <span class="keyword">if</span> (output = E and feature=U01:<span class="string">"东"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func4 = <span class="keyword">if</span> (output = S and feature=U01:<span class="string">"东"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func5 = <span class="keyword">if</span> (output = B and feature=U01:<span class="string">"南"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">...</span><br><span class="line">func25 = <span class="keyword">if</span> (output = B and feature=U01:<span class="string">"你"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func26 = <span class="keyword">if</span> (output = M and feature=U01:<span class="string">"你"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func27 = <span class="keyword">if</span> (output = E and feature=U01:<span class="string">"你"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func28 = <span class="keyword">if</span> (output = S and feature=U01:<span class="string">"你"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>这些函数经过训练后，其权值表示函数内文字对应该标签的概率（形象说法，概率和可大于1）。</p>
<h6 id="unigram"><a href="#unigram" class="headerlink" title="unigram"></a>unigram</h6><p>U00:%x[-2,0]<br>U01:%x[-1,0]<br>U02:%x[0,0]<br>U03:%x[1,0]<br>U04:%x[2,0]<br>U05:%x[-2,0]/%x[-1,0]/%x[0,0]<br>U06:%x[-1,0]/%x[0,0]/%x[1,0]<br>U07:%x[0,0]/%x[1,0]/%x[2,0]<br>U08:%x[-1,0]/%x[0,0]<br>U09:%x[0,0]/%x[1,0] </p>
<p>这是CRF++例子中给出的模板，一共有９个模板，先看第一个模板，表示当前词和前面的第二个词组成的特征，以‘小明今天穿了一件红色上衣’为例，符合CRF++处理格式的这句话应该变成如下形式：<br>小　Ｂ<br>明　Ｉ<br>今　Ｂ   &lt;— current word<br>天　Ｉ<br>穿　Ｓ<br>了　Ｓ<br>一　Ｂ<br>件　Ｉ<br>红　Ｂ<br>色　Ｉ<br>上　Ｂ<br>衣　Ｉ<br>假设我们有三个标记tag，Ｂ（表示一个词的开头那个字），Ｉ（表示一个词的结尾那个字），Ｓ（表示单个字的词），先看第一个模板Ｕ00:%x[-2,0],第一个模板产生的特征如下： 如果当前词是‘今’，那-2位置对应的字就是‘小’， 每个特征对应的字如下： </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>特征模板</th>
<th>意义</th>
<th>代表特征</th>
</tr>
</thead>
<tbody>
<tr>
<td>U00:%x[-2,0]</td>
<td>-2行，0列</td>
<td>小</td>
</tr>
<tr>
<td>U01:%x[-1,0]</td>
<td>-1行，0列</td>
<td>明</td>
</tr>
<tr>
<td>U02:%x[0,0]</td>
<td>0行，0列</td>
<td>今</td>
</tr>
<tr>
<td>U03:%x[1,0]</td>
<td>1行，0列</td>
<td>天</td>
</tr>
<tr>
<td>U04:%x[2,0]</td>
<td>2行，0列</td>
<td>穿</td>
</tr>
<tr>
<td>U05:%x[-2,0]/%x[-1,0]/%x[0,0]</td>
<td>-2行0列、-1行0列与0行0列的组合</td>
<td>小/明/今</td>
</tr>
<tr>
<td>U06:%x[-1,0]/%x[0,0]/%x[1,0]</td>
<td>-1行0列、0行0列与1行0列的组合</td>
<td>明/今/天</td>
</tr>
<tr>
<td>U07:%x[0,0]/%x[1,0]/%x[2,0]</td>
<td>0行0列、1行0列与2行0列的组合</td>
<td>今/天/穿</td>
</tr>
<tr>
<td>U08:%x[-1,0]/%x[0,0]</td>
<td>-1行0列与0行0列的组合</td>
<td>明/今</td>
</tr>
<tr>
<td>U09:%x[0,0]/%x[1,0]</td>
<td>0行0列与1行0列的组合</td>
<td>今/天</td>
</tr>
</tbody>
</table>
</div>
<p>根据第一个模板U00:%x[-2,0]能得到的转移特征函数如下：<br><code>func1=if(output=B and feature=&#39;U00:小&#39; )　return 1 else return 0</code><br>其中output=B 指的是当前词（字）的预测标记，也就是’今‘的预测标记，每个模板会把所有可能的标记输出都列一遍，然后通过训练确定每种标记的权重，合理的标记在训练样本中出现的次数多，对应的权重就高，不合理的标记在训练样本中出现的少，对应的权重就少，但是在利用模板生成转移特征函数是会把所有可能的特征函数都列出来，由模型通过训练决定每个特征的重要程度。<br><code>func2=if(output=M and feature=’U00:小’) return 1 else return 0</code><br><code>func3=if(output=E and feature=’U00:小’) return 1 else return 0</code><br><code>func4=if(output=S and feature=’U00:小) return 1 else return 0</code><br>得到4个特征函数之后当前这个字’今‘的特征函数利用第一个模板就得到全部了，然后扫描下一个字‘天‘，以’天‘字作为当前字预测这个字的标记tag,同样会得到4个特征函数：<br><code>func5=if(output=B and feature=’U00:明’) return 1 else return 0</code><br><code>func6=if(output=M and feature=’U00:明’) return 1 else return 0</code><br><code>func7=if(output=E and feature=’U00:明’) return 1 else return 0</code><br><code>func7=if(output=S and feature=’U00:明’) return 1 else return 0</code><br>特征函数中的feature指的是当前词的-2位置对应的词或对应的词的特征，因为在这里没有其他特征了，所以用字本身做特征，有的会有词性，那feature就会是’明‘这个字对应的词性而不是字本身了。 这个feature的作用就是确定模板所确定的当前词和临近词</p>
<h6 id="bigram"><a href="#bigram" class="headerlink" title="bigram"></a>bigram</h6><p>与Unigram不同的是，Bigram类型模板生成的函数会多一个参数：上个节点的标签 $y_{i-1}$。生成函B01:%x[0,0] (当前词为“东南大学欢迎您” 的“您”)：<code>func1 = if (prev_output = E and output = S and feature=B01:&quot;您&quot;) return 1 else return 0</code></p>
<p>这样，每行模板则会生成 $L<em>L</em>N$ 个特征函数。经过训练后，这些函数的权值反映了上一个节点的标签对当前节点的影响。(L表示隐状态的个数，如BMES为四个隐状态，N表示训练语料的大小。)</p>
<p>trigam理论上也是可行的，考虑前$y_{i-2},y_{i-1}$对$y_i$的影响，不过crf++并没有实现，这实际和二阶HMM模型类似的，有论文详细描述过其求解，比较复杂，但是其效果提升并未多大，所以一般不使用。</p>
<h6 id="产生特征函数"><a href="#产生特征函数" class="headerlink" title="产生特征函数"></a>产生特征函数</h6><p>训练阶段将遍历数据逐一产生特征函数，写成伪码过程为：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">m：样本个数</span><br><span class="line">k：模板个数</span><br><span class="line">templates：一组特征模板</span><br><span class="line">genFeatureFunc：根据模板和序列x，y和位置i产生特征函数</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">    <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; k; j++)&#123;</span><br><span class="line">        genFeatureFunc(templates[j],x,y,i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="特征函数"><a href="#特征函数" class="headerlink" title="特征函数"></a>特征函数</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span> B</span><br><span class="line"><span class="number">16</span> U00:-</span><br><span class="line"><span class="number">20</span> U00:<span class="number">0</span></span><br><span class="line"><span class="number">24</span> U00:<span class="number">1</span></span><br><span class="line"><span class="number">28</span> U00:<span class="number">2</span></span><br><span class="line"><span class="number">32</span> U00:<span class="number">3</span></span><br><span class="line"><span class="number">36</span> U00:<span class="number">4</span></span><br><span class="line"><span class="number">40</span> U00:<span class="number">5</span></span><br><span class="line"><span class="number">44</span> U00:<span class="number">6</span></span><br><span class="line"><span class="number">48</span> U00:<span class="number">7</span></span><br><span class="line"><span class="number">52</span> U00:<span class="number">8</span></span><br><span class="line"><span class="number">56</span> U00:<span class="number">9</span></span><br><span class="line"><span class="number">60</span> U00:_B<span class="number">-1</span></span><br><span class="line"><span class="number">64</span> U00:_B<span class="number">-2</span></span><br><span class="line">……</span><br><span class="line"><span class="number">17404</span> U01:厨</span><br><span class="line"><span class="number">17408</span> U01:去</span><br><span class="line"><span class="number">17412</span> U01:县</span><br><span class="line"><span class="number">17416</span> U01:参</span><br><span class="line"><span class="number">17420</span> U01:又</span><br><span class="line"><span class="number">17424</span> U01:叉</span><br><span class="line"><span class="number">17428</span> U01:及</span><br><span class="line"><span class="number">17432</span> U01:友</span><br><span class="line"><span class="number">17436</span> U01:双</span><br><span class="line"><span class="number">17440</span> U01:反</span><br><span class="line"><span class="number">17444</span> U01:发</span><br><span class="line"><span class="number">17448</span> U01:叔</span><br><span class="line"><span class="number">17452</span> U01:取</span><br><span class="line"><span class="number">17456</span> U01:受</span><br><span class="line">……</span><br><span class="line"><span class="number">77800</span> U05:_B<span class="number">-1</span>/一/个</span><br><span class="line"><span class="number">107540</span> U05:一/方/面</span><br><span class="line"><span class="number">107544</span> U05:一/无/所</span><br><span class="line"><span class="number">107548</span> U05:一/日/三</span><br><span class="line"><span class="number">107552</span> U05:一/日/为</span><br><span class="line"><span class="number">107556</span> U05:一/日/之</span><br><span class="line">……</span><br><span class="line"><span class="number">566536</span> U06:万/吨/_B+<span class="number">1</span></span><br><span class="line">……</span><br><span class="line"><span class="number">2159864</span> U09:ｖ/ｅ</span><br></pre></td></tr></table></figure>
<p>按照[id] [参数o]的格式排列，注意到id不是连续的，而是隔了四个（状态s隐藏起来了），这表示这四个标签（BMES）和公共的参数o组合成了四个特征函数。特别的，0-15为BEMS转移到BEMS的转移函数，也就是f(s’, s, o=null)。_B-1表示句子第一个单词前面的一个单词，_B+1表示末尾后面的一个单词.</p>
<h6 id="特征函数权值"><a href="#特征函数权值" class="headerlink" title="特征函数权值"></a>特征函数权值</h6><p>后面的小数依id顺序对应每个特征函数的权值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">9.0491453814148901</span></span><br><span class="line"><span class="number">7.0388286231971700</span></span><br><span class="line"><span class="number">-7.2545558164093009</span></span><br><span class="line"><span class="number">5.2799470769112835</span></span><br><span class="line"><span class="number">-8.5333633546653758</span></span><br><span class="line"><span class="number">-5.3549190735606933</span></span><br><span class="line"><span class="number">5.2575182675282477</span></span><br><span class="line"><span class="number">-5.4259109736696054</span></span><br></pre></td></tr></table></figure>
<p>比如说有一个句子“商品和服务”，对于每个字都按照上述模板生成一系列U特征函数的参数代入，得到一些类似010101的函数返回值，乘上这些函数的权值求和，就得到了各个标签的分数，由大到小代表输出这些标签的可能性。至于B特征函数（这里特指简单的f(s’, s)），在Viterbi后向解码的时候，前一个标签确定了后就可以代入当前的B特征函数，计算出每个输出标签的分数，再次求和排序即可。</p>
<h5 id="训练及预测"><a href="#训练及预测" class="headerlink" title="训练及预测"></a>训练及预测</h5><p>CRF++的训练命令一般格式如下：<br><code>crf_learn  -f 3 -c 4.0 template train.data model -t</code><br>其中，template为模板文件，train.data为训练语料，-t表示可以得到一个model文件和一个model.txt文件，其他可选参数说明如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">-f, –freq=INT使用属性的出现次数不少于INT(默认为<span class="number">1</span>)</span><br><span class="line">-m, –maxiter=INT设置INT为LBFGS的最大迭代次数 (默认<span class="number">10</span>k)</span><br><span class="line">-c, –cost=FLOAT    设置FLOAT为代价参数，过大会过度拟合 (默认<span class="number">1.0</span>)</span><br><span class="line">-e, –eta=FLOAT设置终止标准FLOAT(默认<span class="number">0.0001</span>)</span><br><span class="line">-C, –convert将文本模式转为二进制模式</span><br><span class="line">-t, –textmodel为调试建立文本模型文件</span><br><span class="line">-a, –algorithm=(CRF|MIRA)    选择训练算法，默认为CRF-L2</span><br><span class="line">-p, –thread=INT线程数(默认<span class="number">1</span>)，利用多个CPU减少训练时间</span><br><span class="line">-H, –shrinking-size=INT    设置INT为最适宜的跌代变量次数 (默认<span class="number">20</span>)</span><br><span class="line">-v, –version显示版本号并退出</span><br><span class="line">-h, –help显示帮助并退出</span><br></pre></td></tr></table></figure>
<p>在训练过程中，会输出一些信息，其意义如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iter：迭代次数。当迭代次数达到maxiter时，迭代终止</span><br><span class="line">terr：标记错误率</span><br><span class="line">serr：句子错误率</span><br><span class="line">obj：当前对象的值。当这个值收敛到一个确定值的时候，训练完成</span><br><span class="line">diff：与上一个对象值之间的相对差。当此值低于eta时，训练完成</span><br></pre></td></tr></table></figure>
<p>在训练完模型后，我们可以使用训练好的模型对新数据进行预测，预测命令格式如下：<br><code>crf_test -m model seg_word_predict.data &gt; predict.txt</code></p>
<p><code>-m model</code>表示使用我们刚刚训练好的model模型，预测的数据文件为seg_word_predict.data, <code>&gt; predict.txt</code>表示将预测后的数据写入到predict.txt中。</p>
<h6 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> CRFPP</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRFModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model=<span class="string">'model_name'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 类初始化</span></span><br><span class="line"><span class="string">        :param model: 模型名称</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_tagger</span><span class="params">(self, tag_data)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 添加语料</span></span><br><span class="line"><span class="string">        :param tag_data: 数据</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        word_str = tag_data.strip()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(self.model):</span><br><span class="line">            print(<span class="string">'模型不存在,请确认模型路径是否正确!'</span>)</span><br><span class="line">            exit()</span><br><span class="line">        tagger = CRFPP.Tagger(<span class="string">"-m &#123;&#125; -v 3 -n2"</span>.format(self.model))</span><br><span class="line">        tagger.clear()</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> word_str:</span><br><span class="line">            tagger.add(word)</span><br><span class="line">        tagger.parse()</span><br><span class="line">        <span class="keyword">return</span> tagger</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">text_mark</span><span class="params">(self, tag_data, begin=<span class="string">'B'</span>, middle=<span class="string">'I'</span>, end=<span class="string">'E'</span>, single=<span class="string">'S'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        文本标记</span></span><br><span class="line"><span class="string">        :param tag_data: 数据</span></span><br><span class="line"><span class="string">        :param begin: 开始标记</span></span><br><span class="line"><span class="string">        :param middle: 中间标记</span></span><br><span class="line"><span class="string">        :param end: 结束标记</span></span><br><span class="line"><span class="string">        :param single: 单字结束标记</span></span><br><span class="line"><span class="string">        :return result: 标记列表</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        tagger = self.add_tagger(tag_data)</span><br><span class="line">        size = tagger.size()</span><br><span class="line">        tag_text = <span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, size):</span><br><span class="line">            word, tag = tagger.x(i, <span class="number">0</span>), tagger.y2(i)</span><br><span class="line">            <span class="keyword">if</span> tag <span class="keyword">in</span> [begin, middle]:</span><br><span class="line">                tag_text += word</span><br><span class="line">            <span class="keyword">elif</span> tag <span class="keyword">in</span> [end, single]:</span><br><span class="line">                tag_text += word + <span class="string">"*&amp;*"</span></span><br><span class="line">        result = tag_text.split(<span class="string">'*&amp;*'</span>)</span><br><span class="line">        result.pop()</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crf_test</span><span class="params">(self, tag_data, separator=<span class="string">'/'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: crf测试</span></span><br><span class="line"><span class="string">        :param tag_data:</span></span><br><span class="line"><span class="string">        :param separator:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        result = self.text_mark(tag_data)</span><br><span class="line">        data = separator.join(result)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crf_learn</span><span class="params">(self, filename)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 训练模型</span></span><br><span class="line"><span class="string">        :param filename: 已标注数据源</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        crf_bash = <span class="string">"crf_learn -f 3 -c 4.0 api/template.txt &#123;&#125; &#123;&#125;"</span>.format(filename, self.model)</span><br><span class="line">        process = subprocess.Popen(crf_bash.split(), stdout=subprocess.PIPE)</span><br><span class="line">        output = process.communicate()[<span class="number">0</span>]</span><br><span class="line">        print(output.decode(encoding=<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/37163081" target="_blank" rel="noopener">简明条件随机场CRF介绍 | 附带纯Keras实现</a></p>
</li>
<li><p><a href="https://blog.csdn.net/qq_37667364/article/details/82919560" target="_blank" rel="noopener">CRF++/CRF/条件随机场的特征函数模板</a></p>
</li>
<li><p><a href="https://www.hankcs.com/nlp/the-crf-model-format-description.html" target="_blank" rel="noopener">CRF++模型格式说明</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>序列标注</tag>
        <tag>条件随机场</tag>
      </tags>
  </entry>
  <entry>
    <title>文本纠错</title>
    <url>/2020/04/24/%E6%96%87%E6%9C%AC%E7%BA%A0%E9%94%99/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><h4 id="常见错误类型"><a href="#常见错误类型" class="headerlink" title="常见错误类型"></a>常见错误类型</h4><ol>
<li>谐音字词，如 配副眼睛-配副眼镜</li>
<li>混淆音字词，如 流浪织女-牛郎织女</li>
<li>字词顺序颠倒，如 伍迪艾伦-艾伦伍迪</li>
<li>字词补全，如 爱有天意-假如爱有天意</li>
<li>形似字错误，如 高梁-高粱</li>
<li>中文拼音全拼，如 xingfu-幸福</li>
<li>中文拼音缩写，如 sz-深圳</li>
<li>语法错误，如 想象难以-难以想象</li>
</ol>
<p>针对不同业务场景，这些问题并不一定全部存在，比如输入法中需要处理前四种，搜索引擎需要处理所有类型，语音识别后文本纠错只需要处理前两种， 其中’形似字错误’主要针对五笔或者笔画手写输入等。</p>
<a id="more"></a>
<h4 id="纠错方法"><a href="#纠错方法" class="headerlink" title="纠错方法"></a>纠错方法</h4><p>纠错算法分为两个方向：基于规则、深度模型</p>
<h5 id="规则的解决思路"><a href="#规则的解决思路" class="headerlink" title="规则的解决思路"></a><strong>规则的解决思路</strong></h5><p>中文纠错分为两步走，第一步是错误检测，第二步是错误纠正；</p>
<p>错误检测部分先通过结巴中文分词器切词，由于句子中含有错别字，所以切词结果往往会有切分错误的情况，这样从字粒度和词粒度两方面检测错误， 整合这两种粒度的疑似错误结果，形成疑似错误位置候选集；</p>
<p>错误纠正部分，是遍历所有的疑似错误位置，并使用音似、形似词典替换错误位置的词，然后通过语言模型计算句子困惑度，对所有候选集结果比较并排序，得到最优纠正词。</p>
<h5 id="深度模型的解决思路"><a href="#深度模型的解决思路" class="headerlink" title="深度模型的解决思路"></a><strong>深度模型的解决思路</strong></h5><p>端到端的深度模型可以避免人工提取特征，减少人工工作量，RNN序列模型对文本任务拟合能力强，rnn_attention在英文文本纠错比赛中取得第一名成绩，证明应用效果不错；</p>
<p>CRF会计算全局最优输出节点的条件概率，对句子中特定错误类型的检测，会根据整句话判定该错误，阿里参赛2016中文语法纠错任务并取得第一名，证明应用效果不错；</p>
<p>seq2seq模型是使用encoder-decoder结构解决序列转换问题，目前在序列转换任务中（如机器翻译、对话生成、文本摘要、图像描述）使用最广泛、效果最好的模型之一。</p>
<h3 id="编辑距离"><a href="#编辑距离" class="headerlink" title="编辑距离"></a>编辑距离</h3><h4 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h4><p>编辑距离的经典应用就是用于拼写检错，如果用户输入的词语不在词典中，自动从词典中找出编辑距离小于某个数n的单词，让用户选择正确的那一个，n通常取到2或者3。</p>
<p>字符串A到B的<strong>编辑距离</strong>是指，只用插入、删除和替换三种操作，最少需要多少步可以把A变成B。例如，从FAME到GATE需要两步（两次替换），从GAME到ACM则需要三步（删除G和E再添加C）。</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minDistance_recursive</span><span class="params">(self, word1, word2)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        递归解法</span></span><br><span class="line"><span class="string">        :type word1: str</span></span><br><span class="line"><span class="string">        :type word2: str</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> word1 <span class="keyword">or</span> <span class="keyword">not</span> word2:</span><br><span class="line">            <span class="comment"># 递归出口：其中一个字符串为空，返回非空字符串的长度</span></span><br><span class="line">            <span class="keyword">return</span> (len(word1) <span class="keyword">or</span> len(word2))</span><br><span class="line">        <span class="keyword">if</span> word1[<span class="number">-1</span>] == word2[<span class="number">-1</span>]:</span><br><span class="line">            <span class="comment"># 如果两个字符串最后一个字符相同</span></span><br><span class="line">            <span class="keyword">return</span> self.minDistance(word1[:<span class="number">-1</span>], word2[:<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> + min(self.minDistance(word1[:<span class="number">-1</span>], word2), self.minDistance(word1, word2[:<span class="number">-1</span>]),</span><br><span class="line">                       self.minDistance(word1[:<span class="number">-1</span>], word2[:<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minDistance_dp</span><span class="params">(self,word1,word2)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        动态规划解法</span></span><br><span class="line"><span class="string">        :param word1:</span></span><br><span class="line"><span class="string">        :param word2:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> word1 <span class="keyword">or</span> <span class="keyword">not</span> word2:</span><br><span class="line">            <span class="keyword">return</span> (len(word1) <span class="keyword">or</span> len(word2))</span><br><span class="line"></span><br><span class="line">        dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(word2)+<span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(word1)+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dp 初始化</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word1)+<span class="number">1</span>):</span><br><span class="line">            dp[i][<span class="number">0</span>] = i</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word2)+<span class="number">1</span>):</span><br><span class="line">            dp[<span class="number">0</span>][i] = i</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(word1)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,len(word2)+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> word1[i<span class="number">-1</span>] == word2[j<span class="number">-1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="number">1</span> + min(dp[i<span class="number">-1</span>][j],dp[i][j<span class="number">-1</span>],dp[i<span class="number">-1</span>][j<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[len(word1)][len(word2)]</span><br></pre></td></tr></table></figure>
<p><a href="https://leetcode-cn.com/problems/edit-distance/solution/bian-ji-ju-chi-by-leetcode-solution/" target="_blank" rel="noopener">leetcode题解</a></p>
<h3 id="拼写自动纠错系统"><a href="#拼写自动纠错系统" class="headerlink" title="拼写自动纠错系统"></a>拼写自动纠错系统</h3><p>给定一待纠错词w,需要从一系列候选词中选出一最可能的词c。也就是：$argmax(p(c|w))$, c in 候选词表。根据贝叶斯原理，$p(c|w) = p(w|c) <em> p(c) / p(w)$. 又有对任意可能的c,p(w)一样，故也就是求使$argmax(p(w|c) </em> p(c))$成立的c.</p>
<h4 id="Candidate-Model"><a href="#Candidate-Model" class="headerlink" title="Candidate Model:"></a>Candidate Model:</h4><p>利用编辑距离小于1或者编辑距离小于2，来构造初步候选集，然后利用语料库，过滤初步候选集，得到符合规范的候选词。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits1</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="string">"All edits that are one edit away from `word`."</span></span><br><span class="line">    letters    = <span class="string">'abcdefghijklmnopqrstuvwxyz'</span></span><br><span class="line">    splits     = [(word[:i], word[i:])    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word) + <span class="number">1</span>)]</span><br><span class="line">    deletes    = [L + R[<span class="number">1</span>:]               <span class="keyword">for</span> L, R <span class="keyword">in</span> splits <span class="keyword">if</span> R]</span><br><span class="line">    transposes = [L + R[<span class="number">1</span>] + R[<span class="number">0</span>] + R[<span class="number">2</span>:] <span class="keyword">for</span> L, R <span class="keyword">in</span> splits <span class="keyword">if</span> len(R)&gt;<span class="number">1</span>]</span><br><span class="line">    replaces   = [L + c + R[<span class="number">1</span>:]           <span class="keyword">for</span> L, R <span class="keyword">in</span> splits <span class="keyword">if</span> R <span class="keyword">for</span> c <span class="keyword">in</span> letters]</span><br><span class="line">    inserts    = [L + c + R               <span class="keyword">for</span> L, R <span class="keyword">in</span> splits <span class="keyword">for</span> c <span class="keyword">in</span> letters]</span><br><span class="line">    <span class="keyword">return</span> set(deletes + transposes + replaces + inserts)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits2</span><span class="params">(word)</span>:</span> </span><br><span class="line">  <span class="string">"编辑距离小于2的候选词"</span></span><br><span class="line">  <span class="keyword">return</span> (e2 <span class="keyword">for</span> e1 <span class="keyword">in</span> edits1(word) <span class="keyword">for</span> e2 <span class="keyword">in</span> edits1(e1))</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">known</span><span class="params">(words)</span>:</span> </span><br><span class="line">  <span class="string">"在语料中过滤出复合拼写规范的（正确）的词"</span></span><br><span class="line">  <span class="keyword">return</span> set(w <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> w <span class="keyword">in</span> corpus)</span><br></pre></td></tr></table></figure>
<h4 id="Language-Model"><a href="#Language-Model" class="headerlink" title="Language Model"></a>Language Model</h4><p>候选集合中的词再语料库中的出现概率。比如：the 在英文文本中出现的概率率为7%，则有：p(the)=0.07</p>
<p>,可以通过统计语料库中$p(c)$ = 词c出现的频次/所有词的出现次数之和。（unigram，bigram）</p>
<h4 id="Error-Model"><a href="#Error-Model" class="headerlink" title="Error Model"></a>Error Model</h4><p>当想要输入the却意外拼写成teh的概率:p(teh|the)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">candidates</span><span class="params">(word)</span>:</span> </span><br><span class="line">    <span class="keyword">return</span> known([word]) <span class="keyword">or</span> known(edits1(word)) <span class="keyword">or</span> known(edits2(word)) <span class="keyword">or</span> [word]</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">correction</span><span class="params">(word)</span>:</span> </span><br><span class="line">    <span class="keyword">return</span> max(candidates(word), key=P)</span><br></pre></td></tr></table></figure>
<h4 id="Selection-Mechanism"><a href="#Selection-Mechanism" class="headerlink" title="Selection Mechanism"></a>Selection Mechanism</h4><p>从正确的拼写候选集中选择最有可能的正确结果，argmax</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ol>
<li>利用编辑距离构造正确结果候选集</li>
<li>利用language model 和error model 来计算概率。</li>
<li>从正确结果候选集中选择概率最大的正确拼写。</li>
</ol>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><p><a href="https://cloud.tencent.com/developer/article/1435917" target="_blank" rel="noopener">中文文本纠错算法走到多远了？</a></p>
</li>
<li><p><a href="https://github.com/shibing624/pycorrector" target="_blank" rel="noopener">pycorrect</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>自然语言处理基础</category>
      </categories>
  </entry>
  <entry>
    <title>查找算法</title>
    <url>/2018/02/10/%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="简单搜索查找"><a href="#简单搜索查找" class="headerlink" title="简单搜索查找"></a>简单搜索查找</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">brute_force_search</span><span class="params">(nums,key)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">        <span class="keyword">if</span> num == key:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h3><p>前提：数组有序</p>
<h4 id="迭代版本"><a href="#迭代版本" class="headerlink" title="迭代版本"></a>迭代版本</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span><span class="params">(nums, key)</span>:</span></span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = len(nums) - <span class="number">1</span></span><br><span class="line">    time = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        time += <span class="number">1</span></span><br><span class="line">        mid = int((low + high) / <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> key &lt; nums[mid]:</span><br><span class="line">            high = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> key &gt; nums[mid]:</span><br><span class="line">            low = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 打印折半的次数</span></span><br><span class="line">            <span class="keyword">print</span> <span class="string">"success! times: %s"</span> % time</span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"fail! times: %s"</span> % time</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h4 id="递归版本"><a href="#递归版本" class="headerlink" title="递归版本"></a>递归版本</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search_recursive</span><span class="params">(nums, item)</span>:</span></span><br><span class="line">    mid = len(nums) // <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> nums[mid] == item:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> mid == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># mid等于0就是找到最后一个元素了。</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> item &gt; nums[mid]:  <span class="comment"># 找后半部分</span></span><br><span class="line">            <span class="comment"># print(lst[mid:])</span></span><br><span class="line">            <span class="keyword">return</span> binary_search_recursive(nums[mid:], item)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> binary_search_recursive(nums[:mid], item)  <span class="comment"># 找前半部分</span></span><br></pre></td></tr></table></figure>
<h3 id="Hash-查找"><a href="#Hash-查找" class="headerlink" title="Hash 查找"></a>Hash 查找</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashTable</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size)</span>:</span></span><br><span class="line">        self.elem = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(size)]  <span class="comment"># 使用list数据结构作为哈希表元素保存方法</span></span><br><span class="line">        self.count = size  <span class="comment"># 最大表长</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hash</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> key % self.count  <span class="comment"># 散列函数采用除留余数法</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert_hash</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="string">"""插入关键字到哈希表内"""</span></span><br><span class="line">        address = self.hash(key)  <span class="comment"># 求散列地址</span></span><br><span class="line">        <span class="keyword">while</span> self.elem[address]:  <span class="comment"># 当前位置已经有数据了，发生冲突。</span></span><br><span class="line">            address = (address+<span class="number">1</span>) % self.count  <span class="comment"># 线性探测下一地址是否可用</span></span><br><span class="line">        self.elem[address] = key  <span class="comment"># 没有冲突则直接保存。</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search_hash</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="string">"""查找关键字，返回布尔值"""</span></span><br><span class="line">        star = address = self.hash(key)</span><br><span class="line">        <span class="keyword">while</span> self.elem[address] != key:</span><br><span class="line">            address = (address + <span class="number">1</span>) % self.count</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.elem[address] <span class="keyword">or</span> address == star:  <span class="comment"># 说明没找到或者循环到了开始的位置</span></span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">      </span><br><span class="line">     </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># hash查找</span></span><br><span class="line">    list_a = [<span class="number">12</span>, <span class="number">67</span>, <span class="number">56</span>, <span class="number">16</span>, <span class="number">25</span>, <span class="number">37</span>, <span class="number">22</span>, <span class="number">29</span>, <span class="number">15</span>, <span class="number">47</span>, <span class="number">48</span>, <span class="number">34</span>]</span><br><span class="line">    hash_table = HashTable(len(list_a))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> list_a:</span><br><span class="line">        hash_table.insert_hash(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> hash_table.elem:</span><br><span class="line">        <span class="keyword">if</span> i:</span><br><span class="line">            print(i, hash_table.elem.index(i))</span><br><span class="line"></span><br><span class="line">    print(hash_table.search_hash(<span class="number">15</span>))</span><br><span class="line">    print(hash_table.search_hash(<span class="number">33</span>))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>查找算法</tag>
      </tags>
  </entry>
  <entry>
    <title>激活函数</title>
    <url>/2018/07/31/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>sigmoid函数也叫Logistic函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。</p>
<p><img src="/2018/07/31/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/sigmoid.png" alt="avatat"></p>
<a id="more"></a>
<p>Sigmoid函数公式：</p>
<script type="math/tex; mode=display">
S(x)=\frac{1}{1+e^{-x}}</script><p>其对x的导数可以用自身表示:</p>
<script type="math/tex; mode=display">
S^{\prime}(x)=\frac{e^{-x}}{\left(1+e^{-x}\right)^{2}}=S(x)(1-S(x))</script><h4 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h4><p>优化神经网络的方法是Back Propagation，即导数的后向传递：先计算输出层对应的loss，然后将loss以导数的形式不断向上一层网络传递，修正相应的参数，达到降低loss的目的。 Sigmoid函数在深度网络中常常会导致导数逐渐变为0，使得参数无法被更新，神经网络无法被优化。原因在于两点：</p>
<ol>
<li>在上图中容易看出，当$\sigma(x)$中$x$较大或较小时，导数接近0，而后向传递的数学依据是微积分求导的链式法则，当前层的导数需要之前各层导数的乘积，几个小数的相乘，结果会很接近0 .</li>
<li>Sigmoid导数的最大值是0.25，这意味着导数在每一层至少会被压缩为原来的1/4，通过两层后被变为1/16，…，通过10层后为$\frac{1}{4^10}$。请注意这里是“至少”，导数达到最大值这种情况还是很少见的。</li>
</ol>
<p>优点：平滑、易于求导。<br>缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。<strong>幂运算相对耗时</strong></p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1</span>+np.exp(-x))</span><br></pre></td></tr></table></figure>
<h3 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h3><p><img src="/2018/07/31/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/tanh.png" alt="avatar"></p>
<p>tanh公式</p>
<script type="math/tex; mode=display">
\tanh x=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}</script><h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p><img src="/2018/07/31/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/relu.png" alt="avatar"></p>
<script type="math/tex; mode=display">
ReLU = max(0,x)</script><p>优点：</p>
<ul>
<li>解决了gradient vanishing问题 (在正区间)</li>
<li>计算速度非常快，只需要判断输入是否大于0</li>
<li>收敛速度远快于sigmoid和tanh</li>
</ul>
<p>存在的问题：</p>
<ol>
<li>ReLU的输出不是zero-centered</li>
<li>Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: <ol>
<li>非常不幸的参数初始化，这种情况比较少见 </li>
<li>learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。</li>
</ol>
</li>
</ol>
<h3 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h3><p><img src="/2018/07/31/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/leakrelu.png" alt="avatar"></p>
<script type="math/tex; mode=display">
f(x)=max(\alpha x,x)</script><p>人们为了解决Dead ReLU Problem，提出了将ReLU的前半段设为$0.01x$而非0。另外一种直观的想法是基于参数的方法，即Parametric ReLU:$f(x)=max(\alpha x,x)$，其中$\alpha$可由back propagation学出来。理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。</p>
<h3 id="ELU-Exponential-Linear-Units"><a href="#ELU-Exponential-Linear-Units" class="headerlink" title="ELU (Exponential Linear Units)"></a><strong>ELU (Exponential Linear Units)</strong></h3><p><img src="/2018/07/31/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/ELU.png" alt="avata"></p>
<script type="math/tex; mode=display">
f(x)=\left\{\begin{array}{ll}
x, & \text { if } x>0 \\
\alpha\left(e^{x}-1\right), & \text { otherwise }
\end{array}\right.</script><p>ELU也是为解决ReLU存在的问题而提出，显然，ELU有ReLU的基本所有优点，以及：</p>
<ul>
<li>不会有Dead ReLU问题</li>
<li>输出的均值接近0，zero-centered</li>
</ul>
<p>它的一个小问题在于计算量稍大。类似于Leaky ReLU，理论上虽然好于ReLU，但在实际使用中目前并没有好的证据ELU总是优于ReLU。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a href="https://baike.baidu.com/item/Sigmoid函数/7981407?fr=aladdin" target="_blank" rel="noopener">Sigmoid函数</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247484448&amp;idx=1&amp;sn=80479bf682d7ade4184fde809ea0cc3e&amp;chksm=970c2cf6a07ba5e0e046e4d5b8169e8fb58c9e858878840b571d43060e30986152a534830bc6&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">神经网络激活函数=生物转换器？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/25110450" target="_blank" rel="noopener">聊一聊深度学习的activation function</a></li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title>背包问题</title>
    <url>/2018/03/01/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>有N件物品和一个容量为<code>v</code>的背包，放入第i件物品耗费空间<code>c[i]</code>得到价值为<code>W[i]</code>,求哪些物品装入背包可使得总价值最大？</p>
<h4 id="动态规划求解："><a href="#动态规划求解：" class="headerlink" title="动态规划求解："></a>动态规划求解：</h4><p>状态：问题所求最大价值，设状态即为背包中物品的价值</p>
<p>状态转移方程：</p>
<p><code>dp[i][j]</code>表示前i件物品放入一个容量为V的背包获得的最大价值状态转移方程为</p>
<script type="math/tex; mode=display">
\mathrm{dp}[\mathrm{i}][\mathrm{v}]=\max \left\{\begin{array}{l}
dp[i-1][v](\text {not select}) \\
dp[i-1][v-C[i]]+W[i](\text {select})
\end{array}\right.</script><p>其中<code>dp[i-1][v-C[i]]</code>表示前<code>i-1</code>件物品放入容量为<code>v-C[i]</code>的背包中。</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># F[0, 0...V] = 0</span></span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>):</span><br><span class="line">    dp[<span class="number">0</span>][v] = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># 容量装不下当前物品时，保留原值</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(<span class="number">0</span>, C[i]):</span><br><span class="line">        dp[i][v] = dp[i<span class="number">-1</span>][v]</span><br><span class="line">    <span class="comment"># 状态转移</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(C[i], V+<span class="number">1</span>):</span><br><span class="line">        dp[i][v] = max(dp[i<span class="number">-1</span>][v], dp[i<span class="number">-1</span>][v-C[i]] + W[i])</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><ol>
<li><p>时间复杂度：<code>O(VN)</code></p>
</li>
<li><p>空间复杂度：<code>O(VN)</code>可优化成<code>O(N)</code></p>
<p><code>dp[i][v]</code>由<code>dp[i-1][v-C[i]]</code>和<code>dp[i-1][v]</code>两个子问题递推得到，所以第<code>i</code>次迭代时，<code>v</code>按照<code>V-&gt;0</code>的逆序计算<code>dp[v]</code>可以保证<code>dp[v]=max(dp[v],dp[v-C[i]]+W[i])</code>，这样在赋值之前，访问到的均为<code>i-1</code>次迭代保留的值。</p>
<p>空间为<code>c[i]</code>的物品不会受到状态<code>dp[0...C[i]]</code>的影响，即不影响装不下该物品的背包能装入的最大价值，所以<code>v</code>中<code>V</code>递减到<code>C[i]</code>就可以了。</p>
</li>
<li><p>代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dp = [<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(V, C[i]<span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        dp[v] = max(dp[v], dp[v-C[i]] + W[i])</span><br></pre></td></tr></table></figure>
<p>对处理0-1背包的物品进行抽象：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_one_pack</span><span class="params">(F, c, w)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(V, c<span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        dp[v] = max(dp[v], dp[v-c] + w)</span><br><span class="line"></span><br><span class="line">dp = [<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    zero_one_pack(dp, C[i], W[i])</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="初始化细节"><a href="#初始化细节" class="headerlink" title="初始化细节"></a>初始化细节</h4><p>初始化dp的过程，实际上就是在没有任何物品可以装入背包时的合法状态（必要但不充分条件）</p>
<ol>
<li><p>恰好装满背包时，求最优解：<code>dp[0]=0,dp[1...V]=</code>$-\infty$</p>
<ul>
<li><p>容量为0时，什么也装不下，价值为0，所以<code>dp[0]=0</code></p>
</li>
<li><p>容量不为0的时候，状态未定义，状态设为$-\infty$</p>
</li>
</ul>
<p>按此进行初始化，迭代过程中，只有能够恰好装入的物品才会更新出合理值（正值），基于<code>dp[0]=0</code></p>
</li>
<li><p>没要求把背包装满：<code>dp[0...V]=0</code></p>
<p>任何容量都有一个合法的“什么都不装”，所以设<code>dp[0...V]=0</code></p>
</li>
</ol>
<h3 id="完全背包"><a href="#完全背包" class="headerlink" title="完全背包"></a>完全背包</h3><p><img src="/Users/lizhen/Library/Application Support/typora-user-images/image-20200321181733468.png" alt="image-20200321181733468"></p>
<h4 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a>题目</h4><p>有N种物品和1个容量为V的背包，每种物品可以有任意多个，放入第i种物品的耗费空间<code>C[i]</code>，价值为<code>W[i]</code>,求背包能够装入的最大价值？</p>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>每种物品可以放入0件，1件，…，<code>int(V/C[i])</code>件，令<code>dp[i][v]</code>表示前i种物品放入容量v的背包获取的最大价值，则<code>dp[i][v]=max(dp[i-1][v-k*C[i]]+k*W[i]),0&lt;=k*C[i]&lt;=v</code></p>
<h4 id="优化-1"><a href="#优化-1" class="headerlink" title="优化"></a>优化</h4><p>时间复杂度：<code>O(nv</code>$\sum \frac{V}{C[i]}$)</p>
<p>简单优化：一个简单有效的优化思路是：单位价值更便宜的应该直接跳过</p>
<p>若两件物品<code>i,j</code>有<code>C[i]&lt;=C[j] and W[i]&gt;=W[j]</code>;则<code>j</code>可以直接跳过，不必考虑，对于随机数据，这会大大减少物品数量，加快速度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sieve = &#123;C[i]: <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>)&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># 跳过装不下的</span></span><br><span class="line">    <span class="keyword">if</span> C[i] &gt; V:</span><br><span class="line">        sieve[C[i]] = <span class="number">-1</span></span><br><span class="line">    <span class="comment"># 跳过 C[i] == C[j] and W[i] &gt; W[j]的j物品</span></span><br><span class="line">    <span class="keyword">elif</span> sieve[C[i]] &lt; W[i]:</span><br><span class="line">        sieve[C[i]] = W[i]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">if</span> sieve[C[i]] &lt; <span class="number">0</span> <span class="keyword">or</span> W[i] &lt; sieve[C[i]]:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(C[i], V+<span class="number">1</span>):</span><br><span class="line">        max_k = V // C[i]</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, max_k+<span class="number">1</span>):</span><br><span class="line">            dp[i][v] = max(dp[i<span class="number">-1</span>][v], dp[i<span class="number">-1</span>][v-k*C[i]] + k*W[i])</span><br></pre></td></tr></table></figure>
<p>最终优化方案：</p>
<p>按照正序容量更新当前总价值，<code>dp[v-C[i]]</code>中可能是<code>i-1</code>次迭代中保留的值，也可能是本次迭代中产生了变化的值。（如果加选第i件物品）</p>
<p>时间复杂度：<code>O(NV)</code></p>
<p>空间复杂度:<code>O(V)</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dp = [<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(C[i], V+<span class="number">1</span>):</span><br><span class="line">        dp[v] = max(dp[v], dp[v-C[i]] + W[i])</span><br></pre></td></tr></table></figure>
<p>对处理一种完全背包的物品过程进行抽象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">complete_pack</span><span class="params">(F, c, w)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(c, V+<span class="number">1</span>):</span><br><span class="line">        F[v] = max(F[v], F[v-c] + w)</span><br><span class="line"></span><br><span class="line">dp = [<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    complete_pack(dp, C[i], W[i])</span><br></pre></td></tr></table></figure>
<h4 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h4><p>0-1 背包：倒序容量，当前新物品之多影响一次当前总价值</p>
<p>完全背包：正序容量，随着容量增加，当前新物品可以多次影响当前总价值</p>
<h3 id="多重背包"><a href="#多重背包" class="headerlink" title="多重背包"></a>多重背包</h3><p>有<code>N</code>种物品和1个容量为<code>V</code>的背包，第i种物品有<code>M[i]</code>件，每件耗费空间<code>C[i]</code>,价值<code>W[i]</code>,求能装入背包的最大价值。</p>
<h4 id="思路1"><a href="#思路1" class="headerlink" title="思路1"></a>思路1</h4><p>和完全背包类似，对i种物品有<code>M[i]+1</code>种策略，取<code>0</code>件，取<code>1</code>件，…，取<code>M[i]</code>,令<code>dp[i][v]</code>表示前i中物品放入容量v中获得的最大价值，则<code>dp[i][v]=max(dp[i-1][v-k*C[i]]+k*W[i]),0&lt;=k&lt;=M[i]</code></p>
<p>时间复杂度：`O(V<script type="math/tex">\sum M_i</script></p>
<h4 id="思路2"><a href="#思路2" class="headerlink" title="思路2"></a>思路2</h4><p>转化为0-1背包问题，把第i种物品换成<code>M[i]</code>件<code>0-1</code>背包中的物品，得到物品数为$\sum M_i$的<code>0-1</code>背包问题，时间复杂度仍为$O(V\sum M_i)$。使用二进制的思想，可以把时间复杂度下降为$O(V\sum{log{M_i}})$,即按照如下的方式进行拆分：</p>
<p>$2^0,2^1,2^2,…,2^{k-1},M_i-2^k+1$</p>
<p>这样拼成的新物品总量等于原来物品的数量，第i件物品拆分成了$O(log{m_i})$</p>
<h5 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiple_pack</span><span class="params">(F, c, w, m)</span>:</span></span><br><span class="line">    <span class="comment"># 如果物品足够多，就变成了完全背包</span></span><br><span class="line">    <span class="keyword">if</span> c * m &gt;= V:</span><br><span class="line">        complete_pack(F, c, w)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 多重背包按照2次幂进行划分成0-1背包</span></span><br><span class="line">    k = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> k &lt; m:</span><br><span class="line">        zero_one_pack(F, k*c, k*w)</span><br><span class="line">        m = m - k</span><br><span class="line">        k = <span class="number">2</span> * k</span><br><span class="line">    <span class="comment"># 把剩余的物品当作1件物品</span></span><br><span class="line">    zero_one_pack(F, c * m, w * m)</span><br><span class="line"></span><br><span class="line">dp = [<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    multiple_pack(dp, C[i], W[i], M[i])</span><br></pre></td></tr></table></figure>
<h3 id="二维费用背包"><a href="#二维费用背包" class="headerlink" title="二维费用背包"></a>二维费用背包</h3><p>对于每件物品有两种不同的cost，每种cost有一个最大容纳值（背包容量），怎样选择物品使得背包容纳最大价值？第i种物品两种费用分别为<code>C[i]</code>和<code>D[i]</code>，两种cost最大容纳值为V,U，物品的价值为<code>W[i]</code>(题目场景样例：打矩形中放小矩形，求最大容量)</p>
<h4 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h4><p>费用加1维，状态加1维；令<code>dp[i][v][u]</code>表示前i件物品付出两种费用分别为v，u时可获得的最大价值。则<code>dp[i][v][u]=max(dp[i-1][v][u],dp[i-1][v-C[i]][u-D[i]]+W[i])</code></p>
<h4 id="优化-2"><a href="#优化-2" class="headerlink" title="优化"></a>优化</h4><p>类似于一维背包空间优化方法，二维背包与之类似：</p>
<ol>
<li>二维0-1背包：v和u采用逆序循环</li>
<li>二维完全背包：v和u采用顺序循环</li>
<li>二维多重背包：拆分物品</li>
</ol>
<h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_dim_zero_one_pack</span><span class="params">(F, c, d, w)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(V, c<span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> range(U, d<span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            F[v][u] = max(F[v][u], F[v-c][u-d] + w)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_dim_complete_pack</span><span class="params">(F, c, d, w)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(c, V+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> range(d, U+<span class="number">1</span>):</span><br><span class="line">            F[v][u] = max(F[v][u], F[v-c][u-d] + w)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_dim_multiple_pack</span><span class="params">(F, c, d, w, m)</span>:</span></span><br><span class="line">    <span class="comment"># 两种cost数量都充足，可直接按照完全背包处理</span></span><br><span class="line">    <span class="keyword">if</span> c * m &gt;= V <span class="keyword">and</span> d * m &gt;= U:</span><br><span class="line">        two_dim_complete_pack(F, c, d, w)</span><br><span class="line">    <span class="comment"># 多重背包按照2次幂进行划分成0-1背包</span></span><br><span class="line">    k = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> k &lt; m:</span><br><span class="line">        two_dim_complete_pack(F, k * c, k * d, k * w)</span><br><span class="line">        m = m - k</span><br><span class="line">        k = <span class="number">2</span> * k</span><br><span class="line">    <span class="comment"># 把剩余的物品当作1件物品</span></span><br><span class="line">    two_dim_zero_one_pack(F, m * c, m * d, m * w)</span><br><span class="line"></span><br><span class="line">F = [[<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)] <span class="keyword">for</span> u <span class="keyword">in</span> range(U+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    two_dim_zero_one_pack(F, C[i], D[i], W[i])</span><br><span class="line">    <span class="comment"># two_dim_complete_pack(F, C[i], D[i], W[i])</span></span><br><span class="line">    <span class="comment"># two_dim_multiple_pack(F, C[i], D[i], W[i], M[i])</span></span><br></pre></td></tr></table></figure>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>二维费用题目的一种隐含表述方式：最多取U件</p>
<p>对于有这种限制的题目，可以将’件数’看作一种费用，每件物品件数费用为1，可以达到的最大件数费用非U，<code>dp[v][u]</code>表示cost为v，最多选择u件物品时所达到的最大价值（再根据物品的特性（0-1,完全，多重）采用不同的方式进行循环，得出答案）</p>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>背包问题</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习总结</title>
    <url>/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h3><h4 id="神经元-感知器"><a href="#神经元-感知器" class="headerlink" title="神经元(感知器)"></a>神经元(感知器)</h4><ol>
<li>感知器是一种人工<strong>神经元</strong>.它接受几个<strong>二进制输出</strong>并产生一个<strong>二进制输入</strong>.如果引入<strong>权重</strong>和<strong>阈值</strong>,那么感知器的参数可以表示为:$f(x)=sign(wx+b)$</li>
<li>感知器是<strong>单输出</strong>的,但这个单输出可以被用于<strong>多个</strong>其它感知器的输入.</li>
<li>感知器可以很容易地计算基本的<strong>逻辑功能</strong>,如<strong>与</strong>,<strong>或</strong>,<strong>与非</strong>.所以感知器网络可以计算任何逻辑功能</li>
<li>使感知器能够自动调整权重和偏置的<strong>学习算法</strong>是神经网络有别于传统逻辑门的关键.</li>
</ol>
<h4 id="S型神经元"><a href="#S型神经元" class="headerlink" title="S型神经元"></a>S型神经元</h4><ol>
<li>网络中单个感知器上权重或偏置的<strong>微小改动</strong>可能会引起<strong>输出翻转</strong>,从而导致其余网络的行为改变.所以<strong>逐步修改</strong>权重和偏置来让输出接近期望很困难,所以引入了<strong>S型神经元(逻辑神经元)</strong></li>
<li>S型神经元和感知器类似,但是权重和偏置的微小改动只引起输出的<strong>微小变化</strong>.S型神经元的输入可以是<strong>0和1中的任意值</strong>,输出是$σ(wx+b)$,其中$σ$被称为s型函数(逻辑函数).</li>
<li>σ函数是阶跃函数的<strong>平滑</strong>版本.这意味着权重和偏置的微小变化会产生一个微小的输出变化,$\Delta output \approx \sum_{j} \frac{\partial \text { output }}{\partial w_{j}} \Delta w_{j}+\frac{\partial \text { output }}{\partial b} \Delta b$,这意味着输出的变化是权重和偏置的变化的<strong>线性函数</strong>.</li>
</ol>
<a id="more"></a>
<h4 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h4><ol>
<li><p>神经网络中经常需要<strong>大量</strong>变量,因此采用<strong>梯度下降法</strong>来计算代价函数最小值:重复计算梯度,然后沿着<strong>相反</strong>的方向移动$v \rightarrow v^{\prime}=v-\eta \nabla C$</p>
</li>
<li><p>假如需要计算每个训练输入的梯度值,训练速度会相当缓慢.因此引入了<strong>随机梯度下降</strong>,只计算小批量数据(mini-batch)的梯度值来求平均值</p>
<script type="math/tex; mode=display">
\begin{aligned}
w_{k} \rightarrow w_{k}^{\prime} &=w_{k}-\frac{\eta}{m} \sum_{j} \frac{\partial C_{X_{j}}}{\partial w_{k}} \\
b_{l} \rightarrow b_{l}^{\prime} &=b_{l}-\frac{\eta}{m} \sum_{j} \frac{\partial C_{X_{j}}}{\partial b_{l}}
\end{aligned}</script></li>
<li><p>随机机梯度下降不断地选定小批量数据来进行训练,直到用完了所有的训练输入,就称为完成了一个<strong>迭代期(epoch)</strong>,然后会开始一个新的迭代期.</p>
</li>
</ol>
<h4 id="反向传播-BP-算法"><a href="#反向传播-BP-算法" class="headerlink" title="反向传播(BP)算法"></a>反向传播(BP)算法</h4><h5 id="神经网络中使用矩阵快速计算输出的方法"><a href="#神经网络中使用矩阵快速计算输出的方法" class="headerlink" title="神经网络中使用矩阵快速计算输出的方法"></a>神经网络中使用矩阵快速计算输出的方法</h5><ol>
<li>$w^l_{jk}$表示从第$(l-1)$层的第$k$个神经元到第$l$层的第$j$个神经元的链接上的<strong>权重</strong>,$b^l_j$表示在第l层第j个神经元的<strong>偏置</strong>.$a^l_j$表示第l层第j个神经元的<strong>激活值</strong>.由此得到了激活值之间的关系$a_{j}^{l}=\sigma\left(\sum_{k} w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\right)$</li>
<li>对每一层l定义<strong>权重矩阵</strong>(其中第j行第k列的元素是$w^l_{jk}$,<strong>偏置向量</strong>(每个元素是$b^l_j$)和<strong>激活向量</strong>(每个元素是$a^l_j$).所以上式可以改写为$a^{l}=\sigma\left(w^{l} a^{l-1}+b^{l}\right)$,其中σ是<strong>向量化函数</strong>(作用σ到向量中的每个元素).中间量$z^{l} \equiv w^{l} a^{l-1}+b^{l}$称为<strong>带权输</strong>入(每个元素是第l层第j个神经元的激活函数的带权输入<strong>)</strong>.</li>
</ol>
<h5 id="关于代价函数的两个假设"><a href="#关于代价函数的两个假设" class="headerlink" title="关于代价函数的两个假设"></a>关于代价函数的两个假设</h5><p><strong>反向传播</strong>的目标是计算代价函数关于w和b的<strong>偏导数</strong>.为了让反向传播可行,需要作出两个主要假设</p>
<ol>
<li>代价函数可以被写成一个在每个训练样本x上的代价函数的<strong>均值</strong>$C=\frac{1}{n} \sum_{x} C_{x}$</li>
<li>代价可以写成神经网络输出的函数,以二次代价函数为例$C=\frac{1}{2}\left|y-a^{L}\right|^{2}=\frac{1}{2} \sum_{j}\left(y_{j}-a_{j}^{L}\right)^{2}$</li>
</ol>
<h5 id="Hadamard乘积-s⊙t"><a href="#Hadamard乘积-s⊙t" class="headerlink" title="Hadamard乘积,s⊙t"></a>Hadamard乘积,s⊙t</h5><p>假设s和t是两个同样维度的向量,那么s⊙t表示<strong>按元素的乘积</strong>,称为<strong>Hadamard乘积</strong></p>
<h5 id="反向传播的四个基本方程"><a href="#反向传播的四个基本方程" class="headerlink" title="反向传播的四个基本方程"></a>反向传播的四个基本方程</h5><p>在第l层第j个神经元上的<strong>误差</strong>,被定义为$\delta_{j}^{l} \equiv \frac{\partial C}{\partial z_{j}^{l}}$</p>
<p><strong>四个基本方程</strong></p>
<ol>
<li><p>输出层误差的方程: $\delta_{j}^{L} = \frac{\partial C}{\partial a^L_j}\frac{\partial a^L_j}{\partial z^l_j} = \frac{\partial C}{\partial a_{j}^{L}} \sigma^{\prime}\left(z_{j}^{L}\right)$ </p>
<p> 右式第⼀个项 $\frac{\partial C}{\partial a^L_j}$ 表⽰代价随着 $j^{th}$输出激活值的变化⽽ 变化的速度。假如 $C$ 不太依赖⼀个特定的输出神经元 $j$，那么$\delta^L_j$就会很⼩，这也是我们想要的效果。右式第⼆项 $\sigma^\prime(Z^L_j)$刻画了在$z^L_j$ 处激活函数 $\sigma$ 变化的速度。</p>
</li>
<li><p>使用下一层的误差来表示当前层的误差:$\delta^{l}=\left(\left(w^{l+1}\right)^{T} \delta^{l+1}\right) \odot \sigma^{\prime}\left(z^{l}\right)$</p>
<p> 由递推公式</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial  C}{\partial  z^l_j} &=\delta^l_j \\
& = \sum_i{\frac{\partial  C}{\partial  z^{l+1}_i}\frac{\partial  z^{l+1}_i}{\partial  z^l_j}} \\ 
&=\sum_i{\delta ^{l+1}_i \frac{\partial  z^{l+1}_i}{\partial  a^l_j}\frac{\partial  a^l_j}{\partial  z^l_j}} \\ 
&=\sum_i{\delta^{l+1}_i w^{l+1}_{ji} \sigma \prime (z^l_j)} \\
&=(w^{l+1}_j)^T\delta^{l+1}\sigma \prime(z^l_j) \\
&=(w^{l+1})^T \sigma^{l+1}\bigodot \sigma \prime (z^l)
\end{aligned}</script></li>
<li><p>代价函数关于网络中任意偏置的改变率:$\frac{\partial C}{\partial b_{j}^{l}}=\delta_{j}^{l}$</p>
</li>
<li><p>代价函数关于任何一个权重的改变率:$\frac{\partial C}{\partial w_{j k}^{l}}=a_{k}^{l-1} \delta_{j}^{l}$（因为$z^l = w^la^{l-1}+b^l$）</p>
</li>
</ol>
<h5 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h5><p>反向传播方程给出了一种计算代价函数梯度的方法:</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/bp.png" alt="avatar"></p>
<ol>
<li><strong>输入x</strong>:为输入层设置对应的激活值$a1$</li>
<li><strong>前向传播</strong>:对从前往后对每层计算相应的$z=wa+b$和$a=σ(z)$</li>
<li><strong>输出层误差</strong>:根据BP1计算误差向量</li>
<li><strong>反向误差传播</strong>:对后往前根据BP2对每层计算误差向量</li>
<li><strong>输出</strong>:根据BP3和BP4计算代价函数的梯度.</li>
</ol>
<h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">    <span class="string">"""Return a tuple "(nabla_b, nabla_w)" representing the</span></span><br><span class="line"><span class="string">    gradient for the cost function C_x.</span></span><br><span class="line"><span class="string">    "nabla_b" and "nabla_w" are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">    to "self.biases" and "self.weights"."""</span></span><br><span class="line"></span><br><span class="line">    nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">    nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># feedforward</span></span><br><span class="line">    activation = x</span><br><span class="line">    activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">    zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">      z = np.dot(w, activation)+b</span><br><span class="line">      zs.append(z)</span><br><span class="line">      activation = sigmoid(z)</span><br><span class="line">      activations.append(activation)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward pass</span></span><br><span class="line">    delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * sigmoid_prime(zs[<span class="number">-1</span>])</span><br><span class="line">    nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">    nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line"><span class="comment"># differently to the notation in Chapter 2 of the book.</span></span><br><span class="line"><span class="comment"># Here,l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line"><span class="comment"># second-last layer, and so on.It's a renumbering of the</span></span><br><span class="line"><span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line"><span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">    	z = zs[-l]</span><br><span class="line">      sp = sigmoid_prime(z)</span><br><span class="line">    	delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">    	nabla_b[-l] = delta</span><br><span class="line">    	nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span><br><span class="line">    <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></span><br><span class="line">  <span class="string">"""Return the vector of partial derivatives \partial C_x /</span></span><br><span class="line"><span class="string">  \partial a for the output activations."""</span></span><br><span class="line">  	<span class="keyword">return</span> (output_activations-y)</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line"><span class="string">"""The sigmoid function."""</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line"><span class="string">"""Derivative of the sigmoid function."""</span></span><br><span class="line">  <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure>
<h4 id="交叉熵代价函数"><a href="#交叉熵代价函数" class="headerlink" title="交叉熵代价函数"></a>交叉熵代价函数</h4><p>人类通常在<strong>犯错比较明显</strong>的时候学习的<strong>速度最快</strong>.而神经元在这种<strong>饱和</strong>情况下学习很有<strong>难度</strong>(也就是偏导数很小),这是因为<strong>二次代价函数</strong>关于权重和偏置的<strong>偏导数</strong>是$\frac{\partial C}{\partial w}=(a-y) \sigma^{\prime}(z) x=a \sigma^{\prime}(z)$和$\frac{\partial C}{\partial b}=(a-y) \sigma^{\prime}(z)=a \sigma^{\prime}(z)$而σ函数的<strong>导数</strong>在接近0和1时都很小。</p>
<p>解决上述问题的方法是引入<strong>交叉熵代价函数</strong>:$C=-\frac{1}{n} \sum_{x}[y \ln a+(1-y) \ln (1-a)]$</p>
<ol>
<li>它是<strong>非负</strong>的,并且当实际输出接近目标值时它<strong>接近0</strong>,因此可以作为代价函数.</li>
<li>它关于权重的<strong>偏导数</strong>是$\frac{\partial C}{\partial w_{j}}=\frac{1}{n} \sum_{x} x_{j}(\sigma(z)-y)$,也就是误差越大,学习速度越快.</li>
<li>如果输出层是<strong>线性神经元</strong>,那么<strong>二次代价函数</strong>不再会导致学习速度下降的问题,可以选用.如果输出神经元是<strong>S型神经元</strong>,<strong>交叉熵</strong>一般都是更好的选择. </li>
<li>对于多分类交叉熵代价函数为：$C=-\sum_{j=1}^{T} y_{j} \log {p_j}$,</li>
</ol>
<h4 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h4><ol>
<li>最好的降低过拟合的方式就是<strong>增加训练样本量</strong>,但训练数据其实是很<strong>难得</strong>的资源.</li>
<li><strong>规范化</strong>也是一种缓解过拟合的技术.效果是让网络倾向于学习<strong>小一点的权重</strong>,它是寻找小的权重和最小化原始代价函数之间的折中,相对重要性由λ控制.<ol>
<li><strong>L2规范化(权重衰减)</strong>的想法是增加一个额外的<strong>规范化项</strong>：$\frac{\lambda}{2 n} \sum_{w} w^{2}$到代价函数上.其中λ是<strong>规范化参数</strong>,注意规范化项里<strong>不包含偏置</strong>.</li>
<li><strong>L1规范化</strong>的规范化项为：$\frac{\lambda}{n} \sum_{w}|w|$在L1规范化中,权重通过一个常量<strong>向0</strong>进行缩小,在L2规范化中,权重通过一个和w成<strong>比例</strong>的量进行缩小.所以L1规范化倾向于<strong>聚集</strong>网络的权重在<strong>相对少量</strong>的高重要度连接上.</li>
</ol>
</li>
<li><strong>dropout</strong></li>
<li><strong>人为扩展训练数据</strong></li>
</ol>
<h4 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h4><ol>
<li><strong>学习速率η</strong></li>
<li><strong>早停</strong></li>
<li><strong>规范化参数λ</strong></li>
<li><strong>小批量数据大小</strong></li>
<li><strong>网格搜索(grid search)</strong></li>
</ol>
<h3 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><p>对于前馈神经网络来说其公式：</p>
<script type="math/tex; mode=display">
O=f(W*X+b)</script><p>其中$W$和$b$是模型的参数，$X$是当前的输入，$f(·)$是激活函数，$*$是矩阵乘法，$O$是当前的输出。即输出等于输入经过线性与非线性映射后的结果。</p>
<p>而RNN通过将前一时刻的运算结果添加到当前的运算中，从而实现了“考虑上文信息”的功能。</p>
<script type="math/tex; mode=display">
\mathrm{O}_{\mathrm{t}}=\mathrm{f}\left(\mathrm{X} * \mathrm{W}+\mathrm{O}_{\mathrm{t}-1} * \mathrm{V}+\mathrm{b}\right)</script><p>其中$W,V,b$是模型的参数，下标$t$代表当前的序列位置／时间点，$t-1$代表上个位置/上个时间点，$X$是当前的输入，$f(·)$是激活函数，$*$是矩阵乘法，$O$是模型输出。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/rnn.png" alt="avatar"></p>
<p>BiRnn:RNN可以考虑上文的信息，那么如何将下文的信息也添加进去呢？这就是BiRNN要做的事情。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/birnn.png" alt="avatar"></p>
<h5 id="RNN存在的问题"><a href="#RNN存在的问题" class="headerlink" title="RNN存在的问题"></a>RNN存在的问题</h5><p>首先，从RNN的前向过程来看，可以认为它只有一层权重矩阵W（先不管V）。由此可见从深层的角度去看RNN的前向过程，就可以认为RNN是<strong>各个层的权重矩阵相同</strong>的深层网络。忽略V和激活函数，就可以近似的认为网络一共有T层（T等于序列的长度），那么第t层的输出就是连乘t次W，也就是$W^t$！,由于<strong>矩阵可以用它的特征值矩阵和特征向量矩阵去近似</strong>，即</p>
<script type="math/tex; mode=display">
W = V diag(\lambda) V^{-1}  \\
W^t = (V diag(\lambda) V^{-1})(V diag(\lambda) V^{-1})\cdots(V diag(\lambda) V^{-1})=V{ diag(\lambda)}^t V^{-1}</script><p>也就是说，<strong>特征值矩阵中的每个特征值都会随着t的增大发生指数级变化！</strong>所以某个特征值大于1时，就容易导致这个维度的值爆炸性增长；当特征值小于1时，会就会导致这个维度的值指数级速度衰减为0！</p>
<p>前向过程如此，误差反向传播的过程也必然近似为输出层的误差会乘以$W^t$来得到倒数第t层的梯度，然而由于刚各个维度不是指数级衰减就是指数级爆炸，很容易看出当更新RNN的靠前的层的时候（即离着输出层较远的那些层，或者说与序列末端离得远的位置），计算出的梯度要么大的吓人，要么小的忽略。小的忽略的值不会对W的更新有任何指导作用，大的吓人的值一下子就把W中的某些值弄的大的吓人了，害得前面的优化都白做了。</p>
<p>一个很简单的想法是进行<strong>梯度截断</strong>，在优化RNN的参数的时候，给梯度值设置一个上限。但是这样显然就会导致模型难以再顾及很靠前的历史信息了，因此理论上RNN可以保存任意长的历史信息来辅助当前时间点的决策，然而由于在优化时（训练RNN时），梯度无法准确合理的传到很靠前的时间点，因此RNN实际上只能记住并不长的序列信息（在NLP中，经验上认为序列大于30的时候，RNN基本记不住多于30的部分，而从多于10的位置开始就变得记性很差了），因此RNN相比前馈网络，可以记住的历史信息要长一些，但是无法记住长距离的信息（比如段落级的序列甚至篇章级的序列，用RNN就鸡肋了）。</p>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>因为RNN存在梯度弥散和梯度爆炸的问题，所以RNN很难完美地处理具有长期依赖的信息。既然仅仅依靠一条线连接后面的神经元不足以解决问题，那么就再加一条线好了，这就是LSTM。</p>
<p>LSTM的关键在于细胞的状态和穿过细胞的线，细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流动保持不变会变得容易。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/lstm_c.png" alt="avatar"></p>
<p>在LSTM中，门可以实现选择性的让信息通过，主要通过一个sigmoid的神经层和一个逐点相乘的操作来实现。LSTM通过三个这样的门结构来实现信息的保护和控制，分别是遗忘门（forget gate）、输入门（input gate）与输出门（output gate）。</p>
<h5 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a><strong>遗忘门</strong></h5><p>在LSTM中的第一步是决定从细胞状态中丢弃什么信息。这个决定通过一个称为遗忘门的结构来完成。遗忘门会读取$h_{t-1}$和$x_t$，输出一个0到1之间的数值给细胞的状态$c_{t-1}$中的数字。1表示完全保留，0表示完全舍弃。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/lstm_forget_gate.png" alt="avatar"></p>
<h5 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a><strong>输入门</strong></h5><p>遗忘门决定让多少新的信息加入到cell的状态中来。实现这个需要两个步骤：</p>
<ol>
<li><p>首先一个叫“input gate layer”的sigmoid层决定哪些信息需要更新；一个tanh层生成一个向量，用来更新状态C。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/input_gate.png" alt="avatar"></p>
</li>
<li><p>把 1 中的两部分联合起来，对cell的状态进行更新，我们把旧的状态与$f_t$相乘，丢弃掉我们确定需要丢弃的信息，接着加上$i_t * \tilde{C}_{t}$</p>
</li>
</ol>
<h5 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h5><p>最终，我们需要确定输出什么值，这个输出将会基于我们的细胞的状态，但是也是一个过滤后的版本。首先，我们通过一个sigmoid层来确定细胞状态的哪些部分将输出出去。接着，我们把细胞状态通过tanh进行处理（得到一个-1到1之间的值）并将它和sigmoid门的输出相乘，最终我们仅仅会输出我们确定输出的部分。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/output_gate.png" alt="avatar"></p>
<h5 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h5><p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/lstm_all.png" alt="avatar"></p>
<h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><p>在LSTM中引入了三个门函数：输入门、遗忘门和输出门来控制输入值、记忆值和输出值。而在GRU模型中只有两个门：分别是更新门和重置门。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/GRU.png" alt="avatar"></p>
<p>图中的$z_t$和$r_t$分别表示更新门和重置门。更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多。重置门控制前一状态有多少信息被写入到当前的候选集 $\tilde{h}_{t}$上，重置门越小，前一状态的信息被写入的越少。</p>
<p>LSTM和GRU都是通过各种门函数来将重要特征保留下来，这样就保证了在long-term传播的时候也不会丢失。此外GRU相对于LSTM少了一个门函数，因此在参数的数量上也是要少于LSTM的，所以整体上GRU的训练速度要快于LSTM的。</p>
<h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention.png" alt="avatar"></p>
<p><strong>核心思想</strong>：在decoding阶段对input中的信息赋予不同权重。</p>
<ol>
<li>Encode所有输入序列,得到对应的$h_1,h_2, \cdots ,h_T$(T为输入序列长度)</li>
<li>Decode输出目标$y_t$之前，会将上一步输出的隐藏状态$S_{t-1}$与之前encode好的$h_1,h_2,\cdots,h_T$进行比对，计算相似度（$e_{t,j}=a(s_{t-1},h_j)$）,$h_j$为之前第j个输入encode得到的隐藏向量，a为任意一种计算相似度的方式</li>
<li>然后通过softmax，即$a_{t,j}=\frac{exp(e_{t,j})}{\sum^{T_x}_{k=1}exp(e_{t,k})}$将之前得到的各个部分的相关系数进行归一化，得到$a_{t,1},a_{t,2},\cdots,a_{t,T}$</li>
<li>在对输入序列的隐藏层进行相关性加权求和得到此时decode需要的context vector ：$c_j=\sum^{T_x}_{j=1}a_{i,j}h_j$</li>
</ol>
<h3 id="词向量模型"><a href="#词向量模型" class="headerlink" title="词向量模型"></a>词向量模型</h3><p>onehot编码方式的缺点：</p>
<ol>
<li>它是一个词袋模型，不考虑词与词之间的顺序（文本中词的顺序信息也是很重要的）</li>
<li>它假设词与词相互独立（在大多数情况下，词与词是相互影响的）</li>
<li>它得到的特征是离散稀疏的，实际应用中，面临着巨大的维度灾难问题</li>
</ol>
<p>将高维词向量嵌入到一个低维空间。word2vec是词嵌入方式的一种。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/word_embedding.png" alt="avatar"></p>
<p>主要解决两个问题：</p>
<ol>
<li>一个是统计语言模型里关注的条件概率$𝑝(𝑤𝑡|𝑐𝑜𝑛𝑡𝑒𝑥𝑡)$的计算</li>
<li>一个是向量空间模型里关注的词向量的表达</li>
</ol>
<h4 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h4><h5 id="CBoW模型"><a href="#CBoW模型" class="headerlink" title="CBoW模型"></a><strong>CBoW模型</strong></h5><p>CBOW是已知上下文，估算当前词语的语言模型。其学习目标是最大化对数似然函数：</p>
<script type="math/tex; mode=display">
\mathcal{L}=\sum_{w \in \mathcal{C}} \log p(w | \text {Context}(w))</script><p>其中，w表示语料库C中任意一个词。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/CBOW.png" alt="avatar"></p>
<ol>
<li><strong>输入层</strong>是上下文的词语的词向量（在训练CBOW模型，词向量只是个副产品，确切来说，是CBOW模型的一个参数。训练开始的时候，词向量是个随机值，随着训练的进行不断被更新）。</li>
<li><strong>投影层</strong>对其求和，所谓求和，就是简单的向量加法。</li>
<li><strong>输出层</strong>输出最可能的w。由于语料库中词汇量是固定的|C|个，所以上述过程其实可以看做一个多分类问题。给定特征，从|C|个分类中挑一个。</li>
</ol>
<p>对于神经网络模型多分类，最朴素的做法是softmax回归：</p>
<script type="math/tex; mode=display">
h_{\theta}\left(x^{(i)}\right)=\left[\begin{array}{c}
p\left(y^{(i)}=1 | x^{(i)} ; \theta\right) \\
p\left(y^{(i)}=2 | x^{(i)} ; \theta\right) \\
\vdots \\
p\left(y^{(i)}=k | x^{(i)} ; \theta\right)
\end{array}\right]=\frac{1}{\sum_{j=1}^{k} e^{\theta_{j}^{T} x^{(i)}}}\left[\begin{array}{c}
e^{\theta_{1}^{T} x^{(i)}} \\
e^{\theta_{2}^{T} x^{(i)}} \\
\vdots \\
e^{\theta_{k}^{T} x^{(i)}}
\end{array}\right]</script><h5 id="skip-gram模型"><a href="#skip-gram模型" class="headerlink" title="skip-gram模型"></a>skip-gram模型</h5><p><strong>target word对context的预测中学习word vector</strong></p>
<script type="math/tex; mode=display">
p(\text {Context}(w) | w)=\prod_{u \in \text {Context}(w)} p(u | w)</script><p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/skipgram.png" alt="avatar"></p>
<p>如何将Skip-gram模型的前向计算过程写成数学形式，我们得到：</p>
<script type="math/tex; mode=display">
p\left(w_{o} | w_{i}\right)=\frac{e^{U_{o} \cdot V_{i}}}{\sum_{j} e^{U_{j} V_{i}}}</script><p>其中，其中，$V_i$是Embedding层矩阵里的列向量，也被称为$w_i$的input vector。$U_j$是softmax层矩阵里的行向量，也被称为$w_i$的output vector。因此，Skip-gram模型的本质是<strong>计算输入word的input vector与目标word的output vector之间的余弦相似度，并进行softmax归一化</strong>。我们要学习的模型参数正是这两类词向量。</p>
<p>然而，直接对词典里的V个词计算相似度并归一化，显然是一件极其耗时的impossible mission。引入了两种优化算法：<strong>层次Softmax（Hierarchical Softmax）</strong>和<strong>负采样（Negative Sampling）</strong>。</p>
<h5 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h5><p>softmax回归需要对语料库中每个词语（类）都计算一遍输出概率并进行归一化，在几十万词汇量的语料上无疑是令人头疼的。在SVM中的多分类，其多分类是由二分类组合而来的：</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/svm_hierarchical.gif" alt="avatar"></p>
<p>这是一种二叉树结构，应用到word2vec中被作者称为Hierarchical Softmax：</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/cbow_hierarchical.jpg" alt="avatar"></p>
<p>上图输出层的树形结构即为Hierarchical Softmax。非叶子节点相当于一个神经元（感知机，逻辑斯谛回归就是感知机的输出代入$f(x)=\frac{1}{1+e^x}$，二分类决策输出1或0，分别代表向下左转或向下右转；每个叶子节点代表语料库中的一个词语，于是每个词语都可以被01唯一地编码，并且其编码序列对应一个事件序列，于是我们可以计算条件概率$p(w|Context(x))$。</p>
<p>在开始计算之前，还是得引入一些符号：</p>
<ol>
<li>$p^w$从根结点出发到达w对应叶子结点的路径.</li>
<li>$l^w$路径中包含结点的个数</li>
<li>$p^w_1,p^w_2,\cdots,p^w_{l^w}$路径$p^w$中的各个节点</li>
<li>$d^w_2,d^w_3,\cdots,d^w_{l^w} \in {0,1}$词w的编码，$d^w_j$表示路径$p^w$第j个节点对应的编码（根节点无编码）</li>
<li>$\theta^w_1,\theta^w_2,\cdots,\theta^w_{l^w-1} \in R^m$路径$p^w$中非叶节点对应的参数向量</li>
</ol>
<p>于是可以给出$w$的条件概率：</p>
<script type="math/tex; mode=display">
p(w | \text { Context }(w))=\prod_{j=2}^{l^{w}} p\left(d_{j}^{w} | \mathbf{x}_{w}, \theta_{j-1}^{w}\right)</script><p>这是个简单明了的式子，从根节点到叶节点经过了$l^w-1$个节点，编码从下标2开始（根节点无编码），对应的参数向量下标从1开始（根节点为1）。其中，每一项是一个逻辑斯谛回归：</p>
<script type="math/tex; mode=display">
p\left(d_{j}^{w} | \mathbf{x}_{w}, \theta_{j-1}^{w}\right)=\left\{\begin{array}{ll}
\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right), & d_{j}^{w}=0 \\
1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right), & d_{j}^{w}=1
\end{array}\right.</script><p>考虑到d只有0和1两种取值，我们可以用指数形式方便地将其写到一起：</p>
<script type="math/tex; mode=display">
p\left(d_{j}^{w} | \mathbf{x}_{w}, \theta_{j-1}^{w}\right)=\left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{1-d^w_{j}} \cdot\left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{d^w_{j}}</script><p>我们的目标函数取对数似然：</p>
<script type="math/tex; mode=display">
\mathcal{L}=\sum_{w \in \mathcal{C}} \log p(w | \text { Context }(w))</script><p>将$p(w|Context(w))$代入上式，有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{L} &=\sum_{w \in \mathcal{C}} \log \prod_{j=2}^{l^{w}}\left\{\left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{1-d_{j}^{v}} \cdot\left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{d_{j}}\right\} \\
&=\sum_{w \in \mathcal{C}} \sum_{j=2}^{l^{w}}\left\{\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]+d_{j}^{w} \cdot \log \left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]\right\}
\end{aligned}</script><p>这也很直白，连乘的对数换成求和。不过还是有点长，我们把每一项简记为：</p>
<script type="math/tex; mode=display">
\mathcal{L}(w, j)=\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]+d_{j}^{w} \cdot \log \left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]</script><p>怎么最大化对数似然函数呢？分别最大化每一项即可（这应该是一种近似，最大化某一项不一定使整体增大，具体收敛的证明还不清楚）。怎么最大化每一项呢？先求函数对每个变量的偏导数，对每一个样本，代入偏导数表达式得到函数在该维度的增长梯度，然后让对应参数加上这个梯度，函数在这个维度上就增长了。</p>
<p>每一项有两个参数，一个是每个节点的参数向量$\theta^w_{j-1}$，另一个是输出层的输入$X_w$，我们分别对其求偏导数：</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}(w, j)}{\partial \theta_{j-1}^{w}}=\frac{\partial}{\partial \theta_{j-1}^{w}}\left\{\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]+d_{j}^{w} \cdot \log \left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]\right\}</script><p>因为sigmoid函数的导数有个很棒的形式：</p>
<script type="math/tex; mode=display">
\sigma^{\prime}(x)=\sigma(x)[1-\sigma(x)]</script><p>于是代入上上式得到：</p>
<script type="math/tex; mode=display">
\left(1-d_{j}^{w}\right)\left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right] \mathbf{x}_{w}-d_{j}^{w} \sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right) \mathbf{x}_{w}</script><p>合并同类项得到：</p>
<script type="math/tex; mode=display">
\left[1-d_{j}^{w}-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right] \mathbf{x}_{w}</script><p>于是$\theta^w_{j-1}$的更新表达式就得到了：</p>
<script type="math/tex; mode=display">
\theta_{j-1}^{w}:=\theta_{j-1}^{w}+\eta\left[1-d_{j}^{w}-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right] \mathbf{x}_{w}</script><p>再来$X_w$的偏导数，注意到$\mathcal{L}(w, j)=\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]+d_{j}^{w} \cdot \log \left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]$中$X_w$和$\theta^w_{j-1}$是对称的，所有直接将$\theta^w_{j-1}$的偏导数中的$\theta^w_{j-1}$替换为$X_w$，得到关于$X_w$的偏导数：</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}(w, j)}{\partial \mathbf{x}_{w}}=\left[1-d_{j}^{w}-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right] \theta_{j-1}^{w}</script><p>不过$X_w$是上下文的词向量的和，不是上下文单个词的词向量。怎么把这个更新量应用到单个词的词向量上去呢？word2vec采取的是直接将$X_w$的更新量整个应用到每个单词的词向量上去：</p>
<script type="math/tex; mode=display">
\mathbf{v}(\tilde{w}):=\mathbf{v}(\tilde{w})+\eta \sum_{j=2}^{l^{w}} \frac{\partial \mathcal{L}(w, j)}{\partial \mathbf{x}_{w}}, \quad \tilde{w} \in \text { Context }(w)</script><p>其中，$V(\tilde w)$代表上下文中某一个单词的词向量。</p>
<h4 id="glove"><a href="#glove" class="headerlink" title="glove"></a>glove</h4><p>CBOW和skip-gram虽然可以很好地进行词汇类比，但是因为这两种方法是基于一个个局部的上下文窗口方法，因此，没有有效地利用全局的词汇共现统计信息。为了克服全局矩阵分解和局部上下文窗口的缺陷，GloVe方法基于全局词汇共现的统计信息来学习词向量，从而将统计信息与局部上下文窗口方法的优点都结合起来，并发现其效果确实得到了提升。</p>
<h5 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h5><p>假设i=ice,j=steam并对k取不同的词汇，如“solid”，“gas”，“water”，“fashion”，根据上面的定义，我们分别计算他们的概率$P(k∣ice)$、$P(k∣steam)$，并计算两者的比率$P(k∣ice)/P(k∣steam)$，可以发现，对于“solid”，其出现在“ice”上下文的概率应该比较大，出现在“steam”上下文的概率应该比较小，因此，他们的比值应该是一个比较大的数，在下表中是8.9，而对于“gas”，出现在“ice”上下文的概率应该比较小，而出现在“steam”上下文的概率应该比较大，因此，两者的比值应该是一个比较小的数，在下表中是$8.5×10^{-2}$ ，而对于“water、fashion”这两个词汇，他们与“ice”和steam“的相关性应该比较小，因此，他们的比值应该都是接近1。因此，这样来看可以发现，比值$P(k∣ice)/P(k∣steam)$在一定程度上可以反映词汇之间的相关性，当相关性比较低时，其值应该在1附近，当相关性比较高时，比值应该偏离1比较远。<br><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/glove.png" alt="avatar"></p>
<h5 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h5><p>基于这样的思想，作者提出了这样一种猜想，能不能通过训练词向量，使得词向量经过某种函数计算之后可以得到上面的比值，具体如下：</p>
<script type="math/tex; mode=display">
F\left(w_{i}, w_{j}, \tilde{w}_{k}\right)=\frac{P_{i k}}{P_{j k}}</script><p>其中，$w_i$,$w_j$,$\tilde{w}_{k}$ 为词汇i,j,k对应的词向量，其维度都为d，而$P_{ik}$，$P_{jk}$ 则可以直接通过语料计算得到，这里F为一个未知的函数。由于词向量都是在一个线性向量空间，因此，可以对$w_i$,$w_j$进行差分，将上式转变为如下：</p>
<script type="math/tex; mode=display">
F\left(w_{i}-w_{j}, \tilde{w}_{k}\right)=\frac{P_{i k}}{P_{j k}}</script><p>由于上式中左侧括号中是两个维度为d的词向量，而右侧是一个标量，因此，很容易会想到向量的内积，因此，上式可以进一步改变为:</p>
<script type="math/tex; mode=display">
F\left(\left(w_{i}-w_{j}\right)^{T} \tilde{w}_{k}\right)=F\left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\right)=\frac{P_{i k}}{P_{j k}}</script><p>由于上式中左侧是一种减法，而右侧是一种除法，很容易联想到指数计算，因此，可以把F限定为指数函数，此时有：</p>
<script type="math/tex; mode=display">
\exp \left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\right)=\frac{\exp \left(w_{i}^{T} w_{k}\right)}{\exp \left(w_{j}^{T} w_{k}\right)}=\frac{P_{i k}}{P_{j k}}</script><p>因此，此时只要确保等式两边分子分母相等即可，即：</p>
<script type="math/tex; mode=display">
\exp \left(w_{i}^{T} w_{k}\right)=P_{i k}, \exp \left(w_{j}^{T} w_{k}\right)=P_{j k}</script><p>进一步的，可以转化为对语料中的所有词汇，考察$exp(w^T_iw_k)$=$P_{ik}$=$\frac{X_{ik}}{X_i}$ ，即：</p>
<script type="math/tex; mode=display">
w_{i}^{T} w_{k}=\log \left(\frac{X_{i k}}{X_{i}}\right)=\log X_{i k}-\log X_{i}</script><p>由于上式左侧$w^T_iw_k$中，调换i和k的值不会改变其结果，即具有对称性，因此，为了确保等式右侧也具备对称性，引入了两个偏置项，即</p>
<script type="math/tex; mode=display">
w_{i}^{T} w_{k}=\log X_{i k}-b_{i}-b_{k}</script><p>此时，$logX_i$已经包含在$bi$当中。因此，此时模型的目标就转化为通过学习词向量的表示，使得上式两边尽量接近，因此，可以通过计算两者之间的平方差来作为目标函数，即：</p>
<script type="math/tex; mode=display">
J=\sum_{i, k=1}^{V}\left(w_{i}^{T} \tilde{w}_{k}+b_{i}+b_{k}-\log X_{i k}\right)^{2}</script><p>但是这样的目标函数有一个缺点，就是对所有的共现词汇都是采用同样的权重，因此，作者对目标函数进行了进一步的修正，通过语料中的词汇共现统计信息来改变他们在目标函数中的权重，具体如下：</p>
<script type="math/tex; mode=display">
J=\sum_{i, k=1}^{V} f\left(X_{i k}\right)\left(w_{i}^{T} \tilde{w}_{k}+b_{i}+b_{k}-\log X_{i k}\right)^{2}</script><p>这里$V$表示词汇的数量，并且权重函数$f$必须具备以下的特性</p>
<ul>
<li>$f(0)=0$，当词汇共现的次数为0时，此时对应的权重应该为0。</li>
<li>f(x)必须是一个非减函数，这样才能保证当词汇共现的次数越大时，其权重不会出现下降的情况。</li>
<li>对于那些太频繁的词，$f(x)$应该能给予他们一个相对小的数值，这样才不会出现过度加权。</li>
</ul>
<p>综合以上三点特性，作者提出了下面的权重函数：</p>
<script type="math/tex; mode=display">
f(x)=\left\{\begin{array}{cc}
\left(x / x_{\max }\right)^{\alpha} & \text { if } x<x_{\max } \\
1 & \text { otherwise }
\end{array}\right.</script><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><ol>
<li>Glove综合了全局词汇共现的统计信息和局部窗口上下文方法的优点，可以说是两个主流方法的一种综合，但是相比于全局矩阵分解方法，由于GloVe不需要计算那些共现次数为0的词汇，因此，可以极大的减少计算量和数据的存储空间。</li>
<li>但是GloVe把语料中的词频共现次数作为词向量学习逼近的目标，当语料比较少时，有些词汇共现的次数可能比较少，笔者觉得可能会出现一种误导词向量训练方向的现象。</li>
</ol>
<h4 id="ELMO"><a href="#ELMO" class="headerlink" title="ELMO"></a>ELMO</h4><h5 id="针对的问题"><a href="#针对的问题" class="headerlink" title="针对的问题"></a>针对的问题</h5><p>word2vec中，只有apple一个词向量，无法对一词多义进行建模。</p>
<h5 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h5><p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/EMLO.png" alt="avatar"></p>
<p>ELMo的主要做法是先训练一个完整的语言模型，再用这个语言模型去处理需要训练的文本，生成相应的词向量，所以在文中一直强调ELMo的模型对同一个字在不同句子中能生成不同的词向量。</p>
<p>在进行有监督的NLP任务时，可以将ELMo直接当做特征拼接到具体任务模型的词向量输入或者是模型的最高层表示上。不像传统的词向量，每一个词只对应一个词向量，ELMo利用预训练好的双向语言模型，然后根据具体输入从该语言模型中可以得到上下文依赖的当前词表示（<strong>对于不同上下文的同一个词的表示是不一样的</strong>），再当成特征加入到具体的NLP有监督模型里。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/emlo_embedding.png" alt="avatar"></p>
<p>EMLO使用的是一个<strong>双向的LSTM语言模型</strong>，由一个前向和一个后向语言模型构成，目标函数就是取这两个方向语言模型的最大似然。</p>
<p>前向LSTM结构：</p>
<script type="math/tex; mode=display">
p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} | t_{1}, t_{2}, \ldots, t_{k-1}\right)</script><p>反向LSTM结构：</p>
<script type="math/tex; mode=display">
p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} | t_{k+1}, t_{k+2}, \ldots, t_{N}\right)</script><p>最大似然函数：</p>
<script type="math/tex; mode=display">
\sum_{k=1}^{N}\left(\log p\left(t_{k} | t_{1}, t_{2}, \ldots, t_{k-1}\right)+\log p\left(t_{k} | t_{k+1}, t_{k+2}, \ldots, t_{N}\right)\right)</script><h5 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h5><ol>
<li>ELMo的假设前提一个词的词向量不应该是固定的，所以在一词多意方面ELMo的效果一定比word2vec要好。</li>
<li>word2vec的学习词向量的过程是通过中心词的上下窗口去学习，学习的范围太小了，而ELMo在学习语言模型的时候是从整个语料库去学习的，而后再通过语言模型生成的词向量就相当于基于整个语料库学习的词向量，更加准确代表一个词的意思。</li>
</ol>
<h4 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h4><p>用Transformer网络代替了LSTM作为语言模型来更好的捕获长距离语言结构。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/gpt_1.png" alt="avatar"></p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/gpt_2.png" alt="avatar"></p>
<h5 id="Masked-Attention"><a href="#Masked-Attention" class="headerlink" title="Masked Attention"></a>Masked Attention</h5><p>在transformer中，Encoder因为要编码整个句子，所以每个词都需要考虑上下文的关系。所以每个词在计算的过程中都是可以看到句子中所有的词的。但是Decoder与Seq2Seq中的解码器类似，每个词都只能看到前面词的状态，所以是一个单向的Self-Attention结构。Masked Attention的实现也非常简单，只要在普通的Self Attention的Softmax步骤之前，与按位乘上一个下三角矩阵M就好了</p>
<p>attention：</p>
<script type="math/tex; mode=display">
\begin{aligned}
Q &=X W_{Q} \\
K &=X W_{K} \\
V &=X W_{V} \\
\text {Attention}(Q, K, V) &=\operatorname{Softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
\end{aligned}</script><p>masked-attention</p>
<script type="math/tex; mode=display">
\text { Attention }(Q, K, V)=\operatorname{Softmax}\left(\frac{Q K^{T} \Theta M}{\sqrt{d_{k}}}\right) V</script><h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h4><p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/bert_gpt_elmo.png" alt="avatar"></p>
<h5 id="Task-1-MLM"><a href="#Task-1-MLM" class="headerlink" title="Task 1: MLM"></a>Task 1: MLM</h5><p>由于BERT需要通过上下文信息，来预测中心词的信息，同时又不希望模型提前看见中心词的信息，因此提出了一种 Masked Language Model 的预训练方式，即随机从输入预料上 mask 掉一些单词，然后通过的上下文预测该单词，类似于一个完形填空任务。</p>
<p>在预训练任务中，15%的 Word Piece 会被mask，这15%的 Word Piece 中，80%的时候会直接替换为 [Mask] ，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token</p>
<ul>
<li>没有100%mask的原因<ul>
<li>如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词</li>
</ul>
</li>
<li>加入10%随机token的原因<ul>
<li>Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘</li>
<li>另外编码器不知道哪些词需要预测的，哪些词是错误的，因此被迫需要学习每一个token的表示向量</li>
</ul>
</li>
<li>另外，每个batchsize只有15%的单词被mask的原因，是因为性能开销的问题，双向编码器比单项编码器训练要更慢</li>
</ul>
<h5 id="Task-2-NSP"><a href="#Task-2-NSP" class="headerlink" title="Task 2: NSP"></a>Task 2: NSP</h5><p>仅仅一个MLM任务是不足以让 BERT 解决阅读理解等句子关系判断任务的，因此添加了额外的一个预训练任务，即 Next Sequence Prediction。</p>
<p>具体任务即为一个句子关系判断任务，即判断句子B是否是句子A的下文，如果是的话输出’IsNext‘，否则输出’NotNext‘。</p>
<p>训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在图4中的[CLS]符号中</p>
<h5 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h5><ul>
<li>Token Embeddings：即传统的词向量层，每个输入样本的首字符需要设置为[CLS]，可以用于之后的分类任务，若有两个不同的句子，需要用[SEP]分隔，且最后一个字符需要用[SEP]表示终止</li>
<li>Segment Embeddings：为[0,1]序列，用来在NSP任务中区别两个句子，便于做句子关系判断任务</li>
<li>Position Embeddings：与Transformer中的位置向量不同，BERT中的位置向量是直接训练出来的</li>
</ul>
<h5 id="Fine-tunninng"><a href="#Fine-tunninng" class="headerlink" title="Fine-tunninng"></a>Fine-tunninng</h5><p>对于不同的下游任务，我们仅需要对BERT不同位置的输出进行处理即可，或者直接将BERT不同位置的输出直接输入到下游模型当中。具体的如下所示：</p>
<ul>
<li>对于情感分析等单句分类任务，可以直接输入单个句子（不需要[SEP]分隔双句），将[CLS]的输出直接输入到分类器进行分类</li>
<li>对于句子对任务（句子关系判断任务），需要用[SEP]分隔两个句子输入到模型中，然后同样仅须将[CLS]的输出送到分类器进行分类</li>
<li>对于问答任务，将问题与答案拼接输入到BERT模型中，然后将答案位置的输出向量进行二分类并在句子方向上进行softmax（只需预测开始和结束位置即可）</li>
<li>对于命名实体识别任务，对每个位置的输出进行分类即可，如果将每个位置的输出作为特征输入到CRF将取得更好的效果。</li>
</ul>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>BERT的预训练任务MLM使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务</li>
<li>另外，BERT没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计</li>
<li>由于最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）</li>
<li>适合处理自然语义理解类任务(NLU)，而不适合自然语言生成类任务(NLG)</li>
</ul>
<h5 id="torch使用"><a href="#torch使用" class="headerlink" title="torch使用"></a>torch使用</h5><ol>
<li>导入相关库</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> transformers <span class="keyword">as</span> ppb <span class="comment"># pytorch transformers</span></span><br></pre></td></tr></table></figure>
<ol>
<li>导入预训练好的 DistilBERT 模型与分词器</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, <span class="string">'distilbert-base-uncased'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Want BERT instead of distilBERT? Uncomment the following line:</span></span><br><span class="line"><span class="comment">#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')# </span></span><br><span class="line"></span><br><span class="line">Load pretrained model/tokenizertokenizer = tokenizer_class.from_pretrained(pretrained_weights)</span><br><span class="line">model = model_class.from_pretrained(pretrained_weights)</span><br></pre></td></tr></table></figure>
<ol>
<li>tokenize：分词+[cls]+[sep]<br><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/bert_tokenize.png" alt="avatar"></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.encode(<span class="string">"a visually stunning rumination on love"</span>, add_special_tokens=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>padding</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">   max_len = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tokenized.values:</span><br><span class="line">    <span class="keyword">if</span> len(i) &gt; max_len:</span><br><span class="line">        max_len = len(i)</span><br><span class="line">padded = np.array([i + [<span class="number">0</span>]*(max_len-len(i)) <span class="keyword">for</span> i <span class="keyword">in</span> tokenized.values])</span><br></pre></td></tr></table></figure>
<ol>
<li>masking</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">attention_mask = np.where(padded != <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>使用bert进行编码</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># last_hidden_states = [batch_size, max_sentence_len, 768]</span></span><br><span class="line">    last_hidden_states = model(input_ids, attention_mask=attention_mask)</span><br></pre></td></tr></table></figure>
<h3 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h3><p>无论是RNN还是CNN，在处理NLP任务时都有缺陷。CNN是其先天的卷积操作不很适合序列化的文本，RNN是其没有并行化，很容易超出内存限制（比如50tokens长度的句子就会占据很大的内存）。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/transformer.png" alt="avatar"></p>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><ol>
<li>如何实现并行计算同时缩短依赖距离？采用自注意机制</li>
<li>如何向CNN一样考虑多通道信息？采用多头注意力</li>
<li>自注意力机制损失了位置信息，如何补偿? 位置嵌入</li>
<li>后面的层中位置信息消散？ 残差连接</li>
</ol>
<h4 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h4><p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention-1.png" alt="avatar"></p>
<h5 id="并行化"><a href="#并行化" class="headerlink" title="并行化"></a>并行化</h5><p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention-2.png" alt="avatar" style="zoom:67%;"></p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention-3.png" alt="avatar" style="zoom: 67%;"></p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention-4.png" alt="avatar" style="zoom: 67%;"></p>
<h5 id="multi-head-self-attention"><a href="#multi-head-self-attention" class="headerlink" title="multi-head self-attention"></a>multi-head self-attention</h5><p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/multi-head.png" alt="avatar" style="zoom:67%;"></p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/multi-head-1.png" alt="avatar"></p>
<h4 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h4><p>Encoder由N=6个相同的layer组成，layer指的就是上图左侧的单元，最左边有个“Nx”，这里是x6个。每个Layer由两个sub-layer组成，分别是multi-head self-attention mechanism和fully connected feed-forward network。其中每个sub-layer都加了residual connection和normalisation，因此可以将sub-layer的输出表示为：</p>
<script type="math/tex; mode=display">
sub_layer_output=LayerNorm(x+(\text {SubLayer}(x)))</script><h5 id="Multi-head-self-attention"><a href="#Multi-head-self-attention" class="headerlink" title="Multi-head self-attention"></a>Multi-head self-attention</h5><p>由于attention：</p>
<script type="math/tex; mode=display">
attention_output=Attention(Q, K, V)</script><p>multi-head attention则是通过h个不同的<strong>线性变换</strong>对Q，K，V进行投影，最后将不同的attention结果拼接起来：</p>
<script type="math/tex; mode=display">
\begin{array}{l}\text { MultiHead(Q,K,V)}= Concat(head_1,\ldots,head_h)W^{O}  \\ head_i = Attention(QW_i\left.^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \\ \text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V\end{array}</script><p>多个head带来的优势是：不同的head，所attention的内容不同，比如：$head_1$ attention 全局的内容，$head_2$ attention local的内容。</p>
<h5 id="Position-wise-feed-forward-networks"><a href="#Position-wise-feed-forward-networks" class="headerlink" title="Position-wise feed-forward networks"></a>Position-wise feed-forward networks</h5><p>这层主要是提供非线性变换。Attention输出的维度是[bsz<em>seq_len, num_heads</em>head_size]，第二个sub-layer是个全连接层，之所以是position-wise是因为过线性层时每个位置i的变换参数是一样的.</p>
<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a><strong>Decoder</strong></h4><p>Decoder和Encoder的结构差不多，但是多了一个attention的sub-layer,decoder的输入输出和解码过程:</p>
<ul>
<li>输出：对应i位置的输出词的概率分布</li>
<li>输入：encoder的输出 &amp; 对应i-1位置decoder的输出。所以中间的attention不是self-attention，它的K，V来自encoder，Q来自上一位置decoder的输出</li>
<li>解码：<strong>编码可以并行计算，一次性全部encoding出来，但解码不是一次把所有序列解出来的，而是像rnn一样一个一个解出来的</strong>，因为要用上一个位置的输入当作attention的query</li>
</ul>
<h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><ol>
<li><p>不能一次对很长文本进行建模（假如文本长度为$10^5$,在一个forward的模块中需要$10^5 * 10^5 = 10^{10}$大小的矩阵来保存score，很容易导致 out of memory 问题）。解决方法：divide long sequence into smaller sequence. 但是这样导致smaller sequence 之间是没有联系的。（使用rnn的方式来建立smaller sequence 之间的联系—&gt; transformer XL）</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/transformer-xl.png" alt="avatar"></p>
<p>在引入rnn的时候，遇到了absolute position的问题，用relative position进行了解决。</p>
</li>
</ol>
<h5 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h5><p>引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</p>
<p>但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247484682&amp;idx=1&amp;sn=51520138eed826ec891ac8154ee550f9&amp;chksm=970c2ddca07ba4ca33ee14542cff0457601bb16236f8edc1ff0e617051d1f063354dda0f8893&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">从前馈到反馈：解析循环神经网络（RNN）及其tricks</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2018-07-06-7" target="_blank" rel="noopener">Step-by-step to LSTM: 解析LSTM神经网络设计原理</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/43493999" target="_blank" rel="noopener">Attention原理和源码解析</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2019-12-28?from=synced&amp;keyword=bert" target="_blank" rel="noopener">BERT模型超酷炫，上手又太难？请查收这份BERT快速入门指南！</a></li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>过去、现在、未来</title>
    <url>/2020/03/20/%E8%BF%87%E5%8E%BB%E3%80%81%E7%8E%B0%E5%9C%A8%E3%80%81%E6%9C%AA%E6%9D%A5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>开这篇 blog post 有两个目的：</p>
<ol>
<li>回首过去，把握现在，不负未来。当蹉跎不前的时候，可以提醒自己想要一个什么样的未来，什么应该做，什么不应该做。</li>
<li>客观的记录自己的成长。</li>
</ol>
<a id="more"></a>
<h3 id="2020-3-找工作"><a href="#2020-3-找工作" class="headerlink" title="2020.3 找工作"></a>2020.3 找工作</h3><h3 id="2019-11-2020-3-学习篇"><a href="#2019-11-2020-3-学习篇" class="headerlink" title="2019.11-2020.3 学习篇"></a>2019.11-2020.3 学习篇</h3><ul>
<li>[ ] 子串匹配算法基本思想，复杂度分析</li>
</ul>
<p>待找完实习再来补。。。</p>
<h3 id="2020-3-2020-4-找实习篇"><a href="#2020-3-2020-4-找实习篇" class="headerlink" title="2020.3-2020.4 找实习篇"></a>2020.3-2020.4 找实习篇</h3>]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>成长</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/2020/05/11/transformer/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/11/transformer/transformer.jpeg" alt></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>自2017年提出以来，Transformer在众多自然语言处理问题中取得了非常好的效果。它不但训练速度更快，而且更适合建模长距离依赖关系，因此大有取代循环或卷积神经网络，一统自然语言处理的深度模型江湖之势。本文结合<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">《Attention is all you need》</a>论文与Harvard的代码<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">《Annotated Transformer》</a>深入理解transformer模型。 </p>
<a id="more"></a>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>Transformer的整体结构如下图所示，在Encoder和Decoder中都使用了Self-attention, Point-wise和全连接层。Encoder和decoder的大致结构分别如下图的左半部分和右半部分所示。</p>
<p><img src="/2020/05/11/transformer/over_all.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Helper: Construct a model from hyperparameters.</span></span><br><span class="line"><span class="string">    src_vocab: 输入词汇表大小</span></span><br><span class="line"><span class="string">    tgt_vocab:输出词汇表大小</span></span><br><span class="line"><span class="string">    N：堆叠个数</span></span><br><span class="line"><span class="string">    d_model:embedding_size</span></span><br><span class="line"><span class="string">    d_ff: 线形层输出维度</span></span><br><span class="line"><span class="string">    h:multi-head中 head 的个数</span></span><br><span class="line"><span class="string">    dropout：</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N), <span class="comment"># encoder</span></span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N), <span class="comment"># decoder</span></span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)), <span class="comment"># src_embed</span></span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)), <span class="comment"># tgt_embed</span></span><br><span class="line">        Generator(d_model, tgt_vocab) <span class="comment"># generator</span></span><br><span class="line">    		)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>transformer的输入是<strong>Word Embedding + Position Embedding</strong>。</p>
<h3 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h3><p>Word embedding在pytorch中通常用 nn.Embedding 实现，其权重矩阵通常有两种选择：</p>
<ol>
<li>使用 Pre-trained的<strong>Embeddings并freeze</strong>，这种情况下实际就是一个 Lookup Table。</li>
<li>对其进行随机初始化(当然也可以选择 Pre-trained 的结果)，但<strong>设为 Trainable</strong>。这样在 training 过程中不断地对 Embeddings 进行改进。</li>
</ol>
<p>transformer选择后者，代码实现如下：</p>
<figure class="highlight python"><figcaption><span>word_embedding</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model) <span class="comment"># vocab 词汇表大小</span></span><br><span class="line">        self.d_model = d_model  <span class="comment">#表示embedding的维度</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure>
<h3 id="Positional-Embedding"><a href="#Positional-Embedding" class="headerlink" title="Positional Embedding"></a>Positional Embedding</h3><p>在RNN中，对句子的处理是一个个word按顺序输入的。但在 Transformer 中，输入句子的所有word是同时处理的，没有考虑词的排序和位置信息。因此，Transformer 的作者提出了加入 <code>positional encoding</code>的方法来解决这个问题。<code>positional encoding</code>使得 Transformer 可以衡量 word 位置有关的信息。</p>
<p><strong>如何实现具有位置信息的encoding？</strong></p>
<p>作者提供了两种思路：</p>
<ul>
<li>通过训练学习 positional encoding 向量；</li>
<li>使用公式来计算 positional encoding向量。</li>
</ul>
<p>试验后发现两种选择的结果是相似的，所以采用了第2种方法，优点是不需要训练参数，而且即使<strong>在训练集中没有出现过的句子长度上也能用</strong>。Positional Encoding的公式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
P E_{(p o s, 2 i)} &=\sin \left(p o s / 10000^{2 i / d_{\text {model }}}\right) \\
P E_{(p o s, 2 i+1)} &=\cos \left(p o s / 10000^{2 i / d_{\text {model }}}\right.
\end{aligned}</script><p>其中，$pos$指的是这个 word 在这个句子中的位置；$2i$指的是 embedding 词向量的偶数维度，$2i+1$指的是embedding 词向量的奇数维度。具体实现如下：</p>
<figure class="highlight python"><figcaption><span>PositionalEncoding</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>) <span class="comment"># [max_len, 1]</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * -(math.log(<span class="number">10000.0</span>) / d_model)) <span class="comment"># ?</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term) <span class="comment"># 偶数列</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term) <span class="comment"># 奇数列</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)         <span class="comment"># [1, max_len, d_model]</span></span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe) <span class="comment"># 上述代码只需计算一次，然后放到寄存器里，随用随取。</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x) <span class="comment"># 据说dropout=0.1</span></span><br></pre></td></tr></table></figure>
<p><code>x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)</code> 这行代码表示；输入模型的整个Embedding是Word Embedding与Positional Embedding直接相加之后的结果。</p>
<p>为什么上面的两个公式能体现单词的相对位置信息呢？</p>
<p>下面一段代码取词向量的4个维度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在位置编码下方，将基于位置添加正弦波。对于每个维度，波的频率和偏移都不同。</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line">y = pe.forward(Variable(torch.zeros(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>))) <span class="comment"># [bs=1,seq_len=100,embed_size=20]</span></span><br><span class="line">plt.plot(np.arange(<span class="number">100</span>), y[<span class="number">0</span>, :, <span class="number">4</span>:<span class="number">8</span>].data.numpy())</span><br><span class="line">plt.legend([<span class="string">"dim %d"</span> %p <span class="keyword">for</span> p <span class="keyword">in</span> [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/11/transformer/position_embedding.png" alt></p>
<p>可以看到某个序列中不同位置的单词，在某一维度上的位置编码数值不一样，即同一序列的不同单词在单个纬度符合某个正弦或者余弦，可认为他们的具有相对关系。</p>
<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>Encoder部分是由个层相同小Encoder Layer串联而成。小Encoder Layer可以简化为两个部分：</p>
<ol>
<li>Multi-Head Self Attention</li>
<li>Feed-Forward Network</li>
</ol>
<p><code>Multi-Head Self Attention</code> 和<code>Feed-Forward Network</code>之后都接了一层<code>Add</code> 和<code>Norm</code></p>
<p>示意图如下:</p>
<p><img src="/2020/05/11/transformer/encoder_layer.png" alt></p>
<h3 id="Muti-Head-Attention"><a href="#Muti-Head-Attention" class="headerlink" title="Muti-Head-Attention"></a>Muti-Head-Attention</h3><p>Multi-Head Self Attention 实际上是<strong>由h个Self Attention 层并行组成，原文中h=8</strong>。</p>
<h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h4><p>self-attention的输入是序列词向量<code>x</code>。<code>x</code>经过一个线性变换得到<code>query(Q)</code>, <code>x</code>经过第二个线性变换得到<code>key(K)</code>,<code>x</code>经过第三个线性变换得到<code>value(V)</code>。也就是：</p>
<ul>
<li>Q = linear_q(x)</li>
<li>K = linear_k(x)</li>
<li>V = linear_v(x)</li>
</ul>
<p>即：</p>
<p><img src="/2020/05/11/transformer/qkv.png" alt></p>
<p>linear_k, linear_q, linear_v是相互独立、权重$𝑊^𝑄,𝑊^𝐾,W^V$)是不同的，通过训练可得到。得到query(Q)，key(K)，value(V)之后按照下面的公式计算attention(Q, K, V)：</p>
<script type="math/tex; mode=display">
\text {Attention}(Q, K, V)=\text {Softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><p><img src="/2020/05/11/transformer/attention.png" alt></p>
<p>这里<code>Z</code>就是<code>attention(Q, K, V)</code>，$𝑑_𝑘=\frac{𝑑_{𝑚𝑜𝑑𝑒𝑙}}{ℎ}=\frac{512}{8}=64$。</p>
<ol>
<li><p>为什么要用$\sqrt{d_k}$ 对 $𝑄𝐾^𝑇$进行缩放呢？</p>
<p>$d_k$实际上是<code>Q/K/V</code>的最后一个维度，当$d_k$越大，$QK^T$就越大，可能会将softmax函数推入梯度极小的区域。</p>
</li>
<li><p>softmax之后值都介于0到1之间，可以理解成得到了 attention weights。然后基于这个 attention weights 对 V 求 weighted sum 值 Attention(Q, K, V)。 </p>
</li>
</ol>
<p>Multi-Head-Attention 就是将<code>embedding</code>之后的X按维度$𝑑_{𝑚𝑜𝑑𝑒𝑙}=512$ 切割成$ℎ=8$个，分别做self-attention之后再合并在一起。</p>
<p><img src="/2020/05/11/transformer/attention_1.png" alt></p>
<figure class="highlight python"><figcaption><span>Attention</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute 'Scaled Dot Product Attention</span></span><br><span class="line"><span class="string">    qkv :[batch, h, seq_len, embed_size/h(d_k)]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)  <span class="comment"># mask </span></span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)     <span class="comment"># dropout=0.1</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/11/transformer/multi_head.png" alt></p>
<figure class="highlight python"><figcaption><span>MultiHeadedAttention</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        实现MultiHeadedAttention。</span></span><br><span class="line"><span class="string">           输入的q，k，v是形状 [batch, seq_len, embed_size(d_model)]。</span></span><br><span class="line"><span class="string">           输出的x 的形状同上。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        <span class="comment">#    [batch, seq_len, embed_size] -&gt;[batch, h, seq_len, embed_size/h]</span></span><br><span class="line">        query, key, value = [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch.</span></span><br><span class="line">        <span class="comment">#    计算注意力attn 得到attn*v 与attn </span></span><br><span class="line">        <span class="comment">#    qkv :[batch, h, seq_len, embed_size/h] --&gt;</span></span><br><span class="line">        <span class="comment">#              x:[batch, h, seq_len, embed_size/h], attn[batch, h, seq_len, seq_len]</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) "Concat" using a view and apply a final linear.</span></span><br><span class="line">        <span class="comment">#     上一步的结果合并在一起还原成原始输入序列的形状</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)  <span class="comment"># 最后再过一个线性层</span></span><br></pre></td></tr></table></figure>
<h4 id="Self-Attention的优点"><a href="#Self-Attention的优点" class="headerlink" title="Self-Attention的优点"></a>Self-Attention的优点</h4><ol>
<li>因为每个词都和周围所有词做attention，所以任意两个位置都相当于有直连线路，可捕获长距离依赖。</li>
<li>Attention的可解释性更好，根据Attention score可以知道一个词和哪些词的关系比较大。</li>
<li>易于并行化，当前层的Attention计算只和前一层的值有关，所以一层的所有节点可并行执行self-attention操作。计算效率高，一次Self-Attention只需要两次矩阵运算，速度很快。</li>
</ol>
<h3 id="Add-amp-Norm"><a href="#Add-amp-Norm" class="headerlink" title="Add &amp; Norm"></a>Add &amp; Norm</h3><p><code>x</code> 序列经过<code>Multi-Head-Self-Attention</code> 之后实际经过一个<code>Add+Norm</code>层，再进入<code>feed-forward network(FFN)</code>，在<code>FFN</code>之后又经过一个<code>norm</code>再输入下一个encoder layer。几乎每个sub layer之后都会经过一个归一化，然后再加在原来的输入上。</p>
<figure class="highlight python"><figcaption><span>Norm</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features)) <span class="comment"># ?</span></span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features)) <span class="comment"># ?</span></span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Add</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="string">"Apply residual connection to any sublayer with the same size."</span></span><br><span class="line">        <span class="comment"># sublayer &lt;-- encoder layer</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure>
<h3 id="Feed-Forward-Network"><a href="#Feed-Forward-Network" class="headerlink" title="Feed-Forward Network"></a>Feed-Forward Network</h3><p>Feed-Forward Network可以细分为有两层，第一层是一个线性激活函数，第二层是激活函数是ReLU。可以表示为：</p>
<script type="math/tex; mode=display">
F F N=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}</script><figure class="highlight python"><figcaption><span>PositionwiseFeedForward</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Implements FFN equation.</span></span><br><span class="line"><span class="string">    positionwise体现在哪里？？</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">      <span class="string">"""</span></span><br><span class="line"><span class="string">      d_model: embedding_size</span></span><br><span class="line"><span class="string">      d_ff: 线形层输出维度</span></span><br><span class="line"><span class="string">      """</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure>
<h3 id="组合出Encoder"><a href="#组合出Encoder" class="headerlink" title="组合出Encoder"></a>组合出Encoder</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span><span class="params">(module, N)</span>:</span></span><br><span class="line">    <span class="string">"Produce N identical layers."</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)  <span class="comment"># sub layer</span></span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Pass the input (and mask) through each layer in turn."</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>总的来说Encoder 是由上述小encoder layer 6个串行叠加组成。encoder sub layer主要包含两个部分：</p>
<ul>
<li>SubLayer-1 做 Multi-Headed Attention</li>
<li>SubLayer-2 做 Feed Forward Neural Network</li>
</ul>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>Decoder与Encoder有所不同，Encoder与Decoder的关系可以用下图描述：</p>
<p><img src="/2020/05/11/transformer/decoder.png" alt></p>
<figure class="highlight python"><figcaption><span>Decoder</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Generic N layer decoder with masking."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span> </span><br><span class="line">      	<span class="comment"># memory Encoder最后的输出。 </span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<p>Decoder 子结构（sub layer）：</p>
<p><img src="/2020/05/11/transformer/decoder_layer.png" alt></p>
<p>Decoder 也是N=6层堆叠的结构。被分为3个 SubLayer，Encoder与Decoder有三大主要的不同：</p>
<ol>
<li>Decoder SubLayer-1 使用的是 “Masked” Multi-Headed Attention 机制，防止为了模型看到要预测的数据，防止泄露。</li>
<li>SubLayer-2 是一个 Encoder-Decoder Multi-head Attention。</li>
<li>LinearLayer 和 SoftmaxLayer 作用于 SubLayer-3 的输出后面，来预测对应的 word 的 probabilities 。</li>
</ol>
<h3 id="Mask-Multi-Head-Attention"><a href="#Mask-Multi-Head-Attention" class="headerlink" title="Mask-Multi-Head-Attention"></a>Mask-Multi-Head-Attention</h3><p>Mask 的目的是防止 Decoder “seeing the future”，防止提前知道预测的内容（ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$.）,这也说明Transformer只是在Encoder阶段可以并行化，Decoder阶段依然要一个个词顺序翻译，依然是<strong>串行</strong>的。这里mask是一个下三角矩阵，对角线以及对角线左下都是1，其余都是0。</p>
<figure class="highlight python"><figcaption><span>subsequent_mask</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="string">""""Mask out subsequent positions.</span></span><br><span class="line"><span class="string">    mask后续的位置，返回[size, size]尺寸下三角Tensor</span></span><br><span class="line"><span class="string">    对角线及其左下角全是1，右上角全是0</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    <span class="comment"># np.triu()  Upper triangle of an array: 返回矩阵上三角</span></span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span> <span class="comment"># 等于0，返回的是矩阵的下三角</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.imshow(subsequent_mask(<span class="number">20</span>)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="/2020/05/11/transformer/subsequent_mask.png" alt></p>
<p>subsequent_mask 返回结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]], dtype=torch.uint8)</span><br></pre></td></tr></table></figure>
<p>当mask不为空的时候，attention计算需要将x做一个操作：scores = scores.masked_fill(mask == 0, -1e9)。即将mask==0的替换为-1e9,其余不变。</p>
<h3 id="Encoder-Decoder-Multi-head-Attention"><a href="#Encoder-Decoder-Multi-head-Attention" class="headerlink" title="Encoder-Decoder Multi-head Attention"></a>Encoder-Decoder Multi-head Attention</h3><p>这部分和Multi-head Attention的区别是该层的输入来自encoder和上一次decoder的结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (right) for connections."</span></span><br><span class="line">      	<span class="comment"># 将decoder的三个Sublayer串联起来</span></span><br><span class="line">        m = memory <span class="comment"># 为encoder 最后的输出</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>
<h3 id="Linear-and-Softmax-to-Produce-Output-Probabilities"><a href="#Linear-and-Softmax-to-Produce-Output-Probabilities" class="headerlink" title="Linear and Softmax to Produce Output Probabilities"></a>Linear and Softmax to Produce Output Probabilities</h3><p>Decoder的最后一个部分是过一个linear layer将decoder的输出扩展到与vocabulary size一样的维度上。经过softmax 后，选择概率最高的一个word作为预测结果。假设我们有一个已经训练好的网络，在做预测时，步骤如下：</p>
<ol>
<li>给 decoder 输入 encoder 对整个句子 embedding 的结果和一个特殊的开始符号 &lt;/s&gt;。decoder 将产生预测，在例子中应该是 <code>I</code>。</li>
<li>给 decoder 输入 encoder 的 embedding 结果和 “&lt;/s&gt; I”，在这一步 decoder 应该产生预测 <code>am</code>。</li>
<li>给 decoder 输入 encoder 的 embedding 结果和 “&lt;/s&gt; I am”，在这一步 decoder 应该产生预测 <code>a</code>。</li>
<li>给 decoder 输入 encoder 的 embedding 结果和 “&lt;/s&gt; I am a”，在这一步 decoder 应该产生预测 <code>student</code>。</li>
<li>给 decoder 输入 encoder 的 embedding 结果和 “&lt;/s&gt;I am a student”, decoder应该生成句子结尾的标记，decoder 应该输出 <code>&lt;/eos&gt;</code>。</li>
<li>然后 decoder 生成了 &lt;/eos&gt;，翻译完成。</li>
</ol>
<p>这部分的代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Define standard linear + softmax generation step."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">    		<span class="string">"""</span></span><br><span class="line"><span class="string">    		d_model:decoder 输出的embedding size</span></span><br><span class="line"><span class="string">    		vocab：目标的词汇表大小</span></span><br><span class="line"><span class="string">    		"""</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="EncoderDecoder"><a href="#EncoderDecoder" class="headerlink" title="EncoderDecoder"></a>EncoderDecoder</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many </span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed <span class="comment"># embedding</span></span><br><span class="line">        self.tgt_embed = tgt_embed <span class="comment"># embedding</span></span><br><span class="line">        self.generator = generator <span class="comment"># 用于生成翻译目标（Linear + softmax --&gt; prob）</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Take in and process masked src and target sequences."</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure>
<h2 id="Full-Model"><a href="#Full-Model" class="headerlink" title="Full Model"></a>Full Model</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout) <span class="comment"># linear(relu(linear(x)))</span></span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><p>这部分主要根据自己的理解对代码加注释</p>
<h2 id="Batches-and-Masking"><a href="#Batches-and-Masking" class="headerlink" title="Batches and Masking"></a>Batches and Masking</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Batch</span>:</span></span><br><span class="line">    <span class="string">"Object for holding a batch of data with mask during training."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, src, trg=None, pad=<span class="number">0</span>)</span>:</span></span><br><span class="line">        self.src = src</span><br><span class="line">        self.src_mask = (src != pad).unsqueeze(<span class="number">-2</span>) <span class="comment"># 对超出句子长度部分mask</span></span><br><span class="line">        <span class="keyword">if</span> trg <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.trg = trg[:, :<span class="number">-1</span>]</span><br><span class="line">            self.trg_y = trg[:, <span class="number">1</span>:]</span><br><span class="line">            self.trg_mask = self.make_std_mask(self.trg, pad) <span class="comment"># mask to hide padding and future words</span></span><br><span class="line">            self.ntokens = (self.trg_y != pad).data.sum()</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_std_mask</span><span class="params">(tgt, pad)</span>:</span></span><br><span class="line">        <span class="string">"Create a mask to hide padding and future words."</span></span><br><span class="line">        tgt_mask = (tgt != pad).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">        tgt_mask = tgt_mask &amp; Variable(</span><br><span class="line">            subsequent_mask(tgt.size(<span class="number">-1</span>)).type_as(tgt_mask.data))</span><br><span class="line">        <span class="keyword">return</span> tgt_mask</span><br></pre></td></tr></table></figure>
<h2 id="Training-Loop"><a href="#Training-Loop" class="headerlink" title="Training Loop"></a>Training Loop</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(data_iter, model, loss_compute)</span>:</span></span><br><span class="line">    <span class="string">"Standard Training and Logging Function"</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    total_tokens = <span class="number">0</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    tokens = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(data_iter):</span><br><span class="line">        out = model.forward(batch.src, batch.trg, </span><br><span class="line">                            batch.src_mask, batch.trg_mask)</span><br><span class="line">        loss = loss_compute(out, batch.trg_y, batch.ntokens)</span><br><span class="line">        total_loss += loss</span><br><span class="line">        total_tokens += batch.ntokens</span><br><span class="line">        tokens += batch.ntokens</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">1</span>:</span><br><span class="line">            elapsed = time.time() - start</span><br><span class="line">            print(<span class="string">"Epoch Step: %d Loss: %f Tokens per Sec: %f"</span> %</span><br><span class="line">                    (i, loss / batch.ntokens, tokens / elapsed))</span><br><span class="line">            start = time.time()</span><br><span class="line">            tokens = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> total_loss / total_tokens</span><br></pre></td></tr></table></figure>
<h2 id="Training-Data"><a href="#Training-Data" class="headerlink" title="Training Data"></a>Training Data</h2><p>使用标准WMT 2014英语-德语数据集进行了训练，该数据集包含大约450万个句子对。 使用字节对的编码方法对句子进行编码，该编码具有大约37000个词的共享源-目标词汇表。 对于英语-法语，使用了WMT 2014 英语-法语数据集，该数据集由36M个句子组成，并将词分成32000个词片(Word-piece)的词汇表。句子对按照近似的序列长度进行批处理。每个训练批包含一组句子对，包含大约25000个源词和25000个目标词。</p>
<p>使用torch text来创建batch。在torchtext的一个函数中创建batch，确保填充到最大batch训练长度的大小不超过阈值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">global</span> max_src_in_batch, max_tgt_in_batch</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_size_fn</span><span class="params">(new, count, sofar)</span>:</span></span><br><span class="line">    <span class="string">"Keep augmenting batch and calculate total number of tokens + padding."</span></span><br><span class="line">    <span class="keyword">global</span> max_src_in_batch, max_tgt_in_batch</span><br><span class="line">    <span class="keyword">if</span> count == <span class="number">1</span>: <span class="comment"># ？？</span></span><br><span class="line">        max_src_in_batch = <span class="number">0</span></span><br><span class="line">        max_tgt_in_batch = <span class="number">0</span></span><br><span class="line">    max_src_in_batch = max(max_src_in_batch,  len(new.src))</span><br><span class="line">    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + <span class="number">2</span>)</span><br><span class="line">    src_elements = count * max_src_in_batch</span><br><span class="line">    tgt_elements = count * max_tgt_in_batch</span><br><span class="line">    <span class="keyword">return</span> max(src_elements, tgt_elements)</span><br></pre></td></tr></table></figure>
<h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><p>选择Adam作为优化器，其参数为$\beta_1 = 0.9, \beta_2=0.98,\epsilon=10^{-9}$。根据$lrate = d_{model}^{- \frac{1}{2}} * min(step_num^{- \frac{1}{2}},step_num \cdot warmup_steps^{-1.5})$，在训练过程中改变了学习率。在<code>warm_up</code>中随步数线性地增加学习速率，随后与步数的反平方根成比例地减小它。预热<code>warmup_steps</code>为4000。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NoamOpt</span>:</span></span><br><span class="line">    <span class="string">"Optim wrapper that implements rate."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model_size, factor, warmup, optimizer)</span>:</span></span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self._step = <span class="number">0</span></span><br><span class="line">        self.warmup = warmup</span><br><span class="line">        self.factor = factor</span><br><span class="line">        self.model_size = model_size</span><br><span class="line">        self._rate = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"Update parameters and rate"</span></span><br><span class="line">        self._step += <span class="number">1</span></span><br><span class="line">        rate = self.rate()</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.optimizer.param_groups:</span><br><span class="line">            p[<span class="string">'lr'</span>] = rate</span><br><span class="line">        self._rate = rate</span><br><span class="line">        self.optimizer.step()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rate</span><span class="params">(self, step = None)</span>:</span></span><br><span class="line">        <span class="string">"Implement `lrate` above"</span></span><br><span class="line">        <span class="keyword">if</span> step <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            step = self._step</span><br><span class="line">        <span class="comment"># 对应上面公式</span></span><br><span class="line">        <span class="keyword">return</span> self.factor * (self.model_size ** (<span class="number">-0.5</span>) * min(step ** (<span class="number">-0.5</span>), step * self.warmup ** (<span class="number">-1.5</span>)))</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_std_opt</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">2</span>, <span class="number">4000</span>,torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">opts = [NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="literal">None</span>), </span><br><span class="line">        NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">8000</span>, <span class="literal">None</span>),</span><br><span class="line">        NoamOpt(<span class="number">256</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="literal">None</span>)]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">20000</span>), [[opt.rate(i) <span class="keyword">for</span> opt <span class="keyword">in</span> opts] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">20000</span>)])</span><br><span class="line">plt.legend([<span class="string">"512:4000"</span>, <span class="string">"512:8000"</span>, <span class="string">"256:4000"</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/11/transformer/warm_up.png" alt="screenshot"></p>
<p>也就是说：</p>
<ol>
<li>在embedding size相同的情况下，warm up步数越少，前期的学习率曲线越陡峭，学的越快。</li>
<li>在warm up步数相同的时候，embedding size 越小，前期的学习率曲线越陡峭，学的越快。</li>
</ol>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><h3 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h3><h4 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h4><p>在多分类训练任务中，输入经过神级网络的计算，会得到当前输入对应于各个类别的置信度分数，这些分数会被softmax进行归一化处理，最终得到当前输入属于每个类别的概率。</p>
<script type="math/tex; mode=display">
q_{i}=\frac{\exp \left(z_{i}\right)}{\sum_{j=1}^{K} \exp \left(z_{j}\right)}</script><p>之后在使用交叉熵函数来计算损失值：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&L o s s=-\sum_{i=1}^{K} p_{i} \log q_{i}\\
&p_{i}=\left\{\begin{array}{l}
1, \text { if }(i=y) \\
0, i f(i \neq y)
\end{array}\right.
\end{aligned}</script><p>其中，i表示多类中的某一类。</p>
<p>最终在训练网络时，最小化预测概率和标签真实概率的交叉熵，从而得到最优的预测概率分布。在此过程中，为了达到最好的拟合效果，最优的预测概率分布为：</p>
<script type="math/tex; mode=display">
Z_{i}=\left\{\begin{array}{l}
+\infty, i f(i=y) \\
0, i f(i \neq y)
\end{array}\right.</script><p>也就是说，网络会驱使自身往正确标签和错误标签差值大的方向学习，在训练数据不足以表征所以的样本特征的情况下，就<strong>会导致网络过拟合</strong>。</p>
<h4 id="label-smoothing原理"><a href="#label-smoothing原理" class="headerlink" title="label smoothing原理"></a>label smoothing原理</h4><p>label smoothing的提出就是为了解决上述问题，是一种正则化的策略。其通过”软化”传统的one-hot类型标签，使得在计算损失值时能够有效抑制过拟合现象。label smoothing相当于减少真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。</p>
<p>1.label smoothing将真实概率分布作如下改变：</p>
<script type="math/tex; mode=display">
P_{i}=\left\{\begin{array}{l}
1, i f(i=y) \\
0, i f(i \neq y)
\end{array} \Longrightarrow  P_{i}=\left\{\begin{array}{l}
(1-\varepsilon), i f(i=y) \\
\frac{\varepsilon}{K-1}, i f(i \neq y)
\end{array}\right.\right.</script><p>其实更新后的分布就相当于往真实分布中加入了噪声，为了便于计算，该噪声服从简单的均匀分布。</p>
<p>2.与之对应，label smoothing将交叉熵损失函数作如下改变：</p>
<script type="math/tex; mode=display">
L o s s=-\sum_{i=1}^{K} p_{i} \log q_{i} \Longrightarrow \operatorname{Loss}_{i}=\left\{\begin{array}{l}
(1-\varepsilon)^{*} \operatorname{Loss}, \text {if}(i=y) \\
\varepsilon^{*} \operatorname{Loss}, \text {if}(i \neq y)
\end{array}\right.</script><p>3.与之对应，label smoothing将最优的预测概率分布作如下改变：</p>
<script type="math/tex; mode=display">
Z_{i}=\left\{\begin{array}{l}
+\infty, i f(i=y) \\
0, i f(i \neq y)
\end{array} \Longrightarrow Z_{i}=\left\{\begin{array}{l}
\log \frac{(k-1)(1-\varepsilon)}{\varepsilon+\alpha}, i f(i=y) \\
\alpha, i f(i \neq y)
\end{array}\right.\right.</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelSmoothing</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement label smoothing."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, padding_idx, smoothing=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(size_average=<span class="literal">False</span>)</span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, target)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>)) <span class="comment"># ?</span></span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)</span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, Variable(true_dist, requires_grad=<span class="literal">False</span>))</span><br></pre></td></tr></table></figure>
<p>在训练期间，采用了值 $\epsilon_{ls}=0.1$的标签平滑。 这种做法提高了困惑度，因为模型变得更加不确定，但提高了准确性和BLEU分数。使用KL div loss实现标签平滑。 相比使用独热目标分布，其包含正确单词的置信度和整个词汇表中分布的其余平滑项。可以看到标签平滑的示例:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Example of label smoothing.</span></span><br><span class="line"><span class="comment"># embed_size = 5, padding_idx=0,smoothing=0.4</span></span><br><span class="line">crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.4</span>)</span><br><span class="line">predict = torch.FloatTensor([[<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">                             [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>], </span><br><span class="line">                             [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>]])</span><br><span class="line">v = crit(Variable(predict.log()), </span><br><span class="line">         Variable(torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the target distributions expected by the system.</span></span><br><span class="line">plt.imshow(crit.true_dist)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/11/transformer/label_smooth.png" alt="screenshot"></p>
<p>Emmm,上图没看懂？？？？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(x)</span>:</span></span><br><span class="line">    d = x + <span class="number">3</span> * <span class="number">1</span></span><br><span class="line">    predict = torch.FloatTensor([[<span class="number">0</span>, x / d, <span class="number">1</span> / d, <span class="number">1</span> / d, <span class="number">1</span> / d],</span><br><span class="line">                                 ])</span><br><span class="line">    <span class="comment">#print(predict)</span></span><br><span class="line">    <span class="keyword">return</span> crit(Variable(predict.log()),</span><br><span class="line">                 Variable(torch.LongTensor([<span class="number">1</span>]))).data[<span class="number">0</span>]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">100</span>), [loss(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>)])</span><br></pre></td></tr></table></figure>
<p>如果对给定的选择非常有信心，标签平滑实际上会开始惩罚模型。???怎么看出来的？我没看懂！</p>
<p><img src="/2020/05/11/transformer/penate.png" alt="screenshot"></p>
<h2 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h2><h3 id="Copy-Task"><a href="#Copy-Task" class="headerlink" title="Copy Task"></a>Copy Task</h3><h4 id="Synthetic-Data"><a href="#Synthetic-Data" class="headerlink" title="Synthetic Data"></a>Synthetic Data</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_gen</span><span class="params">(V, batch, nbatches)</span>:</span></span><br><span class="line">    <span class="string">"Generate random data for a src-tgt copy task."</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nbatches):</span><br><span class="line">        data = torch.from_numpy(np.random.randint(<span class="number">1</span>, V, size=(batch, <span class="number">10</span>)))</span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        src = Variable(data, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        tgt = Variable(data, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> Batch(src, tgt, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Loss-Computation"><a href="#Loss-Computation" class="headerlink" title="Loss Computation"></a>Loss Computation</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleLossCompute</span>:</span></span><br><span class="line">    <span class="string">"A simple loss compute and train function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, generator, criterion, opt=None)</span>:</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        self.criterion = criterion</span><br><span class="line">        self.opt = opt</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x, y, norm)</span>:</span></span><br><span class="line">        x = self.generator(x)</span><br><span class="line">        loss = self.criterion(x.contiguous().view(<span class="number">-1</span>, x.size(<span class="number">-1</span>)), </span><br><span class="line">                              y.contiguous().view(<span class="number">-1</span>)) / norm</span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">if</span> self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.opt.step()</span><br><span class="line">            self.opt.optimizer.zero_grad()</span><br><span class="line">        <span class="keyword">return</span> loss.data[<span class="number">0</span>] * norm</span><br></pre></td></tr></table></figure>
<h4 id="Greedy-Decoding"><a href="#Greedy-Decoding" class="headerlink" title="Greedy Decoding"></a>Greedy Decoding</h4><p>贪心解码。。。大佬们真会起名字！！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Train the simple copy task.</span></span><br><span class="line">V = <span class="number">11</span></span><br><span class="line">criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line">model = make_model(V, V, N=<span class="number">2</span>)</span><br><span class="line">model_opt = NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">1</span>, <span class="number">400</span>,</span><br><span class="line">        torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    run_epoch(data_gen(V, <span class="number">30</span>, <span class="number">20</span>), model, </span><br><span class="line">              SimpleLossCompute(model.generator, criterion, model_opt))</span><br><span class="line">    model.eval()</span><br><span class="line">    print(run_epoch(data_gen(V, <span class="number">30</span>, <span class="number">5</span>), model, </span><br><span class="line">                    SimpleLossCompute(model.generator, criterion, <span class="literal">None</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greedy_decode</span><span class="params">(model, src, src_mask, max_len, start_symbol)</span>:</span></span><br><span class="line">    memory = model.encode(src, src_mask)</span><br><span class="line">    ys = torch.ones(<span class="number">1</span>, <span class="number">1</span>).fill_(start_symbol).type_as(src.data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_len<span class="number">-1</span>):</span><br><span class="line">        out = model.decode(memory, src_mask, </span><br><span class="line">                           Variable(ys), </span><br><span class="line">                           Variable(subsequent_mask(ys.size(<span class="number">1</span>))</span><br><span class="line">                                    .type_as(src.data)))</span><br><span class="line">        prob = model.generator(out[:, <span class="number">-1</span>])</span><br><span class="line">        _, next_word = torch.max(prob, dim = <span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat([ys, </span><br><span class="line">                        torch.ones(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ys</span><br><span class="line"></span><br><span class="line">model.eval()</span><br><span class="line">src = Variable(torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]]) )</span><br><span class="line">src_mask = Variable(torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>) )</span><br><span class="line">print(greedy_decode(model, src, src_mask, max_len=<span class="number">10</span>, start_symbol=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>result</span></figcaption><table><tr><td class="code"><pre><span class="line">  <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span>     <span class="number">4</span>     <span class="number">5</span>     <span class="number">6</span>     <span class="number">7</span>     <span class="number">8</span>     <span class="number">9</span>    <span class="number">10</span></span><br><span class="line">[torch.LongTensor of size <span class="number">1</span>x10]</span><br></pre></td></tr></table></figure>
<p>翻译的例子涉及GPU并行比较复杂，不做介绍。</p>
<h1 id="Attention-Visualization"><a href="#Attention-Visualization" class="headerlink" title="Attention Visualization"></a>Attention Visualization</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tgt_sent = trans.split()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(data, x, y, ax)</span>:</span></span><br><span class="line">    seaborn.heatmap(data, </span><br><span class="line">                    xticklabels=x, square=<span class="literal">True</span>, yticklabels=y, vmin=<span class="number">0.0</span>, vmax=<span class="number">1.0</span>, </span><br><span class="line">                    cbar=<span class="literal">False</span>, ax=ax)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    print(<span class="string">"Encoder Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.encoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data, sent, sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    print(<span class="string">"Decoder Self Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.decoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data[:len(tgt_sent), :len(tgt_sent)], tgt_sent, tgt_sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"Decoder Src Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.decoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data[:len(tgt_sent), :len(sent)],sent, tgt_sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="Encoder-visualization"><a href="#Encoder-visualization" class="headerlink" title="Encoder visualization"></a>Encoder visualization</h2><div class="note info">
            <p>Encoder Layer 2</p>
          </div>
<p><img src="/2020/05/11/transformer/encoder_layer_2.png" alt="screenshot"></p>
<div class="note info">
            <p>Encoder Layer 2</p>
          </div>
<p><img src="/2020/05/11/transformer/encoder_layer_4.png" alt="screenshot"></p>
<div class="note info">
            <p>Encoder Layer 6，这一层说明了啥？？</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511115252714.png" alt="image-20200511115252714"></p>
<p>同一行，比较不同的head,可以看出，不同的head，attention到的内容是各不相同的。</p>
<p>不同行比较的结论？</p>
<h2 id="Decoder-visualization"><a href="#Decoder-visualization" class="headerlink" title="Decoder visualization"></a>Decoder visualization</h2><div class="note info">
            <p>decoder Self Layer 2 :<s>会attention到所有单词，单词大多会attention到自己。</s></p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511120009561.png" alt="image-20200511120009561"></p>
<div class="note info">
            <p>decoder Src Layer 2</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511120705604.png" alt="image-20200511120705604"></p>
<div class="note info">
            <p>decoder Self Layer 4</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511120927309.png" alt="image-20200511120927309"></p>
<div class="note info">
            <p>decoder Src Layer 4</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511120946538.png" alt="image-20200511120946538"></p>
<div class="note info">
            <p>decoder Self Layer 6</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511121002344.png" alt="image-20200511121002344"></p>
<div class="note info">
            <p>decoder Src Layer 6</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511121021648.png" alt="image-20200511121021648"></p>
<p>同一行 ？</p>
<p>不同行？</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="tricks"><a href="#tricks" class="headerlink" title="tricks"></a>tricks</h2><p>在训练过程中，模型没有收敛得很好时，Decoder预测产生的词很可能不是我们想要的。这个时候如果再把错误的数据再输给Decoder，就会越跑越偏。这个时候怎么办？</p>
<ul>
<li>在训练过程中可以使用 “teacher forcing”。因为我们知道应该预测的word是什么，那么可以给Decoder喂一个正确的结果作为输入。</li>
<li>除了选择最高概率的词 (greedy search)，还可以选择是比如 “Beam Search”，可以保留topK个预测的word。 Beam Search 方法不再是只得到一个输出放到下一步去训练了，我们可以设定一个值，拿多个值放到下一步去训练，这条路径的概率等于每一步输出的概率的乘积。</li>
</ul>
<h2 id="Transformer的优缺点"><a href="#Transformer的优缺点" class="headerlink" title="Transformer的优缺点"></a>Transformer的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol>
<li>每层计算复杂度比RNN要低。</li>
<li>可以进行<strong>并行计算</strong>。</li>
<li>从计算一个序列长度为n的信息要经过的路径长度来看, CNN需要增加卷积层数来扩大视野，RNN需要从1到n逐个进行计算，而Self-attention只需要一步矩阵计算就可以。Self-Attention可以比RNN更好地解决长时依赖问题。当然如果计算量太大，比如序列长度N大于序列维度D这种情况，也可以用窗口限制Self-Attention的计算数量。？？？</li>
<li>从作者在附录中给出的栗子可以看出，Self-Attention模型更可解释，Attention结果的分布表明了该模型学习到了一些语法和语义信息。</li>
</ol>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol>
<li>实践上：有些RNN轻易可以解决的问题transformer没做到，比如<strong>复制string</strong>，或者推理时碰到的sequence长度比训练时更长（因为<strong>碰到了没见过的position embedding</strong>）。</li>
<li>理论上：transformers不是computationally universal(图灵完备)，这种非RNN式的模型是非图灵完备的的，<strong>无法单独完成NLP中推理、决策等计算问题</strong>（包括使用transformer的bert模型等等）。？？？</li>
</ol>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">The Annotated Transformer</a></li>
<li><a href="https://www.cnblogs.com/zingp/p/11696111.html" target="_blank" rel="noopener">深入理解Transformer及其源码</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>分词算法</title>
    <url>/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="分词简介"><a href="#分词简介" class="headerlink" title="分词简介"></a>分词简介</h3><p>中文分词算法是指将一个汉字序列切分成一个一个单独的词，与英文以空格作为天然的分隔符不同，中文字符在语义识别时，需要把数个字符组合成词，才能表达出真正的含义。分词算法通常应用于自然语言处理、搜索引擎、智能推荐等领域。</p>
<p>分词算法根据其核心思想主要分为两种，第一种是基于词典的分词，先把句子按照字典切分成词，再寻找词的最佳组合方式；第二种是基于字的分词，即由字构词，先把句子分成一个个字，再将字组合成词，寻找最优的切分策略，同时也可以转化成序列标注问题。</p>
<p>其分类大致可分为：</p>
<ol>
<li><p>基于匹配规则的方法</p>
<ul>
<li>正向最大匹配法(forward maximum matching method, FMM)</li>
<li>逆向最大匹配法(backward maximum matching method, BMM)</li>
<li>最短路径分词算法</li>
</ul>
</li>
<li><p>基于统计以及机器学习的分词方法</p>
<ul>
<li>基于N-gram语言模型的分词方法</li>
<li>基于HMM的分词方法</li>
<li>基于CRF的分词方法</li>
<li>基于词感知机的分词方法</li>
<li>基于深度学习的端到端的分词方法</li>
</ul>
</li>
</ol>
<p>基于规则匹配的分词通常会加入一些启发式规则，比如“正向/反向最大匹配”，“长词优先”等。</p>
<p>基于统计以及机器学习的分词方法，它们基于人工标注的词性和统计特征，对中文进行建模，即根据观测到的数据(标注好的语料)对模型参数进行训练，在分词阶段再通过模型计算各种分词出现的概率，将概率最大的分词结果作为最终结果。这类分词算法能很好处理歧义和未登录词问题，效果比基于规则匹配的方法效果好，但是需要大量的人工标注数据，以及较慢的分词速度。</p>
<p><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码及数据</a></p>
<a id="more"></a>
<h4 id="中文分词的应用"><a href="#中文分词的应用" class="headerlink" title="中文分词的应用"></a>中文分词的应用</h4><p>目前在自然语言处理技术中，中文处理技术比西文处理技术要落后很大一段距离，许多西文的处理方法中文不能直接采用，就是因为中文必需有分词这道工序。中文分词是其他中文信息处理的基础，搜索引擎只是中文分词的一个应用。其他的比如机器翻译（MT）、语音合成、自动分类、自动摘要、自动校对等等，都需要用到分词。因为中文需要分词，可能会影响一些研究，但同时也为一些企业带来机会，因为国外的计算机处理技术要想进入中国市场，首先也是要解决中文分词问题。在中文研究方面，相比外国人来说，中国人有十分明显的优势。</p>
<p>分词准确性对搜索引擎来说十分重要，但如果分词速度太慢，即使准确性再高，对于搜索引擎来说也是不可用的，因为搜索引擎需要处理数以亿计的网页，如果分词耗用的时间过长，会严重影响搜索引擎内容更新的速度。因此对于搜索引擎来说，分词的准确性和速度，二者都需要达到很高的要求。目前研究中文分词的大多是科研院校，清华、北大、中科院、北京语言学院、东北大学、IBM研究院、微软中国研究院等都有自己的研究队伍，而真正专业研究中文分词的商业公司除了海量科技以外，几乎没有。</p>
<h3 id="基于匹配规则的方法"><a href="#基于匹配规则的方法" class="headerlink" title="基于匹配规则的方法"></a>基于匹配规则的方法</h3><p>主要是人工建立词库也叫做词典，通过词典匹配的方式对句子进行划分。其实现简单高效，但是对未登陆词很难进行处理。主要有前向最大匹配法，后向最大匹配法以及双向最大匹配法。</p>
<h4 id="前向最大匹配算法"><a href="#前向最大匹配算法" class="headerlink" title="前向最大匹配算法"></a>前向最大匹配算法</h4><p>前向最大匹配算法，是从待分词句子的左边向右边搜索，寻找词的最大匹配。规定一个词的最大长度，每次扫描的时候寻找当前开始的这个长度的词来和字典中的词匹配，如果没有找到，就缩短长度继续寻找，直到找到字典中的词或者成为单字。</p>
<h5 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h5><ol>
<li>从前向后扫描字典，测试读入的子串是否在字典中</li>
<li>如果存在，则从输入中删除掉该子串，重新按照规则取子串，重复1</li>
<li>如果不存在于字典中，则从右向左减少子串长度，重复1</li>
</ol>
<h5 id="分词实例："><a href="#分词实例：" class="headerlink" title="分词实例："></a>分词实例：</h5><p>比如说输入 “北京大学生前来应聘”，假设词典中最长的单词为 5 个（MAX_LENGTH）。</p>
<ol>
<li>第一轮：取子串 “北京大学生”，正向取词，如果匹配失败，<strong>每次去掉匹配字段最后面的一个字</strong><br>“北京大学生”，扫描 5 字词典，没有匹配，子串长度减 1 变为“北京大学”<br>“北京大学”，扫描 4 字词典，有匹配，输出“北京大学”，输入变为“生前来应聘”</li>
<li>第二轮：取子串“生前来应聘”<br>“生前来应聘”，扫描 5 字词典，没有匹配，子串长度减 1 变为“生前来应”<br>“生前来应”，扫描 4 字词典，没有匹配，子串长度减 1 变为“生前来”<br>“生前来”，扫描 3 字词典，没有匹配，子串长度减 1 变为“生前”<br>“生前”，扫描 2 字词典，有匹配，输出“生前”，输入变为“来应聘””</li>
<li>第三轮：取子串“来应聘”<br>“来应聘”，扫描 3 字词典，没有匹配，子串长度减 1 变为“来应”<br>“来应”，扫描 2 字词典，没有匹配，子串长度减 1 变为“来”<br>颗粒度最小为 1，直接输出“来”，输入变为“应聘”</li>
<li>第四轮：取子串“应聘”<br>“应聘”，扫描 2 字词典，有匹配，输出“应聘”，输入变为“”<br>输入长度为0，扫描终止</li>
</ol>
<h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5><p>数据准备：词库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_max_match</span><span class="params">(sentence, window_size, word_dict)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    前向最大匹配算法：</span></span><br><span class="line"><span class="string">    （1）扫描字典，测试读入的子串是否在字典中</span></span><br><span class="line"><span class="string">    （2）如果存在，则从输入中删除掉该子串，重新按照规则取子串，重复（1）</span></span><br><span class="line"><span class="string">    （3）如果不存在于字典中，则从右向左减少子串长度，重复（1）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param sentence: 待分词的句子</span></span><br><span class="line"><span class="string">    :param window_size: 词的最大长度</span></span><br><span class="line"><span class="string">    :param word_dict: 词典</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    seg_words = []  <span class="comment"># 存放分词结果</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> sentence:</span><br><span class="line">        <span class="keyword">for</span> word_len <span class="keyword">in</span> range(window_size, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> sentence[:word_len] <span class="keyword">in</span> word_dict:</span><br><span class="line">                <span class="comment"># 如果该词再字典中，将该词保存到分词结果中,并将其从句中切出来</span></span><br><span class="line">                seg_words.append(sentence[:word_len])</span><br><span class="line">                sentence = sentence[word_len:]</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 如果窗口中的词不在字典中，将第一个字切分出来</span></span><br><span class="line">            seg_words.append(sentence[:word_len])</span><br><span class="line">            sentence = sentence[word_len:]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'/'</span>.join(seg_words)</span><br></pre></td></tr></table></figure>
<h4 id="后向最大匹配算法"><a href="#后向最大匹配算法" class="headerlink" title="后向最大匹配算法"></a>后向最大匹配算法</h4><p>在词典中从句尾向句首进行扫描，尽可能地选择与词典中最长单词匹配的词作为目标分词，然后进行下一次匹配。</p>
<p>在实践中，逆向最大匹配算法性能优于正向最大匹配算法。 </p>
<h5 id="算法流程-1"><a href="#算法流程-1" class="headerlink" title="算法流程"></a>算法流程</h5><ol>
<li>从后向前扫描字典，测试读入的子串是否在字典中</li>
<li>如果存在，则从输入中删除掉该子串，重新按照规则取子串，重复1</li>
<li>如果不存在于字典中，则从左向右减少子串长度，重复1</li>
</ol>
<h5 id="分词实例"><a href="#分词实例" class="headerlink" title="分词实例"></a>分词实例</h5><p>输入 “北京大学生前来应聘”，假设词典中最长的单词为 5 个（MAX_LENGTH）。</p>
<ol>
<li>第一轮：取子串 “生前来应聘”，逆向取词，如果匹配失败，<strong>每次去掉匹配字段最前面的一个字</strong><br>“生前来应聘”，扫描 5 字词典，没有匹配，字串长度减 1 变为“前来应聘”<br>“前来应聘”，扫描 4 字词典，没有匹配，字串长度减 1 变为“来应聘”<br>“来应聘”，扫描 3 字词典，没有匹配，字串长度减 1 变为“应聘”<br>“应聘”，扫描 2 字词典，有匹配，输出“应聘”，输入变为“大学生前来”</li>
<li>第二轮：取子串“大学生前来”<br>“大学生前来”，扫描 5 字词典，没有匹配，字串长度减 1 变为“学生前来”<br>“学生前来”，扫描 4 字词典，没有匹配，字串长度减 1 变为“生前来”<br>“生前来”，扫描 3 字词典，没有匹配，字串长度减 1 变为“前来”<br>“前来”，扫描 2 字词典，有匹配，输出“前来”，输入变为“北京大学生”</li>
<li>第三轮：取子串“北京大学生”<br>“北京大学生”，扫描 5 字词典，没有匹配，字串长度减 1 变为“京大学生”<br>“京大学生”，扫描 4 字词典，没有匹配，字串长度减 1 变为“大学生”<br>“大学生”，扫描 3 字词典，有匹配，输出“大学生”，输入变为“北京”</li>
<li>第四轮：取子串“北京”<br>“北京”，扫描 2 字词典，有匹配，输出“北京”，输入变为“”<br>输入长度为0，扫描终止</li>
</ol>
<h5 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5><p>数据准备：词库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_max_match</span><span class="params">(sentence, window_size, word_dict)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    后向最大匹配算法：</span></span><br><span class="line"><span class="string">    （1）从后向前扫描字典，测试读入的子串是否在字典中</span></span><br><span class="line"><span class="string">    （2）如果存在，则从输入中删除掉该子串，重新按照规则取子串，重复（1）</span></span><br><span class="line"><span class="string">    （3）如果不存在于字典中，则从右向左减少子串长度，重复（1）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param sentence: 待分词的句子</span></span><br><span class="line"><span class="string">    :param window_size: 词的最大长度</span></span><br><span class="line"><span class="string">    :param word_dict: 词典</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    seg_words = []</span><br><span class="line">    <span class="keyword">while</span> sentence:</span><br><span class="line">        <span class="keyword">for</span> word_len <span class="keyword">in</span> range(window_size, <span class="number">0</span>, <span class="number">-1</span>):  <span class="comment"># 每次去掉匹配字段最前面的一个字</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> sentence[len(sentence) - word_len:] <span class="keyword">in</span> word_dict:</span><br><span class="line">                <span class="comment"># 如果该词再字典中，将该词保存到分词结果中,并将其从句中切出来</span></span><br><span class="line">                seg_words.append(sentence[len(sentence) - word_len:])</span><br><span class="line">                sentence = sentence[:len(sentence) - word_len]</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 如果窗口中的词不在字典中，将最后一个字切分出来</span></span><br><span class="line">            seg_words.append(sentence[<span class="number">-1</span>:])</span><br><span class="line">            sentence = sentence[:<span class="number">-1</span>]</span><br><span class="line">    seg_words.reverse()</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'/'</span>.join(seg_words)</span><br></pre></td></tr></table></figure>
<h4 id="双向最大匹配法"><a href="#双向最大匹配法" class="headerlink" title="双向最大匹配法"></a>双向最大匹配法</h4><p>因为同一个句子，在机械分词中经常会出现多种分词的组合，因此需要进行歧义消除，来得到最优的分词结果。</p>
<p>以很常见的MMSEG机械分词算法为例，MMSEG在搜索引擎Solr中经常使用到，是一种非常可靠高效的分词算法。MMSEG消除歧义的规则有四个，它在使用中依次用这四个规则进行过滤，直到只有一种结果或者第四个规则使用完毕。这个四个规则分别是：</p>
<ol>
<li><p>最大匹配。选择“词组长度最大的”那个词组，然后选择这个词组的第一个词，作为切分出的第一个词，如对于“中国人民万岁”，匹配结果分别为：</p>
<ul>
<li>中/国/人</li>
<li>中国/人/民</li>
<li>中国/人民/万岁</li>
<li>中国人/民/万岁</li>
</ul>
<p>在这个例子“词组长度最长的”词组为后两个，因此选择了“中国人/民/万岁”中的“中国人”，或者“中国/人民/万岁”中的“中国”。</p>
</li>
<li><p>最大平均词语长度。经过规则1过滤后，如果剩余的词组超过1个，那就选择平均词语长度最大的那个(平均词长=词组总字数/词语数量)。比如“生活水平”，可能得到如下词组：</p>
<ul>
<li>生/活水/平 (4/3=1.33)</li>
<li>生活/水/平 (4/3=1.33)</li>
<li>生活/水平 (4/2=2)</li>
</ul>
<p>根据此规则，就可以确定选择“生活/水平”这个词组</p>
</li>
<li><p>词语长度的最小变化率。这个变化率一般可以由标准差来决定。比如对于“中国人民万岁”这个短语，可以计算：</p>
<ul>
<li>中国/人民/万岁(标准差=sqrt(((2-2)^2+(2-2)^2+(2-2^2))/3)=0)</li>
<li>中国人/民/万岁(标准差=sqrt(((2-3)^2+(2-1)^2+(2-2)^2)/3)=0.8165)</li>
</ul>
<p>于是选择“中国/人民/万岁”这个词组。</p>
</li>
<li><p>计算词组中的所有单字词词频的自然对数，然后将得到的值相加，取总和最大的词组。比如：</p>
<ul>
<li>设施/和服/务</li>
<li>设施/和/服务</li>
</ul>
<p>这两个词组中分别有“务”和“和”这两个单字词，假设“务”作为单字词时候的频率是5，“和”作为单字词时候的频率是10，对5和10取自然对数，然后取最大值者，所以取“和”字所在的词组，即“设施/和/服务”。</p>
</li>
</ol>
<p>在实际中根据需求，选择或制定相应的规则来改善分词的质量。</p>
<h5 id="算法流程-2"><a href="#算法流程-2" class="headerlink" title="算法流程"></a>算法流程</h5><ol>
<li>比较正向最大匹配和逆向最大匹配结果</li>
<li>如果分词数量结果不同，那么取分词数量较少的那个</li>
<li>如果分词数量结果相同<ul>
<li>分词结果相同，可以返回任何一个</li>
<li>分词结果不同，返回单字数比较少的那个</li>
</ul>
</li>
</ol>
<p>这种规则的出发点来自语言学上的启发：汉语中单字词的数量要远小于非单字词。因此，算法应当尽量减少结果中的单字，保留更多的完整词语。</p>
<h5 id="分词实例-1"><a href="#分词实例-1" class="headerlink" title="分词实例"></a>分词实例</h5><p>正向匹配最终切分结果为：北京大学 / 生前 / 来 / 应聘，分词数量为 4，单字数为 1<br>逆向匹配最终切分结果为：”北京/ 大学生/ 前来 / 应聘，分词数量为 4，单字数为 0<br>逆向匹配单字数少，因此返回逆向匹配的结果。</p>
<h5 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5><p>数据准备：词库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fb_max_match</span><span class="params">(sentence,window_size,word_dict)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    1. 比较正向最大匹配和逆向最大匹配结果</span></span><br><span class="line"><span class="string">    2. 如果分词数量结果不同，那么取分词数量较少的那个</span></span><br><span class="line"><span class="string">    3. 如果分词数量结果相同</span></span><br><span class="line"><span class="string">       * 分词结果相同，可以返回任何一个</span></span><br><span class="line"><span class="string">       * 分词结果不同，返回单字数比较少的那个</span></span><br><span class="line"><span class="string">    :param sentence: 待分词的句子</span></span><br><span class="line"><span class="string">    :param window_size: 词的最大长度</span></span><br><span class="line"><span class="string">    :param word_dict: 词典</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    forward_seg = forward_max_match(sentence,window_size,word_dict)</span><br><span class="line">    backward_seg = backward_max_match(sentence,window_size,word_dict)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果分词结果不同，返回词数较小的分词结果</span></span><br><span class="line">    <span class="keyword">if</span> len(forward_seg) != len(backward_seg):</span><br><span class="line">        <span class="keyword">return</span> forward_seg <span class="keyword">if</span> len(forward_seg) &lt; len(backward_seg) <span class="keyword">else</span> backward_seg</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果分词结果词数相同，优先考虑返回包含单个字符最少的分词结果</span></span><br><span class="line">        forward_single_word_count = len([filter(<span class="keyword">lambda</span> x:len(x) == <span class="number">1</span>,forward_seg)])</span><br><span class="line">        backward_single_word_count = len([filter(<span class="keyword">lambda</span> x:len(x) == <span class="number">1</span>,backward_seg)])</span><br><span class="line">        <span class="keyword">if</span> forward_single_word_count != backward_single_word_count:</span><br><span class="line">            <span class="keyword">return</span> forward_seg <span class="keyword">if</span> forward_single_word_count &lt; backward_single_word_count <span class="keyword">else</span> backward_seg</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 否则，返回任意结果</span></span><br><span class="line">            <span class="keyword">return</span> forward_seg</span><br></pre></td></tr></table></figure>
<h5 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h5><p>对歧义问题的解决有点弱</p>
<h4 id="N-最短路径分词算法"><a href="#N-最短路径分词算法" class="headerlink" title="(N-)最短路径分词算法"></a>(N-)最短路径分词算法</h4><h5 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h5><p>首先基于词典对文本进行全切分(改编版最大匹配)；然后基于词语的临接关系构建一个有向图；使用模型(比如一阶马尔科夫模型)对图里的边加权；最后使用最短路径算法求，从句首到句末质量最高(比如概率最大)的路径，就得到了分词结果。</p>
<h5 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h5><p>最短路径分词算法首先将一句话中的所有词匹配出来，构成词图（有向无环图DAG），之后寻找从起始点到终点的最短路径作为最佳组合方式，以“他说的确实在理”为例，给出对这句话的3-最短路：<br><img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/最短路径分词算法.png" alt="avatar"></p>
<ol>
<li><p>构建有向图：根据一个已有词典构造出的有向无环图（全切分，根据临接关系构建有向图）。它将字串分为单个的字，每个字用图中相邻的两个结点表示，故对于长度为n的字串，需要n+1个结点。两节点间若有边，则表示两节点间所包含的所有结点构成的词，如图中结点2、3、4构成词“的确”。</p>
</li>
<li><p>计算权重：本例子中权重为1（为了简单），实际应用中，可以使用一阶马尔科夫模型进行权重计算。</p>
<ul>
<li>p(实，在)=p(实)*p(在|实)。概率取值越大，说明一个边出现的概率越大，这条边会提升所在分句结果的概率，由于最后要计算最短路径，需要构造一个连接权重与分局结果质量成反比的指标，因此对概率取了倒：weight=1/p(实,在)</li>
<li><p>这个概率可能非常小，得到的权重取值非常大。而我们后面在计算路径的长度时，会将若干个边的权重加起来，这时候有上溢出的可能性。避免上溢出的常用策略是取对数。：weight=log(1/p(实，在))。</p>
</li>
<li><p>概率的估算：概率就用频率来估计，p(实) = （“实”字在语料中出现的次数）/（语料的总词数），</p>
<p>p(在|实) = p(实在)/p(实)=(“实在”在语料中出现的次数)/(“实”在语料中出现的次数)</p>
</li>
</ul>
</li>
</ol>
<h5 id="代码（维特比）"><a href="#代码（维特比）" class="headerlink" title="代码（维特比）"></a><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码（维特比）</a></h5><p>数据准备：1、词库，2、词语之间的条件概率（用来计算路径权重）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用维特比算法求词图的最短路径</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(self, word_graph)</span>:</span></span><br><span class="line">    path_length_map = &#123;&#125;  <span class="comment"># 用于存储所有的路径，后面的邻接词语所在位置，以及对应的长度</span></span><br><span class="line">    word_graph = [[[<span class="string">"&lt;start&gt;"</span>, <span class="number">1</span>]]] + word_graph + [[[<span class="string">"&lt;end&gt;"</span>, <span class="number">-1</span>]]]</span><br><span class="line">    <span class="comment"># 这是一种比较简单的数据结构</span></span><br><span class="line">    path_length_map[(<span class="string">"&lt;start&gt;"</span>,)] = [<span class="number">1</span>, <span class="number">0</span>]  <span class="comment"># start处，后面的临接词语在列表的1处，路径长度是0,。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(word_graph)):</span><br><span class="line">        distance_from_start2current = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> len(word_graph[i]) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> former_path <span class="keyword">in</span> list(path_length_map.keys()):  <span class="comment"># path_length_map内容一直在变，需要深拷贝key,也就是已经积累的所有路径</span></span><br><span class="line">            <span class="comment"># 取出已经积累的路径，后面的临接词语位置，以及路径的长度。</span></span><br><span class="line">            [next_index_4_former_path, former_distance] = path_length_map[former_path]</span><br><span class="line">            former_word = former_path[<span class="number">-1</span>]</span><br><span class="line">            later_path = list(former_path)</span><br><span class="line">            <span class="keyword">if</span> next_index_4_former_path == i:  <span class="comment"># 如果这条路径的临接词语的位置就是当前索引</span></span><br><span class="line">                <span class="keyword">for</span> current_word <span class="keyword">in</span> word_graph[i]:  <span class="comment"># 遍历词图数据中，这个位置上的所有换选词语，然后与former_path拼接新路径</span></span><br><span class="line">                    current_word, next_index = current_word</span><br><span class="line">                    new_path = tuple(later_path + [current_word])  <span class="comment"># 只有int, string, tuple这种不可修改的数据类型可以hash，</span></span><br><span class="line">                    <span class="comment"># 也就是成为dict的key</span></span><br><span class="line">                    <span class="comment"># 计算新路径的长度</span></span><br><span class="line">                    new_path_len = former_distance + self.word_distance.get((former_word, current_word), <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">                    path_length_map[new_path] = [next_index, new_path_len]  <span class="comment"># 存储新路径后面的临接词语，以及路径长度</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 维特比的部分。选择到达当前节点的路径中，最短的那一条</span></span><br><span class="line">                    <span class="keyword">if</span> current_word <span class="keyword">in</span> distance_from_start2current:  <span class="comment"># 如果已经有到达当前词语的路径，需要择优</span></span><br><span class="line">                        <span class="keyword">if</span> distance_from_start2current[current_word][<span class="number">1</span>] &gt; new_path_len:  <span class="comment"># 如果当前新路径比已有的更短</span></span><br><span class="line">                            distance_from_start2current[current_word] = [new_path, new_path_len]  <span class="comment"># 用更短的路径数据覆盖原来的</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        distance_from_start2current[current_word] = [new_path, new_path_len]  <span class="comment"># 如果还没有这条路径，就记录它</span></span><br><span class="line">    shortest_path = distance_from_start2current[<span class="string">"&lt;end&gt;"</span>][<span class="number">0</span>]</span><br><span class="line">    shortest_path = shortest_path[<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> shortest_path</span><br></pre></td></tr></table></figure>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ol>
<li>对歧义和新词的处理不是很好，对词典中未出现的词没法进行处理。</li>
<li>分词效果取决于词典的质量。</li>
</ol>
<h3 id="基于统计以及机器学习的分词方法"><a href="#基于统计以及机器学习的分词方法" class="headerlink" title="基于统计以及机器学习的分词方法"></a>基于统计以及机器学习的分词方法</h3><h4 id="基于N-gram语言模型的分词方法"><a href="#基于N-gram语言模型的分词方法" class="headerlink" title="基于N-gram语言模型的分词方法"></a>基于N-gram语言模型的分词方法</h4><h5 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h5><p>n-gram模型，称为N元模型，可用于定义字符串中的距离，也可用于中文的分词；该模型假设第n个词的出现只与前面n-1个词相关，与其他词都不相关，整个语句的概率就是各个词出现概率的乘积；而这些概率，利用语料，统计同时出现相关词的概率次数计算得到；常用的模型是Bi-gram和Tri-gram模型。<br>假设一个字符串s由m个词组成，因此我们需要计算出$p(w_1,w_2,\ldots,w_m)$的概率，根据概率论中的链式法则得到如下：</p>
<script type="math/tex; mode=display">
p(w_1,w_2,\ldots,w_m) = p(w_1)*p(w_2|w_1)*p(w_3|w_2,w_1)\ldots p(w_m|w_{m-1},\ldots,w_2,w_1)=\prod_{i}{p(w_i|w_1,w_2,\ldots,w_{i-1})}</script><p>那么下面的问题是如何计算上面每一个概率，比如$p(w_1,w_2,w_3,w_4,w_5)$，一种比较直观的计算就是计数然后用除法：</p>
<script type="math/tex; mode=display">
p(w_5|w_1,w_2,w_3,w_4) = \frac{Count(w_1,w_2,w_3,w_4,w_5)}{Count(w_1,w_2,w_3,w_4)}</script><p>直接计算这个概率的难度有点大：</p>
<ol>
<li><p>直接这样计算会导致参数空间过大。</p>
<p>一个语言模型的参数就是所有的这些条件概率，试想按上面方式计算$p(w_5|w_1,w_2,w_3,w_4)$,这里$w_5$有词典大小取值的可能，记词典大小：$|V|$，则该模型的参数个数是$|V|^5$，而且这还不包含$P(w_4|w_1,w_2,w_3)$的个数，可以看到这样去计算条件概率会使语言模型参数个数过多而无法使用。</p>
</li>
<li><p>数据稀疏严重。我的理解是像上面那样计数计算，比如计数分子$w_1,w_2,w_3,w_4,w_5$,在我们所能见的文本中出现的次数是很小的，这样计算的结果是过多的条件概率会等于0，因为我们根本没有看到足够的文本来统计！假设一个语料库中单词的数量为$|V|$个，一个句子由$n$个词组成，那么每个单词都可能有$|V|$个取值，那么由这些词组成的$n$元组合的数目为$|V|^n$种，也就是说，组合数会随着$n$的增加而呈现指数级别的增长，随着$n$的增加，语料数据库能够提供的数据是非常有限的，除非有海量的各种类型的语料数据，否则还有大量的$n$元组合都没有在语料库中出现过（即由$n$个单词所组成的一些句子根本就没出现过，可以理解为很多的$n$元组所组成的句子不能被人很好地理解）也就是说依据最大似然估计得到的概率将会是0，模型可能仅仅能够计算寥寥几个句子。怎么解决呢？</p>
</li>
</ol>
<p>解决參数空间过大的问题。引入了马尔科夫假设：<strong>随意一个词出现的概率只与它前面出现的有限的一个或者几个词有关。</strong>假设第$w_i$个词语只与它前面的$n$个词语相关，这样我们就得到前面的条件概率计算简化如下</p>
<script type="math/tex; mode=display">
p(w_i|w_{i-1},\ldots,w_2,w_1) \approx p(w_i|w_{i-1},\ldots,w_{i-n})\\
p(w_1,w_2,\ldots,w_m) \approx \prod_{i} p(w_i|w_{i-1},\ldots,w_{i-n})</script><p>当n=1，即一元模型（Uni-gram）,即$w_i$与它前面的0个词相关，即$w_i$不与任何词相关，每一个词都是相互独立的：</p>
<script type="math/tex; mode=display">
P\left(w_{1}, w_{2}, \cdots, w_{m}\right)=\prod_{i=1}^{m} P\left(w_{i}\right)</script><p>当n=2，即二元模型（Bi-gram）,此时$w_i$与它前面1个词相关：</p>
<script type="math/tex; mode=display">
P\left(w_{1}, w_{2}, \cdots, w_{m}\right)=\prod_{i=1}^{m} P\left(w_{i} | w_{i-1}\right)</script><p>当n=3时，即三元模型（Tri-gram）,此时$w_i$与它前面2个词相关：</p>
<script type="math/tex; mode=display">
P\left(w_{1}, w_{2}, \cdots, w_{m}\right)=\prod_{i=1}^{m} P\left(w_{i} | w_{i-2} w_{i-1}\right)</script><p>一般来说，N元模型就是假设当前词的出现概率只与它前面的N-1个词有关。而这些概率参数都是可以通过大规模语料库来计算。<strong>在实践中用的最多的就是bigram和trigram了，高于四元的用的非常少，由于训练它须要更庞大的语料，并且数据稀疏严重，时间复杂度高，精度却提高的不多。</strong></p>
<h5 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h5><p>要计算出模型中的条件概率，这些条件概率也称为模型的参数，得到这些参数的过程称为训练。用最大似然性估计计算下面的条件概率：</p>
<script type="math/tex; mode=display">
P\left(w_{i} | w_{i-1}\right)=\frac{c\left(w_{i-1}, w_{i}\right)}{c\left(w_{i-1}\right)}</script><p>一元语言模型中：句子概率定义为：$P\left(s\right)=\prod_{i=1}^{m} P\left(w_{i}\right)$,这个式子成立的条件是有一个假设，就是条件无关假设，我们认为每个词都是条件无关的。这里的参数种类是一种 $P(w_n)$,但是参数实例有$|V|$个(V是词典大小),我们应该如何得到每个参数实例的值。用的是极大似然估计法。比如训练语料是:”星期五早晨，我特意起了个大早，为的就是看看早晨的天空。”那么我们的字典为：星 期 五 早 晨 ，我 特 意 起 了 个 大 早 为 的 就 是 看 天 空 。 22个不同词，每个词语的概率直接用极大似然估计法估计得到。如：p(星) = 1/27，p(期) = 1/27。于是需要存储学习得到的模型参数，一个向量，22维，每个维度保存着每个单词的概率值。当需要计算：p(我看看早晨的天空)=p(我)p(看)p(看)p(早)p(晨)p(的)p(天)p(空)=$\frac{1}{27}<em>\frac{1}{27}</em>\frac{1}{27}\ldots *\frac{1}{27}$，可以直接计算出来。于是只要将每句话拆开为每个单词然后用累积形式运算，这样就能算出每句话的概率。缺点是：不包含语序信息。</p>
<p>二元语言模型中：为了计算对应的二元模型的参数，即$P(w_i | w_{i-1})$，要先计数即$c(w_{i-1},w_i)$，然后计数$c(w_{i-1})$,再用除法可得到这些条件概率.可以看到对于$c(w_{i-1},w_i)$来说，$w_{i-1}$有语料库词典大小（记作$|V|$）的可能取值，$w_i$也是，所以$c(w_{i-1},w_i)$要计算的个数有$|V|^2$。</p>
<p>$c(w_{i-1},w_i)$计数结果如下:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>english</th>
</tr>
</thead>
<tbody>
<tr>
<td>i</td>
<td>5</td>
<td>827</td>
<td>0</td>
<td>9</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>want</td>
<td>2</td>
<td>0</td>
<td>608</td>
<td>1</td>
<td>6</td>
<td>6</td>
<td>5</td>
<td>1</td>
</tr>
<tr>
<td>to</td>
<td>2</td>
<td>0</td>
<td>4</td>
<td>686</td>
<td>2</td>
<td>0</td>
<td>6</td>
<td>211</td>
</tr>
<tr>
<td>eat</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>16</td>
<td>2</td>
<td>42</td>
<td>0</td>
</tr>
<tr>
<td>chinese</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>82</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>food</td>
<td>15</td>
<td>0</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>4</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>lunch</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>spend</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>$c(w_{i-1})$的计数如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>english</th>
</tr>
</thead>
<tbody>
<tr>
<td>2533</td>
<td>927</td>
<td>2417</td>
<td>746</td>
<td>158</td>
<td>1093</td>
<td>341</td>
<td>278</td>
</tr>
</tbody>
</table>
</div>
<p>那么二元模型的参数计算结果如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>english</th>
</tr>
</thead>
<tbody>
<tr>
<td>i</td>
<td>0.002</td>
<td>0.33</td>
<td>0</td>
<td>0.0036</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.00079</td>
</tr>
<tr>
<td>want</td>
<td>0.0022</td>
<td>0</td>
<td>0.66</td>
<td>0.0011</td>
<td>0.0065</td>
<td>0.0065</td>
<td>0.0054</td>
<td>0.0011</td>
</tr>
<tr>
<td>to</td>
<td>0.00083</td>
<td>0</td>
<td>0.0017</td>
<td>0.28</td>
<td>0.00083</td>
<td>0</td>
<td>0.0025</td>
<td>0.087</td>
</tr>
<tr>
<td>eat</td>
<td>0</td>
<td>0</td>
<td>0.0027</td>
<td>0</td>
<td>0.021</td>
<td>0.0027</td>
<td>0.056</td>
<td>0</td>
</tr>
<tr>
<td>chinese</td>
<td>0.0063</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.52</td>
<td>0.0063</td>
<td>0</td>
</tr>
<tr>
<td>food</td>
<td>0.014</td>
<td>0</td>
<td>0.0014</td>
<td>0</td>
<td>0.00092</td>
<td>0.0037</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>lunch</td>
<td>0.0059</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.0029</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>spend</td>
<td>0.0036</td>
<td>0</td>
<td>0.0036</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>比如计算其中的P(want | i) = 0.33如:$p(want|i) = \frac{c(i,want)}{c(i)}=\frac{827}{2533} \approx0.33$,针对这个语料库的二元模型建立好了后，可以计算目标，即一个句子的概率了，一个例子如下：</p>
<p>$p(s)=p(i want english food) = p(i|<start>)<em>p(want|i)</em>p(english|want)<em>p(food|english)</em>p(<end>|food) \approx 0.000031$</end></start></p>
<p>该二元模型所捕捉到的一些实际信息:</p>
<ul>
<li>$p(english|want) = 0.0011,p(chinese|want)=0.0065$,want chinese 的概率更高，这和真实世界情况相对应，因为chinese food 比 English food 更受欢迎。</li>
<li>$p(to|want)=0.66$, want to 的概率很高，反映了语法特性</li>
<li>$p(food|to)=0$ ，to food 概率为0，因为这种搭配不常见</li>
<li>$p(want|spend)=0$, spend want 概率为0，因为这样违反了语法。</li>
</ul>
<p>常常在对数空间里面计算概率，原因有两个：</p>
<ol>
<li>防止溢出，如果计算的句子很长，最后得到的结果将非常小，甚至会溢出，比如计算得到的概率是0.001，那么假设以10为底取对数的结果就是-3，这样就不会溢出。</li>
<li>对数空间里面加法可以代替乘法，因为log(p1p2) = logp1 + logp2，而在计算机内部，显然加法比乘法执行更快！</li>
</ol>
<h5 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5><p>数据准备：unigram:(word, freq),bigram:(word1,word2,freq) 语料数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="comment"># global parameter</span></span><br><span class="line">DELIMITER = <span class="string">" "</span>  <span class="comment"># 分词之后的分隔符</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DNASegment</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.word1_dict_prob = &#123;&#125;  <span class="comment"># 记录概率,1-gram</span></span><br><span class="line">        self.word1_dict_count = &#123;&#125;  <span class="comment"># 记录词频,1-gram</span></span><br><span class="line">        self.word1_dict_count[<span class="string">"&lt;S&gt;"</span>] = <span class="number">8310575403</span>  <span class="comment"># 开始的&lt;S&gt;的个数</span></span><br><span class="line"></span><br><span class="line">        self.word2_dict_prob = &#123;&#125;  <span class="comment"># 记录概率,2-gram</span></span><br><span class="line">        self.word2_dict_count = &#123;&#125;  <span class="comment"># 记录词频,2-gram</span></span><br><span class="line"></span><br><span class="line">        self.gmax_word_length = <span class="number">0</span></span><br><span class="line">        self.all_freq = <span class="number">0</span>  <span class="comment"># 所有词的词频总和,1-gram的</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 估算未出现的词的概率,根据beautiful data里面的方法估算</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_unkonw_word_prob</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> math.log(<span class="number">10.</span> / (self.all_freq * <span class="number">10</span> ** len(word)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得片段的概率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_prob</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> self.word1_dict_prob:  <span class="comment"># 如果字典包含这个词</span></span><br><span class="line">            prob = self.word1_dict_prob[word]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            prob = self.get_unkonw_word_prob(word)</span><br><span class="line">        <span class="keyword">return</span> prob</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得两个词的转移概率(bigram)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_trans_prob</span><span class="params">(self, first_word, second_word)</span>:</span></span><br><span class="line">        trans_word = first_word + <span class="string">" "</span> + second_word</span><br><span class="line">        <span class="comment"># print trans_word</span></span><br><span class="line">        <span class="keyword">if</span> trans_word <span class="keyword">in</span> self.word2_dict_count:</span><br><span class="line">            trans_prob = math.log(self.word2_dict_count[trans_word] / self.word1_dict_count[first_word])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            trans_prob = self.get_word_prob(second_word)</span><br><span class="line">        <span class="keyword">return</span> trans_prob</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 寻找node的最佳前驱节点</span></span><br><span class="line">    <span class="comment"># 方法为寻找所有可能的前驱片段</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_best_pre_node</span><span class="params">(self, sequence, node, node_state_list)</span>:</span></span><br><span class="line">        <span class="comment"># 如果node比最大词长小，取的片段长度以node的长度为限</span></span><br><span class="line">        max_seg_length = min([node, self.gmax_word_length])</span><br><span class="line">        pre_node_list = []  <span class="comment"># 前驱节点列表</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得所有的前驱片段，并记录累加概率</span></span><br><span class="line">        <span class="keyword">for</span> segment_length <span class="keyword">in</span> range(<span class="number">1</span>, max_seg_length + <span class="number">1</span>):</span><br><span class="line">            segment_start_node = node - segment_length</span><br><span class="line">            segment = sequence[segment_start_node:node]  <span class="comment"># 获取片段</span></span><br><span class="line"></span><br><span class="line">            pre_node = segment_start_node  <span class="comment"># 取该片段，则记录对应的前驱节点</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> pre_node == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 如果前驱片段开始节点是序列的开始节点，</span></span><br><span class="line">                <span class="comment"># 则概率为&lt;S&gt;转移到当前词的概率</span></span><br><span class="line">                <span class="comment"># segment_prob = self.get_word_prob(segment)</span></span><br><span class="line">                segment_prob = self.get_word_trans_prob(<span class="string">"&lt;S&gt;"</span>, segment)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 如果不是序列开始节点，按照二元概率计算</span></span><br><span class="line">                <span class="comment"># 获得前驱片段的前一个词</span></span><br><span class="line">                pre_pre_node = node_state_list[pre_node][<span class="string">"pre_node"</span>]</span><br><span class="line">                pre_pre_word = sequence[pre_pre_node:pre_node]</span><br><span class="line">                segment_prob = self.get_word_trans_prob(pre_pre_word, segment)</span><br><span class="line"></span><br><span class="line">            pre_node_prob_sum = node_state_list[pre_node][<span class="string">"prob_sum"</span>]  <span class="comment"># 前驱节点的概率的累加值</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 当前node一个候选的累加概率值</span></span><br><span class="line">            candidate_prob_sum = pre_node_prob_sum + segment_prob</span><br><span class="line"></span><br><span class="line">            pre_node_list.append((pre_node, candidate_prob_sum))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 找到最大的候选概率值</span></span><br><span class="line">        (best_pre_node, best_prob_sum) = max(pre_node_list, key=<span class="keyword">lambda</span> d: d[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> (best_pre_node, best_prob_sum)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最大概率分词</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mp_seg</span><span class="params">(self, sequence)</span>:</span></span><br><span class="line">        sequence = sequence.strip()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化</span></span><br><span class="line">        node_state_list = []  <span class="comment"># 记录节点的最佳前驱，index就是位置信息</span></span><br><span class="line">        <span class="comment"># 初始节点，也就是0节点信息</span></span><br><span class="line">        ini_state = &#123;&#125;</span><br><span class="line">        ini_state[<span class="string">"pre_node"</span>] = <span class="number">-1</span>  <span class="comment"># 前一个节点</span></span><br><span class="line">        ini_state[<span class="string">"prob_sum"</span>] = <span class="number">0</span>  <span class="comment"># 当前的概率总和</span></span><br><span class="line">        node_state_list.append(ini_state)</span><br><span class="line">        <span class="comment"># 字符串概率为2元概率</span></span><br><span class="line">        <span class="comment"># P(a b c) = P(a|&lt;S&gt;)P(b|a)P(c|b)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 逐个节点寻找最佳前驱节点</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> range(<span class="number">1</span>, len(sequence) + <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 寻找最佳前驱，并记录当前最大的概率累加值</span></span><br><span class="line">            (best_pre_node, best_prob_sum) = self.get_best_pre_node(sequence, node, node_state_list)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 添加到队列</span></span><br><span class="line">            cur_node = &#123;&#125;</span><br><span class="line">            cur_node[<span class="string">"pre_node"</span>] = best_pre_node</span><br><span class="line">            cur_node[<span class="string">"prob_sum"</span>] = best_prob_sum</span><br><span class="line">            node_state_list.append(cur_node)</span><br><span class="line">            <span class="comment"># print "cur node list",node_state_list</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># step 2, 获得最优路径,从后到前</span></span><br><span class="line">        best_path = []</span><br><span class="line">        node = len(sequence)  <span class="comment"># 最后一个点</span></span><br><span class="line">        best_path.append(node)</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            pre_node = node_state_list[node][<span class="string">"pre_node"</span>]</span><br><span class="line">            <span class="keyword">if</span> pre_node == <span class="number">-1</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            node = pre_node</span><br><span class="line">            best_path.append(node)</span><br><span class="line">        best_path.reverse()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># step 3, 构建切分</span></span><br><span class="line">        word_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(best_path) - <span class="number">1</span>):</span><br><span class="line">            left = best_path[i]</span><br><span class="line">            right = best_path[i + <span class="number">1</span>]</span><br><span class="line">            word = sequence[left:right]</span><br><span class="line">            word_list.append(word)</span><br><span class="line"></span><br><span class="line">        seg_sequence = DELIMITER.join(word_list)</span><br><span class="line">        <span class="keyword">return</span> seg_sequence</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载词典，为词\t词频的格式</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initial_dict</span><span class="params">(self, gram1_file, gram2_file)</span>:</span></span><br><span class="line">        <span class="comment"># 读取unigram文件</span></span><br><span class="line">        dict_file = open(gram1_file, <span class="string">"r"</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> dict_file:</span><br><span class="line">            sequence = line.strip()</span><br><span class="line">            key = sequence.split(<span class="string">'\t'</span>)[<span class="number">0</span>]</span><br><span class="line">            value = float(sequence.split(<span class="string">'\t'</span>)[<span class="number">1</span>])</span><br><span class="line">            self.word1_dict_count[key] = value</span><br><span class="line">        <span class="comment"># 计算频率</span></span><br><span class="line">        self.all_freq = sum(self.word1_dict_count.values())  <span class="comment"># 所有词的词频</span></span><br><span class="line">        self.gmax_word_length = max(len(key) <span class="keyword">for</span> key <span class="keyword">in</span> self.word1_dict_count.keys())</span><br><span class="line">        self.gmax_word_length = <span class="number">20</span></span><br><span class="line">        self.all_freq = <span class="number">1024908267229.0</span></span><br><span class="line">        <span class="comment"># 计算1gram词的概率</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> self.word1_dict_count:</span><br><span class="line">            self.word1_dict_prob[key] = math.log(self.word1_dict_count[key] / self.all_freq)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取2_gram_file，同时计算转移概率</span></span><br><span class="line">        dict_file = open(gram2_file, <span class="string">"r"</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> dict_file:</span><br><span class="line">            sequence = line.strip()</span><br><span class="line">            key = sequence.split(<span class="string">'\t'</span>)[<span class="number">0</span>]</span><br><span class="line">            value = float(sequence.split(<span class="string">'\t'</span>)[<span class="number">1</span>])</span><br><span class="line">            first_word = key.split(<span class="string">" "</span>)[<span class="number">0</span>]</span><br><span class="line">            second_word = key.split(<span class="string">" "</span>)[<span class="number">1</span>]</span><br><span class="line">            self.word2_dict_count[key] = float(value)</span><br><span class="line">            <span class="keyword">if</span> first_word <span class="keyword">in</span> self.word1_dict_count:</span><br><span class="line">                <span class="comment"># 取自然对数</span></span><br><span class="line">                self.word2_dict_prob[key] = math.log(value / self.word1_dict_count[first_word])  </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.word2_dict_prob[key] = self.word1_dict_prob[second_word]</span><br></pre></td></tr></table></figure>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p><strong>列举出所有可能的分词方式，再分别计算句子概率，选择概率最大的作为最终分词结果。穷举法，速度慢。</strong></p>
<p>缺点：</p>
<ol>
<li>N-grams有一些不足，因为语言存在一个长距离依赖关系，比如：“The computer which I had just put into the machine room on the fifth floor crashed.”假如要预测最后一个词语crashed出现的概率，如果采用二元模型，那么crashed与floor实际关联可能性应该非常小，相反的，这句子的主语computer与crashed的相关性很大，但是n-grams并没有捕捉到这个信息。</li>
<li>一个词是由前一个或者几个词决定的，这样可以去除一部分歧义问题，但是n-gram模型还是基于马尔科夫模型的，其基本原理就是无后效性，就是后续的节点的状态不影响前面的状态，就是先前的分词形式一旦确定，无论后续跟的是什么词，都不会再有变化，这在现实中显然是不成立的。。因此就有一些可以考虑到后续词的算法，如crf等方法。</li>
</ol>
<h4 id="基于HMM的分词算法"><a href="#基于HMM的分词算法" class="headerlink" title="基于HMM的分词算法"></a>基于HMM的分词算法</h4><h5 id="隐马尔科夫模型-HMM"><a href="#隐马尔科夫模型-HMM" class="headerlink" title="隐马尔科夫模型(HMM)"></a>隐马尔科夫模型(HMM)</h5><p>隐马尔可夫模型是关于<strong>时序</strong>的概率模型,描述由一个隐藏的马尔可夫链随机生成不可观测的<strong>状态序列</strong>,再由各个状态生成一个观测而产生<strong>观测随机序列</strong>的过程.HMM是一种生成式模型，它的理论基础是朴素贝叶斯，本质上就类似于我们将朴素贝叶斯在单样本分类问题上的应用推广到序列样本分类问题上。</p>
<h6 id="模型表示"><a href="#模型表示" class="headerlink" title="模型表示"></a>模型表示</h6><p>设Q是所有可能的状态的集合$Q=\left\{q_{1}, q_{2}, \cdots, q_{N}\right\}$,V是所有可能的观测的集合$V=\left\{v_{1}, v_{2}, \cdots, v_{M}\right\}$,I是长度为T的状态序列$I=\left(i_{1}, i_{2}, \cdots, i_{T}\right)$, O是对应的观测序列$O=\left(o_{1}, o_{2}, \cdots, o_{T}\right)$,</p>
<ol>
<li>A是<strong>状态转移概率矩阵</strong>$A=\left[a_{i j}\right]_{N \times N}$,$a_{ij}$表示在时刻t处于状态$q_i$的条件下在时刻t+1转移到状态$q_j$的概率.</li>
<li>B是<strong>观测概率矩阵</strong> $B=\left[b_{j}(k)\right]_{N \times M}$,$b_{ij}$是在时刻t处于状态$q_j$的条件下生成观测$v_k$的概率.</li>
<li>$\pi$是<strong>初始状态概率向量</strong>$\pi=\pi(x)$,$\pi_i$表示时刻t=1处于状态qi的概率.</li>
</ol>
<p>隐马尔可夫模型由初始状态概率向量$pi$,状态转移概率矩阵A以及观测概率矩阵B确定.$\pi$和A决定即隐藏的<strong>马尔可夫链</strong>,生成不可观测的<strong>状态序列</strong>.B决定如何从状态生成观测,与状态序列综合确定了<strong>观测序列</strong>.因此,隐马尔可夫模型可以用<strong>三元符号</strong>表示$\lambda = (A,B,\pi)$</p>
<h6 id="两个基本假设"><a href="#两个基本假设" class="headerlink" title="两个基本假设"></a>两个基本假设</h6><ol>
<li><strong>齐次马尔可夫性假设</strong>:假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态.</li>
<li><strong>观测独立性假设</strong>:假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态.</li>
</ol>
<h6 id="三个基本问题"><a href="#三个基本问题" class="headerlink" title="三个基本问题"></a>三个基本问题</h6><p><strong>1. 概率计算问题</strong></p>
<p>给定模型$\lambda = (A,B,\pi)$和观测序列,$O=\left(o_{1}, o_{2}, \cdots, o_{T}\right)$计算在模型$\lambda$下观测序列O出现的概率$P(O|λ)$.</p>
<ol>
<li>直接计算：列举所有可能长度为T的状态序列,求各个状态序列I与观测序列O的联合概率,但计算量太大,实际操作不可行.</li>
<li><strong>前向算法</strong>：定义到时刻t部分观测序列为$o_1$~$o_t$且状态为$q_i$的概率为<strong>前向概率</strong>,记作$\alpha_{t}(i)=P\left(o_{1}, o_{2}, \cdots, o_{t}, i_{t}=q_{i} | \lambda\right)$.初始化前向概率$\alpha_{1}(i)=\pi_{i} b_{i}\left(o_{1}\right), \quad i=1,2, \cdots, N$，递推，对$t=1$ ~ $T-1$,$\alpha_{t+1}(i)=\left[\sum_{j=1}^{N} \alpha_{t}(j) a_{j i}\right] b_{i}\left(o_{t+1}\right), \quad i=1,2, \cdots, N$,得到$P(O | \lambda)=\sum_{i=1}^{N} \alpha_{T}(i)$减少计算量的原因在于每一次计算直接引用前一个时刻的计算结果,避免重复计算.</li>
<li><strong>后向算法</strong>:定义在时刻t状态为$q_i$的条件下,从t+1到T的部分观测序列为$o_{i+1}$~$o_T$的概率为<strong>后向概率</strong>,记作$\beta_{t}(i)=P\left(o_{t+1}, o_{t+2}, \cdots, o_{r} | i_{t}=q_{i}, \lambda\right)$.初始化后向概率$\beta_{r}(i)=1, \quad i=1,2, \cdots, N$,递推,对$t=T-1$~$1$$\beta_{t}(i)=\sum_{j=1}^{N} a_{i j} b_{j}\left(o_{i+1}\right) \beta_{i+1}(j), \quad i=1,2, \cdots, N$,得到$P(O | \lambda)=\sum_{i=1}^{N} \pi_{i} b_{i}\left(o_{1}\right) \beta_{1}(i)$</li>
</ol>
<p><strong>2. 学习算法</strong></p>
<p>已知观测序列$O=(o_1,o_2, \cdots,o_r)$,估计模型$\lambda = (A,B,\pi)$,的参数,使得在该模型下观测序列概率$p(O|\lambda)$最大.根据训练数据是否包括观察序列对应的状态序列分别由监督学习与非监督学习实现.</p>
<ol>
<li><p>监督学习：估计转移概率$\hat{a}_{i j}=\frac{A_{i j}}{\sum_{j=1}^{N} A_{i j}}, \quad i=1,2, \cdots, N ; \quad j=1,2, \cdots, N$ 和观测概率$\hat{b}_{j}(k)=\frac{B_{j k}}{\sum_{k=1}^{M} B_{j k}}, \quad j=1,2, \cdots, N, k=1,2, \cdots, M$.初始状态概率$\pi_i$的估计为S个样本中初始状态为$q_i$的频率.</p>
</li>
<li><p><strong>非监督学习(Baum-Welch算法)</strong>:将观测序列数据看作观测数据O,状态序列数据看作不可观测的<strong>隐数据</strong>I.首先确定完全数据的对数似然函数$log p(O,I|\lambda)$,求Q函数</p>
</li>
</ol>
<script type="math/tex; mode=display">
   \begin{aligned}
   Q(\lambda, \bar{\lambda})=& \sum_{t} \log \pi_{i_1} P(O, I | \bar{\lambda}) \\
   &+\sum_{I}\left(\sum_{i=1}^{I-1} \log a_{i_t,i_{t+1}}\right) P(O, I | \bar{\lambda}) \\
   &+\sum_{I}\left(\sum_{i=1}^{T} \log b_{i_t}\left(o_{t}\right)\right) P(O, I | \bar{\lambda})
   \end{aligned}</script><p>   ,用拉格朗日乘子法极大化Q函数求模型参数$\pi_{i}=\frac{P\left(O, i_{1}=i | \bar{\lambda}\right)}{P(O | \bar{\lambda})}$,$a_{i j}=\frac{\sum_{i=1}^{T-1} P\left(O, i_{t}=i, i_{t+1}=j | \bar{\lambda}\right)}{\sum_{t=1}^{T-1} P\left(O, i_{t}=i | \bar{\lambda}\right)}$,$b_{j}(k)=\frac{\sum_{i=1}^{T} P\left(O, i_{t}=j | \bar{\lambda}\right) I\left(o_{i}=v_{k}\right)}{\sum_{i=1}^{T} P\left(O, i_{t}=j | \bar{\lambda}\right)}$,</p>
<p><strong>3. 预测问题</strong></p>
<p>也称为解码问题.已知模型$\lambda = (A,B,\pi)$和观测序列$O=(O_1,O_2,\cdots,O_T)$,求对给定观测序列条件概率$P(I|O)$最大的状态序列$I=(i_1,i_2,\cdots,i_T)$</p>
<ol>
<li><p><strong>近似算法</strong>: 在每个时刻t选择在该时刻最有可能出现的状态$i_t^<em>$,从而得到一个状态序列作为预测的结果.优点是<em>*计算简单</em></em>,缺点是不能保证状态序列整体是最有可能的状态序列</p>
</li>
<li><p><strong>维特比算法</strong>:用<strong>动态规划</strong>求概率最大路径,这一条路径对应着一个状态序列.从t=1开始,递推地计算在时刻t状态为i的各条部分路径的最大概率,直至得到时刻t=T状态为i的各条路径的最大概率.时刻t=T的最大概率即为<strong>最优路径</strong>的概率$P^\star$,最优路径的<strong>终结点</strong>$i_t^\star$也同时得到,之后从终结点开始由后向前逐步求得<strong>结点</strong>,得到最优路径.</p>
</li>
</ol>
<h5 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h5><p>在分词算法中，分词对应着三大问题中的预测问题（解码问题），隐马尔可夫经常用作能够发现新词的算法，通过海量的数据学习，能够将人名、地名、互联网上的新词等一一识别出来，具有广泛的应用场景。</p>
<p>其分词过程:</p>
<p><img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/hmm_seg.png" alt="avatar"></p>
<h5 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5><p>数据准备：分好词的词库</p>
<p>step 1:通过统计语料库中词的频次，计算三个概率：初始状态概率start，状态转移概率矩阵trans，发射概率emit。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HmmTrain</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.line_index = <span class="number">-1</span></span><br><span class="line">        self.char_set = set()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init</span><span class="params">(self)</span>:</span>  <span class="comment"># 初始化字典</span></span><br><span class="line">        trans_dict = &#123;&#125;  <span class="comment"># 存储状态转移概率</span></span><br><span class="line">        emit_dict = &#123;&#125;  <span class="comment"># 发射概率(状态-&gt;词语的条件概率)</span></span><br><span class="line">        Count_dict = &#123;&#125;  <span class="comment"># 存储所有状态序列 ，用于归一化分母</span></span><br><span class="line">        start_dict = &#123;&#125;  <span class="comment"># 存储状态的初始概率</span></span><br><span class="line">        state_list = [<span class="string">'B'</span>, <span class="string">'M'</span>, <span class="string">'E'</span>, <span class="string">'S'</span>]  <span class="comment"># 状态序列</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> state <span class="keyword">in</span> state_list:</span><br><span class="line">            trans_dict[state] = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> state1 <span class="keyword">in</span> state_list:</span><br><span class="line">                trans_dict[state][state1] = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> state <span class="keyword">in</span> state_list:</span><br><span class="line">            start_dict[state] = <span class="number">0.0</span></span><br><span class="line">            emit_dict[state] = &#123;&#125;</span><br><span class="line">            Count_dict[state] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(trans_dict) #&#123;'B': &#123;'B': 0.0, 'S': 0.0, 'M': 0.0, 'E': 0.0&#125;, 'S': &#123;'B': 0.0, 'S': 0.0, 'M': 0.0, 'E': 0.0&#125;,。。。&#125;</span></span><br><span class="line">        <span class="comment"># print(emit_dict) # &#123;'B': &#123;&#125;, 'S': &#123;&#125;, 'M': &#123;&#125;, 'E': &#123;&#125;&#125;</span></span><br><span class="line">        <span class="comment"># print(start_dict) # &#123;'B': 0.0, 'S': 0.0, 'M': 0.0, 'E': 0.0&#125;</span></span><br><span class="line">        <span class="comment"># print(Count_dict) # &#123;'B': 0, 'S': 0, 'M': 0, 'E': 0&#125;</span></span><br><span class="line">        <span class="keyword">return</span> trans_dict, emit_dict, start_dict, Count_dict</span><br><span class="line"></span><br><span class="line">    <span class="string">'''保存模型'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_model</span><span class="params">(self, word_dict, model_path)</span>:</span></span><br><span class="line">        f = open(model_path, <span class="string">'w'</span>)</span><br><span class="line">        f.write(str(word_dict))</span><br><span class="line">        f.close()</span><br><span class="line"></span><br><span class="line">    <span class="string">'''词语状态转换'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_status</span><span class="params">(self, word)</span>:</span>  <span class="comment"># 根据词语，输出词语对应的SBME状态</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        S:单字词</span></span><br><span class="line"><span class="string">        B:词的开头</span></span><br><span class="line"><span class="string">        M:词的中间</span></span><br><span class="line"><span class="string">        E:词的末尾</span></span><br><span class="line"><span class="string">        能 ['S']</span></span><br><span class="line"><span class="string">        前往 ['B', 'E']</span></span><br><span class="line"><span class="string">        科威特 ['B', 'M', 'E']</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        word_status = []</span><br><span class="line">        <span class="keyword">if</span> len(word) == <span class="number">1</span>:</span><br><span class="line">            word_status.append(<span class="string">'S'</span>)</span><br><span class="line">        <span class="keyword">elif</span> len(word) == <span class="number">2</span>:</span><br><span class="line">            word_status = [<span class="string">'B'</span>, <span class="string">'E'</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            M_num = len(word) - <span class="number">2</span></span><br><span class="line">            M_list = [<span class="string">'M'</span>] * M_num</span><br><span class="line">            word_status.append(<span class="string">'B'</span>)</span><br><span class="line">            word_status.extend(M_list)</span><br><span class="line">            word_status.append(<span class="string">'E'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> word_status</span><br><span class="line"></span><br><span class="line">    <span class="string">'''基于人工标注语料库，训练发射概率，初始状态， 转移概率'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, train_filepath, trans_path, emit_path, start_path)</span>:</span></span><br><span class="line">        trans_dict, emit_dict, start_dict, Count_dict = self.init()</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> open(train_filepath):</span><br><span class="line">            self.line_index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            line = line.strip()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            char_list = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(line)):</span><br><span class="line">                <span class="keyword">if</span> line[i] == <span class="string">" "</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                char_list.append(line[i])</span><br><span class="line"></span><br><span class="line">            self.char_set = set(char_list)  <span class="comment"># 训练预料库中所有字的集合</span></span><br><span class="line"></span><br><span class="line">            word_list = line.split(<span class="string">" "</span>)</span><br><span class="line">            line_status = []  <span class="comment"># 统计状态序列</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> word_list:</span><br><span class="line">                line_status.extend(self.get_word_status(word))  <span class="comment"># 一句话对应一行连续的状态</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> len(char_list) == len(line_status):</span><br><span class="line">                <span class="comment"># print(word_list) # ['但', '从', '生物学', '眼光', '看', '就', '并非', '如此', '了', '。']</span></span><br><span class="line">                <span class="comment"># print(line_status) # ['S', 'S', 'B', 'M', 'E', 'B', 'E', 'S', 'S', 'B', 'E', 'B', 'E', 'S', 'S']</span></span><br><span class="line">                <span class="comment"># print('******')</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(len(line_status)):</span><br><span class="line">                    <span class="keyword">if</span> i == <span class="number">0</span>:  <span class="comment"># 如果只有一个词，则直接算作是初始概率</span></span><br><span class="line">                        start_dict[line_status[<span class="number">0</span>]] += <span class="number">1</span>  <span class="comment"># start_dict记录句子第一个字的状态，用于计算初始状态概率</span></span><br><span class="line">                        Count_dict[line_status[<span class="number">0</span>]] += <span class="number">1</span>  <span class="comment"># 记录每一个状态的出现次数</span></span><br><span class="line">                    <span class="keyword">else</span>:  <span class="comment"># 统计上一个状态到下一个状态，两个状态之间的转移概率</span></span><br><span class="line">                        trans_dict[line_status[i - <span class="number">1</span>]][line_status[i]] += <span class="number">1</span>  <span class="comment"># 用于计算转移概率</span></span><br><span class="line">                        Count_dict[line_status[i]] += <span class="number">1</span></span><br><span class="line">                        <span class="comment"># 统计发射概率</span></span><br><span class="line">                        <span class="keyword">if</span> char_list[i] <span class="keyword">not</span> <span class="keyword">in</span> emit_dict[line_status[i]]:</span><br><span class="line">                            emit_dict[line_status[i]][char_list[i]] = <span class="number">0.0</span></span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            emit_dict[line_status[i]][char_list[i]] += <span class="number">1</span>  <span class="comment"># 用于计算发射概率</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(emit_dict)#&#123;'S': &#123;'否': 10.0, '昔': 25.0, '直': 238.0, '六': 1004.0, '殖': 17.0, '仗': 36.0, '挪': 15.0, '朗': 151.0</span></span><br><span class="line">        <span class="comment"># print(trans_dict)#&#123;'S': &#123;'S': 747969.0, 'E': 0.0, 'M': 0.0, 'B': 563988.0&#125;, 'E': &#123;'S': 737404.0, 'E': 0.0, 'M': 0.0, 'B': 651128.0&#125;,</span></span><br><span class="line">        <span class="comment"># print(start_dict) #&#123;'S': 124543.0, 'E': 0.0, 'M': 0.0, 'B': 173416.0&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 进行归一化</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> start_dict:  <span class="comment"># 状态的初始概率</span></span><br><span class="line">            start_dict[key] = start_dict[key] * <span class="number">1.0</span> / self.line_index</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> trans_dict:  <span class="comment"># 状态转移概率</span></span><br><span class="line">            <span class="keyword">for</span> key1 <span class="keyword">in</span> trans_dict[key]:</span><br><span class="line">                trans_dict[key][key1] = trans_dict[key][key1] / Count_dict[key]</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> emit_dict:  <span class="comment"># 发射概率(状态-&gt;词语的条件概率)</span></span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> emit_dict[key]:</span><br><span class="line">                emit_dict[key][word] = emit_dict[key][word] / Count_dict[key]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(emit_dict)#&#123;'S': &#123;'否': 6.211504202703743e-06, '昔': 1.5528760506759358e-05, '直': 0.0001478338000243491,</span></span><br><span class="line">        <span class="comment"># print(trans_dict)#&#123;'S': &#123;'S': 0.46460125869921165, 'E': 0.0, 'M': 0.0, 'B': 0.3503213832274479&#125;,</span></span><br><span class="line">        <span class="comment"># print(start_dict)#&#123;'S': 0.41798844132394497, 'E': 0.0, 'M': 0.0, 'B': 0.5820149148537713&#125;</span></span><br><span class="line">        self.save_model(trans_dict, trans_path)</span><br><span class="line">        self.save_model(emit_dict, emit_path)</span><br><span class="line">        self.save_model(start_dict, start_path)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> trans_dict, emit_dict, start_dict</span><br></pre></td></tr></table></figure>
<p>Step 2: 使用维特比算法，求解概率最大路径</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HmmCut</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,start_model_path,trans_model_path,emit_model_path)</span>:</span></span><br><span class="line">        self.prob_trans = self.load_model(trans_model_path)</span><br><span class="line">        self.prob_emit = self.load_model(emit_model_path)</span><br><span class="line">        self.prob_start = self.load_model(start_model_path)</span><br><span class="line"></span><br><span class="line">    <span class="string">'''加载模型'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_model</span><span class="params">(self, model_path)</span>:</span></span><br><span class="line">        f = open(model_path, <span class="string">'r'</span>)</span><br><span class="line">        a = f.read()</span><br><span class="line">        word_dict = eval(a)</span><br><span class="line">        f.close()</span><br><span class="line">        <span class="keyword">return</span> word_dict</span><br><span class="line"></span><br><span class="line">    <span class="string">'''verterbi算法求解'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(self, obs, states, start_p, trans_p, emit_p)</span>:</span>  <span class="comment"># 维特比算法（一种递归算法）</span></span><br><span class="line">        <span class="comment"># 算法的局限在于训练语料要足够大，需要给每个词一个发射概率,.get(obs[0], 0)的用法是如果dict中不存在这个key,则返回0值</span></span><br><span class="line">        V = [&#123;&#125;]</span><br><span class="line">        path = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">            V[<span class="number">0</span>][y] = start_p[y] * emit_p[y].get(obs[<span class="number">0</span>], <span class="number">0</span>)  <span class="comment"># 在位置0，以y状态为末尾的状态序列的最大概率</span></span><br><span class="line">            path[y] = [y]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, len(obs)):</span><br><span class="line">            V.append(&#123;&#125;)</span><br><span class="line">            newpath = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">                state_path = ([(V[t - <span class="number">1</span>][y0] * trans_p[y0].get(y, <span class="number">0</span>) * emit_p[y].get(obs[t], <span class="number">0</span>), y0) <span class="keyword">for</span> y0 <span class="keyword">in</span> states <span class="keyword">if</span> V[t - <span class="number">1</span>][y0] &gt; <span class="number">0</span>])</span><br><span class="line">                <span class="keyword">if</span> state_path == []:</span><br><span class="line">                    (prob, state) = (<span class="number">0.0</span>, <span class="string">'S'</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    (prob, state) = max(state_path)</span><br><span class="line">                V[t][y] = prob</span><br><span class="line">                newpath[y] = path[state] + [y]</span><br><span class="line"></span><br><span class="line">            path = newpath  <span class="comment"># 记录状态序列</span></span><br><span class="line">        (prob, state) = max([(V[len(obs) - <span class="number">1</span>][y], y) <span class="keyword">for</span> y <span class="keyword">in</span> states])  <span class="comment"># 在最后一个位置，以y状态为末尾的状态序列的最大概率</span></span><br><span class="line">        <span class="keyword">return</span> (prob, path[state])  <span class="comment"># 返回概率和状态序列</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分词主控函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cut</span><span class="params">(self, sent)</span>:</span></span><br><span class="line">        prob, pos_list = self.viterbi(sent, (<span class="string">'B'</span>, <span class="string">'M'</span>, <span class="string">'E'</span>, <span class="string">'S'</span>), self.prob_start, self.prob_trans, self.prob_emit)</span><br><span class="line">        seglist = list()</span><br><span class="line">        word = list()</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(len(pos_list)):</span><br><span class="line">            <span class="keyword">if</span> pos_list[index] == <span class="string">'S'</span>:</span><br><span class="line">                word.append(sent[index])</span><br><span class="line">                seglist.append(word)</span><br><span class="line">                word = []</span><br><span class="line">            <span class="keyword">elif</span> pos_list[index] <span class="keyword">in</span> [<span class="string">'B'</span>, <span class="string">'M'</span>]:</span><br><span class="line">                word.append(sent[index])</span><br><span class="line">            <span class="keyword">elif</span> pos_list[index] == <span class="string">'E'</span>:</span><br><span class="line">                word.append(sent[index])</span><br><span class="line">                seglist.append(word)</span><br><span class="line">                word = []</span><br><span class="line">        seglist = [<span class="string">''</span>.join(tmp) <span class="keyword">for</span> tmp <span class="keyword">in</span> seglist]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> seglist</span><br></pre></td></tr></table></figure>
<p>如果画出状态转移图像，发现状态只能在层间点转移，转移路径上的分数为概率，求一条概率最大的路径。这可以用维特比算法计算，本质上就是一个动态规划</p>
<p><img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/hmm_viterbi.png" alt="avatar"></p>
<h5 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h5><p><strong>从三方面比较HMM和N-gram。</strong></p>
<ol>
<li>HMM模型是一个生成模型，区别于N-gram语言模型，HMM没有直接对给定观测值后状态的分布 $P(S|O)$（<em>O</em> 代表观测序列）进行建模，而是对状态序列本身的分布$P(S)$和给定状态后观测值的分布 $p(O|S)$建模 ；</li>
<li>学习过程与N-gram相同，HMM在有监督学习的情况下，使用极大似然估计参数；</li>
<li>预测时，HMM采用维特比算法。</li>
</ol>
<h4 id="CRF分词"><a href="#CRF分词" class="headerlink" title="CRF分词"></a>CRF分词</h4><h5 id="条件随机场CRF"><a href="#条件随机场CRF" class="headerlink" title="条件随机场CRF"></a>条件随机场CRF</h5><p>条件随机场基于概率无向图模型，利用最大团理论，对随机变量的联合分布$P(Y)$进行建模。</p>
<p>在序列标注任务中的条件随机场，往往是指<strong>线性链条件随机场</strong>，这时，在条件概率模型$P(Y|X)$中，$Y$是输出变量，表示标记序列（或状态序列），$X$是输入变量，表示需要标注的观测序列。学习时，利用训练数据集通过极大似然估计或正则化的极大似然估计得到条件概率模型$p(Y|X)$；预测时，对于给定的输入序列x，求出条件概率$p(y|x)$最大的输出序列y.</p>
<h6 id="条件随机场的参数化形式"><a href="#条件随机场的参数化形式" class="headerlink" title="条件随机场的参数化形式"></a>条件随机场的参数化形式</h6><script type="math/tex; mode=display">
P(y|x)=\frac{1}{Z(x)} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, j} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right) 
\\
Z(x)=\sum_{y} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, j} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right)</script><h5 id="条件随机场分词方法"><a href="#条件随机场分词方法" class="headerlink" title="条件随机场分词方法"></a>条件随机场分词方法</h5><p>条件随机场和隐马尔可夫一样，也是使用BMES四个状态位来进行分词。以如下句子为例：</p>
<p>中 国 是 泱 泱 大 国<br>B  B  B  B  B  B  B<br>M M M M M M M<br>E  E  E  E  E  E  E<br>S  S  S  S  S  S  S</p>
<p>条件随机场解码就是在以上由标记组成的数组中搜索一条最优的路径。</p>
<p>要把每一个字(即观察变量)对应的每一个状态BMES(即标记变量)的概率都求出来。例如对于观察变量“国”，当前标记变量为E，前一个观察变量为“中”，前一个标记变量为B，则：t(B, E, ‘国’) 对应到条件随机场里相邻标记变量$(y_{i-1},y_i)$的势函数。s(E, ‘国’) 对应到条件随机场里单个标记变量$y_i$对应的势函数$s_l(y_i,x,i)$。t(B, E, ‘国’), s(E, ‘国’)相应的权值$λ_k,\mu_l$， 都是由条件随机场用大量的标注语料训练出来。因此分词的标记识别就是求对于各个观察变量，它们的标记变量(BMES)状态序列的概率最大值，即求：的概率组合最大值。这个解法与隐马尔可夫类似，可以用viterbi算法求解。</p>
<h5 id="代码-5"><a href="#代码-5" class="headerlink" title="代码"></a><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5><p>使用crf++实现对模型的训练，crf++ 安装、数据格式及模板参数请参考：<a href="https://jeffery0628.github.io/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/">序列标注</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> CRFPP</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRFModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model=<span class="string">'model_name'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 类初始化</span></span><br><span class="line"><span class="string">        :param model: 模型名称</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_tagger</span><span class="params">(self, tag_data)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 添加语料</span></span><br><span class="line"><span class="string">        :param tag_data: 数据</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        word_str = tag_data.strip()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(self.model):</span><br><span class="line">            print(<span class="string">'模型不存在,请确认模型路径是否正确!'</span>)</span><br><span class="line">            exit()</span><br><span class="line">        tagger = CRFPP.Tagger(<span class="string">"-m &#123;&#125; -v 3 -n2"</span>.format(self.model))</span><br><span class="line">        tagger.clear()</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> word_str:</span><br><span class="line">            tagger.add(word)</span><br><span class="line">        tagger.parse()</span><br><span class="line">        <span class="keyword">return</span> tagger</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">text_mark</span><span class="params">(self, tag_data, begin=<span class="string">'B'</span>, middle=<span class="string">'I'</span>, end=<span class="string">'E'</span>, single=<span class="string">'S'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        文本标记</span></span><br><span class="line"><span class="string">        :param tag_data: 数据</span></span><br><span class="line"><span class="string">        :param begin: 开始标记</span></span><br><span class="line"><span class="string">        :param middle: 中间标记</span></span><br><span class="line"><span class="string">        :param end: 结束标记</span></span><br><span class="line"><span class="string">        :param single: 单字结束标记</span></span><br><span class="line"><span class="string">        :return result: 标记列表</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        tagger = self.add_tagger(tag_data)</span><br><span class="line">        size = tagger.size()</span><br><span class="line">        tag_text = <span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, size):</span><br><span class="line">            word, tag = tagger.x(i, <span class="number">0</span>), tagger.y2(i)</span><br><span class="line">            <span class="keyword">if</span> tag <span class="keyword">in</span> [begin, middle]:</span><br><span class="line">                tag_text += word</span><br><span class="line">            <span class="keyword">elif</span> tag <span class="keyword">in</span> [end, single]:</span><br><span class="line">                tag_text += word + <span class="string">"*&amp;*"</span></span><br><span class="line">        result = tag_text.split(<span class="string">'*&amp;*'</span>)</span><br><span class="line">        result.pop()</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crf_test</span><span class="params">(self, tag_data, separator=<span class="string">'/'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: crf测试</span></span><br><span class="line"><span class="string">        :param tag_data:</span></span><br><span class="line"><span class="string">        :param separator:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        result = self.text_mark(tag_data)</span><br><span class="line">        data = separator.join(result)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crf_learn</span><span class="params">(self, filename)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 训练模型</span></span><br><span class="line"><span class="string">        :param filename: 已标注数据源</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        crf_bash = <span class="string">"crf_learn -f 3 -c 4.0 data/template.txt &#123;&#125; &#123;&#125;"</span>.format(filename, self.model)</span><br><span class="line">        process = subprocess.Popen(crf_bash.split(), stdout=subprocess.PIPE)</span><br><span class="line">        output = process.communicate()[<span class="number">0</span>]</span><br><span class="line">        print(output.decode(encoding=<span class="string">'utf-8'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    crf_model = CRFModel(model=<span class="string">'data/model_crf'</span>)</span><br><span class="line">    crf_model.crf_learn(filename=<span class="string">'data/train_file_crf.txt'</span>)</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    print(crf_model.crf_test(tag_data=<span class="string">'我们一定要战胜敌人，我们认为它们都是纸老虎。'</span>))</span><br></pre></td></tr></table></figure>
<h5 id="条件随机场分词的优缺点"><a href="#条件随机场分词的优缺点" class="headerlink" title="条件随机场分词的优缺点"></a>条件随机场分词的优缺点</h5><p>条件随机场分词是一种精度很高的分词方法，它比隐马尔可夫的精度要高，是因为隐马尔可夫假设观察变量$x_i$只与当前状态$y_i$有关，而与其它状态$y_{i-1}$，$y_{i+1}$无关;而条件随机场假设了当前观察变量$x_i$与上下文相关，如 ，就是考虑到上一个字标记状态为B时，当前标记状态为E并且输出“国”字的概率。因此通过上下文的分析，条件随机场分词会提升到更高的精度。但因为复杂度比较高，条件随机场一般训练代价都比较大。</p>
<h4 id="结构化感知机分词算法"><a href="#结构化感知机分词算法" class="headerlink" title="结构化感知机分词算法"></a>结构化感知机分词算法</h4><p>利用隐马尔科夫模型实现基于序列标注的中文分词器，效果并不理想。事实上，隐马尔可夫模型假设人们说的话仅仅取决于一个隐藏的BMES序列，这个假设太单纯了，不符合语言规律。语言不是由这么简单的标签序列生成，语言含有更多特征，而隐马尔科夫模型没有捕捉到。<strong>隐马弥可夫模型能捕捉的特征仅限于两种: 其一，前一个标签是什么；其二，当前字符是什么</strong>。为了利用更多的特征，线性模型( linear model )应运而生。线性模型由两部分构成: 一系列用来提取特征的特征函数$\phi$，以及相应的权重向量$w$。</p>
<h5 id="感知机算法"><a href="#感知机算法" class="headerlink" title="感知机算法"></a>感知机算法</h5><p>感知机算法是一种迭代式的算法：在训练集上运行多个迭代，每次读入一个样本，执行预测，将预测结果与正确答案进行对比，计算误差，根据误差更新模型参数，再次进行训练，直到误差最小为止。</p>
<ul>
<li><strong>损失函数</strong>: 从数值优化的角度来讲，迭代式机器学习算法都在优化(减小)一个损失函数。损失函数 J(w) 用来衡量模型在训练集上的错误程度，自变量是模型参数 $w$，因变量是一个标量，表示模型在训练集上的损失的大小。</li>
<li><strong>梯度下降</strong>: 给定样本，其特征向量 $x$ 只是常数，对 $J(w)$ 求导，得到一个梯度向量 $\Delta w$，它的反方向一定是当前位置损失函数减小速度最快的方向。如果参数点 $w$ 反方向移动就会使损失函数减小，叫梯度下降。</li>
<li><strong>学习率</strong>: 梯度下降的步长叫做学习率。</li>
<li><strong>随机梯度下降</strong>(SGD): 如果算法每次迭代随机选取部分样本计算损失函数的梯度，则称为随机梯度下降。</li>
</ul>
<p>假如数据本身线性不可分，感知机损失函数不会收敛，每次迭代分离超平面都会剧烈振荡。这时可以对感知机算法打补丁，使用投票感知机或平均感知机。</p>
<ol>
<li><p><strong>投票感知机</strong>：每次迭代的模型都保留，准确率也保留，预测时，每个模型都给出自己的结果，乘以它的准确率加权平均值作为最终结果。</p>
</li>
<li><p>投票感知机要求存储多个模型及加权，计算开销较大，更实际的做法是取多个模型的权重的平均，这就是<strong>平均感知机</strong>。</p>
</li>
</ol>
<h5 id="结构化预测问题"><a href="#结构化预测问题" class="headerlink" title="结构化预测问题"></a>结构化预测问题</h5><p>自然语言处理问题大致可分为两类，一种是分类问题，另一种就是结构化预测问题，序列标注只是结构化预测的一个特例，对感知机稍作拓展，分类器就能支持结构化预测。<br>信息的层次结构特点称作结构化。<strong>那么结构化预测</strong>(structhre，prediction)则是预测对象结构的一类监督学习问题。相应的模型训练过程称作<strong>结构化学习</strong>(stutured laming )。分类问题的预测结果是一个决策边界， 回归问题的预测结果是一个实数标量，而结构化预测的结果则是一个完整的结构。<br>自然语言处理中有许多任务是结构化预测，比如序列标注预测结构是一整个序列，句法分析预测结构是一棵句法树，机器翻译预测结构是一段完整的译文。这些结构由许多部分构成，最小的部分虽然也是分类问题(比如中文分词时每个字符分类为{B,M,E,S} ),但必须考虑结构整体的合理程度。</p>
<h6 id="结构化预测与学习流程"><a href="#结构化预测与学习流程" class="headerlink" title="结构化预测与学习流程"></a>结构化预测与学习流程</h6><p>结构化预测的过程就是给定一个模型 λ 及打分函数 score，利用打分函数给一些备选结构打分，选择分数最高的结构作为预测输出，公式如下:</p>
<script type="math/tex; mode=display">
\hat{y}=\arg \max _{y \in Y} \operatorname{score}({\lambda}(x, y))</script><p>其中，Y 是备选结构的集合。既然结构化预测就是搜索得分最高的结构 y，那么结构化学习的目标就是想方设法让正确答案 y 的得分最高。不同的模型有不同的算法，对于线性模型，训练算法为结构化感知机。</p>
<h5 id="结构化感知机算法"><a href="#结构化感知机算法" class="headerlink" title="结构化感知机算法"></a>结构化感知机算法</h5><p>要让线性模型支持结构化预测，必须先设计打分函数。打分函数的输入有两个缺一不可的参数: 特征 $x$ 和结构$y$。但之前的线性模型的“打分函数”只接受一个自变量 $x$。做法是定义新的特征函数$\phi (x,y)$，把结构 $y$ 也作为一种特征，输出新的“结构化特征向量”。新特征向量与权重向量做点积后，就得到一个标量，将其作为分数:</p>
<script type="math/tex; mode=display">
\operatorname{score}(x, y)=w \cdot \phi(x, y)</script><p>打分函数有了，取分值最大的结构作为预测结果，得到结构化预测函数:</p>
<script type="math/tex; mode=display">
\hat{y}=\arg \max _{y \in Y}(w \cdot \phi(x, y))</script><p>预测函数与线性分类器的决策函数很像，都是权重向量点积特征向量。那么感知机算法也可以拓展复用，得到线性模型的结构化学习算法:</p>
<ol>
<li>读入样本 $(x,y)$，进行结构化预测$\hat{y}=\arg \max _{y \in Y}(w \cdot \phi(x, y))$</li>
<li>与正确答案相比，若不相等，则更新参数: 奖励正确答案触发的特征函数的权重，否则进行惩罚:$w \leftarrow w+\phi\left(x^{(i)}, y\right)-\phi\left(x^{(i)}, \hat{y}\right)$</li>
<li>调整学习率:$\boldsymbol{w} \leftarrow \boldsymbol{w}+\alpha\left(\phi\left(\boldsymbol{x}^{(i)}, \boldsymbol{y}\right)-\phi\left(\boldsymbol{x}^{(i)}, \hat{\boldsymbol{y}}\right)\right)$</li>
</ol>
<h6 id="结构化感知机与感知机算法比较"><a href="#结构化感知机与感知机算法比较" class="headerlink" title="结构化感知机与感知机算法比较"></a>结构化感知机与感知机算法比较</h6><ul>
<li>结构化感知机修改了特征向量。</li>
<li>结构化感知机的参数更新赏罚分明。</li>
</ul>
<h6 id="结构化感知机与序列标注"><a href="#结构化感知机与序列标注" class="headerlink" title="结构化感知机与序列标注"></a>结构化感知机与序列标注</h6><p>序列标注最大的结构特点就是标签相互之间的依赖性，这种依赖性利用初始状态概率想俩狗和状态转移概率矩阵体系那，那么对于结构化感知机，就可以使用<strong>转移特征</strong>来表示:</p>
<script type="math/tex; mode=display">
\phi_{k}\left(y_{t-1}, y_{t}\right)=\left\{\begin{array}{ll}
1, & y_{t-1}=s_{i}, and , y_{t}=s_{j} \\
0
\end{array} \quad i=0, \cdots, N ; j=1, \cdots, N\right.</script><p>其中，$y_t$为序列第 t 个标签，$s_i$为标注集第 i 种标签，N 为标注集大小。</p>
<p><strong>状态特征</strong>，类似于隐马尔可夫模型的发射概率矩阵，状态特征只与当前的状态有关，与之前的状态无关:</p>
<script type="math/tex; mode=display">
\phi_{i}\left(x_{i}, y_{i}\right)=\left\{\begin{array}{l}
1 \\
0
\end{array}\right.</script><p>于是，结构化感知机的特征函数就是转移特征和状态特征的合集:</p>
<script type="math/tex; mode=display">
\phi=\left[\phi_{k} ; \phi_{l}\right] \quad k=1, \cdots, N^{2}+N ; l=N^{2}+N+1, \cdots</script><p>基于以上公式，统一用打分函数来表示:</p>
<script type="math/tex; mode=display">
\operatorname{score}(\boldsymbol{x}, \boldsymbol{y})=\sum_{t=1}^{T} \boldsymbol{w} \cdot \phi\left(y_{t-1}, y_{t}, \boldsymbol{x}_{t}\right)</script><p>有了打分公式，就可以利用维特比算法求解得分最高的序列。</p>
<h5 id="代码-6"><a href="#代码-6" class="headerlink" title="代码"></a><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CPTTrain</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, segment, train)</span>:</span></span><br><span class="line">        self.__char_type = &#123;&#125;</span><br><span class="line">        data_path = <span class="string">"data"</span></span><br><span class="line">        <span class="keyword">for</span> ind, name <span class="keyword">in</span> enumerate([<span class="string">"punc"</span>, <span class="string">"alph"</span>, <span class="string">"date"</span>, <span class="string">"num"</span>]):</span><br><span class="line">            fn = data_path + <span class="string">"/"</span> + name</span><br><span class="line">            <span class="keyword">if</span> os.path.isfile(fn):</span><br><span class="line">                <span class="keyword">for</span> line <span class="keyword">in</span> open(fn, <span class="string">"r"</span>):</span><br><span class="line">                    self.__char_type[line.strip()] = ind</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">"can't open"</span>, fn)</span><br><span class="line">                exit()</span><br><span class="line"></span><br><span class="line">        self.__train_insts = <span class="literal">None</span>  <span class="comment"># all instances for training.</span></span><br><span class="line">        self.__feats_weight = <span class="literal">None</span>  <span class="comment"># ["b", "m", "e", "s"][all the features] --&gt; weight.</span></span><br><span class="line">        self.__words_num = <span class="literal">None</span>  <span class="comment"># total words num in all the instances.</span></span><br><span class="line">        self.__insts_num = <span class="literal">None</span>  <span class="comment"># namley the sentences' num.</span></span><br><span class="line">        self.__cur_ite_ID = <span class="literal">None</span>  <span class="comment"># current iteration index.</span></span><br><span class="line">        self.__cur_inst_ID = <span class="literal">None</span>  <span class="comment"># current index_th instance.</span></span><br><span class="line">        self.__real_inst_ID = <span class="literal">None</span>  <span class="comment"># the accurate index in training instances after randimizing.</span></span><br><span class="line">        self.__last_update = <span class="literal">None</span>  <span class="comment"># ["b".."s"][feature] --&gt; [last_update_ite_ID, last_update_inst_ID]</span></span><br><span class="line">        self.__feats_weight_sum = <span class="literal">None</span>  <span class="comment"># sum of ["b".."s"][feature] from begin to end.</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> segment <span class="keyword">and</span> train <span class="keyword">or</span> <span class="keyword">not</span> segment <span class="keyword">and</span> <span class="keyword">not</span> train:</span><br><span class="line">            print(<span class="string">"there is only a True and False in segment and train"</span>)</span><br><span class="line">            exit()</span><br><span class="line">        <span class="keyword">elif</span> train:</span><br><span class="line">            self.Train = self.__Train</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.__LoadModel()</span><br><span class="line">            self.Segment = self.__Segment</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__LoadModel</span><span class="params">(self)</span>:</span></span><br><span class="line">        model = <span class="string">"data/avgmodel"</span></span><br><span class="line">        print(<span class="string">"load"</span>, model, <span class="string">"..."</span>)</span><br><span class="line">        self.__feats_weight = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> os.path.isfile(model):</span><br><span class="line">            start = time.clock()</span><br><span class="line">            self.__feats_weight = pickle.load(open(model, <span class="string">"rb"</span>))</span><br><span class="line">            end = time.clock()</span><br><span class="line">            print(<span class="string">"It takes %d seconds"</span> % (end - start))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"can't open"</span>, model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Train</span><span class="params">(self, corp_file_name, max_train_num, max_ite_num)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.__LoadCorp(corp_file_name, max_train_num):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        starttime = time.clock()</span><br><span class="line"></span><br><span class="line">        self.__feats_weight = &#123;&#125;</span><br><span class="line">        self.__last_update = &#123;&#125;</span><br><span class="line">        self.__feats_weight_sum = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> self.__cur_ite_ID <span class="keyword">in</span> range(max_ite_num):</span><br><span class="line">            <span class="keyword">if</span> self.__Iterate():</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        self.__SaveModel()</span><br><span class="line">        endtime = time.clock()</span><br><span class="line">        print(<span class="string">"total iteration times is %d seconds"</span> % (endtime - starttime))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__GenerateFeats</span><span class="params">(self, inst)</span>:</span></span><br><span class="line">        inst_feat = []</span><br><span class="line">        <span class="keyword">for</span> ind, [c, tag, t] <span class="keyword">in</span> enumerate(inst):</span><br><span class="line">            inst_feat.append([])</span><br><span class="line">            <span class="keyword">if</span> t == <span class="number">-1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># Cn</span></span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">-2</span>, <span class="number">3</span>):</span><br><span class="line">                inst_feat[<span class="number">-1</span>].append(<span class="string">"C%d==%s"</span> % (n, inst[ind + n][<span class="number">0</span>]))</span><br><span class="line">            <span class="comment"># CnCn+1</span></span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">-2</span>, <span class="number">2</span>):</span><br><span class="line">                inst_feat[<span class="number">-1</span>].append(<span class="string">"C%dC%d==%s%s"</span> % (n, n + <span class="number">1</span>, inst[ind + n][<span class="number">0</span>], inst[ind + n + <span class="number">1</span>][<span class="number">0</span>]))</span><br><span class="line">            <span class="comment"># C-1C1</span></span><br><span class="line">            inst_feat[<span class="number">-1</span>].append(<span class="string">"C-1C1==%s%s"</span> % (inst[ind - <span class="number">1</span>][<span class="number">0</span>], inst[ind + <span class="number">1</span>][<span class="number">0</span>]))</span><br><span class="line">            <span class="comment"># Pu(C0)</span></span><br><span class="line">            inst_feat[<span class="number">-1</span>].append(<span class="string">"Pu(%s)==%d"</span> % (c, int(t == <span class="number">0</span>)))</span><br><span class="line">            <span class="comment"># T(C-2)T(C-1)T(C0)T(C1)T(C2)</span></span><br><span class="line">            inst_feat[<span class="number">-1</span>].append(<span class="string">"T-2...2=%d%d%d%d%d"</span> % (</span><br><span class="line">            inst[ind - <span class="number">2</span>][<span class="number">2</span>], inst[ind - <span class="number">1</span>][<span class="number">2</span>], inst[ind][<span class="number">2</span>], inst[ind + <span class="number">1</span>][<span class="number">2</span>], inst[ind + <span class="number">2</span>][<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> inst_feat</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__SaveModel</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># the last time to sum all the features.</span></span><br><span class="line">        norm = float(self.__cur_ite_ID + <span class="number">1</span>) * self.__insts_num</span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> self.__feats_weight_sum:</span><br><span class="line">            last_ite_ID = self.__last_update[feat][<span class="number">0</span>]</span><br><span class="line">            last_inst_ID = self.__last_update[feat][<span class="number">1</span>]</span><br><span class="line">            c = (self.__cur_ite_ID - last_ite_ID) * self.__insts_num + self.__cur_inst_ID - last_inst_ID</span><br><span class="line">            self.__feats_weight_sum[feat] += self.__feats_weight[feat] * c</span><br><span class="line">            self.__feats_weight_sum[feat] = self.__feats_weight_sum[feat] / norm</span><br><span class="line"></span><br><span class="line">        pickle.dump(self.__feats_weight_sum, open(<span class="string">"data/avgmodel"</span>, <span class="string">"wb"</span>))</span><br><span class="line">        self.__train_insts = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__LoadCorp</span><span class="params">(self, corp_file_name, max_train_num)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(corp_file_name):</span><br><span class="line">            print(<span class="string">"can't open"</span>, corp_file_name)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        self.__train_insts = []</span><br><span class="line">        self.__words_num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> ind, line <span class="keyword">in</span> enumerate(open(corp_file_name, <span class="string">"r"</span>)):</span><br><span class="line">            <span class="keyword">if</span> max_train_num &gt; <span class="number">0</span> <span class="keyword">and</span> ind &gt;= max_train_num:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            self.__train_insts.append(self.__PreProcess(line.strip()))</span><br><span class="line">            self.__words_num += len(self.__train_insts[<span class="number">-1</span>]) - <span class="number">4</span></span><br><span class="line">        self.__insts_num = len(self.__train_insts)</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"number of total insts is"</span>, self.__insts_num)</span><br><span class="line">        print(<span class="string">"number of total characters is"</span>, self.__words_num)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__PreProcess</span><span class="params">(self, sent)</span>:</span></span><br><span class="line">        inst = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            inst.append([<span class="string">"&lt;s&gt;"</span>, <span class="string">"s"</span>, <span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sent.split():</span><br><span class="line">            rt = word.rpartition(<span class="string">"/"</span>)</span><br><span class="line">            t = self.__char_type.get(rt[<span class="number">0</span>], <span class="number">4</span>)</span><br><span class="line">            inst.append([rt[<span class="number">0</span>], rt[<span class="number">2</span>], t])  <span class="comment"># [c, tag, t]</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            inst.append([<span class="string">"&lt;s&gt;"</span>, <span class="string">"s"</span>, <span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> inst</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Segment</span><span class="params">(self, src)</span>:</span></span><br><span class="line">        <span class="string">"""suppose there is one sentence once."""</span></span><br><span class="line">        inst = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            inst.append([<span class="string">"&lt;s&gt;"</span>, <span class="string">"s"</span>, <span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> src:</span><br><span class="line">            inst.append([c, <span class="string">""</span>, self.__char_type.get(c, <span class="number">4</span>)])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            inst.append([<span class="string">"&lt;s&gt;"</span>, <span class="string">"s"</span>, <span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        feats = self.__GenerateFeats(inst)</span><br><span class="line">        tags = self.__DPSegment(inst, feats)</span><br><span class="line"></span><br><span class="line">        rst = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(tags) - <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">if</span> tags[i] <span class="keyword">in</span> [<span class="string">"s"</span>, <span class="string">"b"</span>]:</span><br><span class="line">                rst.append(inst[i][<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                rst[<span class="number">-1</span>] += inst[i][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">" "</span>.join(rst)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Iterate</span><span class="params">(self)</span>:</span></span><br><span class="line">        start = time.clock()</span><br><span class="line">        print(<span class="string">"%d th iteration"</span> % self.__cur_ite_ID)</span><br><span class="line"></span><br><span class="line">        train_list = random.sample(range(self.__insts_num), self.__insts_num)</span><br><span class="line">        error_sents_num = <span class="number">0</span></span><br><span class="line">        error_words_num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> self.__cur_inst_ID, self.__real_inst_ID <span class="keyword">in</span> enumerate(train_list):</span><br><span class="line">            num = self.__TrainInstance()</span><br><span class="line">            error_sents_num += <span class="number">1</span> <span class="keyword">if</span> num &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            error_words_num += num</span><br><span class="line"></span><br><span class="line">        st = <span class="number">1</span> - float(error_sents_num) / self.__insts_num</span><br><span class="line">        wt = <span class="number">1</span> - float(error_words_num) / self.__words_num</span><br><span class="line"></span><br><span class="line">        end = time.clock()</span><br><span class="line">        print(<span class="string">"sents accuracy = %f%%, words accuracy = %f%%, it takes %d seconds"</span> % (st * <span class="number">100</span>, wt * <span class="number">100</span>, end - start))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> error_sents_num == <span class="number">0</span> <span class="keyword">and</span> error_words_num == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__TrainInstance</span><span class="params">(self)</span>:</span></span><br><span class="line">        cur_inst = self.__train_insts[self.__real_inst_ID]</span><br><span class="line">        feats = self.__GenerateFeats(cur_inst)</span><br><span class="line"></span><br><span class="line">        seg = self.__DPSegment(cur_inst, feats)</span><br><span class="line">        <span class="keyword">return</span> self.__Correct(seg, feats)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__DPSegment</span><span class="params">(self, inst, feats)</span>:</span></span><br><span class="line">        num = len(inst)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get all position's score.</span></span><br><span class="line">        value = [&#123;&#125; <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, num - <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> [<span class="string">"b"</span>, <span class="string">"m"</span>, <span class="string">"e"</span>, <span class="string">"s"</span>]:</span><br><span class="line">                value[i][t] = self.__GetScore(i, t, feats)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># find optimal path.</span></span><br><span class="line">        tags = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]</span><br><span class="line">        best = [<span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]  <span class="comment"># best[i]: [i, i + length(i)) is optimal segment.</span></span><br><span class="line">        length = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num - <span class="number">2</span> - <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">for</span> dis <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">                <span class="keyword">if</span> i + dis &gt; num - <span class="number">2</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                cur_score = best[i + dis]</span><br><span class="line">                self.__Tag(i, i + dis, tags)</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(i, i + dis):</span><br><span class="line">                    cur_score += value[k][tags[k]]</span><br><span class="line">                <span class="keyword">if</span> length[i] <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> cur_score &gt; best[i]:</span><br><span class="line">                    best[i] = cur_score</span><br><span class="line">                    length[i] = dis</span><br><span class="line"></span><br><span class="line">        i = <span class="number">2</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; num - <span class="number">2</span>:</span><br><span class="line">            self.__Tag(i, i + length[i], tags)</span><br><span class="line">            i += length[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tags</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__GetScore</span><span class="params">(self, pos, t, feats)</span>:</span></span><br><span class="line">        pos_feats = feats[pos]</span><br><span class="line">        score = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> pos_feats:</span><br><span class="line">            score += self.__feats_weight.get(feat + <span class="string">"=&gt;"</span> + t, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Tag</span><span class="params">(self, f, t, tags)</span>:</span></span><br><span class="line">        <span class="string">"""tag the sequence tags in the xrange of [f, t)"""</span></span><br><span class="line">        <span class="keyword">if</span> t - f == <span class="number">1</span>:</span><br><span class="line">            tags[f] = <span class="string">"s"</span></span><br><span class="line">        <span class="keyword">elif</span> t - f &gt;= <span class="number">2</span>:</span><br><span class="line">            tags[f], tags[t - <span class="number">1</span>] = <span class="string">"b"</span>, <span class="string">"e"</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(f + <span class="number">1</span>, t - <span class="number">1</span>):</span><br><span class="line">                tags[i] = <span class="string">"m"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Correct</span><span class="params">(self, tags, feats)</span>:</span></span><br><span class="line">        updates = &#123;&#125;</span><br><span class="line">        cur_inst = self.__train_insts[self.__real_inst_ID]</span><br><span class="line">        error_words_num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(cur_inst) - <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">if</span> tags[i] == cur_inst[i][<span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            error_words_num += <span class="number">1</span></span><br><span class="line">            pos_feats = feats[i]</span><br><span class="line">            target = cur_inst[i][<span class="number">1</span>]</span><br><span class="line">            mine = tags[i]</span><br><span class="line">            <span class="keyword">for</span> feat <span class="keyword">in</span> pos_feats:</span><br><span class="line">                updates[feat + <span class="string">"=&gt;"</span> + target] = updates.get(feat + <span class="string">"=&gt;"</span> + target, <span class="number">0.0</span>) + <span class="number">1</span></span><br><span class="line">                updates[feat + <span class="string">"=&gt;"</span> + mine] = updates.get(feat + <span class="string">"=&gt;"</span> + mine, <span class="number">0.0</span>) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        self.__Update(updates)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> error_words_num</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Update</span><span class="params">(self, updates)</span>:</span></span><br><span class="line">        <span class="comment"># update the features weight.</span></span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> updates:</span><br><span class="line">            pair = self.__last_update.get(feat, [<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">            last_ite_ID = pair[<span class="number">0</span>]</span><br><span class="line">            last_inst_ID = pair[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            c = (self.__cur_ite_ID - last_ite_ID) * self.__insts_num + self.__cur_inst_ID - last_inst_ID</span><br><span class="line">            self.__feats_weight_sum[feat] = self.__feats_weight_sum.get(feat, <span class="number">0</span>) + c * self.__feats_weight.get(feat, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            self.__feats_weight[feat] = self.__feats_weight.get(feat, <span class="number">0</span>) + updates[feat]</span><br><span class="line">            self.__last_update[feat] = [self.__cur_ite_ID, self.__cur_inst_ID]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"></span><br><span class="line">    train = CPTTrain(train=<span class="literal">True</span>, segment=<span class="literal">False</span>)</span><br><span class="line">    train.Train(<span class="string">"data/msr_train.txt"</span>, max_train_num=<span class="number">1000000</span>, max_ite_num=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    srcs = [<span class="string">"夏天的清晨"</span>,</span><br><span class="line">            <span class="string">"“人们常说生活是一部教科书，而血与火的战争更是不可多得的教科书，她确实是名副其实的‘我的大学’。"</span>,</span><br><span class="line">            <span class="string">"夏天的清晨夏天看见猪八戒和嫦娥了。"</span>,</span><br><span class="line">            <span class="string">"海运业雄踞全球之首，按吨位计占世界总数的１７％。"</span>]</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"avg"</span>)</span><br><span class="line">    seg = CPTTrain(train=<span class="literal">False</span>, segment=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> src <span class="keyword">in</span> srcs:</span><br><span class="line">        print(seg.Segment(src))</span><br></pre></td></tr></table></figure>
<h4 id="基于深度学习的端到端的分词方法"><a href="#基于深度学习的端到端的分词方法" class="headerlink" title="基于深度学习的端到端的分词方法"></a>基于深度学习的端到端的分词方法</h4><p>在中文分词上，基于神经网络的方法，往往使用「字向量 + BiLSTM + CRF」模型，利用神经网络来学习特征，将传统 CRF 中的人工特征工程量降到最低</p>
<p>BiLSTM、BiLSTM+CRF、BiLSTM+CNN+CRF、BERT、BERT+CRF、BERT+BiLSTM、BERT+BiLSTM+CRF，由于以上模型均可对序列标注任务进行建模求解，所以均可拿来做中文分词。以比较典型的「字向量 + BiLSTM （+CNN）+ CRF」模型为例：</p>
<p>BiLSTM融合两组学习方向相反（一个按句子顺序，一个按句子逆序）的LSTM层，能够在理论上实现当前词即包含历史信息、又包含未来信息，更有利于对当前词进行标注。虽然依赖于神经网络强大的非线性拟合能力，理论上我们已经能够学习出不错的模型。但是，BiLSTM只考虑了标签上的上下文信息。对于序列标注任务来说，当前位置的标签$y_t$与前一个位置$y_{t-1}$、后一个位置$y_{t+1}$都有潜在的关系。例如，“东南大学欢迎您”被标注为“东/S 南/M 大/M 学/E 欢/B 迎/E 您/S”，由分词的标注规则可知，B标签后只能接M和E，BiLSTM没有利用这种标签之间的上下文信息。因此，就有人提出了在模型后接一层CRF层，用于在整个序列上学习最优的标签序列：</p>
<p><img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/bilstm_crf.jpeg" alt="avatar"></p>
<p>BiLSTM+CNN+CRF：对于分词任务，当前词的标签基本上只与前几个和和几个词有关联。BiLSTM在学习较长句子时，可能因为模型容量问题丢弃一些重要信息，因此在模型中加了一个CNN层，用于提取当前词的局部特征。（不知效果怎样？？！！）</p>
<p><img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/bilstm_cnn_crf.png" alt="avatar"></p>
<h5 id="模型比较"><a href="#模型比较" class="headerlink" title="模型比较"></a>模型比较</h5><p>Trained and tested with pku dataset</p>
<p>CRF</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">Template</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">crf_template</td>
<td style="text-align:center">0.938</td>
<td style="text-align:center">0.923</td>
<td style="text-align:center">0.931</td>
</tr>
</tbody>
</table>
</div>
<p>Bi-LSTM</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">Structure</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">emb256_hid256_l3</td>
<td style="text-align:center">0.9252</td>
<td style="text-align:center">0.9237</td>
<td style="text-align:center">0.9243</td>
</tr>
</tbody>
</table>
</div>
<p>Bi-LSTM + CRF</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">Structure</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">emb256_hid256_l3</td>
<td style="text-align:center">0.9343</td>
<td style="text-align:center">0.9336</td>
<td style="text-align:center">0.9339</td>
</tr>
</tbody>
</table>
</div>
<p>BERT + Bi-LSTM</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">Structure</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">emb768_hid512_l2</td>
<td style="text-align:center">0.9698</td>
<td style="text-align:center">0.9650</td>
<td style="text-align:center">0.9646</td>
</tr>
</tbody>
</table>
</div>
<h5 id="代码-7"><a href="#代码-7" class="headerlink" title="代码"></a>代码</h5><p>以BERT+RNN+CRF为例，可根据需求进行改动。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> TorchCRF <span class="keyword">import</span> CRF </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertRNNCRF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_tags, rnn_type, bert_path, bert_train, seg_vocab_size,hidden_dim, n_layers, bidirectional, batch_first,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout, restrain)</span>:</span></span><br><span class="line">        super(BertRNNCRF, self).__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="comment"># 对bert进行训练</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.bert.named_parameters():</span><br><span class="line">            param.requires_grad = bert_train</span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                               hidden_size=hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        self.crf = CRF(num_tags, batch_first=<span class="literal">True</span>, restrain_matrix=restrain, loss_side=<span class="number">2.5</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            self.fc_tags = nn.Linear(hidden_dim*<span class="number">2</span>, num_tags)</span><br><span class="line">            <span class="comment"># self.fc_tags = nn.Linear(768, num_tags)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.fc_tags = nn.Linear(hidden_dim, num_tags)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, context, seq_len,max_seq_len, mask_bert)</span>:</span></span><br><span class="line">        <span class="comment"># context    输入的句子</span></span><br><span class="line">        <span class="comment"># mask_bert  对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        bert_sentence, bert_cls = self.bert(context, attention_mask=mask_bert)</span><br><span class="line">        <span class="comment"># sentence_len = bert_sentence.shape[1]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># bert_cls = bert_cls.unsqueeze(dim=1).repeat(1, sentence_len, 1)</span></span><br><span class="line">        <span class="comment"># bert_sentence = bert_sentence + bert_cls</span></span><br><span class="line">        encoder_out, sorted_seq_lengths, desorted_indices = self.prepare_pack_padded_sequence(bert_sentence, seq_len)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(encoder_out, sorted_seq_lengths,</span><br><span class="line">                                                            batch_first=self.batch_first)</span><br><span class="line">        <span class="comment"># self.rnn.flatten_parameters()</span></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        <span class="comment"># output = [ batch size,sent len, hidden_dim * bidirectional]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first,total_length=max_seq_len)</span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        out = self.fc_tags(self.dropout(output.contiguous()))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out[:, <span class="number">1</span>:, :]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_pack_padded_sequence</span><span class="params">(self, inputs_words, seq_lengths, descending=True)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param device:</span></span><br><span class="line"><span class="string">        :param inputs_words:</span></span><br><span class="line"><span class="string">        :param seq_lengths:</span></span><br><span class="line"><span class="string">        :param descending:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        sorted_seq_lengths, indices = torch.sort(seq_lengths, descending=descending)</span><br><span class="line">        _, desorted_indices = torch.sort(indices, descending=<span class="literal">False</span>)</span><br><span class="line">        sorted_inputs_words = inputs_words[indices]</span><br><span class="line">        <span class="keyword">return</span> sorted_inputs_words, sorted_seq_lengths, desorted_indices</span><br></pre></td></tr></table></figure>
<h3 id="分词中的难题"><a href="#分词中的难题" class="headerlink" title="分词中的难题"></a>分词中的难题</h3><p>中文是一种十分复杂的语言，让计算机理解中文语言更是困难。在中文分词过程中，有两大难题一直没有完全突破。</p>
<h4 id="1、歧义切分问题"><a href="#1、歧义切分问题" class="headerlink" title="1、歧义切分问题"></a>1、歧义切分问题</h4><p>歧义是指同样的一句话，可能有两种或者更多的切分方法。</p>
<ul>
<li>交集歧义：“结合成”，结合/成，结/合成</li>
<li>组合歧义：“起身”，他站起/身/来。 他明天/起身/去北京。</li>
<li>混合型歧义：同时具备交集歧义和组合歧义的特点。1，这篇文章写得太平淡了。2，这墙抹的太平了。3，即使太平时期也不应该放松警惕。“太平淡”是交集型，“太平”是组合型。 </li>
</ul>
<p>交集歧义相对组合歧义来说是还算比较容易处理，组合歧义需要根据整个句子来判断。（特征：上下文语义分析，韵律分析，语气，重音，停顿）</p>
<h4 id="2、未登录词"><a href="#2、未登录词" class="headerlink" title="2、未登录词"></a>2、未登录词</h4><p>未登录词（生词，新词）：1.已有的词表中没有收录的词。2.已有的训练语料中未曾出现的词（集外词OOV）。</p>
<p>类型：</p>
<ol>
<li>新出现的普通词汇：博客，超女，给力。</li>
<li>专有名词：人名、地名、组织名、时间、数字表达、</li>
<li>专业名词和研究领域名词：苏丹红，禽流感</li>
<li>其他专用名词：新出现的产品名，电影，书记。</li>
</ol>
<p>对于真实数据来说，未登录词对分词精度的影响远远超过了歧义切分。未登录词是很难处理的问题，而且这些又正好是人们经常使用的词，因此对于搜索引擎来说，分词系统中的未登录词识别十分重要。目前未登录词识别准确率已经成为评价一个分词系统好坏的重要标志之一。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><p><a href="https://blog.csdn.net/selinda001/article/details/79345072" target="_blank" rel="noopener">中文分词引擎 java 实现 — 正向最大、逆向最大、双向最大匹配法</a></p>
</li>
<li><p><a href="https://www.cnblogs.com/sxron/articles/6391926.html" target="_blank" rel="noopener">中文分词算法总结</a></p>
</li>
<li><p><a href="https://blog.csdn.net/Zh823275484/article/details/87878512" target="_blank" rel="noopener">基于n-gram模型的中文分词</a></p>
</li>
<li><p><a href="https://blog.csdn.net/qq_27825451/article/details/102457058" target="_blank" rel="noopener">详解语言模型NGram及困惑度Perplexity</a></p>
</li>
<li><p><a href="https://blog.csdn.net/wangliang_f/article/details/17532633" target="_blank" rel="noopener">分词学习(3)，基于ngram语言模型的n元分词</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/33261835" target="_blank" rel="noopener">中文分词算法简介</a></p>
</li>
<li><p><a href="https://www.jianshu.com/p/715fa597c6bc" target="_blank" rel="noopener">NLP：分词算法综述</a></p>
</li>
<li><p><a href="https://github.com/NLP-LOVE/Introduction-NLP/blob/master/chapter/5.%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%88%86%E7%B1%BB%E4%B8%8E%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8.md" target="_blank" rel="noopener">感知机分类与序列标注</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>自然语言处理基础</category>
      </categories>
      <tags>
        <tag>自然语言处理基础</tag>
        <tag>自然语言处理</tag>
        <tag>分词算法</tag>
      </tags>
  </entry>
  <entry>
    <title>文本分类模型及比较</title>
    <url>/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>文本分类可以用于新闻分类、简历分类、邮件分类、办公文档分类、区域分类等诸多方面，还能够实现文本过滤，从大量文本中快速识别和过滤出符合特殊要求的信息。</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>准备了长短文本，二分类十分类进行比较</p>
<h4 id="情感二分类"><a href="#情感二分类" class="headerlink" title="情感二分类"></a>情感二分类</h4><p>weibo_senti_100k：共119988条数据，正例：59993,负例59995   </p>
<p>句子最大长度：260，最小长度：3，平均长度：66.04</p>
<p>部分样例</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>label</th>
<th>review</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]</td>
</tr>
<tr>
<td>1</td>
<td>@张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心，酒店都全部OK啦。</td>
</tr>
<tr>
<td>1</td>
<td>姑娘都羡慕你呢…还有招财猫高兴……//@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢//@李欣芸SharonLee:大佬范儿[书呆子]</td>
</tr>
<tr>
<td>1</td>
<td>美<del>~</del>[爱你]</td>
</tr>
<tr>
<td>1</td>
<td>梦想有多大，舞台就有多大![鼓掌]</td>
</tr>
<tr>
<td>0</td>
<td>今天真冷啊，难道又要穿棉袄了[晕]？今年的春天真的是百变莫测啊[抓狂]</td>
</tr>
<tr>
<td>0</td>
<td>[衰][衰][衰]像给剥了皮的蛇</td>
</tr>
<tr>
<td>0</td>
<td>酒驾的危害，这回是潜水艇。//@上海译文丁丽洁:[泪]</td>
</tr>
<tr>
<td>0</td>
<td>积压了这么多的枕边书，没一本看完了的，现在我读书的最佳地点尽然是公交车[晕]</td>
</tr>
<tr>
<td>0</td>
<td>[泪]错过了……</td>
</tr>
</tbody>
</table>
</div>
<p><a href="https://github.com/Embedding/Chinese-Word-Vectors" target="_blank" rel="noopener">词向量下载</a></p>
<p><a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">中文预训练bert模型下载</a></p>
<p><a href="https://github.com/jeffery0628/text_classification" target="_blank" rel="noopener">情感二分类github仓库代码</a></p>
<a id="more"></a>
<h4 id="新闻十分类"><a href="#新闻十分类" class="headerlink" title="新闻十分类"></a>新闻十分类</h4><p>类别：’体育’ ‘娱乐’ ‘家居’ ‘房产’ ‘教育’ ‘时尚’ ‘时政’ ‘游戏’ ‘科技’ ‘财经’<br>训练集：50000 条数据，最大长度：27467，最小长度：8，类别个数：10,平均长度：913.31<br>验证集：5000 条数据，最大长度：10919，最小长度：15，类别个数：10<br>测试集：10000 条数据，最大长度：14720，最小长度：13，类别个数：10</p>
<p>使用数据集<a href="https://pan.baidu.com/s/1OOTK374IZ1DHnz5COUcfbw" target="_blank" rel="noopener">cnews</a>  密码:hf6o</p>
<p>训练集部分样例及每个类别的统计：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>label</th>
<th>text</th>
<th>数量</th>
</tr>
</thead>
<tbody>
<tr>
<td>体育</td>
<td>黄蜂vs湖人首发：科比带伤战保罗 加索尔救赎之战 新浪体育讯北京时间4月27日，NBA季后赛首轮洛杉矶湖人主场迎战新奥尔良黄蜂，此前的比赛中，双方战成2-2平，因此本场比赛对于两支球队来说都非常重要，赛前双方也公布了首发阵容：湖人队：费舍尔、科比、阿泰斯特、加索尔、拜纳姆黄蜂队：保罗、贝里内利、阿里扎、兰德里、奥卡福[新浪NBA官方微博][新浪NBA湖人新闻动态微博][新浪NBA专题]<a href="新浪体育">黄蜂vs湖人图文直播室</a></td>
<td>5000</td>
</tr>
<tr>
<td>娱乐</td>
<td>皮克斯首部3D动画《飞屋历险记》预告发布(图)视频：动画片《飞屋历险记》先行版43秒预告新浪娱乐讯 迪士尼、皮克斯2009暑期3D动画力作《飞屋历险记》(Up)发布预告片，虽然这款预告片仅有43秒，并且只出现了被汽球吊起来的房屋，但门前老爷爷卡尔的一声“下午好”着实让人忍俊不禁。该片由《怪兽电力公司》导演彼特·道格特(Pete Docter)执导，曾在《海底总动员》、《料理鼠王》担任编剧的皮克斯老班底鲍勃-派特森(Bob Peterson)亦将在本片担任共同导演，献出自己的导演处女作。《飞屋历险记》同时会是皮克斯有史以来第一部以3-D电影呈现的里程碑作品，之后皮克斯的所有影片都将制作成立体电影。《飞屋历险记》讲述了一老一少的冒险旅程。78岁的老翁卡尔·弗雷德里克森(Carl Fredricksen)一生中都梦想着能环游世界、出没于异境险地体验，却平淡地渡过了一生。在他生活的最后阶段，卡而仿佛在命运的安排下，带着8岁的亚裔小鬼头Russell一同踏上了冒险的旅程。这对一老一小的奇特组合肩并肩闯荡江湖，共同经历了荒野跋涉、丛林野兽与反派坏蛋的狙击。田野/文</td>
<td>5000</td>
</tr>
<tr>
<td>家居</td>
<td>橱柜价格关键在计价方式 教你如何挑选买过橱柜的人都知道，橱柜的计价很复杂，商家的报价方式也不尽相同，那么哪种计价方式对消费者更有利？在计价过程中应该注意哪些问题？消费者在购买橱柜之前一定要了解清楚。 橱柜的主要计价方式——延米计价和单元柜体计价 现在市场上橱柜主要有两种计价方式——延米计价和单元柜体计价。 延米计价是指地柜和吊柜各一米的总价(有些还包含台面)。在此基础上，如果有局部区域只要地柜不要吊柜，就会按就会按“2/8”或“4/6”的比例折算。如某橱柜材料的延米价为2000元/延米，某顾客做2米的吊柜、4米的地柜，则吊柜价=2000X0.4X2=1600元，地柜价=2000X0.6X4=4800元(此吊柜、地柜价按4/6的比例计算)，再加上所选台面、配件、电器等附加费用即为整套橱柜的价格。 延米报价有许多不合理之处，水槽、燃气灶、嵌入式电器等部分所需门板很少，但仍按延米来算价，对消费者来说很不划算。例如一款1000元/延米的橱柜，一个水槽约0.8米长，但消费者还是要按1000元的单价乘以0.8米付费，这个实际上只是几块材料简单组合的水槽柜需要消费者花800元，而同样材质、同样大小的水槽柜仅需400元左右，二者价格相差数百元。 按延米计价，所有的配件费用都是在原有的基础上增加，虽然有些厂家宣称抽屉、拉篮不加钱，但其实那是最基本的配置，一旦顾客要求调整方案，就会要多加钱，此外不足一米的部分要按一米计价，因此对顾客来说，如此计价会多花不少冤枉钱。 “单元柜体计价”是国际惯例的橱柜计价方式，是按每一个组成厨柜的单元柜价格进行计算，然后加出总价。具体为：某吊柜单价×个数+某地柜单价×个数……。利用单元柜体计价，更为合理。举个例子说，外观相同的柜体，抽屉数量、五金件、托架数量如果不同，在以延米计价时，商家往往只给消费者最简单、最省成本的产品。而按单元柜体计价，一款尺寸相同的抽屉柜可按不同配置报出不同价格：同样是一款30cm宽、66cm高的单体柜，如果门改成弧型，是多少钱；如果抽屉里加上可拆装的金属篮，是多少钱；如果抽屉的侧板是木质的多少钱……把橱柜的每个细节都分解开来，消费者可以在预算之内把可有可无的配置省掉，把钱花在自己更需要的功能上。 两全其美报价方式——延米计价和单元柜体计价相结合 现在中国橱柜市场上仍普遍采用延米计价，但进口品牌及国内一些大品牌橱柜都采用单元柜体计价方式，如德宝·西克曼、海尔等品牌即是采用单元柜体计价方式。不过德宝·西克曼厨柜的工作人员介绍到，如果一开始就用单元柜体计价来进行报价，不够直观，同时为了便于顾客进行比较，他们会用延米计价给顾客所选定的材料进行一个初始报价，让顾客对自己的厨房装修要花多少钱心里大概有个底。在对厨房进行量尺后，设计师会按照顾客的需求，设计出厨房效果图。这时，销售人员会按单元柜体计价给顾客进行一个报价。对于每一种标准柜体都有相应的报价，顾客实际用到几组柜子，将这些柜子价格累加，再加上台面及其他相关费用，便是整个橱柜的价格。</td>
<td>5000</td>
</tr>
<tr>
<td>房产</td>
<td>冯仑：继续增持高GDP城市商业地产确立商业地产投资战略不久的万通地产(企业专区,旗下楼盘)(600246)，今年上半年遭遇了业绩下滑。公司昨日公布的半年报显示，其商业物业在报告期内实现的营业收入同比下降33.71%，营业利润率比上年同期下降47.29个百分点。不过，公司董事长冯仑日前表示，依然看好人均GDP8000美元以上城市的商业地产，万通将继续增加高GDP城市的商业地产；计划用5-10年，商业物业收入占比达到30%-50%。逆向运作地产投资冯仑指出，根据历史经验，GDP的增长、城市化的增长，和房地产物业形态有一定关系，即人均GDP在8000美元以下时，住宅是市场的核心，主流产品都将围绕住宅展开。目前，在中国的城市中，人均GDP8000美元的城市大约有十个，大部分省会城市依然在3000美元至5000美元之间，因此，未来5-10年，中国房地产市场的产品结构仍然是以住宅为主。 冯仑认为，万通地产从现在开始扩大商业地产的比重，在目前的市场中，是一种逆向运作的思维，但符合长期趋势。他指出，在人均GDP达到8000美元的经济实体中，商用不动产会成为地产业的主角。以美国为例，商业地产的市场规模大约是住宅的两倍。中国商业地产未来的市场空间很大。根据万通地产的发展战略，除了在环渤海区域内发展住宅以外，还会重点发展商业不动产。未来，公司业务结构将逐步调整，商用地产的收入会逐年增加；今后，公司商业物业收入将占到整体营业收入的一半左右。对于目前商业地产面临的不景气局面，万通地产董事会秘书程晓?指出，公司战略不会因市场的短期波动而改变，公司将继续加大商用物业项目的投资力度，以营运带动开发，以财务安排的多样化实施商用物业投资。改变商业模式冯仑表示，就房地产开发模式而言，过去两百年主要经历了三次变化，即从“地主加工头”到“厂长加资本家”，再到“导演加制片”。目前，国内多数地产商的开发模式属于“地主加工头”和“厂长加资本家”的阶段；而商业地产的开发模式，不能停留在这两个阶段。所谓“导演加制片”模式，即由专业的房地产投资和资产管理公司负责运营商业地产项目，实现收入的多元化。而这种模式需要相应的金融创新产品支持。业内人士指出，房地产金融领域内的REITS、抵押贷款等金融产品体系的完善，将支持商用地产在一个多元化的不动产经营环境中快速的成长。而商业模式的改变需要较长一段时间。数据显示，香港主流房地产企业在人均GDP10000美元的时候开始逐步发展商业地产，先后经过13-15年确立起新的商业模式。其中，长江实业经过13年的发展，商业地产在业务机构的比重占到30%，新鸿基则经过15年的调整，商业地产比重占到50%。SOHO中国(企业专区,旗下楼盘)董事长潘石屹也指出，现在的市场虽然在调整，不过也给从事商业地产开发的企业提供了良好机会和平台，应及时在地域、开发物业的品种、品牌的建设、销售和持有物业的比重四个方面做出调整。 我要评论</td>
<td>5000</td>
</tr>
<tr>
<td>教育</td>
<td>2010年6月英语六级考试考后难度调查2010年6月大学英语六级考试将于19日下午举行，欢迎各位考生在考试后参加难度调查，发表你对这次考试的看法。点击进入论坛，参与考后大讨论</td>
<td>5000</td>
</tr>
<tr>
<td>时尚</td>
<td>组图：萝莉潮人示范春季复古实穿导语：萝莉潮人示范春季复古实穿，在乍暖还寒的初春，有的甜美、有的优雅、有的性感，但无论是哪种风格都给人强烈的视觉冲击力，在这个缤纷的春季更加脱俗动人。</td>
<td>5000</td>
</tr>
<tr>
<td>时政</td>
<td>香港特区行政长官曾荫权将离港休假中新社香港八月七日电 香港特区行政长官曾荫权将于八月九日至十五日离港休假。特区政府发言人七日透露，曾荫权离港期间，八月九日由特区财政司司长曾俊华署理行政长官职务；八月十日至十五日由政务司司长唐英年署理行政长官职务。(完)</td>
<td>5000</td>
</tr>
<tr>
<td>游戏</td>
<td>全国人大常委会将对59件法律相关条文作出修改新华社快讯：全国人大常委会27日表决通过了关于修改部分法律的决定，对59件法律的相关条文作出修改。</td>
<td>5000</td>
</tr>
<tr>
<td>科技</td>
<td>入门级时尚卡片机 尼康S220套装仅1150尼康S220延续了S系列纤巧超薄的机身设计，采用铝合金材质打造，表面质地细腻，不易沾染指纹。S220拥有紫色、深蓝、洋红、水晶绿和柔银五款靓丽颜色可供选择。</td>
<td>5000</td>
</tr>
</tbody>
</table>
</div>
<p>使用搜狗新闻词向量和字向量，300d：<a href="https://pan.baidu.com/s/1svFOwFBKnnlsqrF1t99Lnw" target="_blank" rel="noopener">词向量</a>,<a href="https://pan.baidu.com/s/1tUghuTno5yOvOx4LXA9-wg" target="_blank" rel="noopener">字向量</a></p>
<h3 id="数据处理代码（微博）"><a href="#数据处理代码（微博）" class="headerlink" title="数据处理代码（微博）"></a>数据处理代码（微博）</h3><h4 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WeiboDataset</span><span class="params">(NLPDataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,data_path,test=False,stop_words_path=None,batch_first=False,include_lengths=False,tokenizer_language=<span class="string">'cn'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param data_path:</span></span><br><span class="line"><span class="string">        :param test: 如果为测试集，则不加载label</span></span><br><span class="line"><span class="string">        :param stop_words_path:</span></span><br><span class="line"><span class="string">        :param batch_first:</span></span><br><span class="line"><span class="string">        :param include_lengths:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.data = pd.read_csv(data_path)</span><br><span class="line">        print(<span class="string">'read data from &#123;&#125;'</span>.format(data_path))</span><br><span class="line">        self.text_field = <span class="string">"review"</span></span><br><span class="line">        self.label_field = <span class="string">"label"</span></span><br><span class="line">        self.test = test</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> stop_words_path:</span><br><span class="line">            stop_words = read_stop_words(stop_words_path)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            stop_words = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.LABEL = LabelField(sequential=<span class="literal">False</span>, use_vocab=<span class="literal">False</span>, dtype=torch.float)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># lambda x: [y for y in x]</span></span><br><span class="line">        self.TEXT = Field(sequential=<span class="literal">True</span>,stop_words=stop_words, tokenize=<span class="keyword">lambda</span> x: [y <span class="keyword">for</span> y <span class="keyword">in</span> x], batch_first=batch_first,tokenizer_language=tokenizer_language,</span><br><span class="line">                          include_lengths=include_lengths)  <span class="comment"># include_lengths=True for LSTM</span></span><br><span class="line"></span><br><span class="line">        self.fields = [(<span class="string">"text"</span>, self.TEXT), (<span class="string">"label"</span>, self.LABEL)]</span><br><span class="line"></span><br><span class="line">        self.examples = self.build_examples()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_examples</span><span class="params">(self)</span>:</span></span><br><span class="line">        examples = []</span><br><span class="line">        <span class="keyword">if</span> self.test:</span><br><span class="line">            <span class="comment"># 如果为测试集，则不加载label</span></span><br><span class="line">            <span class="keyword">for</span> text <span class="keyword">in</span> tqdm(self.data[self.text_field]):</span><br><span class="line">                examples.append(Example.fromlist([text, <span class="literal">None</span>], self.fields))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> text, label <span class="keyword">in</span> tqdm(zip(self.data[self.text_field], self.data[self.label_field])):</span><br><span class="line">                <span class="comment"># Example: Defines a single training or test example.</span></span><br><span class="line">                <span class="comment"># Stores each column of the example as an attribute.</span></span><br><span class="line">                examples.append(Example.fromlist([text, label], self.fields))</span><br><span class="line">        <span class="keyword">return</span> examples</span><br></pre></td></tr></table></figure>
<h4 id="dataloader"><a href="#dataloader" class="headerlink" title="dataloader"></a>dataloader</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WeiboDataLoader</span><span class="params">(NLPDataLoader)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,dataset,split_ratio=None, batch_size=<span class="number">64</span>, sort_key=None, device=None,train=True, repeat=False, shuffle=None, sort=None,sort_within_batch=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 batch_size_fn=None,  use_pretrained_word_embedding=False, word_embedding_path=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        微博二分类数据集加载器</span></span><br><span class="line"><span class="string">        :param dataset:</span></span><br><span class="line"><span class="string">        :param split_ratio:</span></span><br><span class="line"><span class="string">        :param batch_size:</span></span><br><span class="line"><span class="string">        :param device:</span></span><br><span class="line"><span class="string">        :param train: Whether the iterator represents a train set</span></span><br><span class="line"><span class="string">        :param repeat:</span></span><br><span class="line"><span class="string">        :param shuffle:</span></span><br><span class="line"><span class="string">        :param sort:</span></span><br><span class="line"><span class="string">        :param sort_within_batch:</span></span><br><span class="line"><span class="string">        :param use_pretrained_word_embedding:</span></span><br><span class="line"><span class="string">        :param word_embedding_path:</span></span><br><span class="line"><span class="string">        :param vocab_size:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 构建数据集</span></span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        <span class="comment"># 迭代器参数</span></span><br><span class="line">        self.split_ratio = split_ratio</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.sort_key = sort_key</span><br><span class="line">        self.device = device</span><br><span class="line">        self.batch_size_fn = batch_size_fn</span><br><span class="line">        self.train = train</span><br><span class="line">        self.repeat = repeat</span><br><span class="line">        self.shuffle = shuffle</span><br><span class="line">        self.sort = sort</span><br><span class="line">        self.sort_within_batch = sort_within_batch</span><br><span class="line">        self.use_pretrained_word_embedding = use_pretrained_word_embedding</span><br><span class="line">        self.word_embedding_path = word_embedding_path</span><br><span class="line">        self.train_iter, self.valid_iter, self.test_iter = self.get_iterators()</span><br><span class="line">        self.vocab = self.dataset.TEXT.vocab</span><br><span class="line">        self.vocab.pad_index = self.dataset.TEXT.vocab.stoi[self.dataset.TEXT.pad_token]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_iterators</span><span class="params">(self)</span>:</span></span><br><span class="line">        train_iter, valid_iter, test_iter = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.split_ratio:</span><br><span class="line">            <span class="comment"># 构建词汇表</span></span><br><span class="line">            <span class="keyword">if</span> self.use_pretrained_word_embedding:</span><br><span class="line">                vectors = Vectors(name=self.word_embedding_path)  <span class="comment"># 使用预训练的词向量</span></span><br><span class="line">                self.dataset.TEXT.build_vocab(self.dataset, vectors=vectors,</span><br><span class="line">                                              unk_init=torch.Tensor.normal_)</span><br><span class="line">                self.dataset.LABEL.build_vocab(self.dataset)</span><br><span class="line">                print(<span class="string">'label types:&#123;&#125;'</span>.format(self.dataset.LABEL.vocab.stoi))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.dataset.TEXT.build_vocab(self.dataset)  <span class="comment"># 不使用预训练的词向量</span></span><br><span class="line">                self.dataset.LABEL.build_vocab(self.dataset)</span><br><span class="line">                print(<span class="string">'label types:&#123;&#125;'</span>.format(self.dataset.LABEL.vocab.stoi))</span><br><span class="line"></span><br><span class="line">            train_iter = Iterator(self.dataset, batch_size=self.batch_size, device=self.device, sort_key=self.sort_key,</span><br><span class="line">                                  sort_within_batch=self.sort_within_batch, repeat=self.repeat,</span><br><span class="line">                                  batch_size_fn=self.batch_size_fn</span><br><span class="line">                                  , train=self.train, shuffle=self.shuffle, sort=self.sort)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            train_data, valid_data, test_data = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">            <span class="keyword">if</span> isinstance(self.split_ratio, list):</span><br><span class="line">                <span class="keyword">if</span> len(self.split_ratio) == <span class="number">3</span>:</span><br><span class="line">                    train_data, valid_data, test_data = self.dataset.split(split_ratio=self.split_ratio)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    train_data, valid_data = self.dataset.split(split_ratio=self.split_ratio)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                train_data, valid_data = self.dataset.split(split_ratio=self.split_ratio)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 构建词汇表</span></span><br><span class="line">            <span class="keyword">if</span> self.use_pretrained_word_embedding:</span><br><span class="line">                vectors = Vectors(name=self.word_embedding_path)  <span class="comment"># 使用预训练的词向量</span></span><br><span class="line">                self.dataset.TEXT.build_vocab(train_data, vectors=vectors,</span><br><span class="line">                                              unk_init=torch.Tensor.normal_)</span><br><span class="line">                self.dataset.LABEL.build_vocab(train_data)</span><br><span class="line">                print(<span class="string">'label types:&#123;&#125;'</span>.format(self.dataset.LABEL.vocab.stoi))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.dataset.TEXT.build_vocab(train_data)  <span class="comment"># 不使用预训练的词向量</span></span><br><span class="line">                self.dataset.LABEL.build_vocab(train_data)</span><br><span class="line">                print(<span class="string">'label types:&#123;&#125;'</span>.format(self.dataset.LABEL.vocab.stoi))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 构建迭代器</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> train_data:</span><br><span class="line">                train_iter = Iterator(train_data, batch_size=self.batch_size, device=self.device,</span><br><span class="line">                                      sort_key=self.sort_key,</span><br><span class="line">                                      sort_within_batch=self.sort_within_batch, repeat=self.repeat,</span><br><span class="line">                                      batch_size_fn=self.batch_size_fn</span><br><span class="line">                                      , train=self.train, shuffle=self.shuffle, sort=self.sort)</span><br><span class="line">            <span class="keyword">if</span> valid_data:</span><br><span class="line">                valid_iter = Iterator(valid_data, batch_size=self.batch_size, device=self.device,</span><br><span class="line">                                      sort_key=self.sort_key,</span><br><span class="line">                                      sort_within_batch=self.sort_within_batch, repeat=self.repeat,</span><br><span class="line">                                      batch_size_fn=self.batch_size_fn</span><br><span class="line">                                      , train=self.train, shuffle=self.shuffle, sort=self.sort)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                test_iter = Iterator(test_data, batch_size=self.batch_size, device=self.device, sort_key=self.sort_key,</span><br><span class="line">                                     sort_within_batch=self.sort_within_batch, repeat=self.repeat,</span><br><span class="line">                                     batch_size_fn=self.batch_size_fn</span><br><span class="line">                                     , train=self.train, shuffle=self.shuffle, sort=self.sort)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> train_iter, valid_iter, test_iter</span><br></pre></td></tr></table></figure>
<h4 id="Vocab属性："><a href="#Vocab属性：" class="headerlink" title="Vocab属性："></a>Vocab属性：</h4><ol>
<li>freqs</li>
<li>itos</li>
<li>stoi</li>
<li>vectors</li>
</ol>
<h4 id="Bert-Dataprocess"><a href="#Bert-Dataprocess" class="headerlink" title="Bert Dataprocess"></a>Bert Dataprocess</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertDataProcess</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data_path, batch_size, device, processed_data_path=None, bert_model_path=None)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.bert_model_path = bert_model_path</span><br><span class="line">        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_path)</span><br><span class="line">        self.CLS = self.bert_tokenizer.cls_token</span><br><span class="line">        self.data_path = data_path</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.device = device</span><br><span class="line">        self.processed_data_path = processed_data_path  <span class="comment"># 经过load_data 处理过的数据保存到该路径下。</span></span><br><span class="line">        self.train_data,self.val_data = self.built_dataset()</span><br><span class="line">        self.train_iter = self.built_iterater(self.train_data)</span><br><span class="line">        self.val_iter = self.built_iterater(self.val_data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">(self, path)</span>:</span></span><br><span class="line">        contents = []</span><br><span class="line">        <span class="keyword">with</span> open(path, <span class="string">'r'</span>, encoding=<span class="string">'UTF-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(f):</span><br><span class="line">                lin = line.strip()</span><br><span class="line">                <span class="keyword">if</span> <span class="string">'label,review'</span> <span class="keyword">in</span> lin:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> lin:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                label, content = lin.split(<span class="string">','</span>,<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># print("content&gt;&gt;&gt;:", content)</span></span><br><span class="line">                token = self.bert_tokenizer.tokenize(content)</span><br><span class="line">                token = [self.CLS] + token</span><br><span class="line">                seq_len = len(token)</span><br><span class="line">                token_ids = self.bert_tokenizer.convert_tokens_to_ids(token)</span><br><span class="line">                contents.append((token_ids, int(label), seq_len))</span><br><span class="line">        <span class="keyword">return</span> contents</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">built_dataset</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(self.processed_data_path):</span><br><span class="line">            all_data = self.load_dataset(self.data_path)</span><br><span class="line">            pickle.dump(all_data, open(self.processed_data_path,<span class="string">'wb'</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            all_data = pickle.load(open(self.processed_data_path,<span class="string">'rb'</span>))</span><br><span class="line"></span><br><span class="line">        train_data, val_data = train_test_split(all_data, test_size=<span class="number">0.3</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> train_data, val_data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">built_iterater</span><span class="params">(self, dataset)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> BertDatasetIterater(dataset, self.batch_size, self.device,self.bert_model_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertDatasetIterater</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, batches, batch_size, device,bert_model_path)</span>:</span></span><br><span class="line">        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_path)</span><br><span class="line"></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.batches = batches</span><br><span class="line">        self.n_batches = len(batches) // batch_size</span><br><span class="line">        self.residue = <span class="literal">False</span>  <span class="comment"># 记录batch数量是否为整数</span></span><br><span class="line">        <span class="keyword">if</span> len(batches) % self.n_batches != <span class="number">0</span>:</span><br><span class="line">            self.residue = <span class="literal">True</span></span><br><span class="line">        self.index = <span class="number">0</span></span><br><span class="line">        self.device = device</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_mask_sentence_len</span><span class="params">(self,content,max_sent_len)</span>:</span></span><br><span class="line">        pad_index = self.bert_tokenizer.pad_token_id</span><br><span class="line">        sent_len = len(content)</span><br><span class="line">        mask = [<span class="number">1</span>]*sent_len + ([<span class="number">0</span>] * (max_sent_len-sent_len))</span><br><span class="line">        content = content.extend([pad_index] * (max_sent_len-sent_len))</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_to_tensor</span><span class="params">(self, datas)</span>:</span></span><br><span class="line">        datas = pd.DataFrame(datas,columns=[<span class="string">'content'</span>,<span class="string">'label'</span>,<span class="string">'sent len'</span>])</span><br><span class="line">        max_sent_len = max(datas[<span class="string">'sent len'</span>])</span><br><span class="line">        datas[<span class="string">"mask"</span>] = datas[<span class="string">"content"</span>].apply(self._mask_sentence_len,args=&#123;max_sent_len&#125;)</span><br><span class="line"></span><br><span class="line">        x = torch.LongTensor(pd.DataFrame(datas[<span class="string">'content'</span>].to_list()).values).to(self.device)</span><br><span class="line">        y = torch.FloatTensor(datas[<span class="string">'label'</span>].values).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pad前的长度(超过pad_size的设为pad_size)</span></span><br><span class="line">        seq_len = torch.LongTensor(datas[<span class="string">'sent len'</span>]).to(self.device)</span><br><span class="line">        mask = torch.LongTensor(pd.DataFrame(datas[<span class="string">'mask'</span>].to_list()).values).to(self.device)</span><br><span class="line">        <span class="keyword">return</span> (x, seq_len, mask), y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.residue <span class="keyword">and</span> self.index == self.n_batches:</span><br><span class="line">            batches = self.batches[self.index * self.batch_size: len(self.batches)]</span><br><span class="line">            self.index += <span class="number">1</span></span><br><span class="line">            batches = self._to_tensor(batches)</span><br><span class="line">            <span class="keyword">return</span> batches</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> self.index &gt; self.n_batches:</span><br><span class="line">            self.index = <span class="number">0</span></span><br><span class="line">            <span class="keyword">raise</span> StopIteration</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            batches = self.batches[self.index * self.batch_size: (self.index + <span class="number">1</span>) * self.batch_size]</span><br><span class="line">            self.index += <span class="number">1</span></span><br><span class="line">            batches = self._to_tensor(batches)</span><br><span class="line">            <span class="keyword">return</span> batches</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.residue:</span><br><span class="line">            <span class="keyword">return</span> self.n_batches + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.n_batches</span><br></pre></td></tr></table></figure>
<h3 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>fastText是Facebook于2016年开源的一个词向量计算和文本分类工具。在文本分类任务中，fastText（浅层网络）往往能取得和深度网络相媲美的精度，却在训练时间上比深度网络快许多数量级。在标准的多核CPU上， 在10分钟之内能够训练10亿词级别语料库的词向量，在1分钟之内能够分类有着30万多类别的50多万句子。</p>
<p>fastText是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：<br>1、fastText在保持高精度的情况下加快了训练速度和测试速度<br>2、fastText不需要预训练好的词向量，fastText会自己训练词向量<br>3、fastText两个重要的优化：Hierarchical Softmax、N-gram</p>
<h4 id="fastText模型架构"><a href="#fastText模型架构" class="headerlink" title="fastText模型架构"></a>fastText模型架构</h4><p>fastText模型架构和word2vec中的CBOW很相似， 不同之处是fastText预测标签而CBOW预测的是中间词，即模型架构类似但是模型的任务不同。下面我们先看一下CBOW的架构：</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/cbow.png" alt="avatar"></p>
<p>word2vec将上下文关系转化为多分类任务，进而训练逻辑回归模型，这里的类别数量|V|词库大小。通常的文本数据中，词库少则数万，多则百万，在训练中直接训练多分类逻辑回归并不现实。word2vec中提供了两种针对大规模多分类问题的优化手段， negative sampling 和hierarchical softmax。在优化中，negative sampling 只更新少量负面类，从而减轻了计算量。hierarchical softmax 将词库表示成前缀树，从树根到叶子的路径可以表示为一系列二分类器，一次多分类计算的复杂度从|V|降低到了树的高度<br>fastText模型架构:其中x1,x2,…,xN−1,xN表示一个文本中的n-gram向量，每个特征是词向量的平均值。这和前文中提到的cbow相似，cbow用上下文去预测中心词，而此处用全部的n-gram去预测指定类别</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/fasttext.png" alt="avatar"></p>
<h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h4><blockquote>
<p>我不喜欢这类电影，但是喜欢这一个。</p>
<p>我喜欢这类电影，但是不喜欢这一个。</p>
</blockquote>
<p><strong>这样的两句句子经过词向量平均以后已经送入单层神经网络的时候已经完全一模一样了，分类器不可能分辨出这两句话的区别</strong>，只有添加n-gram特征以后才可能有区别。因此，在实际应用的时候需要对数据有足够的了解,然后在选择模型。</p>
<h4 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FastText</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab, embedding_dim, output_dim, use_pretrain_embedding=False)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        <span class="keyword">if</span> use_pretrain_embedding:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(vocab.vectors)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.embedding = nn.Embedding(len(vocab), embedding_dim, padding_idx=vocab.pad_index)</span><br><span class="line">        <span class="comment"># 把unknown 和 pad 向量设置为零</span></span><br><span class="line">        self.embedding.weight.data[vocab.unk_index] = torch.zeros(embedding_dim)</span><br><span class="line">        self.embedding.weight.data[vocab.pad_index] = torch.zeros(embedding_dim)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(embedding_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [sent len, batch size]</span></span><br><span class="line"></span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedded = [sent len, batch size, emb dim]</span></span><br><span class="line"></span><br><span class="line">        embedded = embedded.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line"></span><br><span class="line">        pooled = F.avg_pool2d(embedded, (embedded.shape[<span class="number">1</span>], <span class="number">1</span>)).squeeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pooled = [batch size, embedding_dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.fc(pooled)</span><br></pre></td></tr></table></figure>
<h3 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h3><h4 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h4><p><strong>Yoon Kim</strong>在论文<a href="https://arxiv.org/abs/1408.5882" target="_blank" rel="noopener">(2014 EMNLP) Convolutional Neural Networks for Sentence Classification</a>提出TextCNN。</p>
<p>将<strong>卷积神经网络CNN</strong>应用到<strong>文本分类</strong>任务，利用<strong>多个不同size的kernel</strong>来提取句子中的关键信息（类似于多窗口大小的ngram)，从而能够更好地捕捉局部相关性。</p>
<h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/textcnn.png" alt="avatar"></p>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>TextCNN的详细过程原理图如下：</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/textcnndetail.png" alt="avatar"></p>
<p>TextCNN详细过程：</p>
<ul>
<li><strong>Embedding</strong>：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点。</li>
<li><strong>Convolution</strong>：然后经过 kernel_sizes=(2,3,4) 的一维卷积层，每个kernel_size 有两个输出 channel。</li>
<li><strong>MaxPolling</strong>：第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示。</li>
<li><p><strong>FullConnection and Softmax</strong>：最后接一层全连接的 softmax 层，输出每个类别的概率。</p>
<p>通道（Channels）：</p>
</li>
<li><p>图像中可以利用 (R, G, B) 作为不同channel；</p>
</li>
<li>文本的输入的channel通常是不同方式的embedding方式（比如 word2vec或Glove），实践中也有利用静态词向量和fine-tunning词向量作为不同channel的做法。</li>
</ul>
<p>一维卷积（conv-1d）：</p>
<ul>
<li>图像是二维数据；</li>
<li><strong>文本是一维数据，因此在TextCNN卷积用的是一维卷积</strong>（在<strong>word-level</strong>上是一维卷积；虽然文本经过词向量表达后是二维数据，但是在embedding-level上的二维卷积没有意义）。一维卷积带来的问题是需要<strong>通过设计不同 kernel_size 的 filter 获取不同宽度的视野</strong>。</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><p>TextCNN模型最大的问题也是这个全局的max pooling丢失了结构信息，因此很难去发现文本中的转折关系等复杂模式。针对这个问题，可以尝试k-max pooling做一些优化，k-max pooling针对每个卷积核都不只保留最大的值，他保留前k个最大值，并且保留这些值出现的顺序，也即按照文本中的位置顺序来排列这k个最大值。在某些比较复杂的文本上相对于1-max pooling会有提升。</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab, embedding_dim, n_filters, filter_sizes, output_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout, use_pretrain_embedding=False)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        <span class="keyword">if</span> use_pretrain_embedding:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(vocab.vectors)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.embedding = nn.Embedding(len(vocab), embedding_dim, padding_idx=vocab.pad_index)</span><br><span class="line">        <span class="comment"># 把unknown 和 pad 向量设置为零</span></span><br><span class="line">        self.embedding.weight.data[vocab.unk_index] = torch.zeros(embedding_dim)</span><br><span class="line">        self.embedding.weight.data[vocab.pad_index] = torch.zeros(embedding_dim)</span><br><span class="line"></span><br><span class="line">        self.convs = nn.ModuleList([</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">1</span>,</span><br><span class="line">                      out_channels=n_filters,</span><br><span class="line">                      kernel_size=(fs, embedding_dim))</span><br><span class="line">            <span class="keyword">for</span> fs <span class="keyword">in</span> filter_sizes</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [batch size, sent len]</span></span><br><span class="line"></span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line"></span><br><span class="line">        embedded = embedded.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedded = [batch size, 1, sent len, emb dim]</span></span><br><span class="line"></span><br><span class="line">        conved = [F.relu(conv(embedded)).squeeze(<span class="number">3</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]</span></span><br><span class="line"></span><br><span class="line">        pooled = [F.max_pool1d(conv, conv.shape[<span class="number">2</span>]).squeeze(<span class="number">2</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> conved]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pooled_n = [batch size, n_filters]</span></span><br><span class="line"></span><br><span class="line">        cat = self.dropout(torch.cat(pooled, dim=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># cat = [batch size, n_filters * len(filter_sizes)]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.fc(cat)</span><br></pre></td></tr></table></figure>
<h3 id="HAN（Hierarchy-Attention-Network）"><a href="#HAN（Hierarchy-Attention-Network）" class="headerlink" title="HAN（Hierarchy Attention Network）"></a>HAN（Hierarchy Attention Network）</h3><p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/HAN.png" alt="avatar"></p>
<p>整个网络结构包括五个部分：</p>
<ol>
<li>词序列编码器</li>
<li>基于词级的注意力层</li>
<li>句子编码器</li>
<li>基于句子级的注意力层</li>
<li>分类</li>
</ol>
<p>整个网络结构由双向GRU网络和注意力机制组合而成。</p>
<h4 id="词序列编码器"><a href="#词序列编码器" class="headerlink" title="词序列编码器"></a>词序列编码器</h4><p>给定一个句子中的单词$w_{it}$，其中 $i$ 表示第$i$ 个句子，$t$ 表示第 $t$ 个词。通过一个词嵌入矩阵 $W_e$ 将单词转换成向量表示，具体如下所示：</p>
<script type="math/tex; mode=display">
x_{it}=W_e w_{it}</script><p>利用双向GRU实现的整个编码流程：</p>
<script type="math/tex; mode=display">
\begin{aligned}
x_{i t} &=W_{e} w_{i t}, t \in[1, T] \\
\overrightarrow{h}_{i t} &=\overrightarrow{\operatorname{GRU}}\left(x_{i t}\right), t \in[1, T] \\
\overleftarrow{h}_{i t} &=\overleftarrow{\operatorname{GRU}}\left(x_{i t}\right), t \in[T, 1] \\
{h}_{i t} &= [\overrightarrow{h}_{i t},\overleftarrow{h}_{i t} ]
\end{aligned}</script><h4 id="词级的注意力层"><a href="#词级的注意力层" class="headerlink" title="词级的注意力层"></a>词级的注意力层</h4><p>但是对于一句话中的单词，并不是每一个单词对分类任务都是有用的，比如在做文本的情绪分类时，可能我们就会比较关注“很好”、“伤感”这些词。为了能使循环神经网络也能自动将“注意力”放在这些词汇上，作者设计了基于单词的注意力层的具体流程如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
u_{i t} &=\tanh \left(W_{w} h_{i t}+b_{w}\right) \\
\alpha_{i t} &=\frac{\exp \left(u_{i t}^{\top} u_{w}\right)}{\sum_{t} \exp \left(u_{i t}^{\top} u_{w}\right)} \\
s_{i} &=\sum_{t} \alpha_{i t} h_{i t}
\end{aligned}</script><p>上面式子中，$u_{it}$ 是 $h_{it}$ 的隐层表示，$a_{it}$ 是经 softmax 函数处理后的归一化权重系数，$u_w$是一个随机初始化的向量，之后会作为模型的参数一起被训练，$s_i$ 就是我们得到的第 i 个句子的向量表示。</p>
<h4 id="句子编码器"><a href="#句子编码器" class="headerlink" title="句子编码器"></a>句子编码器</h4><p>句子编码器也是基于双向GRU实现编码的，</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\overrightarrow{h}_{i}=\overrightarrow{\operatorname{GRU}}\left(s_{i}\right), i \in[1, L]\\
&\overleftarrow{h}_{i}=\overleftarrow{\operatorname{GRU}}\left(s_{i}\right), t \in[L, 1]
\end{aligned}</script><p>公式和词编码类似，最后的 $h_i$ 也是通过拼接得到的.</p>
<h4 id="句子级注意力层"><a href="#句子级注意力层" class="headerlink" title="句子级注意力层"></a>句子级注意力层</h4><p>注意力层的流程如下，和词级的一致:</p>
<script type="math/tex; mode=display">
\begin{aligned}
u_{i} &=\tanh \left(W_{s} h_{i}+b_{s}\right) \\
\alpha_{i} &=\frac{\exp \left(u_{i}^{\top} u_{s}\right)}{\sum_{i} \exp \left(u_{i}^{\top} u_{s}\right)} \\
v &=\sum_{i} \alpha_{i} h_{i}
\end{aligned}</script><p>最后得到的向量$v$ 就是文档的向量表示，这是文档的高层表示。接下来就可以用可以用这个向量表示作为文档的特征。</p>
<h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><p>使用最常用的softmax分类器对整个文本进行分类了</p>
<script type="math/tex; mode=display">
p=\operatorname{softmax}\left(W_{c} v+b_{c}\right)</script><p>损失函数</p>
<script type="math/tex; mode=display">
L=-\sum_{d} \log p_{d j}</script><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><h3 id="RNN系列"><a href="#RNN系列" class="headerlink" title="RNN系列"></a>RNN系列</h3><p>通过将前一时刻的运算结果添加到当前的运算中，从而实现了“考虑上文信息”的功能。</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/rnn.png" alt="avatar"></p>
<p>RNN可以考虑上文的信息，那么如何将下文的信息也添加进去呢？这就是BiRNN要做的事情。</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/birnn.png" alt="avatar"></p>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>因为RNN存在梯度弥散和梯度爆炸的问题，所以RNN很难完美地处理具有长期依赖的信息。既然仅仅依靠一条线连接后面的神经元不足以解决问题，那么就再加一条线好了，这就是LSTM。</p>
<p>LSTM的关键在于细胞的状态和穿过细胞的线，细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流动保持不变会变得容易。</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/lstm_c.png" alt="avatar"></p>
<p>在LSTM中，门可以实现选择性的让信息通过，主要通过一个sigmoid的神经层和一个逐点相乘的操作来实现。LSTM通过三个这样的门结构来实现信息的保护和控制，分别是遗忘门（forget gate）、输入门（input gate）与输出门（output gate）。</p>
<h5 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a><strong>遗忘门</strong></h5><p>在LSTM中的第一步是决定从细胞状态中丢弃什么信息。这个决定通过一个称为遗忘门的结构来完成。遗忘门会读取$h_{t-1}$和$x_t$，输出一个0到1之间的数值给细胞的状态$c_{t-1}$中的数字。1表示完全保留，0表示完全舍弃。</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/lstm_forget_gate.png" alt="avatar"></p>
<h5 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a><strong>输入门</strong></h5><p>遗忘门决定让多少新的信息加入到cell的状态中来。实现这个需要两个步骤：</p>
<ol>
<li><p>首先一个叫“input gate layer”的sigmoid层决定哪些信息需要更新；一个tanh层生成一个向量，用来更新状态C。</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/input_gate.png" alt="avatar"></p>
</li>
<li><p>把 1 中的两部分联合起来，对cell的状态进行更新，我们把旧的状态与$f_t$相乘，丢弃掉我们确定需要丢弃的信息，接着加上$i_t * \tilde{C}_{t}$</p>
</li>
</ol>
<h5 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h5><p>最终，我们需要确定输出什么值，这个输出将会基于我们的细胞的状态，但是也是一个过滤后的版本。首先，我们通过一个sigmoid层来确定细胞状态的哪些部分将输出出去。接着，我们把细胞状态通过tanh进行处理（得到一个-1到1之间的值）并将它和sigmoid门的输出相乘，最终我们仅仅会输出我们确定输出的部分。</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/output_gate.png" alt="avatar"></p>
<h5 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h5><p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/lstm_all.png" alt="avatar"></p>
<h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><p>在LSTM中引入了三个门函数：输入门、遗忘门和输出门来控制输入值、记忆值和输出值。而在GRU模型中只有两个门：分别是更新门和重置门。</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/GRU.png" alt="avatar"></p>
<p>图中的$z_t$和$r_t$分别表示更新门和重置门。更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多。重置门控制前一状态有多少信息被写入到当前的候选集 $\tilde{h}_{t}$上，重置门越小，前一状态的信息被写入的越少。</p>
<p>LSTM和CRU都是通过各种门函数来将重要特征保留下来，这样就保证了在long-term传播的时候也不会丢失。此外GRU相对于LSTM少了一个门函数，因此在参数的数量上也是要少于LSTM的，所以整体上GRU的训练速度要快于LSTM的。</p>
<h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h4><p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/self-attention.png" alt="avatar"></p>
<ol>
<li>Encode所有输入序列,得到对应的$h_1,h_2, \cdots ,h_T$(T为输入序列长度)</li>
<li>Decode输出目标$y_t$之前，会将上一步输出的隐藏状态$S_{t-1}$与之前encode好的$h_1,h_2,\cdots,h_T$进行比对，计算相似度（$e_{t,j}=a(s_{t-1},h_j)$）,$h_j$为之前第j个输入encode得到的隐藏向量，a为任意一种计算相似度的方式</li>
<li>然后通过softmax，即$a_{t,j}=\frac{exp(e_{t,j})}{\sum^{T_x}_{k=1}exp(e_{t,k})}$将之前得到的各个部分的相关系数进行归一化，得到$a_{t,1},a_{t,2},\cdots,a_{t,T}$</li>
<li>在对输入序列的隐藏层进行相关性加权求和得到此时decode需要的context vector ：$</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RnnModel</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, vocab, embedding_dim, hidden_dim, output_dim, n_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 bidirectional, dropout, batch_first=False,use_pretrain_embedding=False)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        <span class="keyword">if</span> use_pretrain_embedding:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(vocab.vectors)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.embedding = nn.Embedding(len(vocab), embedding_dim, padding_idx=vocab.pad_index)</span><br><span class="line">        <span class="comment"># 把unknown 和 pad 向量设置为零</span></span><br><span class="line">        self.embedding.weight.data[vocab.unk_index] = torch.zeros(embedding_dim)</span><br><span class="line">        self.embedding.weight.data[vocab.pad_index] = torch.zeros(embedding_dim)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(embedding_dim,</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(embedding_dim,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(embedding_dim,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(hidden_dim * n_layers, output_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [sent len, batch size]</span></span><br><span class="line"></span><br><span class="line">        embedded = self.dropout(self.embedding(text))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedded = [sent len, batch size, emb dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.bidirectional:</span><br><span class="line">            hidden = torch.reshape(hidden,(hidden.shape[<span class="number">1</span>],self.hidden_dim * self.n_layers))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hidden = torch.reshape(hidden, (<span class="number">-1</span>,hidden.shape[<span class="number">1</span>], self.hidden_dim * self.n_layers))</span><br><span class="line">            hidden = torch.mean(hidden,dim=<span class="number">0</span>)</span><br><span class="line">        output = torch.sum(output,dim=<span class="number">0</span>)</span><br><span class="line">        fc_input = self.dropout(output+hidden)</span><br><span class="line">        out = self.fc(fc_input)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h4 id="Rnn-Attenton"><a href="#Rnn-Attenton" class="headerlink" title="Rnn-Attenton"></a>Rnn-Attenton</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RnnAttentionModel</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, vocab, embedding_dim, hidden_dim, output_dim, n_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 bidirectional, dropout, batch_first=False,use_pretrain_embedding=False)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_pretrain_embedding:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(vocab.vectors)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.embedding = nn.Embedding(len(vocab), embedding_dim, padding_idx=vocab.pad_index)</span><br><span class="line">        <span class="comment"># 把unknown 和 pad 向量设置为零</span></span><br><span class="line">        self.embedding.weight.data[vocab.unk_index] = torch.zeros(embedding_dim)</span><br><span class="line">        self.embedding.weight.data[vocab.pad_index] = torch.zeros(embedding_dim)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(embedding_dim,</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(embedding_dim,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(embedding_dim,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.tanh1 = nn.Tanh()</span><br><span class="line">        self.tanh2 = nn.Tanh()</span><br><span class="line">        <span class="comment"># self.u = nn.Parameter(torch.Tensor(self.hidden_dim * 2,self.hidden_dim*2))</span></span><br><span class="line">        self.w = nn.Parameter(torch.randn(hidden_dim),requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="keyword">if</span> bidirectional:</span><br><span class="line">            self.w = nn.Parameter(torch.randn(hidden_dim * <span class="number">2</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">            self.fc = nn.Linear(hidden_dim * <span class="number">2</span>, output_dim)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.w = nn.Parameter(torch.randn(hidden_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line">            self.fc = nn.Linear(hidden_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [sent len, batch size]</span></span><br><span class="line"></span><br><span class="line">        embedded = self.dropout(self.embedding(text))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedded = [sent len, batch size, emb dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        <span class="comment"># output = [sent len, batch size, hidden dim * num_direction]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># attention</span></span><br><span class="line">        <span class="comment"># M = [sent len, batch size, hidden dim * num_direction]</span></span><br><span class="line">        <span class="comment"># M = self.tanh1(output)</span></span><br><span class="line"></span><br><span class="line">        alpha = F.softmax(torch.matmul(self.tanh1(output), self.w), dim=<span class="number">0</span>).unsqueeze(<span class="number">-1</span>) <span class="comment"># dim=0表示针对文本中的每个词的输出softmax</span></span><br><span class="line">        output_attention = output * alpha</span><br><span class="line"></span><br><span class="line">        <span class="comment"># hidden = [n_layers * num_direction,batch_size, hidden_dim]</span></span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            hidden = torch.mean(torch.reshape(hidden, (<span class="number">-1</span>,hidden.shape[<span class="number">1</span>], self.hidden_dim * <span class="number">2</span>)),dim=<span class="number">0</span>)  <span class="comment"># hidden = [batch_size, hidden_dim * num_direction]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hidden = torch.mean(torch.reshape(hidden, (<span class="number">-1</span>, hidden.shape[<span class="number">1</span>], self.hidden_dim)), dim=<span class="number">0</span>)   <span class="comment"># hidden = [batch_size, hidden_dim]</span></span><br><span class="line"></span><br><span class="line">        output_attention = torch.sum(output_attention,dim=<span class="number">0</span>)</span><br><span class="line">        output = torch.sum(output,dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        fc_input = self.dropout(output+output_attention+hidden)</span><br><span class="line">        <span class="comment"># fc_input = self.dropout(output_attention)</span></span><br><span class="line">        out = self.fc(fc_input)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h3 id="TextRCNN"><a href="#TextRCNN" class="headerlink" title="TextRCNN"></a>TextRCNN</h3><h4 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h4><p>就深度学习领域来说，RNN和CNN作为文本分类问题的主要模型架构，都存在各自的优点及局限性。RNN擅长处理序列结构，能够考虑到句子的上下文信息，但RNN属于“biased model”，一个句子中越往后的词重要性越高，这有可能影响最后的分类结果，因为对句子分类影响最大的词可能处在句子任何位置。CNN属于无偏模型，能够通过最大池化获得最重要的特征，但是CNN的滑动窗口大小不容易确定，选的过小容易造成重要信息丢失，选的过大会造成巨大参数空间。为了解决二者的局限性，这篇文章提出了一种新的网络架构，用双向循环结构获取上下文信息，这比传统的基于窗口的神经网络更能减少噪声，而且在学习文本表达时可以大范围的保留词序。其次使用最大池化层获取文本的重要部分，自动判断哪个特征在文本分类过程中起更重要的作用。</p>
<p><a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552" target="_blank" rel="noopener">论文</a></p>
<h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/rcnn.png" alt="avatar"></p>
<h4 id="Word-Representation-Learning"><a href="#Word-Representation-Learning" class="headerlink" title="Word Representation Learning"></a>Word Representation Learning</h4><p>作者提出将单词的左上下文、右上下文、单词本身结合起来作为单词表示。作者使用了双向RNN来分别提取句子的上下文信息。公式如下:</p>
<script type="math/tex; mode=display">
\begin{array}{l}
c_{l}\left(w_{i}\right)=f\left(W^{(l)} c_{l}\left(w_{i-1}\right)+W^{(s l)} e\left(w_{i-1}\right)\right)  \\
c_{r}\left(w_{i}\right)=f\left(W^{(r)} c_{r}\left(w_{i+1}\right)+W^{(s r)} e\left(w_{i+1}\right)\right)
\end{array}</script><p>其中，$c_l(w_i)$代表单词$w_i$的左上下文，$c_l(w_i)$由上一个单词的左上下文$c_l$和$c_l(w_{i-1})$上一个单词的词嵌入向量 $e(w_{i-1})$计算得到，如公式（1）所示，所有句子第一个单词的左侧上下文使用相同的共享参数$c_l(w_1)$。 $W^{(l)},W^{(sl)}$用于将上一个单词的左上下文语义和上一个单词的语义结合到单词 $w_i$的左上下文表示中。右上下文的处理与左上下文完全相同，同样所有句子最后一个单词的右侧上下文使用相同的共享参数$c_r(w_n)$。 得到句子中每个单词的左上下文表示和右上下文表示后，就可以定义单词  $w_i$的表示如下</p>
<script type="math/tex; mode=display">
\boldsymbol{x}_{i}=\left[\boldsymbol{c}_{l}\left(w_{i}\right) ; \boldsymbol{e}\left(w_{i}\right) ; \boldsymbol{c}_{r}\left(w_{i}\right)\right]</script><p>实际就是单词$w_i$，单词的词嵌入表示向量 $e(w_i)$以及单词的右上下文向量$c_e(w_i)$ 的拼接后的结果。得到$w_i$的表示$x_i$后，就可以输入激活函数得到$w_i$的潜在语义向量 $y_i^{(2)}$ 。</p>
<script type="math/tex; mode=display">
\boldsymbol{y}_{i}^{(2)}=\tanh \left(W^{(2)} \boldsymbol{x}_{i}+\boldsymbol{b}^{(2)}\right)</script><h4 id="Text-Representation-Learning"><a href="#Text-Representation-Learning" class="headerlink" title="Text Representation Learning"></a>Text Representation Learning</h4><p>经过卷积层后，获得了所有词的表示，首先对其进行最大池化操作，最大池化可以帮助找到句子中最重要的潜在语义信息。</p>
<script type="math/tex; mode=display">
\boldsymbol{y}^{(3)}=\max _{i=1}^{n} \boldsymbol{y}_{i}^{(2)}</script><p>然后经过全连接层得到文本的表示，最后通过softmax层进行分类。</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol{y}^{(4)}=W^{(4)} \boldsymbol{y}^{(3)}+\boldsymbol{b}^{(4)}\\
&p_{i}=\frac{\exp \left(\boldsymbol{y}_{i}^{(4)}\right)}{\sum_{k=1}^{n} \exp \left(\boldsymbol{y}_{k}^{(4)}\right)}
\end{aligned}</script><h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RCNNModel</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, vocab, embedding_dim, hidden_dim, output_dim, n_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 bidirectional, dropout, pad_size=<span class="number">32</span>,batch_first=False,use_pretrain_embedding=False)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.pad_size = pad_size</span><br><span class="line">        <span class="keyword">if</span> use_pretrain_embedding:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(vocab.vectors)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.embedding = nn.Embedding(len(vocab), embedding_dim, padding_idx=vocab.pad_index)</span><br><span class="line">        <span class="comment"># 把unknown 和 pad 向量设置为零</span></span><br><span class="line">        self.embedding.weight.data[vocab.unk_index] = torch.zeros(embedding_dim)</span><br><span class="line">        self.embedding.weight.data[vocab.pad_index] = torch.zeros(embedding_dim)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(embedding_dim,</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(embedding_dim,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(embedding_dim,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.maxpool = nn.MaxPool1d()</span></span><br><span class="line">        self.fc = nn.Linear(hidden_dim * n_layers + embedding_dim, output_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [sent len, batch size]</span></span><br><span class="line"></span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedded = [sent len, batch size, emb dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># packed_output</span></span><br><span class="line">        <span class="comment"># hidden [n_layers * bi_direction,batch_size,hidden_dim]</span></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        <span class="comment"># output [sent len, batch_size * n_layers * bi_direction]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if not self.bidirectional:</span></span><br><span class="line">        <span class="comment">#     hidden = torch.reshape(hidden,(hidden.shape[1],self.hidden_dim * self.n_layers))</span></span><br><span class="line">        <span class="comment"># else:</span></span><br><span class="line">        <span class="comment">#     hidden = torch.reshape(hidden, (-1,hidden.shape[1], self.hidden_dim * self.n_layers))</span></span><br><span class="line">        <span class="comment">#     hidden = torch.mean(hidden,dim=0)</span></span><br><span class="line"></span><br><span class="line">        output = torch.cat((output,embedded),<span class="number">2</span>)</span><br><span class="line">        out = output.relu().permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)</span><br><span class="line">        max_sentence_len = output_lengths[<span class="number">0</span>].item()</span><br><span class="line">        out = nn.MaxPool1d(max_sentence_len)(out).squeeze()</span><br><span class="line">        out = self.fc(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h3 id="DPCNN"><a href="#DPCNN" class="headerlink" title="DPCNN"></a>DPCNN</h3><h4 id="简介："><a href="#简介：" class="headerlink" title="简介："></a>简介：</h4><p>ACL2017年中，腾讯AI-lab提出了Deep Pyramid Convolutional Neural Networks for Text Categorization(DPCNN)。论文中提出了一种基于word-level级别的网络-DPCNN，由于TextCNN 不能通过卷积获得文本的长距离依赖关系，而论文中DPCNN通过不断加深网络，可以抽取长距离的文本依赖关系。实验证明在不增加太多计算成本的情况下，增加网络深度就可以获得最佳的准确率。‍</p>
<h4 id="网络结构-1"><a href="#网络结构-1" class="headerlink" title="网络结构"></a>网络结构</h4><p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/DPCNN.jpg" alt="avatar"></p>
<h5 id="Region-embedding"><a href="#Region-embedding" class="headerlink" title="Region embedding"></a>Region embedding</h5><p>作者将TextCNN的包含多尺寸卷积滤波器的卷积层的卷积结果称之为Region embedding，意思就是对一个文本区域/片段（比如3gram）进行一组卷积操作后生成的embedding。<br>卷积操作有两种选择：</p>
<ol>
<li>保留词序：也就是设置一组size=3*D的二维卷积核对3gram进行卷积（其中D是word embedding维度）</li>
<li>不保留词序（即使用词袋模型），即首先对3gram中的3个词的embedding取均值得到一个size=D的向量，然后设置一组size=D的一维卷积核对该3gram进行卷积。</li>
</ol>
<p>TextCNN里使用的是保留词序的做法，而DPCNN使用的是词袋模型的做法，DPCNN作者argue前者做法更容易造成过拟合，后者的性能却跟前者差不多。</p>
<h4 id="卷积和全连接的权衡"><a href="#卷积和全连接的权衡" class="headerlink" title="卷积和全连接的权衡"></a>卷积和全连接的权衡</h4><p>产生region embedding后，按照经典的TextCNN的做法的话，就是从每个特征图中挑选出最有代表性的特征，也就是直接应用全局最大池化层（max-over-time-pooling layer），这样就生成了这段文本的特征向量（假如卷积滤波器的size有3，4，5这三种，每种size包含100个卷积核，那么当然就会产生3<em>100幅特征图，然后将max-over-time-pooling操作应用到每个特征图上，于是文本的特征向量即3</em>100=300维）。<br>TextCNN这样做的意义本质上与词袋模型（含ngram）+weighting+NB/MaxEnt/SVM的经典文本分类模型没本质区别，只不过one-hot表示到word embedding表示的转变避免了词袋模型遭遇的数据稀疏问题。可以说，TextCNN本质上收益于词向量的引入带来的“近义词有相近向量表示”的bonus，同时TextCNN恰好可以较好的利用词向量中的知识（近义关系）。这意味着，经典模型里难以学习的远距离信息（如12gram）在TextCNN中依然难以学习。</p>
<h5 id="等长卷积"><a href="#等长卷积" class="headerlink" title="等长卷积"></a>等长卷积</h5><p>假设输入的序列长度为n，卷积核大小为m，步长(stride)为s,输入序列两端各填补p个零(zero padding),那么该卷积层的输出序列为(n-m+2p)/s+1。</p>
<ol>
<li>窄卷积(narrow convolution):步长s=1,两端不补零，即p=0，卷积后输出长度为n-m+1。</li>
<li>宽卷积(wide onvolution) ：步长s=1,两端补零p=m-1，卷积后输出长度 n+m-1。</li>
<li>等长卷积(equal-width convolution): 步长s=1,两端补零p=(m-1)/2，卷积后输出长度为n。</li>
</ol>
<p>那么对文本，或者说对word embedding序列进行等长卷积的意义是什么呢？<br>既然输入输出序列的位置数一样多，我们将输入输出序列的第n个embedding称为第n个词位，那么这时size为n的卷积核产生的等长卷积的意义就很明显了，那就是将输入序列的每个词位及其左右((n-1)/2)个词的上下文信息压缩为该词位的embedding，也就是说，产生了每个词位的被上下文信息修饰过的更高level更加准确的语义。</p>
<p>回到DPCNN上来，我们想要克服TextCNN的缺点，捕获长距离模式，显然就要用到深层CNN啦。那么直接等长卷积堆等长卷积可不可以呢？<br>显然这样会让每个词位包含进去越来越多，越来越长的上下文信息，但是这样效率也太低了，显然会让网络层数变得非常非常非常深，但是这种方式太笨重。不过，既然等长卷积堆等长卷积会让每个词位的embedding描述语义描述的更加丰富准确，那么当然我们可以适当的堆两层来提高词位embedding的表示的丰富性。<br><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/equal_cnn.png" alt="avatar"></p>
<h5 id="固定feature-map的数量"><a href="#固定feature-map的数量" class="headerlink" title="固定feature map的数量"></a>固定feature map的数量</h5><p>在表示好每个词位的语义后，其实很多邻接词或者邻接ngram的词义是可以合并的，例如“小明 人 不要 太好”中的“不要”和“太好”虽然语义本来离得很远，但是作为邻接词“不要太好”出现时其语义基本等价为“很好”，这样完全可以把“不要”和“太好”的语义进行合并。同时，合并的过程完全可以在原始的embedding space中进行的，毕竟原文中直接把“不要太好”合并为“很好”是很可以的，完全没有必要动整个语义空间。<br>实际上，相比图像中这种从“点、线、弧”这种low-level特征到“眼睛、鼻子、嘴”这种high-level特征的明显层次性的特征区分，文本中的特征进阶明显要扁平的多，即从单词（1gram）到短语再到3gram、4gram的升级，其实很大程度上均满足“语义取代”的特性。而图像中就很难发生这种”语义取代“现象<br>因此，DPCNN与ResNet很大一个不同就是，<strong>在DPCNN中固定死了feature map的数量</strong>，也就是固定住了embedding space的维度（为了方便理解，以下简称语义空间），使得网络有可能让整个邻接词（邻接ngram）的合并操作在原始空间或者与原始空间相似的空间中进行（当然，网络在实际中会不会这样做是不一定的，只是提供了这么一种条件）。也就是说，整个网络虽然形状上来看是深层的，但是从语义空间上来看完全可以是扁平的。而ResNet则是不断的改变语义空间，使得图像的语义随着网络层的加深也不断的跳向更高level的语义空间。</p>
<h5 id="池化"><a href="#池化" class="headerlink" title="池化"></a><strong>池化</strong></h5><p>所以提供了这么好的合并条件后，我们就可以用pooling layer进行合并啦。每经过一个size=3, stride=2（大小为3，步长为2）的池化层（以下简称1/2池化层），序列的长度就被压缩成了原来的一半。这样同样是size=3的卷积核，每经过一个1/2池化层后，其能感知到的文本片段就比之前长了一倍。例如之前是只能感知3个词位长度的信息，经过1/2池化层后就能感知6个词位长度的信息，这时把1/2池化层和size=3的卷积层组合起来如图：<br><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/dpcnn_pooling.png" alt="avatar"></p>
<h5 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h5><p>于我们在初始化深度CNN时，往往各层权重都是初始化为一个很小的值，这就导致最开始的网络中，后续几乎每层的输入都是接近0，这时网络的输出自然是没意义的，而这些小权重同时也阻碍了梯度的传播，使得网络的初始训练阶段往往要迭代好久才能启动。<br>同时，就算网络启动完成，由于深度网络中仿射矩阵（每两层间的连接边）近似连乘，训练过程中网络也非常容易发生梯度爆炸或弥散问题（虽然由于非共享权重，深度CNN网络比RNN网络要好点）。<br>那么如何解决深度CNN网络的梯度弥散问题呢？<br>ResNet中提出的shortcut-connection/skip-connection/residual-connection（残差连接）就是一种非常简单、合理、有效的解决方案。<br><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/dpcnn_resnet.png" alt="avatar"><br>既然每个block的输入在初始阶段容易是0而无法激活，那么直接用一条线把region embedding层连接到每个block的输入乃至最终的池化层/输出层不就可以。有了shortcut后，梯度就可以忽略卷积层权重的削弱，从shortcut一路无损的传递到各个block手里，直至网络前端，从而极大的缓解了梯度消失问题。</p>
<h4 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DPCNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab, embedding_dim, num_filters, use_pretrain_embedding,num_classes)</span>:</span></span><br><span class="line">        super(DPCNN, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> use_pretrain_embedding:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(vocab.vectors)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.embedding = nn.Embedding(len(vocab), embedding_dim, padding_idx=vocab.pad_index)</span><br><span class="line">        self.conv_region = nn.Conv2d(<span class="number">1</span>, num_filters, (<span class="number">3</span>, embedding_dim), stride=<span class="number">1</span>)</span><br><span class="line">        self.conv = nn.Conv2d(num_filters, num_filters, (<span class="number">3</span>, <span class="number">1</span>), stride=<span class="number">1</span>)</span><br><span class="line">        self.max_pool = nn.MaxPool2d(kernel_size=(<span class="number">3</span>, <span class="number">1</span>), stride=<span class="number">2</span>)</span><br><span class="line">        self.padding1 = nn.ZeroPad2d((<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># top bottom</span></span><br><span class="line">        self.padding2 = nn.ZeroPad2d((<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>))  <span class="comment"># bottom</span></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.fc = nn.Linear(num_filters, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text [batch_size,seq_len]</span></span><br><span class="line">        x = self.embedding(text) <span class="comment"># x=[batch_size,seq_len,embedding_dim]</span></span><br><span class="line">        x = x.unsqueeze(<span class="number">1</span>)  <span class="comment"># [batch_size, 1, seq_len, embedding_dim]</span></span><br><span class="line">        x = self.conv_region(x)  <span class="comment"># x = [batch_size, num_filters, seq_len-3+1, 1]</span></span><br><span class="line"></span><br><span class="line">        x = self.padding1(x)  <span class="comment"># [batch_size, num_filters, seq_len, 1]</span></span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv(x)  <span class="comment"># [batch_size, num_filters, seq_len-3+1, 1]</span></span><br><span class="line">        x = self.padding1(x)  <span class="comment"># [batch_size, num_filters, seq_len, 1]</span></span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv(x)  <span class="comment"># [batch_size, num_filters, seq_len-3+1, 1]</span></span><br><span class="line">        <span class="keyword">while</span> x.size()[<span class="number">2</span>] &gt;= <span class="number">2</span>:</span><br><span class="line">            x = self._block(x) <span class="comment"># [batch_size, num_filters,1,1]</span></span><br><span class="line">        x = x.squeeze()  <span class="comment"># [batch_size, num_filters]</span></span><br><span class="line">        x = self.fc(x) <span class="comment"># [batch_size, 1]</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_block</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.padding2(x)</span><br><span class="line">        px = self.max_pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.padding1(px)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line"></span><br><span class="line">        x = self.padding1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Short Cut</span></span><br><span class="line">        x = x + px</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h3><h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h4><p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/bert_gpt_elmo.png" alt="avatar"></p>
<h5 id="Task-1-MLM"><a href="#Task-1-MLM" class="headerlink" title="Task 1: MLM"></a>Task 1: MLM</h5><p>由于BERT需要通过上下文信息，来预测中心词的信息，同时又不希望模型提前看见中心词的信息，因此提出了一种 Masked Language Model 的预训练方式，即随机从输入预料上 mask 掉一些单词，然后通过的上下文预测该单词，类似于一个完形填空任务。</p>
<p>在预训练任务中，15%的 Word Piece 会被mask，这15%的 Word Piece 中，80%的时候会直接替换为 [Mask] ，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token</p>
<ul>
<li>没有100%mask的原因<ul>
<li>如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词</li>
</ul>
</li>
<li>加入10%随机token的原因<ul>
<li>Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘</li>
<li>另外编码器不知道哪些词需要预测的，哪些词是错误的，因此被迫需要学习每一个token的表示向量</li>
</ul>
</li>
<li>另外，每个batchsize只有15%的单词被mask的原因，是因为性能开销的问题，双向编码器比单项编码器训练要更慢</li>
</ul>
<h5 id="Task-2-NSP"><a href="#Task-2-NSP" class="headerlink" title="Task 2: NSP"></a>Task 2: NSP</h5><p>仅仅一个MLM任务是不足以让 BERT 解决阅读理解等句子关系判断任务的，因此添加了额外的一个预训练任务，即 Next Sequence Prediction。</p>
<p>具体任务即为一个句子关系判断任务，即判断句子B是否是句子A的下文，如果是的话输出’IsNext‘，否则输出’NotNext‘。</p>
<p>训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在图4中的[CLS]符号中</p>
<h5 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h5><ul>
<li>Token Embeddings：即传统的词向量层，每个输入样本的首字符需要设置为[CLS]，可以用于之后的分类任务，若有两个不同的句子，需要用[SEP]分隔，且最后一个字符需要用[SEP]表示终止</li>
<li>Segment Embeddings：为[0,1][0,1]序列，用来在NSP任务中区别两个句子，便于做句子关系判断任务</li>
<li>Position Embeddings：与Transformer中的位置向量不同，BERT中的位置向量是直接训练出来的</li>
</ul>
<h5 id="Fine-tunninng"><a href="#Fine-tunninng" class="headerlink" title="Fine-tunninng"></a>Fine-tunninng</h5><p>对于不同的下游任务，我们仅需要对BERT不同位置的输出进行处理即可，或者直接将BERT不同位置的输出直接输入到下游模型当中。具体的如下所示：</p>
<ul>
<li>对于情感分析等单句分类任务，可以直接输入单个句子（不需要[SEP]分隔双句），将[CLS]的输出直接输入到分类器进行分类</li>
<li>对于句子对任务（句子关系判断任务），需要用[SEP]分隔两个句子输入到模型中，然后同样仅须将[CLS]的输出送到分类器进行分类</li>
<li>对于问答任务，将问题与答案拼接输入到BERT模型中，然后将答案位置的输出向量进行二分类并在句子方向上进行softmax（只需预测开始和结束位置即可）</li>
<li>对于命名实体识别任务，对每个位置的输出进行分类即可，如果将每个位置的输出作为特征输入到CRF将取得更好的效果。</li>
</ul>
<h5 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>BERT的预训练任务MLM使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务</li>
<li>另外，BERT没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计</li>
<li>由于最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）</li>
<li>适合处理自然语义理解类任务(NLU)，而不适合自然语言生成类任务(NLG)</li>
</ul>
<h5 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bert</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bert_model_path,num_classes)</span>:</span></span><br><span class="line">        super(Bert, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_model_path)</span><br><span class="line">        <span class="comment"># 不对bert进行训练</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.bert.parameters():</span><br><span class="line">            param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text [ batch_size,senten_len]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># context = x[0]  # 输入的句子</span></span><br><span class="line">        <span class="comment"># mask = x[2]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># cls [batch_size, 768]</span></span><br><span class="line">        <span class="comment"># sentence [batch size,sen len,  768]</span></span><br><span class="line">        sentence, cls = self.bert(text)</span><br><span class="line"></span><br><span class="line">        out = self.fc(cls)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h4 id="bert-config-json"><a href="#bert-config-json" class="headerlink" title="bert_config.json"></a>bert_config.json</h4><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "attention_probs_dropout_prob": 0.1, #乘法attention时，softmax后dropout概率 </span><br><span class="line">  "directionality": "bidi", </span><br><span class="line">  "hidden_act": "gelu",   #激活函数 </span><br><span class="line">  "hidden_dropout_prob": 0.1, #隐藏层dropout概率 </span><br><span class="line">  "hidden_size": 768, #隐藏单元数 </span><br><span class="line">  "initializer_range": 0.02, #初始化范围 </span><br><span class="line">  "intermediate_size": 3072, #升维维度</span><br><span class="line">  "max_position_embeddings": 512, #一个大于seq_length的参数，用于生成position_embedding</span><br><span class="line">  "num_attention_heads": 12,#每个隐藏层中的attention head数 </span><br><span class="line">  "num_hidden_layers": 2, #隐藏层数 </span><br><span class="line">  "pooler_fc_size": 768, </span><br><span class="line">  "pooler_num_attention_heads": 12, </span><br><span class="line">  "pooler_num_fc_layers": 3, </span><br><span class="line">  "pooler_size_per_head": 128, </span><br><span class="line">  "pooler_type": "first_token_transform", </span><br><span class="line">  "type_vocab_size": 2, #segment_ids类别 [0,1] </span><br><span class="line">  "vocab_size": 21128#词典中词数</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="微博情感分类结果比较"><a href="#微博情感分类结果比较" class="headerlink" title="微博情感分类结果比较"></a>微博情感分类结果比较</h3><div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>速度</th>
<th style="text-align:center">效果(best)：</th>
<th>效果：</th>
<th>参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>FastText</td>
<td>4.43s/epoch</td>
<td style="text-align:center">字向量,训练集：97.30%,验证集：96.39%，测试集：96.00%</td>
<td>词向量,速度：13.97s/epoch，训练集：95.92% 验证集：93.31%，测试集：92.19%</td>
<td>“embedding_dim”: 300, “output_dim”: 1</td>
</tr>
<tr>
<td>Textcnn</td>
<td>18.99s/epoch</td>
<td style="text-align:center">字向量，训练集：97.28%,验证集：97.68%，测试集：97.12%</td>
<td>词向量,速度：36.79s/epoch,训练集：95.63，验证集：94.08%，测试集：94.58%</td>
<td>embedding_dim: 300, n_filters: 100, filter_sizes: [3,4,5], dropout: 0.5, output_dim: 1</td>
</tr>
<tr>
<td>Rnn</td>
<td>23.33s/epoch</td>
<td style="text-align:center">字向量：训练集：97.81%,验证集：97.67%，测试集：97.66%</td>
<td>词向量：速度：22.44s/epoch,训练集：96.03%，验证集：95.46%，测试集：95.12%</td>
<td>“embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 1, “n_layers”:1, “bidirectional”: false, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>Rnn</td>
<td>46.78s/epoch</td>
<td style="text-align:center">字向量：训练集：97.76%，验证集：97.71%，测试集：97.65%</td>
<td>词向量：速度：32.34s/epoch,训练集：92.49%，验证集：95.43%，测试集：92.20%</td>
<td>“embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 1, “<strong>n_layers</strong>“:2, “<strong>bidirectional</strong>“: true, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>Lstm</td>
<td>27.19s/epoch</td>
<td style="text-align:center">字向量：训练集：98.00%，验证集：97.76%，测试集：97.68%</td>
<td>词向量：速度：24.14s/epoch,训练集：93.99%，验证集：96.16%，测试集：96.20%</td>
<td>“embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 1, “n_layers”:1, “bidirectional”: false, “dropout”: 0, “batch_first”: false</td>
</tr>
<tr>
<td>Lstm</td>
<td>65.50s/epoch</td>
<td style="text-align:center">字向量：训练集：98.03%，验证集：97.95%，测试集：97.88%</td>
<td>词向量：速度：40.78s/epoch,训练集：96.93%，验证集：97.03%，测试集：96.80%</td>
<td>“embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 1, “<strong>n_layers</strong>“:2, “<strong>bidirectional</strong>“: true, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>Rnn-attention</td>
<td>24.27s/epoch</td>
<td style="text-align:center">字向量：训练集：97.68%，验证集：97.87%，测试集：97.66%</td>
<td>词向量：速度：22.53s/epoch,训练集：96.18%，验证集：95.61%，测试集：95.53%</td>
<td>“embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 1, “n_layers”: 1, “bidirectional”: false, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>Rnn-attention</td>
<td>48.73s/epoch</td>
<td style="text-align:center">字向量：训练集：98.24%，验证集：97.89%，测试集：97.76%</td>
<td>词向量：速度：33.19s/epoch,训练集：96.53%，验证集：95.78%，测试集：95.50%</td>
<td>“embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 1, “<strong>n_layers</strong>“:2, “<strong>bidirectional</strong>“: true, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>Rcnn</td>
<td>27.51s/epoch</td>
<td style="text-align:center">字向量：训练集：98.36%，验证集：98.36%，测试集：98.30%</td>
<td>词向量：速度：23.61s/epoch,训练集：97.66%，验证集：97.29%，测试集：97.20%</td>
<td>“embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 1, “n_layers”:1, “bidirectional”: false, “dropout”: 0, “batch_first”: false</td>
</tr>
<tr>
<td>Rcnn</td>
<td>54.01s/epoch</td>
<td style="text-align:center">字向量：训练集：98.34%，验证集：98.31%，测试集：98.30%</td>
<td>词向量：速度：34.92s/epoch,训练集：95.77%，验证集：97.57%，测试集：97.49%</td>
<td>“embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 1, “<strong>n_layers</strong>“:2, “<strong>bidirectional</strong>“: true, “dropout”: 0, “batch_first”: false</td>
</tr>
<tr>
<td>Bert</td>
<td>143.17s/epoch</td>
<td style="text-align:center">训练集：97.47%，验证集：97.78%，测试集：97.66%</td>
<td></td>
<td>“num_classes”: 1</td>
</tr>
<tr>
<td>Bert-cnn</td>
<td>254.66s/epoch</td>
<td style="text-align:center">训练集：98.01，验证集：98.32%，测试集：98.21%</td>
<td></td>
<td>“num_filters”: 100, “hidden_size”: 768, “filter_sizes”: [3,4,5], “dropout”: 0.1, “num_classes”: 1</td>
</tr>
<tr>
<td>Bert-rnn</td>
<td>195.59s/epoch</td>
<td style="text-align:center">训练集：97.53%，验证集：97.83%,测试集：97.55%</td>
<td></td>
<td>“rnn_type”: “rnn”, “bert_embedding_dim”: 768, “hidden_dim”: 256, “n_layers”: 2, “bidirectional”: true, “batch_first”: true, “dropout”: 0.1, “num_classes”: 1</td>
</tr>
<tr>
<td>Bert-rcnn</td>
<td>200.17s/epoch</td>
<td style="text-align:center">训练集：98.17%，验证集：98.15%,测试集：98.13%</td>
<td></td>
<td>“rnn_type”: “rnn”, “bert_embedding_dim”: 768, “hidden_dim”: 256, “num_classes”: 1, “n_layers”:2, “bidirectional”: true, “dropout”: 0, “batch_first”: false</td>
</tr>
<tr>
<td>DPCNN</td>
<td>9.76s/epoch</td>
<td style="text-align:center">字向量：训练集：97.93%，验证集：97.72%,测试集：97.62%</td>
<td>词向量：速度：7.62s/epoch,训练集：93.16%，验证集：91.70%，测试集：92.00%</td>
<td>“embedding_dim”: 300, “num_filters”: 100, “num_classes”: 1</td>
</tr>
</tbody>
</table>
</div>
<h3 id="新闻十分类结果比较"><a href="#新闻十分类结果比较" class="headerlink" title="新闻十分类结果比较"></a>新闻十分类结果比较</h3><div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>速度</th>
<th>效果：</th>
<th>效果：</th>
<th>参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>Fast_Text</td>
<td>17.68s/epoch</td>
<td>字向量,训练集：96.57%,验证集：92.87%</td>
<td>18.26s/epoch,词向量，训练集：96.7%，验证集：92.61%</td>
<td>“embedding_dim”: 300, “output_dim”: 10</td>
</tr>
<tr>
<td>TextCNN</td>
<td>143.61s/epoch</td>
<td>字向量,训练集：98.13%,验证集：96.42%</td>
<td>142.95s/epoch,词向量，训练集：98.32%，验证集：95.7%</td>
<td>“embedding_dim”: 300, “n_filters”: 50, “filter_sizes”: [3,4,5], “dropout”: 0.5, “output_dim”: 10</td>
</tr>
<tr>
<td>TextCNN1d</td>
<td>58.45s/epoch</td>
<td>字向量,训练集：99.09%,验证集：96.42%</td>
<td>57.66s/epoch,词向量，训练集：98.97%，验证集：96.69%</td>
<td>“embedding_dim”: 300, “n_filters”: 100, “filter_sizes”: [3,4,5], “dropout”: 0.5, “output_dim”: 10</td>
</tr>
<tr>
<td>DPCNN</td>
<td>58.19s/epoch</td>
<td>字向量,训练集：97.12%,验证集：93.98%</td>
<td>57.87s/epoch,词向量，训练集：96.97%，验证集：95.04%</td>
<td>“use_pretrain_embedding”: true, “embedding_dim”: 300, “num_filters”: 100, “num_classes”: 10</td>
</tr>
<tr>
<td>rcnn</td>
<td>461s/epoch</td>
<td>字向量,训练集：97.85%,验证集：95.41%</td>
<td>461.04s/epoch,词向量，训练集：98.5%，验证集：95.9%</td>
<td>“rnn_type”: “lstm”, “embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 10, “n_layers”:2, “bidirectional”: true, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>rnn（1 layer）</td>
<td>131.58s/epoch</td>
<td>字向量,训练集：93.43%,验证集：90.06%</td>
<td>131.04s/epoch,词向量，训练集：98.06%，验证集：93.19</td>
<td>“rnn_type”: “rnn”, “embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 10, “n_layers”:1, “bidirectional”: false, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>rnn（2，layer）</td>
<td>304.57s/epoch</td>
<td>字向量,训练集：96.5%,验证集：92.6%</td>
<td>304s/epoch,词向量，训练集：94.60%，验证集：93.39%</td>
<td>“rnn_type”: “rnn”, “embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 10, “n_layers”:2, “bidirectional”: true, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>lstm</td>
<td>437.34s/epoch</td>
<td>字向量,训练集：98.45%,验证集：96.01%</td>
<td>439.16s/epoch,词向量，训练集：97.73%，验证集：94.30%</td>
<td>“rnn_type”: “lstm”, “embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 10, “n_layers”:2, “bidirectional”: true, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>Lstm-attention</td>
<td>381.35s/epoch</td>
<td>字向量,训练集：97.09%,验证集：95.00%</td>
<td>380.58s/epoch,词向量，训练集：99.15%，验证集：96.32</td>
<td>“rnn_type”: “lstm”, “embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 10, “n_layers”: 2, “bidirectional”: true, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>Bert</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Bert-cnn</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Bert-rnn</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Bert-rcnn</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>HAN</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>使用bert进行训练，机子跑步起来了，而且即使跑起来也是阉割版的，似乎没什么意义。</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><h4 id="长短文本分类的比较"><a href="#长短文本分类的比较" class="headerlink" title="长短文本分类的比较"></a>长短文本分类的比较</h4><p>对于词嵌入技术的文本表示，短文本和长文本表示上没有差别，此时分类效果的优劣主要在分类模型和训练数据上。</p>
<p>对于数据而言：随着文本越长，语义的重要性就越高，在文本很短的情况下，语义的重要性就很小，比如：“今天 天气 怎么样”，“今天 怎么样 天气”，“怎么样 天气 今天”。你甚至不必要考虑句子是否通顺，基本上可以当一句话处理，没有第二个意思。但是随着文本越来越长，比如512个字符，颠倒一下可能就要归为两类了。</p>
<p>对于模型而言：对于短文本，CNN配合Max-pooling池化(如TextCNN模型)速度快，而且效果也很好。因为短文本上的关键词比较容易找到，而且Max-pooling会直接过滤掉模型认为不重要特征。具体工作机制是：卷积窗口沿着长度为n的文本一个个滑动，类似于n-gram机制对文本切词，然后和文本中的每个词进行相似度计算，因为后面接了个Max-pooling，因此只会保留和卷积核最相近的词。微博数据集属于情感分类，为了判断句子的情感极性，只需要让分类器能识别出“不开心”这类词是个负极性的词，“高兴”、“开心”等这类词是正极性的词，其他词是偏中性词就可以了。因此，当我们把该句子中的各个词条输入给模型去分类时，并不需要去“瞻前顾后”，因此使用一个关注局部的前馈神经网络往往表现更佳。虽然Attention也突出了重点特征，但是难以过滤掉所有低分特征。但是对于长文本直接用CNN就不行了，TextCNN会比HAN模型泛化能力差很多。如果在TextCNN前加一层LSTM，这样效果可以提升很大。</p>
<h4 id="为什么长文本分类的实验中，cnn-和-rnn-没有拉开差距？"><a href="#为什么长文本分类的实验中，cnn-和-rnn-没有拉开差距？" class="headerlink" title="为什么长文本分类的实验中，cnn 和 rnn 没有拉开差距？"></a>为什么长文本分类的实验中，cnn 和 rnn 没有拉开差距？</h4><p>cnn和rnn的精度都很高，分析主要还是分类的文章规则性比较强，且属于特定领域，词量不多，类别差异可能比较明显。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>复杂的模型未必会有很好的结果，简单模型效果未必不理想，没必要一味追求深度学习、复杂模型什么的。选什么样的模型还是要根据数据来的。同一类问题，不同的数据效果差异很大，不要小看任何一类问题，例如分类，我们通常觉得它很简单，但有些数据并非你所想。</p>
<h3 id="文本分类tricks"><a href="#文本分类tricks" class="headerlink" title="文本分类tricks"></a>文本分类tricks</h3><h4 id="分词器"><a href="#分词器" class="headerlink" title="分词器"></a>分词器</h4><p><strong>分词器所分出的词与词向量表中的token粒度match是更重要的事情</strong></p>
<h5 id="已知预训练词向量的分词器"><a href="#已知预训练词向量的分词器" class="headerlink" title="已知预训练词向量的分词器"></a>已知预训练词向量的分词器</h5><p>像word2vec、glove、fasttext这些官方release的预训练词向量都会公布相应训练语料的信息，包括预处理策略如分词，这种情况下直接使用官方的训练该词向量所使用的分词器，此分词器在下游任务的表现十之八九会比其他花里胡哨的分词器好用。</p>
<h5 id="不知道预训练词向量的分词器"><a href="#不知道预训练词向量的分词器" class="headerlink" title="不知道预训练词向量的分词器"></a>不知道预训练词向量的分词器</h5><p>这时就需要去“猜”一下分词器。怎么猜呢？<br>首先，拿到预训练词向量表后，去里面search一些特定词汇比如一些网站、邮箱、成语、人名等，英文里还有n’t等，看看训练词向量使用的分词器是把它们分成什么粒度。<br>然后跑几个分词器，看看哪个分词器的粒度跟他最接近就用哪个，如果不放心，就放到下游任务里跑跑看。</p>
<p>最理想的情况是：先确定最适合当前任务数据集的分词器，再使用同分词器产出的预训练词向量。如果无法满足理想情况，则需要自己在下游任务训练集或者大量同分布无监督语料上训练的词向量更有利于进一步压榨模型的性能。</p>
<h4 id="关于中文字向量"><a href="#关于中文字向量" class="headerlink" title="关于中文字向量"></a>关于中文字向量</h4><p>预训练中文字向量的时候，把窗口开大一些，不要直接使用word-level的窗口大小，效果会比随机初始化的字向量明显的好。</p>
<h4 id="数据集噪声很严重"><a href="#数据集噪声很严重" class="headerlink" title="数据集噪声很严重"></a>数据集噪声很严重</h4><p>里噪声严重有两种情况。对于数据集D(X, Y)，一种是X内部噪声很大（比如文本为口语化表述或由互联网用户生成），一种是Y的噪声很大（一些样本被明显的错误标注，一些样本人也很难定义是属于哪一类，甚至具备类别二义性）。</p>
<h5 id="X内部噪声很大"><a href="#X内部噪声很大" class="headerlink" title="X内部噪声很大"></a>X内部噪声很大</h5><p>法一：直接将模型的输入变成char-level（中文中就是字的粒度），然后train from scratch（不使用预训练词向量）去跟word-level的对比一下，如果char-level的明显的效果好，那么短时间之内就直接基于char-level去做模型。</p>
<p>法二：使用特殊超参的FastText去训练一份词向量：<br>一般来说fasttext在英文中的char ngram的窗口大小一般取值3～6，但是在处理中文时，如果我们的目的是为了去除输入中的噪声，那么我们可以把这个窗口限制为1～2，这种小窗口有利于模型去捕获错别字（比如，我们打一个错误词的时候，一般都是将其中的一个字达成同音异形的另一个字），比如word2vec学出来的“似乎”的最近词可能是“好像”，然而小ngram窗口fasttext学出来的“似乎”最近词则很有可能是“是乎”等内部包含错别字的词，这样就一下子让不太过分的错别字构成的词们又重新回到了一起，甚至可以一定程度上对抗分词器产生的噪声（把一个词切分成多个字）。</p>
<h5 id="Y的噪声很大"><a href="#Y的噪声很大" class="headerlink" title="Y的噪声很大"></a>Y的噪声很大</h5><p>首先忽略这个噪声，强行的把模型尽可能好的训出来。然后让训练好的模型去跑训练集和开发集，取出训练集中的错误样本和开发集中那些以很高的置信度做出错误决策的样本（比如以99%的把握把一个标签为0的样本预测为1），然后去做这些bad cases的分析，如果发现错误标注有很强的规律性，则直接撸一个脚本批量纠正一下（只要确保纠正后的标注正确率比纠正前明显高就行）。<br>如果没有什么规律，但是发现模型高置信度做错的这些样本大部分都是标注错误的话，就直接把这些样本都删掉，常常也可以换来性能的小幅提升，毕竟测试集都是人工标注的，困难样本和错标样本不会太多。</p>
<h4 id="baseline选用CNN还是RNN？"><a href="#baseline选用CNN还是RNN？" class="headerlink" title="baseline选用CNN还是RNN？"></a>baseline选用CNN还是RNN？</h4><p>看数据集，如果感觉数据集里很多很强的ngram可以直接帮助生成正确决策，那就CNN。<br>如果感觉很多case都是那种需要把一个句子看完甚至看两三遍才容易得出正确tag，那就RNN。<br>还可以CNN、RNN的模型都跑出来简单集成一下。</p>
<h4 id="Dropout加在哪里"><a href="#Dropout加在哪里" class="headerlink" title="Dropout加在哪里"></a>Dropout加在哪里</h4><p>word embedding层后、pooling层后、FC层（全联接层）后。</p>
<h4 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h4><p>二分类问题不一定要用sigmoid作为输出层的激活函数，尝试一下包含俩类别的softmax。可能多一条分支就多一点信息，实践中常常带来零点几个点的提升，也是比较玄学了。</p>
<h4 id="多标签分类"><a href="#多标签分类" class="headerlink" title="多标签分类"></a>多标签分类</h4><p>如果一个样本同时拥有多个标签，甚至标签同时还构成了DAG（有向无环图），先用binary-cross-entropy训出个baseline来（即把每个类别变成一个二分类问题，这样N个类别的多标签分类问题就变成了N个二分类问题），这个baseline做好后好像多标签问题不大了，DAG问题自己也基本解决了（虽然模型层并没有专门针对这个问题作处理），然后就可以安心做模型辣。</p>
<h4 id="样本类别不均衡问题"><a href="#样本类别不均衡问题" class="headerlink" title="样本类别不均衡问题"></a>样本类别不均衡问题</h4><p>如果正负样本比小于9:1的话，继续做深度模型调超参，模型做好后会发现这点不均衡对模型来说不值一提，决策阈值也完全不用手调。<br>但是，如果经常一个batch中完全就是同一个类别的样本，或者一些类别的样本经过好多batch都难遇到一个的话，均衡就非常非常有必要了。<a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247484993&amp;idx=1&amp;sn=0bd32089a638e5a1b48239656febb6e0&amp;chksm=970c2e97a07ba7818d63dddbb469486dccb369ecc11f38ffdea596452b9e5bf65772820a8ac9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何优雅而时髦的解决不均衡分类问题</a></p>
<h4 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h4><ol>
<li>别太纠结文本截断长度使用120还是150</li>
<li>别太纠结对性能不敏感的超参数带来的开发集性能的微小提升</li>
<li>别太纠结未登陆词的embedding是初始化成全0还是随机初始化，别跟PAD共享embedding就行</li>
<li>别太纠结优化器用Adam还是MomentumSGD，如果跟SGD的感情还不深，就无脑Adam，最后再用MomentumSGD跑几遍</li>
<li>还是不会用tricks但是就是想跑出个好结果，bert大力出奇迹。</li>
</ol>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a href="https://blog.csdn.net/feilong_csdn/article/details/88655927" target="_blank" rel="noopener">fastText原理和文本分类实战，看这一篇就够了</a></li>
<li><a href="https://blog.csdn.net/asialee_bird/article/details/88813385#一、论文笔记" target="_blank" rel="noopener">TextCNN文本分类（keras实现）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/55263066" target="_blank" rel="noopener">浅谈基于深度学习的文本分类问题</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2018-06-22-4" target="_blank" rel="noopener">从DPCNN出发，撩一下深层word-level文本分类模型</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2019-01-24-5" target="_blank" rel="noopener">文本分类有哪些论文中很少提及却对性能有重要影响的tricks？</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247484682&amp;idx=1&amp;sn=51520138eed826ec891ac8154ee550f9&amp;chksm=970c2ddca07ba4ca33ee14542cff0457601bb16236f8edc1ff0e617051d1f063354dda0f8893&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">从前馈到反馈：解析循环神经网络（RNN）及其tricks</a></li>
<li><a href="http://www.52nlp.cn/tag/长文本分类" target="_blank" rel="noopener"><a href="http://www.52nlp.cn/如何用深度学习做好长文本分类与法律文书智能化" target="_blank" rel="noopener">达观数据曾彦能：如何用深度学习做好长文本分类与法律文书智能化处理</a></a></li>
<li><a href="https://www.zhihu.com/question/326770917/answer/698646465" target="_blank" rel="noopener">短文本分类和长文本分类的模型如何进行选择？</a></li>
<li><a href="https://www.pianshen.com/article/4319299677/" target="_blank" rel="noopener">NLP实践九：HAN原理与文本分类实践</a></li>
</ol>
]]></content>
      <categories>
        <category>文本分类</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>FastText</tag>
      </tags>
  </entry>
  <entry>
    <title>python数据科学速查</title>
    <url>/2020/04/11/python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%80%9F%E6%9F%A5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="速查表"><a href="#速查表" class="headerlink" title="速查表"></a>速查表</h2><h3 id="python-基础"><a href="#python-基础" class="headerlink" title="python 基础"></a>python 基础</h3><p><img src="/2020/04/11/python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%80%9F%E6%9F%A5/python.png" alt="avatar"></p>
<a id="more"></a>
<h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h3><p><img src="/2020/04/11/python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%80%9F%E6%9F%A5/import_data.png" alt="avatar"></p>
<h3 id="jupyter-notebook"><a href="#jupyter-notebook" class="headerlink" title="jupyter notebook"></a>jupyter notebook</h3><p><img src="/2020/04/11/python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%80%9F%E6%9F%A5/jupyter_notebook.png" alt="avatar"></p>
<h3 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h3><p><img src="/2020/04/11/python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%80%9F%E6%9F%A5/numpy.png" alt="avatar"></p>
<h3 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h3><p><img src="/2020/04/11/python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%80%9F%E6%9F%A5/pandas_1.png" alt="avatar"></p>
<p><img src="/2020/04/11/python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%80%9F%E6%9F%A5/pandas_2.png" alt="avatar"></p>
<h3 id="matplotlib"><a href="#matplotlib" class="headerlink" title="matplotlib"></a>matplotlib</h3><p><img src="/2020/04/11/python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%80%9F%E6%9F%A5/matplotlib.png" alt="avatar"></p>
<h3 id="Seaborn"><a href="#Seaborn" class="headerlink" title="Seaborn"></a>Seaborn</h3><p><img src="/2020/04/11/python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%80%9F%E6%9F%A5/seaborn.png" alt="avatar"></p>
<h3 id="SciPy"><a href="#SciPy" class="headerlink" title="SciPy"></a>SciPy</h3><p><img src="/2020/04/11/python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%80%9F%E6%9F%A5/scipy.png" alt="avatar"></p>
<h3 id="scikit-learn"><a href="#scikit-learn" class="headerlink" title="scikit-learn"></a>scikit-learn</h3><p><img src="/2020/04/11/python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%80%9F%E6%9F%A5/scikit_learn.png" alt="avatar"></p>
<h3 id="keras"><a href="#keras" class="headerlink" title="keras"></a>keras</h3><p><img src="/2020/04/11/python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%80%9F%E6%9F%A5/kearas.png" alt="avatar"></p>
<h3 id="bokeh"><a href="#bokeh" class="headerlink" title="bokeh"></a>bokeh</h3><p><img src="/2020/04/11/python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%80%9F%E6%9F%A5/bokeh.png" alt="avatar"></p>
<h3 id="PySpark"><a href="#PySpark" class="headerlink" title="PySpark"></a>PySpark</h3><h4 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h4><p><img src="/2020/04/11/python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%80%9F%E6%9F%A5/spark_sql.png" alt="avatar"></p>
<h4 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h4><p><img src="/2020/04/11/python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%80%9F%E6%9F%A5/spark_rdd.png" alt="avatar"></p>
<h2 id="python"><a href="#python" class="headerlink" title="python"></a>python</h2><h3 id="Main"><a href="#Main" class="headerlink" title="Main"></a>Main</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:     <span class="comment"># Runs main() if file wasn't imported.</span></span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h3 id="List"><a href="#List" class="headerlink" title="List"></a>List</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;list&gt; = &lt;list&gt;[from_inclusive : to_exclusive : ±step_size]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;list&gt;.append(&lt;el&gt;)            <span class="comment"># Or: &lt;list&gt; += [&lt;el&gt;]</span></span><br><span class="line">&lt;list&gt;.extend(&lt;collection&gt;)    <span class="comment"># Or: &lt;list&gt; += &lt;collection&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;list&gt;.sort()</span><br><span class="line">&lt;list&gt;.reverse()</span><br><span class="line">&lt;list&gt; = sorted(&lt;collection&gt;)</span><br><span class="line">&lt;iter&gt; = reversed(&lt;list&gt;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sum_of_elements  = sum(&lt;collection&gt;)</span><br><span class="line">elementwise_sum  = [sum(pair) <span class="keyword">for</span> pair <span class="keyword">in</span> zip(list_a, list_b)]</span><br><span class="line">sorted_by_second = sorted(&lt;collection&gt;, key=<span class="keyword">lambda</span> el: el[<span class="number">1</span>])</span><br><span class="line">sorted_by_both   = sorted(&lt;collection&gt;, key=<span class="keyword">lambda</span> el: (el[<span class="number">1</span>], el[<span class="number">0</span>]))</span><br><span class="line">flatter_list     = list(itertools.chain.from_iterable(&lt;list&gt;))</span><br><span class="line">product_of_elems = functools.reduce(<span class="keyword">lambda</span> out, x: out * x, &lt;collection&gt;)</span><br><span class="line">list_of_chars    = list(&lt;str&gt;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">index = &lt;list&gt;.index(&lt;el&gt;)     <span class="comment"># Returns index of first occurrence or raises ValueError.</span></span><br><span class="line">&lt;list&gt;.insert(index, &lt;el&gt;)     <span class="comment"># Inserts item at index and moves the rest to the right.</span></span><br><span class="line">&lt;el&gt; = &lt;list&gt;.pop([index])     <span class="comment"># Removes and returns item at index or from the end.</span></span><br><span class="line">&lt;list&gt;.remove(&lt;el&gt;)            <span class="comment"># Removes first occurrence of item or raises ValueError.</span></span><br><span class="line">&lt;list&gt;.clear()                 <span class="comment"># Removes all items. Also works on dict and set.</span></span><br></pre></td></tr></table></figure>
<h3 id="Dictionary"><a href="#Dictionary" class="headerlink" title="Dictionary"></a>Dictionary</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;view&gt; = &lt;dict&gt;.keys()                          <span class="comment"># Coll. of keys that reflects changes.</span></span><br><span class="line">&lt;view&gt; = &lt;dict&gt;.values()                        <span class="comment"># Coll. of values that reflects changes.</span></span><br><span class="line">&lt;view&gt; = &lt;dict&gt;.items()                         <span class="comment"># Coll. of key-value tuples.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">value  = &lt;dict&gt;.get(key, default=<span class="literal">None</span>)          <span class="comment"># Returns default if key does not exist.</span></span><br><span class="line">value  = &lt;dict&gt;.setdefault(key, default=<span class="literal">None</span>)   <span class="comment"># Same, but also adds default to dict.</span></span><br><span class="line">&lt;dict&gt; = collections.defaultdict(&lt;type&gt;)        <span class="comment"># Creates a dict with default value of type.</span></span><br><span class="line">&lt;dict&gt; = collections.defaultdict(<span class="keyword">lambda</span>: <span class="number">1</span>)     <span class="comment"># Creates a dict with default value 1.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;dict&gt;.update(&lt;dict&gt;)</span><br><span class="line">&lt;dict&gt; = dict(&lt;collection&gt;)                     <span class="comment"># Creates a dict from coll. of key-value pairs.</span></span><br><span class="line">&lt;dict&gt; = dict(zip(keys, values))                <span class="comment"># Creates a dict from two collections.</span></span><br><span class="line">&lt;dict&gt; = dict.fromkeys(keys [, value])          <span class="comment"># Creates a dict from collection of keys.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">value = &lt;dict&gt;.pop(key)                         <span class="comment"># Removes item from dictionary.</span></span><br><span class="line">&#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> &lt;dict&gt;.items() <span class="keyword">if</span> k <span class="keyword">in</span> keys&#125;  <span class="comment"># Filters dictionary by keys.</span></span><br></pre></td></tr></table></figure>
<h3 id="Counter"><a href="#Counter" class="headerlink" title="Counter"></a>Counter</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>colors = [<span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'yellow'</span>, <span class="string">'blue'</span>, <span class="string">'red'</span>, <span class="string">'blue'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>counter = Counter(colors)</span><br><span class="line">Counter(&#123;<span class="string">'blue'</span>: <span class="number">3</span>, <span class="string">'red'</span>: <span class="number">2</span>, <span class="string">'yellow'</span>: <span class="number">1</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>counter.most_common()[<span class="number">0</span>]</span><br><span class="line">(<span class="string">'blue'</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;set&gt; = set()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;set&gt;.add(&lt;el&gt;)                               <span class="comment"># Or: &lt;set&gt; |= &#123;&lt;el&gt;&#125;</span></span><br><span class="line">&lt;set&gt;.update(&lt;collection&gt;)                    <span class="comment"># Or: &lt;set&gt; |= &lt;set&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;set&gt;  = &lt;set&gt;.union(&lt;coll.&gt;)                 <span class="comment"># Or: &lt;set&gt; | &lt;set&gt;</span></span><br><span class="line">&lt;set&gt;  = &lt;set&gt;.intersection(&lt;coll.&gt;)          <span class="comment"># Or: &lt;set&gt; &amp; &lt;set&gt;</span></span><br><span class="line">&lt;set&gt;  = &lt;set&gt;.difference(&lt;coll.&gt;)            <span class="comment"># Or: &lt;set&gt; - &lt;set&gt;</span></span><br><span class="line">&lt;set&gt;  = &lt;set&gt;.symmetric_difference(&lt;coll.&gt;)  <span class="comment"># Or: &lt;set&gt; ^ &lt;set&gt;</span></span><br><span class="line">&lt;bool&gt; = &lt;set&gt;.issubset(&lt;coll.&gt;)              <span class="comment"># Or: &lt;set&gt; &lt;= &lt;set&gt;</span></span><br><span class="line">&lt;bool&gt; = &lt;set&gt;.issuperset(&lt;coll.&gt;)            <span class="comment"># Or: &lt;set&gt; &gt;= &lt;set&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;set&gt;.remove(&lt;el&gt;)                            <span class="comment"># Raises KeyError.</span></span><br><span class="line">&lt;set&gt;.discard(&lt;el&gt;)                           <span class="comment"># Doesn't raise an error.</span></span><br></pre></td></tr></table></figure>
<h4 id="Frozen-Set"><a href="#Frozen-Set" class="headerlink" title="Frozen Set"></a>Frozen Set</h4><ul>
<li><strong>Is immutable and hashable.</strong></li>
<li><strong>That means it can be used as a key in a dictionary or as an element in a set.</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;frozenset&gt; = frozenset(&lt;collection&gt;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Tuple"><a href="#Tuple" class="headerlink" title="Tuple"></a>Tuple</h3><hr>
<p><strong>Tuple is an immutable and hashable list.</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;tuple&gt; = ()</span><br><span class="line">&lt;tuple&gt; = (&lt;el&gt;, )</span><br><span class="line">&lt;tuple&gt; = (&lt;el_1&gt;, &lt;el_2&gt;, ...)</span><br></pre></td></tr></table></figure></p>
<h4 id="Named-Tuple"><a href="#Named-Tuple" class="headerlink" title="Named Tuple"></a>Named Tuple</h4><p><strong>Tuple’s subclass with named elements.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Point = namedtuple(<span class="string">'Point'</span>, <span class="string">'x y'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p = Point(<span class="number">1</span>, y=<span class="number">2</span>)</span><br><span class="line">Point(x=<span class="number">1</span>, y=<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p[<span class="number">0</span>]</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p.x</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>getattr(p, <span class="string">'y'</span>)</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p._fields  <span class="comment"># Or: Point._fields</span></span><br><span class="line">(<span class="string">'x'</span>, <span class="string">'y'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Range"><a href="#Range" class="headerlink" title="Range"></a>Range</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;range&gt; = range(to_exclusive)</span><br><span class="line">&lt;range&gt; = range(from_inclusive, to_exclusive)</span><br><span class="line">&lt;range&gt; = range(from_inclusive, to_exclusive, ±step_size)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">from_inclusive = &lt;range&gt;.start</span><br><span class="line">to_exclusive   = &lt;range&gt;.stop</span><br></pre></td></tr></table></figure>
<h3 id="Enumerate"><a href="#Enumerate" class="headerlink" title="Enumerate"></a>Enumerate</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, el <span class="keyword">in</span> enumerate(&lt;collection&gt; [, i_start]):</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<h3 id="Iterator"><a href="#Iterator" class="headerlink" title="Iterator"></a>Iterator</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;iter&gt; = iter(&lt;collection&gt;)                 <span class="comment"># Calling `iter(&lt;iter&gt;)` returns unmodified iterator.</span></span><br><span class="line">&lt;iter&gt; = iter(&lt;function&gt;, to_exclusive)     <span class="comment"># Sequence of return values until 'to_exclusive'.</span></span><br><span class="line">&lt;el&gt;   = next(&lt;iter&gt; [, default])           <span class="comment"># Raises StopIteration or returns 'default' on end.</span></span><br></pre></td></tr></table></figure>
<h4 id="Itertools"><a href="#Itertools" class="headerlink" title="Itertools"></a>Itertools</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> count, repeat, cycle, chain, islice</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;iter&gt; = count(start=<span class="number">0</span>, step=<span class="number">1</span>)             <span class="comment"># Returns incremented value endlessly.</span></span><br><span class="line">&lt;iter&gt; = repeat(&lt;el&gt; [, times])             <span class="comment"># Returns element endlessly or 'times' times.</span></span><br><span class="line">&lt;iter&gt; = cycle(&lt;collection&gt;)                <span class="comment"># Repeats the sequence indefinitely.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;iter&gt; = chain(&lt;coll.&gt;, &lt;coll.&gt; [, ...])    <span class="comment"># Empties collections in order.</span></span><br><span class="line">&lt;iter&gt; = chain.from_iterable(&lt;collection&gt;)  <span class="comment"># Empties collections inside a collection in order.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;iter&gt; = islice(&lt;collection&gt;, to_exclusive)</span><br><span class="line">&lt;iter&gt; = islice(&lt;collection&gt;, from_inclusive, to_exclusive)</span><br><span class="line">&lt;iter&gt; = islice(&lt;collection&gt;, from_inclusive, to_exclusive, +step_size)</span><br></pre></td></tr></table></figure>
<h3 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h3><hr>
<ul>
<li><strong>Convenient way to implement the iterator protocol.</strong></li>
<li><strong>Any function that contains a yield statement returns a generator object.</strong></li>
<li><strong>Generators and iterators are interchangeable.</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span><span class="params">(start, step)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">yield</span> start</span><br><span class="line">        start += step</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>counter = count(<span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(counter), next(counter), next(counter)</span><br><span class="line">(<span class="number">10</span>, <span class="number">12</span>, <span class="number">14</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Type"><a href="#Type" class="headerlink" title="Type"></a>Type</h3><hr>
<ul>
<li><strong>Everything is an object.</strong></li>
<li><strong>Every object has a type.</strong></li>
<li><strong>Type and class are synonymous.</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;type&gt; = type(&lt;el&gt;)                <span class="comment"># Or: &lt;el&gt;.__class__</span></span><br><span class="line">&lt;bool&gt; = isinstance(&lt;el&gt;, &lt;type&gt;)  <span class="comment"># Or: issubclass(type(&lt;el&gt;), &lt;type&gt;)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>type(<span class="string">'a'</span>), <span class="string">'a'</span>.__class__, str</span><br><span class="line">(&lt;class 'str'&gt;, &lt;class 'str'&gt;, &lt;class 'str'&gt;)</span><br></pre></td></tr></table></figure>
<h4 id="Some-types-do-not-have-builtin-names-so-they-must-be-imported"><a href="#Some-types-do-not-have-builtin-names-so-they-must-be-imported" class="headerlink" title="Some types do not have builtin names, so they must be imported:"></a>Some types do not have builtin names, so they must be imported:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> types <span class="keyword">import</span> FunctionType, MethodType, LambdaType, GeneratorType</span><br></pre></td></tr></table></figure>
<h4 id="ABC"><a href="#ABC" class="headerlink" title="ABC"></a>ABC</h4><p><strong>An abstract base class introduces virtual subclasses, that don’t inherit from it but are still recognized by isinstance() and issubclass().</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections.abc <span class="keyword">import</span> Sequence, Collection, Iterable</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], Iterable)</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+------------------+----------+------------+----------+</span><br><span class="line">|                  | Sequence | Collection | Iterable |</span><br><span class="line">+------------------+----------+------------+----------+</span><br><span class="line">| list, range, str |   yes    |    yes     |   yes    |</span><br><span class="line">| dict, set        |          |    yes     |   yes    |</span><br><span class="line">| iter             |          |            |   yes    |</span><br><span class="line">+------------------+----------+------------+----------+</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> numbers <span class="keyword">import</span> Integral, Rational, Real, Complex, Number</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>isinstance(<span class="number">123</span>, Number)</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+--------------------+----------+----------+------+---------+--------+</span><br><span class="line">|                    | Integral | Rational | Real | Complex | Number |</span><br><span class="line">+--------------------+----------+----------+------+---------+--------+</span><br><span class="line">| int                |   yes    |   yes    | yes  |   yes   |  yes   |</span><br><span class="line">| fractions.Fraction |          |   yes    | yes  |   yes   |  yes   |</span><br><span class="line">| float              |          |          | yes  |   yes   |  yes   |</span><br><span class="line">| complex            |          |          |      |   yes   |  yes   |</span><br><span class="line">+--------------------+----------+----------+------+---------+--------+</span><br></pre></td></tr></table></figure>
<h3 id="String"><a href="#String" class="headerlink" title="String"></a>String</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;str&gt;  = &lt;str&gt;.strip()                       <span class="comment"># Strips all whitespace characters from both ends.</span></span><br><span class="line">&lt;str&gt;  = &lt;str&gt;.strip(<span class="string">'&lt;chars&gt;'</span>)              <span class="comment"># Strips all passed characters from both ends.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;list&gt; = &lt;str&gt;.split()                       <span class="comment"># Splits on one or more whitespace characters.</span></span><br><span class="line">&lt;list&gt; = &lt;str&gt;.split(sep=<span class="literal">None</span>, maxsplit=<span class="number">-1</span>)  <span class="comment"># Splits on 'sep' str at most 'maxsplit' times.</span></span><br><span class="line">&lt;list&gt; = &lt;str&gt;.splitlines(keepends=<span class="literal">False</span>)    <span class="comment"># Splits on line breaks. Keeps them if 'keepends'.</span></span><br><span class="line">&lt;str&gt;  = &lt;str&gt;.join(&lt;coll_of_strings&gt;)       <span class="comment"># Joins elements using string as separator.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;bool&gt; = &lt;sub_str&gt; <span class="keyword">in</span> &lt;str&gt;                  <span class="comment"># Checks if string contains a substring.</span></span><br><span class="line">&lt;bool&gt; = &lt;str&gt;.startswith(&lt;sub_str&gt;)         <span class="comment"># Pass tuple of strings for multiple options.</span></span><br><span class="line">&lt;bool&gt; = &lt;str&gt;.endswith(&lt;sub_str&gt;)           <span class="comment"># Pass tuple of strings for multiple options.</span></span><br><span class="line">&lt;int&gt;  = &lt;str&gt;.find(&lt;sub_str&gt;)               <span class="comment"># Returns start index of first match or -1.</span></span><br><span class="line">&lt;int&gt;  = &lt;str&gt;.index(&lt;sub_str&gt;)              <span class="comment"># Same but raises ValueError.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;str&gt;  = &lt;str&gt;.replace(old, new [, count])   <span class="comment"># Replaces 'old' with 'new' at most 'count' times.</span></span><br><span class="line">&lt;bool&gt; = &lt;str&gt;.isnumeric()                   <span class="comment"># True if str contains only numeric characters.</span></span><br><span class="line">&lt;list&gt; = textwrap.wrap(&lt;str&gt;, width)         <span class="comment"># Nicely breaks string into lines.</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Also: <code>&#39;lstrip()&#39;</code>, <code>&#39;rstrip()&#39;</code>.</strong></li>
<li><strong>Also: <code>&#39;lower()&#39;</code>, <code>&#39;upper()&#39;</code>, <code>&#39;capitalize()&#39;</code> and <code>&#39;title()&#39;</code>.</strong></li>
</ul>
<h4 id="Char"><a href="#Char" class="headerlink" title="Char"></a>Char</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;str&gt; = chr(&lt;int&gt;)  <span class="comment"># Converts int to unicode char.</span></span><br><span class="line">&lt;int&gt; = ord(&lt;str&gt;)  <span class="comment"># Converts unicode char to int.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>ord(<span class="string">'0'</span>), ord(<span class="string">'9'</span>)</span><br><span class="line">(<span class="number">48</span>, <span class="number">57</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ord(<span class="string">'A'</span>), ord(<span class="string">'Z'</span>)</span><br><span class="line">(<span class="number">65</span>, <span class="number">90</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ord(<span class="string">'a'</span>), ord(<span class="string">'z'</span>)</span><br><span class="line">(<span class="number">97</span>, <span class="number">122</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Regex"><a href="#Regex" class="headerlink" title="Regex"></a>Regex</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">&lt;str&gt;   = re.sub(&lt;regex&gt;, new, text, count=<span class="number">0</span>)  <span class="comment"># Substitutes all occurrences.</span></span><br><span class="line">&lt;list&gt;  = re.findall(&lt;regex&gt;, text)            <span class="comment"># Returns all occurrences.</span></span><br><span class="line">&lt;list&gt;  = re.split(&lt;regex&gt;, text, maxsplit=<span class="number">0</span>)  <span class="comment"># Use brackets in regex to keep the matches.</span></span><br><span class="line">&lt;Match&gt; = re.search(&lt;regex&gt;, text)             <span class="comment"># Searches for first occurrence of pattern.</span></span><br><span class="line">&lt;Match&gt; = re.match(&lt;regex&gt;, text)              <span class="comment"># Searches only at the beginning of the text.</span></span><br><span class="line">&lt;iter&gt;  = re.finditer(&lt;regex&gt;, text)           <span class="comment"># Returns all occurrences as match objects.</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Argument <code>&#39;flags=re.IGNORECASE&#39;</code> can be used with all functions.</strong></li>
<li><strong>Argument <code>&#39;flags=re.MULTILINE&#39;</code> makes <code>&#39;^&#39;</code> and <code>&#39;$&#39;</code> match the start/end of each line.</strong></li>
<li><strong>Argument <code>&#39;flags=re.DOTALL&#39;</code> makes dot also accept newline.</strong></li>
<li><strong>Use <code>r&#39;\1&#39;</code> or <code>&#39;\\1&#39;</code> for backreference.</strong></li>
<li><strong>Use <code>&#39;?&#39;</code> to make an operator non-greedy.</strong></li>
</ul>
<h4 id="Match-Object"><a href="#Match-Object" class="headerlink" title="Match Object"></a>Match Object</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;str&gt;   = &lt;Match&gt;.group()   <span class="comment"># Whole match. Also group(0).</span></span><br><span class="line">&lt;str&gt;   = &lt;Match&gt;.group(<span class="number">1</span>)  <span class="comment"># Part in first bracket.</span></span><br><span class="line">&lt;tuple&gt; = &lt;Match&gt;.groups()  <span class="comment"># All bracketed parts.</span></span><br><span class="line">&lt;int&gt;   = &lt;Match&gt;.start()   <span class="comment"># Start index of a match.</span></span><br><span class="line">&lt;int&gt;   = &lt;Match&gt;.end()     <span class="comment"># Exclusive end index of a match.</span></span><br></pre></td></tr></table></figure>
<h4 id="Special-Sequences"><a href="#Special-Sequences" class="headerlink" title="Special Sequences"></a>Special Sequences</h4><ul>
<li><strong>By default digits, whitespaces and alphanumerics from all alphabets are matched, unless <code>&#39;flags=re.ASCII&#39;</code> argument is used.</strong></li>
<li><strong>Use capital letters for negation.</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'\d'</span> == <span class="string">'[0-9]'</span>             <span class="comment"># Digit</span></span><br><span class="line"><span class="string">'\s'</span> == <span class="string">'[ \t\n\r\f\v]'</span>     <span class="comment"># Whitespace</span></span><br><span class="line"><span class="string">'\w'</span> == <span class="string">'[a-zA-Z0-9_]'</span>      <span class="comment"># Alphanumeric</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Format"><a href="#Format" class="headerlink" title="Format"></a>Format</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;str&gt; = <span class="string">f'<span class="subst">&#123;&lt;el_1&gt;&#125;</span>, <span class="subst">&#123;&lt;el_2&gt;&#125;</span>'</span></span><br><span class="line">&lt;str&gt; = <span class="string">'&#123;&#125;, &#123;&#125;'</span>.format(&lt;el_1&gt;, &lt;el_2&gt;)</span><br></pre></td></tr></table></figure>
<h4 id="Attributes"><a href="#Attributes" class="headerlink" title="Attributes"></a>Attributes</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Person = namedtuple(<span class="string">'Person'</span>, <span class="string">'name height'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>person = Person(<span class="string">'Jean-Luc'</span>, <span class="number">187</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">f'<span class="subst">&#123;person.height&#125;</span>'</span></span><br><span class="line"><span class="string">'187'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'&#123;p.height&#125;'</span>.format(p=person)</span><br><span class="line"><span class="string">'187'</span></span><br></pre></td></tr></table></figure>
<h4 id="General-Options"><a href="#General-Options" class="headerlink" title="General Options"></a>General Options</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;&lt;el&gt;:&lt;<span class="number">10</span>&#125;       <span class="comment"># '&lt;el&gt;      '</span></span><br><span class="line">&#123;&lt;el&gt;:^<span class="number">10</span>&#125;       <span class="comment"># '   &lt;el&gt;   '</span></span><br><span class="line">&#123;&lt;el&gt;:&gt;<span class="number">10</span>&#125;       <span class="comment"># '      &lt;el&gt;'</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;&lt;el&gt;:.&lt;<span class="number">10</span>&#125;      <span class="comment"># '&lt;el&gt;......'</span></span><br><span class="line">&#123;&lt;el&gt;:&gt;<span class="number">0</span>&#125;        <span class="comment"># '&lt;el&gt;'</span></span><br></pre></td></tr></table></figure>
<h4 id="Strings"><a href="#Strings" class="headerlink" title="Strings"></a>Strings</h4><p><strong><code>&#39;!r&#39;</code> calls object’s repr() method, instead of str(), to get a string.</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">'abcde'</span>!r:&lt;<span class="number">10</span>&#125;  <span class="comment"># "'abcde'   "</span></span><br><span class="line">&#123;<span class="string">'abcde'</span>:<span class="number">.3</span>&#125;     <span class="comment"># 'abc'</span></span><br><span class="line">&#123;<span class="string">'abcde'</span>:<span class="number">10.3</span>&#125;   <span class="comment"># 'abc       '</span></span><br></pre></td></tr></table></figure></p>
<h4 id="Numbers"><a href="#Numbers" class="headerlink" title="Numbers"></a>Numbers</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123; <span class="number">123456</span>:<span class="number">10</span>,&#125;    <span class="comment"># '   123,456'</span></span><br><span class="line">&#123; <span class="number">123456</span>:<span class="number">10</span>_&#125;    <span class="comment"># '   123_456'</span></span><br><span class="line">&#123; <span class="number">123456</span>:+<span class="number">10</span>&#125;    <span class="comment"># '   +123456'</span></span><br><span class="line">&#123;<span class="number">-123456</span>:=<span class="number">10</span>&#125;    <span class="comment"># '-   123456'</span></span><br><span class="line">&#123; <span class="number">123456</span>: &#125;      <span class="comment"># ' 123456'</span></span><br><span class="line">&#123;<span class="number">-123456</span>: &#125;      <span class="comment"># '-123456'</span></span><br></pre></td></tr></table></figure>
<h4 id="Floats"><a href="#Floats" class="headerlink" title="Floats"></a>Floats</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="number">1.23456</span>:<span class="number">10.3</span>&#125;   <span class="comment"># '      1.23'</span></span><br><span class="line">&#123;<span class="number">1.23456</span>:<span class="number">10.3</span>f&#125;  <span class="comment"># '     1.235'</span></span><br><span class="line">&#123;<span class="number">1.23456</span>:<span class="number">10.3</span>e&#125;  <span class="comment"># ' 1.235e+00'</span></span><br><span class="line">&#123;<span class="number">1.23456</span>:<span class="number">10.3</span>%&#125;  <span class="comment"># '  123.456%'</span></span><br></pre></td></tr></table></figure>
<h4 id="Comparison-of-float-presentation-types"><a href="#Comparison-of-float-presentation-types" class="headerlink" title="Comparison of float presentation types:"></a>Comparison of float presentation types:</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+----------------+----------------+---------------+----------------+-----------------+</span><br><span class="line">|                |    &#123;&lt;float&gt;&#125;   |  &#123;&lt;float&gt;:f&#125;  |   &#123;&lt;float&gt;:e&#125;  |   &#123;&lt;float&gt;:%&#125;   |</span><br><span class="line">+----------------+----------------+---------------+----------------+-----------------+</span><br><span class="line">|    0.000056789 |   &#39;5.6789e-05&#39; |    &#39;0.000057&#39; | &#39;5.678900e-05&#39; |     &#39;0.005679%&#39; |</span><br><span class="line">|    0.00056789  |   &#39;0.00056789&#39; |    &#39;0.000568&#39; | &#39;5.678900e-04&#39; |     &#39;0.056789%&#39; |</span><br><span class="line">|    0.0056789   |   &#39;0.0056789&#39;  |    &#39;0.005679&#39; | &#39;5.678900e-03&#39; |     &#39;0.567890%&#39; |</span><br><span class="line">|    0.056789    |   &#39;0.056789&#39;   |    &#39;0.056789&#39; | &#39;5.678900e-02&#39; |     &#39;5.678900%&#39; |</span><br><span class="line">|    0.56789     |   &#39;0.56789&#39;    |    &#39;0.567890&#39; | &#39;5.678900e-01&#39; |    &#39;56.789000%&#39; |</span><br><span class="line">|    5.6789      |   &#39;5.6789&#39;     |    &#39;5.678900&#39; | &#39;5.678900e+00&#39; |   &#39;567.890000%&#39; |</span><br><span class="line">|   56.789       |  &#39;56.789&#39;      |   &#39;56.789000&#39; | &#39;5.678900e+01&#39; |  &#39;5678.900000%&#39; |</span><br><span class="line">|  567.89        | &#39;567.89&#39;       |  &#39;567.890000&#39; | &#39;5.678900e+02&#39; | &#39;56789.000000%&#39; |</span><br><span class="line">+----------------+----------------+---------------+----------------+-----------------+</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+----------------+----------------+---------------+----------------+-----------------+</span><br><span class="line">|                |  &#123;&lt;float&gt;:.2&#125;  | &#123;&lt;float&gt;:.2f&#125; |  &#123;&lt;float&gt;:.2e&#125; |  &#123;&lt;float&gt;:.2%&#125;  |</span><br><span class="line">+----------------+----------------+---------------+----------------+-----------------+</span><br><span class="line">|    0.000056789 |   &#39;5.7e-05&#39;    |      &#39;0.00&#39;   |   &#39;5.68e-05&#39;   |       &#39;0.01%&#39;   |</span><br><span class="line">|    0.00056789  |   &#39;0.00057&#39;    |      &#39;0.00&#39;   |   &#39;5.68e-04&#39;   |       &#39;0.06%&#39;   |</span><br><span class="line">|    0.0056789   |   &#39;0.0057&#39;     |      &#39;0.01&#39;   |   &#39;5.68e-03&#39;   |       &#39;0.57%&#39;   |</span><br><span class="line">|    0.056789    |   &#39;0.057&#39;      |      &#39;0.06&#39;   |   &#39;5.68e-02&#39;   |       &#39;5.68%&#39;   |</span><br><span class="line">|    0.56789     |   &#39;0.57&#39;       |      &#39;0.57&#39;   |   &#39;5.68e-01&#39;   |      &#39;56.79%&#39;   |</span><br><span class="line">|    5.6789      |   &#39;5.7&#39;        |      &#39;5.68&#39;   |   &#39;5.68e+00&#39;   |     &#39;567.89%&#39;   |</span><br><span class="line">|   56.789       |   &#39;5.7e+01&#39;    |     &#39;56.79&#39;   |   &#39;5.68e+01&#39;   |    &#39;5678.90%&#39;   |</span><br><span class="line">|  567.89        |   &#39;5.7e+02&#39;    |    &#39;567.89&#39;   |   &#39;5.68e+02&#39;   |   &#39;56789.00%&#39;   |</span><br><span class="line">+----------------+----------------+---------------+----------------+-----------------+</span><br></pre></td></tr></table></figure>
<h4 id="Ints"><a href="#Ints" class="headerlink" title="Ints"></a>Ints</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="number">90</span>:c&#125;           <span class="comment"># 'Z'</span></span><br><span class="line">&#123;<span class="number">90</span>:X&#125;           <span class="comment"># '5A'</span></span><br><span class="line">&#123;<span class="number">90</span>:b&#125;           <span class="comment"># '1011010'</span></span><br></pre></td></tr></table></figure>
<h3 id="Numbers-1"><a href="#Numbers-1" class="headerlink" title="Numbers"></a>Numbers</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;int&gt;      = int(&lt;float/str/bool&gt;)    <span class="comment"># Or: math.floor(&lt;float&gt;)</span></span><br><span class="line">&lt;float&gt;    = float(&lt;int/str/bool&gt;)</span><br><span class="line">&lt;complex&gt;  = complex(real=<span class="number">0</span>, imag=<span class="number">0</span>)  <span class="comment"># Or: &lt;real&gt; + &lt;real&gt;j</span></span><br><span class="line">&lt;Fraction&gt; = fractions.Fraction(numerator=<span class="number">0</span>, denominator=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>&#39;int(&lt;str&gt;)&#39;</code> and <code>&#39;float(&lt;str&gt;)&#39;</code> raise ‘ValueError’ on malformed strings.</strong></li>
</ul>
<h4 id="Basic-Functions"><a href="#Basic-Functions" class="headerlink" title="Basic Functions"></a>Basic Functions</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;num&gt;  = pow(&lt;num&gt;, &lt;num&gt;)            <span class="comment"># Or: &lt;num&gt; ** &lt;num&gt;</span></span><br><span class="line">&lt;real&gt; = abs(&lt;num&gt;)</span><br><span class="line">&lt;int&gt;  = round(&lt;real&gt;)</span><br><span class="line">&lt;real&gt; = round(&lt;real&gt;, ±ndigits)      <span class="comment"># `round(126, -1) == 130`</span></span><br></pre></td></tr></table></figure>
<h4 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> e, pi, inf, nan</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> cos, acos, sin, asin, tan, atan, degrees, radians</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log, log10, log2</span><br></pre></td></tr></table></figure>
<h4 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> statistics <span class="keyword">import</span> mean, median, variance, pvariance, pstdev</span><br></pre></td></tr></table></figure>
<h4 id="Random"><a href="#Random" class="headerlink" title="Random"></a>Random</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> random, randint, choice, shuffle</span><br><span class="line">&lt;float&gt; = random()</span><br><span class="line">&lt;int&gt;   = randint(from_inclusive, to_inclusive)</span><br><span class="line">&lt;el&gt;    = choice(&lt;list&gt;)</span><br><span class="line">shuffle(&lt;list&gt;)</span><br></pre></td></tr></table></figure>
<h4 id="Bin-Hex"><a href="#Bin-Hex" class="headerlink" title="Bin, Hex"></a>Bin, Hex</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;int&gt;     = <span class="number">0</span>b&lt;bin&gt;            <span class="comment"># Or: 0x&lt;hex&gt;</span></span><br><span class="line">&lt;int&gt;     = int(<span class="string">'0b&lt;bin&gt;'</span>, <span class="number">0</span>)  <span class="comment"># Or: int('0x&lt;hex&gt;', 0)</span></span><br><span class="line">&lt;int&gt;     = int(<span class="string">'&lt;bin&gt;'</span>, <span class="number">2</span>)    <span class="comment"># Or: int('&lt;hex&gt;', 16)</span></span><br><span class="line"><span class="string">'0b&lt;bin&gt;'</span> = bin(&lt;int&gt;)         <span class="comment"># Or: '0x&lt;hex&gt;' = hex(&lt;int&gt;)</span></span><br></pre></td></tr></table></figure>
<h4 id="Bitwise-Operators"><a href="#Bitwise-Operators" class="headerlink" title="Bitwise Operators"></a>Bitwise Operators</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;int&gt;     = &lt;int&gt; &amp; &lt;int&gt;      <span class="comment"># And</span></span><br><span class="line">&lt;int&gt;     = &lt;int&gt; | &lt;int&gt;      <span class="comment"># Or</span></span><br><span class="line">&lt;int&gt;     = &lt;int&gt; ^ &lt;int&gt;      <span class="comment"># Xor (0 if both bits equal)</span></span><br><span class="line">&lt;int&gt;     = &lt;int&gt; &lt;&lt; n_bits    <span class="comment"># Shift left</span></span><br><span class="line">&lt;int&gt;     = &lt;int&gt; &gt;&gt; n_bits    <span class="comment"># Shift right</span></span><br><span class="line">&lt;int&gt;     = ~&lt;int&gt;             <span class="comment"># Compliment (flips bits)</span></span><br></pre></td></tr></table></figure>
<h3 id="Combinatorics"><a href="#Combinatorics" class="headerlink" title="Combinatorics"></a>Combinatorics</h3><hr>
<ul>
<li><strong>Every function returns an iterator.</strong></li>
<li><strong>If you want to print the iterator, you need to pass it to the list() function!</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product, combinations, combinations_with_replacement, permutations</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>product([<span class="number">0</span>, <span class="number">1</span>], repeat=<span class="number">3</span>)</span><br><span class="line">[(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line"> (<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>product(<span class="string">'ab'</span>, <span class="string">'12'</span>)</span><br><span class="line">[(<span class="string">'a'</span>, <span class="string">'1'</span>), (<span class="string">'a'</span>, <span class="string">'2'</span>),</span><br><span class="line"> (<span class="string">'b'</span>, <span class="string">'1'</span>), (<span class="string">'b'</span>, <span class="string">'2'</span>)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>combinations(<span class="string">'abc'</span>, <span class="number">2</span>)</span><br><span class="line">[(<span class="string">'a'</span>, <span class="string">'b'</span>), (<span class="string">'a'</span>, <span class="string">'c'</span>), (<span class="string">'b'</span>, <span class="string">'c'</span>)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>combinations_with_replacement(<span class="string">'abc'</span>, <span class="number">2</span>)</span><br><span class="line">[(<span class="string">'a'</span>, <span class="string">'a'</span>), (<span class="string">'a'</span>, <span class="string">'b'</span>), (<span class="string">'a'</span>, <span class="string">'c'</span>),</span><br><span class="line"> (<span class="string">'b'</span>, <span class="string">'b'</span>), (<span class="string">'b'</span>, <span class="string">'c'</span>),</span><br><span class="line"> (<span class="string">'c'</span>, <span class="string">'c'</span>)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>permutations(<span class="string">'abc'</span>, <span class="number">2</span>)</span><br><span class="line">[(<span class="string">'a'</span>, <span class="string">'b'</span>), (<span class="string">'a'</span>, <span class="string">'c'</span>),</span><br><span class="line"> (<span class="string">'b'</span>, <span class="string">'a'</span>), (<span class="string">'b'</span>, <span class="string">'c'</span>),</span><br><span class="line"> (<span class="string">'c'</span>, <span class="string">'a'</span>), (<span class="string">'c'</span>, <span class="string">'b'</span>)]</span><br></pre></td></tr></table></figure>
<h3 id="Datetime"><a href="#Datetime" class="headerlink" title="Datetime"></a>Datetime</h3><hr>
<ul>
<li><strong>Module ‘datetime’ provides ‘date’ <code>&lt;D&gt;</code>, ‘time’ <code>&lt;T&gt;</code>, ‘datetime’ <code>&lt;DT&gt;</code> and ‘timedelta’ <code>&lt;TD&gt;</code> classes. All are immutable and hashable.</strong></li>
<li><strong>Time and datetime can be ‘aware’ <code>&lt;a&gt;</code>, meaning they have defined timezone, or ‘naive’ <code>&lt;n&gt;</code>, meaning they don’t.</strong></li>
<li><strong>If object is naive it is presumed to be in system’s timezone.</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> date, time, datetime, timedelta</span><br><span class="line"><span class="keyword">from</span> dateutil.tz <span class="keyword">import</span> UTC, tzlocal, gettz</span><br></pre></td></tr></table></figure>
<h4 id="Constructors"><a href="#Constructors" class="headerlink" title="Constructors"></a>Constructors</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;D&gt;  = date(year, month, day)</span><br><span class="line">&lt;T&gt;  = time(hour=<span class="number">0</span>, minute=<span class="number">0</span>, second=<span class="number">0</span>, microsecond=<span class="number">0</span>, tzinfo=<span class="literal">None</span>, fold=<span class="number">0</span>)</span><br><span class="line">&lt;DT&gt; = datetime(year, month, day, hour=<span class="number">0</span>, minute=<span class="number">0</span>, second=<span class="number">0</span>, ...)</span><br><span class="line">&lt;TD&gt; = timedelta(days=<span class="number">0</span>, seconds=<span class="number">0</span>, microseconds=<span class="number">0</span>, milliseconds=<span class="number">0</span>,</span><br><span class="line">                 minutes=<span class="number">0</span>, hours=<span class="number">0</span>, weeks=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Use <code>&#39;&lt;D/DT&gt;.weekday()&#39;</code> to get the day of the week (Mon == 0).</strong></li>
<li><strong><code>&#39;fold=1&#39;</code> means second pass in case of time jumping back for one hour.</strong></li>
</ul>
<h4 id="Now"><a href="#Now" class="headerlink" title="Now"></a>Now</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;D/DTn&gt;  = D/DT.today()                     <span class="comment"># Current local date or naive datetime.</span></span><br><span class="line">&lt;DTn&gt;    = DT.utcnow()                      <span class="comment"># Naive datetime from current UTC time.</span></span><br><span class="line">&lt;DTa&gt;    = DT.now(&lt;tzinfo&gt;)                 <span class="comment"># Aware datetime from current tz time.</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>To extract time use <code>&#39;&lt;DTn&gt;.time()&#39;</code>, <code>&#39;&lt;DTa&gt;.time()&#39;</code> or <code>&#39;&lt;DTa&gt;.timetz()&#39;</code>.</strong></li>
</ul>
<h4 id="Timezone"><a href="#Timezone" class="headerlink" title="Timezone"></a>Timezone</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;tzinfo&gt; = UTC                              <span class="comment"># UTC timezone. London without DST.</span></span><br><span class="line">&lt;tzinfo&gt; = tzlocal()                        <span class="comment"># Local timezone. Also gettz().</span></span><br><span class="line">&lt;tzinfo&gt; = gettz(<span class="string">'&lt;Cont.&gt;/&lt;City&gt;'</span>)          <span class="comment"># Timezone from 'Continent/City_Name' str.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;DTa&gt;    = &lt;DT&gt;.astimezone(&lt;tzinfo&gt;)        <span class="comment"># Datetime, converted to passed timezone.</span></span><br><span class="line">&lt;Ta/DTa&gt; = &lt;T/DT&gt;.replace(tzinfo=&lt;tzinfo&gt;)  <span class="comment"># Unconverted object with new timezone.</span></span><br></pre></td></tr></table></figure>
<h4 id="Encode"><a href="#Encode" class="headerlink" title="Encode"></a>Encode</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;D/T/DT&gt; = D/T/DT.fromisoformat(<span class="string">'&lt;iso&gt;'</span>)    <span class="comment"># Object from ISO string.</span></span><br><span class="line">&lt;DT&gt;     = DT.strptime(&lt;str&gt;, <span class="string">'&lt;format&gt;'</span>)   <span class="comment"># Datetime from str, according to format.</span></span><br><span class="line">&lt;D/DTn&gt;  = D/DT.fromordinal(&lt;int&gt;)          <span class="comment"># D/DTn from days since Christ, at midnight.</span></span><br><span class="line">&lt;DTn&gt;    = DT.fromtimestamp(&lt;real&gt;)         <span class="comment"># Local time DTn from seconds since Epoch.</span></span><br><span class="line">&lt;DTa&gt;    = DT.fromtimestamp(&lt;real&gt;, &lt;tz.&gt;)  <span class="comment"># Aware datetime from seconds since Epoch.</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>ISO strings come in following forms: <code>&#39;YYYY-MM-DD&#39;</code>, <code>&#39;HH:MM:SS.ffffff[±&lt;offset&gt;]&#39;</code>, or both separated by <code>&#39;T&#39;</code>. Offset is formatted as: <code>&#39;HH:MM&#39;</code>.</strong></li>
<li><strong>On Unix systems Epoch is <code>&#39;1970-01-01 00:00 UTC&#39;</code>, <code>&#39;1970-01-01 01:00 CET&#39;</code>, …</strong></li>
</ul>
<h4 id="Decode"><a href="#Decode" class="headerlink" title="Decode"></a>Decode</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;str&gt;    = &lt;D/T/DT&gt;.isoformat()             <span class="comment"># ISO string representation.</span></span><br><span class="line">&lt;str&gt;    = &lt;D/T/DT&gt;.strftime(<span class="string">'&lt;format&gt;'</span>)    <span class="comment"># Custom string representation.</span></span><br><span class="line">&lt;int&gt;    = &lt;D/DT&gt;.toordinal()               <span class="comment"># Days since Christ, ignoring time and tz.</span></span><br><span class="line">&lt;float&gt;  = &lt;DTn&gt;.timestamp()                <span class="comment"># Seconds since Epoch from DTn in local time.</span></span><br><span class="line">&lt;float&gt;  = &lt;DTa&gt;.timestamp()                <span class="comment"># Seconds since Epoch from DTa.</span></span><br></pre></td></tr></table></figure>
<h4 id="Format-1"><a href="#Format-1" class="headerlink" title="Format"></a>Format</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dt = datetime.strptime(<span class="string">'2015-05-14 23:39:00.00 +0200'</span>, <span class="string">'%Y-%m-%d %H:%M:%S.%f %z'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dt.strftime(<span class="string">"%A, %dth of %B '%y, %I:%M%p %Z"</span>)</span><br><span class="line"><span class="string">"Thursday, 14th of May '15, 11:39PM UTC+02:00"</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>For abbreviated weekday and month use <code>&#39;%a&#39;</code> and <code>&#39;%b&#39;</code>.</strong></li>
<li><strong>When parsing, <code>&#39;%z&#39;</code> also accepts <code>&#39;±HH:MM&#39;</code>.</strong></li>
</ul>
<h4 id="Arithmetics"><a href="#Arithmetics" class="headerlink" title="Arithmetics"></a>Arithmetics</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;D/DT&gt;   = &lt;D/DT&gt; ±  &lt;TD&gt;</span><br><span class="line">&lt;TD&gt;     = &lt;TD&gt;   ±  &lt;TD&gt;</span><br><span class="line">&lt;TD&gt;     = &lt;TD&gt;   */ &lt;real&gt;</span><br><span class="line">&lt;float&gt;  = &lt;TD&gt;   /  &lt;TD&gt;</span><br></pre></td></tr></table></figure>
<h3 id="Arguments"><a href="#Arguments" class="headerlink" title="Arguments"></a>Arguments</h3><hr>
<h4 id="Inside-Function-Call"><a href="#Inside-Function-Call" class="headerlink" title="Inside Function Call"></a>Inside Function Call</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;function&gt;(&lt;positional_args&gt;)                  <span class="comment"># f(0, 0)</span></span><br><span class="line">&lt;function&gt;(&lt;keyword_args&gt;)                     <span class="comment"># f(x=0, y=0)</span></span><br><span class="line">&lt;function&gt;(&lt;positional_args&gt;, &lt;keyword_args&gt;)  <span class="comment"># f(0, y=0)</span></span><br></pre></td></tr></table></figure>
<h4 id="Inside-Function-Definition"><a href="#Inside-Function-Definition" class="headerlink" title="Inside Function Definition"></a>Inside Function Definition</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(&lt;nondefault_args&gt;)</span>:</span>                      <span class="comment"># def f(x, y):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(&lt;default_args&gt;)</span>:</span>                         <span class="comment"># def f(x=0, y=0):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(&lt;nondefault_args&gt;, &lt;default_args&gt;)</span>:</span>      <span class="comment"># def f(x, y=0):</span></span><br></pre></td></tr></table></figure>
<h3 id="Splat-Operator"><a href="#Splat-Operator" class="headerlink" title="Splat Operator"></a>Splat Operator</h3><hr>
<h4 id="Inside-Function-Call-1"><a href="#Inside-Function-Call-1" class="headerlink" title="Inside Function Call"></a>Inside Function Call</h4><p><strong>Splat expands a collection into positional arguments, while splatty-splat expands a dictionary into keyword arguments.</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">args   = (<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">kwargs = &#123;<span class="string">'x'</span>: <span class="number">3</span>, <span class="string">'y'</span>: <span class="number">4</span>, <span class="string">'z'</span>: <span class="number">5</span>&#125;</span><br><span class="line">func(*args, **kwargs)</span><br></pre></td></tr></table></figure></p>
<h5 id="Is-the-same-as"><a href="#Is-the-same-as" class="headerlink" title="Is the same as:"></a>Is the same as:</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">func(<span class="number">1</span>, <span class="number">2</span>, x=<span class="number">3</span>, y=<span class="number">4</span>, z=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Inside-Function-Definition-1"><a href="#Inside-Function-Definition-1" class="headerlink" title="Inside Function Definition"></a>Inside Function Definition</h4><p><strong>Splat combines zero or more positional arguments into a tuple, while splatty-splat combines zero or more keyword arguments into a dictionary.</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(*a)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum(a)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>add(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure>
<h5 id="Legal-argument-combinations"><a href="#Legal-argument-combinations" class="headerlink" title="Legal argument combinations:"></a>Legal argument combinations:</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x, y, z)</span>:</span>                <span class="comment"># f(x=1, y=2, z=3) | f(1, y=2, z=3) | f(1, 2, z=3) | f(1, 2, 3)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(*, x, y, z)</span>:</span>             <span class="comment"># f(x=1, y=2, z=3)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x, *, y, z)</span>:</span>             <span class="comment"># f(x=1, y=2, z=3) | f(1, y=2, z=3)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x, y, *, z)</span>:</span>             <span class="comment"># f(x=1, y=2, z=3) | f(1, y=2, z=3) | f(1, 2, z=3)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(*args)</span>:</span>                  <span class="comment"># f(1, 2, 3)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x, *args)</span>:</span>               <span class="comment"># f(1, 2, 3)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(*args, z)</span>:</span>               <span class="comment"># f(1, 2, z=3)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x, *args, z)</span>:</span>            <span class="comment"># f(1, 2, z=3)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(**kwargs)</span>:</span>               <span class="comment"># f(x=1, y=2, z=3)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x, **kwargs)</span>:</span>            <span class="comment"># f(x=1, y=2, z=3) | f(1, y=2, z=3)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(*, x, **kwargs)</span>:</span>         <span class="comment"># f(x=1, y=2, z=3)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(*args, **kwargs)</span>:</span>        <span class="comment"># f(x=1, y=2, z=3) | f(1, y=2, z=3) | f(1, 2, z=3) | f(1, 2, 3)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x, *args, **kwargs)</span>:</span>     <span class="comment"># f(x=1, y=2, z=3) | f(1, y=2, z=3) | f(1, 2, z=3) | f(1, 2, 3)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(*args, y, **kwargs)</span>:</span>     <span class="comment"># f(x=1, y=2, z=3) | f(1, y=2, z=3)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x, *args, z, **kwargs)</span>:</span>  <span class="comment"># f(x=1, y=2, z=3) | f(1, y=2, z=3) | f(1, 2, z=3)</span></span><br></pre></td></tr></table></figure>
<h4 id="Other-Uses"><a href="#Other-Uses" class="headerlink" title="Other Uses"></a>Other Uses</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;list&gt;  = [*&lt;collection&gt; [, ...]]</span><br><span class="line">&lt;set&gt;   = &#123;*&lt;collection&gt; [, ...]&#125;</span><br><span class="line">&lt;tuple&gt; = (*&lt;collection&gt;, [...])</span><br><span class="line">&lt;dict&gt;  = &#123;**&lt;dict&gt; [, ...]&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">head, *body, tail = &lt;collection&gt;</span><br></pre></td></tr></table></figure>
<h3 id="Inline"><a href="#Inline" class="headerlink" title="Inline"></a>Inline</h3><hr>
<h4 id="Lambda"><a href="#Lambda" class="headerlink" title="Lambda"></a>Lambda</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;function&gt; = <span class="keyword">lambda</span>: &lt;return_value&gt;</span><br><span class="line">&lt;function&gt; = <span class="keyword">lambda</span> &lt;argument_1&gt;, &lt;argument_2&gt;: &lt;return_value&gt;</span><br></pre></td></tr></table></figure>
<h4 id="Comprehension"><a href="#Comprehension" class="headerlink" title="Comprehension"></a>Comprehension</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;list&gt; = [i+<span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)]                   <span class="comment"># [1, 2, ..., 10]</span></span><br><span class="line">&lt;set&gt;  = &#123;i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>) <span class="keyword">if</span> i &gt; <span class="number">5</span>&#125;            <span class="comment"># &#123;6, 7, 8, 9&#125;</span></span><br><span class="line">&lt;iter&gt; = (i+<span class="number">5</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))                   <span class="comment"># (5, 6, ..., 14)</span></span><br><span class="line">&lt;dict&gt; = &#123;i: i*<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)&#125;                <span class="comment"># &#123;0: 0, 1: 2, ..., 9: 18&#125;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out = [i+j <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>) <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br></pre></td></tr></table></figure>
<h4 id="Is-the-same-as-1"><a href="#Is-the-same-as-1" class="headerlink" title="Is the same as:"></a>Is the same as:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        out.append(i+j)</span><br></pre></td></tr></table></figure>
<h3 id="Map-Filter-Reduce"><a href="#Map-Filter-Reduce" class="headerlink" title="Map, Filter, Reduce"></a>Map, Filter, Reduce</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line">&lt;iter&gt; = map(<span class="keyword">lambda</span> x: x + <span class="number">1</span>, range(<span class="number">10</span>))            <span class="comment"># (1, 2, ..., 10)</span></span><br><span class="line">&lt;iter&gt; = filter(<span class="keyword">lambda</span> x: x &gt; <span class="number">5</span>, range(<span class="number">10</span>))         <span class="comment"># (6, 7, 8, 9)</span></span><br><span class="line">&lt;int&gt;  = reduce(<span class="keyword">lambda</span> out, x: out + x, range(<span class="number">10</span>))  <span class="comment"># 45</span></span><br></pre></td></tr></table></figure>
<h4 id="Any-All"><a href="#Any-All" class="headerlink" title="Any, All"></a>Any, All</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;bool&gt; = any(&lt;collection&gt;)                          <span class="comment"># False if empty.</span></span><br><span class="line">&lt;bool&gt; = all(el[<span class="number">1</span>] <span class="keyword">for</span> el <span class="keyword">in</span> &lt;collection&gt;)          <span class="comment"># True if empty.</span></span><br></pre></td></tr></table></figure>
<h4 id="If-Else"><a href="#If-Else" class="headerlink" title="If - Else"></a>If - Else</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;expression_if_true&gt; <span class="keyword">if</span> &lt;condition&gt; <span class="keyword">else</span> &lt;expression_if_false&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>[a <span class="keyword">if</span> a <span class="keyword">else</span> <span class="string">'zero'</span> <span class="keyword">for</span> a <span class="keyword">in</span> (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>)]</span><br><span class="line">[<span class="string">'zero'</span>, <span class="number">1</span>, <span class="string">'zero'</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<h4 id="Namedtuple-Enum-Dataclass"><a href="#Namedtuple-Enum-Dataclass" class="headerlink" title="Namedtuple, Enum, Dataclass"></a>Namedtuple, Enum, Dataclass</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line">Point     = namedtuple(<span class="string">'Point'</span>, <span class="string">'x y'</span>)</span><br><span class="line">point     = Point(<span class="number">0</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> enum <span class="keyword">import</span> Enum</span><br><span class="line">Direction = Enum(<span class="string">'Direction'</span>, <span class="string">'n e s w'</span>)</span><br><span class="line">direction = Direction.n</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> make_dataclass</span><br><span class="line">Creature  = make_dataclass(<span class="string">'Creature'</span>, [<span class="string">'location'</span>, <span class="string">'direction'</span>])</span><br><span class="line">creature  = Creature(Point(<span class="number">0</span>, <span class="number">0</span>), Direction.n)</span><br></pre></td></tr></table></figure>
<h3 id="Closure"><a href="#Closure" class="headerlink" title="Closure"></a>Closure</h3><hr>
<p><strong>We have a closure in Python when:</strong></p>
<ul>
<li><strong>A nested function references a value of its enclosing function and then</strong></li>
<li><strong>the enclosing function returns the nested function.</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_multiplier</span><span class="params">(a)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">out</span><span class="params">(b)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> a * b</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>multiply_by_3 = get_multiplier(<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>multiply_by_3(<span class="number">10</span>)</span><br><span class="line"><span class="number">30</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>If multiple nested functions within enclosing function reference the same value, that value gets shared.</strong></li>
<li><strong>To dynamically access function’s first free variable use <code>&#39;&lt;function&gt;.__closure__[0].cell_contents&#39;</code>.</strong></li>
</ul>
<h4 id="Partial"><a href="#Partial" class="headerlink" title="Partial"></a>Partial</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line">&lt;function&gt; = partial(&lt;function&gt; [, &lt;arg_1&gt;, &lt;arg_2&gt;, ...])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> operator <span class="keyword">as</span> op</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>multiply_by_3 = partial(op.mul, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>multiply_by_3(<span class="number">10</span>)</span><br><span class="line"><span class="number">30</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Partial is also useful in cases when a function needs to be passed as an argument, because it enables us to set its arguments beforehand.</strong></li>
<li><strong>A few examples being <code>&#39;defaultdict(&lt;function&gt;)&#39;</code>, <code>&#39;iter(&lt;function&gt;, to_exclusive)&#39;</code> and dataclass’s <code>&#39;field(default_factory=&lt;function&gt;)&#39;</code>.</strong></li>
</ul>
<h4 id="Nonlocal"><a href="#Nonlocal" class="headerlink" title="Nonlocal"></a>Nonlocal</h4><p><strong>If variable is being assigned to anywhere in the scope, it is regarded as a local variable, unless it is declared as a ‘global’ or a ‘nonlocal’.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_counter</span><span class="params">()</span>:</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">out</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">nonlocal</span> i</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>counter = get_counter()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>counter(), counter(), counter()</span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Decorator"><a href="#Decorator" class="headerlink" title="Decorator"></a>Decorator</h3><hr>
<p><strong>A decorator takes a function, adds some functionality and returns it.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@decorator_name</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function_that_gets_passed_to_decorator</span><span class="params">()</span>:</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<h4 id="Debugger-Example"><a href="#Debugger-Example" class="headerlink" title="Debugger Example"></a>Debugger Example</h4><p><strong>Decorator that prints function’s name every time it gets called.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">debug</span><span class="params">(func)</span>:</span></span><br><span class="line"><span class="meta">    @wraps(func)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">out</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        print(func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="meta">@debug</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x + y</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Wraps is a helper decorator that copies metadata of function add() to function out().</strong></li>
<li><strong>Without it <code>&#39;add.__name__&#39;</code> would return <code>&#39;out&#39;</code>.</strong></li>
</ul>
<h4 id="LRU-Cache"><a href="#LRU-Cache" class="headerlink" title="LRU Cache"></a>LRU Cache</h4><p><strong>Decorator that caches function’s return values. All function’s arguments must be hashable.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"></span><br><span class="line"><span class="meta">@lru_cache(maxsize=None)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> n <span class="keyword">if</span> n &lt; <span class="number">2</span> <span class="keyword">else</span> fib(n<span class="number">-2</span>) + fib(n<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Recursion depth is limited to 1000 by default. To increase it use <code>&#39;sys.setrecursionlimit(&lt;depth&gt;)&#39;</code>.</strong></li>
</ul>
<h4 id="Parametrized-Decorator"><a href="#Parametrized-Decorator" class="headerlink" title="Parametrized Decorator"></a>Parametrized Decorator</h4><p><strong>A decorator that accepts arguments and returns a normal decorator that accepts a function.</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">debug</span><span class="params">(print_result=False)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decorator</span><span class="params">(func)</span>:</span></span><br><span class="line"><span class="meta">        @wraps(func)</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">out</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">            result = func(*args, **kwargs)</span><br><span class="line">            print(func.__name__, result <span class="keyword">if</span> print_result <span class="keyword">else</span> <span class="string">''</span>)</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    <span class="keyword">return</span> decorator</span><br><span class="line"></span><br><span class="line"><span class="meta">@debug(print_result=True)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x + y</span><br></pre></td></tr></table></figure></p>
<h3 id="Class"><a href="#Class" class="headerlink" title="Class"></a>Class</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;<span class="title">name</span>&gt;:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        self.a = a</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        class_name = self.__class__.__name__</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f'<span class="subst">&#123;class_name&#125;</span>(<span class="subst">&#123;self.a!r&#125;</span>)'</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> str(self.a)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_class_name</span><span class="params">(cls)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls.__name__</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Return value of repr() should be unambiguous and of str() readable.</strong></li>
<li><strong>If only repr() is defined, it will be also used for str().</strong></li>
</ul>
<h4 id="Str-use-cases"><a href="#Str-use-cases" class="headerlink" title="Str() use cases:"></a>Str() use cases:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(&lt;el&gt;)</span><br><span class="line">print(<span class="string">f'<span class="subst">&#123;&lt;el&gt;&#125;</span>'</span>)</span><br><span class="line"><span class="keyword">raise</span> Exception(&lt;el&gt;)</span><br><span class="line">logging.debug(&lt;el&gt;)</span><br><span class="line">csv.writer(&lt;file&gt;).writerow([&lt;el&gt;])</span><br></pre></td></tr></table></figure>
<h4 id="Repr-use-cases"><a href="#Repr-use-cases" class="headerlink" title="Repr() use cases:"></a>Repr() use cases:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print([&lt;el&gt;])</span><br><span class="line">print(<span class="string">f'<span class="subst">&#123;&lt;el&gt;!r&#125;</span>'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&lt;el&gt;</span><br><span class="line">loguru.logger.exception()</span><br><span class="line">Z = dataclasses.make_dataclass(<span class="string">'Z'</span>, [<span class="string">'a'</span>]); print(Z(&lt;el&gt;))</span><br></pre></td></tr></table></figure>
<h4 id="Constructor-Overloading"><a href="#Constructor-Overloading" class="headerlink" title="Constructor Overloading"></a>Constructor Overloading</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;<span class="title">name</span>&gt;:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a=None)</span>:</span></span><br><span class="line">        self.a = a</span><br></pre></td></tr></table></figure>
<h4 id="Inheritance"><a href="#Inheritance" class="headerlink" title="Inheritance"></a>Inheritance</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age  = age</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Employee</span><span class="params">(Person)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age, staff_num)</span>:</span></span><br><span class="line">        super().__init__(name, age)</span><br><span class="line">        self.staff_num = staff_num</span><br></pre></td></tr></table></figure>
<h4 id="Multiple-Inheritance"><a href="#Multiple-Inheritance" class="headerlink" title="Multiple Inheritance"></a>Multiple Inheritance</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span> <span class="keyword">pass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span>:</span> <span class="keyword">pass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span><span class="params">(A, B)</span>:</span> <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p><strong>MRO determines the order in which parent classes are traversed when searching for a method:</strong><br><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>C.mro()</span><br><span class="line">[&lt;class 'C'&gt;, &lt;class 'A'&gt;, &lt;class 'B'&gt;, &lt;class 'object'&gt;]</span><br></pre></td></tr></table></figure></p>
<h4 id="Property"><a href="#Property" class="headerlink" title="Property"></a>Property</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>:</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">a</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._a</span><br><span class="line"></span><br><span class="line"><span class="meta">    @a.setter</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">a</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        self._a = value</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>el = MyClass()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>el.a = <span class="number">123</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>el.a</span><br><span class="line"><span class="number">123</span></span><br></pre></td></tr></table></figure>
<h4 id="Dataclass"><a href="#Dataclass" class="headerlink" title="Dataclass"></a>Dataclass</h4><p><strong>Decorator that automatically generates init(), repr() and eq() special methods.</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass, field</span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass(order=False, frozen=False)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> &lt;<span class="title">class_name</span>&gt;:</span></span><br><span class="line">    &lt;attr_name_1&gt;: &lt;type&gt;</span><br><span class="line">    &lt;attr_name_2&gt;: &lt;type&gt; = &lt;default_value&gt;</span><br><span class="line">    &lt;attr_name_3&gt;: list/dict/set = field(default_factory=list/dict/set)</span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong>An object can be made sortable with <code>&#39;order=True&#39;</code> or immutable with <code>&#39;frozen=True&#39;</code>.</strong></li>
<li><strong>Function field() is needed because <code>&#39;&lt;attr_name&gt;: list = []&#39;</code> would make a list that is shared among all instances.</strong></li>
<li><strong>Default_factory can be any callable.</strong></li>
</ul>
<h4 id="Slots"><a href="#Slots" class="headerlink" title="Slots"></a>Slots</h4><p><strong>Mechanism that restricts objects to attributes listed in ‘slots’ and significantly reduces their memory footprint.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClassWithSlots</span>:</span></span><br><span class="line">    __slots__ = [<span class="string">'a'</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.a = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h4 id="Copy"><a href="#Copy" class="headerlink" title="Copy"></a>Copy</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> copy, deepcopy</span><br><span class="line">&lt;object&gt; = copy(&lt;object&gt;)</span><br><span class="line">&lt;object&gt; = deepcopy(&lt;object&gt;)</span><br></pre></td></tr></table></figure>
<h3 id="Duck-Types"><a href="#Duck-Types" class="headerlink" title="Duck Types"></a>Duck Types</h3><hr>
<p><strong>A duck type is an implicit type that prescribes a set of special methods. Any object that has those methods defined is considered a member of that duck type.</strong></p>
<h4 id="Comparable"><a href="#Comparable" class="headerlink" title="Comparable"></a>Comparable</h4><ul>
<li><strong>If eq() method is not overridden, it returns <code>&#39;id(self) == id(other)&#39;</code>, which is the same as <code>&#39;self is other&#39;</code>.</strong></li>
<li><strong>That means all objects compare not equal by default.</strong></li>
<li><strong>Only left side object has eq() method called, unless it returns ‘NotImplemented’, in which case the right object is consulted.</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyComparable</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        self.a = a</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__eq__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(other, type(self)):</span><br><span class="line">            <span class="keyword">return</span> self.a == other.a</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">NotImplemented</span></span><br></pre></td></tr></table></figure>
<h4 id="Hashable"><a href="#Hashable" class="headerlink" title="Hashable"></a>Hashable</h4><ul>
<li><strong>Hashable object needs both hash() and eq() methods and its hash value should never change.</strong></li>
<li><strong>Hashable objects that compare equal must have the same hash value, meaning default hash() that returns <code>&#39;id(self)&#39;</code> will not do.</strong></li>
<li><strong>That is why Python automatically makes classes unhashable if you only implement eq().</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyHashable</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        self._a = copy.deepcopy(a)</span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">a</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._a</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__eq__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(other, type(self)):</span><br><span class="line">            <span class="keyword">return</span> self.a == other.a</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">NotImplemented</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__hash__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> hash(self.a)</span><br></pre></td></tr></table></figure>
<h4 id="Sortable"><a href="#Sortable" class="headerlink" title="Sortable"></a>Sortable</h4><ul>
<li><strong>With ‘total_ordering’ decorator you only need to provide eq() and one of lt(), gt(), le() or ge() special methods.</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> total_ordering</span><br><span class="line"></span><br><span class="line"><span class="meta">@total_ordering</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySortable</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        self.a = a</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__eq__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(other, type(self)):</span><br><span class="line">            <span class="keyword">return</span> self.a == other.a</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">NotImplemented</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(other, type(self)):</span><br><span class="line">            <span class="keyword">return</span> self.a &lt; other.a</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">NotImplemented</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="Iterator-1"><a href="#Iterator-1" class="headerlink" title="Iterator"></a>Iterator</h4><ul>
<li><strong>Next() should return next item or raise ‘StopIteration’.</strong></li>
<li><strong>Iter() should return ‘self’.</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Counter</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.i = <span class="number">0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self.i</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>counter = Counter()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(counter), next(counter), next(counter)</span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Callable"><a href="#Callable" class="headerlink" title="Callable"></a>Callable</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Counter</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.i = <span class="number">0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self.i</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>counter = Counter()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>counter(), counter(), counter()</span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Context-Manager"><a href="#Context-Manager" class="headerlink" title="Context Manager"></a>Context Manager</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyOpen</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, filename)</span>:</span></span><br><span class="line">        self.filename = filename</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = open(self.filename)</span><br><span class="line">        <span class="keyword">return</span> self.file</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        self.file.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> open(<span class="string">'test.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> file:</span><br><span class="line"><span class="meta">... </span>    file.write(<span class="string">'Hello World!'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> MyOpen(<span class="string">'test.txt'</span>) <span class="keyword">as</span> file:</span><br><span class="line"><span class="meta">... </span>    print(file.read())</span><br><span class="line">Hello World!</span><br></pre></td></tr></table></figure>
<h4 id="List-of-existing-context-managers"><a href="#List-of-existing-context-managers" class="headerlink" title="List of existing context managers:"></a>List of existing context managers:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'&lt;path&gt;'</span>, ...) <span class="keyword">as</span> file: ...</span><br><span class="line"><span class="keyword">with</span> wave.open(<span class="string">'&lt;path&gt;'</span>, ...) <span class="keyword">as</span> wave_file: ...</span><br><span class="line"><span class="keyword">with</span> memoryview(&lt;bytes/bytearray/array&gt;) <span class="keyword">as</span> view: ...</span><br><span class="line">db = sqlite3.connect(<span class="string">'&lt;path&gt;'</span>); <span class="keyword">with</span> db: db.execute(<span class="string">'&lt;insert_query&gt;'</span>)</span><br><span class="line">lock = threading.RLock(); <span class="keyword">with</span> lock: ...</span><br></pre></td></tr></table></figure>
<h3 id="Iterable-Duck-Types"><a href="#Iterable-Duck-Types" class="headerlink" title="Iterable Duck Types"></a>Iterable Duck Types</h3><hr>
<h4 id="Iterable"><a href="#Iterable" class="headerlink" title="Iterable"></a>Iterable</h4><ul>
<li><strong>Only required method is iter(). It should return an iterator of object’s items.</strong></li>
<li><strong>Contains() automatically works on any object that has iter() defined.</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyIterable</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        self.a = a</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> el <span class="keyword">in</span> self.a:</span><br><span class="line">            <span class="keyword">yield</span> el</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = MyIterable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>iter(a)</span><br><span class="line">&lt;generator object MyIterable.__iter__ at <span class="number">0x1026c18b8</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">1</span> <span class="keyword">in</span> a</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h4 id="Collection"><a href="#Collection" class="headerlink" title="Collection"></a>Collection</h4><ul>
<li><strong>Only required methods are iter() and len().</strong></li>
<li><strong>This cheatsheet actually means <code>&#39;&lt;iterable&gt;&#39;</code> when it uses <code>&#39;&lt;collection&gt;&#39;</code>.</strong></li>
<li><strong>I chose not to use the name ‘iterable’ because it sounds scarier and more vague than ‘collection’.</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCollection</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        self.a = a</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> el <span class="keyword">in</span> self.a:</span><br><span class="line">            <span class="keyword">yield</span> el</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__contains__</span><span class="params">(self, el)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> el <span class="keyword">in</span> self.a</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.a)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="Sequence"><a href="#Sequence" class="headerlink" title="Sequence"></a>Sequence</h4><ul>
<li><strong>Only required methods are len() and getitem(), that should return an item at index or raise ‘IndexError’.</strong></li>
<li><strong>Iter() and contains() automatically work on any object that has getitem() defined.</strong></li>
<li><strong>Reversed() automatically works on any object that has getitem() and len() defined.</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySequence</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        self.a = a</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> el <span class="keyword">in</span> self.a:</span><br><span class="line">            <span class="keyword">yield</span> el</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__contains__</span><span class="params">(self, el)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> el <span class="keyword">in</span> self.a</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.a)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, i)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.a[i]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__reversed__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> reversed(self.a)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="Collections-abc-Sequence"><a href="#Collections-abc-Sequence" class="headerlink" title="Collections.abc.Sequence"></a>Collections.abc.Sequence</h4><ul>
<li><strong>It’s a richer interface than the basic sequence.</strong></li>
<li><strong>Extending it generates iter(), contains(), reversed(), index(), and count().</strong></li>
<li><strong>Unlike <code>&#39;abc.Iterable&#39;</code> and <code>&#39;abc.Collection&#39;</code>, it is not a duck type. That is why <code>&#39;issubclass(MySequence, collections.abc.Sequence)&#39;</code> would return ‘False’ even if it had all the methods defined.</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAbcSequence</span><span class="params">(collections.abc.Sequence)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        self.a = a</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.a)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, i)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.a[i]</span><br></pre></td></tr></table></figure>
<h4 id="Table-of-required-and-available-special-methods"><a href="#Table-of-required-and-available-special-methods" class="headerlink" title="Table of required and available special methods:"></a>Table of required and available special methods:</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+------------+----------+------------+----------+--------------+</span><br><span class="line">|            | Iterable | Collection | Sequence | abc.Sequence |</span><br><span class="line">+------------+----------+------------+----------+--------------+</span><br><span class="line">| iter()     |   REQ    |    REQ     |   yes    |     yes      |</span><br><span class="line">| contains() |   yes    |    yes     |   yes    |     yes      |</span><br><span class="line">| len()      |          |    REQ     |   REQ    |     REQ      |</span><br><span class="line">| getitem()  |          |            |   REQ    |     REQ      |</span><br><span class="line">| reversed() |          |            |   yes    |     yes      |</span><br><span class="line">| index()    |          |            |          |     yes      |</span><br><span class="line">| count()    |          |            |          |     yes      |</span><br><span class="line">+------------+----------+------------+----------+--------------+</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Other useful ABCs that automatically generate missing methods are: MutableSequence, Set, MutableSet, Mapping and MutableMapping.</strong></li>
</ul>
<h3 id="Enum"><a href="#Enum" class="headerlink" title="Enum"></a>Enum</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> enum <span class="keyword">import</span> Enum, auto</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> &lt;<span class="title">enum_name</span>&gt;<span class="params">(Enum)</span>:</span></span><br><span class="line">    &lt;member_name_1&gt; = &lt;value_1&gt;</span><br><span class="line">    &lt;member_name_2&gt; = &lt;value_2_a&gt;, &lt;value_2_b&gt;</span><br><span class="line">    &lt;member_name_3&gt; = auto()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_member_names</span><span class="params">(cls)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [a.name <span class="keyword">for</span> a <span class="keyword">in</span> cls.__members__.values()]</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>If there are no numeric values before auto(), it returns 1.</strong></li>
<li><strong>Otherwise it returns an increment of last numeric value.</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;member&gt; = &lt;enum&gt;.&lt;member_name&gt;</span><br><span class="line">&lt;member&gt; = &lt;enum&gt;[<span class="string">'&lt;member_name&gt;'</span>]</span><br><span class="line">&lt;member&gt; = &lt;enum&gt;(&lt;value&gt;)</span><br><span class="line">name     = &lt;member&gt;.name</span><br><span class="line">value    = &lt;member&gt;.value</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list_of_members = list(&lt;enum&gt;)</span><br><span class="line">member_names    = [a.name <span class="keyword">for</span> a <span class="keyword">in</span> &lt;enum&gt;]</span><br><span class="line">member_values   = [a.value <span class="keyword">for</span> a <span class="keyword">in</span> &lt;enum&gt;]</span><br><span class="line">random_member   = random.choice(list(&lt;enum&gt;))</span><br></pre></td></tr></table></figure>
<h3 id="Inline-1"><a href="#Inline-1" class="headerlink" title="Inline"></a>Inline</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Cutlery = Enum(<span class="string">'Cutlery'</span>, [<span class="string">'fork'</span>, <span class="string">'knife'</span>, <span class="string">'spoon'</span>])</span><br><span class="line">Cutlery = Enum(<span class="string">'Cutlery'</span>, <span class="string">'fork knife spoon'</span>)</span><br><span class="line">Cutlery = Enum(<span class="string">'Cutlery'</span>, &#123;<span class="string">'fork'</span>: <span class="number">1</span>, <span class="string">'knife'</span>: <span class="number">2</span>, <span class="string">'spoon'</span>: <span class="number">3</span>&#125;)</span><br></pre></td></tr></table></figure>
<h4 id="Functions-can-not-be-values-so-they-must-be-wrapped"><a href="#Functions-can-not-be-values-so-they-must-be-wrapped" class="headerlink" title="Functions can not be values, so they must be wrapped:"></a>Functions can not be values, so they must be wrapped:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line">LogicOp = Enum(<span class="string">'LogicOp'</span>, &#123;<span class="string">'AND'</span>: partial(<span class="keyword">lambda</span> l, r: l <span class="keyword">and</span> r),</span><br><span class="line">                           <span class="string">'OR'</span> : partial(<span class="keyword">lambda</span> l, r: l <span class="keyword">or</span> r)&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Another solution in this particular case, is to use <code>&#39;and_&#39;</code> and <code>&#39;or_&#39;</code> functions from module <a href="#operator">Operator</a>.</strong></li>
</ul>
<h3 id="Exceptions"><a href="#Exceptions" class="headerlink" title="Exceptions"></a>Exceptions</h3><hr>
<h4 id="Basic-Example"><a href="#Basic-Example" class="headerlink" title="Basic Example"></a>Basic Example</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    &lt;code&gt;</span><br><span class="line"><span class="keyword">except</span> &lt;exception&gt;:</span><br><span class="line">    &lt;code&gt;</span><br></pre></td></tr></table></figure>
<h4 id="Complex-Example"><a href="#Complex-Example" class="headerlink" title="Complex Example"></a>Complex Example</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    &lt;code_1&gt;</span><br><span class="line"><span class="keyword">except</span> &lt;exception_a&gt;:</span><br><span class="line">    &lt;code_2_a&gt;</span><br><span class="line"><span class="keyword">except</span> &lt;exception_b&gt;:</span><br><span class="line">    &lt;code_2_b&gt;</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    &lt;code_2_c&gt;</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    &lt;code_3&gt;</span><br></pre></td></tr></table></figure>
<h4 id="Catching-Exceptions"><a href="#Catching-Exceptions" class="headerlink" title="Catching Exceptions"></a>Catching Exceptions</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">except</span> &lt;exception&gt;:</span><br><span class="line"><span class="keyword">except</span> &lt;exception&gt; <span class="keyword">as</span> &lt;name&gt;:</span><br><span class="line"><span class="keyword">except</span> (&lt;exception_1&gt;, &lt;exception_2&gt;, ...):</span><br><span class="line"><span class="keyword">except</span> (&lt;exception_1&gt;, &lt;exception_2&gt;, ...) <span class="keyword">as</span> &lt;name&gt;:</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Also catches subclasses of the exception.</strong></li>
</ul>
<h4 id="Raising-Exceptions"><a href="#Raising-Exceptions" class="headerlink" title="Raising Exceptions"></a>Raising Exceptions</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">raise</span> &lt;exception&gt;</span><br><span class="line"><span class="keyword">raise</span> &lt;exception&gt;()</span><br><span class="line"><span class="keyword">raise</span> &lt;exception&gt;(&lt;el&gt;)</span><br><span class="line"><span class="keyword">raise</span> &lt;exception&gt;(&lt;el_1&gt;, &lt;el_2&gt;, ...)</span><br></pre></td></tr></table></figure>
<h4 id="Useful-built-in-exceptions"><a href="#Useful-built-in-exceptions" class="headerlink" title="Useful built-in exceptions:"></a>Useful built-in exceptions:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">raise</span> ValueError(<span class="string">'Argument is of right type but inappropriate value!'</span>)</span><br><span class="line"><span class="keyword">raise</span> TypeError(<span class="string">'Argument is of wrong type!'</span>)</span><br><span class="line"><span class="keyword">raise</span> RuntimeError(<span class="string">'None of above!'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Re-raising-caught-exception"><a href="#Re-raising-caught-exception" class="headerlink" title="Re-raising caught exception:"></a>Re-raising caught exception:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">except</span> &lt;exception&gt;:</span><br><span class="line">    &lt;code&gt;</span><br><span class="line">    <span class="keyword">raise</span></span><br></pre></td></tr></table></figure>
<h4 id="Common-Built-in-Exceptions"><a href="#Common-Built-in-Exceptions" class="headerlink" title="Common Built-in Exceptions"></a>Common Built-in Exceptions</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">BaseException</span><br><span class="line"> +-- SystemExit                   # Raised by the sys.exit() function.</span><br><span class="line"> +-- KeyboardInterrupt            # Raised when the user hits the interrupt key.</span><br><span class="line"> +-- Exception                    # User-defined exceptions should be derived from this class.</span><br><span class="line">      +-- StopIteration           # Raised by next() when run on an empty iterator.</span><br><span class="line">      +-- ArithmeticError         # Base class for arithmetic errors.</span><br><span class="line">      |    +-- ZeroDivisionError  # Raised when dividing by zero.</span><br><span class="line">      +-- AttributeError          # Raised when an attribute is missing.</span><br><span class="line">      +-- EOFError                # Raised by input() when it hits end-of-file condition.</span><br><span class="line">      +-- LookupError             # Raised when a look-up on sequence or dict fails.</span><br><span class="line">      |    +-- IndexError         # Raised when a sequence index is out of range.</span><br><span class="line">      |    +-- KeyError           # Raised when a dictionary key is not found.</span><br><span class="line">      +-- NameError               # Raised when a variable name is not found.</span><br><span class="line">      +-- OSError                 # Failures such as “file not found” or “disk full”.</span><br><span class="line">      |    +-- FileNotFoundError  # When a file or directory is requested but doesn&#39;t exist.</span><br><span class="line">      +-- RuntimeError            # Raised by errors that don&#39;t fall in other categories.</span><br><span class="line">      |    +-- RecursionError     # Raised when the the maximum recursion depth is exceeded.</span><br><span class="line">      +-- TypeError               # Raised when an argument is of wrong type.</span><br><span class="line">      +-- ValueError              # When an argument is of right type but inappropriate value.</span><br><span class="line">           +-- UnicodeError       # Raised when encoding&#x2F;decoding strings from&#x2F;to bytes fails.</span><br></pre></td></tr></table></figure>
<h4 id="User-defined-Exceptions"><a href="#User-defined-Exceptions" class="headerlink" title="User-defined Exceptions"></a>User-defined Exceptions</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyError</span><span class="params">(Exception)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyInputError</span><span class="params">(MyError)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h3 id="Print"><a href="#Print" class="headerlink" title="Print"></a>Print</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(&lt;el_1&gt;, ..., sep=<span class="string">' '</span>, end=<span class="string">'\n'</span>, file=sys.stdout, flush=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Use <code>&#39;file=sys.stderr&#39;</code> for errors.</strong></li>
<li><strong>Use <code>&#39;flush=True&#39;</code> to forcibly flush the stream.</strong></li>
</ul>
<h4 id="Pretty-Print"><a href="#Pretty-Print" class="headerlink" title="Pretty Print"></a>Pretty Print</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pprint(dir())</span><br><span class="line">[<span class="string">'__annotations__'</span>,</span><br><span class="line"> <span class="string">'__builtins__'</span>,</span><br><span class="line"> <span class="string">'__doc__'</span>, ...]</span><br></pre></td></tr></table></figure>
<h3 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h3><hr>
<ul>
<li><strong>Reads a line from user input or pipe if present.</strong></li>
<li><strong>Trailing newline gets stripped.</strong></li>
<li><strong>Prompt string is printed to the standard output before reading input.</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;str&gt; = input(prompt=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Prints-lines-until-EOF"><a href="#Prints-lines-until-EOF" class="headerlink" title="Prints lines until EOF:"></a>Prints lines until EOF:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        print(input())</span><br><span class="line">    <span class="keyword">except</span> EOFError:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h3 id="Command-Line-Arguments"><a href="#Command-Line-Arguments" class="headerlink" title="Command Line Arguments"></a>Command Line Arguments</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">script_name = sys.argv[<span class="number">0</span>]</span><br><span class="line">arguments   = sys.argv[<span class="number">1</span>:]</span><br></pre></td></tr></table></figure>
<h4 id="Argparse"><a href="#Argparse" class="headerlink" title="Argparse"></a>Argparse</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser, FileType</span><br><span class="line">p = ArgumentParser(description=&lt;str&gt;)</span><br><span class="line">p.add_argument(<span class="string">'-&lt;short_name&gt;'</span>, <span class="string">'--&lt;name&gt;'</span>, action=<span class="string">'store_true'</span>)  <span class="comment"># Flag</span></span><br><span class="line">p.add_argument(<span class="string">'-&lt;short_name&gt;'</span>, <span class="string">'--&lt;name&gt;'</span>, type=&lt;type&gt;)          <span class="comment"># Option</span></span><br><span class="line">p.add_argument(<span class="string">'&lt;name&gt;'</span>, type=&lt;type&gt;, nargs=<span class="number">1</span>)                    <span class="comment"># Argument</span></span><br><span class="line">p.add_argument(<span class="string">'&lt;name&gt;'</span>, type=&lt;type&gt;, nargs=<span class="string">'+'</span>)                  <span class="comment"># Arguments</span></span><br><span class="line">args  = p.parse_args()</span><br><span class="line">value = args.&lt;name&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Use <code>&#39;help=&lt;str&gt;&#39;</code> for argument description.</strong></li>
<li><strong>Use <code>&#39;type=FileType(&lt;mode&gt;)&#39;</code> for files.</strong></li>
</ul>
<h3 id="Open"><a href="#Open" class="headerlink" title="Open"></a>Open</h3><hr>
<p><strong>Opens a file and returns a corresponding file object.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;file&gt; = open(<span class="string">'&lt;path&gt;'</span>, mode=<span class="string">'r'</span>, encoding=<span class="literal">None</span>, newline=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>&#39;encoding=None&#39;</code> means default encoding is used, which is platform dependent. Best practice is to use <code>&#39;encoding=&quot;utf-8&quot;&#39;</code> whenever possible.</strong></li>
<li><strong><code>&#39;newline=None&#39;</code> means all different end of line combinations are converted to ‘\n’ on read, while on write all ‘\n’ characters are converted to system’s default line separator.</strong></li>
<li><strong><code>&#39;newline=&quot;&quot;&#39;</code> means no conversions take place, but lines are still broken by readline() on either ‘\n’, ‘\r’ or ‘\r\n’.</strong></li>
</ul>
<h4 id="Modes"><a href="#Modes" class="headerlink" title="Modes"></a>Modes</h4><ul>
<li><strong><code>&#39;r&#39;</code>  - Read (default).</strong></li>
<li><strong><code>&#39;w&#39;</code>  - Write (truncate).</strong></li>
<li><strong><code>&#39;x&#39;</code>  - Write or fail if the file already exists.</strong></li>
<li><strong><code>&#39;a&#39;</code>  - Append.</strong></li>
<li><strong><code>&#39;w+&#39;</code> - Read and write (truncate).</strong></li>
<li><strong><code>&#39;r+&#39;</code> - Read and write from the start.</strong></li>
<li><strong><code>&#39;a+&#39;</code> - Read and write from the end.</strong></li>
<li><strong><code>&#39;t&#39;</code>  - Text mode (default).</strong></li>
<li><strong><code>&#39;b&#39;</code>  - Binary mode.</strong></li>
</ul>
<h4 id="Exceptions-1"><a href="#Exceptions-1" class="headerlink" title="Exceptions"></a>Exceptions</h4><ul>
<li><strong><code>&#39;FileNotFoundError&#39;</code> can be risen when reading with <code>&#39;r&#39;</code> or <code>&#39;r+&#39;</code>.</strong>  </li>
<li><strong><code>&#39;FileExistsError&#39;</code> can be risen when writing with <code>&#39;x&#39;</code>.</strong></li>
<li><strong><code>&#39;IsADirectoryError&#39;</code> and <code>&#39;PermissionError&#39;</code> can be risen by any.</strong></li>
<li><strong><code>&#39;OSError&#39;</code> is the parent class of all listed exceptions.</strong></li>
</ul>
<h4 id="File"><a href="#File" class="headerlink" title="File"></a>File</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;file&gt;.seek(<span class="number">0</span>)                      <span class="comment"># Moves to the start of the file.</span></span><br><span class="line">&lt;file&gt;.seek(offset)                 <span class="comment"># Moves 'offset' chars/bytes from the start.</span></span><br><span class="line">&lt;file&gt;.seek(<span class="number">0</span>, <span class="number">2</span>)                   <span class="comment"># Moves to the end of the file.</span></span><br><span class="line">&lt;bin_file&gt;.seek(±offset, &lt;anchor&gt;)  <span class="comment"># Anchor: 0 start, 1 current pos., 2 end.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;str/bytes&gt; = &lt;file&gt;.read(size=<span class="number">-1</span>)  <span class="comment"># Reads 'size' chars/bytes or until EOF.</span></span><br><span class="line">&lt;str/bytes&gt; = &lt;file&gt;.readline()     <span class="comment"># Returns a line or empty string on EOF.</span></span><br><span class="line">&lt;list&gt;      = &lt;file&gt;.readlines()    <span class="comment"># Returns a list of lines or empty list.</span></span><br><span class="line">&lt;str/bytes&gt; = next(&lt;file&gt;)          <span class="comment"># Returns a line using buffer. Do not mix.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;file&gt;.write(&lt;str/bytes&gt;)           <span class="comment"># Writes a string or bytes object.</span></span><br><span class="line">&lt;file&gt;.writelines(&lt;coll.&gt;)          <span class="comment"># Writes a coll. of strings or bytes objects.</span></span><br><span class="line">&lt;file&gt;.flush()                      <span class="comment"># Flushes write buffer.</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Methods do not add or strip trailing newlines, even writelines().</strong></li>
</ul>
<h4 id="Read-Text-from-File"><a href="#Read-Text-from-File" class="headerlink" title="Read Text from File"></a>Read Text from File</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_file</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">        <span class="keyword">return</span> file.readlines()</span><br></pre></td></tr></table></figure>
<h4 id="Write-Text-to-File"><a href="#Write-Text-to-File" class="headerlink" title="Write Text to File"></a>Write Text to File</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_file</span><span class="params">(filename, text)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">        file.write(text)</span><br></pre></td></tr></table></figure>
<h3 id="Path"><a href="#Path" class="headerlink" title="Path"></a>Path</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> path, listdir</span><br><span class="line"><span class="keyword">from</span> glob <span class="keyword">import</span> glob</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;bool&gt; = path.exists(<span class="string">'&lt;path&gt;'</span>)</span><br><span class="line">&lt;bool&gt; = path.isfile(<span class="string">'&lt;path&gt;'</span>)</span><br><span class="line">&lt;bool&gt; = path.isdir(<span class="string">'&lt;path&gt;'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;list&gt; = listdir(<span class="string">'&lt;path&gt;'</span>)         <span class="comment"># List of filenames located at 'path'. </span></span><br><span class="line">&lt;list&gt; = glob(<span class="string">'&lt;pattern&gt;'</span>)         <span class="comment"># Filenames matching the wildcard pattern.</span></span><br></pre></td></tr></table></figure>
<h4 id="Pathlib"><a href="#Pathlib" class="headerlink" title="Pathlib"></a>Pathlib</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cwd    = Path()</span><br><span class="line">&lt;Path&gt; = Path(<span class="string">'&lt;path&gt;'</span> [, <span class="string">'&lt;path&gt;'</span>, &lt;Path&gt;, ...])</span><br><span class="line">&lt;Path&gt; = &lt;Path&gt; / <span class="string">'&lt;dir&gt;'</span> / <span class="string">'&lt;file&gt;'</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;bool&gt; = &lt;Path&gt;.exists()</span><br><span class="line">&lt;bool&gt; = &lt;Path&gt;.is_file()</span><br><span class="line">&lt;bool&gt; = &lt;Path&gt;.is_dir()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;iter&gt; = &lt;Path&gt;.iterdir()          <span class="comment"># Iterator of filenames located at path.</span></span><br><span class="line">&lt;iter&gt; = &lt;Path&gt;.glob(<span class="string">'&lt;pattern&gt;'</span>)  <span class="comment"># Filenames matching the wildcard pattern.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;str&gt;  = str(&lt;Path&gt;)               <span class="comment"># Returns path as a string.</span></span><br><span class="line">&lt;tup.&gt; = &lt;Path&gt;.parts              <span class="comment"># Returns all components as strings.</span></span><br><span class="line">&lt;Path&gt; = &lt;Path&gt;.resolve()          <span class="comment"># Returns absolute Path without symlinks.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;str&gt;  = &lt;Path&gt;.name               <span class="comment"># Final component.</span></span><br><span class="line">&lt;str&gt;  = &lt;Path&gt;.stem               <span class="comment"># Final component without extension.</span></span><br><span class="line">&lt;str&gt;  = &lt;Path&gt;.suffix             <span class="comment"># Final component's extension.</span></span><br><span class="line">&lt;Path&gt; = &lt;Path&gt;.parent             <span class="comment"># Path without final component.</span></span><br></pre></td></tr></table></figure>
<h3 id="Command-Execution"><a href="#Command-Execution" class="headerlink" title="Command Execution"></a>Command Execution</h3><hr>
<h4 id="Files-and-Directories-Commands"><a href="#Files-and-Directories-Commands" class="headerlink" title="Files and Directories Commands"></a>Files and Directories Commands</h4><ul>
<li><strong>Paths can be either strings or Path objects.</strong></li>
<li><strong>All exceptions are either ‘OSError’ or its subclasses.</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.chdir(&lt;path&gt;)                  <span class="comment"># Changes the current working directory.</span></span><br><span class="line">&lt;str&gt; = os.getcwd()               <span class="comment"># Returns current working directory.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.remove(&lt;path&gt;)                 <span class="comment"># Deletes the file.</span></span><br><span class="line">os.rmdir(&lt;path&gt;)                  <span class="comment"># Deletes empty directory.</span></span><br><span class="line">shutil.rmtree(&lt;path&gt;)             <span class="comment"># Deletes an entire directory tree.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.rename(<span class="keyword">from</span>, to)               <span class="comment"># Renames the file or directory.</span></span><br><span class="line">os.replace(<span class="keyword">from</span>, to)              <span class="comment"># Same, but overwrites 'to' if it exists.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.mkdir(&lt;path&gt;, mode=<span class="number">0o777</span>)      <span class="comment"># Creates a directory.</span></span><br><span class="line">&lt;iter&gt; = os.scandir(path=<span class="string">'.'</span>)     <span class="comment"># Returns os.DirEntry objects located at path.</span></span><br></pre></td></tr></table></figure>
<h4 id="DirEntry"><a href="#DirEntry" class="headerlink" title="DirEntry:"></a>DirEntry:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;str&gt;  = &lt;DirEntry&gt;.name          <span class="comment"># Final component of the path.</span></span><br><span class="line">&lt;str&gt;  = &lt;DirEntry&gt;.path          <span class="comment"># Path with final component.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;bool&gt; = &lt;DirEntry&gt;.is_file()</span><br><span class="line">&lt;bool&gt; = &lt;DirEntry&gt;.is_dir()</span><br><span class="line">&lt;bool&gt; = &lt;DirEntry&gt;.is_symlink()</span><br><span class="line">&lt;Path&gt; = Path(&lt;DirEntry&gt;)</span><br></pre></td></tr></table></figure>
<h4 id="Shell-Commands"><a href="#Shell-Commands" class="headerlink" title="Shell Commands"></a>Shell Commands</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">&lt;str&gt; = os.popen(<span class="string">'&lt;shell_command&gt;'</span>).read()</span><br></pre></td></tr></table></figure>
<h4 id="Using-subprocess"><a href="#Using-subprocess" class="headerlink" title="Using subprocess:"></a>Using subprocess:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> subprocess, shlex</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = subprocess.run(shlex.split(<span class="string">'ls -a'</span>), stdout=subprocess.PIPE)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.stdout</span><br><span class="line"><span class="string">b'.\n..\nfile1.txt\nfile2.txt\n'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.returncode</span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure>
<h3 id="CSV"><a href="#CSV" class="headerlink" title="CSV"></a>CSV</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line">&lt;reader&gt; = csv.reader(&lt;file&gt;, dialect=<span class="string">'excel'</span>, delimiter=<span class="string">','</span>)</span><br><span class="line">&lt;list&gt;   = next(&lt;reader&gt;)  <span class="comment"># Returns a row as list of strings.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;writer&gt; = csv.writer(&lt;file&gt;, dialect=<span class="string">'excel'</span>, delimiter=<span class="string">','</span>)</span><br><span class="line">&lt;writer&gt;.writerow(&lt;collection&gt;)</span><br><span class="line">&lt;writer&gt;.writerows(&lt;coll_of_coll&gt;)</span><br></pre></td></tr></table></figure>
<h4 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h4><ul>
<li><strong><code>&#39;dialect&#39;</code> - Master parameter that sets the default values.</strong></li>
<li><strong><code>&#39;delimiter&#39;</code> - A one-character string used to separate fields.</strong></li>
<li><strong><code>&#39;quotechar&#39;</code> - Character for quoting fields that contain special characters.</strong></li>
<li><strong><code>&#39;doublequote&#39;</code> - Whether quotechars inside fields get doubled or escaped.</strong></li>
<li><strong><code>&#39;skipinitialspace&#39;</code> - Whether whitespace after delimiter gets stripped.</strong></li>
<li><strong><code>&#39;lineterminator&#39;</code> - How does writer terminate lines.</strong></li>
<li><strong><code>&#39;quoting&#39;</code> - Controls the amount of quoting: 0 - as necessary, 1 - all.</strong></li>
<li><strong><code>&#39;escapechar&#39;</code> - Character for escaping quotechar if doublequote is false.</strong></li>
</ul>
<h4 id="Dialects"><a href="#Dialects" class="headerlink" title="Dialects"></a>Dialects</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+------------------+-----------+-----------+--------------+</span><br><span class="line">|                  |   excel   | excel_tab | unix_dialect |</span><br><span class="line">+------------------+-----------+-----------+--------------+</span><br><span class="line">| delimiter        |      &#39;,&#39;  |     &#39;\t&#39;  |       &#39;,&#39;    |</span><br><span class="line">| quotechar        |      &#39;&quot;&#39;  |      &#39;&quot;&#39;  |       &#39;&quot;&#39;    |</span><br><span class="line">| doublequote      |     True  |     True  |      True    |</span><br><span class="line">| skipinitialspace |    False  |    False  |     False    |</span><br><span class="line">| lineterminator   |   &#39;\r\n&#39;  |   &#39;\r\n&#39;  |      &#39;\n&#39;    |</span><br><span class="line">| quoting          |        0  |        0  |         1    |</span><br><span class="line">| escapechar       |     None  |     None  |      None    |</span><br><span class="line">+------------------+-----------+-----------+--------------+</span><br></pre></td></tr></table></figure>
<h4 id="Read-Rows-from-CSV-File"><a href="#Read-Rows-from-CSV-File" class="headerlink" title="Read Rows from CSV File"></a>Read Rows from CSV File</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_csv_file</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, encoding=<span class="string">'utf-8'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> file:</span><br><span class="line">        <span class="keyword">return</span> csv.reader(file)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>If <code>&#39;newline=&quot;&quot;&#39;</code> is not specified, then newlines embedded inside quoted fields will not be interpreted correctly.</strong></li>
</ul>
<h4 id="Write-Rows-to-CSV-File"><a href="#Write-Rows-to-CSV-File" class="headerlink" title="Write Rows to CSV File"></a>Write Rows to CSV File</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_csv_file</span><span class="params">(filename, rows)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> file:</span><br><span class="line">        writer = csv.writer(file)</span><br><span class="line">        writer.writerows(rows)</span><br></pre></td></tr></table></figure>
<h3 id="JSON"><a href="#JSON" class="headerlink" title="JSON"></a>JSON</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line">&lt;str&gt;    = json.dumps(&lt;object&gt;, ensure_ascii=<span class="literal">True</span>, indent=<span class="literal">None</span>)</span><br><span class="line">&lt;object&gt; = json.loads(&lt;str&gt;)</span><br></pre></td></tr></table></figure>
<h4 id="Read-Object-from-JSON-File"><a href="#Read-Object-from-JSON-File" class="headerlink" title="Read Object from JSON File"></a>Read Object from JSON File</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_json_file</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">        <span class="keyword">return</span> json.load(file)</span><br></pre></td></tr></table></figure>
<h4 id="Write-Object-to-JSON-File"><a href="#Write-Object-to-JSON-File" class="headerlink" title="Write Object to JSON File"></a>Write Object to JSON File</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_json_file</span><span class="params">(filename, an_object)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">        json.dump(an_object, file, ensure_ascii=<span class="literal">False</span>, indent=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Pickle"><a href="#Pickle" class="headerlink" title="Pickle"></a>Pickle</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line">&lt;bytes&gt;  = pickle.dumps(&lt;object&gt;)</span><br><span class="line">&lt;object&gt; = pickle.loads(&lt;bytes&gt;)</span><br></pre></td></tr></table></figure>
<h4 id="Read-Object-from-File"><a href="#Read-Object-from-File" class="headerlink" title="Read Object from File"></a>Read Object from File</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_pickle_file</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'rb'</span>) <span class="keyword">as</span> file:</span><br><span class="line">        <span class="keyword">return</span> pickle.load(file)</span><br></pre></td></tr></table></figure>
<h4 id="Write-Object-to-File"><a href="#Write-Object-to-File" class="headerlink" title="Write Object to File"></a>Write Object to File</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_pickle_file</span><span class="params">(filename, an_object)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> file:</span><br><span class="line">        pickle.dump(an_object, file)</span><br></pre></td></tr></table></figure>
<h3 id="SQLite"><a href="#SQLite" class="headerlink" title="SQLite"></a>SQLite</h3><hr>
<p><strong>Server-less database engine that stores each database into separate file.</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlite3</span><br><span class="line">db = sqlite3.connect(<span class="string">'&lt;path&gt;'</span>)                <span class="comment"># Also ':memory:'.</span></span><br><span class="line">...</span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong>New database will be created if path doesn’t exist.</strong></li>
</ul>
<h4 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cursor = db.execute(<span class="string">'&lt;query&gt;'</span>)</span><br><span class="line"><span class="keyword">if</span> cursor:</span><br><span class="line">    &lt;tuple&gt; = cursor.fetchone()               <span class="comment"># First row.</span></span><br><span class="line">    &lt;list&gt;  = cursor.fetchall()               <span class="comment"># Remaining rows.</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Returned values can be of type str, int, float, bytes or None.</strong></li>
</ul>
<h4 id="Write"><a href="#Write" class="headerlink" title="Write"></a>Write</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">db.execute(<span class="string">'&lt;query&gt;'</span>)</span><br><span class="line">db.commit()</span><br></pre></td></tr></table></figure>
<h4 id="Placeholders"><a href="#Placeholders" class="headerlink" title="Placeholders"></a>Placeholders</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">db.execute(<span class="string">'&lt;query&gt;'</span>, &lt;list/tuple&gt;)           <span class="comment"># Replaces '?'s in query with values.</span></span><br><span class="line">db.execute(<span class="string">'&lt;query&gt;'</span>, &lt;dict/namedtuple&gt;)      <span class="comment"># Replaces ':&lt;key&gt;'s with values.</span></span><br><span class="line">db.executemany(<span class="string">'&lt;query&gt;'</span>, &lt;coll_of_above&gt;)    <span class="comment"># Runs execute() many times.</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Passed values can be of type str, int, float, bytes, None, bool, datetime.date or datetime.datetme.</strong></li>
</ul>
<h4 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h4><p><strong>Has a very similar interface, with differences listed below.</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $ pip3 install mysql-connector</span></span><br><span class="line"><span class="keyword">from</span> mysql <span class="keyword">import</span> connector</span><br><span class="line">db = connector.connect(host=&lt;str&gt;, user=&lt;str&gt;, password=&lt;str&gt;, database=&lt;str&gt;)</span><br><span class="line">cursor = db.cursor()</span><br><span class="line">cursor.execute(<span class="string">'&lt;query&gt;'</span>)                     <span class="comment"># Connector doesn't have execute method.</span></span><br><span class="line">cursor.execute(<span class="string">'&lt;query&gt;'</span>, &lt;list/tuple&gt;)       <span class="comment"># Replaces '%s's in query with values.</span></span><br><span class="line">cursor.execute(<span class="string">'&lt;query&gt;'</span>, &lt;dict/namedtuple&gt;)  <span class="comment"># Replaces '%(&lt;key&gt;)s's with values.</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Bytes"><a href="#Bytes" class="headerlink" title="Bytes"></a>Bytes</h3><hr>
<p><strong>Bytes object is an immutable sequence of single bytes. Mutable version is called ‘bytearray’.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;bytes&gt; = <span class="string">b'&lt;str&gt;'</span>                       <span class="comment"># Only accepts ASCII characters and \x00 - \xff.</span></span><br><span class="line">&lt;int&gt;   = &lt;bytes&gt;[&lt;index&gt;]               <span class="comment"># Returns int in range from 0 to 255.</span></span><br><span class="line">&lt;bytes&gt; = &lt;bytes&gt;[&lt;slice&gt;]               <span class="comment"># Returns bytes even if it has only one element.</span></span><br><span class="line">&lt;bytes&gt; = &lt;bytes&gt;.join(&lt;coll_of_bytes&gt;)  <span class="comment"># Joins elements using bytes object as separator.</span></span><br></pre></td></tr></table></figure>
<h4 id="Encode-1"><a href="#Encode-1" class="headerlink" title="Encode"></a>Encode</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;bytes&gt; = &lt;str&gt;.encode(<span class="string">'utf-8'</span>)          <span class="comment"># Or: bytes(&lt;str&gt;, 'utf-8')</span></span><br><span class="line">&lt;bytes&gt; = bytes(&lt;coll_of_ints&gt;)          <span class="comment"># Ints must be in range from 0 to 255.</span></span><br><span class="line">&lt;bytes&gt; = &lt;int&gt;.to_bytes(&lt;length&gt;, byteorder=<span class="string">'big|little'</span>, signed=<span class="literal">False</span>)</span><br><span class="line">&lt;bytes&gt; = bytes.fromhex(<span class="string">'&lt;hex&gt;'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Decode-1"><a href="#Decode-1" class="headerlink" title="Decode"></a>Decode</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;str&gt;   = &lt;bytes&gt;.decode(<span class="string">'utf-8'</span>)        <span class="comment"># Or: str(&lt;bytes&gt;, 'utf-8')</span></span><br><span class="line">&lt;list&gt;  = list(&lt;bytes&gt;)                  <span class="comment"># Returns ints in range from 0 to 255.</span></span><br><span class="line">&lt;int&gt;   = int.from_bytes(&lt;bytes&gt;, byteorder=<span class="string">'big|little'</span>, signed=<span class="literal">False</span>)</span><br><span class="line"><span class="string">'&lt;hex&gt;'</span> = &lt;bytes&gt;.hex()</span><br></pre></td></tr></table></figure>
<h4 id="Read-Bytes-from-File"><a href="#Read-Bytes-from-File" class="headerlink" title="Read Bytes from File"></a>Read Bytes from File</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_bytes</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'rb'</span>) <span class="keyword">as</span> file:</span><br><span class="line">        <span class="keyword">return</span> file.read()</span><br></pre></td></tr></table></figure>
<h4 id="Write-Bytes-to-File"><a href="#Write-Bytes-to-File" class="headerlink" title="Write Bytes to File"></a>Write Bytes to File</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_bytes</span><span class="params">(filename, bytes_obj)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> file:</span><br><span class="line">        file.write(bytes_obj)</span><br></pre></td></tr></table></figure>
<h3 id="Struct"><a href="#Struct" class="headerlink" title="Struct"></a>Struct</h3><hr>
<ul>
<li><strong>Module that performs conversions between a sequence of numbers and a C struct, represented as a Python bytes object.</strong></li>
<li><strong>Machine’s native type sizes and byte order are used by default.</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> struct <span class="keyword">import</span> pack, unpack, iter_unpack, calcsize</span><br><span class="line">&lt;bytes&gt;  = pack(<span class="string">'&lt;format&gt;'</span>, &lt;num_1&gt; [, &lt;num_2&gt;, ...])</span><br><span class="line">&lt;tuple&gt;  = unpack(<span class="string">'&lt;format&gt;'</span>, &lt;bytes&gt;)</span><br><span class="line">&lt;tuples&gt; = iter_unpack(<span class="string">'&lt;format&gt;'</span>, &lt;bytes&gt;)</span><br></pre></td></tr></table></figure>
<h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>pack(<span class="string">'&gt;hhl'</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="string">b'\x00\x01\x00\x02\x00\x00\x00\x03'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>unpack(<span class="string">'&gt;hhl'</span>, <span class="string">b'\x00\x01\x00\x02\x00\x00\x00\x03'</span>)</span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>calcsize(<span class="string">'&gt;hhl'</span>)</span><br><span class="line"><span class="number">8</span></span><br></pre></td></tr></table></figure>
<h4 id="Format-2"><a href="#Format-2" class="headerlink" title="Format"></a>Format</h4><h5 id="For-standard-sizes-start-format-string-with"><a href="#For-standard-sizes-start-format-string-with" class="headerlink" title="For standard sizes start format string with:"></a>For standard sizes start format string with:</h5><ul>
<li><strong><code>&#39;=&#39;</code> - native byte order</strong></li>
<li><strong><code>&#39;&lt;&#39;</code> - little-endian</strong></li>
<li><strong><code>&#39;&gt;&#39;</code> - big-endian</strong></li>
</ul>
<h5 id="Integer-types-Use-capital-letter-for-unsigned-type-Standard-sizes-are-in-brackets"><a href="#Integer-types-Use-capital-letter-for-unsigned-type-Standard-sizes-are-in-brackets" class="headerlink" title="Integer types. Use capital letter for unsigned type. Standard sizes are in brackets:"></a>Integer types. Use capital letter for unsigned type. Standard sizes are in brackets:</h5><ul>
<li><strong><code>&#39;x&#39;</code> - pad byte</strong></li>
<li><strong><code>&#39;b&#39;</code> - char (1)</strong></li>
<li><strong><code>&#39;h&#39;</code> - short (2)</strong></li>
<li><strong><code>&#39;i&#39;</code> - int (4)</strong></li>
<li><strong><code>&#39;l&#39;</code> - long (4)</strong></li>
<li><strong><code>&#39;q&#39;</code> - long long (8)</strong></li>
</ul>
<h5 id="Floating-point-types"><a href="#Floating-point-types" class="headerlink" title="Floating point types:"></a>Floating point types:</h5><ul>
<li><strong><code>&#39;f&#39;</code> - float (4)</strong></li>
<li><strong><code>&#39;d&#39;</code> - double (8)</strong></li>
</ul>
<h3 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h3><hr>
<p><strong>List that can only hold numbers of predefined type. Available types and their sizes in bytes are listed above.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> array <span class="keyword">import</span> array</span><br><span class="line">&lt;array&gt; = array(<span class="string">'&lt;typecode&gt;'</span> [, &lt;collection&gt;])</span><br></pre></td></tr></table></figure>
<h3 id="Memory-View"><a href="#Memory-View" class="headerlink" title="Memory View"></a>Memory View</h3><hr>
<p><strong>Used for accessing the internal data of an object that supports the buffer protocol.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;memoryview&gt; = memoryview(&lt;bytes&gt; / &lt;bytearray&gt; / &lt;array&gt;)</span><br><span class="line">&lt;memoryview&gt;.release()</span><br></pre></td></tr></table></figure>
<h3 id="Deque"><a href="#Deque" class="headerlink" title="Deque"></a>Deque</h3><hr>
<p><strong>A thread-safe list with efficient appends and pops from either side. Pronounced “deck”.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line">&lt;deque&gt; = deque(&lt;collection&gt;, maxlen=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;deque&gt;.appendleft(&lt;el&gt;)</span><br><span class="line">&lt;el&gt; = &lt;deque&gt;.popleft()</span><br><span class="line">&lt;deque&gt;.extendleft(&lt;collection&gt;)            <span class="comment"># Collection gets reversed.</span></span><br><span class="line">&lt;deque&gt;.rotate(n=<span class="number">1</span>)                         <span class="comment"># Rotates elements to the right.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = deque([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], maxlen=<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.append(<span class="number">4</span>)</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.appendleft(<span class="number">5</span>)</span><br><span class="line">[<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.insert(<span class="number">1</span>, <span class="number">6</span>)</span><br><span class="line">IndexError: deque already at its maximum size</span><br></pre></td></tr></table></figure>
<h3 id="Threading"><a href="#Threading" class="headerlink" title="Threading"></a>Threading</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> threading <span class="keyword">import</span> Thread, RLock</span><br></pre></td></tr></table></figure>
<h4 id="Thread"><a href="#Thread" class="headerlink" title="Thread"></a>Thread</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">thread = Thread(target=&lt;function&gt;, args=(&lt;first_arg&gt;, ))</span><br><span class="line">thread.start()</span><br><span class="line">...</span><br><span class="line">thread.join()</span><br></pre></td></tr></table></figure>
<h4 id="Lock"><a href="#Lock" class="headerlink" title="Lock"></a>Lock</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lock = RLock()</span><br><span class="line">lock.acquire()</span><br><span class="line">...</span><br><span class="line">lock.release()</span><br></pre></td></tr></table></figure>
<h5 id="Or"><a href="#Or" class="headerlink" title="Or:"></a>Or:</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lock = RLock()</span><br><span class="line"><span class="keyword">with</span> lock:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<h3 id="Introspection"><a href="#Introspection" class="headerlink" title="Introspection"></a>Introspection</h3><hr>
<p><strong>Inspecting code at runtime.</strong></p>
<h4 id="Variables"><a href="#Variables" class="headerlink" title="Variables"></a>Variables</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;list&gt; = dir()      <span class="comment"># Names of variables in current scope.</span></span><br><span class="line">&lt;dict&gt; = locals()   <span class="comment"># Dict of local variables. Also vars().</span></span><br><span class="line">&lt;dict&gt; = globals()  <span class="comment"># Dict of global variables.</span></span><br></pre></td></tr></table></figure>
<h4 id="Attributes-1"><a href="#Attributes-1" class="headerlink" title="Attributes"></a>Attributes</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;dict&gt; = vars(&lt;object&gt;)</span><br><span class="line">&lt;bool&gt; = hasattr(&lt;object&gt;, <span class="string">'&lt;attr_name&gt;'</span>)</span><br><span class="line">value  = getattr(&lt;object&gt;, <span class="string">'&lt;attr_name&gt;'</span>)</span><br><span class="line">setattr(&lt;object&gt;, <span class="string">'&lt;attr_name&gt;'</span>, value)</span><br></pre></td></tr></table></figure>
<h4 id="Parameters-1"><a href="#Parameters-1" class="headerlink" title="Parameters"></a>Parameters</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> inspect <span class="keyword">import</span> signature</span><br><span class="line">&lt;sig&gt;        = signature(&lt;function&gt;)</span><br><span class="line">no_of_params = len(&lt;sig&gt;.parameters)</span><br><span class="line">param_names  = list(&lt;sig&gt;.parameters.keys())</span><br></pre></td></tr></table></figure>
<h3 id="Metaprograming"><a href="#Metaprograming" class="headerlink" title="Metaprograming"></a>Metaprograming</h3><hr>
<p><strong>Code that generates code.</strong></p>
<h4 id="Type-1"><a href="#Type-1" class="headerlink" title="Type"></a>Type</h4><p><strong>Type is the root class. If only passed an object it returns its type (class). Otherwise it creates a new class.</strong></p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;class&gt; = type(&lt;class_name&gt;, &lt;parents_tuple&gt;, &lt;attributes_dict&gt;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>Z = type(<span class="string">'Z'</span>, (), &#123;<span class="string">'a'</span>: <span class="string">'abcde'</span>, <span class="string">'b'</span>: <span class="number">12345</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = Z()</span><br></pre></td></tr></table></figure>
<h4 id="Meta-Class"><a href="#Meta-Class" class="headerlink" title="Meta Class"></a>Meta Class</h4><p><strong>Class that creates class.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_meta_class</span><span class="params">(name, parents, attrs)</span>:</span></span><br><span class="line">    attrs[<span class="string">'a'</span>] = <span class="string">'abcde'</span></span><br><span class="line">    <span class="keyword">return</span> type(name, parents, attrs)</span><br></pre></td></tr></table></figure>
<h5 id="Or-1"><a href="#Or-1" class="headerlink" title="Or:"></a>Or:</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyMetaClass</span><span class="params">(type)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(cls, name, parents, attrs)</span>:</span></span><br><span class="line">        attrs[<span class="string">'a'</span>] = <span class="string">'abcde'</span></span><br><span class="line">        <span class="keyword">return</span> type.__new__(cls, name, parents, attrs)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>New() is a class method that gets called before init(). If it returns an instance of its class, then that instance gets passed to init() as a ‘self’ argument.</strong></li>
<li><strong>It receives the same arguments as init(), except for the first one that specifies the desired class of returned instance (</strong><code>&#39;MyMetaClass&#39;</code> <strong>in our case).</strong></li>
<li><strong>New() can also be called directly, usually from a new() method of a child class (</strong><code>def __new__(cls): return super().__new__(cls)</code><strong>), in which case init() is not called.</strong></li>
</ul>
<h4 id="Metaclass-Attribute"><a href="#Metaclass-Attribute" class="headerlink" title="Metaclass Attribute"></a>Metaclass Attribute</h4><p><strong>Right before a class is created it checks if it has metaclass defined. If not, it recursively checks if any of his parents has it defined and eventually comes to type().</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(metaclass=MyMetaClass)</span>:</span></span><br><span class="line">    b = <span class="number">12345</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>MyClass.a, MyClass.b</span><br><span class="line">(<span class="string">'abcde'</span>, <span class="number">12345</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Type-Diagram"><a href="#Type-Diagram" class="headerlink" title="Type Diagram"></a>Type Diagram</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">type(MyClass)     == MyMetaClass  <span class="comment"># MyClass is an instance of MyMetaClass.</span></span><br><span class="line">type(MyMetaClass) == type         <span class="comment"># MyMetaClass is an instance of type.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+---------+-------------+</span><br><span class="line">| Classes | Metaclasses |</span><br><span class="line">+---------+-------------|</span><br><span class="line">| MyClass &gt; MyMetaClass |</span><br><span class="line">|         |     v       |</span><br><span class="line">|  object ---&gt; type &lt;+  |</span><br><span class="line">|         |    ^ +---+  |</span><br><span class="line">|   str -------+        |</span><br><span class="line">+---------+-------------+</span><br></pre></td></tr></table></figure>
<h4 id="Inheritance-Diagram"><a href="#Inheritance-Diagram" class="headerlink" title="Inheritance Diagram"></a>Inheritance Diagram</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">MyClass.__base__     == object    <span class="comment"># MyClass is a subclass of object.</span></span><br><span class="line">MyMetaClass.__base__ == type      <span class="comment"># MyMetaClass is a subclass of type.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+---------+-------------+</span><br><span class="line">| Classes | Metaclasses |</span><br><span class="line">+---------+-------------|</span><br><span class="line">| MyClass | MyMetaClass |</span><br><span class="line">|    v    |     v       |</span><br><span class="line">|  object &lt;--- type     |</span><br><span class="line">|    ^    |             |</span><br><span class="line">|   str   |             |</span><br><span class="line">+---------+-------------+</span><br></pre></td></tr></table></figure>
<h3 id="Operator"><a href="#Operator" class="headerlink" title="Operator"></a>Operator</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add, sub, mul, truediv, floordiv, mod, pow, neg, abs</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> eq, ne, lt, le, gt, ge</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> and_, or_, not_</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter, attrgetter, methodcaller</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> operator <span class="keyword">as</span> op</span><br><span class="line">sorted_by_second = sorted(&lt;collection&gt;, key=op.itemgetter(<span class="number">1</span>))</span><br><span class="line">sorted_by_both   = sorted(&lt;collection&gt;, key=op.itemgetter(<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line">product_of_elems = functools.reduce(op.mul, &lt;collection&gt;)</span><br><span class="line">LogicOp          = enum.Enum(<span class="string">'LogicOp'</span>, &#123;<span class="string">'AND'</span>: op.and_, <span class="string">'OR'</span> : op.or_&#125;)</span><br><span class="line">last_el          = op.methodcaller(<span class="string">'pop'</span>)(&lt;list&gt;)</span><br></pre></td></tr></table></figure>
<h3 id="Eval"><a href="#Eval" class="headerlink" title="Eval"></a>Eval</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> ast <span class="keyword">import</span> literal_eval</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>literal_eval(<span class="string">'1 + 2'</span>)</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>literal_eval(<span class="string">'[1, 2, 3]'</span>)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>literal_eval(<span class="string">'abs(1)'</span>)</span><br><span class="line">ValueError: malformed node <span class="keyword">or</span> string</span><br></pre></td></tr></table></figure>
<h3 id="Coroutine"><a href="#Coroutine" class="headerlink" title="Coroutine"></a>Coroutine</h3><hr>
<ul>
<li><strong>Similar to generator, but generator pulls data through the pipe with iteration, while coroutine pushes data into the pipeline with send().</strong></li>
<li><strong>Coroutines provide more powerful data routing possibilities than iterators.</strong></li>
<li><strong>If you build a collection of simple data processing components, you can glue them together into complex arrangements of pipes, branches, merging, etc.</strong></li>
</ul>
<h4 id="Helper-Decorator"><a href="#Helper-Decorator" class="headerlink" title="Helper Decorator"></a>Helper Decorator</h4><ul>
<li><strong>All coroutines must be “primed” by first calling next().</strong></li>
<li><strong>Remembering to call next() is easy to forget.</strong></li>
<li><strong>Solved by wrapping coroutines with a decorator:</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coroutine</span><span class="params">(func)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">out</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        cr = func(*args, **kwargs)</span><br><span class="line">        next(cr)</span><br><span class="line">        <span class="keyword">return</span> cr</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h4 id="Pipeline-Example"><a href="#Pipeline-Example" class="headerlink" title="Pipeline Example"></a>Pipeline Example</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reader</span><span class="params">(target)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        target.send(i)</span><br><span class="line">    target.close()</span><br><span class="line"></span><br><span class="line"><span class="meta">@coroutine</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adder</span><span class="params">(target)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        value = (<span class="keyword">yield</span>)</span><br><span class="line">        target.send(value + <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@coroutine</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printer</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        value = (<span class="keyword">yield</span>)</span><br><span class="line">        print(value)</span><br><span class="line"></span><br><span class="line">reader(adder(printer()))  <span class="comment"># 100, 101, ..., 109</span></span><br></pre></td></tr></table></figure>
<p><br><br></p>
<h2 id="Libraries"><a href="#Libraries" class="headerlink" title="Libraries"></a>Libraries</h2><h3 id="Progress-Bar"><a href="#Progress-Bar" class="headerlink" title="Progress Bar"></a>Progress Bar</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $ pip3 install tqdm</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]):</span><br><span class="line">    sleep(<span class="number">0.2</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(range(<span class="number">100</span>)):</span><br><span class="line">    sleep(<span class="number">0.02</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Plot"><a href="#Plot" class="headerlink" title="Plot"></a>Plot</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $ pip3 install matplotlib</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line">pyplot.plot(&lt;data_1&gt; [, &lt;data_2&gt;, ...])  <span class="comment"># Or: hist(&lt;data&gt;).</span></span><br><span class="line">pyplot.savefig(&lt;filename&gt;)</span><br><span class="line">pyplot.show()</span><br><span class="line">pyplot.clf()                             <span class="comment"># Clears figure.</span></span><br></pre></td></tr></table></figure>
<h3 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h3><hr>
<h4 id="Prints-a-CSV-file-as-an-ASCII-table"><a href="#Prints-a-CSV-file-as-an-ASCII-table" class="headerlink" title="Prints a CSV file as an ASCII table:"></a>Prints a CSV file as an ASCII table:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $ pip3 install tabulate</span></span><br><span class="line"><span class="keyword">from</span> tabulate <span class="keyword">import</span> tabulate</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">with</span> open(&lt;filename&gt;, encoding=<span class="string">'utf-8'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> file:</span><br><span class="line">    lines   = csv.reader(file)</span><br><span class="line">    headers = [header.title() <span class="keyword">for</span> header <span class="keyword">in</span> next(lines)]</span><br><span class="line">    table   = tabulate(lines, headers)</span><br><span class="line">    print(table)</span><br></pre></td></tr></table></figure>
<h3 id="Curses"><a href="#Curses" class="headerlink" title="Curses"></a>Curses</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> curses <span class="keyword">import</span> wrapper, ascii</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    wrapper(draw)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(screen)</span>:</span></span><br><span class="line">    screen.clear()</span><br><span class="line">    screen.addstr(<span class="number">0</span>, <span class="number">0</span>, <span class="string">'Press ESC to quit.'</span>)</span><br><span class="line">    <span class="keyword">while</span> screen.getch() != ascii.ESC:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_border</span><span class="params">(screen)</span>:</span></span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line">    P = namedtuple(<span class="string">'P'</span>, <span class="string">'y x'</span>)</span><br><span class="line">    height, width = screen.getmaxyx()</span><br><span class="line">    <span class="keyword">return</span> P(height<span class="number">-1</span>, width<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h3 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $ pip3 install loguru</span></span><br><span class="line"><span class="keyword">from</span> loguru <span class="keyword">import</span> logger</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logger.add(<span class="string">'debug_&#123;time&#125;.log'</span>, colorize=<span class="literal">True</span>)  <span class="comment"># Connects a log file.</span></span><br><span class="line">logger.add(<span class="string">'error_&#123;time&#125;.log'</span>, level=<span class="string">'ERROR'</span>)  <span class="comment"># Another file for errors or higher.</span></span><br><span class="line">logger.&lt;level&gt;(<span class="string">'A logging message.'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Levels: <code>&#39;debug&#39;</code>, <code>&#39;info&#39;</code>, <code>&#39;success&#39;</code>, <code>&#39;warning&#39;</code>, <code>&#39;error&#39;</code>, <code>&#39;critical&#39;</code>.</strong></li>
</ul>
<h4 id="Exceptions-2"><a href="#Exceptions-2" class="headerlink" title="Exceptions"></a>Exceptions</h4><p><strong>Error description, stack trace and values of variables are appended automatically.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">except</span> &lt;exception&gt;:</span><br><span class="line">    logger.exception(<span class="string">'An error happened.'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Rotation"><a href="#Rotation" class="headerlink" title="Rotation"></a>Rotation</h4><p><strong>Argument that sets a condition when a new log file is created.</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rotation=&lt;int&gt;|&lt;datetime.timedelta&gt;|&lt;datetime.time&gt;|&lt;str&gt;</span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong><code>&#39;&lt;int&gt;&#39;</code> - Max file size in bytes.</strong></li>
<li><strong><code>&#39;&lt;timedelta&gt;&#39;</code> - Max age of a file.</strong></li>
<li><strong><code>&#39;&lt;time&gt;&#39;</code> - Time of day.</strong></li>
<li><strong><code>&#39;&lt;str&gt;&#39;</code> - Any of above as a string: <code>&#39;100 MB&#39;</code>, <code>&#39;1 month&#39;</code>, <code>&#39;monday at 12:00&#39;</code>, …</strong></li>
</ul>
<h4 id="Retention"><a href="#Retention" class="headerlink" title="Retention"></a>Retention</h4><p><strong>Sets a condition which old log files are deleted.</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">retention=&lt;int&gt;|&lt;datetime.timedelta&gt;|&lt;str&gt;</span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong><code>&#39;&lt;int&gt;&#39;</code> - Max number of files.</strong></li>
<li><strong><code>&#39;&lt;timedelta&gt;&#39;</code> - Max age of a file.</strong></li>
<li><strong><code>&#39;&lt;str&gt;&#39;</code> - Max age as a string: <code>&#39;1 week, 3 days&#39;</code>, <code>&#39;2 months&#39;</code>, …</strong></li>
</ul>
<h3 id="Scraping"><a href="#Scraping" class="headerlink" title="Scraping"></a>Scraping</h3><hr>
<h4 id="Scrapes-and-prints-Python’s-URL-and-version-number-from-Wikipedia"><a href="#Scrapes-and-prints-Python’s-URL-and-version-number-from-Wikipedia" class="headerlink" title="Scrapes and prints Python’s URL and version number from Wikipedia:"></a>Scrapes and prints Python’s URL and version number from Wikipedia:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $ pip3 install requests beautifulsoup4</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">url   = <span class="string">'https://en.wikipedia.org/wiki/Python_(programming_language)'</span></span><br><span class="line">page  = requests.get(url)</span><br><span class="line">doc   = BeautifulSoup(page.text, <span class="string">'html.parser'</span>)</span><br><span class="line">table = doc.find(<span class="string">'table'</span>, class_=<span class="string">'infobox vevent'</span>)</span><br><span class="line">rows  = table.find_all(<span class="string">'tr'</span>)</span><br><span class="line">link  = rows[<span class="number">11</span>].find(<span class="string">'a'</span>)[<span class="string">'href'</span>]</span><br><span class="line">ver   = rows[<span class="number">6</span>].find(<span class="string">'div'</span>).text.split()[<span class="number">0</span>]</span><br><span class="line">print(link, ver)</span><br></pre></td></tr></table></figure>
<h3 id="Web"><a href="#Web" class="headerlink" title="Web"></a>Web</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $ pip3 install bottle</span></span><br><span class="line"><span class="keyword">from</span> bottle <span class="keyword">import</span> run, route, post, template, request, response</span><br><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></table></figure>
<h4 id="Run"><a href="#Run" class="headerlink" title="Run"></a>Run</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">run(host=<span class="string">'localhost'</span>, port=<span class="number">8080</span>)</span><br><span class="line">run(host=<span class="string">'0.0.0.0'</span>, port=<span class="number">80</span>, server=<span class="string">'cherrypy'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Static-Request"><a href="#Static-Request" class="headerlink" title="Static Request"></a>Static Request</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@route('/img/&lt;image&gt;')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_image</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> static_file(image, <span class="string">'images/'</span>, mimetype=<span class="string">'image/png'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Dynamic-Request"><a href="#Dynamic-Request" class="headerlink" title="Dynamic Request"></a>Dynamic Request</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@route('/&lt;sport&gt;')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_page</span><span class="params">(sport)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> template(<span class="string">'&lt;h1&gt;&#123;&#123;title&#125;&#125;&lt;/h1&gt;'</span>, title=sport)</span><br></pre></td></tr></table></figure>
<h4 id="REST-Request"><a href="#REST-Request" class="headerlink" title="REST Request"></a>REST Request</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@post('/odds/&lt;sport&gt;')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">odds_handler</span><span class="params">(sport)</span>:</span></span><br><span class="line">    team = request.forms.get(<span class="string">'team'</span>)</span><br><span class="line">    home_odds, away_odds = <span class="number">2.44</span>, <span class="number">3.29</span></span><br><span class="line">    response.headers[<span class="string">'Content-Type'</span>] = <span class="string">'application/json'</span></span><br><span class="line">    response.headers[<span class="string">'Cache-Control'</span>] = <span class="string">'no-cache'</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps([team, home_odds, away_odds])</span><br></pre></td></tr></table></figure>
<h4 id="Test"><a href="#Test" class="headerlink" title="Test:"></a>Test:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $ pip3 install requests</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>url  = <span class="string">'http://localhost:8080/odds/football'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = &#123;<span class="string">'team'</span>: <span class="string">'arsenal f.c.'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response = requests.post(url, data=data)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.json()</span><br><span class="line">[<span class="string">'arsenal f.c.'</span>, <span class="number">2.44</span>, <span class="number">3.29</span>]</span><br></pre></td></tr></table></figure>
<h3 id="Profile"><a href="#Profile" class="headerlink" title="Profile"></a>Profile</h3><hr>
<h3 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line">start_time = time()                  <span class="comment"># Seconds since Epoch.</span></span><br><span class="line">...</span><br><span class="line">duration = time() - start_time</span><br></pre></td></tr></table></figure>
<h4 id="High-Performance"><a href="#High-Performance" class="headerlink" title="High Performance"></a>High Performance</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> perf_counter <span class="keyword">as</span> pc</span><br><span class="line">start_time = pc()                    <span class="comment"># Seconds since restart.</span></span><br><span class="line">...</span><br><span class="line">duration = pc() - start_time</span><br></pre></td></tr></table></figure>
<h4 id="Timing-a-Snippet"><a href="#Timing-a-Snippet" class="headerlink" title="Timing a Snippet"></a>Timing a Snippet</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> timeit <span class="keyword">import</span> timeit</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>timeit(<span class="string">'"-".join(str(a) for a in range(100))'</span>,</span><br><span class="line"><span class="meta">... </span>       number=<span class="number">10000</span>, globals=globals(), setup=<span class="string">'pass'</span>)</span><br><span class="line"><span class="number">0.34986</span></span><br></pre></td></tr></table></figure>
<h4 id="Line-Profiler"><a href="#Line-Profiler" class="headerlink" title="Line Profiler"></a>Line Profiler</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $ pip3 install line_profiler</span></span><br><span class="line"><span class="meta">@profile</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    a = [*range(<span class="number">10000</span>)]</span><br><span class="line">    b = &#123;*range(<span class="number">10000</span>)&#125;</span><br><span class="line">main()</span><br></pre></td></tr></table></figure>
<h4 id="Usage"><a href="#Usage" class="headerlink" title="Usage:"></a>Usage:</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kernprof -lv test.py</span><br><span class="line">Line #      Hits         Time  Per Hit   % Time  Line Contents</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">     1                                           @profile</span><br><span class="line">     2                                           def main():</span><br><span class="line">     3         1       1128.0   1128.0     27.4      a &#x3D; [*range(10000)]</span><br><span class="line">     4         1       2994.0   2994.0     72.6      b &#x3D; &#123;*range(10000)&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Call-Graph"><a href="#Call-Graph" class="headerlink" title="Call Graph"></a>Call Graph</h4><h5 id="Generates-a-PNG-image-of-a-call-graph-with-highlighted-bottlenecks"><a href="#Generates-a-PNG-image-of-a-call-graph-with-highlighted-bottlenecks" class="headerlink" title="Generates a PNG image of a call graph with highlighted bottlenecks:"></a>Generates a PNG image of a call graph with highlighted bottlenecks:</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $ pip3 install pycallgraph</span></span><br><span class="line"><span class="keyword">from</span> pycallgraph <span class="keyword">import</span> output, PyCallGraph</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line">time_str = datetime.now().strftime(<span class="string">'%Y%m%d%H%M%S'</span>)</span><br><span class="line">filename = <span class="string">f'profile-<span class="subst">&#123;time_str&#125;</span>.png'</span></span><br><span class="line">drawer = output.GraphvizOutput(output_file=filename)</span><br><span class="line"><span class="keyword">with</span> PyCallGraph(drawer):</span><br><span class="line">    &lt;code_to_be_profiled&gt;</span><br></pre></td></tr></table></figure>
<h3 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h3><hr>
<p><strong>Array manipulation mini language. Can run up to one hundred times faster than equivalent Python code.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $ pip3 install numpy</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;array&gt; = np.array(&lt;list&gt;)</span><br><span class="line">&lt;array&gt; = np.arange(from_inclusive, to_exclusive, ±step_size)</span><br><span class="line">&lt;array&gt; = np.ones(&lt;shape&gt;)</span><br><span class="line">&lt;array&gt; = np.random.randint(from_inclusive, to_exclusive, &lt;shape&gt;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;array&gt;.shape = &lt;shape&gt;</span><br><span class="line">&lt;view&gt;  = &lt;array&gt;.reshape(&lt;shape&gt;)</span><br><span class="line">&lt;view&gt;  = np.broadcast_to(&lt;array&gt;, &lt;shape&gt;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;array&gt; = &lt;array&gt;.sum(axis)</span><br><span class="line">indexes = &lt;array&gt;.argmin(axis)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Shape is a tuple of dimension sizes.</strong></li>
<li><strong>Axis is an index of dimension that gets collapsed. Leftmost dimension has index 0.</strong></li>
</ul>
<h4 id="Indexing"><a href="#Indexing" class="headerlink" title="Indexing"></a>Indexing</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;el&gt;       = &lt;2d_array&gt;[0, 0]        <span class="comment"># First element.</span></span><br><span class="line">&lt;1d_view&gt;  = &lt;2d_array&gt;[0]           <span class="comment"># First row.</span></span><br><span class="line">&lt;1d_view&gt;  = &lt;2d_array&gt;[:, 0]        <span class="comment"># First column. Also [..., 0].</span></span><br><span class="line">&lt;3d_view&gt;  = &lt;2d_array&gt;[None, :, :]  <span class="comment"># Expanded by dimension of size 1.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;1d_array&gt; = &lt;2d_array&gt;[&lt;1d_row_indexes&gt;, &lt;1d_column_indexes&gt;]</span><br><span class="line">&lt;2d_array&gt; = &lt;2d_array&gt;[&lt;2d_row_indexes&gt;, &lt;2d_column_indexes&gt;]</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;2d_bools&gt; = &lt;2d_array&gt; &gt; 0</span><br><span class="line">&lt;1d_array&gt; = &lt;2d_array&gt;[&lt;2d_bools&gt;]</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>If row and column indexes differ in shape, they are combined with broadcasting.</strong></li>
</ul>
<h4 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting"></a>Broadcasting</h4><p><strong>Broadcasting is a set of rules by which NumPy functions operate on arrays of different sizes and/or dimensions.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">left  = [[<span class="number">0.1</span>], [<span class="number">0.6</span>], [<span class="number">0.8</span>]]  <span class="comment"># Shape: (3, 1)</span></span><br><span class="line">right = [ <span class="number">0.1</span> ,  <span class="number">0.6</span> ,  <span class="number">0.8</span> ]  <span class="comment"># Shape: (3)</span></span><br></pre></td></tr></table></figure>
<h5 id="1-If-array-shapes-differ-in-length-left-pad-the-shorter-shape-with-ones"><a href="#1-If-array-shapes-differ-in-length-left-pad-the-shorter-shape-with-ones" class="headerlink" title="1. If array shapes differ in length, left-pad the shorter shape with ones:"></a>1. If array shapes differ in length, left-pad the shorter shape with ones:</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">left  = [[<span class="number">0.1</span>], [<span class="number">0.6</span>], [<span class="number">0.8</span>]]  <span class="comment"># Shape: (3, 1)</span></span><br><span class="line">right = [[<span class="number">0.1</span> ,  <span class="number">0.6</span> ,  <span class="number">0.8</span>]]  <span class="comment"># Shape: (1, 3) &lt;- !</span></span><br></pre></td></tr></table></figure>
<h5 id="2-If-any-dimensions-differ-in-size-expand-the-ones-that-have-size-1-by-duplicating-their-elements"><a href="#2-If-any-dimensions-differ-in-size-expand-the-ones-that-have-size-1-by-duplicating-their-elements" class="headerlink" title="2. If any dimensions differ in size, expand the ones that have size 1 by duplicating their elements:"></a>2. If any dimensions differ in size, expand the ones that have size 1 by duplicating their elements:</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">left  = [[<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>], [<span class="number">0.6</span>, <span class="number">0.6</span>, <span class="number">0.6</span>], [<span class="number">0.8</span>, <span class="number">0.8</span>, <span class="number">0.8</span>]]  <span class="comment"># Shape: (3, 3) &lt;- !</span></span><br><span class="line">right = [[<span class="number">0.1</span>, <span class="number">0.6</span>, <span class="number">0.8</span>], [<span class="number">0.1</span>, <span class="number">0.6</span>, <span class="number">0.8</span>], [<span class="number">0.1</span>, <span class="number">0.6</span>, <span class="number">0.8</span>]]  <span class="comment"># Shape: (3, 3) &lt;- !</span></span><br></pre></td></tr></table></figure>
<h5 id="3-If-neither-non-matching-dimension-has-size-1-rise-an-error"><a href="#3-If-neither-non-matching-dimension-has-size-1-rise-an-error" class="headerlink" title="3. If neither non-matching dimension has size 1, rise an error."></a>3. If neither non-matching dimension has size 1, rise an error.</h5><h4 id="Example-1"><a href="#Example-1" class="headerlink" title="Example"></a>Example</h4><h5 id="For-each-point-returns-index-of-its-nearest-point-0-1-0-6-0-8-gt-1-2-1"><a href="#For-each-point-returns-index-of-its-nearest-point-0-1-0-6-0-8-gt-1-2-1" class="headerlink" title="For each point returns index of its nearest point ([0.1, 0.6, 0.8] =&gt; [1, 2, 1]):"></a>For each point returns index of its nearest point (<code>[0.1, 0.6, 0.8] =&gt; [1, 2, 1]</code>):</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>points = np.array([<span class="number">0.1</span>, <span class="number">0.6</span>, <span class="number">0.8</span>])</span><br><span class="line">[ <span class="number">0.1</span>,  <span class="number">0.6</span>,  <span class="number">0.8</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>wrapped_points = points.reshape(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">[[ <span class="number">0.1</span>],</span><br><span class="line"> [ <span class="number">0.6</span>],</span><br><span class="line"> [ <span class="number">0.8</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>distances = wrapped_points - points</span><br><span class="line">[[ <span class="number">0.</span> , <span class="number">-0.5</span>, <span class="number">-0.7</span>],</span><br><span class="line"> [ <span class="number">0.5</span>,  <span class="number">0.</span> , <span class="number">-0.2</span>],</span><br><span class="line"> [ <span class="number">0.7</span>,  <span class="number">0.2</span>,  <span class="number">0.</span> ]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>distances = np.abs(distances)</span><br><span class="line">[[ <span class="number">0.</span> ,  <span class="number">0.5</span>,  <span class="number">0.7</span>],</span><br><span class="line"> [ <span class="number">0.5</span>,  <span class="number">0.</span> ,  <span class="number">0.2</span>],</span><br><span class="line"> [ <span class="number">0.7</span>,  <span class="number">0.2</span>,  <span class="number">0.</span> ]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i = np.arange(<span class="number">3</span>)</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>distances[i, i] = np.inf</span><br><span class="line">[[ inf,  <span class="number">0.5</span>,  <span class="number">0.7</span>],</span><br><span class="line"> [ <span class="number">0.5</span>,  inf,  <span class="number">0.2</span>],</span><br><span class="line"> [ <span class="number">0.7</span>,  <span class="number">0.2</span>,  inf]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>distances.argmin(<span class="number">1</span>)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h3 id="Image"><a href="#Image" class="headerlink" title="Image"></a>Image</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $ pip3 install pillow</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br></pre></td></tr></table></figure>
<h4 id="Creates-a-PNG-image-of-a-rainbow-gradient"><a href="#Creates-a-PNG-image-of-a-rainbow-gradient" class="headerlink" title="Creates a PNG image of a rainbow gradient:"></a>Creates a PNG image of a rainbow gradient:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">width  = <span class="number">100</span></span><br><span class="line">height = <span class="number">100</span></span><br><span class="line">size   = width * height</span><br><span class="line">pixels = [<span class="number">255</span> * i/size <span class="keyword">for</span> i <span class="keyword">in</span> range(size)]</span><br><span class="line"></span><br><span class="line">img = Image.new(<span class="string">'HSV'</span>, (width, height))</span><br><span class="line">img.putdata([(int(a), <span class="number">255</span>, <span class="number">255</span>) <span class="keyword">for</span> a <span class="keyword">in</span> pixels])</span><br><span class="line">img.convert(mode=<span class="string">'RGB'</span>).save(<span class="string">'test.png'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Adds-noise-to-a-PNG-image"><a href="#Adds-noise-to-a-PNG-image" class="headerlink" title="Adds noise to a PNG image:"></a>Adds noise to a PNG image:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line">add_noise = <span class="keyword">lambda</span> value: max(<span class="number">0</span>, min(<span class="number">255</span>, value + randint(<span class="number">-20</span>, <span class="number">20</span>)))</span><br><span class="line">img = Image.open(<span class="string">'test.png'</span>).convert(mode=<span class="string">'HSV'</span>)</span><br><span class="line">img.putdata([(add_noise(h), s, v) <span class="keyword">for</span> h, s, v <span class="keyword">in</span> img.getdata()])</span><br><span class="line">img.convert(mode=<span class="string">'RGB'</span>).save(<span class="string">'test.png'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Modes-1"><a href="#Modes-1" class="headerlink" title="Modes"></a>Modes</h4><ul>
<li><strong><code>&#39;1&#39;</code> - 1-bit pixels, black and white, stored with one pixel per byte.</strong></li>
<li><strong><code>&#39;L&#39;</code> - 8-bit pixels, greyscale.</strong></li>
<li><strong><code>&#39;RGB&#39;</code> - 3x8-bit pixels, true color.</strong></li>
<li><strong><code>&#39;RGBA&#39;</code> - 4x8-bit pixels, true color with transparency mask.</strong></li>
<li><strong><code>&#39;HSV&#39;</code> - 3x8-bit pixels, Hue, Saturation, Value color space.</strong></li>
</ul>
<h3 id="Audio"><a href="#Audio" class="headerlink" title="Audio"></a>Audio</h3><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> wave</span><br><span class="line"><span class="keyword">from</span> struct <span class="keyword">import</span> pack, iter_unpack</span><br></pre></td></tr></table></figure>
<h4 id="Read-Frames-from-WAV-File"><a href="#Read-Frames-from-WAV-File" class="headerlink" title="Read Frames from WAV File"></a>Read Frames from WAV File</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_wav_file</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> wave.open(filename, <span class="string">'rb'</span>) <span class="keyword">as</span> wf:</span><br><span class="line">        frames = wf.readframes(wf.getnframes())</span><br><span class="line">        <span class="keyword">return</span> [a[<span class="number">0</span>] <span class="keyword">for</span> a <span class="keyword">in</span> iter_unpack(<span class="string">'&lt;h'</span>, frames)]</span><br></pre></td></tr></table></figure>
<h4 id="Write-Frames-to-WAV-File"><a href="#Write-Frames-to-WAV-File" class="headerlink" title="Write Frames to WAV File"></a>Write Frames to WAV File</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_wav_file</span><span class="params">(filename, frames_int, mono=True)</span>:</span></span><br><span class="line">    frames_short = (pack(<span class="string">'&lt;h'</span>, a) <span class="keyword">for</span> a <span class="keyword">in</span> frames_int)</span><br><span class="line">    <span class="keyword">with</span> wave.open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> wf:</span><br><span class="line">        wf.setnchannels(<span class="number">1</span> <span class="keyword">if</span> mono <span class="keyword">else</span> <span class="number">2</span>)</span><br><span class="line">        wf.setsampwidth(<span class="number">2</span>)</span><br><span class="line">        wf.setframerate(<span class="number">44100</span>)</span><br><span class="line">        wf.writeframes(<span class="string">b''</span>.join(frames_short))</span><br></pre></td></tr></table></figure>
<h4 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h4><h5 id="Saves-a-sine-wave-to-a-mono-WAV-file"><a href="#Saves-a-sine-wave-to-a-mono-WAV-file" class="headerlink" title="Saves a sine wave to a mono WAV file:"></a>Saves a sine wave to a mono WAV file:</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> pi, sin</span><br><span class="line">frames_f = (sin(i * <span class="number">2</span> * pi * <span class="number">440</span> / <span class="number">44100</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100000</span>))</span><br><span class="line">frames_i = (int(a * <span class="number">30000</span>) <span class="keyword">for</span> a <span class="keyword">in</span> frames_f)</span><br><span class="line">write_to_wav_file(<span class="string">'test.wav'</span>, frames_i)</span><br></pre></td></tr></table></figure>
<h5 id="Adds-noise-to-a-mono-WAV-file"><a href="#Adds-noise-to-a-mono-WAV-file" class="headerlink" title="Adds noise to a mono WAV file:"></a>Adds noise to a mono WAV file:</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line">add_noise = <span class="keyword">lambda</span> value: max(<span class="number">-32768</span>, min(<span class="number">32767</span>, value + randint(<span class="number">-500</span>, <span class="number">500</span>)))</span><br><span class="line">frames_i  = (add_noise(a) <span class="keyword">for</span> a <span class="keyword">in</span> read_wav_file(<span class="string">'test.wav'</span>))</span><br><span class="line">write_to_wav_file(<span class="string">'test.wav'</span>, frames_i)</span><br></pre></td></tr></table></figure>
<h4 id="Synthesizer"><a href="#Synthesizer" class="headerlink" title="Synthesizer"></a>Synthesizer</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $ pip3 install simpleaudio</span></span><br><span class="line"><span class="keyword">import</span> simpleaudio, math, struct</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> chain, repeat</span><br><span class="line">F  = <span class="number">44100</span></span><br><span class="line">P1 = <span class="string">'71♪,69,,71♪,66,,62♪,66,,59♪,,,'</span></span><br><span class="line">P2 = <span class="string">'71♪,73,,74♪,73,,74,,71,,73♪,71,,73,,69,,71♪,69,,71,,67,,71♪,,,'</span></span><br><span class="line">get_pause  = <span class="keyword">lambda</span> seconds: repeat(<span class="number">0</span>, int(seconds * F))</span><br><span class="line">sin_f      = <span class="keyword">lambda</span> i, hz: math.sin(i * <span class="number">2</span> * math.pi * hz / F)</span><br><span class="line">get_wave   = <span class="keyword">lambda</span> hz, seconds: (sin_f(i, hz) <span class="keyword">for</span> i <span class="keyword">in</span> range(int(seconds * F)))</span><br><span class="line">get_hz     = <span class="keyword">lambda</span> key: <span class="number">8.176</span> * <span class="number">2</span> ** (int(key) / <span class="number">12</span>)</span><br><span class="line">parse_note = <span class="keyword">lambda</span> note: (get_hz(note[:<span class="number">2</span>]), <span class="number">0.25</span> <span class="keyword">if</span> <span class="string">'♪'</span> <span class="keyword">in</span> note <span class="keyword">else</span> <span class="number">0.125</span>)</span><br><span class="line">get_frames = <span class="keyword">lambda</span> note: get_wave(*parse_note(note)) <span class="keyword">if</span> note <span class="keyword">else</span> get_pause(<span class="number">0.125</span>)</span><br><span class="line">frames_f   = chain.from_iterable(get_frames(n) <span class="keyword">for</span> n <span class="keyword">in</span> <span class="string">f'<span class="subst">&#123;P1&#125;</span><span class="subst">&#123;P1&#125;</span><span class="subst">&#123;P2&#125;</span>'</span>.split(<span class="string">','</span>))</span><br><span class="line">frames_b   = <span class="string">b''</span>.join(struct.pack(<span class="string">'&lt;h'</span>, int(f * <span class="number">30000</span>)) <span class="keyword">for</span> f <span class="keyword">in</span> frames_f)</span><br><span class="line">simpleaudio.play_buffer(frames_b, <span class="number">1</span>, <span class="number">2</span>, F)</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>机器学习大总结</title>
    <url>/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="机器学习大总结"><a href="#机器学习大总结" class="headerlink" title="机器学习大总结"></a>机器学习大总结</h2><h3 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h3><h4 id="进程和线程"><a href="#进程和线程" class="headerlink" title="进程和线程"></a><strong>进程和线程</strong></h4><p>进程和线程都是一个时间段的描述，是CPU工作时间段的描述，不过是颗粒大小不同.进程就是包换上下文切换的程序执行时间总和 = CPU加载上下文+CPU执行+CPU保存上下文.线程是共享了进程的上下文环境的更为细小的CPU时间段。</p>
<h4 id="判别式模型和生成式模型"><a href="#判别式模型和生成式模型" class="headerlink" title="判别式模型和生成式模型:"></a><strong>判别式模型和生成式模型</strong>:</h4><ol>
<li>判别式模型直接学习决策函数$f(X)$或条件概率分布$P(Y|X)$作为预测的模型.往往准确率更高,并且可以简化学习问题.<ol>
<li>k近邻法、感知机、决策树、最大熵模型、Logistic回归、线性判别分析(LDA)、支持向量机(SVM)、Boosting、CRF、线性回归、神经网络</li>
</ol>
</li>
<li>生成式模型由数据学习<strong>联合概率分布P(X,Y)</strong>,然后由P(Y|X)=P(X,Y)/P(X)求出条件概率分布作为预测的模型,即生成模型.当存在隐变量时只能用生成方法学习.<ol>
<li>混合高斯模型和其他混合模型、隐马尔可夫模型(HMM)、朴素贝叶斯、依赖贝叶斯(AODE)、LDA文档主题生成模型</li>
</ol>
</li>
</ol>
<a id="more"></a>
<h4 id="概率质量函数-概率密度函数-累积分布函数"><a href="#概率质量函数-概率密度函数-累积分布函数" class="headerlink" title="概率质量函数,概率密度函数,累积分布函数:"></a><strong>概率质量函数,概率密度函数,累积分布函数</strong>:</h4><ol>
<li>概率质量函数 (probability mass function，PMF)是离散随机变量在各特定取值上的概率。</li>
<li>概率密度函数（probability density function，PDF)是对 连续随机变量 定义的，本身不是概率，只有对连续随机变量的取值进行积分后才是概率。</li>
<li><p>累积分布函数（cumulative distribution function，CDF)能完整描述一个实数随机变量X的概率分布，是概率密度函数的积分。对於所有实数x ，与pdf相对。</p>
<h4 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a><strong>极大似然估计</strong></h4></li>
</ol>
<p>已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。</p>
<h4 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h4><p>二乘的英文是least square,找一个（组）估计值,使得实际值与估计值之差的平方加总之后的值最小$Q=\min \sum_{i}^{n}\left(y_{i e}-y_{i}\right)^{2}$.求解方式是对参数求偏导,令偏导为0即可.样本量小时速度快.</p>
<h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a><strong>梯度下降法</strong></h4><p>负梯度方向是函数值下降最快的方向,每次更新值都等于原值加学习率(<strong>步长</strong>)乘损失函数的<strong>梯度</strong>.每次都试一个步长看会不会下降一定的程度,如果没有的话就按比例减小步长.不断应用参数更新公式直到收敛,可以得到局部最小值.初始值的不同组合可以得到不同局部最小值.在最优点时会有震荡.</p>
<ol>
<li><p><strong>批量梯度下降(BGD)</strong>:每次都使用所有的m个样本来更新,容易找到全局最优解,但是m较大时速度较慢。$\theta_{j}^{\prime}=\theta_{j}+\frac{1}{m} \sum_{i=1}^{m}\left(y^{i}-h_{\theta}\left(x^{i}\right)\right) x_{j}^{i}$</p>
</li>
<li><p><strong>随机梯度下降(SGD)</strong>：每次只使用一个样本来更新,训练速度快,但是噪音较多,不容易找到全局最优解,以损失很小的一部分精确度和增加一定数量的迭代次数为代价,换取了总体的优化效率的提升.注意控制步长缩小,减少震荡.</p>
</li>
</ol>
<script type="math/tex; mode=display">
\theta_{j}=\theta_{j}+\left(y^{i}-h_{\theta}\left(x^{i}\right)\right) x_{j}^{i}</script><ol>
<li><strong>小批量梯度下降(MBGD)</strong>:每次使用一部分样本来更新.</li>
</ol>
<h4 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a><strong>牛顿法</strong></h4><p>牛顿法是<strong>二次收敛</strong>,因此收敛速度快.从几何上看是每次用一个二次曲面来拟合当前所处位置的局部曲面,而梯度下降法是用一个平面来拟合.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/newton.png" alt="avatar"></p>
<ol>
<li><strong>黑塞矩阵</strong>是由目标函数f(x)在点X处的二阶偏导数组成的n*n阶对称矩阵。</li>
<li><strong>牛顿法</strong>:将f(x)在x(k)附近进行<strong>二阶泰勒展开</strong>:$f(x)=f\left(x^{(k)}\right)+g_{k}^{\top}\left(x-x^{(k)}\right)+\frac{1}{2}\left(x-x^{(k)}\right)^{\mathrm{T}} H\left(x^{(k)}\right)\left(x-x^{(k)}\right)$。其中$g_k$是$f(x)$的梯度向量在$x^{(k)}$的值,$H(x^{(k)})$是$f(x)$的黑塞矩阵在点$x^k$的值.牛顿法利用极小点的必要条件$f(x)$处的梯度为0,每次迭代中从点$x^{(k)}$开始,假设$\nabla f\left(x^{(k+1)}\right)=0$，对二阶泰勒展开求偏导有 $\nabla f(x)=g_{k}+H_{k}\left(x-x^{(k)}\right)$，代入得到$g_{k}+H_{k}\left(x^{(k+1)}-x^{(k)}\right)=0$,即$\quad x^{(k+1)}=x^{(k)}-H_{k}^{-1} g_{k}$,以此为迭代公式就是牛顿法.</li>
</ol>
<h4 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h4><p>用一个<strong>n阶正定矩阵Gk</strong>=G(x(k))来<strong>近似代替</strong>黑塞矩阵的<strong>逆矩阵</strong>就是拟牛顿法的基本思想.在牛顿法中黑塞矩阵满足的条件如下:$g_{k+1}-g_{k}=H_{k}\left(x^{(k+1)}-x^{(k)}\right)$,令$ {y_{k}}=g_{k+1}-g_{k}, \quad \delta_{k}=x^{(k+1)}-x^{(k)}$，则有  $ H_{k}^{-1} y_{k}=\delta_{k}$,称为拟牛顿条件.</p>
<ol>
<li><strong>DFP算法</strong>:假设每一步$G_{k+1}=G_{k}+P_{k}+Q_{k}$，为使$G_{k+1}$满足拟牛顿条件,可使$P_k$和$Q_k$满足$P_{k} y_{k}=\delta_{k}, Q_{k} y_{k}=-G_{k} y_{k}$,例如取$P_{k}=\frac{\delta_{k} \delta_{k}^{\tau}}{\delta_{k}^{\top} y_{k}} \quad Q_{k}=-\frac{G_{k} y_{k} y_{k}^{\top} G_{k}}{y_{k}^{\top} G_{k} y_{k}}$，就得到迭代公式$G_{k+1}=G_{k}+\frac{\delta_{k} \delta_{k}^{\top}}{\delta_{k}^{\tau} y_{k}}-\frac{G_{k} y_{k} y_{k}^{\top} G_{k}}{y_{k}^{T} G_{k} y_{k}}$</li>
<li><strong>BFGS算法</strong>: 最流行的拟牛顿算法.它用$B_k$逼近黑塞矩阵,此时相应的拟牛顿条件是$B_{k+1} \delta_{k}=y_{k}$,假设每一步$B_{k+1}=B_{k}+P_{k}+Q_{k}$，则 $P_k$ 和 $Q_k$ 满足$P_{k} \delta_{k}=y_{k}, \quad Q_{k} \delta_{k}=-B_{k} \delta_{k}$，类似得到迭代公式$B_{k+1}=B_{k}+\frac{y_{k} y_{k}^{\top}}{y_{k}^{\top} \delta_{k}}-\frac{B_{k} \delta_{k} \delta_{k}^{\mathrm{T}} B_{k}}{\delta_{k}^{\top} B_{k} \delta_{k}}$</li>
</ol>
<h4 id="先验概率和后验概率"><a href="#先验概率和后验概率" class="headerlink" title="先验概率和后验概率"></a><strong>先验概率和后验概率</strong></h4><ol>
<li>先验概率就是事情发生前的预测概率.</li>
<li>后验概率是一种条件概率，它限定了事件为隐变量取值，而条件为观测结果。一般的条件概率，条件和事件可以是任意的.</li>
<li>贝叶斯公式$P(y|x) = ( P(x|y) * P(y) ) / P(x)$中,$P(y|x)$是后验概率,$P(x|y)$是条件概率,$P(y)$是先验概率.</li>
</ol>
<h4 id="偏差-方差-噪声"><a href="#偏差-方差-噪声" class="headerlink" title="偏差,方差,噪声"></a><strong>偏差,方差,噪声</strong></h4><ol>
<li>偏差:度量了学习算法的期望预测和真实结果偏离程度</li>
<li>方差:度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响</li>
<li>噪声:可以认为是数据自身的波动性，表达了目前任何学习算法所能达到泛化误差的下限</li>
<li><p><strong>泛化误差</strong>可以分解为偏差、方差与噪声之和</p>
<h4 id="对偶原理"><a href="#对偶原理" class="headerlink" title="对偶原理"></a><strong>对偶原理</strong></h4></li>
</ol>
<p>一个优化问题可以从主问题和对偶问题两个方面考虑.在推导对偶问题时,通过将拉格朗日函数对$x$求导并使导数为0来获得对偶函数.对偶函数给出了主问题最优解的下界,因此对偶问题一般是凸问题,那么只需求解对偶函数的最优解就可以了.</p>
<h4 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a><strong>KKT条件</strong></h4><p>通常我们要求解的最优化条件有如下三种:</p>
<ol>
<li>无约束优化问题:通常使用求导,使导数为零,求解候选最优值</li>
<li>有等式约束的优化问题:通常使用拉格朗日乘子法,即把等式约束用拉格朗日乘子和优化问题合并为一个式子,通过对各个变量求导使其为零,求解候选最优值.拉格朗日乘数法其实是KKT条件在等式约束优化问题的简化版.</li>
<li>有不等式约束的优化问题:通常使用KKT条件.即把不等式约束,等式约束和优化问题合并为一个式子.假设有多个等式约束$h(x)$和不等式约束$g(x)$,$L(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\mu})=f(\boldsymbol{x})+\sum_{i=1}^{m} \lambda_{i} h_{i}(\boldsymbol{x})+\sum_{i=1}^{n} \mu_{j} g_{j}(\boldsymbol{x})$,则不等式约束引入的KKT条件如下:<br>$\left\{\begin{array}{l}g_{j}(\boldsymbol{x}) \leqslant 0 \\ mu_{j} \geqslant 0  \\mu_{j} g_{j}(\boldsymbol{x})=0\end{array}\right.$ ,实质是最优解在$g(x)&lt;0$区域内时,约束条件不起作用,等价于对$μ$置零然后对原函数的偏导数置零;当$g(x)=0$时与情况2相近.结合两种情况,那么只需要使$L$对$x$求导为零,使$h(x)$为零,使$μg(x)$为零三式即可求解候选最优值.</li>
</ol>
<h4 id="降维方法"><a href="#降维方法" class="headerlink" title="降维方法"></a><strong>降维方法</strong></h4><ol>
<li>主成分分析(PCA):降维,不断选择与已有坐标轴正交且方差最大的坐标轴.</li>
<li>奇异值分解(SVD):矩阵分解,降维,推荐系统. </li>
<li>线性判别分析(LDA)</li>
</ol>
<h4 id="集成方法"><a href="#集成方法" class="headerlink" title="集成方法"></a>集成方法</h4><h5 id="bagging"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</h5><p>装袋法的核心思想是构建多个相互独立的评估器，然后对其预测进行平均或多数表决原则来决定集成评估器的结果。</p>
<ol>
<li>评估器：相互独立，同时运行</li>
<li>抽样：有放回抽样</li>
<li>如何决定集成的结果：平均或者少数服从多数</li>
<li>目标：降低方差</li>
<li>基学习器过拟合：能够一定程度上解决基学习器过拟合的问题</li>
<li>基学习器学习能力弱：不是很有帮助</li>
<li>代表算法：随机森林<h5 id="boosting"><a href="#boosting" class="headerlink" title="boosting"></a>boosting</h5>提升方法中，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本进行预测，从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树。</li>
<li>评估器：相互关联，按顺序依次构建，后建的模型在先建模型预测失败的样本上有更多的权重</li>
<li>抽样：有放回的采样，但会确认数据的权重，每次抽样都会给预测失败的样本更多的权重</li>
<li>如何决定集成的结果：加权平均，在训练集上表现好的模型会有更大的权重</li>
<li>目标：降低偏差，提高模型整体的精确度</li>
<li>基学习器过拟合：加剧过拟合问题</li>
<li>基学习器学习能力弱：提升模型表现</li>
<li>代表算法：GBDT,Adaboost<h5 id="stacking"><a href="#stacking" class="headerlink" title="stacking"></a>stacking</h5>Stacking模型是指将多种分类器组合在一起来取得更好表现的一种集成学习模型。一般情况下，Stacking模型分为两层。第一层中我们训练多个不同的模型，然后再以第一层训练的各个模型的输出作为输入来训练第二层的模型，以得到一个最终的输出。</li>
</ol>
<h4 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a><strong>性能度量</strong></h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center">预测值</th>
<th style="text-align:center">预测值</th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">真实值</td>
<td style="text-align:center">1</td>
<td style="text-align:center">11</td>
<td style="text-align:center">10</td>
<td style="text-align:center">Recall = $\frac{11}{11+10}$</td>
</tr>
<tr>
<td style="text-align:center">真实值</td>
<td style="text-align:center">0</td>
<td style="text-align:center">01</td>
<td style="text-align:center">00</td>
<td style="text-align:center">FPR = $\frac{01}{01+00}$</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">Precision=$\frac{11}{11+01}$</td>
<td style="text-align:center"></td>
<td style="text-align:center">Acc = $\frac{11+00}{11+10+01+00}$</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li><strong>准确度</strong>,最常用,但在数据集不平衡的情况下不好</li>
<li><strong>Precision(精确度/查准率)</strong>:$P=TP/(TP+FP)$</li>
<li><strong>Recall(召回率/查全率)</strong>:$R=TP/(TP+FN)$</li>
<li><strong>Fβ度量</strong>:$F_{\beta}=\frac{\left(1+\beta^{2}\right) r p}{\beta^{2} * p+r}$,当β=1时退化为F1度量,是精确率和召回率的调和均值.</li>
<li><strong>TPR(真正例率)</strong>:$TPR=TP/(TP+FN)$</li>
<li><strong>FPR(假正例率)</strong>:$FPR=FP/(TN+FP)$</li>
<li><strong>PR曲线</strong>:纵轴为Precision,横轴为Recall,一般使用平衡点(BEP,即Precsion=Recall的点)作为衡量标准.</li>
<li><strong>ROC(接受者操作特征)曲线</strong>:（<strong>每判断正确一个少数类，就有多少个多数类会被判断错误。</strong>）纵轴为TRP,横轴为FPR,在绘图时将分类阈值依次设为每个样例的预测值,再连接各点.ROC曲线围住的面积称为AOC,AOC越大则学习器性能越好.</li>
</ol>
<h4 id="损失函数和风险函数"><a href="#损失函数和风险函数" class="headerlink" title="损失函数和风险函数"></a><strong>损失函数和风险函数</strong></h4><ol>
<li>损失函数度量模型一次预测的好坏.常用的损失函数有:0-1损失函数,平方损失函数,绝对损失函数,对数似然损失函数.</li>
<li>损失函数的期望是理论上模型关于联合分布P(X,Y)的平均意义下的损失,称为风险函数,也叫<strong>期望风险</strong>.但是联合分布是未知的,期望风险不能直接计算.</li>
<li>当样本容量N趋于无穷时经验风险趋于期望风险,但现实中训练样本数目有限.</li>
</ol>
<h4 id="经验风险最小化和结构风险最小化"><a href="#经验风险最小化和结构风险最小化" class="headerlink" title="经验风险最小化和结构风险最小化"></a><strong>经验风险最小化和结构风险最小化</strong></h4><ol>
<li>模型关于训练数据集的平均损失称为经验风险.经验风险最小化的策略就是最小化经验风险.当样本数量足够大时学习效果较好.比如当模型是条件概率分布,损失函数是对数损失函数时,经验风险最小化就等价于极大似然估计.但是当样本容量很小时会出现过拟合.</li>
<li>结构风险最小化等于正则化.结构风险在经验风险上加上表示模型复杂度的正则化项.比如当模型是条件概率分布,损失函数是对数损失函数,模型复杂度由模型的先验概率表示时,结构风险最小化就等价于最大后验概率估计.</li>
</ol>
<h4 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a><strong>过拟合</strong></h4><p>指学习时选择的模型所包含的参数过多,以致于对已知数据预测得很好,但对未知数据预测很差的现象.模型选择旨在避免过拟合并提高模型的预测能力.</p>
<h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a><strong>正则化</strong></h4><p>模型选择的典型方法.正则化项一般是模型复杂度的单调递增函数,比如模型参数向量的范数.</p>
<h4 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a><strong>交叉验证</strong></h4><p>是另一常用的模型选择方法,可分为简单交叉验证,K折交叉验证,留一交叉验证等.</p>
<h4 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h4><p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/sklearn.png" alt="avatar"></p>
<h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>感知机是<strong>二类分类</strong>的线性模型,属于判别模型.感知机学习旨在求出将训练数据进行线性划分的分离超平面.是神经网络和支持向量机的基础.</p>
<h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p>$f(x)=sign(wx + b)$,$w$叫作权值向量,$b$叫做偏置,$sign$是符号函数.</p>
<h4 id="感知机的几何解释"><a href="#感知机的几何解释" class="headerlink" title="感知机的几何解释"></a><strong>感知机的几何解释</strong></h4><p>$wx+b$对应于特征空间中的一个分离超平面S,其中$w$是$S$的法向量,$b$是$S$的截距.$S$将特征空间划分为两个部分,位于两个部分的点分别被分为正负两类.</p>
<h4 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h4><p>假设训练数据集是线性可分的,感知机的损失函数是误分类点到超平面$S$的总距离.因为误分类点到超平面$S$的距离是$\frac{1}{|w|}\left|w \cdot x_{0}+b\right|$,且对于误分类的数据来说,总有$-y_i(wx_i+b)&gt;0$成立,因此不考虑$\frac{1}{|w|}$,就得到感知机的损失函数:</p>
<p>$L(w, b)=-\sum_{x \in M} y_{i}\left(w \cdot x_{i}+b\right)$,其中M是误分类点的集合.感知机学习的策略就是选取使损失函数最小的模型参数.</p>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p>感知机的最优化方法采用<strong>随机梯度下降法</strong>.首先任意选取一个超平面$w_0,b_0$,然后不断地极小化目标函数.在极小化过程中一次随机选取一个误分类点更新$w,b$,直到损失函数为0.$w \leftarrow w+\eta y_{i} x_{i}$ ， $b \leftarrow b+\eta y_{i}$，其中$η$表示步长.该算法的直观解释是:当一个点被误分类,就调整$w,b$使分离超平面向该误分类点接近.感知机的解可以不同.</p>
<h4 id="对偶形式"><a href="#对偶形式" class="headerlink" title="对偶形式"></a><strong>对偶形式</strong></h4><p>假设原始形式中的$w_0$和$b_0$均为0,设逐步修改$w$和$b$共n次,令$a=nη$,最后学习到的$w,b$可以表示为$w =\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}$,$b =\sum_{i=1}^{N}{\alpha_{i} y_{i}}$ ,那么对偶算法就变为设初始$a$和$b$均为0,每次选取数据更新$a$和$b$直至没有误分类点为止.对偶形式的意义在于可以将训练集中实例间的内积计算出来,存在Gram矩阵中,可以大大加快训练速度.</p>
<h3 id="k近邻法"><a href="#k近邻法" class="headerlink" title="k近邻法"></a>k近邻法</h3><p>k近邻法根据其<strong>k个最近邻</strong>的训练实例的类别,通过多数表决等方式进行预测.当k=1时称为最近邻算法.</p>
<h4 id="三个基本要素"><a href="#三个基本要素" class="headerlink" title="三个基本要素:"></a>三个基本要素:</h4><ol>
<li>k值的选择</li>
<li>距离度量</li>
<li>分类决策规则</li>
</ol>
<h4 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h4><p>当训练集,距离度量,k值以及分类决策规则确定后,特征空间已经根据这些要素被划分为一些子空间,且子空间里每个点所属的类也已被确定.</p>
<h4 id="策略-1"><a href="#策略-1" class="headerlink" title="策略"></a><strong>策略</strong></h4><ol>
<li><strong>距离</strong>:特征空间中两个实例点的距离是相似程度的反映,k近邻算法一般使用欧氏距离,也可以使用更一般的Lp距离或Minkowski距离.</li>
<li><strong>k值</strong>:k值较小时,整体模型变得复杂,容易发生过拟合.k值较大时,整体模型变得简单.在应用中k一般取较小的值,通过交叉验证法选取最优的k.</li>
<li><p><strong>分类决策规则</strong>:k近邻中的分类决策规则往往是多数表决,多数表决规则等价于经验风险最小化.</p>
<h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4></li>
</ol>
<p>根据给定的距离度量,在训练集中找出与x最邻近的k个点,根据分类规则决定x的类别y.</p>
<p><strong>kd树</strong></p>
<ol>
<li><p>kd树就是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构.kd树更适用于<strong>训练实例数远大于空间维数</strong>时的k近邻搜索.</p>
</li>
<li><p><strong>构造</strong>:可以通过如下<strong>递归</strong>实现:在超矩形区域上选择一个<strong>坐标轴</strong>和此坐标轴上的一个<strong>切分点</strong>,确定一个超平面,该超平面将当前超矩形区域切分为两个子区域.在子区域上重复切分直到子区域内没有实例时终止.通常依次选择坐标轴和选定坐标轴上的<strong>中位数点</strong>为切分点,这样可以得到平衡kd树.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/kd_tree.png" alt="avatar"></p>
</li>
</ol>
<h4 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a><strong>搜索</strong></h4><p>从根节点出发,若目标点x当前维的坐标小于切分点的坐标则移动到左子结点,否则移动到右子结点,直到子结点为叶结点为止.以此叶结点为”当前最近点”,<strong>递归</strong>地向上回退,在每个结点:(a)如果该结点比当前最近点距离目标点更近,则以该结点为”当前最近点”(b)”当前最近点”一定存在于该结点一个子结点对应的区域,检查该结点的另一子结点对应的区域是否与以目标点为球心,以目标点与”当前最近点”间的距离为半径的超球体相交.如果相交,移动到另一个子结点,如果不相交,向上回退.持续这个过程直到回退到根结点,最后的”当前最近点”即为最近邻点.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/search_kd.png" alt="avatar"></p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/search_kd_2.png" alt="avatar"></p>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p>朴素贝叶斯是基于<strong>贝叶斯定理</strong>和<strong>特征条件独立假设</strong>的分类方法.首先学习输入/输出的联合概率分布,然后基于此模型,对给定的输入x,利用贝叶斯定理求出后验概率最大的输出y.属于生成模型.</p>
<h4 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a><strong>模型</strong></h4><ol>
<li>首先学习先验概率分布$p(c_k),k=1,2,3,4,…,k$</li>
<li>然后学习条件概率分布,$P\left(X=x | Y=c_{k}\right)=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right)$.如果估计实际,需要指数级的计算,所以朴素贝叶斯法对条件概率分布作了条件独立性的假设,上式变成$\prod_{j=1}^{n} P\left(X^{(n)}=x^{(n)} | Y=c_{k}\right)$</li>
<li>在分类时,通过学习到的模型计算后验概率分布,由贝叶斯定理得到$P\left(Y=c_{k} | X=x\right)=\frac{P\left(X=x | Y=c_{k}\right) P\left(Y=c_{k}\right)}{\sum_{k} P\left(X=x | Y=c_{k}\right) P\left(Y=c_{k}\right)}$</li>
<li>将条件独立性假设得到的等式代入,并且注意到分母都是相同的,所以得到朴素贝叶斯分类器:$y=\arg \max _{a_{k}} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(l)}=x^{(l)} | Y=c_{k}\right)$</li>
</ol>
<p>朴素贝叶斯将实例分到后验概率最大的类中,这等价于期望风险最小化.</p>
<h4 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a><strong>算法</strong></h4><ol>
<li>使用<strong>极大似然估计法</strong>估计相应的先验概率:$P\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}{N}, \quad k=1,2, \cdots, K$和条件概率$P\left(X^{(j)}=a_{n} | Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(n)}=a_{j i} y_{i}=c_{k}\right)}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}$</li>
<li>计算条件独立性假设下的实例各个取值的可能性,$P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right), \quad k=1,2, \cdots, K$</li>
<li>选取其中的最大值作为输出.</li>
</ol>
<h4 id="特殊情况"><a href="#特殊情况" class="headerlink" title="特殊情况"></a>特殊情况</h4><ol>
<li>用极大似然估计可能会出现所要估计的概率值为0的情况,在累乘后会影响后验概率的计算结果,使分类产生偏差.可以采用<strong>贝叶斯估计</strong>,在随机变量各个取值的频数上赋予一个正数.$P_{\lambda}\left(X^{(1)}=a_{j q} | Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(l)}=a_{\beta}, y_{i}=c_{k}\right)+\lambda}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)+S_{j} \lambda}$，$S_j$为j属性可能取值数量,当$λ=0$时就是极大似然估计.常取$λ=1$,称为<strong>拉普拉斯平滑</strong>.</li>
<li>如果是连续值的情况,可以假设连续变量服从高斯分布,然后用训练数据估计参数.$P\left(X_{i}=x_{i} | Y=y_{i}\right)=\frac{1}{\sqrt{2 \pi} \sigma_{i j}^{2}} e^{\frac{\left(x_{i}-\mu_{j}\right)^{2}}{2 \sigma_{i}^{2}}}$</li>
</ol>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p>决策树是一种基本的分类与回归方法.它可以认为是<strong>if-then规则</strong>的集合,也可以认为是定义在特征空间与类空间上的<strong>条件概率分布</strong>.主要优点是模型具有可读性,分类速度快.其主要围绕着两个问题：</p>
<ol>
<li>如何从数据表中找出最佳节点和最佳分枝？</li>
<li>如何让决策树停止生长，防止过拟合？</li>
</ol>
<h4 id="模型-3"><a href="#模型-3" class="headerlink" title="模型"></a>模型</h4><p>分类决策树由<strong>结点</strong>和<strong>有向边</strong>组成.结点分为<strong>内部结点</strong>(表示一个特征或属性)和<strong>叶结点</strong>(表示一个类).决策树的路径具有<strong>互斥且完备</strong>的性质.</p>
<h4 id="策略-2"><a href="#策略-2" class="headerlink" title="策略"></a>策略</h4><p>决策树学习本质上是从训练数据集中归纳出一组分类规则.我们需要的是一个与训练数据<strong>矛盾较小</strong>,同时具有很好的<strong>泛化能力</strong>的决策树.从所有可能的决策树中选取最优决策树是NP完全问题,所以现实中常采用<strong>启发式方法</strong>近似求解.</p>
<h4 id="算法-3"><a href="#算法-3" class="headerlink" title="算法"></a>算法</h4><p>决策树学习算法包含<strong>特征选择</strong>,<strong>决策树的生成</strong>与<strong>决策树的剪枝过程</strong>.生成只考虑局部最优,剪枝则考虑全局最优。</p>
<h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p>如果利用一个特征进行分类的结果与随机分类的结果没有很大差别,则称这个特征是<strong>没有分类能力</strong>的.扔掉这样的特征对决策树学习的精度影响不大.</p>
<ol>
<li><strong>信息熵</strong>：熵是衡量<strong>随机变量不确定性</strong>的度量.熵越大,随机变量的不确定性就越大.信息熵是信息量的期望，$\left.H(X)=-\sum_{x \in X} P(x) \log P(x)\right)$</li>
<li><strong>条件熵</strong>：条件熵表示在已知随机变量X的条件下随机变量Y的不确定性.$H(Y | X)=\sum_{i=1}^{n} p_{i} H\left(Y | X=x_{i}\right)$</li>
<li><strong>信息增益</strong>：表示得知特征X的信息而使得类Y的信息的<strong>不确定性减少</strong>的程度.定义为集合D的经验熵与特征A在给定条件下D的经验条件熵之差$g(D,A)=H(D)-H(D|A)$,也就是训练数据集中类与特征的<strong>互信息</strong>.</li>
<li><strong>信息增益算法</strong>:计算数据集D的经验熵$H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|_{\mathrm{log}_{2}}\left|C_{k}\right|}{|D|}$,计算特征A对数据集D的经验条件熵$H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{\mu}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{k}\right|}{\left|D_{i}\right|}$,计算信息增益,选取信息增益最大的特征.</li>
<li><strong>信息增益比</strong>:信息增益值的大小是相对于训练数据集而言的,并无绝对意义.使用信息增益比,$g_{R}(D, A)=\frac{g(D, A)}{H(D)}$可以对这一问题进行校正.</li>
</ol>
<h4 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a><strong>决策树的生成</strong></h4><ol>
<li><strong>ID3算法</strong>:核心是在决策树各个结点上应用<strong>信息增益准则</strong>选择信息增益最大且大于阈值的特征,递归地构建决策树.ID3相当于用极大似然法进行概率模型的选择.由于算法只有树的生成,所以容易产生过拟合.</li>
<li><strong>C4.5算法</strong>:C4.5算法与ID3算法相似,改用<strong>信息增益比</strong>来选择特征.</li>
</ol>
<h4 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a><strong>决策树的剪枝</strong></h4><ol>
<li>在学习时过多考虑如何提高对训练数据的正确分类,从而构建出过于复杂的决策树,产生<strong>过拟合</strong>现象.解决方法是对已生成的决策树进行简化,称为剪枝.</li>
<li>设树的叶结点个数为$|T|$,每个叶结点有$N_t$个样本点,其中$k$类样本点有$N_{tk}$个,剪枝往往通过极小化决策树整体的损失函数$C_{\alpha}(T)=\sum_{i=1}^{\pi} N_{i} H_{i}(T)+\alpha|T|$来实现,其中经验熵$H_{t}(T)=-\sum_{k} \frac{N_{a}}{N_{t}} \log \frac{N_{u}}{N_{t}}$.剪枝通过加入$a|T|$项来考虑模型复杂度,实际上就是用正则化的极大似然估计进行模型选择.</li>
<li><strong>剪枝算法</strong>:剪去某一子结点,如果生成的新的整体树的<strong>损失函数值</strong>小于原树,则进行剪枝,直到不能继续为止.具体可以由动态规划实现.</li>
</ol>
<h4 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a><strong>CART算法</strong></h4><ol>
<li><p>CART既可以用于<strong>分类也</strong>可以用于<strong>回归</strong>.它假设决策树是<strong>二叉树</strong>,内部结点特征的取值为”是”和”否”.递归地构建二叉树,对回归树用<strong>平方误差</strong>最小化准则,对分类数用<strong>基尼指数</strong>最小化准则.</p>
</li>
<li><p><strong>回归树的生成</strong>:在训练数据集所在的输入空间中,递归地将每个区域划分为两个子区域.选择第j个变量和它取的值s作为切分变量和切分点,并定义两个区域$R_{1}(j, s)=\left\{x | x^{(j)} \leqslant s\right\} \quad$和$\quad R_{2}(j, s)=\left\{x | x^{(l)}&gt;s\right\}$,遍历变量j,对固定的j扫描切分点s,求解$\min _{j, z}\left[\min _{c_1} \sum_{x \in R_{1}(j, x)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x \in R_{2}(j, x)}\left(y_{i}-c_{2}\right)^{2}\right]$.用选定的对(j,s)划分区域并决定相应的输出值$\hat{c}_{m}=\frac{1}{N_{m}} \sum_{x_{i} \in R_{m}(j, x)} y_{i}, x \in R_{m}, \quad m=1,2$,直到满足停止条件.</p>
</li>
<li><p><strong>基尼指数</strong>:假设有K个类,样本属于第k类的概率为$p_k$,则概率分布的基尼指数为:$\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}$,表示不确定性.在特征A的条件下集合D的基尼指数定义为$\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)$,表示分割后集合D的不确定性.基尼指数越大,样本集合的<strong>不确定性</strong>也就越大.</p>
</li>
<li><p><strong>分类树的生成</strong></p>
<ol>
<li>从根结点开始,设结点的训练数据集为D,对每个特征A和其可能取的每个值a,计算A=a时的基尼指数,</li>
<li>选择<strong>基尼指数最小</strong>的特征及其对应的切分点作为<strong>最优特征</strong>与<strong>最优切分点</strong>,生成两个子结点</li>
<li>递归进行以上操作,直至满足<strong>停止条件</strong>.停止条件一般是结点中的样本个数小于阈值,或样本集的基尼指数小于阈值,或没有更多特征.</li>
</ol>
</li>
<li><p><strong>CART剪枝</strong></p>
<p>$T_t$表示以t为根结点的子树,$|T_t|$是$T_t$的叶结点个数.可以证明当$\alpha=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}$时,$T_t$与$t$有相同的损失函数值,且$t$的结点少,因此$t$比$T_t$更可取,对$T_t$进行剪枝.<strong>自下而上</strong>地对各内部结点t计算$g(t)=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}$,并令$a=min(g(t))$,<strong>自上而下</strong>地访问内部节点t,如果有$g(t)=a$,进行剪枝,并对t以<strong>多数表决法</strong>决定其类,得到子树T,如此循环地生成一串<strong>子树序列</strong>,直到新生成的T是由根结点单独构成的树为止.利用<strong>交叉验证法</strong>在子树序列中选取最优子树.</p>
<p>如果是<strong>连续值</strong>的情况,一般用<strong>二分法</strong>作为结点来划分.</p>
</li>
</ol>
<h3 id="logistic回归和最大熵模型"><a href="#logistic回归和最大熵模型" class="headerlink" title="logistic回归和最大熵模型"></a>logistic回归和最大熵模型</h3><h4 id="逻辑斯谛分布"><a href="#逻辑斯谛分布" class="headerlink" title="逻辑斯谛分布"></a><strong>逻辑斯谛分布</strong></h4><p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/logistic.png" alt="avatar"></p>
<p>分布函数$f(x)$以点$(μ,1/2)$为中心对称,$γ$的值越小,曲线在中心附近增长得越快.</p>
<h4 id="逻辑斯谛回归模型"><a href="#逻辑斯谛回归模型" class="headerlink" title="逻辑斯谛回归模型"></a><strong>逻辑斯谛回归模型</strong></h4><p>对于给定的输入x,根据$P(Y=1 | x)=\frac{\exp (w \cdot x+b)}{1+\exp (w \cdot x+b)}$ 和 $P(Y=0 | x)=\frac{1}{1+\exp (w \cdot x+b)}$计算出两个条件概率值的大小,将x分到概率值较大的那一类.将偏置b加入到权值向量w中,并在x的最后添加常数项1,得到$P(Y=1 | x)=\frac{\exp (w \cdot x)}{1+\exp (w \cdot x)}$ 和 $P(Y=0 | x)=\frac{1}{1+\exp (w \cdot x)}$。</p>
<h4 id="对数几率"><a href="#对数几率" class="headerlink" title="对数几率"></a>对数几率</h4><p>如果某事件发生的概率是p,则该事件发生的<strong>几率</strong>(此处几率指该事件发生概率与不发生概率之比)是$\frac{p}{1-p}$,<strong>对数几率</strong>是$log(\frac{p}{1-p})$,那么$\log \frac{P(Y=1 | x)}{1-P(Y=1 | x)}=w \cdot x$，也就是说在逻辑斯谛回归模型中,输出Y=1的对数几率是输入x的<strong>线性函数</strong>,线性函数值越接近正无穷,概率值就越接近1,反之则越接近0.</p>
<h4 id="似然估计"><a href="#似然估计" class="headerlink" title="似然估计"></a><strong>似然估计</strong></h4><p>给定x的情况下参数θ是真实参数的可能性.</p>
<h4 id="模型参数估计"><a href="#模型参数估计" class="headerlink" title="模型参数估计"></a><strong>模型参数估计</strong></h4><p>对于给定的二分类训练数据集,对数似然函数为</p>
<script type="math/tex; mode=display">
\begin{aligned} L(w) &=\sum_{i=1}^{N}\left[y_{i} \log \pi\left(x_{i}\right)+\left(1-y_{i}\right)\log\left(1-\pi\left(x_{i}\right)\right)\right]\\ &=\sum_{i=1}^{N}\left[y_{i} \log \frac{\pi\left(x_{i}\right)}{1-\pi\left(x_{i}\right)}+\log \left(1-\pi\left(x_{i}\right)\right)\right]\\&=\sum_{i=1}^{N}\left[y_{i}\left(w \cdot x_{i}\right)-\log \left(1+\exp \left(w \cdot x_{i}\right)\right]\right.\end{aligned}</script><p>也就是<strong>损失函数</strong>.其中$P(Y=1|x)=π(x)$,对$L(w)$求极大值,就可以得到$w$的估计值.问题变成了以对数似然函数为目标函数的最优化问题.</p>
<h4 id="多项逻辑斯谛回归"><a href="#多项逻辑斯谛回归" class="headerlink" title="多项逻辑斯谛回归"></a><strong>多项逻辑斯谛回归</strong></h4><p>当问题是多分类问题时,可以作如下推广:设Y有K类可能取值,$P(Y=k | x)=\frac{\exp \left(w_{k} \cdot x\right)}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}, \quad k=1,2, \cdots, K-1 \quad P(Y=K | x)=\frac{1}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}$,实际上就是<strong>one-vs-all</strong>的思想,将其他所有类当作一个类,问题转换为二分类问题.</p>
<p>使用最大似然法衡量模型输出的概率与真实概率的差别，假设样本一共有N个，那么这组样本发生的总概率可以表示为：</p>
<script type="math/tex; mode=display">
P(\boldsymbol{W})=\prod_{n=1}^{N}\left(\frac{e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}}{\sum_{k^{\prime}}^{C} e^{\boldsymbol{w}_{k^{\prime}}^{T} \boldsymbol{x}_{n}}}\right)</script><p>对函数取对数再乘以-1，推导得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
F(\boldsymbol{W})=-\ln (P(\boldsymbol{W})) &=\sum_{n=1}^{N} \ln \left(\frac{\sum_{k^{\prime}}^{C} e^{\boldsymbol{w}_{k^{\prime}}^{T} \boldsymbol{x}_{n}}}{e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}}\right) \\
&=\sum_{n=1}^{N} \ln \left(\frac{e^{\boldsymbol{w}_{1}^{T} \boldsymbol{x}}+e^{\boldsymbol{w}_{2}^{T} \boldsymbol{x}}+\ldots+e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}+\ldots+e^{\boldsymbol{w}_{c}^{T} \boldsymbol{x}}}{e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}}\right) \\
&=\sum_{n=1}^{N} \ln \left(1+\sum_{k \neq y_{n}} e^{\boldsymbol{w}_{k} \boldsymbol{x}_{n}-\boldsymbol{w}_{y_{n}} \boldsymbol{x}_{n}}\right)
\end{aligned}</script><h4 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a><strong>最大熵原理</strong></h4><p>学习概率模型时,在所有可能的概率模型中,<strong>熵最大</strong>的模型是最好的模型.直观地,最大熵原理认为模型首先要满足已有的事实,即<strong>约束条件</strong>.在没有更多信息的情况下,那些不确定的部分都是”<strong>等可能的</strong>“.</p>
<h4 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a><strong>最大熵模型</strong></h4><p>给定训练数据集,可以确定联合分布P(X,Y)的经验分布$\tilde{P}(X=x, Y=y)=\frac{v(X=x, Y=y)}{N}$和边缘分布P(X)的经验分布$\tilde{P}(X=x)=\frac{v(X=x)}{N}$,其中v表示频数,N表示样本容量.用<strong>特征函数$f(x,y)$</strong>=1描述x与y满足某一事实,可以得到特征函数关于P(X,Y)的经验分布的期望值和关于模型P(Y|X)与P(X)的经验分布的期望值,假设两者相等,就得到了<strong>约束条件</strong>$\sum_{x, y} \tilde{P}(x) P(y | x) f(x, y)=\sum_{x, y} \tilde{P}(x, y) f(x, y)$.定义在条件概率分布P(Y|X)上的条件熵为$H(P)=-\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)$,则<strong>条件熵最大</strong>的模型称为最大熵模型.</p>
<h4 id="最大熵模型的学习"><a href="#最大熵模型的学习" class="headerlink" title="最大熵模型的学习"></a><strong>最大熵模型的学习</strong></h4><p>就是求解最大熵模型的过程.等价于<strong>约束最优化问题</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
&\max _{P_{e c}} H(P)=-\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)\\
&\text { s.t. } \quad E_{p}\left(f_{i}\right)=E_{p}\left(f_{i}\right), \quad i=1,2, \cdots, n\\
     &\sum_{y} P(y | x)=1
\end{aligned}</script><p>,将求最大值问题改为等价的求最小值问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\min _{R \in C}-H(P)=\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)\\
&\text { s.t. } \quad E_{P}\left(f_{i}\right)-E_{\beta}\left(f_{i}\right)=0, \quad i=1,2, \cdots, n\\
&\sum_{y} P(y | x)=1
\end{aligned}</script><p>引入<strong>拉格朗日乘子</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
L(P, w) & \equiv-H(P)+w_{0}\left(1-\sum_{y} P(y | x)\right)+\sum_{i=1}^{n} w_{i}\left(E_{p}\left(f_{i}\right)-E_{P}\left(f_{i}\right)\right) \\
&=\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)+w_{0}\left(1-\sum_{y} P(y | x)\right) \\
&+\sum_{i=1}^{n} w_{i}\left(\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P(y | x) f_{i}(x, y)\right)
\end{aligned}</script><p>将原始问题$\min _{p \in C} \max _{w} L(P, w)$转换为无约束最优化的<strong>对偶问题</strong>$\max _{w} \min _{P \in \mathbf{C}} L(P, w)$.首先求解内部的<strong>极小化问题</strong>,即求$L(P,W)$对$P(y|x)$的偏导数.</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial L(P, w)}{\partial P(y | x)} &=\sum_{x, y} \tilde{P}(x)(\log P(y | x)+1)-\sum_{y} w_{0}-\sum_{x, y}\left(\tilde{P}(x) \sum_{i=1}^{n} w_{i} f_{i}(x, y)\right) \\
&=\sum_{x, y} \tilde{P}(x)\left(\log P(y | x)+1-w_{0}-\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
\end{aligned}</script><p>,并令偏导数等于0,解得$Z_{w}(x)=\sum_{y} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)$.可以证明对偶函数等价于对数似然函数,那么对偶函数极大化等价于最大熵模型的<strong>极大似然估计</strong>$L(w)=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)$.之后可以用最优化算法求解得到w.</p>
<h4 id="逻辑回归与最大熵模型的共同点"><a href="#逻辑回归与最大熵模型的共同点" class="headerlink" title="逻辑回归与最大熵模型的共同点"></a>逻辑回归与最大熵模型的共同点</h4><p>最大熵模型与逻辑斯谛回归模型有类似的形式,它们又称为<strong>对数线性模型</strong>.模型学习就是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计.</p>
<h4 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h4><p>似然函数是<strong>光滑的凸函数</strong>,因此多种最优化方法都适用.</p>
<ol>
<li><strong>改进的迭代尺度法(IIS)</strong>:假设当前的参数向量是w,如果能找到一种方法<strong>w-&gt;w+δ</strong>使对数似然函数值变大,就可以<strong>重复</strong>使用这一方法,直到找到最大值.</li>
<li>逻辑斯谛回归常应用梯度下降法,牛顿法或拟牛顿法.</li>
</ol>
<h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h3><h4 id="模型-4"><a href="#模型-4" class="headerlink" title="模型"></a>模型</h4><p>支持向量机(SVM)是一种<strong>二类分类模型</strong>.它的基本模型是定义在特征空间上的<strong>间隔最大</strong>的线性分类器.支持向量机还包括<strong>核技巧</strong>,使它成为实质上的非线性分类器.<strong>分离超平面</strong>$w^\star x+b^\star =0$,<strong>分类决策函数</strong>$f(s)=sign(w^\star x + b^\star)$.</p>
<h4 id="策略-3"><a href="#策略-3" class="headerlink" title="策略"></a>策略</h4><p><strong>间隔最大化</strong>,可形式化为一个求解<strong>凸二次规划</strong>的问题,也等价于正则化的<strong>合页损失函数</strong>的最小化问题.</p>
<h4 id="数据可分、近似可分、不可分"><a href="#数据可分、近似可分、不可分" class="headerlink" title="数据可分、近似可分、不可分"></a>数据可分、近似可分、不可分</h4><p>当训练数据<strong>线性可分</strong>时,通过硬间隔最大化,学习出<strong>线性可分支持向量机</strong>.当训练数据<strong>近似线性可分</strong>时,通过软间隔最大化,学习出<strong>线性支持向量机</strong>.当训练数据<strong>线性不可分</strong>时,通过使用核技巧及软间隔最大化,学习<strong>非线性支持向量机</strong>.</p>
<h4 id="核技巧"><a href="#核技巧" class="headerlink" title="核技巧"></a>核技巧</h4><p>当输入空间为欧式空间或离散集合,特征空间为希尔伯特空间时,核函数表示将输入从输入空间<strong>映射</strong>到特征空间得到的特征向量之间的<strong>内积</strong>.通过核函数学习非线性支持向量机等价于在高维的特征空间中学习线性支持向量机.这样的方法称为核技巧.</p>
<h4 id="输入空间和特征空间"><a href="#输入空间和特征空间" class="headerlink" title="输入空间和特征空间"></a>输入空间和特征空间</h4><p>考虑一个二类分类问题,假设输入空间与特征空间为两个不同的空间,输入空间为<strong>欧氏空间或离散集合</strong>,特征空间为<strong>欧氏空间或希尔伯特空间</strong>.支持向量机都将输入映射为特征向量,所以支持向量机的学习是在<strong>特征空间</strong>进行的.</p>
<h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><p>支持向量机的最优化问题一般通过对偶问题化为<strong>凸二次规划问题</strong>求解,具体步骤是将等式约束条件代入优化目标,通过求偏导求得优化目标在不等式约束条件下的极值.</p>
<h4 id="线性可分支持向量机"><a href="#线性可分支持向量机" class="headerlink" title="线性可分支持向量机"></a><strong>线性可分支持向量机</strong></h4><p>当训练数据集线性可分时,存在无穷个分离超平面可将两类数据正确分开.利用<strong>间隔最大化</strong>得到<strong>唯一</strong>最优分离超平面$w^\star x +b = 0$和相应的分类决策函数$f(s)=sign(w^\star x + b^\star)$.称为线性可分支持向量机.</p>
<h5 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h5><p>一般来说,一个点距离分离超平面的<strong>远近</strong>可以表示分类预测的<strong>确信程度</strong>.在超平面$w^\star x +b = 0$确定的情况下,$|wx+b|$能够相对地表示点x距离超平面的远近,而$wx+b$与$y$的符号是否一致能够表示分类是否正确.所以可用$\hat{\gamma}_{i}=y_{i}\left(w \cdot x_{i}+b\right)$来表示分类的正确性及确信度,这就是<strong>函数间隔</strong>.注意到即使超平面不变,函数间隔仍会受w和b的绝对大小影响.</p>
<h5 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a><strong>几何间隔</strong></h5><p>一般地,当样本点被超平面正确分类时,点x与超平面的距离是$\gamma_{i}=y_{i}\left(\frac{w}{|w|} \cdot x_{i}+\frac{b}{|w|}\right)$其中$||w||$是$w$的$l2$范数.这就是<strong>几何间隔</strong>的定义.定义超平面关于训练数据集T的几何间隔为超平面关于T中所有样本点的几何间隔之<strong>最小值</strong>$\gamma=\min _{i,…,N} \gamma_{1}$.可知$\gamma=\frac{\hat{\gamma}}{|\boldsymbol{w}|}$当$||w||=1$时几何间隔和函数间隔<strong>相等</strong>.</p>
<h5 id="硬间隔最大化"><a href="#硬间隔最大化" class="headerlink" title="硬间隔最大化"></a><strong>硬间隔最大化</strong></h5><p>对线性可分的训练集而言,这里的间隔最大化又称为<strong>硬间隔最大化</strong>.直观解释是对训练集找到几何间隔最大的超平面意味着以<strong>充分大的确信度</strong>对训练数据进行分类.求最大间隔分离超平面即约束最优化问题:</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\max _{w, b} \quad \gamma\\
&\text { s.t. } \quad y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right) \geqslant \gamma, \quad i=1,2, \cdots, N
\end{aligned}</script><p>,将几何间隔用函数间隔表示:</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\max _{w, b} \frac{\hat{\gamma}}{\|w\|}\\
&\text { s.t. } \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant \hat{p}, \quad i=1,2, \cdots, N
\end{aligned}</script><p>,并且注意到函数间隔的取值并不影响最优化问题的解,不妨令函数间隔=1,并让最大化$\frac{1}{||w||}$等价为最小化$\frac{||w||^2}{2}$,问题变为<strong>凸二次规划问题</strong></p>
<script type="math/tex; mode=display">
\min _{w, b} \frac{1}{2}\|w\|^{2} \\
s.t. \quad y_{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N</script><h5 id="支持向量和间隔边界"><a href="#支持向量和间隔边界" class="headerlink" title="支持向量和间隔边界"></a><strong>支持向量和间隔边界</strong></h5><p>与分离超平面距离<strong>最近的样本点</strong>的实例称为<strong>支持向量</strong>.支持向量是使最优化问题中的约束条件等号成立的点.因此对$y=+1$的正例点和$y=-1$的负例点,支持向量分别在超平面H1:$wx+b=+1$和H2:$wx+b=-1$.H1和H2平行,两者之间形成一条长带,长带的宽度!称$\frac{2}{||w||}$为<strong>间隔</strong>,H1和H2称为<strong>间隔边界</strong>.在决定分离超平面时只有支持向量起作用,所以支持向量机是由很少的”重要的”训练样本确定的.由对偶问题同样可以得到支持向量一定在间隔边界上.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/support_vector.png" alt="avatar"></p>
<h5 id="对偶算法"><a href="#对偶算法" class="headerlink" title="对偶算法"></a><strong>对偶算法</strong></h5><p>引进拉格朗日乘子,定义拉格朗日函数$L(w, b, \alpha)=\frac{1}{2}|w|^{2}-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}$,根据拉格朗日对偶性,原始问题的对偶问题是极大极小问题:$\max _{\alpha} \min _{w, b} L(w, b, \alpha)$.先求对w,b的<strong>极小值</strong>.将$L(w,b,a)$分别对w,b求偏导数并令其等于0,得</p>
<script type="math/tex; mode=display">
\begin{array}{l}
w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} \\
\sum_{i=1}^{N} \alpha_{i} y_{i}=0
\end{array}</script><p>,代入拉格朗日函数得</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(w, b, \alpha) &=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j}\right) \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i} \\
&=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}
\end{aligned}</script><p>这就是极小值.接下来对极小值求对a的极大,即是<strong>对偶问题</strong></p>
<script type="math/tex; mode=display">
\begin{array}{l}
\max _{a}-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
\quad \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}</script><p>.将求极大转换为求极小</p>
<script type="math/tex; mode=display">
\begin{array}{cl}
\min _{\alpha} & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } & \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
& \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}</script><p>.由<strong>KKT条件</strong>成立得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
&w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}\\
&b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i} \cdot x_{j}\right)
\end{aligned}</script><p>,其中$j$为使$a_j^<em>&gt;0$的下标之一.所以问题就变为求对偶问题的解$a^</em>$,再求得原始问题的解$w^<em>,b^</em>$,从而得分离超平面及分类决策函数可以看出$w^<em>$和$b^</em>$都只依赖训练数据中$a_i^<em>&gt;0$的样本点Z$(x_i,y_i)$,这些实例点$x_i$被称为<em>*支持向量</em></em>.</p>
<h4 id="线性支持向量机"><a href="#线性支持向量机" class="headerlink" title="线性支持向量机"></a><strong>线性支持向量机</strong></h4><p>如果训练数据是<strong>线性不可分</strong>的(近似线性可分),那么上述方法中的不等式约束并不能都成立,需要修改硬间隔最大化,使其成为<strong>软间隔最大化</strong>.</p>
<h5 id="松弛变量"><a href="#松弛变量" class="headerlink" title="松弛变量"></a>松弛变量</h5><p>线性不可分意味着某些<strong>特异点</strong>不能满足函数间隔大于等于1的约束条件,可以对每个样本点引进一个<strong>松弛变量</strong>,使函数间隔加上松弛变量大于等于1,约束条件变为$y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}$,同时对每个松弛变量,支付一个代价,目标函数变为$\frac{1}{2}|w|^{2}+C \sum_{i=1}^{N} \xi_{i}$,其中$C&gt;0$称为<strong>惩罚参数</strong>,C值越大对误分类的惩罚也越大.新目标函数包含了两层含义:使<strong>间隔尽量大</strong>,同时使误分类点的<strong>个数尽量小</strong>.</p>
<h5 id="软间隔最大化"><a href="#软间隔最大化" class="headerlink" title="软间隔最大化"></a>软间隔最大化</h5><p>学习问题变成如下<strong>凸二次规划</strong>问题:</p>
<script type="math/tex; mode=display">
\min _{w, b, k} \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i} \\
s.t. \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N \\
\xi_{i} \geqslant 0, \quad i=1,2, \cdots, N</script><p>,可以证明w的解是唯一的,但b的解存在一个<strong>区间</strong>.线性支持向量机包含线性可分支持向量机,因此<strong>适用性更广</strong>.</p>
<h5 id="对偶算法-1"><a href="#对偶算法-1" class="headerlink" title="对偶算法"></a>对偶算法</h5><p>原始问题的对偶问题是,构造<strong>拉格朗日函数</strong> $L(w, b, \xi, \alpha, \mu) \equiv \frac{1}{2}|w|^{2}+C \sum_{i=1}^{N} \xi_{i}-\sum_{i=1}^{N} \alpha_{i}\left(y_{i}\left(w \cdot x_{i}+b\right)-1+\xi_{i}\right)-\sum_{i=1}^{N} \mu_{i} \xi_{i}$ ，先求对w,b,ξ的<strong>极小值</strong>,分别求偏导并令导数为0,得</p>
<script type="math/tex; mode=display">
\begin{aligned}
&w=\sum_{i=1} \alpha_{i} y_{i} x_{i}\\
&\sum_{i=1}^{N} \alpha_{i} y_{i}=0\\
&C-\alpha_{i}-\mu_{i}=0
\end{aligned}</script><p>,代入原函数,再对极小值求a的<strong>极大值</strong>,得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\max _{a}-\frac{1}{2} \sum_{i=1}^{N} \sum_{i=1}^{N} \alpha_{i} \alpha, y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}\\
&\text { s.t. } \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0\\
&\begin{array}{l}
C-\alpha_{i}-\mu_{i}=0 \\
\alpha_{i} \geqslant 0 \\
\mu_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
\end{aligned}</script><p>,利用后三条约束<strong>消去μ</strong>,再将求极大转换为<strong>求极小</strong>,得到<strong>对偶问题</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
&\min _{\alpha} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}\\
&\text { s.t. } \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0\\
&0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{aligned}</script><p>由<strong>KKT条件</strong>成立可以得到</p>
<script type="math/tex; mode=display">
\begin{array}{c}
w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \\
b^{*}=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{*}\left(x_{i} \cdot x_{j}\right)
\end{array}</script><p>$j$是满足$0&lt;\alpha_j^<em><C$的下标之一.问题就变为选择惩罚参数$C>0$,求得对偶问题(<strong>凸二次规划问题</strong>)的**最优解$\alpha^</C$的下标之一.问题就变为选择惩罚参数$C></em>$<strong>,代入计算$w^<em>$和$b^</em>$,求得分离超平面和分类决策函数.因为$b$的解并不唯一,所以实际计算$b^*$时可以取所有样本点上的</strong>平均值**.</p>
<h5 id="支持向量"><a href="#支持向量" class="headerlink" title="支持向量"></a>支持向量</h5><p>在<strong>线性不可分</strong>的情况下,将对应与$\alpha_i^<em>&gt;0$的样本点$(x_i,y_i)$的实例点$x_i$称为<em>*支持向量</em></em>.软间隔的支持向量或者在间隔边界上,或者在间隔边界与分类超平面之间,或者再分离超平面误分一侧.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/support_soft.png" alt="avatar"></p>
<h5 id="合页损失"><a href="#合页损失" class="headerlink" title="合页损失"></a>合页损失</h5><p>可以认为是0-1损失函数的上界,而线性支持向量机可以认为是优化合页损失函数构成的目标函数.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/0_1loss.png" alt="avatar"></p>
<h4 id="非线性支持向量机"><a href="#非线性支持向量机" class="headerlink" title="非线性支持向量机"></a><strong>非线性支持向量机</strong></h4><p>如果分类问题是<strong>非线性</strong>的,就要使用<strong>非线性支持向量机</strong>.主要特点是使用<strong>核技巧</strong>.</p>
<h5 id="非线性分类问题"><a href="#非线性分类问题" class="headerlink" title="非线性分类问题"></a><strong>非线性分类问题</strong></h5><p>用线性分类方法求解非线性分类问题分为两步:首先使用一个变换将原空间的数据映射到新空间,然后在新空间里用线性分类学习方法从训练数据中学习分类模型.</p>
<h5 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a><strong>核函数</strong></h5><p>设X是<strong>输入空间</strong>(欧式空间的子集或离散集合),H为<strong>特征空间</strong>(希尔伯特空间),一般是<strong>高维</strong>甚至无穷维的.如果存在一个从X到H的映射$\phi(x): \mathcal{X} \rightarrow \mathcal{H}$使得对所有x,z属于X,函数K(x,z)满足条件$K(x, z)=\phi(x) \cdot \phi(z)$,点乘代表<strong>内积</strong>,则称K(x,z)为<strong>核函数</strong>.</p>
<h5 id="核技巧-1"><a href="#核技巧-1" class="headerlink" title="核技巧"></a><strong>核技巧</strong></h5><p>基本思想是通过一个<strong>非线性变换</strong>将输入空间对应于一个<strong>特征空间</strong>,使得在输入空间中的<strong>超曲面模型</strong>对应于特征空间中的<strong>超平面模型</strong>(支持向量机).在学习和预测中只定义核函数$K(x,z)$,而<strong>不显式</strong>地定义映射函数.对于给定的核$K(x,z)$,特征空间和映射函数的取法并<strong>不唯一</strong>.注意到在线性支持向量机的对偶问题中,目标函数和决策函数都只涉及输入实例与实例之间的<strong>内积</strong>,$x_i,x_j$可以用核函数$K(x_i,x_j)=\phi (x_i)\phi (x_j)$来<strong>代替</strong>.当映射函数是非线性函数时,学习到的含有核函数的支持向量机是非线性分类模型.在实际应用中,往往依赖领域知识<strong>直接选择</strong>核函数.</p>
<h5 id="正定核"><a href="#正定核" class="headerlink" title="正定核"></a><strong>正定核</strong></h5><p>通常所说的核函数是指<strong>正定核函数</strong>.只要满足正定核的充要条件,那么给定的函数K(x,z)就是正定核函数.设K是定义在X<em>X上的<strong>对称函数</strong>,如果任意xi属于X,K(x,z)对应的<strong>Gram矩阵</strong>$K=\left[K\left(x_{i}, x_{j}\right)\right]_{mxm}$是<em>*半正定矩阵</em></em>,则称$K(x,z)$是正定核.这一定义在构造核函数时很有用,但要验证一个具体函数是否为正定核函数并不容易,所以在实际问题中往往应用已有的核函数.</p>
<h5 id="算法-4"><a href="#算法-4" class="headerlink" title="算法"></a><strong>算法</strong></h5><p>选取适当的核函数K(x,z)和适当的参数C,将线性支持向量机对偶形式中的内积换成核函数,构造并求解最优化问题</p>
<script type="math/tex; mode=display">
\min _{a} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
s.t. \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N</script><p>,选择最优解$a^<em>$的一个正分量$0&lt;a_j^</em>&lt;C$计算$b^{<em>}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{</em>} y_{i} K\left(x_{i} \cdot x_{j}\right)$,构造决策函数$f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{<em>} y_{i} K\left(x \cdot x_{i}\right)+b^{</em>}\right)$</p>
<h5 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a><strong>常用核函数</strong></h5><ol>
<li><strong>多项式核函数(polynomial kernel function)</strong> :$K(x, z)=(x \cdot z+1)^{p}$,对应的支持向量机是一个p次多项式分类器,分类决策函数为:$f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{i}} a_{i}^{<em>} y_{i}\left(x_{i} \cdot x+1\right)^{p}+b^{</em>}\right)$</li>
<li><strong>高斯核函数(Gaussian krenel function)</strong> :$K(x, z)=\exp \left(-\frac{|x-z|^{2}}{2 \sigma^{2}}\right)$ ,对应的支持向量机是高斯径向基函数(RBF)分类器.分类决策函数为$f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{t}} a_{i}^{<em>} y_{i} \exp \left(-\frac{|x-z|^{2}}{2 \sigma^{2}}\right)+b^{</em>}\right)$</li>
<li><strong>字符串核函数(string kernel function)</strong>: 核函数不仅可以定义在欧氏空间上,还可以定义在<strong>离散数据的集合</strong>上.字符串核函数给出了字符串中长度等于n的所有子串组成的特征向量的余弦相似度.</li>
</ol>
<h5 id="序列最小最优化-SMO-算法"><a href="#序列最小最优化-SMO-算法" class="headerlink" title="序列最小最优化(SMO)算法"></a><strong>序列最小最优化(SMO)算法</strong></h5><p>SMO是一种<em>快速求解凸二次规划问题</em></p>
<script type="math/tex; mode=display">
\begin{aligned}
&\min _{\alpha} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}\\
&\text { s.t. } \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0\\
&0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{aligned}</script><p>的算法.基本思路是:如果所有变量都满足此优化问题的KKT条件,那么解就得到了.否则,选择<strong>两个变量</strong>,固定其他变量,针对这两个变量构建一个二次规划问题.不断地将原问题分解为<strong>子问题</strong>并对子问题求解,就可以求解原问题.注意子问题两个变量中只有一个是<strong>自由变量</strong>,另一个由<strong>等式约束</strong>确定.</p>
<h5 id="两个变量二次规划的求解方法"><a href="#两个变量二次规划的求解方法" class="headerlink" title="两个变量二次规划的求解方法"></a><strong>两个变量二次规划的求解方法</strong></h5><p>假设选择的两个变量是a1,a2,其他变量是固定的,于是得到子问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\begin{aligned}
\min _{\alpha, \alpha_{i}} W\left(\alpha_{1}, \alpha_{2}\right)=& \frac{1}{2} K_{11} \alpha_{1}^{2}+\frac{1}{2} K_{22} \alpha_{2}^{2}+y_{1} y_{2} K_{12} \alpha_{1} \alpha_{2} \\
&-\left(\alpha_{1}+\alpha_{2}\right)+y_{1} \alpha_{1} \sum_{i=1}^{N} y_{i} \alpha_{i} K_{n}+y_{2} \alpha_{2} \sum_{i=1}^{N} y_{i} \alpha_{i} K_{i}
\end{aligned}\\
&\text { s.t. } \quad \alpha_{1} y_{1}+\alpha_{2} y_{2}=-\sum_{i=3}^{N} y_{i} \alpha_{i}=\zeta\\
&0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2
\end{aligned}</script><p>,$\epsilon$是常数,目标函数式省略了不含$a_1,a_2$的常数项.考虑不等式约束和等式约束,要求的是目标函数在一条平行于对角线的线段上的最优值</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/1.png" alt="avatar"></p>
<p>问题变为<strong>单变量</strong>的最优化问题.假设初始可行解为aold,最优解为anew,考虑沿着约束方向未经剪辑的最优解anew,unc(即未考虑不等式约束).对该问题求偏导数,并令导数为0,代入原式,令$E_{i}=g\left(x_{i}\right)-y_{i}=\left(\sum_{j=1}^{N} \alpha, y_{j} K\left(x_{j}, x_{i}\right)+b\right)-y_{i}, \quad i=1,2$,得到$\alpha_{2}^{\text {new, } \text { unc }}=\alpha_{2}^{\text {old }}+\frac{y_{2}\left(E_{1}-E_{2}\right)}{\eta}$,经剪辑后a2的解是</p>
<script type="math/tex; mode=display">
\alpha_{2}^{\text {new }}=\left\{\begin{array}{ll}H, & \alpha_{2}^{\text {new }, \text { unc }}>H \\ \alpha_{2}^{\text {new }, \text { unc }}, & L \leqslant \alpha_{2}^{\text {new}, \text { unc}} \leqslant H \\ L, & \alpha_{2}^{\text {new }, \text { unc }}<L\end{array}\right.</script><p>L与H是$a_2^{new}$所在的对角线段端点的界.并解得$\alpha_{1}^{\mathrm{new}}=\alpha_{1}^{\mathrm{old}}+y_{1} y_{2}\left(\alpha_{2}^{\mathrm{old}}-\alpha_{2}^{\mathrm{new}}\right)$</p>
<h5 id="变量的选择方法"><a href="#变量的选择方法" class="headerlink" title="变量的选择方法"></a><strong>变量的选择方法</strong></h5><p>在每个子问题中选择两个变量优化,其中至少一个变量是违反KKT条件的.第一个变量的选取标准是<strong>违反KKT条件最严重</strong>的样本点,第二个变量的选取标准是希望能使该变量有<strong>足够大的变化</strong>,一般可以选取使对应的$|E1-E2|$最大的点.在每次选取完点后,<strong>更新</strong>阈值$b$和差值$Ei$.</p>
<h3 id="提升方法"><a href="#提升方法" class="headerlink" title="提升方法"></a>提升方法</h3><h4 id="提升方法-1"><a href="#提升方法-1" class="headerlink" title="提升方法"></a>提升方法</h4><p>boosting是<strong>一种常用的统计学习方法,是集成学习的一种.它通过改变训练样本的权重(概率分布),学习</strong>多个<strong>弱分类器(基本分类器),并将这些分类器</strong>线性组合**来构成一个强分类器提高分类的性能.</p>
<h4 id="加法模型和前向分步算法"><a href="#加法模型和前向分步算法" class="headerlink" title="加法模型和前向分步算法"></a>加法模型和前向分步算法</h4><p>加法模型$f(x)= \sum_{m=1}^M{\beta_mb(x;\gamma_m)}$,其中，$b(x;\gamma_m)$为基函数，$\gamma_m$为基函数参数，$\beta_m$为基函数的系数。在给定训练数据及损失函数$L(y,f(x))$的条件下，学习加法模型$f(x)$成为经验风险极小化即损失函数极小化问题：</p>
<script type="math/tex; mode=display">
min_{\beta_m,\gamma_m}\sum_{m=1}^{M}{\beta_m b(x_i;\gamma_m)}</script><p>通常这是一个复杂的优化问题。</p>
<p>前向分步算法求解这一优化问题的想法是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数：$\left(\beta_{m}, \gamma_{m}\right)=\arg \min _{\beta, y} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\beta b\left(x_{i} ; \gamma\right)\right)$，得到参数βm和γm,更新$f_{m}(x)=f_{m-1}(x)+\beta_{m} b\left(x ; \gamma_{m}\right)$,逐步逼近优化目标函数式，那么就可以简化优化的复杂度，最终得到加法模型。</p>
<h4 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a><strong>AdaBoost</strong></h4><h5 id="思想："><a href="#思想：" class="headerlink" title="思想："></a>思想：</h5><p>AdaBoost提高那些被前一轮弱分类器错误分类样本的权值,而降低那些被正确分类样本的权值.然后采取<strong>加权多数表决</strong>的方法组合弱分类器.</p>
<h5 id="算法-5"><a href="#算法-5" class="headerlink" title="算法"></a><strong>算法</strong></h5><p>首先假设训练数据集具有均匀的权值分布D1,使用具有<strong>权值分布</strong>Dm的训练数据集学习得到<strong>基本分类器</strong>Gm(x),计算<strong>分类误差率</strong>$e_{m}=P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)$和Gm(x)的<strong>系数</strong>$\alpha_{m}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}$,更新训练数据集的权值分布$D_{m+1}=\left(w_{m+1,1}, \cdots, w_{m+1, i}, \cdots, w_{m+1, N}\right)$，其中</p>
<script type="math/tex; mode=display">
w_{m+1, i}=\left\{\begin{array}{ll}
\frac{w_{mi}}{Z_{m}} \mathrm{e}^{-\alpha_{m}}, & G_{m}\left(x_{i}\right)=y_{i} \\
\frac{w_{m i}}{Z_{m}} \mathrm{e}^{\alpha_{m}}, & G_{m}\left(x_{i}\right) \neq y_{i}
\end{array}\right.</script><p>$Z_m$是使Dm+1成为概率分布的<strong>规范化因子</strong>$Z_{m}=\sum_{i=1}^{N} w_{m i} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right)$.重复上述操作M次后得到M个弱分类器,构建线性组合得到<strong>最终分类器</strong>$G(x)=\operatorname{sign}(f(x))=\operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)$</p>
<h4 id="提升树"><a href="#提升树" class="headerlink" title="提升树"></a><strong>提升树</strong></h4><p>提升树是模型为加法模型,算法为前向分布算法,基函数为<strong>决策树</strong>的提升方法.第m步的模型是$f_{m}(x)=f_{m-1}(x)+T\left(x ; \Theta_{m}\right)$,通过经验风险极小化确定下一棵决策树的参数$\hat{\Theta}_{m}=\arg \min _{\theta_{m}} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \Theta_{m}\right)\right)$.不同问题的提升树学习算法主要区别在于使用的<strong>损失函数</strong>不同.</p>
<h5 id="二类分类问题"><a href="#二类分类问题" class="headerlink" title="二类分类问题"></a><strong>二类分类问题</strong></h5><p>只需将AdaBoost算法中的基本分类器限制为二类分类数即可.</p>
<h5 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a><strong>回归问题</strong></h5><p>如果将输入空间划分为J个互不相交的区域,并且在每个区域上确定输出的常量$C_j$,那么树可表示为$T(x ; \Theta)=\sum_{j=1}^{J} c_{j} I\left(x \in R_{j}\right)$,其中$\Theta=\left\{\left(R_{1}, c_{1}\right),\left(R_{2}, c_{2}\right), \cdots,\left(R_{J}, c_{J}\right)\right\}$提升树采用<strong>前向分步算法</strong>:</p>
<script type="math/tex; mode=display">
\begin{aligned}
&f_{0}(x)=0\\
&\begin{array}{l}
f_{m}(x)=f_{m-1}(x)+T\left(x ; \Theta_{m}\right), m=1,2, \cdots, M \\
f_{M}(x)=\sum_{m=1}^{M} T\left(x ; \Theta_{m}\right)
\end{array}
\end{aligned}</script><p>.当采用平方误差损失函数时,损失变为</p>
<script type="math/tex; mode=display">
\begin{array}{l}
L\left(y, f_{m-1}(x)+T\left(x ; \Theta_{m}\right)\right) \\
\quad=\left[y-f_{m-1}(x)-T\left(x ; \Theta_{m}\right)\right]^{2} \\
\quad=\left[r-T\left(x ; \Theta_{m}\right)\right]^{2}
\end{array}</script><p>,其中r是当前模型拟合数据的<strong>残差</strong>.每一步都只需<strong>拟合残差</strong>学习一个回归树即可.</p>
<h5 id="存在的缺点"><a href="#存在的缺点" class="headerlink" title="存在的缺点"></a>存在的缺点</h5><p>当损失函数式平方误差损失函数或者交叉熵损失函数的时候，残差即为对应的梯度，这个时候损失函数沿着梯度的方向下降最快。当损失函数式其他损失函数的时候，残差和损失函数的导数并不相等，函数收敛的速度就会没有沿着梯度的方向收敛的快。</p>
<h4 id="梯度提升树-GBDT"><a href="#梯度提升树-GBDT" class="headerlink" title="梯度提升树(GBDT)"></a>梯度提升树(GBDT)</h4><p>利用最速下降法的近似方法来实现每一步的优化,关键在于用损失函数的<strong>负梯度</strong>在当前模型的值$-\left[\frac{\partial L\left(y, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right]_{f(x)=f_{-1}(x)}$作为回归问题中提升树算法中的残差的<strong>近似值</strong>,每一步以此来估计回归树叶结点区域以拟合残差的近似值,并利用线性搜索估计叶结点区域的值使损失函数最小化,然后更新回归树即可.</p>
<h4 id="Xgboost"><a href="#Xgboost" class="headerlink" title="Xgboost"></a>Xgboost</h4><p>相比传统GBDT有以下优点:</p>
<ol>
<li>在优化时用到了二阶导数信息.</li>
<li>在代价函数里加入了正则项.</li>
<li>每次迭代后都将叶子结点的权重乘上一个系数,削弱每棵树的影响.</li>
<li>列抽样.</li>
<li>在训练前对数据进行排序,保存为block结构,并行地对各个特征进行增益计算.</li>
</ol>
<h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>EM算法是一种<strong>迭代</strong>算法,用于含有<strong>隐变量</strong>的概率模型参数的极大似然估计.每次迭代由两步组成:E步,求<strong>期望</strong>(expectation),M步,求<strong>极大值</strong>(maximization),直至收敛为止.</p>
<h4 id="隐变量"><a href="#隐变量" class="headerlink" title="隐变量"></a><strong>隐变量</strong></h4><p>不能被直接观察到，但是对系统的状态和能观察到的输出存在影响的一种东西.</p>
<h4 id="算法-6"><a href="#算法-6" class="headerlink" title="算法"></a><strong>算法</strong></h4><ol>
<li>选择参数的初始值θ(0),开始迭代.注意EM算法对初值是<strong>敏感</strong>的.</li>
<li><strong>E步</strong>:θ(i)为第i次迭代参数θ的估计值,在第i+1次迭代的E步,计算$\begin{aligned} Q\left(\theta, \theta^{(i)}\right) &amp;=E_{z}\left[\log P(Y, Z | \theta) | Y, \theta^{(i)}\right] =\sum_{z} \log P(Y, Z | \theta) P\left(Z | Y, \theta^{(i)}\right) \end{aligned}$ ,$P(Z|Y,θ(i))$是在给定<strong>观测数据</strong>Y和当前参数估计θ(i)下<strong>隐变量数据</strong>Z的条件概率分布.</li>
<li><strong>M步</strong>:求使Q(θ,θ(i))极大化的θ,确定第i+1次迭代的参数的估计值$\theta^{(i+1)}=\arg \max _{\theta} Q\left(\theta, \theta^{(i)}\right)$</li>
<li>重复2和3直到<strong>收敛</strong>,一般是对较小的正数$\varepsilon1$和$\varepsilon2$满足$\left|\theta^{(i+1)}-\theta^{(i)}\right|&lt;\varepsilon_{1} \quad$ 或$\quad\left|Q\left(\theta^{(i+1)}, \theta^{(i)}\right)-Q\left(\theta^{(i)}, \theta^{(i)}\right)\right|&lt;\varepsilon_{2}$则停止迭代.</li>
</ol>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>EM算法是通过不断求解<strong>下界</strong>的极大化逼近求解对数似然函数极大化的算法.可以用于生成模型的<strong>非监督学习</strong>.生成模型由联合概率分布P(X,Y)表示.X为观测数据,Y为未观测数据.</p>
<h3 id="隐马尔科夫模型-HMM"><a href="#隐马尔科夫模型-HMM" class="headerlink" title="隐马尔科夫模型(HMM)"></a>隐马尔科夫模型(HMM)</h3><p>隐马尔可夫模型是关于<strong>时序</strong>的概率模型,描述由一个隐藏的马尔可夫链随机生成不可观测的<strong>状态序列</strong>,再由各个状态生成一个观测而产生<strong>观测随机序列</strong>的过程.HMM是一种生成式模型，它的理论基础是朴素贝叶斯，本质上就类似于我们将朴素贝叶斯在单样本分类问题上的应用推广到序列样本分类问题上。</p>
<h4 id="模型表示"><a href="#模型表示" class="headerlink" title="模型表示"></a>模型表示</h4><p>设Q是所有可能的状态的集合$Q=\left\{q_{1}, q_{2}, \cdots, q_{N}\right\}$,V是所有可能的观测的集合$V=\left\{v_{1}, v_{2}, \cdots, v_{M}\right\}$,I是长度为T的状态序列$I=\left(i_{1}, i_{2}, \cdots, i_{T}\right)$, O是对应的观测序列$O=\left(o_{1}, o_{2}, \cdots, o_{T}\right)$,</p>
<ol>
<li>A是<strong>状态转移概率矩阵</strong>$A=\left[a_{i j}\right]_{N \times N}$,$a_{ij}$表示在时刻t处于状态$q_i$的条件下在时刻t+1转移到状态$q_j$的概率.</li>
<li>.B是<strong>观测概率矩阵</strong> $B=\left[b_{j}(k)\right]_{N \times M}$,$b_{ij}$是在时刻t处于状态$q_j$的条件下生成观测$v_k$的概率.</li>
<li>$\pi$是<strong>初始状态概率向量</strong>$\pi=\pi(x)$,$\pi_i$表示时刻t=1处于状态qi的概率.</li>
</ol>
<p>隐马尔可夫模型由初始状态概率向量$pi$,状态转移概率矩阵A以及观测概率矩阵B确定.$\pi$和A决定即隐藏的<strong>马尔可夫链</strong>,生成不可观测的<strong>状态序列</strong>.B决定如何从状态生成观测,与状态序列综合确定了<strong>观测序列</strong>.因此,隐马尔可夫模型可以用<strong>三元符号</strong>表示$\lambda = (A,B,\pi)$</p>
<h4 id="两个基本假设"><a href="#两个基本假设" class="headerlink" title="两个基本假设"></a>两个基本假设</h4><ol>
<li><strong>齐次马尔可夫性假设</strong>:假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态.</li>
<li><strong>观测独立性假设</strong>:假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态.</li>
</ol>
<h4 id="三个基本问题"><a href="#三个基本问题" class="headerlink" title="三个基本问题"></a>三个基本问题</h4><h5 id="1-概率计算问题"><a href="#1-概率计算问题" class="headerlink" title="1. 概率计算问题"></a><strong>1. 概率计算问题</strong></h5><p>给定模型$\lambda = (A,B,\pi)$和观测序列,$O=\left(o_{1}, o_{2}, \cdots, o_{T}\right)$计算在模型$\lambda$下观测序列O出现的概率$P(O|λ)$.</p>
<ol>
<li>直接计算：列举所有可能长度为T的状态序列,求各个状态序列I与观测序列O的联合概率,但计算量太大,实际操作不可行.</li>
<li><strong>前向算法</strong>：定义到时刻t部分观测序列为$o_1$~$o_t$且状态为$q_i$的概率为<strong>前向概率</strong>,记作$\alpha_{t}(i)=P\left(o_{1}, o_{2}, \cdots, o_{t}, i_{t}=q_{i} | \lambda\right)$.初始化前向概率$\alpha_{1}(i)=\pi_{i} b_{i}\left(o_{1}\right), \quad i=1,2, \cdots, N$，递推，对$t=1$ ~ $T-1$,$\alpha_{t+1}(i)=\left[\sum_{j=1}^{N} \alpha_{t}(j) a_{j i}\right] b_{i}\left(o_{t+1}\right), \quad i=1,2, \cdots, N$,得到$P(O | \lambda)=\sum_{i=1}^{N} \alpha_{T}(i)$减少计算量的原因在于每一次计算直接引用前一个时刻的计算结果,避免重复计算.</li>
<li><strong>后向算法</strong>:定义在时刻t状态为$q_i$的条件下,从t+1到T的部分观测序列为$o_{i+1}$~$o_T$的概率为<strong>后向概率</strong>,记作$\beta_{t}(i)=P\left(o_{t+1}, o_{t+2}, \cdots, o_{r} | i_{t}=q_{i}, \lambda\right)$.初始化后向概率$\beta_{r}(i)=1, \quad i=1,2, \cdots, N$,递推,对$t=T-1$~$1$$\beta_{t}(i)=\sum_{j=1}^{N} a_{i j} b_{j}\left(o_{i+1}\right) \beta_{i+1}(j), \quad i=1,2, \cdots, N$,得到$P(O | \lambda)=\sum_{i=1}^{N} \pi_{i} b_{i}\left(o_{1}\right) \beta_{1}(i)$</li>
</ol>
<h5 id="2-学习算法"><a href="#2-学习算法" class="headerlink" title="2. 学习算法"></a><strong>2. 学习算法</strong></h5><p>已知观测序列$O=(o_1,o_2, \cdots,o_r)$,估计模型$\lambda = (A,B,\pi)$,的参数,使得在该模型下观测序列概率$p(O|\lambda)$最大.根据训练数据是否包括观察序列对应的状态序列分别由监督学习与非监督学习实现.</p>
<ol>
<li><p>监督学习：估计转移概率$\hat{a}_{i j}=\frac{A_{i j}}{\sum_{j=1}^{N} A_{i j}}, \quad i=1,2, \cdots, N ; \quad j=1,2, \cdots, N$ 和观测概率$\hat{b}_{j}(k)=\frac{B_{j k}}{\sum_{k=1}^{M} B_{j k}}, \quad j=1,2, \cdots, N, k=1,2, \cdots, M$.初始状态概率$\pi_i$的估计为S个样本中初始状态为$q_i$的频率.</p>
</li>
<li><p><strong>非监督学习(Baum-Welch算法)</strong>:将观测序列数据看作观测数据O,状态序列数据看作不可观测的<strong>隐数据</strong>I.首先确定完全数据的对数似然函数$log p(O,I|\lambda)$,求Q函数</p>
<script type="math/tex; mode=display">
\begin{aligned}
Q(\lambda, \bar{\lambda})=& \sum_{t} \log \pi_{i_1} P(O, I | \bar{\lambda}) \\
&+\sum_{I}\left(\sum_{i=1}^{I-1} \log a_{i_t,i_{t+1}}\right) P(O, I | \bar{\lambda}) \\
&+\sum_{I}\left(\sum_{i=1}^{T} \log b_{i_t}\left(o_{t}\right)\right) P(O, I | \bar{\lambda})
\end{aligned}</script><p>,用拉格朗日乘子法极大化Q函数求模型参数$\pi_{i}=\frac{P\left(O, i_{1}=i | \bar{\lambda}\right)}{P(O | \bar{\lambda})}$,$a_{i j}=\frac{\sum_{i=1}^{T-1} P\left(O, i_{t}=i, i_{t+1}=j | \bar{\lambda}\right)}{\sum_{t=1}^{T-1} P\left(O, i_{t}=i | \bar{\lambda}\right)}$,$b_{j}(k)=\frac{\sum_{i=1}^{T} P\left(O, i_{t}=j | \bar{\lambda}\right) I\left(o_{i}=v_{k}\right)}{\sum_{i=1}^{T} P\left(O, i_{t}=j | \bar{\lambda}\right)}$,</p>
</li>
</ol>
<h5 id="3-预测问题"><a href="#3-预测问题" class="headerlink" title="3. 预测问题"></a><strong>3. 预测问题</strong></h5><p>也称为解码问题.已知模型$\lambda = (A,B,\pi)$和观测序列$O=(O_1,O_2,\cdots,O_T)$,求对给定观测序列条件概率$P(I|O)$最大的状态序列$I=(i_1,i_2,\cdots,i_T)$</p>
<ol>
<li><p><strong>近似算法</strong>: 在每个时刻t选择在该时刻最有可能出现的状态$i_t^<em>$,从而得到一个状态序列作为预测的结果.优点是<em>*计算简单</em></em>,缺点是不能保证状态序列整体是最有可能的状态序列</p>
</li>
<li><p><strong>维特比算法</strong>:用<strong>动态规划</strong>求概率最大路径,这一条路径对应着一个状态序列.从t=1开始,递推地计算在时刻t状态为i的各条部分路径的最大概率,直至得到时刻t=T状态为i的各条路径的最大概率.时刻t=T的最大概率即为<strong>最优路径</strong>的概率$P^\star$,最优路径的<strong>终结点</strong>$i_t^\star$也同时得到,之后从终结点开始由后向前逐步求得<strong>结点</strong>,得到最优路径.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(obs, states, Pi, A, B)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param obs:观测序列</span></span><br><span class="line"><span class="string">    :param states:隐状态</span></span><br><span class="line"><span class="string">    :param Pi:初始概率（隐状态）</span></span><br><span class="line"><span class="string">    :param A:转移概率（隐状态）</span></span><br><span class="line"><span class="string">    :param B: 发射概率 （隐状态表现为显状态的概率）</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 路径概率表 V[时间][隐状态] = 概率</span></span><br><span class="line">    V = [&#123;&#125;]</span><br><span class="line">    <span class="comment"># 一个中间变量，代表当前状态是哪个隐状态</span></span><br><span class="line">    path = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化初始状态 (t == 0)</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">        V[<span class="number">0</span>][y] = Pi[y] * B[y][obs[<span class="number">0</span>]]</span><br><span class="line">        path[y] = [y]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对 t &gt; 0 跑一遍维特比算法</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, len(obs)):</span><br><span class="line">        V.append(&#123;&#125;)</span><br><span class="line">        newpath = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">            <span class="comment"># 概率 隐状态 =    前状态是y0的概率 * y0转移到y的概率 * y表现为当前状态的概率</span></span><br><span class="line">            (prob, state) = max([(V[t - <span class="number">1</span>][y0] * A[y0][y] * B[y][obs[t]], y0) <span class="keyword">for</span> y0 <span class="keyword">in</span> states])</span><br><span class="line">            <span class="comment"># 记录最大概率</span></span><br><span class="line">            V[t][y] = prob</span><br><span class="line">            <span class="comment"># 记录路径</span></span><br><span class="line">            newpath[y] = path[state] + [y]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 不需要保留旧路径</span></span><br><span class="line">        path = newpath</span><br><span class="line"></span><br><span class="line">    print_dptable(V)</span><br><span class="line">    (prob, state) = max([(V[len(obs) - <span class="number">1</span>][y], y) <span class="keyword">for</span> y <span class="keyword">in</span> states])</span><br><span class="line">    <span class="keyword">return</span> (prob, path[state])</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 打印路径概率表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_dptable</span><span class="params">(V)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"    "</span>,</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(V)): <span class="keyword">print</span> <span class="string">"%7d"</span> % i,</span><br><span class="line">    <span class="keyword">print</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> V[<span class="number">0</span>].keys():</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"%.5s: "</span> % y,</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(len(V)):</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"%.7s"</span> % (<span class="string">"%f"</span> % V[t][y]),</span><br><span class="line">        <span class="keyword">print</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="最大熵马尔科夫模型-MEMM"><a href="#最大熵马尔科夫模型-MEMM" class="headerlink" title="最大熵马尔科夫模型(MEMM)"></a>最大熵马尔科夫模型(MEMM)</h3><p>最大熵马尔科夫模型利用判别式模型的特点，直接对每一个时刻的状态建立一个分类器，然后将所有的分类器的概率值连乘起来$P\left(y_{1}^{n} | x_{1}^{n}\right)=\prod_{t=1}^{n} P\left(y_{t} | y_{t-1}, x_{t}\right)$。为了实现是对整个序列进行的分类，在每个时刻t时，它的特征不仅来自当前观测值$x_t$，而且还来自前一状态值$y_{t-1}$,通过最大熵分类器建模$P\left(y_{t}=y^{<em>} | y_{t-1}=y^{\prime}, x_{t}\right)=\frac{1}{Z\left(x_{t}, y^{\prime}\right)} \exp \left(\sum_{a} \lambda_{a} f_{a}\left(x_{t}, y^{\prime}, y^{</em>}\right)\right)$,其中，$Z\left(x_{t}, y \prime\right)=\sum_{y} \exp \left(\sum_{a} \lambda_{a} f_{a}\left(x_{t}, y \prime, y\right)\right)$，</p>
<h4 id="标注偏置问题"><a href="#标注偏置问题" class="headerlink" title="标注偏置问题"></a>标注偏置问题</h4><p>使用维特比算法进行解码时，$v_{t}(j)=\max _{i} v_{t-1}(i) * P\left(y_{j} | y_{i}, x_{t}\right) 1 \leq j \leq n, 1&lt;t&lt;T$。最大熵模型在每一个时刻，针对不同的前一状态y′进行归一化操作，这是一种局部的归一化操作，会存在标签偏置问题。</p>
<h5 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h5><p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/label_bias.png" alt="avatar"></p>
<p>状态转换(1→2),(2→3),(4→5),(5→3)的概率值都是1，而无论观测值是什么，换言之有$P(2|1,i)=P(2|1,o)=1$</p>
<p>你可能会很惊讶$P(2∣1,i)=1,P(2∣1,o)=1$怎么可能会成立呢？你可以套用上面的公式试一试，由于状态”1”的只能转换为”2”，所以计算归一化项时， 其实只有一个枚举值，就是状态”2”，所以无论你分子为多少，分母都和它一样，所以概率值就是1。在这种情况下，其实观测值并没有任何作用，这就是标签偏置。</p>
<h5 id="后果："><a href="#后果：" class="headerlink" title="后果："></a><strong>后果：</strong></h5><p>它会造成什么后果呢？<br>他会导致模型进行预测时只依赖数据统计出来的概率值，没有利用到样本的特征。<br>假设训练集现在有3个rib和1个rob，当我们在测试阶段，遇到词rob，它会被解码成什么状态序列呢？答案是(0→1→2→3)！你可以套公式试一试，因为$P(1∣0,r)&gt;P(4∣0,r)$,$P(2∣1,o)=P(5∣4,0)=1,P(3∣2,b)&gt;P(3∣5,b)$。</p>
<h5 id="原因"><a href="#原因" class="headerlink" title="原因"></a><strong>原因</strong></h5><p>那么问题出在哪里呢？因为MEMM中在每一时刻t，都在前一时刻某状态y′下做了局部的归一化操作，如何解决这种标签偏置问题呢？<br>在CRF中并不是对每个时刻都进行一次分类，而是直接对整个序列进行分类，做一个全局的归一化操作。</p>
<h3 id="条件随机场CRF"><a href="#条件随机场CRF" class="headerlink" title="条件随机场CRF"></a>条件随机场CRF</h3><h4 id="概率图模型"><a href="#概率图模型" class="headerlink" title="概率图模型"></a>概率图模型</h4><p>结点表示随机变量，边表示随机变量之间的概率依赖关系。</p>
<h5 id="马尔科夫性"><a href="#马尔科夫性" class="headerlink" title="马尔科夫性"></a>马尔科夫性</h5><ol>
<li>成对马尔可夫性</li>
<li>局部马尔科夫性</li>
<li>全局马尔科夫性</li>
</ol>
<h5 id="概率无向图模型的因式分解"><a href="#概率无向图模型的因式分解" class="headerlink" title="概率无向图模型的因式分解"></a>概率无向图模型的因式分解</h5><h6 id="团与最大团"><a href="#团与最大团" class="headerlink" title="团与最大团"></a>团与最大团</h6><p>无向图G中任何两个结点均有边连接的结点子集称为<strong>团</strong>。若C是无向图G的一个团，并且不能再加进任何一个G的结点使其成为一个更大的团，称此C为<strong>最大团</strong>。</p>
<p>给定概率无向图模型，设其无向图为G，C为G上的最大团，$Y_C$表示C对应的随机变量。那么概率无向图模型的联合概率分布$P(Y)$可写作图中所有最大团C上的函数$\phi_C(Y_C)$的乘积的形式。</p>
<p><strong>Hammersley-Clifford定理</strong><br>概率无向图模型的联合概率分布$P(Y)$可以表示为如下形式：</p>
<script type="math/tex; mode=display">
P(Y)=\frac{1}{Z}\prod_C{\phi_C(Y_C)}\\
Z=\sum_Y\prod_C{\phi_C(Y_C)}</script><h5 id="条件随机场CRF-1"><a href="#条件随机场CRF-1" class="headerlink" title="条件随机场CRF"></a>条件随机场CRF</h5><p>条件随机场基于概率无向图模型，利用最大团理论，对随机变量的联合分布$P(Y)$进行建模。</p>
<p>在序列标注任务中的条件随机场，往往是指<strong>线性链条件随机场</strong></p>
<h6 id="条件随机场的参数化形式"><a href="#条件随机场的参数化形式" class="headerlink" title="条件随机场的参数化形式"></a>条件随机场的参数化形式</h6><script type="math/tex; mode=display">
P(y|x)=\frac{1}{Z(x)} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, j} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right) 
\\
Z(x)=\sum_{y} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, j} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right)</script><h6 id="条件随机场的简化形式"><a href="#条件随机场的简化形式" class="headerlink" title="条件随机场的简化形式"></a>条件随机场的简化形式</h6><script type="math/tex; mode=display">
\begin{aligned}
P(y | x) &=\frac{1}{Z(x)} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x) \\
Z(x) &=\sum_{y} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x)
\end{aligned}</script><h6 id="条件随机场的矩阵形式"><a href="#条件随机场的矩阵形式" class="headerlink" title="条件随机场的矩阵形式"></a>条件随机场的矩阵形式</h6><script type="math/tex; mode=display">
P_{w}(y | x)=\frac{1}{Z_{w}(x)} \prod_{i=1}^{n+1} M_{i}\left(y_{i-1}, y_{i} | x\right)\\
Z_{w}(x)=\left(M_{1}(x) M_{2}(x) \cdots M_{n+1}(x)\right)_{\text {start, stop }}</script><h3 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h3><p>K-Means是<strong>无监督</strong>的<strong>聚类</strong>算法.思想是对于给定的样本集,按照样本之间的距离大小将样本集划分为K个簇,让簇内的点尽量紧密地连在一起,而让簇间的距离尽量的大.</p>
<h4 id="传统算法"><a href="#传统算法" class="headerlink" title="传统算法"></a>传统算法</h4><ol>
<li>用先验知识或交叉验证选择一个合适的<strong>k</strong>值.</li>
<li>随机选择k个样本作为初始的<strong>质心</strong>.注意初始化质心的选择对最后的聚类结果和运行时间都有很大的影响.</li>
<li>计算每个样本点和各个质心的距离,将样本点标记为<strong>距离最小</strong>的质心所对应的簇.</li>
<li>重新计算每个<strong>簇</strong>的质心,取该簇中每个点位置的平均值.</li>
<li>重复2,3,4步直到k个质心都没有发生变化为止.</li>
</ol>
<h4 id="K-Means-1"><a href="#K-Means-1" class="headerlink" title="K-Means++"></a>K-Means++</h4><p>用于优化随机初始化质心的方法</p>
<ol>
<li>从输入样本点中随机选择一个点作为第一个质心.</li>
<li>计算每一个样本点到已选择的质心中<strong>最近质心</strong>的距离D(x).</li>
<li>选择一个新的样本点作为新的质心,选择原则是D(x)越大的点被选中的概率越大.</li>
<li>重复2和3直到选出k个质心.</li>
</ol>
<h4 id="Elkan-K-Means"><a href="#Elkan-K-Means" class="headerlink" title="Elkan K-Means"></a><strong>Elkan K-Means</strong></h4><p>利用两边之和大于第三边以及两边之差小于第三边来减少距离的计算.不适用于特征稀疏的情况.</p>
<h4 id="Mini-Batch-K-Means"><a href="#Mini-Batch-K-Means" class="headerlink" title="Mini Batch K-Means"></a><strong>Mini Batch K-Means</strong></h4><p>样本量很大时,只用其中的一部分来做传统的K-Means.一般多用几次该算法,从不同的随即采样中选择最优的聚类簇.</p>
<h3 id="Apriori"><a href="#Apriori" class="headerlink" title="Apriori"></a>Apriori</h3><p>Apriori是常用的挖掘出<strong>数据关联规则</strong>的算法,用于找出数据值中<strong>频繁</strong>出现的数据集合.一般使用支持度或者支持度与置信度的组合作为<strong>评估标准</strong>.</p>
<ol>
<li>支持度：几个关联的数据在数据集中出现的次数占总数据集的比重Support$(X, Y)=P(X Y)=\frac{\text {number}(X Y)}{\text {num}(\text {AllSamples})}$</li>
<li><strong>置信度</strong>：一个数据出现后.另一个数据出现的概率Confidence$(X \Leftarrow Y)=P(X | Y)=P(X Y) / P(Y)$</li>
</ol>
<p>Apriori算法的目标是找到最大的<strong>K项频繁集</strong>.假设使用支持度来作为评估标准,首先搜索出<strong>候选1项集</strong>及对应的支持度,<strong>剪枝</strong>去掉低于支持度的1项集,得到<strong>频繁1项集</strong>.然后对剩下的频繁1项集进行<strong>连接</strong>,得到候选的频繁2项集……以此类推,不断迭代,直到无法找到频繁k+1项集为止,对应的频繁k项集的集合即为输出结果.</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a href="https://blog.csdn.net/qq_20989105/article/details/81218696" target="_blank" rel="noopener">HMM隐马尔可夫模型与viterbi维特比算法</a></li>
</ol>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
</search>
