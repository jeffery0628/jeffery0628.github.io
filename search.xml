<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>过去、现在、未来</title>
    <url>/2020/03/20/life/past_present_future/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">此文章已被加密，需要输入密码访问.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="f8e9a8fdedf7169e9c3500063fd0a96c556a52d1de41f1a544e84ed4b52bc8d3">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>生活/成长</category>
      </categories>
      <tags>
        <tag>成长</tag>
      </tags>
  </entry>
  <entry>
    <title>时光集</title>
    <url>/2020/06/01/life/timeline/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/06/01/life/timeline/1.jpg" alt></p>
<a id="more"></a>
<h1 id="qi-yue">七月</h1>
<h2 id="7-1">7.1</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox0" checked="true" disabled="true"><label for="checkbox0"> 实习入职</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox1" disabled="true"><label for="checkbox1"> 模型融合</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox2" disabled="true"><label for="checkbox2"> 算法</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox3" disabled="true"><label for="checkbox3"> 事件抽取模型</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox4" disabled="true"><label for="checkbox4"> 模型复现</label></div></li>
</ul>
<h1 id="liu-yue">六月</h1>
<h2 id="zong-jie">总结</h2>
<p>这个月主要忙于构建法律文书的事件抽取信息，事件抽取模型以及实验，喜得三作。代码能力再次得到锻炼，学会如何通过使用图卷积的方式把依存句法特征编码进模型。</p>
<p>期间穿插部分算法练习。</p>
<h2 id="qi-yue-ji-hua">七月计划</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox5" disabled="true"><label for="checkbox5"> 掌握Tensorflow</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox6" disabled="true"><label for="checkbox6"> 学会对数据可视化分析</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox7" disabled="true"><label for="checkbox7"> 算法题不间断，坚持下去</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox8" disabled="true"><label for="checkbox8"> 坚持跑步、早睡、规律作息</label></div></li>
</ul>
<h2 id="6-30">6.30</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox9" checked="true" disabled="true"><label for="checkbox9"> 帮写法律抽取代码</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox10" checked="true" disabled="true"><label for="checkbox10"> 打王者</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox11" checked="true" disabled="true"><label for="checkbox11"> 刷算法题</label></div></li>
</ul>
<h2 id="6-29">6.29</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox12" checked="true" disabled="true"><label for="checkbox12"> 收拾行李</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox13" checked="true" disabled="true"><label for="checkbox13"> 论文实验代码</label></div></li>
</ul>
<h2 id="6-28">6.28</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox14" checked="true" disabled="true"><label for="checkbox14"> 过生日-空白的一天</label></div></li>
</ul>
<h2 id="6-27">6.27</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox15" checked="true" disabled="true"><label for="checkbox15"> Leetcode 双指针</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox16" checked="true" disabled="true"><label for="checkbox16"> 实验数据整理</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox17" checked="true" disabled="true"><label for="checkbox17"> 电影：心火</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox18" checked="true" disabled="true"><label for="checkbox18"> 早睡</label></div></li>
</ul>
<h2 id="6-26">6.26</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox19" checked="true" disabled="true"><label for="checkbox19"> 完成PLMEE</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox20" checked="true" disabled="true"><label for="checkbox20"> 返程</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox21" checked="true" disabled="true"><label for="checkbox21"> 跑实验</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox22" checked="true" disabled="true"><label for="checkbox22"> 跑步(达成)</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox23" checked="true" disabled="true"><label for="checkbox23"> 算法屠龙14式：双指针</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox24" checked="true" disabled="true"><label for="checkbox24"> 早睡计划，启动（11：40）</label></div></li>
</ul>
<h2 id="6-25">6.25</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox25" checked="true" disabled="true"><label for="checkbox25"> 吃烤鱼</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox26" checked="true" disabled="true"><label for="checkbox26"> 打游戏</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox27" checked="true" disabled="true"><label for="checkbox27"> 出发去仪征</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox28" checked="true" disabled="true"><label for="checkbox28"> 跑实验</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox29" checked="true" disabled="true"><label for="checkbox29"> 早睡计划</label></div></li>
</ul>
<h2 id="6-24">6.24</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox30" checked="true" disabled="true"><label for="checkbox30"> PLMEE(数据处理部分完成)</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox31" checked="true" disabled="true"><label for="checkbox31"> 跑步(达成)</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox32" checked="true" disabled="true"><label for="checkbox32"> 修改to do list 样式</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox33" checked="true" disabled="true"><label for="checkbox33"> 王者荣耀</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox34" checked="true" disabled="true"><label for="checkbox34"> 讨论实验方案</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox35" disabled="true"><label for="checkbox35"> 早睡计划失败(明天我一定十一点之前睡！！！)</label></div></li>
</ul>
<h2 id="6-23">6.23</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox36" checked="true" disabled="true"><label for="checkbox36"> 数据处理</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox37" checked="true" disabled="true"><label for="checkbox37"> 和申博聚餐</label></div></li>
<li>[x]</li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox38" checked="true" disabled="true"><label for="checkbox38"> 改实验模型，debug</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox39" checked="true" disabled="true"><label for="checkbox39"> 跑步</label></div></li>
</ul>
<h2 id="6-22">6.22</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox40" checked="true" disabled="true"><label for="checkbox40"> 数据处理</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox41" checked="true" disabled="true"><label for="checkbox41"> debug</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox42" checked="true" disabled="true"><label for="checkbox42"> 跑步</label></div></li>
</ul>
<h2 id="6-21">6.21</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox43" disabled="true"><label for="checkbox43"> 整理https://mp.weixin.qq.com/s/ONv5oPA2VMZLRUhB037hFw</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox44" checked="true" disabled="true"><label for="checkbox44"> 改模型，数据处理，debug</label></div></li>
</ul>
<h2 id="6-20">6.20</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox45" checked="true" disabled="true"><label for="checkbox45"> 数据处理</label></div></li>
</ul>
<h2 id="6-19">6.19</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox46" checked="true" disabled="true"><label for="checkbox46"> 数据处理</label></div></li>
</ul>
<h2 id="6-18">6.18</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox47" checked="true" disabled="true"><label for="checkbox47"> 数据处理</label></div></li>
</ul>
<h2 id="6-17">6.17</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox48" disabled="true"><label for="checkbox48"> JMEE</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox49" disabled="true"><label for="checkbox49"> DMCNN</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox50" disabled="true"><label for="checkbox50"> JRNN</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox51" checked="true" disabled="true"><label for="checkbox51"> Word embedding pytorch 实现</label></div></li>
</ul>
<h2 id="6-16">6.16</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox52" checked="true" disabled="true"><label for="checkbox52"> 深度优先搜索</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox53" checked="true" disabled="true"><label for="checkbox53"> JMEE论文/讨论</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox54" checked="true" disabled="true"><label for="checkbox54"> GCN？如何嵌入GCN？</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox55" checked="true" disabled="true"><label for="checkbox55"> 跑步30min</label></div></li>
</ul>
<h2 id="6-15">6.15</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox56" checked="true" disabled="true"><label for="checkbox56"> 模型baseline实现</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox57" checked="true" disabled="true"><label for="checkbox57"> 深度优先搜索模板</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox58" checked="true" disabled="true"><label for="checkbox58"> 划水摸鱼:神盾局特工3</label></div></li>
</ul>
<h2 id="6-14">6.14</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox59" checked="true" disabled="true"><label for="checkbox59"> 数据处理</label></div></li>
</ul>
<h2 id="6-13">6.13</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox60" checked="true" disabled="true"><label for="checkbox60"> 10.201.104.93 T640</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox61" checked="true" disabled="true"><label for="checkbox61"> TensorFlow2.0的学习</label></div></li>
</ul>
<h2 id="6-12">6.12</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox62" checked="true" disabled="true"><label for="checkbox62"> 广度优先搜索</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox63" checked="true" disabled="true"><label for="checkbox63"> tensorflow2.0 的学习</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox64" checked="true" disabled="true"><label for="checkbox64"> 跑步30min</label></div></li>
</ul>
<h2 id="6-11">6.11</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox65" checked="true" disabled="true"><label for="checkbox65"> 聚餐</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox66" checked="true" disabled="true"><label for="checkbox66"> 颓废一下午</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox67" checked="true" disabled="true"><label for="checkbox67"> 屠龙14：二分</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox68" checked="true" disabled="true"><label for="checkbox68"> 跑步30min</label></div></li>
</ul>
<h2 id="6-10">6.10</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox69" checked="true" disabled="true"><label for="checkbox69"> 检查标注质量，返工，统计标注人员</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox70" checked="true" disabled="true"><label for="checkbox70"> 越狱</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox71" checked="true" disabled="true"><label for="checkbox71"> 神盾局特工1，2</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox72" checked="true" disabled="true"><label for="checkbox72"> 跑步30min</label></div></li>
</ul>
<h2 id="6-9">6.9</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox73" checked="true" disabled="true"><label for="checkbox73"> 文本摘要整理</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox74" checked="true" disabled="true"><label for="checkbox74"> 学习编辑Latex</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox75" checked="true" disabled="true"><label for="checkbox75"> 屠龙14：滑动窗口</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox76" checked="true" disabled="true"><label for="checkbox76"> 跑步30min</label></div></li>
</ul>
<h2 id="6-8">6.8</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox77" checked="true" disabled="true"><label for="checkbox77"> 标注质量管理</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox78" checked="true" disabled="true"><label for="checkbox78"> 跑步30min</label></div></li>
</ul>
<h2 id="6-7">6.7</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox79" checked="true" disabled="true"><label for="checkbox79"> 标注质量管理</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox80" checked="true" disabled="true"><label for="checkbox80">  配置latex环境，画法律事件抽取表格</label></div></li>
</ul>
<h2 id="6-5">6.5</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox81" checked="true" disabled="true"><label for="checkbox81"> 论辩挖掘比赛baseline复现。</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox82" checked="true" disabled="true"><label for="checkbox82"> 军事爬虫2个网站,星岛环球</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox83" checked="true" disabled="true"><label for="checkbox83"> 跑步30min</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox84" disabled="true"><label for="checkbox84"> <a href="https://www.kesci.com/home/project/5eb7958f366f4d002d783d4a" target="_blank" rel="noopener">pyecharts</a></label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox85" disabled="true"><label for="checkbox85"> 数据结构屠龙14式</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox86" disabled="true"><label for="checkbox86"> 比赛trick总结：防止过拟合的损失阶段</label></div></li>
</ul>
<h2 id="6-4">6.4</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox87" checked="true" disabled="true"><label for="checkbox87"> 2015年以来引用较多的，法律事件抽取的文章（谷歌学术）legal event 10片左右，15年之前的综述一下。</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox88" disabled="true"><label for="checkbox88"> 复盘事件抽取比赛(除了苏神，暂时没有其他人放出来)</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox89" checked="true" disabled="true"><label for="checkbox89"> 跑步30min</label></div></li>
</ul>
<h2 id="6-3">6.3</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox90" checked="true" disabled="true"><label for="checkbox90"> 复习LightGBM</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox91" checked="true" disabled="true"><label for="checkbox91"> 复习SVM</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox92" checked="true" disabled="true"><label for="checkbox92"> 跑步30min</label></div></li>
</ul>
<h2 id="6-2">6.2</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox93" checked="true" disabled="true"><label for="checkbox93"> 军事命名实体识别任务：数据扩充，主要方法 1. 捞出每个类别下的全部实体 2. 把实体通过bert，得到每个实体的向量表示 3. 替换原始文本中相似（相似度计算：向量相乘）的实体，替换方案：对于每个实体80%不进行替换，15%替换成相同类别的其他实体，5%替换成其他类别的实体。</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox94" checked="true" disabled="true"><label for="checkbox94"> 被浪费掉的时间：服务器网速。</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox95" checked="true" disabled="true"><label for="checkbox95">  跑步1h</label></div></li>
</ul>
<h2 id="6-1">6.1</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox96" checked="true" disabled="true"><label for="checkbox96"> 军事命名实体识别任务：修改bert映射关系，学会了向句子中加入词、动词、语法关系等特征。（记得不要下标越界！）</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox97" checked="true" disabled="true"><label for="checkbox97"> 完成Xgboost的复习</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox98" checked="true" disabled="true"><label for="checkbox98"> 卖掉打印机1020p</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox99" checked="true" disabled="true"><label for="checkbox99"> 讨论比赛（裁判文书的论辩挖掘）效率很低！</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox100" checked="true" disabled="true"><label for="checkbox100"> 跑步45‘</label></div></li>
</ul>
]]></content>
      <categories>
        <category>生活/成长</category>
      </categories>
      <tags>
        <tag>时光集</tag>
      </tags>
  </entry>
  <entry>
    <title>数据探索性分析(EDA)</title>
    <url>/2020/07/01/machine_learning/exploratory-data-analysis/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/eda.png" alt></p>
<a id="more"></a>
<p>导入一些包</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> missingno <span class="keyword">as</span> msno <span class="comment"># 缺失值的可视化处理</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""导入数据集"""</span></span><br><span class="line"><span class="comment"># data = pd.read_csv('test.csv'，sep ='|' ，header = 0，skiprows = 10，nrows = 10)</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'./dataset/used_car_train_20200313.csv'</span>, sep=<span class="string">' '</span>) <span class="comment"># 指定分隔符为空格</span></span><br><span class="line">test_data = pd.read_csv(<span class="string">'./dataset/used_car_testA_20200313.csv'</span>, sep=<span class="string">' '</span>)</span><br></pre></td></tr></table></figure>
<h1 id="shu-ju-chu-shi">数据初识</h1>
<p>主要是对读取的数据有一个大致的了解，包括简单了解数据的行列信息，数据的统计特征等。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看数据的形状（行数和列数）</span></span><br><span class="line">print(<span class="string">'train_data shape :'</span>, train_data.shape) <span class="comment"># (150000, 31)</span></span><br><span class="line">print(<span class="string">'test_data shape :'</span>, test_data.shape) <span class="comment"># (50000, 30)</span></span><br><span class="line"><span class="comment"># 从形状可以看出训练集共 150000 个样本，30 个特征，1 列价格；测试集共 50000 个样本，30 个特征</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据简要概览</span></span><br><span class="line">train_data.head().append(train_data.tail()) <span class="comment"># 将开头5行和结尾5行拼接起来展示，head()和tail()默认值是5</span></span><br><span class="line">test_data.head().append(test_data.tail())</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8LEk6S.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据信息的查看 .info()可以看到每列的type，以及NAN缺失值的信息</span></span><br><span class="line">train_data.info()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8LZ5wQ.png" alt></p>
<p>通过<code>info()</code>可以发现几点信息，首先就是字段的类型，有一个 object（后面需要单独处理）。其次有一些字段有空值，清洗的时候需要处理。</p>
<p>通过<code>info()</code>了解数据每列的 type，还有助于了解是否存在除了 nan 以外的特殊符号异常</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 通过 .columns 查看列名</span></span><br><span class="line">train_data.columns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据的统计信息概览</span></span><br><span class="line">train_data.describe()</span><br></pre></td></tr></table></figure>
<p><code>describe()</code>中有每列的统计值，包括：个数 count、平均值 mean、方差 std、最小值 min、下四分位数 25%、中位数 50%、上四分位数 75%、以及最大值 max。查看这些信息可以瞬间掌握数据的大概范围<strong>以及异常值判断，例如 999999，-1 这些值其实是 nan 的另一种表达方式</strong>，有时候需要注意下。</p>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8LmXa4.png" alt></p>
<p>注意看左下角，提示有 30 列，但是刚才输出<code>shape</code>的时候明明提示有 31 列。注意，<code>describe()</code>是不包括 object 类型字段的统计信息的，毕竟不是数值类型。当然也可以单独用<code>describe()</code>看看</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data[<span class="string">'notRepairedDamage'</span>].describe()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果：</span></span><br><span class="line">count     <span class="number">150000</span></span><br><span class="line">unique         <span class="number">3</span></span><br><span class="line">top          <span class="number">0.0</span></span><br><span class="line">freq      <span class="number">111361</span></span><br><span class="line">Name: notRepairedDamage, dtype: object</span><br></pre></td></tr></table></figure>
<h1 id="shu-ju-gan-zhi">数据感知</h1>
<p>数据感知是在数据初识的基础上，进一步挖掘数据的信息，主要包括数据的确实值和异常值等</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看每列存在nan的情况</span></span><br><span class="line">train_data.isnull().sum()</span><br><span class="line"><span class="comment"># test_data.isnull().sum()</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8LuGnK.png" alt></p>
<p>可以看出，<code>model</code>、<code>bodyType</code>、<code>fuelType</code>、<code>gearbox</code>有缺失值。还可以对 nan 进行可视化，看的更加明显。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">missing = train_data.isnull().sum()</span><br><span class="line">missing = missing[missing &gt; <span class="number">0</span>]</span><br><span class="line">missing.sort_values(inplace=<span class="literal">True</span>)</span><br><span class="line">missing.plot.bar()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8LMJFe.png" alt></p>
<p>可视化 nan 的个数主要目的在于，查看 nan 存在的个数是否真的很大，如果很小一般选择填充，如果使用 lgb 等树模型可以直接让树自己去优化，但如果 nan 存在的过多，可以考虑删掉。</p>
<p>下面是可视化缺失值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可视化缺失值</span></span><br><span class="line">msno.matrix(train_data.sample(<span class="number">250</span>)) <span class="comment"># sample(250)表示抽取250个样本</span></span><br><span class="line"><span class="comment"># msno.matrix(test_data.sample(250))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># msno.bar(train_data.sample(1000))</span></span><br><span class="line"><span class="comment"># msno.bar(test_data.sample(1000))</span></span><br></pre></td></tr></table></figure>
<p>下图是代码运行后得到的结果，白线越多，代表缺失值越多（fuleType 缺失的最多）。</p>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8LQpkD.png" alt></p>
<p><strong>对数据持着怀疑的角度审视，尤其是 object 字段</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 看看object这个字段的取值情况</span></span><br><span class="line">train_data[<span class="string">'notRepairedDamage'</span>].value_counts()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.0</span>    <span class="number">111361</span></span><br><span class="line">-       <span class="number">24324</span></span><br><span class="line"><span class="number">1.0</span>     <span class="number">14315</span></span><br><span class="line">Name: notRepairedDamage, dtype: int64</span><br></pre></td></tr></table></figure>
<p>这个字段里面居然有个<code>-</code>值，如果单看比赛给的字段描述：0 代表有未修复的损害，1 代表没有，如果不持着怀疑的态度，很难发现这里还有个<code>-</code>，这个也代表缺失，因为很多模型可以对 nan 直接处理，所以这里我们先将<code>-</code>替换为 nan。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data[<span class="string">'noRepairedDamage'</span>].replace(<span class="string">'-'</span>, np.nan, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># test_data['noRepairedDamage'].replace('-', np.nan, inplace=True)</span></span><br></pre></td></tr></table></figure>
<h1 id="shu-ju-bu-huo">数据不惑</h1>
<p>通过初识和感知，不仅认识了数据，还发现了一些异常和缺失，下面进一步挖掘数据信息，主要包括查看预测值的分布以及将字段分成数值型和类别型，后面分开查看和处理</p>
<h2 id="liao-jie-shu-ju-de-fen-bu">了解数据的分布</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""查看预测值的频数"""</span></span><br><span class="line">train_data[<span class="string">'price'</span>].value_counts()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直方图可视化 自动划分10（默认值）个价格区间 统计每个区间的频数</span></span><br><span class="line">plt.hist(train_data[<span class="string">'price'</span>], orientation=<span class="string">'vertical'</span>, histtype=<span class="string">'bar'</span>, color=<span class="string">'red'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8L36H0.png" alt></p>
<p>查看频数，发现价格大于 20000 的值极少，其实这里也可以把这些值当作特殊值（或异常值）直接删掉，不过直接删掉不太好，毕竟这是个回归问题。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""总体分布概况（无界约翰逊分布等）"""</span></span><br><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> st</span><br><span class="line">y = train_data[<span class="string">'price'</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">'Johnson SU'</span>)</span><br><span class="line">sns.distplot(y, kde=<span class="literal">False</span>, fit=st.johnsonsu)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">'normal'</span>)</span><br><span class="line">sns.distplot(y, kde=<span class="literal">False</span>, fit=st.norm)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">plt.title(<span class="string">'Log Normal'</span>)</span><br><span class="line">sns.distplot(y, kde=<span class="literal">False</span>, fit=st.lognorm)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8LGmWD.png" alt></p>
<p>可以发现，价格不服从正态分布，所以在进行回归之前，必须将它进行转换，最佳拟合的是无界约翰逊分布。对预测标签做 log 转换，使其更加服从正态分布</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># log变换之后的分布会变得比较均匀，可以进行log变换进行预测，这也是预测问题常用的trick</span></span><br><span class="line">plt.hist(np.log(train_data[<span class="string">'price'</span>]), orientation=<span class="string">'vertical'</span>, histtype=<span class="string">'bar'</span>, color=<span class="string">'red'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8LGW6J.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""查看偏度和峰度"""</span></span><br><span class="line">sns.distplot(train_data[<span class="string">'price'</span>])</span><br><span class="line">print(<span class="string">'Skewness : %f'</span> % train_data[<span class="string">'price'</span>].skew()) <span class="comment"># 偏度</span></span><br><span class="line">print(<span class="string">'Kurtosis : %f'</span> % train_data[<span class="string">'price'</span>].kurt()) <span class="comment"># 峰度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">Skewness : <span class="number">3.346487</span></span><br><span class="line">Kurtosis : <span class="number">18.995183</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8LJDjH.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># train_data.skew(), train_data.kurt()</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">sns.distplot(train_data.skew(), color=<span class="string">'blue'</span>, axlabel=<span class="string">'Skewness'</span>)</span><br><span class="line">plot.subplot(<span class="number">122</span>)</span><br><span class="line">sns.distplot(train_data.kurt(), color=<span class="string">'orange'</span>, axlabel=<span class="string">'Kurtness'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8LYJxg.png" alt></p>
<p>峰度 Kurt 代表数据分布的尖锐程度，偏度简单来说就是数据的不对称程度。</p>
<blockquote>
<p>我们一般会拿偏度和峰度来看数据的分布形态，而且一般会跟正态分布做比较，我们把正态分布的偏度和峰度都看做零。如果我们在实操中，算到偏度峰度不为0，即表明变量存在左偏右偏，或者是高顶平顶这么一说。</p>
<p><strong>偏度（Skewness）</strong>: 是描述数据分布形态的统计量，其描述的是某总体取值分布的<strong>对称性</strong>，简单来说就是数据的不对称程度。\(Skewness=E[\frac{x-E(x)}{\sqrt{D(x)}^3}]\)</p>
<ol>
<li>
<p>Skewness = 0 ，分布形态与正态分布偏度相同。</p>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/histogram_symmetrical_nonskewed_normal.png" alt></p>
</li>
<li>
<p>Skewness &gt; 0 ，正偏差数值较大，为正偏或右偏。长尾巴拖在右边，数据右端有较多的极端值。</p>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/histogram_right_skewness_with_arrow.png" alt></p>
</li>
<li>
<p>Skewness &lt; 0 ，负偏差数值较大，为负偏或左偏。长尾巴拖在左边，数据左端有较多的极端值。</p>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/histogram_left_skewness_with_arrow.png" alt></p>
</li>
<li>
<p>数值的绝对值越大，表明数据分布越不对称，偏斜程度大。</p>
</li>
</ol>
<p><strong>峰度（Kurtosis）</strong>:是描述某变量所有取值分布形态陡缓程度的统计量，简单来说就是数据分布顶的<strong>尖锐程度</strong>。</p>
<ol>
<li>
<p>Kurtosis=0 与正态分布的陡缓程度相同。</p>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/distribution_plot_normal_dist_for_kurtosis.png" alt="img"></p>
</li>
<li>
<p>Kurtosis&gt;0 比正态分布的高峰更加陡峭——尖顶峰:具有正峰度值的分布表明，相比于正态分布，该分布有更重的尾部。实线表示正态分布，虚线表示具有正峰度值的分布。</p>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/distribution_plot_positive_kurtosis.png" alt></p>
</li>
<li>
<p>Kurtosis&lt;0 比正态分布的高峰来得平台——平顶峰:具有负峰度值的分布表明，相比于正态分布，该分布有更轻的尾部。实线表示正态分布，虚线表示具有负峰度值的分布。</p>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/distribution_plot_negative_kurtosis.png" alt></p>
</li>
</ol>
</blockquote>
<h2 id="ba-zi-duan-fen-wei-shu-zhi-zi-duan-he-lei-bie-zi-duan">把字段分为数值字段和类别字段</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""先分离出label值"""</span></span><br><span class="line">y_train = train_data[<span class="string">'price'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数值特征</span></span><br><span class="line"><span class="comment"># numeric_features = train_data.select_dtypes(include=[np.number])</span></span><br><span class="line"><span class="comment"># numeric_features.columns</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 类别特征</span></span><br><span class="line"><span class="comment"># categorical_features = train_data.select_dtypes(include=[np.object])</span></span><br><span class="line"><span class="comment"># categorical_features.columns</span></span><br></pre></td></tr></table></figure>
<p>上面是自动选取的方式，也可以人为设定</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""人为设定"""</span></span><br><span class="line">numeric_features = [<span class="string">'power'</span>, <span class="string">'kilometer'</span>].extend([<span class="string">'v_'</span>+str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">15</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里我感觉这个name和预测值没有关系，所以虽然是类别，可以先去掉看看, 日期的也去掉</span></span><br><span class="line">categorical_features = [<span class="string">'model'</span>, <span class="string">'brand'</span>, <span class="string">'bodyType'</span>, <span class="string">'fuelType'</span>, <span class="string">'gearbox'</span>, </span><br><span class="line">                        <span class="string">'notRepairedDamage'</span>,<span class="string">'regionCode'</span>, <span class="string">'seller'</span>, <span class="string">'offerType'</span>]</span><br></pre></td></tr></table></figure>
<h1 id="shu-ju-dong-xuan">数据洞玄</h1>
<p>前面的工作已经分析了预测值的分布，从分布中可以看到，如果把预测值进行对数变化一下，效果可能更好。然后把特征字段拆分为数值型和类别型。接下来我们主要对数值特征和类别特征进一步挖掘信息，包括类别偏斜，类别分布可视化，数值可视化等。</p>
<h2 id="lei-bie-te-zheng-de-tan-suo">类别特征的探索</h2>
<p>类别特征主要是看一下每个类别字段的取值和分布，会用到箱型图、小提琴图、柱状图等各种可视化技巧。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""类别偏斜处理"""</span></span><br><span class="line"><span class="keyword">for</span> cat_fea <span class="keyword">in</span> categorical_features:</span><br><span class="line">    print(cate_fea + <span class="string">'特征分布如下：'</span>)</span><br><span class="line">    print(<span class="string">'&#123;&#125;特征有&#123;&#125;不同的值'</span>.format(cate_fea, len(train_data[cat_fea].unique())))</span><br><span class="line">    print(train_data[cat_fea].value_counts())</span><br><span class="line">    print()</span><br></pre></td></tr></table></figure>
<p>这里主要是重点查看一下类别特征有没有数量严重偏斜的情况（由于太多，不在这里显示），这样的情况一般对预测没有什么帮助</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data[<span class="string">'seller'</span>].value_counts()</span><br><span class="line"></span><br><span class="line"><span class="number">0</span>    <span class="number">149999</span></span><br><span class="line"><span class="number">1</span>         <span class="number">1</span></span><br><span class="line">Name: seller, dtype: int64</span><br><span class="line"></span><br><span class="line">train_data[<span class="string">'offerType'</span>].value_counts()</span><br><span class="line"></span><br><span class="line"><span class="number">0</span>    <span class="number">150000</span></span><br><span class="line">Name: offerType, dtype: int64</span><br></pre></td></tr></table></figure>
<p>像<code>seller</code>、<code>offerType</code>字段偏斜就比较严重，直接删除这些字段:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">del</span> train_data[<span class="string">'seller'</span>]</span><br><span class="line"><span class="keyword">del</span> train_data[<span class="string">'offerType'</span>]</span><br><span class="line"><span class="keyword">del</span> test_data[<span class="string">'seller'</span>]</span><br><span class="line"><span class="keyword">del</span> test_data[<span class="string">'offerType'</span>]</span><br><span class="line"></span><br><span class="line">categorical_features.remove(<span class="string">'seller'</span>)</span><br><span class="line">categorical_features.remove(<span class="string">'offerType'</span>)</span><br></pre></td></tr></table></figure>
<p>下面看一下每个字段，有多少类（unique）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""类别的unique分布"""</span></span><br><span class="line"><span class="keyword">for</span> cat <span class="keyword">in</span> categorical_features:</span><br><span class="line">    print(len(train_data[cat].unique()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">249</span></span><br><span class="line"><span class="number">40</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line"><span class="number">8</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">7905</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为regionCode的类别太稀疏了，所以先去掉，因为后面要可视化，不画稀疏的</span></span><br><span class="line">categorical_features.remove(<span class="string">'regionCode'</span>)</span><br></pre></td></tr></table></figure>
<p>下面使用各种可视化方式，可视化类别特征:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""类别特征箱型图可视化"""</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> categorical_features:</span><br><span class="line">    train_data[c] = train_data[c].astype(<span class="string">'category'</span>)</span><br><span class="line">    <span class="keyword">if</span> train_data[c].isnull().any():</span><br><span class="line">        train_data[c] = train_data[c].cat.add_categories([<span class="string">'MISSING'</span>])</span><br><span class="line">        train_data[c] = train_data[c].fillna(<span class="string">'MISSING'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">boxplot</span><span class="params">(x, y, **kwargs)</span>:</span></span><br><span class="line">    sns.boxenplot(x=x, y=y)</span><br><span class="line">    x = plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line"></span><br><span class="line">f = pd.melt(train_data, id_vars=[<span class="string">'price'</span>], value_vars=categorical_features)</span><br><span class="line">g = sns.FacetGrid(f, col=<span class="string">"variable"</span>,  col_wrap=<span class="number">3</span>, sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>, size=<span class="number">5</span>)</span><br><span class="line">g = g.map(boxplot, <span class="string">"value"</span>, <span class="string">"price"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8LaCQO.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""类别特征的小提琴图可视化， 小提琴图类似箱型图，比后者高级点，图好看些"""</span></span><br><span class="line">catg_list = categorical_features</span><br><span class="line">target = <span class="string">'price'</span></span><br><span class="line"><span class="keyword">for</span> catg <span class="keyword">in</span> catg_list :</span><br><span class="line">    sns.violinplot(x=catg, y=target, data=train_data)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>小提琴的不在这展示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""类别特征的柱形图可视化"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bar_plot</span><span class="params">(x, y, **kwargs)</span>:</span></span><br><span class="line">    sns.barplot(x=x, y=y)</span><br><span class="line">    x=plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line"></span><br><span class="line">f = pd.melt(train_data, id_vars=[<span class="string">'price'</span>], value_vars=categorical_features)</span><br><span class="line">g = sns.FacetGrid(f, col=<span class="string">"variable"</span>,  col_wrap=<span class="number">3</span>, sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>, size=<span class="number">5</span>)</span><br><span class="line">g = g.map(bar_plot, <span class="string">"value"</span>, <span class="string">"price"</span>)</span><br></pre></td></tr></table></figure>
<p>看一下柱形图的结果</p>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8La50H.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""类别特征的每个类别频数可视化(count_plot)"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_plot</span><span class="params">(x,  **kwargs)</span>:</span></span><br><span class="line">    sns.countplot(x=x)</span><br><span class="line">    x=plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line"></span><br><span class="line">f = pd.melt(train_data,  value_vars=categorical_features)</span><br><span class="line">g = sns.FacetGrid(f, col=<span class="string">"variable"</span>,  col_wrap=<span class="number">3</span>, sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>, size=<span class="number">5</span>)</span><br><span class="line">g = g.map(count_plot, <span class="string">"value"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8Lda4I.png" alt></p>
<blockquote>
<p>规整数据/转换数据:melt()</p>
<p>df.melt() 是 df.pivot() 逆转操作函数,将列名转换为列数据(columns name → column values)，重构DataFrame。如果说 df.pivot() 将长数据集转换成宽数据集，df.melt() 则是将宽数据集变成长数据集。</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>frame</td>
<td>dataframe</td>
<td>被 melt 的数据集名称</td>
</tr>
<tr>
<td>id_vars</td>
<td>tuple、list、ndarray</td>
<td><strong>不需要被转换的列名</strong>，在转换后作为标识符列（不是索引列）</td>
</tr>
<tr>
<td>value_vars</td>
<td>tuple、list、ndarray</td>
<td><strong>需要被转换的现有列</strong>，如果未指明，除 id_vars 之外的其他列都被转换</td>
</tr>
<tr>
<td>var_name</td>
<td>ndarray</td>
<td>自定义列名名称，<strong>设置由 ‘value_vars’ 组成的新的 column name</strong></td>
</tr>
<tr>
<td>value_name</td>
<td>string</td>
<td>自定义列名名称，<strong>设置由 ‘value_vars’ 的数据组成的新的 column name</strong></td>
</tr>
<tr>
<td>col_level</td>
<td>int、string</td>
<td>如果列是MultiIndex，则使用此级别</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df = pd.DataFrame(&#123;<span class="string">'A'</span>: &#123;<span class="number">0</span>: <span class="string">'a'</span>, <span class="number">1</span>: <span class="string">'b'</span>, <span class="number">2</span>: <span class="string">'c'</span>&#125;,</span><br><span class="line"><span class="meta">... </span>                   <span class="string">'B'</span>: &#123;<span class="number">0</span>: <span class="number">1</span>, <span class="number">1</span>: <span class="number">3</span>, <span class="number">2</span>: <span class="number">5</span>&#125;,</span><br><span class="line"><span class="meta">... </span>                   <span class="string">'C'</span>: &#123;<span class="number">0</span>: <span class="number">2</span>, <span class="number">1</span>: <span class="number">4</span>, <span class="number">2</span>: <span class="number">6</span>&#125;&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df</span><br><span class="line">   A  B  C</span><br><span class="line"><span class="number">0</span>  a  <span class="number">1</span>  <span class="number">2</span></span><br><span class="line"><span class="number">1</span>  b  <span class="number">3</span>  <span class="number">4</span></span><br><span class="line"><span class="number">2</span>  c  <span class="number">5</span>  <span class="number">6</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#保留 B 列</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.melt(id_vars=[<span class="string">'A'</span>], value_vars=[<span class="string">'B'</span>])</span><br><span class="line">   A variable  value</span><br><span class="line"><span class="number">0</span>  a        B      <span class="number">1</span></span><br><span class="line"><span class="number">1</span>  b        B      <span class="number">3</span></span><br><span class="line"><span class="number">2</span>  c        B      <span class="number">5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#保留 B C 列</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.melt(id_vars=[<span class="string">'A'</span>], value_vars=[<span class="string">'B'</span>, <span class="string">'C'</span>])</span><br><span class="line">   A variable  value</span><br><span class="line"><span class="number">0</span>  a        B      <span class="number">1</span></span><br><span class="line"><span class="number">1</span>  b        B      <span class="number">3</span></span><br><span class="line"><span class="number">2</span>  c        B      <span class="number">5</span></span><br><span class="line"><span class="number">3</span>  a        C      <span class="number">2</span></span><br><span class="line"><span class="number">4</span>  b        C      <span class="number">4</span></span><br><span class="line"><span class="number">5</span>  c        C      <span class="number">6</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#自定义列名</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.melt(id_vars=[<span class="string">'A'</span>], value_vars=[<span class="string">'B'</span>],var_name=<span class="string">'myVarname'</span>, value_name=<span class="string">'myValname'</span>)</span><br><span class="line">   A myVarname  myValname</span><br><span class="line"><span class="number">0</span>  a         B          <span class="number">1</span></span><br><span class="line"><span class="number">1</span>  b         B          <span class="number">3</span></span><br><span class="line"><span class="number">2</span>  c         B          <span class="number">5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#如果 columns 是MultiIndex</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.columns = [list(<span class="string">'ABC'</span>), list(<span class="string">'DEF'</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df</span><br><span class="line">   A  B  C</span><br><span class="line">   D  E  F</span><br><span class="line"><span class="number">0</span>  a  <span class="number">1</span>  <span class="number">2</span></span><br><span class="line"><span class="number">1</span>  b  <span class="number">3</span>  <span class="number">4</span></span><br><span class="line"><span class="number">2</span>  c  <span class="number">5</span>  <span class="number">6</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.melt(col_level=<span class="number">0</span>, id_vars=[<span class="string">'A'</span>], value_vars=[<span class="string">'B'</span>])</span><br><span class="line">   A variable  value</span><br><span class="line"><span class="number">0</span>  a        B      <span class="number">1</span></span><br><span class="line"><span class="number">1</span>  b        B      <span class="number">3</span></span><br><span class="line"><span class="number">2</span>  c        B      <span class="number">5</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.melt(id_vars=[(<span class="string">'A'</span>, <span class="string">'D'</span>)], value_vars=[(<span class="string">'B'</span>, <span class="string">'E'</span>)])</span><br><span class="line">  (A, D) variable_0 variable_1  value</span><br><span class="line"><span class="number">0</span>      a          B          E      <span class="number">1</span></span><br><span class="line"><span class="number">1</span>      b          B          E      <span class="number">3</span></span><br><span class="line"><span class="number">2</span>      c          B          E      <span class="number">5</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/70.png" alt></p>
</blockquote>
<h2 id="shu-zhi-te-zheng-de-tan-suo">数值特征的探索</h2>
<p>数值特征的探索我们要分析相关性等，也会学习各种相关性可视化的技巧：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">numeric_train_data = train_data[numeric_features]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把price这一列加上，这个也是数值</span></span><br><span class="line">numeric_train_data[<span class="string">'price'</span>] = Y_train</span><br><span class="line"></span><br><span class="line"><span class="string">"""相关性分析"""</span></span><br><span class="line">correlation = numeric_train_data.corr()</span><br><span class="line">print(correlation[<span class="string">'price'</span>].sort_values(ascending=<span class="literal">False</span>), <span class="string">'\n'</span>)   <span class="comment"># 与price相关的特征排序</span></span><br></pre></td></tr></table></figure>
<p><code>.corr()</code>可以看到每个特征与 price 的相关性，并且排了个序。下面进行相关性可视化，使用热力图比较合适。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 热力图可视化</span></span><br><span class="line">f, ax = plt.subplots(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">plt.title(<span class="string">'Correlation of Numeric Features with Price'</span>, y=<span class="number">1</span>, size=<span class="number">16</span>)</span><br><span class="line">sns.heatmap(correlation, square=<span class="literal">True</span>, vmax=<span class="number">0.8</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8LcF3j.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 删除price</span></span><br><span class="line"><span class="keyword">del</span> numeric_train_data[<span class="string">'price'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">"""查看几个数值特征的偏度和峰度"""</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> numeric_train_data.columns:</span><br><span class="line">     print(<span class="string">'&#123;:15&#125;'</span>.format(col), </span><br><span class="line">          <span class="string">'Skewness: &#123;:05.2f&#125;'</span>.format(numeric_train_data[col].skew()) , </span><br><span class="line">          <span class="string">'   '</span> ,</span><br><span class="line">          <span class="string">'Kurtosis: &#123;:06.2f&#125;'</span>.format(numeric_train_data[col].kurt())  </span><br><span class="line">         )</span><br><span class="line"></span><br><span class="line"><span class="string">"""每个数字特征得分布可视化"""</span></span><br><span class="line">f = pd.melt(train_data, value_vars=numeric_features)</span><br><span class="line">g = sns.FacetGrid(f, col=<span class="string">"variable"</span>,  col_wrap=<span class="number">5</span>, sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>)</span><br><span class="line">g = g.map(sns.distplot, <span class="string">"value"</span>)</span><br></pre></td></tr></table></figure>
<p>数值特征的分布可视化，从这里可以看到数值特征的分布情况，其中匿名特征的分布相对均匀.</p>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8LcgVf.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""数字特征相互之间的关系可视化"""</span></span><br><span class="line">sns.set()</span><br><span class="line">columns = [<span class="string">'price'</span>, <span class="string">'v_12'</span>, <span class="string">'v_8'</span> , <span class="string">'v_0'</span>, <span class="string">'power'</span>, <span class="string">'v_5'</span>,  <span class="string">'v_2'</span>, <span class="string">'v_6'</span>, <span class="string">'v_1'</span>, <span class="string">'v_14'</span>]</span><br><span class="line">sns.pairplot(train_data[columns],size = <span class="number">2</span> ,kind =<span class="string">'scatter'</span>,diag_kind=<span class="string">'kde'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>这里面会看到有些特征之间是相关的， 比如 v_1 和 v_6:</p>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8L2c4S.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""多变量之间的关系可视化"""</span></span><br><span class="line">fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) = plt.subplots(nrows=<span class="number">5</span>, ncols=<span class="number">2</span>, figsize=(<span class="number">24</span>, <span class="number">20</span>))</span><br><span class="line"><span class="comment"># ['v_12', 'v_8' , 'v_0', 'power', 'v_5',  'v_2', 'v_6', 'v_1', 'v_14']</span></span><br><span class="line">v_12_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_12'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_12'</span>,y = <span class="string">'price'</span>, data = v_12_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax1)</span><br><span class="line"></span><br><span class="line">v_8_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_8'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_8'</span>,y = <span class="string">'price'</span>,data = v_8_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax2)</span><br><span class="line"></span><br><span class="line">v_0_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_0'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_0'</span>,y = <span class="string">'price'</span>,data = v_0_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax3)</span><br><span class="line"></span><br><span class="line">power_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'power'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'power'</span>,y = <span class="string">'price'</span>,data = power_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax4)</span><br><span class="line"></span><br><span class="line">v_5_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_5'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_5'</span>,y = <span class="string">'price'</span>,data = v_5_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax5)</span><br><span class="line"></span><br><span class="line">v_2_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_2'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_2'</span>,y = <span class="string">'price'</span>,data = v_2_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax6)</span><br><span class="line"></span><br><span class="line">v_6_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_6'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_6'</span>,y = <span class="string">'price'</span>,data = v_6_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax7)</span><br><span class="line"></span><br><span class="line">v_1_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_1'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_1'</span>,y = <span class="string">'price'</span>,data = v_1_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax8)</span><br><span class="line"></span><br><span class="line">v_14_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_14'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_14'</span>,y = <span class="string">'price'</span>,data = v_14_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax9)</span><br><span class="line"></span><br><span class="line">v_13_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_13'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_13'</span>,y = <span class="string">'price'</span>,data = v_13_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax10)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8L2Xv9.png" alt></p>
<h1 id="shu-ju-zhi-ming">数据知命</h1>
<p>这里会综合上面的这些过程，用 pandas_profiling 这个包使用函数 ProfileReport 生成一份数据探索性报告， 在这里面会看到：</p>
<ul>
<li>总体的数据信息（首先是数据集信息：变量数 (列)、观察数 (行)、数据缺失率、内存；数据类型的分布情况）</li>
<li>警告信息
<ul>
<li>类型，唯一值，缺失值</li>
<li>分位数统计量，如最小值，Q1，中位数，Q3，最大值，范围，四分位数范围</li>
<li>描述性统计数据，如均值，模式，标准差，总和，中位数绝对偏差，变异系数，峰度，偏度</li>
</ul>
</li>
<li>单变量描述（对每一个变量进行描述）</li>
<li>相关性分析（皮尔逊系数和斯皮尔曼系数）</li>
<li>采样查看等</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 两行简单的代码即可搞定上面的这些信息</span></span><br><span class="line">pfr = ppf.ProfileReport(train_data)</span><br><span class="line">pfr.to_file(<span class="string">"./EDA.html"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/01/machine_learning/exploratory-data-analysis/8LRgVx.png" alt></p>
<h1 id="can-kao">参考</h1>
<ol>
<li>
<p><a href="https://wmathor.com/index.php/archives/1425/" target="_blank" rel="noopener">数据探索性分析</a></p>
</li>
<li>
<p><a href="https://support.minitab.com/zh-cn/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/data-concepts/how-skewness-and-kurtosis-affect-your-distribution/" target="_blank" rel="noopener">偏度和峰度如何影响您的分布</a></p>
</li>
<li>
<p><a href="https://www.cnblogs.com/wyy1480/p/10474046.html" target="_blank" rel="noopener">数据的偏度和峰度——df.skew()、df.kurt()</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>EDA</tag>
      </tags>
  </entry>
  <entry>
    <title>模型融合</title>
    <url>/2020/06/30/machine_learning/model_fusion/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/06/30/machine_learning/model_fusion/model_fusion.png" alt></p>
<a id="more"></a>
<p>一般来说，通过融合多个不同模型的结果，可以提升最终的成绩，所以这以方法在各种数据竞赛中应用非常广泛。模型融合又可以从<strong>模型结果</strong>、<strong>模型自身</strong>、<strong>样本集</strong>等不同的角度进行融合。</p>
<h1 id="jian-dan-jia-quan-rong-he">简单加权融合</h1>
<h2 id="hui-gui-ren-wu-zhong-de-jia-quan-rong-he">回归任务中的加权融合</h2>
<p>对于回归问题，对各种模型的预测结果进行平均，所得到的结果能够减少过拟合，并使得边界更加平滑，(单个模型的边界可能很粗糙)。</p>
<p><img src="/2020/06/30/machine_learning/model_fusion/1739510-20190711144542394-704578871.png" alt></p>
<p><strong>加权融合根据各个模型的最终预测表现分配不同的权重，以改变其队最终结果影响的大小</strong>。例如，对于正确率低的模型给予较小的权重，而正确率高的模型给予更高的权重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成一些简单的样本，test_predi代表第i个模型的预测值</span></span><br><span class="line">test_pred1 = [<span class="number">1.2</span>, <span class="number">3.2</span>, <span class="number">2.1</span>, <span class="number">6.2</span>]</span><br><span class="line">test_pred2 = [<span class="number">0.9</span>, <span class="number">3.1</span>, <span class="number">2.0</span>, <span class="number">5.9</span>]</span><br><span class="line">test_pred3 = [<span class="number">1.1</span>, <span class="number">2.9</span>, <span class="number">2.2</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_test_true 代表模型的真实值</span></span><br><span class="line">y_test_true = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以先看一下各个模型的预测结果</span></span><br><span class="line">print(<span class="string">'Pred1 MAE:'</span>,mean_absolute_error(y_test_true, test_pred1)) </span><br><span class="line">print(<span class="string">'Pred2 MAE:'</span>,mean_absolute_error(y_test_true, test_pred2)) </span><br><span class="line">print(<span class="string">'Pred3 MAE:'</span>,mean_absolute_error(y_test_true, test_pred3))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 结果：</span></span><br><span class="line">Pred1 MAE: <span class="number">0.175</span></span><br><span class="line">Pred2 MAE: <span class="number">0.075</span></span><br><span class="line">Pred3 MAE: <span class="number">0.1</span></span><br></pre></td></tr></table></figure>
<p>可以发现，第 2 个模型的误差更小，准确率更高，所以应该给第二个模型的预测值赋予更高的权重</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加权融合，权重的默认值是(1/n)，n为模型个数，相当于默认使用平均加权融合</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weighted_method</span><span class="params">(test_pred1, test_pred2, test_pred3, w = [<span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>/<span class="number">3</span>])</span>:</span></span><br><span class="line">    weighted_result = w[<span class="number">0</span>] * pd.Series(test_pred1) + w[<span class="number">1</span>] * pd.Series(test_pred2) + w[<span class="number">2</span>] * pd.Series(test_pred3)</span><br><span class="line">    <span class="keyword">return</span> weighted</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据上面的MAE，计算每个模型的权重（MAE越小，权重越大）</span></span><br><span class="line">w = [<span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.3</span>] <span class="comment"># 这个权重是自定义的，也可以使用一些其它方法，例如softmax</span></span><br><span class="line">weighed_pred = weighted_method(test_pred1, test_pred2, test_pred3, w)</span><br><span class="line">print(<span class="string">'Weighted_pred MAE:'</span>,mean_absolute_error(y_test_true, Weighted_pre))   <span class="comment"># 融合之后效果提高了一些</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">Weighted_pre MAE: <span class="number">0.0575</span></span><br></pre></td></tr></table></figure>
<p>上述加权融合的技术是从模型结果的层面进行的，就是让每个模型跑一遍结果，然后对所有的结果进行融合，当然融合的方式不只有加权平均，还有例如平均、取中位数等：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义结果的平均函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Mean_method</span><span class="params">(test_pre1,test_pre2,test_pre3)</span>:</span></span><br><span class="line">    Mean_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=<span class="number">1</span>).mean(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> Mean_result</span><br><span class="line"></span><br><span class="line"><span class="comment">## 定义结果的中位数平均函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Median_method</span><span class="params">(test_pre1,test_pre2,test_pre3)</span>:</span></span><br><span class="line">    Median_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=<span class="number">1</span>).median(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> Median_result</span><br></pre></td></tr></table></figure>
<h2 id="fen-lei-ren-wu-zhong-de-voting">分类任务中的 Voting</h2>
<p>投票（Voting）是集成学习里面针对分类问题的一种结果融合策略。其基本思想是<strong>选择所有模型输出结果中，最多的那个类，即少数服从多数</strong>。</p>
<p>在不改变模型的情况下，直接对不同模型的预测结果进行投票或者平均，是一种简单却行之有效的融合方式。比如分类问题，假设有三个相互独立的模型，每个模型的正确率都是 70%，采用少数服从多数的方式进行投票，那么最终的正确率将是：<br>
\[
0.7 * 0.7 * 0.7+0.7 * 0.7 * 0.3 * 3=0.343+0.441=0.784
\]<br>
融合后预测正确的情况有两种，一种是<strong>三个模型都预测对了</strong>，另一种是<strong>其中两个模型预测对了，有一个模型预测错了</strong>。对于这两种情况，由于<strong>少数服从多数</strong>的机制存在，会使得最终结果都对。3 个 0.7 相乘对应的就是第一种情况，\(0.7∗0.7∗0.3∗3\) 对应的就是第二种情况。</p>
<p>经过简单的投票后，正确率提升了 8%。这是一个简单的概率问题——如果进行投票的模型越多，显然其结果将会更好，但前提条件是<strong>模型之间相互独立，结果之间没有相关性。越相近的模型进行融合，融合效果也会越差</strong>。</p>
<p>比如对于一个正确输出全为1的测试，我们有三个很相近的的预测结果，分别为：<br>
\[
\begin{array}{l}
1111111100=80 \% \text { accuracy } \\
11111111100=80 \% \text { accuracy } \\
1011111100=70 \% \text { accuracy }
\end{array}
\]</p>
<p>进行投票其结果为：<br>
\[
\begin{array}{l}
11111111100=80 \% \text { accuracy }
\end{array}
\]<br>
而假如各个预测结果之间有很大差异：<br>
\[
\begin{array}{l}
1111111100=80 \% \text { accuracy } \\
0111011101=70 \% \text { accuracy } \\
1000101111=60 \% \text { accuracy }
\end{array}
\]<br>
其投票结果将为：<br>
\[
\begin{array}{l}
1000101111=90 \% \text { accuracy }
\end{array}
\]<br>
可见模型之间差异越大(<strong>不是指正确率的差异，而是指模型之间相关性的差异</strong>)，融合所得的结果将会更好。</p>
<p><code>sklearn</code> 中的 <code>VotingClassifier</code> 实现了投票法。投票法的输出有两种类型：一种是直接输出类别标签，另一种是输出类别概率。前者叫做硬投票（Marjority/Hard voting），后者叫做软投票（Soft voting）。硬投票就是少数服从多数的原则，但有时候少数服从多数并不适用，更加合理的投票方式应该是有权值的投票。比如在唱歌比赛中，专业评审一人可以投 10 票，而观众一人只能投一票。</p>
<blockquote>
<ul>
<li>
<p><strong>硬投票</strong>选择各个模型输出最多的标签，如果标签数量相同，那么按照升序的次序进行选择:</p>
<p><img src="/2020/06/30/machine_learning/model_fusion/GwharR.png" alt></p>
</li>
</ul>
<p>hard voting 的少数服从多数原则在上面这种情况似乎不太合理，虽然只有模型 1 和模型 4 结果为 A，但它们俩的概率的高于 90%，也就是说很确定结果为 A，其它三个模型结果为 B，但从概率来看，并不是很确定。</p>
<ul>
<li>
<p><strong>软投票</strong>是根据各个模型输出的类别概率来进行类别的预测。如果给定权重，则会得到每个类别概率的加权平均值；否则就是普通的算术平均值。</p>
<p><img src="/2020/06/30/machine_learning/model_fusion/Gw4mFK.png" alt></p>
</li>
</ul>
</blockquote>
<p>以鸢尾花数据集测试对比投票法和单个模型的效果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line">x = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">clf1 = XGBClassifier(learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">150</span>, max_depth=<span class="number">3</span>, min_child_weight=<span class="number">2</span>, subsample=<span class="number">0.7</span>,</span><br><span class="line">                     colsample_bytree=<span class="number">0.6</span>, objective=<span class="string">'binary:logistic'</span>)</span><br><span class="line">clf2 = RandomForestClassifier(n_estimators=<span class="number">50</span>, max_depth=<span class="number">1</span>, min_samples_split=<span class="number">4</span>,</span><br><span class="line">                              min_samples_leaf=<span class="number">63</span>,oob_score=<span class="literal">True</span>)</span><br><span class="line">clf3 = SVC(C=<span class="number">0.1</span>, probability=<span class="literal">True</span>)  <span class="comment"># 软投票的时候，probability必须指定且为true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 硬投票</span></span><br><span class="line">eclf = VotingClassifier(estimators=[(<span class="string">'xgb'</span>, clf1), (<span class="string">'rf'</span>, clf2), (<span class="string">'svc'</span>, clf3)], voting=<span class="string">'hard'</span>)</span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> zip([clf1, clf2, clf3, eclf], [<span class="string">'XGBBoosting'</span>, <span class="string">'Random Forest'</span>, <span class="string">'SVM'</span>, <span class="string">'Voting'</span>]):</span><br><span class="line">    scores = cross_val_score(clf, x, y, cv=<span class="number">5</span>, scoring=<span class="string">'accuracy'</span>)</span><br><span class="line">    print(<span class="string">"Accuracy: %0.2f (+/- %0.2f) [%s]"</span> % (scores.mean(), scores.std(), label))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line">Accuracy: <span class="number">0.96</span> (+/- <span class="number">0.02</span>) [XGBBoosting]</span><br><span class="line">Accuracy: <span class="number">0.33</span> (+/- <span class="number">0.00</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.03</span>) [SVM]</span><br><span class="line">Accuracy: <span class="number">0.94</span> (+/- <span class="number">0.04</span>) [Voting]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 软投票只需要设置voting='soft'即可，这样最后的Voting正确率会成为0.96</span></span><br></pre></td></tr></table></figure>
<p>投票法非常简单，但是如果融合的模型中有些结果并不是很好，就会把整体的结果往下拉。</p>
<p>回归任务一般将多个模型的结果进行加权融合，分类任务一般采用投票法获取最终的结果。</p>
<h1 id="boosting-bagging">Boosting/Bagging</h1>
<p>Boosting/Bagging 都是从样本集的角度考虑把多个若模型集成起来的一种方式，只不过两者在集成的时候有些区别。xgb、lgb 属于 Boosting，而随机森林是 Bagging 的方式。</p>
<h2 id="boosting">Boosting</h2>
<p>Boosting 是将各种弱分类器串联起来的集成学习方式，每一个分类器的训练都依赖于前一个分类器的结果。串联（顺序运行）的方式导致了运行速度比较慢。和所有融合方式一样，它不会考虑各个弱分类器的内部结构，只是对训练数据（样本集）和连接方式进行操纵，以获得更小的误差。其基本思想是一种迭代的方法，<strong>每次训练的时候都更关心分类错误的样例，给这些分类错误的样例增加更大的权重，下一次迭代的目标就是更容易辨别出上一轮分类错误的样例</strong>。最终将这些弱分类器进行加权相加。</p>
<p><img src="/2020/06/30/machine_learning/model_fusion/GwOnHg.png" alt></p>
<p><img src="/2020/06/30/machine_learning/model_fusion/GwO33q.png" alt></p>
<p>Boosting 可以这么理解，比如用很多模型 M1,M2,…,Mn 去预测二手车的价格，但是这些模型的具体工作是这样安排的。首先让 M1 先训练然后预测价格，等 M1 预测完了之后，M2 的训练是对 M1 训练的改进和提升，即优化 M1 没有做好的事情。同样，M3 会基于 M2 的结果再次进行优化，这样一直到 Mn。这就是所谓的串联，即在训练过程中这 K 个模型之间是有依赖关系的，当引入第 i 个模型的时候，实际上是对前 i-1 个模型进行优化。最终的预测结果是对这 k 个模型结果的一个大组合。</p>
<p>Boosting 家族的代表有 adaboost、GBDT、xgboost、lightbgm 等，但是这些模型之间还是有区别的，可以分成 AdaBoost 流派和 GBDT 流派。比如 AdaBoost，在引入 M2 的时候，其实它关注的是 M1 预测不好的那些样本，这些样本在 M2 训练的时候，<strong>会加大权重</strong>。后面的模型引入也都是这个道理， 即关注前面模型预测不好的那些样本；而 GBDT，包括后面的 xgboost 这些，它们则是更加<strong>聚焦于残差</strong>，即 M2 引入的时候，它关注的是 M1 的所有预测结果与真实结果之间的差距，它想减少这个差距，后面的模型引入也是这个道理，即关注前面模型预测结果与真实结果之间的差距，然后一步一步的进行缩小。</p>
<h2 id="bagging">Bagging</h2>
<p>Bagging 是 Bootstrap Aggregating 的缩写。这种方法不对模型本身进行操作，而是作用于样本集上。采用的是随机有放回的选择性训练数据，然后构造分类器，最后进行组合。<strong>与 Boosting 方法中每个分类器之间相互依赖和串行运行不同，Bagging 方法中的学习器之间不存在强依赖关系，而是同时生成并运行</strong></p>
<p>其基本思路为：</p>
<ul>
<li>在样本集中进行 K 轮有放回的抽样，每次抽取 n 个样本，得到 K 个训练集；</li>
<li>分别用 K 个训练集训练得到 K 个模型</li>
<li>对得到的 K 个模型预测结果用投票或平均的方式进行融合</li>
</ul>
<p>在这里，训练集的选取可能不会包含所有样本集，未被包含的数据将成为<strong>包外数据</strong>，用来进行包外误差的泛化估计。每个模型的训练过程中，每次训练集可以取全部的特征进行训练，也可以随机取部分特征进行训练。极具代表性的随机森林算法就是每次随机选取部分特征。</p>
<p>下面仅从思想层面介绍随机森林算法：</p>
<ul>
<li>在样本集中进行 K 轮有放回的抽样，每次抽取 n 个样本，得到 K 个训练集，其中 n 一般远小于总样本数量</li>
<li>选取训练集，在整体特征集 M 中选取部分特征集 m 构建决策树，其中 <em>m</em>&lt;&lt;<em>M</em></li>
<li>在构造每颗决策树的过程中，按照选取最小的基尼指数进行分裂节点的选取，构建决策树。决策树的其它节点都采取相同的分裂规则进行构建，直到该节点的所有训练样例都属于同一类或达到树的最大深度</li>
<li>重复上述步骤，得到随机森林</li>
<li>多颗决策树同时进行预测，对结果进行投票或平均得到最终的分类结果</li>
</ul>
<p>多次随机选择的过程，使得随机森林不容易过拟合且有很好的抗干扰能力</p>
<h2 id="boosting-he-bagging-de-bi-jiao">Boosting和Bagging的比较</h2>
<h3 id="you-hua-fang-shi">优化方式</h3>
<p>在机器学习中，训练一个模型的过程通常是将 Loss 最小化的过程。但是单单最小化 Loss 并不能保证模型在解决一般化的问题时能够最优，甚至不能保证模型可用，也就是模型泛化能力不够。训练数据集的 Loss 与一般化数据集的 Loss 之间的差异被称为 generalization error：<br>
\[
\text {error}=\text {Bias}+\text {Variance}
\]<br>
<code>Variance</code>过大会导致模型过拟合，而<code>Bias</code>过大会导致模型欠拟合。</p>
<p><strong>Bagging 方法主要通过降低 <code>Variance</code> 来降低 <code>error</code>，Boosting 方法主要通过降低 <code>Bias</code> 来降低 <code>error</code></strong></p>
<blockquote>
<p>Bagging 方法采用多个不完全相同的训练集训练多个模型，最后结果取平均，由于\(E\left[\frac{\sum X_{i}}{n}\right]=E\left[X_{i}\right]\)，所以最终结果的 <code>Bias</code> 于单个模型的 <code>Bias</code> 很相近，一般不会显著降低 <code>Bias</code>。</p>
<p>对于 Variance</p>
<ol>
<li>子模型相互独立时，有：\(\operatorname{Var}\left[\frac{\sum X_{i}}{n}\right]=\frac{\operatorname{Var}\left[X_{i}\right]}{n}\)</li>
<li>子模型完全相同时，有：\(\operatorname{Var}\left[\frac{\sum X_{i}}{n}\right]=\operatorname{Var}\left[X_{i}\right]\)。</li>
</ol>
<p>Bagging 的多个子模型由不完全相同的数据集训练而成，子模型间有一定的相关性但又不完全独立，所以其结果在上述两式的中间状态，因此可以在一定程度上降低 Variance，从而使得总 error 减小。</p>
</blockquote>
<blockquote>
<p>Boosting 方法从优化角度来说， 是用 forward-stagewise 这种贪心法去最小化损失函数\(L\left(y, \sum a_{i} f_{i}(x)\right)\)。forward-stagewise 就是在迭代的第 n 步，求解新的子模型 \(f(x)\)及步长 <em>a</em> 来最小化\(L\left(y, f_{n-1}(x)+a f(x)\right)\),这里的\(f_{n-1}(x)\)是前 n 步得到的子模型的和。因此 Boosting 在最小化损失函数，Bias 自然逐步下降，而由于模型之间的强相关性，所以并不能显著降低 Variance。</p>
</blockquote>
<h3 id="yang-ben-xuan-ze">样本选择</h3>
<p>Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。</p>
<p>Boosting：每一轮的训练集不变，只是训练集中每个样本在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整的。</p>
<h3 id="yang-ben-quan-zhong">样本权重</h3>
<p>Bagging：使用均匀取样，每个样本的权重相等</p>
<p>Boosting：根据错误率不断调整样本的权重，错误率越大则权重越大</p>
<h3 id="yu-ce-han-shu">预测函数</h3>
<p>Bagging：所有预测函数的权重相等</p>
<p>Boosting：每个弱分类器都有相应的权重，误差小的分类器权重更大</p>
<h3 id="bing-xing-ji-suan">并行计算</h3>
<p>Bagging：各个预测函数可以并行生成</p>
<p>Boosting：理论上各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果</p>
<h1 id="stacking-blending">Stacking/Blending</h1>
<h2 id="stacking">Stacking</h2>
<p>Stacking 的本质是一种分层的结构，用了大量的基分类器，将其预测的结果作为下一层输入的特征，这样的结构使得它比相互独立训练的模型能够获得更多的特征。</p>
<p>下面以一种易于理解但不会实际使用的两层 stacking 方法为例，简要说明其结构和工作原理：</p>
<p><img src="/2020/06/30/machine_learning/model_fusion/G0PSzQ.png" alt></p>
<p>假设有三个基模型 <code>M1</code>，<code>M2</code>，<code>M3</code> 和一个元模型 <code>M4</code>，有训练集 train 和测试集 test，则：</p>
<ol>
<li>用训练集 train 训练基模型 M1（<code>M1.fit(train)</code>），然后分别在 train 和 test 上做预测，得到 P1（<code>M1.predict(train)</code>）和 T1（<code>M1.predict(test)</code>）</li>
<li>用训练集 train 训练基模型 M2（<code>M2.fit(train)</code>），然后分别在 train 和 test 上做预测，得到 P2（<code>M2.predict(train)</code>）和 T2（<code>M2.predict(test)</code>）</li>
<li>用训练集 train 训练基模型 M3（<code>M3.fit(train)</code>），然后分别在 train 和 test 上做预测，得到 P3（<code>M3.predict(train)</code>）和 T3（<code>M3.predict(test)</code>）</li>
</ol>
<p>这样第一层的模型就训练结束了，接下来</p>
<ol>
<li>把 P1，P2，P3 进行合并组成新的训练集 train2，把 T1，T2，T3 进行合并组成新的测试集 test2</li>
<li>用新的训练集 train2 训练元模型 M4（<code>M4.fit(train2)</code>），然后在 test2 上进行预测得到最终的预测结果 Y_pred（<code>M4.predict(test2)</code>）</li>
</ol>
<p>这样第二层训练预测就得到了最终的预测结果。这就是两层堆叠的一种基本的原始思路。Stacking 本质上就是这么直接的思路，但是直接这样做，对于训练集和测试集分布不那么一致的情况下是有问题的，<strong>其问题在于用训练集训练原始模型，又接着用训练的模型去预测训练集，会严重过拟合</strong>,因此，问题变成如何降低再训练的过拟合问题。一般有两种解决方法:</p>
<ul>
<li>次级模型尽量选择简单的线性模型</li>
<li>第一层训练模型使用交叉验证的方式</li>
</ul>
<p>第一种方法很容易理解，重点是看第二种方法到底是怎么做的:</p>
<p><img src="/2020/06/30/machine_learning/model_fusion/G0i24K.png" alt></p>
<p>以 5 折交叉验证为例</p>
<ol>
<li>
<p>首先将训练集分成 5 份。</p>
</li>
<li>
<p>对于每一个基模型 \(i\) 来说，用其中 4 份进行训练，然后用另一份训练集作验证集进行预测，得到 \(P_i\) 的一部分，然后再用测试集进行预测得到 \(T_i\) 的一部分，这样 5 轮下来之后，验证集的预测值就会拼接成一个完整的 P，测试集的 label 值取个平均就会得到一个完整的 T。</p>
</li>
<li>
<p>所有的 \(P_i\) 合并就得到了下一层的训练集 train2，所有的 \(T_i\) 合并就得到了下一层的测试集 test2。</p>
</li>
<li>
<p>利用 train2 训练第二层的模型，然后在 test2 上得到预测结果，就是最终的结果。</p>
<p><img src="/2020/06/30/machine_learning/model_fusion/G0knSS.png" alt></p>
</li>
</ol>
<p>Stacking 的过程可以用下面的图表示：</p>
<p><img src="/2020/06/30/machine_learning/model_fusion/G0kay4.png" alt></p>
<h3 id="hui-gui-zhong-de-stacking">回归中的stacking</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成一些简单的样本数据， test_predi代表第i个模型的预测值</span></span><br><span class="line">train_reg1 = [<span class="number">3.2</span>, <span class="number">8.2</span>, <span class="number">9.1</span>, <span class="number">5.2</span>]</span><br><span class="line">train_reg2 = [<span class="number">2.9</span>, <span class="number">8.1</span>, <span class="number">9.0</span>, <span class="number">4.9</span>]</span><br><span class="line">train_reg3 = [<span class="number">3.1</span>, <span class="number">7.9</span>, <span class="number">9.2</span>, <span class="number">5.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_test_true代表模型的真实值</span></span><br><span class="line">y_train_true = [<span class="number">3</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">test_pred1 = [<span class="number">1.2</span>, <span class="number">3.2</span>, <span class="number">2.1</span>, <span class="number">6.2</span>]</span><br><span class="line">test_pred2 = [<span class="number">0.9</span>, <span class="number">3.1</span>, <span class="number">2.0</span>, <span class="number">5.9</span>]</span><br><span class="line">test_pred3 = [<span class="number">1.1</span>, <span class="number">2.9</span>, <span class="number">2.2</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line">y_test_true = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Stacking_method</span><span class="params">(train_reg1, train_reg2, train_reg3, y_train_true, test_pred1, test_pred2, test_pred3, model_L2 = LinearRegression<span class="params">()</span>)</span>:</span></span><br><span class="line">    model_L2.fit(pd.concat([pd.Series(train_reg1), pd.Series(train_reg2), pd.Series(train_reg3)], axis=<span class="number">1</span>).values, y_train_true)</span><br><span class="line">    Stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1), pd.Series(test_pre2), pd.Series(test_pre3)], axis=<span class="number">1</span>).values)</span><br><span class="line">    <span class="keyword">return</span> Stacking_result</span><br><span class="line"> </span><br><span class="line">model_L2 = LinearRegression()</span><br><span class="line">Stacking_pre = Stacking_method(train_reg1, train_reg2, train_reg3, y_train_true,</span><br><span class="line">                               test_pre1, test_pre2, test_pre3, model_L2)</span><br><span class="line">print(<span class="string">'Stacking_pre MAE:'</span>, mean_absolute_error(y_test_true, Stacking_pre))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 结果：</span></span><br><span class="line">Stacking_pre MAE: <span class="number">0.04213</span></span><br></pre></td></tr></table></figure>
<p>这里的逻辑就是把第一层模型在训练集上的预测值，当作第二层训练集的特征，第一层模型在测试集上的预测值，当作第二层测试集的特征，然后在第二层建立一个简单的线性模型进行训练。</p>
<p>可以发现最终误差相对于之前进一步提升了，需要注意的是，<strong>第二层的模型不宜选的过于复杂</strong>，否则会导致模型过拟合。</p>
<p>接下来介绍一款强大的 stacking 工具 StackingCVRegressor，这是一种继承学习的元回归器，首先导入</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mxltend.regressor <span class="keyword">import</span> StackingCVRegressor</span><br></pre></td></tr></table></figure>
<p>在标准 stacking 过程中，拟合一级回归器的时候，如果使用了第二级回归器输入的相同训练集，就会导致过拟合。但是，StackingCVRegressor 使用了 “非折叠预测” 的概念：数据被分成 K 折，并且在 K 个连续的循环中，使用 K-1 折来拟合第一级回归器（即 K 折交叉验证的 StackingRegressor）。在每一轮中（一共 K 轮），一级回归器先后被应用于在每次迭代中还未用过的 1 个子集，然后将得到的预测叠加起来作为输入数据提供给二级回归器。在 StackingCVRegressor 训练完之后，一级回归器拟合整个数据集以获得最佳预测。这就是前面介绍的原理。</p>
<p>具体 API 及参数如下：</p>
<blockquote>
<p>StackingCVRegressor(regressors，meta_regressor，cv = 5，shuffle = True，use_features_in_secondary = False)</p>
<ul>
<li>regressors：基回归器，列表的形式，第一层模型。例如我打算第一层用 xgb 和 lgb，第二层用线性模型，那么这里就应该写 [xgb,lgb]</li>
<li>meta_regressor：元回归器，可以理解为第二层的模型，一般不能太复杂，例如使用一个普通的线性模型 lr</li>
<li>cv：交叉验证策略，默认是 5 折交叉验证</li>
<li>use_features_in_secondary：默认是 False，表示第二层的回归器只接受第一层回归器的结果进行训练和预测。如果设置为 True，表示第二层的回归器不仅接收第一层回归器的结果，还接收原始的数据集一块进行训练</li>
<li>shuffle：是否打乱样本的顺序</li>
<li>训练依然是用<code>.fit(x, y)</code>，但这里的 x 和 y 要求是数组，所以如果是 DataFrame，需要<code>np.array()</code>一下，并且 x 的 shape 应该是 (n_samples，n_features)，y 的 shape 应该是（n_samples）</li>
<li>预测依然是用<code>.predict(x_test)</code>，只不过 x_test 也是数组，形状和上面的一样</li>
</ul>
</blockquote>
<h3 id="fen-lei-zhong-de-stacking">分类中的stacking</h3>
<p>以鸢尾花数据集为例，首先手写实现 stacking 加深理解，然后使用<code>mlxtend.classifier.StackingClassifier</code>实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier, GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>, :]</span><br><span class="line"></span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型融合中用到的单个模型</span></span><br><span class="line">clfs = [LogisticRegression(solver=<span class="string">'lbfgs'</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'entropy'</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分一部分数据作为训练集</span></span><br><span class="line">X, X_predict, y, y_predict = train_test_split(data, target, test_size=<span class="number">0.3</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">dataset_blend_train = np.zeros((X.shape[<span class="number">0</span>], len(clfs)))   <span class="comment"># 每个模型的预测作为第二层的特征</span></span><br><span class="line">dataset_blend_test = np.zeros((X_predict.shape[<span class="number">0</span>], len(clfs)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5折stacking</span></span><br><span class="line">n_splits = <span class="number">5</span></span><br><span class="line">skf = StratifiedKFold(n_splits)</span><br><span class="line">skf = skf.split(X, y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> enumerate(clfs):</span><br><span class="line">    <span class="comment"># 依次训练各个单模型</span></span><br><span class="line">    dataset_blend_test_j = np.zeros((X_predict.shape[<span class="number">0</span>], <span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> i, (train, test) <span class="keyword">in</span> enumerate(skf):</span><br><span class="line">        <span class="comment"># 5—fold交叉训练，使用第i个部分作为预测， 剩余的部分来训练模型， 获得其预测的输出作为第i部分的新特征。</span></span><br><span class="line">        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]</span><br><span class="line">        clf.fit(X_train, y_train)</span><br><span class="line">        y_submission = clf.predict_proba(X_test)[:,<span class="number">1</span>]</span><br><span class="line">        dataset_blend_train[test, j] = y_submission</span><br><span class="line">        dataset_blend_test_j[:, j] = clf.predict_proba(X_predict)[:, <span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 对于测试集， 直接用这k个模型的预测值均值作为新的特征</span></span><br><span class="line">    dataset_blend_test[:, j] = dataset_blend_test_j.mean(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">"val auc Score: %f"</span> % roc_auc_score(y_predict, dataset_blend_test[:, j]))</span><br><span class="line"></span><br><span class="line">clf = LogisticRegression(solver=<span class="string">'lbfgs'</span>)</span><br><span class="line">clf.fit(dataset_blend_train, y)</span><br><span class="line">y_submission = clf.predict_proba(dataset_blend_test)[:,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Val auc Score of Stacking: %f"</span> % (roc_auc_score(y_predict, y_submission)))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 结果如下：</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">Val auc Score of Stacking: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure>
<p>StackingClassifier 的 API 及参数如下：</p>
<blockquote>
<p>StackingClassifier(classifiers, meta_classifier, use_probas=False, average_probas=False, verbose=0, use_features_in_secondary=False)， 这里的参数和上面的 StackingCVRegressor 基本上差不多</p>
<ul>
<li>classifiers：基分类器， 数组形式 [clf1, clf2, clf3], 每个基分类器的属性被存储在类属性 <code>self.clfs_</code>中</li>
<li>meta_classifier：目标分类器，即第二层的分类器</li>
<li>use_probas：bool (default: False) 。如果设置为 True， 那么目标分类器的输入就是前面分类输出的类别概率值而不是类别标签</li>
<li>average_probas：bool (default: False)。用来设置上一个参数当使用概率值输出的时候是否使用平均值</li>
<li>verbose：int, optional (default=0)。用来控制使用过程中的日志输出，当 <code>verbose = 0</code>时，什么也不输出；<code>verbose = 1</code>时，输出回归器的序号和名字；<code>verbose = 2</code>时，输出详细的参数信息</li>
<li>use_features_in_secondary：bool (default: False)。如果设置为 True，那么最终的目标分类器就被基分类器产生的数据和最初的数据集同时训练。如果设置为 False，最终的分类器只会使用基分类器产生的数据训练。</li>
<li>常用方法： <code>.fit()</code>，<code>.predict()</code></li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingCVClassifier</span><br><span class="line"><span class="comment"># 上面的这个操作，如果换成StackingClassifier， 是这样的形式：</span></span><br><span class="line">clf1 = RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>)</span><br><span class="line">clf2 = ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>)</span><br><span class="line">clf3 = ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'entropy'</span>)</span><br><span class="line">clf4 = GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)</span><br><span class="line">clf5 = LogisticRegression(solver=<span class="string">'lbfgs'</span>)</span><br><span class="line"></span><br><span class="line">sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3, clf4], meta_classifier=clf, cv=<span class="number">3</span>)</span><br><span class="line">sclf.fit(X, y)</span><br><span class="line"><span class="comment"># 5这交叉验证</span></span><br><span class="line"><span class="comment">#scores = cross_val_score(sclf, X, y, cv=3, scoring='accuracy')</span></span><br><span class="line"></span><br><span class="line">y_submission = sclf.predict(X_predict)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Val auc Score of Stacking: %f"</span> % (roc_auc_score(y_predict, y_submission)))</span><br></pre></td></tr></table></figure>
<h2 id="blending">Blending</h2>
<p>Blending 是一种和 Stacking 很相像的模型融合方式，它与 Stacking 的区别在于训练集不是通过 K-Fold 的策略来获得预测值，而是先建立一个 Holdout（留出集）。</p>
<h3 id="blending-dan-chun-holdout">Blending（单纯Holdout）</h3>
<p>单纯的 Holdout 就是直接把训练集分成两部分，70% 作为新的训练集，30% 作为验证集，然后用这 70% 的训练集分别训练第一层的模型，然后在 30% 的验证集上进行预测，把预测的结果作为第二层模型的训练集特征，这是训练部分。预测部分就是把<strong>真正的测试集</strong>先用第一层的模型预测，把预测结果作为第二层测试集的特征进行第二层的预测。 过程图如下：</p>
<p><img src="/2020/06/30/machine_learning/model_fusion/G0nNNj.png" alt></p>
<p>这种方法实现起来也比较容易，基本和 stacking 的代码差不多，只不过少了内层的循环，毕竟每个模型不用交叉验证了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#创建训练的数据集</span></span><br><span class="line"><span class="comment">#创建训练的数据集</span></span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>,:]</span><br><span class="line"></span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"> </span><br><span class="line"><span class="comment">#模型融合中使用到的各个单模型</span></span><br><span class="line">clfs = [LogisticRegression(solver=<span class="string">'lbfgs'</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'entropy'</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>),</span><br><span class="line">        <span class="comment">#ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),</span></span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"> </span><br><span class="line"><span class="comment">#切分一部分数据作为测试集</span></span><br><span class="line">X, X_predict, y, y_predict = train_test_split(data, target, test_size=<span class="number">0.3</span>, random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#切分训练数据集为d1,d2两部分</span></span><br><span class="line">X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=<span class="number">0.5</span>, random_state=<span class="number">2020</span>)</span><br><span class="line">dataset_d1 = np.zeros((X_d2.shape[<span class="number">0</span>], len(clfs)))</span><br><span class="line">dataset_d2 = np.zeros((X_predict.shape[<span class="number">0</span>], len(clfs)))</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> enumerate(clfs):</span><br><span class="line">    <span class="comment">#依次训练各个单模型</span></span><br><span class="line">    clf.fit(X_d1, y_d1)</span><br><span class="line">    y_submission = clf.predict_proba(X_d2)[:, <span class="number">1</span>]</span><br><span class="line">    dataset_d1[:, j] = y_submission</span><br><span class="line">    <span class="comment">#对于测试集，直接用这k个模型的预测值作为新的特征。</span></span><br><span class="line">    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment">#print("val auc Score: %f" % roc_auc_score(y_predict, dataset_d2[:, j]))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#融合使用的模型</span></span><br><span class="line">clf = GradientBoostingClassifier(learning_rate=<span class="number">0.02</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">30</span>)</span><br><span class="line">clf.fit(dataset_d1, y_d2)</span><br><span class="line">y_submission = clf.predict_proba(dataset_d2)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">"Val auc Score of Blending: %f"</span> % (roc_auc_score(y_predict, y_submission)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果：</span></span><br><span class="line">Val auc Score of Blending: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure>
<h3 id="blending-holdout-jiao-cha">Blending(Holdout交叉)</h3>
<p>第二种引入了交叉验证的思想，也就是每个模型看到的 Holdout 集合并不一样。说白了，就是把 Stacking 流程中的 K-Fold CV 改成 HoldOut CV。第二阶段的 stacker 模型就基于第一阶段模型对这 30% 训练数据的预测值进行拟合</p>
<ol>
<li>在第一层中， 用 70% 的训练集训练多个模型， 然后去预测那 30% 的数据得到预测值 \(P_i\)， 同时也预测 test 集得到预测值 \(T_i\)。这里注意，那 30% 的数据每个模型并不是一样，也是类似于交叉验证的那种划分方式，只不过 stacking 那里是每个模型都会经历 K 折交叉验证，也就是有多少模型，就会有多少次 K 折交叉验证，而 blending 这里是所有模型合起来只经历了一次 K 折交叉验证（看下图就容易懂了）</li>
<li>第二层直接对 <em>P**i</em> 进行合并，作为新的训练集 train2，test 集的预测值 <em>T**i</em> 合并作为新的测试集 test2，然后训练第二层的模型</li>
</ol>
<p>Blending 的过程训练和预测过程可以使用下图来表示：</p>
<p><img src="/2020/06/30/machine_learning/model_fusion/G0ufoQ.png" alt></p>
<blockquote>
<p>Blending 的优势在于：</p>
<ul>
<li>Blending 比较简单，而 Stacking 相对比较复杂；</li>
<li>能够防止信息泄露：generalizers 和 stackers 使用不同的数据；</li>
</ul>
<p>Blending 缺点在于：</p>
<ul>
<li>只用了整体数据的一部分；</li>
<li>最终模型可能对留出集（holdout set）过拟合；</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型融合中用到的单个模型</span></span><br><span class="line">clfs = [LogisticRegression(solver=<span class="string">'lbfgs'</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'entropy'</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分一部分数据作为训练集</span></span><br><span class="line">X, X_predict, y, y_predict = train_test_split(data, target, test_size=<span class="number">0.3</span>, random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line">dataset_blend_train = np.zeros((int(X.shape[<span class="number">0</span>]/n_splits), len(clfs)))   <span class="comment"># 每个模型的预测作为第二层的特征</span></span><br><span class="line">dataset_blend_test = np.zeros((X_predict.shape[<span class="number">0</span>], len(clfs)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5折stacking</span></span><br><span class="line">n_splits = <span class="number">5</span></span><br><span class="line">skf = StratifiedKFold(n_splits)</span><br><span class="line">skf = skf.split(X, y)</span><br><span class="line"></span><br><span class="line">fold = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i, (train, test) <span class="keyword">in</span> enumerate(skf):</span><br><span class="line">    fold[i] = (X[train], y[train], X[test], y[test])</span><br><span class="line">Y_blend = []</span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> enumerate(clfs):</span><br><span class="line">    <span class="comment"># 依次训练各个单模型</span></span><br><span class="line">    dataset_blend_test_j = np.zeros((X_predict.shape[<span class="number">0</span>], <span class="number">5</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 5—fold交叉训练，使用第i个部分作为预测， 剩余的部分来训练模型， 获得其预测的输出作为第i部分的新特征。</span></span><br><span class="line">    X_train, y_train, X_test, y_test = fold[j]</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    dataset_blend_train[:, j] =  clf.predict(X_test)</span><br><span class="line">    Y_blend.extend(y_test)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对于测试集，直接用这k个模型的预测值作为新的特征</span></span><br><span class="line">    dataset_blend_test[:, j] = clf.predict(X_predict)</span><br><span class="line">        </span><br><span class="line">    print(<span class="string">"val auc Score: %f"</span> % roc_auc_score(y_predict, dataset_blend_test[:, j]))</span><br><span class="line"></span><br><span class="line">dataset_blend_train = dataset_blend_train.T.reshape(<span class="number">70</span>, <span class="number">-1</span>)</span><br><span class="line">dataset_blend_test = np.mean(dataset_blend_test, axis=<span class="number">1</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">Y_blend = np.array(Y_blend).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">clf = LogisticRegression(solver=<span class="string">'lbfgs'</span>)</span><br><span class="line">clf.fit(dataset_blend_train, Y_blend)</span><br><span class="line">y_submission = clf.predict(dataset_blend_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Val auc Score of Stacking: %f"</span> % (roc_auc_score(y_predict, y_submission)))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 结果：</span></span><br><span class="line">Val auc Score of Stacking: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure>
<h1 id="zong-jie">总结</h1>
<p>这篇文章是基于已经调参好的模型去研究如何发挥出模型更大的性能。从模型的结果、样本集的集成和模型自身融合三个方面去整理。</p>
<ol>
<li><strong>模型的结果方面</strong>，对于回归问题，可以对模型的结果进行加权融合等方式；对于分类问题，我们可以使用 Voting 的方式去得到最终的结果。</li>
<li><strong>样本集的集成技术方面</strong>，有 Boosting 和 Bagging 方式，都是把多个弱分类器进行集成的技术，但是两者是不同的。</li>
<li><strong>模型自身的融合方面</strong>， Stacking 和 Blending 的原理及具体实现方法，介绍了 mlxtend 库里面的模型融合工具</li>
</ol>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://wmathor.com/index.php/archives/1428/" target="_blank" rel="noopener">模型融合</a></li>
<li><a href="https://www.cnblogs.com/libin47/p/11169994.html" target="_blank" rel="noopener">模型融合方法学习总结</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>模型融合</tag>
      </tags>
  </entry>
  <entry>
    <title>事件抽取模型复现核心代码</title>
    <url>/2020/06/26/research/ee_model/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/06/26/research/ee_model/image-20200626162628002.png" alt></p>
<a id="more"></a>
<h1 id="plmee">plmee</h1>
<p>plmee事件抽取用于裁判文书事件抽取</p>
<h2 id="hong-fa-qi-de-chou-qu">触发器的抽取</h2>
<p>触发器抽取器的目的是预测出触发了事件的token，形式化为token级别的多类别分类任务，分类标签是事件类型。在BERT上添加一个多类分类器就构成了触发器抽取器。</p>
<p>触发器抽取器的输入和BERT的一样，是WordPiece嵌入、位置嵌入和segment嵌入的和。因为输入只有一个句子，所以所有的segment ids设为0。句子首尾的token分别是[CLS]和[SEP]。</p>
<p>触发词有时不是一个单词而是一个词组。因此，作者令连续的tokens共享同一个预测标签，作为一个完整的触发词。</p>
<p>采用交叉熵损失函数用于微调（fine-tune）。</p>
<p><img src="/2020/06/26/research/ee_model/image-20200626162628002.png" alt></p>
<h3 id="mo-xing">模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TriggerExtractor</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bert_path, bert_train, dropout,event_type_num)</span>:</span></span><br><span class="line">        super(TriggerExtractor,self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="comment"># 对bert进行训练</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.bert.named_parameters():</span><br><span class="line">            param.requires_grad = bert_train</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.taggers = nn.ModuleList([nn.Linear(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],<span class="number">2</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(event_type_num)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text_lengths, text_ids, masks)</span>:</span></span><br><span class="line">        bert_out,bert_cls = self.bert(text_ids,attention_mask=masks)</span><br><span class="line">        bert_out = self.dropout(bert_out)</span><br><span class="line">        outputs = []</span><br><span class="line">        <span class="keyword">for</span> tagger <span class="keyword">in</span> self.taggers:</span><br><span class="line">            out = tagger(bert_out).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">            outputs.append(out[:,<span class="number">1</span>:,:])</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs,dim=<span class="number">-2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="ping-ce-zhi-biao">评测指标</h3>
<ol>
<li>位置正确</li>
<li>位置正确且类别正确</li>
</ol>
<h2 id="yuan-su-de-chou-qu">元素的抽取</h2>
<p>给定触发器的条件下，元素抽取器的目的是抽取出和触发器所对应事件相关的元素，以及这些元素扮演的角色。</p>
<p>和触发器抽取相比较，元素的抽取更加复杂，主要有3个原因：</p>
<ol>
<li>元素对触发器的依赖；</li>
<li>大多数元素是较长的名词短语；</li>
<li>角色重叠问题。</li>
</ol>
<p>和触发器抽取器一样，元素抽取器也需要3种嵌入相加作为输入，但还需要知道哪些tokens组成了触发器，因此特征表示输入的segment将触发词所在的span设为1。</p>
<p>为了克服元素抽取面临的后2个问题，作者在BERT上添加了多组二类分类器（多组分类器设为所有角色标签的集合，对每个元素判断所有类型角色的概率）。每组分类器服务于一个角色，以确定所有属于它的元素的范围（span：每个span都包括start和end）。</p>
<p>由于预测和角色是分离的，所以一个元素可以扮演多个角色（对一个元素使用一组二类分类器，一组二类分类器中有多个分类器，对应多个角色），一个token可以属于不同的元素。这就缓解了角色重叠问题。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArgumentExtractor</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bert_path, bert_train, role_type_num, dropout)</span>:</span></span><br><span class="line">        super(ArgumentExtractor, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="comment"># 对bert进行训练</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.bert.named_parameters():</span><br><span class="line">            param.requires_grad = bert_train</span><br><span class="line"></span><br><span class="line">        self.start_taggers = nn.ModuleList(</span><br><span class="line">            [nn.Linear(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>], <span class="number">2</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(role_type_num)])</span><br><span class="line">        self.end_taggers = nn.ModuleList(</span><br><span class="line">            [nn.Linear(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>], <span class="number">2</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(role_type_num)])</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text_lengths, text_ids, masks, type_ids)</span>:</span></span><br><span class="line">        bert_out, bert_cls = self.bert(text_ids, attention_mask=masks, token_type_ids=type_ids)</span><br><span class="line">        bert_out = self.dropout(bert_out)</span><br><span class="line">        start_outputs = []</span><br><span class="line">        end_outputs = []</span><br><span class="line">        <span class="keyword">for</span> start_tagger, end_tagger <span class="keyword">in</span> zip(self.start_tagger, self.end_taggers):</span><br><span class="line">            start_out = start_tagger(bert_out).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">            end_out = end_tagger(bert_out).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">            start_outputs.append(start_out[:, <span class="number">1</span>:, :])</span><br><span class="line">            end_outputs.append(end_out[:, <span class="number">1</span>:, :])</span><br><span class="line">        <span class="keyword">return</span> torch.cat(start_outputs, dim=<span class="number">-2</span>), torch.cat(end_outputs, dim=<span class="number">-2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="ping-ce-zhi-biao-1">评测指标</h3>
<ol>
<li>位置正确</li>
<li>位置正确且类别正确</li>
</ol>
<h2 id="af-ief">AF-IEF</h2>
<p>Role Frequency （RF）<br>
RF定义为角色r在类型为v的事件中出现的频率：<br>
\[
\operatorname{RF}(r, v)=\frac{N_{v}^{r}}{\sum_{k \in \mathcal{R}} N_{v}^{k}}
\]<br>
Inverse Event Frequency （IEF）</p>
<p>log内的分母表示论元角色r在多少个事件从出现<br>
\[
\operatorname{IEF}(r)=\log \frac{|\mathcal{V}|}{|\{v \in \mathcal{V}: r \in v\}|}
\]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_AF_IEF</span><span class="params">(self, dataset)</span>:</span></span><br><span class="line">        event_num = len(self.schema.event_type_2_id)</span><br><span class="line">        role_num = len(self.schema.role_type_2_id)</span><br><span class="line"></span><br><span class="line">        self.rf = np.zeros((event_num, role_num), dtype=float)</span><br><span class="line">        self.ief = np.zeros((role_num, ), dtype=float)</span><br><span class="line">    </span><br><span class="line">         <span class="keyword">with</span> open(os.path.join(self.data_dir, file), <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">             <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                json_item = json.loads(line)</span><br><span class="line">                <span class="keyword">for</span> event <span class="keyword">in</span> json_item[<span class="string">'events'</span>].values():</span><br><span class="line">                    event_id = self.schema.event_type_2_id[event[<span class="string">'event_type'</span>]]</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">for</span> argu <span class="keyword">in</span> event[<span class="string">'argument'</span>]:</span><br><span class="line">                        role_id = self.schema.role_type_2_id[argu[<span class="string">'role_type'</span>]]</span><br><span class="line">                        self.rf[event_id, role_id] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> idx <span class="keyword">in</span> range(role_num):</span><br><span class="line">                    self.ief[idx] = np.log(event_num/np.sum(self.af[:,idx] != <span class="number">0</span>))</span><br><span class="line">                role_count_per_event = np.sum(self.rf, axis=<span class="number">1</span>)</span><br><span class="line">                self.rf = self.rf / (role_count_per_event+<span class="number">1e-13</span>)</span><br></pre></td></tr></table></figure>
<p>将\(RF(r,v)\)和\(IEF(r)\)相乘得到\(RF−IEF(r,v)\)，使用RF-IEF度量角色r对于v类型事件的重要性：</p>
<p>\[
I(r, v)=\frac{\exp ^{\operatorname{RF}-\operatorname{IEF}}(r, v)}{\sum_{r^{\prime} \in R} \exp ^{\operatorname{RF}-\operatorname{IEF}}\left(r^{\prime}, v\right)}
\]<br>
给定输入的事件类型\(v\)，根据每个角色对于\(v\)类型事件的重要性，计算损失\(L_s\)和\(L_e\) ，将两者取平均就得到最终的损失。<br>
\[
\begin{aligned}
\mathcal{L}_{s} &amp;=\sum_{r \in \mathcal{R}} \frac{I(r, v)}{|\mathcal{S}|} \mathrm{CE}\left(P_{s}^{r}, \boldsymbol{y}_{s}^{r}\right) \\
\mathcal{L}_{e} &amp;=\sum_{r \in \mathcal{R}} \frac{I(r, v)}{|\mathcal{S}|} \mathrm{CE}\left(P_{e}^{r}, \boldsymbol{y}_{e}^{r}\right)
\end{aligned}
\]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rf_ief = ief * rf</span><br><span class="line">batch_weight = torch.exp(rf_ief)</span><br><span class="line">batch_weight_sum = torch.sum(batch_weight)</span><br><span class="line">batch_weight = batch_weight / batch_weight_sum</span><br><span class="line">batch_loss = batch_loss * batch_weight</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>技术/论文复现</category>
      </categories>
      <tags>
        <tag>信息抽取</tag>
        <tag>事件抽取</tag>
      </tags>
  </entry>
  <entry>
    <title>词向量</title>
    <url>/2020/06/17/word-embedding/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/06/17/word-embedding/Word2Vec-Training-Models.png" alt></p>
<a id="more"></a>
<h1 id="ci-qian-word-2-vec">词嵌⼊（word2vec）</h1>
<p>⾃然语⾔是⼀套⽤来表达含义的复杂系统。在这套系统中，词是表义的基本单元。顾名思义，词 向量是⽤来表⽰词的向量，也可被认为是词的特征向量或表征。把词映射为实数域向量的技术也叫词嵌⼊（word embedding）。</p>
<h2 id="wei-he-bu-cai-one-hot-xiang-liang">为何不采⽤one-hot向量</h2>
<p>假设词典中不同词的数量（词典⼤小）为\(N\)，每个词可以和从\(0\)到\(N −1\)的连续整数⼀⼀对应。这些与词对应的整数叫作词的索引。假设⼀个词的索引为\(i\)，为了得到该词的one-hot向量表⽰，我们创建⼀个全0的⻓为\(N\)的向量，并将其第\(i\)位设成1。这样⼀来，每个词就表⽰成了⼀个⻓度为\(N\)的向量，可以直接被神经⽹络使⽤。</p>
<p>虽然one-hot词向量构造起来很容易，但通常并不是⼀个好选择。⼀个主要的原因是，one-hot词 向量⽆法准确表达不同词之间的相似度，如我们常常使⽤的余弦相似度。对于向量\(x,y ∈ R^d\) ，它们的余弦相似度是它们之间夹⻆的余弦值:</p>
<p>\[
\frac{\boldsymbol{x}^\top \boldsymbol{y}}{\|\boldsymbol{x}\| \|\boldsymbol{y}\|} \in [-1, 1].
\]</p>
<p>由于任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度难以通过one-hot向量准确地体现出来。</p>
<p>word2vec工具的提出正是为了解决上面这个问题。它将每个词表示成一个定长的向量，并使得这些向量能较好地表达不同词之间的相似和类比关系。word2vec工具包含了两个模型，即跳字模型(skip-gram)和连续词袋模型(continuous bag of words，CBOW)。</p>
<h1 id="skip-gram">skip-gram</h1>
<p>跳字模型假设基于某个词来生成它在文本序列周围的词。举个例子，假设文本序列是the man loves his son。以“loves”作为中心词，设背景窗口大小为2。如下图所示，跳字模型所关心的是，给定中心词“loves”，生成与它距离不超过2个词的背景词the、man、his、son的条件概率，即</p>
<p>\[
P(\textrm{the},\textrm{man},\textrm{his},\textrm{son}\mid\textrm{loves})
\]</p>
<p>假设给定中心词的情况下，背景词的生成是相互独立的，那么上式可以改写成</p>
<p>\[
P(\textrm{the}\mid\textrm{loves})\cdot P(\textrm{man}\mid\textrm{loves})\cdot P(\textrm{his}\mid\textrm{loves})\cdot P(\textrm{son}\mid\textrm{loves})
\]</p>
<p><img src="/2020/06/17/word-embedding/skip-gram.svg" alt></p>
<p>在跳字模型中，每个词被表示成两个\(d\)维向量，用来计算条件概率。假设这个词在词典中索引为\(i\)，当它为中心词时向量表示为\(\boldsymbol{v}_i\in\mathbb{R}^d\)，而为背景词时向量表示为\(\boldsymbol{u}_i\in\mathbb{R}^d\)。设中心词\(w_c\)在词典中索引为\(c\)，背景词\(w_o\)在词典中索引为\(o\)，给定中心词生成背景词的条件概率可以通过对向量内积做softmax运算而得到：</p>
<p>\[
P(w_o \mid w_c) = \frac{\text{exp}(\boldsymbol{u}_o^\top \boldsymbol{v}_c)}{ \sum_{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}
\]</p>
<p>其中词典索引集\(\mathcal{V} = \{0, 1, \ldots, |\mathcal{V}|-1\}\)。假设给定一个长度为\(T\)的文本序列，设时间步\(t\)的词为\(w^{(t)}\)。假设给定中心词的情况下背景词的生成相互独立，当背景窗口大小为\(m\)时，跳字模型的似然函数即给定任一中心词生成所有背景词的概率：</p>
<p>\[
\prod_{t=1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} P(w^{(t+j)} \mid w^{(t)})
\]</p>
<p>这里小于1或大于\(T\)的时间步可以被忽略。</p>
<h2 id="xun-lian-skip-gram">训练skip-gram</h2>
<p>跳字模型的参数是每个词所对应的中心词向量和背景词向量。训练中我们通过最大化似然函数来学习模型参数，即最大似然估计。这等价于最小化以下损失函数：</p>
<p>\[
-\sum_{t=1}^{T} \sum_{-m \leq j \leq m,\ j \neq 0} \text{log}\, P(w^{(t+j)} \mid w^{(t)})
\]</p>
<p>如果使用随机梯度下降，那么在每一次迭代里，随机采样一个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数。梯度计算的关键是条件概率的对数有关中心词向量和背景词向量的梯度。根据定义，</p>
<p>\[
\log P(w_o \mid w_c) =
\boldsymbol{u}_o^\top \boldsymbol{v}_c - \log\left(\sum_{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)\right)
\]</p>
<p>通过微分，我们可以得到上式中\(\boldsymbol{v}_c\)的梯度</p>
<p>\[
\begin{aligned}
\frac{\partial \text{log}\, P(w_o \mid w_c)}{\partial \boldsymbol{v}_c} 
&amp;= \boldsymbol{u}_o - \frac{\sum_{j \in \mathcal{V}} \exp(\boldsymbol{u}_j^\top \boldsymbol{v}_c)\boldsymbol{u}_j}{\sum_{i \in \mathcal{V}} \exp(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}\\
&amp;= \boldsymbol{u}_o - \sum_{j \in \mathcal{V}} \left(\frac{\text{exp}(\boldsymbol{u}_j^\top \boldsymbol{v}_c)}{ \sum_{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}\right) \boldsymbol{u}_j\\ 
&amp;= \boldsymbol{u}_o - \sum_{j \in \mathcal{V}} P(w_j \mid w_c) \boldsymbol{u}_j.
\end{aligned}
\]</p>
<p>它的计算需要词典中所有词以\(w_c\)为中心词的条件概率。有关其他词向量的梯度同理可得。</p>
<p>训练结束后，对于词典中的任一索引为\(i\)的词，均得到该词作为中心词和背景词的两组词向量\(\boldsymbol{v}_i\)和\(\boldsymbol{u}_i\)。在自然语言处理应用中，一般使用跳字模型的<strong>中心词向量作为词的表征向量</strong>。</p>
<h1 id="cbow">CBOW</h1>
<p>连续词袋模型与跳字模型类似。与跳字模型最大的不同在于，连续词袋模型假设基于某中心词在文本序列前后的背景词来生成该中心词。在同样的文本序列the man loves his son里，以“loves”作为中心词，且背景窗口大小为2时，连续词袋模型关心的是，给定背景词the、man、his、son生成中心词“loves”的条件概率，也就是</p>
<p>\[
P(\textrm{loves}\mid\textrm{the},\textrm{man},\textrm{his},\textrm{son})
\]</p>
<p><img src="/2020/06/17/word-embedding/cbow.svg" alt></p>
<p>因为连续词袋模型的背景词有多个，将这些背景词向量取<strong>平均</strong>，然后使用和跳字模型一样的方法来计算条件概率。设\(\boldsymbol{v_i}\in\mathbb{R}^d\)和\(\boldsymbol{u_i}\in\mathbb{R}^d\)分别表示词典中索引为\(i\)的词作为背景词和中心词的向量（注意符号的含义与跳字模型中的相反）。设中心词\(w_c\)在词典中索引为\(c\)，背景词\(w_{o_1}, \ldots, w_{o_{2m}}\)在词典中索引为\(o_1, \ldots, o_{2m}\)，那么给定背景词生成中心词的条件概率</p>
<p>\[
P(w_c \mid w_{o_1}, \ldots, w_{o_{2m}}) = \frac{\text{exp}\left(\frac{1}{2m}\boldsymbol{u}_c^\top (\boldsymbol{v}_{o_1} + \ldots + \boldsymbol{v}_{o_{2m}}) \right)}{ \sum_{i \in \mathcal{V}} \text{exp}\left(\frac{1}{2m}\boldsymbol{u}_i^\top (\boldsymbol{v}_{o_1} + \ldots + \boldsymbol{v}_{o_{2m}}) \right)}
\]</p>
<p>为了让符号更加简单，我们记\(\mathcal{W}_o= \{w_{o_1}, \ldots, w_{o_{2m}}\}\)，且\(\bar{\boldsymbol{v}}_o = \left(\boldsymbol{v}_{o_1} + \ldots + \boldsymbol{v}_{o_{2m}} \right)/(2m)\)，那么上式可以简写成</p>
<p>\[
P(w_c \mid \mathcal{W}_o) = \frac{\exp\left(\boldsymbol{u}_c^\top \bar{\boldsymbol{v}}_o\right)}{\sum_{i \in \mathcal{V}} \exp\left(\boldsymbol{u}_i^\top \bar{\boldsymbol{v}}_o\right)}
\]</p>
<p>给定一个长度为\(T\)的文本序列，设时间步\(t\)的词为\(w^{(t)}\)，背景窗口大小为\(m\)。连续词袋模型的似然函数是由背景词生成任一中心词的概率<br>
\[
\prod_{t=1}^{T}  P(w^{(t)} \mid  w^{(t-m)}, \ldots,  w^{(t-1)},  w^{(t+1)}, \ldots,  w^{(t+m)})
\]</p>
<h2 id="xun-lian-cbow">训练CBOW</h2>
<p>训练连续词袋模型同训练跳字模型基本一致。连续词袋模型的最大似然估计等价于最小化损失函数</p>
<p>\[
-\sum_{t=1}^T  \text{log}\, P(w^{(t)} \mid  w^{(t-m)}, \ldots,  w^{(t-1)},  w^{(t+1)}, \ldots,  w^{(t+m)})
\]</p>
<p>注意到</p>
<p>\[
\log\,P(w_c \mid \mathcal{W}_o) = \boldsymbol{u}_c^\top \bar{\boldsymbol{v}}_o - \log\,\left(\sum_{i \in \mathcal{V}} \exp\left(\boldsymbol{u}_i^\top \bar{\boldsymbol{v}}_o\right)\right)
\]</p>
<p>通过微分，可以计算出上式中条件概率的对数有关任一背景词向量\(\boldsymbol{v}_{o_i}\)（\(i = 1, \ldots, 2m\)）的梯度</p>
<p>\[
\frac{\partial \log\, P(w_c \mid \mathcal{W}_o)}{\partial \boldsymbol{v}_{o_i}} = \frac{1}{2m} \left(\boldsymbol{u}_c - \sum_{j \in \mathcal{V}} \frac{\exp(\boldsymbol{u}_j^\top \bar{\boldsymbol{v}}_o)\boldsymbol{u}_j}{ \sum_{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \bar{\boldsymbol{v}}_o)} \right) = \frac{1}{2m}\left(\boldsymbol{u}_c - \sum_{j \in \mathcal{V}} P(w_j \mid \mathcal{W}_o) \boldsymbol{u}_j \right)
\]</p>
<p>有关其他词向量的梯度同理可得。同跳字模型不一样的一点在于，一般使用连续词袋模型的<strong>背景词向量作为词的表征向量</strong>。</p>
<h1 id="pytorch-dai-ma-shi-xian">pytorch 代码实现</h1>
<h2 id="zhun-bei">准备</h2>
<p>PyTorch 中的 nn.Embedding中，有两个必选的参数：<code>num_embeddings</code>表示单词的总数目，<code>embedding_dim</code>表示每个单词需要用什么维度的向量表示。而nn.Embedding权重的维度也是(num_embeddings, embedding_dim)，默认是随机初始化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">embeds = nn.Embedding(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">embeds.weight</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[<span class="number">-1.1454</span>,  <span class="number">0.3675</span>, <span class="number">-0.3718</span>,  <span class="number">0.3733</span>,  <span class="number">0.5979</span>],</span><br><span class="line">        [<span class="number">-0.7952</span>, <span class="number">-0.9794</span>,  <span class="number">0.6292</span>, <span class="number">-0.3633</span>, <span class="number">-0.2037</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果使用预训练好的词向量</span></span><br><span class="line">pretrained_weight = np.array(pretrained_weight)</span><br><span class="line">embeds.weight.data.copy_(torch.from_numpy(pretrained_weight))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 想要查看某个词的词向量，需要传入这个词在词典中的 index，并且这个 index 得是 LongTensor 型的</span></span><br><span class="line">embeds = nn.Embedding(<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line">embeds(torch.LongTensor([<span class="number">50</span>]))</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([[<span class="number">-1.9562e-03</span>,  <span class="number">1.8971e+00</span>,  <span class="number">7.0230e-01</span>, <span class="number">-6.3762e-01</span>, <span class="number">-1.9426e-01</span>,</span><br><span class="line">          <span class="number">3.4200e-01</span>, <span class="number">-2.0908e+00</span>, <span class="number">-3.0827e-01</span>,  <span class="number">9.6250e-01</span>, <span class="number">-7.2700e-01</span>]],</span><br><span class="line">       grad_fn=&lt;EmbeddingBackward&gt;)</span><br></pre></td></tr></table></figure>
<p>首先 Embedding 层输入的 shape 是<code>(batchsize, seq_len)</code>，输出的 shape 是<code>(batchsize, embedding_dim)</code></p>
<p><img src="/2020/06/17/word-embedding/GOImi8.png" alt></p>
<p>上图的流程是把文章中的单词使用词向量来表示</p>
<ol>
<li>提取文章所有的单词，把所有的单词按照频次降序排序（取前 4999 个，表示常出现的单词。其余所有单词均用’<UNK>'表示。所以一共有 5000 个单词）</UNK></li>
<li>5000 个单词使用 one-hot 编码</li>
<li>通过训练会生成一个 \(5000×300\) 的矩阵，每一行向量表示一个词的词向量。这里的 <code>300</code> 是人为指定，想要每个词最终编码为词向量的维度，也可以设置成别的。</li>
</ol>
<p>这个矩阵如何获得呢？在 Skip-gram 模型中，首先会随机初始化这个矩阵，然后通过一层神经网络来训练。最终这个一层神经网络的所有权重，就是要求的词向量的矩阵</p>
<p><img src="/2020/06/17/word-embedding/GOHgHA.png" alt></p>
<p>从上面的图中看到，我们所学习的 embedding 层是一个训练任务的一小部分，根据任务目标反向传播，学习到 embedding 层里的权重 weight。</p>
<h2 id="shu-ju">数据</h2>
<p><a href="https://share.weiyun.com/jYYyKpON" target="_blank" rel="noopener">训练语料</a>下载，文件中的内容是英文文本，去除了标点符号，每个单词之间用空格隔开。</p>
<p><img src="/2020/06/17/word-embedding/Gv38YD.png" alt></p>
<h2 id="dai-ma">代码</h2>
<figure class="highlight python"><figcaption><span>数据处理</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> tud</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line"></span><br><span class="line">random.seed(<span class="number">1</span>)</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># parameters</span></span><br><span class="line">C = <span class="number">3</span> <span class="comment"># context window，论文中选取左右多少个单词作为背景词</span></span><br><span class="line">K = <span class="number">15</span> <span class="comment"># number of negative samples，表示随机选取 15 个噪声词。</span></span><br><span class="line">epochs = <span class="number">20</span></span><br><span class="line">MAX_VOCAB_SIZE = <span class="number">50000</span> <span class="comment"># 训练 50000 个词的词向量，但实际上我只会选出语料库中出现次数最多的 49999 个词，还有一个词是&lt;UNK&gt;用来表示所有的其它词。</span></span><br><span class="line">EMBEDDING_SIZE = <span class="number">300</span> <span class="comment"># 词向量维度</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取文本数据</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'text8.train.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    text = f.read() <span class="comment"># 得到文本内容</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line">text = text.lower().split() <span class="comment">#　分割成单词列表</span></span><br><span class="line">vocab_dict = dict(Counter(text).most_common(MAX_VOCAB_SIZE - <span class="number">1</span>)) <span class="comment"># 得到单词字典表，key是单词，value是次数</span></span><br><span class="line">vocab_dict[<span class="string">'&lt;UNK&gt;'</span>] = len(text) - np.sum(list(vocab_dict.values())) <span class="comment"># 把不常用的单词都编码为"&lt;UNK&gt;"</span></span><br><span class="line">idx2word = [word <span class="keyword">for</span> word <span class="keyword">in</span> vocab_dict.keys()]</span><br><span class="line">word2idx = &#123;word:i <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(idx2word)&#125;</span><br><span class="line">word_counts = np.array([count <span class="keyword">for</span> count <span class="keyword">in</span> vocab_dict.values()], dtype=np.float32)</span><br><span class="line">word_freqs = word_counts / np.sum(word_counts)</span><br><span class="line">word_freqs = word_freqs ** (<span class="number">3.</span>/<span class="number">4.</span>)</span><br></pre></td></tr></table></figure>
<p>最后一行代码，<code>word_freqs</code>存储了每个单词的频率，然后又将所有的频率变为原来的 0.75 次方，这是因为 word2vec 论文里面推荐这么做。</p>
<blockquote>
<p>（频繁词的二次采样）根据论文描述在大的语料库中，频繁词如容易出现很多次的the\in\a提供的信息量远没有罕见词提供的信息量多，因此在后续的训练中频繁词无法提供更多的信息甚至会将网络带偏，因此提出了频繁词二次采样方式：即在每次训练时按照如下公式对训练集的单词\(w_i\)进行丢弃：<br>
\[
P\left(w_{i}\right)=1-\sqrt{\frac{t}{f\left(w_{i}\right)}}
\]</p>
</blockquote>
<p><img src="/2020/06/17/word-embedding/GvGoQJ.png" alt></p>
<figure class="highlight python"><figcaption><span>DataLoader</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordEmbeddingDataset</span><span class="params">(tud.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, text, word2idx, idx2word, word_freqs, word_counts)</span>:</span></span><br><span class="line">        <span class="string">''' text: a list of words, all text from the training dataset</span></span><br><span class="line"><span class="string">            word2idx: the dictionary from word to index</span></span><br><span class="line"><span class="string">            idx2word: index to word mapping</span></span><br><span class="line"><span class="string">            word_freqs: the frequency of each word</span></span><br><span class="line"><span class="string">            word_counts: the word counts</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super(WordEmbeddingDataset, self).__init__() <span class="comment"># #通过父类初始化模型，然后重写两个方法</span></span><br><span class="line">        self.text_encoded = [word2idx.get(word, word2idx[<span class="string">'&lt;UNK&gt;'</span>]) <span class="keyword">for</span> word <span class="keyword">in</span> text] <span class="comment"># 把单词数字化表示。如果不在词典中，也表示为unk</span></span><br><span class="line">        self.text_encoded = torch.LongTensor(self.text_encoded) <span class="comment"># nn.Embedding需要传入LongTensor类型</span></span><br><span class="line">        self.word2idx = word2idx</span><br><span class="line">        self.idx2word = idx2word</span><br><span class="line">        self.word_freqs = torch.Tensor(word_freqs)</span><br><span class="line">        self.word_counts = torch.Tensor(word_counts)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.text_encoded) <span class="comment"># 返回所有单词的总数，即item的总数</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="string">''' 这个function返回以下数据用于训练</span></span><br><span class="line"><span class="string">            - 中心词</span></span><br><span class="line"><span class="string">            - 这个单词附近的positive word</span></span><br><span class="line"><span class="string">            - 随机采样的K个单词作为negative word</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        center_words = self.text_encoded[idx] <span class="comment"># 取得中心词</span></span><br><span class="line">        pos_indices = list(range(idx - C, idx)) + list(range(idx + <span class="number">1</span>, idx + C + <span class="number">1</span>)) <span class="comment"># 先取得中心左右各C个词的索引</span></span><br><span class="line">        pos_indices = [i % len(self.text_encoded) <span class="keyword">for</span> i <span class="keyword">in</span> pos_indices] <span class="comment"># 为了避免索引越界，所以进行取余处理</span></span><br><span class="line">        pos_words = self.text_encoded[pos_indices] <span class="comment"># tensor(list)</span></span><br><span class="line">        </span><br><span class="line">        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[<span class="number">0</span>], <span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># torch.multinomial作用是对self.word_freqs做K * pos_words.shape[0]次取值，输出的是self.word_freqs对应的下标</span></span><br><span class="line">        <span class="comment"># 取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大</span></span><br><span class="line">        <span class="comment"># 每采样一个正确的单词(positive word)，就采样K个错误的单词(negative word)，pos_words.shape[0]是正确单词数量</span></span><br><span class="line">        <span class="keyword">return</span> center_words, pos_words, neg_words</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 构建数据集和dataloader</span></span><br><span class="line">dataset = WordEmbeddingDataset(text, word2idx, idx2word, word_freqs, word_counts)</span><br><span class="line">dataloader = tud.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>假设所有文本分词，转为索引之后的 list 如下图所示</p>
<p><img src="/2020/06/17/word-embedding/AZoONacteCBhF67.png" alt></p>
<p>根据论文所述，假定 window size=2，即每个中心词左右各取 2 个词作为背景词，那么对于上面的 list，窗口每次滑动，选定的中心词和背景词如下图所示</p>
<p><img src="/2020/06/17/word-embedding/K1VA8E7lHtOohme.png" alt></p>
<p>那么 skip_grams 变量里存的就是中心词和背景词一一配对后的 list，例如中心词 2，有背景词 0,1,0,1，一一配对以后就会产生 [2,0],[2,1],[2,0],[2,1]。skip_grams 如下图所示</p>
<p><img src="/2020/06/17/word-embedding/idWyM2YgruoGzUa.png" alt></p>
<figure class="highlight python"><figcaption><span>模型</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbeddingModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size)</span>:</span></span><br><span class="line">        super(EmbeddingModel, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        </span><br><span class="line">        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size)</span><br><span class="line">        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_labels, pos_labels, neg_labels)</span>:</span></span><br><span class="line">        input_embedding = self.in_embed(input_labels) <span class="comment"># [batch_size, 1,embed_size],中心词：1个</span></span><br><span class="line">        pos_embedding = self.out_embed(pos_labels)<span class="comment"># [batch_size, (window * 2), embed_size]</span></span><br><span class="line">        neg_embedding = self.out_embed(neg_labels) <span class="comment"># [batch_size, (window * 2 * K), embed_size]</span></span><br><span class="line">        </span><br><span class="line">        input_embedding = input_embedding.unsqueeze(<span class="number">2</span>) <span class="comment"># [batch_size, embed_size, 1]</span></span><br><span class="line">        </span><br><span class="line">        pos_dot = torch.bmm(pos_embedding, input_embedding) <span class="comment"># [batch_size, (window * 2), 1]</span></span><br><span class="line">        pos_dot = pos_dot.squeeze(<span class="number">2</span>) <span class="comment"># [batch_size, (window * 2)]</span></span><br><span class="line">        </span><br><span class="line">        neg_dot = torch.bmm(neg_embedding, -input_embedding) <span class="comment"># [batch_size, (window * 2 * K), 1]</span></span><br><span class="line">        neg_dot = neg_dot.squeeze(<span class="number">2</span>) <span class="comment"># batch_size, (window * 2 * K)]</span></span><br><span class="line">        </span><br><span class="line">        log_pos = F.logsigmoid(pos_dot).sum(<span class="number">1</span>) <span class="comment"># .sum()结果只为一个数，.sum(1)结果是一维的张量</span></span><br><span class="line">        log_neg = F.logsigmoid(neg_dot).sum(<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        loss = log_pos + log_neg</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> -loss</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">input_embedding</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.in_embed.weight.numpy()</span><br></pre></td></tr></table></figure>
<p>这里为什么要分两个 embedding 层来训练？</p>
<p>很明显，对于任一一个词，它既有可能作为中心词出现，也有可能作为背景词出现，所以每个词需要用两个向量去表示。<code>in_embed</code>训练出来的权重就是每个词作为中心词的权重。<code>out_embed</code>训练出来的权重就是每个词作为背景词的权重。那么最后到底用什么向量来表示一个词呢？是中心词向量？还是背景词向量？按照 Word2Vec 论文所写，推荐使用中心词向量，所以这里最后返回的是<code>in_embed.weight</code>。</p>
<blockquote>
<p>介绍一个比较常用的batch 矩阵乘法：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch1 = torch.randn(<span class="number">10</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">batch2 = torch.randn(<span class="number">10</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">res = torch.bmm(batch1, batch2)</span><br><span class="line">print(res.size())</span><br><span class="line"><span class="comment"># torch.Size([10, 3, 5])</span></span><br></pre></td></tr></table></figure>
</blockquote>
<figure class="highlight python"><figcaption><span>训练模型</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> i, (input_labels, pos_labels, neg_labels) <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">        input_labels = input_labels.long()</span><br><span class="line">        pos_labels = pos_labels.long()</span><br><span class="line">        neg_labels = neg_labels.long()</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss = model(input_labels, pos_labels, neg_labels).mean()</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch'</span>, e, <span class="string">'iteration'</span>, i, loss.item())</span><br><span class="line"></span><br><span class="line">embedding_weights = model.input_embeddings()</span><br><span class="line">torch.save(model.state_dict(), <span class="string">"embedding-&#123;&#125;.th"</span>.format(EMBEDDING_SIZE))</span><br></pre></td></tr></table></figure>
<h2 id="ci-xiang-liang-ying-yong">词向量应用</h2>
<p>找出与某个词相近的一些词，比方说输入 good，他能找出 nice，better，best 之类的词。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_nearest</span><span class="params">(word)</span>:</span></span><br><span class="line">    index = word2idx[word]</span><br><span class="line">    embedding = embedding_weights[index]</span><br><span class="line">    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) <span class="keyword">for</span> e <span class="keyword">in</span> embedding_weights])</span><br><span class="line">    <span class="keyword">return</span> [idx_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> cos_dis.argsort()[:<span class="number">10</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> [<span class="string">"two"</span>, <span class="string">"america"</span>, <span class="string">"computer"</span>]:</span><br><span class="line">    print(word, find_nearest(word))</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">two [<span class="string">'two'</span>, <span class="string">'zero'</span>, <span class="string">'four'</span>, <span class="string">'one'</span>, <span class="string">'six'</span>, <span class="string">'five'</span>, <span class="string">'three'</span>, <span class="string">'nine'</span>, <span class="string">'eight'</span>, <span class="string">'seven'</span>]</span><br><span class="line">america [<span class="string">'america'</span>, <span class="string">'states'</span>, <span class="string">'japan'</span>, <span class="string">'china'</span>, <span class="string">'usa'</span>, <span class="string">'west'</span>, <span class="string">'africa'</span>, <span class="string">'italy'</span>, <span class="string">'united'</span>, <span class="string">'kingdom'</span>]</span><br><span class="line">computer [<span class="string">'computer'</span>, <span class="string">'machine'</span>, <span class="string">'earth'</span>, <span class="string">'pc'</span>, <span class="string">'game'</span>, <span class="string">'writing'</span>, <span class="string">'board'</span>, <span class="string">'result'</span>, <span class="string">'code'</span>, <span class="string">'website'</span>]</span><br></pre></td></tr></table></figure>
<h2 id="nn-linear-vs-nn-embedding">nn.Linear vs nn.Embedding</h2>
<p>Word2Vec 论文中给出的架构其实就一个单层神经网络，那么为什么直接用<code>nn.Linear()</code>来训练呢？<code>nn.Linear()</code>不是也能训练出一个 weight 吗？</p>
<p>答案是可以的，当然可以直接使用<code>nn.Linear()</code>，只不过输入要改为 one-hot Encoding，而不能像<code>nn.Embedding()</code>这种方式直接传入一个 index。还有就是需要设置<code>bias</code>，因为只需要训练一个权重矩阵，不训练偏置。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Word2Vec</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    super(Word2Vec, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># W and V is not Traspose relationship</span></span><br><span class="line">    self.W = nn.Parameter(torch.randn(voc_size, embedding_size).type(dtype))</span><br><span class="line">    self.V = nn.Parameter(torch.randn(embedding_size, voc_size).type(dtype))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="comment"># X : [batch_size, voc_size] one-hot</span></span><br><span class="line">    <span class="comment"># torch.mm only for 2 dim matrix, but torch.matmul can use to any dim</span></span><br><span class="line">    hidden_layer = torch.matmul(X, self.W) <span class="comment"># hidden_layer : [batch_size, embedding_size]</span></span><br><span class="line">    output_layer = torch.matmul(hidden_layer, self.V) <span class="comment"># output_layer : [batch_size, voc_size]</span></span><br><span class="line">    <span class="keyword">return</span> output_layer</span><br></pre></td></tr></table></figure>
<h1 id="zong-jie">总结</h1>
<ol>
<li>word2vec包含跳字模型和连续词袋模型。</li>
<li>跳字模型假设基于中心词来生成背景词。在应用中，使用中心词的词向量作为词的表征向量</li>
<li>连续词袋模型假设基于背景词来生成中心词。在应用中，使用背景词的词向量作为词的表征向量。</li>
<li>word2vec是静态词向量预训练模型，词向量是固定的，不能解决多义词问题，无法考虑预料全局信息。</li>
</ol>
<h1 id="can-kao">参考</h1>
<ol>
<li>
<p>Penn Tree Bank. <a href="https://catalog.ldc.upenn.edu/LDC99T42" target="_blank" rel="noopener">https://catalog.ldc.upenn.edu/LDC99T42</a></p>
</li>
<li>
<p>Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119).</p>
</li>
<li>
<p><a href="https://www.wmathor.com/index.php/archives/1435/" target="_blank" rel="noopener">PyTorch 实现 Word2Vec</a></p>
</li>
<li>
<p><a href="https://www.bilibili.com/video/BV12W411v7Ga?from=search&amp;seid=9314405103672286153" target="_blank" rel="noopener">[MXNet/Gluon] 动手学深度学习第十六课：词向量（word2vec）</a></p>
</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>Circle Loss</title>
    <url>/2020/06/07/research/circle_loss/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/06/07/research/circle_loss/image-20200607122847009.png" alt></p>
<a id="more"></a>
<p>旷世在CVPR 2020上的一篇<a href="https://arxiv.org/abs/2002.10857" target="_blank" rel="noopener">论文</a>。</p>
<p>简单来讲，原来特征学习有 2 种基本范式，分类学习和 pairwise 学习，人们普遍都觉得这两者虽然有联系，但是总体上仍是割裂的。旷视在这项工作中首次将两者放在一个统一的框架下，用一个general 的公式定义了这两种范式，且在这统一的公式下，获得了比两者各自最高水平方法都要好的性能。这项工作已经发表在CVPR 2020。</p>
<h1 id="dong-ji">动机</h1>
<p>深度特征学习有两种基本范式，分别是使用类标签和使用正负样本对标签进行学习。使用类标签时，一般需要用分类损失函数（比如 softmax + cross entropy）优化样本和权重向量之间的相似度；使用样本对标签时，通常用度量损失函数（比如 triplet 损失）来优化样本之间的相似度。</p>
<blockquote>
<p>softmax : \(S_j = \frac{e^{a_j}}{\sum^N_{k=1}{e^{a_k}}}\)</p>
<p>softmax 能够保证</p>
<ol>
<li>所有的值都是 [0, 1] 之间的（因为概率必须是 [0, 1]）</li>
<li>所有的值加起来等于 1</li>
</ol>
<p>Cross-Entropy：\(H(p_{y^\prime},q)=-\sum_{j=1}^K{p_{y^{\prime}_j}log(q_j)}\),其中\(log(q(k))\)是logSoftmax。</p>
<p>Cross-Entropy是用来衡量两个概率分布之间的距离的,也就是预测类别和正确类别之间的损失。如：<code>a=[1,2,3]</code>,<code>b=[2,4,6]</code>，那么 cross entropy距离就是0。</p>
<p>在pytorch中，<code>torch.nn.CrossEntropyLoss = torch.nn.LogSoftmax+torch.nn.NLLLoss</code></p>
<p>triplet loss</p>
<p>Triplet loss 最初是在人脸识别中为了学习一个较好的人脸embedding，而<code>softmax</code>最终的类别数是确定的，而<code>Triplet loss</code>学到的是一个好的<code>embedding</code>，相似的图像在<code>embedding</code>空间里是相近的，可以判断是否是同一个人脸。</p>
<p>输入是一个三元组 <code>&lt;a, p, n&gt;</code></p>
<ul>
<li><code>a： anchor</code></li>
<li><code>p： positive</code>, 与 <code>a</code> 是同一类别的样本</li>
<li><code>n： negative</code>, 与 <code>a</code> 是不同类别的样本</li>
</ul>
<p>公式是：<br>
\[
L = max(d(a,p)-d(a,n)+margin,0)
\]<br>
最终的优化目标是拉近 <code>a, p</code> 的距离， 拉远 <code>a, n</code> 的距离</p>
</blockquote>
<p>这两种学习方法之间并无本质区别，其目标都是最大化类内相似度\(S_p\)和最小化类间相似度\(S_n\)。从这个角度看，很多常用的损失函数（如 triplet 损失、softmax 损失及其变体）有着相似的优化模式：它们会将\(S_n\)和\(S_p\)组合成相似对来优化，并试图减小\((S_n-S_p)\)。在\((S_n - S_p)\)中，增大\(S_p\)等效于降低\(S_n\)。这种对称的优化方法容易出现下面两个问题：</p>
<p><img src="/2020/06/07/research/circle_loss/image-20200607122847009.png" alt></p>
<h2 id="1-you-hua-que-fa-ling-huo-xing">1.  优化缺乏灵活性</h2>
<p>\(S_n\)和\(S_p\)上的惩罚力度是严格相等的。换而言之，给定指定的损失函数，在\(S_n\)和\(S_p\)上的梯度的幅度总是一样的。例如图<code>(a)</code>中A点，它的\(S_n\)已经很小了，但是\(S_n\)会不断受到较大的梯度。这样的现象低效切不合理。</p>
<h2 id="2-shou-lian-zhuang-tai-bu-ming-que">2. 收敛状态不明确</h2>
<p>优化\((S_n-S_p)\)得到的决策边界为\(S_n-S_p=m\)。这个决策边界平行于\(S_n=S_p\),维持边界上任意两个点(比如\(T=(0.4,0.7)\)和\(T^\prime (0.2,0.5)\))的对应难度相等，这种决策边界允许模棱两可的收敛状态。比如，\(T\)和\(T^\prime\)都满足了\(S_p-S_n=0.3\)的目标，可是比较两者时，会发现二者之间的分离量只有0.1\((S_p^\prime -S-n) = 0.1\),从而降低了特征空间的可分性。</p>
<h1 id="jian-jie">简介</h1>
<p>为此，旷视研究院仅仅做了一项非常简单的改变，把\((S_n-S_p)\)泛化为\((\alpha_nS_n-\alpha_pS_p)\),从而允许\(S_n\)和\(S_p\)能以各自不同的步调学习。</p>
<p>具体来讲，把\(\alpha_n\)和\(\alpha_p\) 分别实现为\(S_n\) 和\(S_p\) 各自的线性函数，使学习速度与优化状态相适应。相似度分数偏离最优值越远，加权因子就越大。如此优化得到的决策边界为\(\alpha_nS_n-\alpha_pS_p=m\),能够证明这个分界面是\((S_n,S_p)\) 空间中的一段圆弧，因此，这一新提出的损失函数称之为 <code>Circle Loss</code>，即圆损失函数。<br>
由图 (a) 可知，降低\(S_n-S_p\) 容易导致优化不灵活（A、B、C 相较于\(S_n\)和\(S_p\)的梯度都相等）以及收敛状态不明确（决策边界上的 \(T\) 和 \(T\prime\) 都可接受）；而在 <code>Circle Loss</code> 所对应的图 (b) 中，减小\((\alpha_nS_n-\alpha_pS_p)\)  会动态调整其在\(S_n\) 和\(S_p\) 上的梯度，由此能使优化过程更加灵活。</p>
<p>对于状态 A，它的\(S_p\)很小（而\(S_n\) 已经足够小），因此其重点是增大\(S_p\) ；对于 B，它的 \(S_n\) 很大 （而\(S_p\)已经足够大），因此其重点是降低\(S_n\) 。此外，圆形决策边界上的特定点 \(T\)（圆弧与45度斜线的切点）更有利于收敛。因此，<code>Circle Loss</code> 设计了一个更灵活的优化途径，通向一个更明确的优化目标。</p>
<p><code>Circle Loss</code> 非常简单，而它对深度特征学习的意义却非常本质，表现为以下三个方面:</p>
<ol>
<li>统一的（广义）损失函数。从统一的相似度配对优化角度出发，它为两种基本学习范式（即使用类别标签和使用样本对标签的学习）提出了一种统一的损失函数；</li>
<li>灵活的优化方式。在训练期间，向 \(S_n\) 或\(S_p\) 的梯度反向传播会根据权重\(\alpha_n\) 或\(\alpha_p\) 来调整幅度大小。那些优化状态不佳的相似度分数，会被分配更大的权重因子，并因此获得更大的更新梯度。如图 (b) 所示，在 <code>Circle Loss</code> 中，A、B、C 三个状态对应的优化各有不同。</li>
<li>明确的收敛状态。在这个圆形的决策边界上，Circle Loss 更偏爱特定的收敛状态（图  (b) 中的 \(T\)）。这种明确的优化目标有利于提高特征鉴别力。</li>
</ol>
<h1 id="tong-yi-de-xiang-si-xing-you-hua-shi-jiao">统一的相似性优化视角</h1>
<p>深度特征学习的优化目标是最大化 \(S_p\) ，最小化\(S_n\) 。在两种基本学习范式中，采用的损失函数通常大相径庭，比如sofmax loss 和 triplet loss。<br>
这里不去在意相似性计算的具体方式——无论是样本对之间的相似性（相似性对标签情况下）还是样本与类别代理之间的相似性（类别标签情况下）。本文仅仅做这样一个假设定义：给定特征空间中的单个样本 \(x\)，假设与\(x\)相关的类内相似度分数有\(K\) 个，与 \(x\) 相关的类间相似度分数有\(L\) 个，分别记为 \(\{S^i_p\}(i=1,2,\ldots,K)\) 和 \(\{S^j_n\}(j=1,2,\ldots,L)\)。</p>
<p>为了实现最大化\(S_p\)最小化\(S_n\)的优化目标，本文提出把所有的\(S_p\)和\(S_n\) 两两配对，并通过在所有的相似性对上穷举、减小二者之差，来获得以下的统一损失函数：<br>
\[
\begin{aligned}
\mathcal{L}_{u n i} &amp;=\log \left[1+\sum_{i=1}^{K} \sum_{j=1}^{L} \exp \left(\gamma\left(s_{n}^{j}-s_{p}^{i}+m\right)\right)\right] \\
&amp;=\log \left[1+\sum_{j=1}^{L} \exp \left(\gamma\left(s_{n}^{j}+m\right)\right) \sum_{i=1}^{K} \exp \left(\gamma\left(-s_{p}^{i}\right)\right)\right]
\end{aligned}
\]<br>
这个公式仅需少量修改就能降级得到常见的 triplet 损失或分类损失，比如得到:</p>
<p>AM-Softmax 损失。</p>
<blockquote>
<p>Softmax loss通常擅长优化类间差异（即，分离不同的类），但是不善于减少类内的变化（即，使相同类的特征紧凑）。AM-Softmax 将角度间距（angular margin）引入到softmax中，来最小化类内变化。</p>
</blockquote>
<p>\[
\begin{aligned}
\mathcal{L}_{a m} &amp;=\log \left[1+\sum_{j=1}^{N-1} \exp \left(\gamma\left(s_{n}^{j}+m\right)\right) \exp \left(-\gamma s_{p}\right)\right] \\
&amp;=-\log \frac{\exp \left(\gamma\left(s_{p}-m\right)\right)}{\exp \left(\gamma\left(s_{p}-m\right)\right)+\sum_{j=1}^{N-1} \exp \left(\gamma s_{n}^{j}\right)}
\end{aligned}
\]</p>
<p>或 triplet 损失：<br>
\[
\begin{aligned}
\mathcal{L}_{t r i} &amp;=\lim _{\gamma \rightarrow+\infty} \frac{1}{\gamma} \mathcal{L}_{u n i} \\
&amp;=\lim _{\gamma \rightarrow+\infty} \frac{1}{\gamma} \log \left[1+\sum_{i=1}^{K} \sum_{j=1}^{L} \exp \left(\gamma\left(s_{n}^{j}-s_{p}^{i}+m\right)\right)\right] \\
&amp;=\max \left[s_{n}^{j}-s_{p}^{i}\right]_{+}
\end{aligned}
\]</p>
<h1 id="circle-loss-zi-ding-bu-diao-de-jia-quan-fang-shi">Circle Loss：自定步调的加权方式</h1>
<p>忽略上文<code>Circle Loss</code>等式中的余量项 \(m\)并对\(S_n\) 和\(S_p\)  进行加权，可得到新提出的 <code>Circle Loss</code>：<br>
\[
\begin{aligned}
\mathcal{L}_{\text {circle }} &amp;=\log \left[1+\sum_{i=1}^{K} \sum_{j=1}^{L} \exp \left(\gamma\left(\alpha_{n}^{j} s_{n}^{j}-\alpha_{p}^{i} s_{p}^{i}\right)\right)\right] \\
&amp;=\log \left[1+\sum_{j=1}^{L} \exp \left(\gamma \alpha_{n}^{j} s_{n}^{j}\right) \sum_{i=1}^{K} \exp \left(-\gamma \alpha_{p}^{i} s_{p}^{i}\right)\right.
\end{aligned}
\]<br>
再定义\(S_p\)  的最优值为\(O_p\)，\(S_n\) 的最优值为\(O_n\)；\(O_n &lt; O_p\) 。当一个相似性得分与最优值偏离较远，<code>Circle Loss</code> 将分配较大的权重，从而对它进行强烈的优化更新。自定步调（self-paced）的方式给出了如下定义：<br>
\[
\left\{\begin{array}{c}
\alpha_{p}^{i}=\left[O_{p}-s_{p}^{i}\right]_{+} \\
\alpha_{n}^{j}=\left[s_{n}^{j}-O_{n}\right]_{+}
\end{array}\right.
\]</p>
<h1 id="lei-nei-yu-liang-he-lei-jian-yu-liang">类内余量和类间余量</h1>
<p>不同于优化\(S_n-S_p\) 的损失函数，在 <code>Circle Loss</code> 中， \(S_n\) 和 \(S_p\) 是不对称的，论文为其各自定义了余量\(\Delta_n\)和\(\Delta_p\),这样可得到最终带余量的 <code>Circle Loss</code>：<br>
\[
\mathcal{L}_{\text {circle}}=\log \left[1+\sum_{j=1}^{L} \exp \left(\gamma \alpha_{n}^{j}\left(s_{n}^{j}-\Delta_{n}\right)\right) \sum_{i=1}^{K} \exp \left(-\gamma \alpha_{p}^{i}\left(s_{p}^{i}-\Delta_{p}\right)\right)\right]
\]<br>
通过推导决策边界，论文进一步分析\(\Delta_n\)和\(\Delta_p\)。为简单起见，这里以二元分类的情况进行说明，其中决策边界是在\(\alpha_n(S_n-\Delta_n)-\alpha_p(s_p-\Delta_p)=0\)处得到。结合:<br>
\[
\left\{\begin{array}{c}
\alpha_{p}^{i}=\left[O_{p}-s_{p}^{i}\right]_{+} \\
\alpha_{n}^{j}=\left[s_{n}^{j}-O_{n}\right]_{+}
\end{array}\right.
\]<br>
可得决策边界：<br>
\[
\left(s_{n}-\frac{O_{n}+\Delta_{n}}{2}\right)^{2}+\left(s_{p}-\frac{O_{p}+\Delta_{p}}{2}\right)^{2}=C
\]<br>
其中，\(C=\frac{\left(O_{n}-\Delta_{n}\right)^{2}+\left(O_{p}-\Delta_{p}\right)^{2}}{4}\)。</p>
<p><code>Circle Loss</code> 有 5 个超参数，即\(O_p\) 、\(O_n\)、\(\gamma\) 、\(\Delta_n\)和\(\Delta_p\)。通过将\(O_p=1+m,O_n=-m,\Delta_p=1-m,\Delta_n=m\) 。可将上式约简为：<br>
\[
\left(s_{n}-0\right)^{2}+\left(s_{p}-1\right)^{2}=2 m^{2}
\]<br>
基于上式定义的决策边界，可对 <code>Circle Loss</code> 进行另外一番解读。其目标是优化 \(S_p\) --&gt; 1 和\(S_n\) --&gt; 0。参数 \(m\)控制着决策边界的半径，并可被视为一个松弛因子。换句话说，<code>Circle Loss</code> 期望\(S_p^i > 1-m\)且 \(s^i_n &lt; m\)。因此，超参数仅有 2 个，即扩展因子 \(\gamma\) 和松弛因子\(m\) 。</p>
<blockquote>
<p>\(m\)(即半径)越大，损失越小。</p>
</blockquote>
<h1 id="you-shi">优势</h1>
<p><code>Circle Loss</code> 在\(S_n^j\) 和 \(S^i_p\)上的梯度分别为：<br>
\[
\begin{array}{l}
\frac{\partial \mathcal{L}_{\text {circle}}}{\partial s_{n}^{j}}=Z \frac{\exp \left(\gamma\left(\left(s_{n}^{j}\right)^{2}-m^{2}\right)\right)}{\sum_{l=1}^{L} \exp \left(\gamma\left(\left(s_{n}^{l}\right)^{2}-m^{2}\right)\right)} \gamma\left(s_{n}^{j}+m\right) \\
\frac{\partial \mathcal{L}_{\text {circle}}}{\partial s_{p}^{i}}=Z \frac{\exp \left(\gamma\left(m^{2}-\left(s_{p}^{i}-1\right)^{2}\right)\right)}{\sum_{k=1}^{K} \exp \left(\gamma\left(m^{2}-\left(s_{p}^{k}-1\right)^{2}\right)\right)} \gamma\left(s_{p}^{i}-1-m\right)
\end{array}
\]<br>
下图在二元分类的实验场景中可视化了不同 \(m\)值设置下的梯度情况，对比图中(a) 和 (b) 的triplet 损失和 AMSoftmax 损失的梯度，可知 Circle Loss 有这些优势：</p>
<ol>
<li>在\(S_n\) 和 \(S_p\)上能进行平衡的优化</li>
<li>梯度会逐渐减弱</li>
<li>收敛目标更加明确</li>
</ol>
<p><img src="/2020/06/07/research/circle_loss/image-20200607211829089.png" alt></p>
<p>上图的可视化结果表明，triplet 损失和 AMSoftmax 损失都缺乏优化的灵活性。它们相对于 \(S_n\)（左图）和 \(S_p\)（右图）的梯度严格相等，而且在收敛方面出现了陡然的下降（相似度配对 B）。比如，在 A 处，类内相似度分数\(S_p\) 已接近 1 ，但仍出现了较大的梯度。此外，决策边界平行于\(S_p=S_n\) ，这会导致收敛不明确。</p>
<p>相对而言，新提出的 <code>Circle Loss</code> 可根据相似性得分与最优值的距离，动态地为相似度分数分配不同的梯度。对于 A（ \(S_n\)和\(S_p\)都很大），<code>Circle Loss</code> 的重点是优化 \(S_n\) ；对于 B，因为\(S_n\)  显著下降，<code>Circle Loss</code> 会降低它的梯度，并因此会施加温和的优化。</p>
<p><code>Circle Loss</code> 的决策边界是圆形的，与 \(S_n-S_p=m\) 直线有着明确的切点，而这个切点将成为明确的收敛目标。这是因为，对于同样的损失值，该切点具有最小的类间-类内差距，是最容易维持的。</p>
<h1 id="dai-ma">代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_label_to_similarity</span><span class="params">(normed_feature: Tensor, label: Tensor)</span> -&gt; Tuple[Tensor, Tensor]:</span></span><br><span class="line">    similarity_matrix = normed_feature @ normed_feature.transpose(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    label_matrix = label.unsqueeze(<span class="number">1</span>) == label.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    positive_matrix = label_matrix.triu(diagonal=<span class="number">1</span>)</span><br><span class="line">    negative_matrix = torch.ByteTensor(np.logical_not(label_matrix.cpu().detach().numpy())).cuda().triu(diagonal=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    similarity_matrix = similarity_matrix.view(<span class="number">-1</span>)</span><br><span class="line">    positive_matrix = positive_matrix.view(<span class="number">-1</span>)</span><br><span class="line">    negative_matrix = negative_matrix.view(<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> similarity_matrix[positive_matrix], similarity_matrix[negative_matrix]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CircleLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, m: float, gamma: float)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        super(CircleLoss, self).__init__()</span><br><span class="line">        self.m = m</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.soft_plus = nn.Softplus()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, sp: Tensor, sn: Tensor)</span> -&gt; Tensor:</span></span><br><span class="line">        ap = torch.clamp_min(- sp.detach() + <span class="number">1</span> + self.m, min=<span class="number">0.</span>)</span><br><span class="line">        an = torch.clamp_min(sn.detach() + self.m, min=<span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line">        delta_p = <span class="number">1</span> - self.m</span><br><span class="line">        delta_n = self.m</span><br><span class="line"></span><br><span class="line">        logit_p = - ap * (sp - delta_p) * self.gamma</span><br><span class="line">        logit_n = an * (sn - delta_n) * self.gamma</span><br><span class="line"></span><br><span class="line">        loss = self.soft_plus(torch.logsumexp(logit_n, dim=<span class="number">0</span>) + torch.logsumexp(logit_p, dim=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    feat = nn.functional.normalize(torch.rand(<span class="number">256</span>, <span class="number">64</span>, requires_grad=<span class="literal">True</span>))</span><br><span class="line">    lbl = torch.randint(high=<span class="number">10</span>, size=(<span class="number">256</span>,))</span><br><span class="line"></span><br><span class="line">    inp_sp, inp_sn = convert_label_to_similarity(feat, lbl)</span><br><span class="line"></span><br><span class="line">    criterion = CircleLoss(m=<span class="number">0.25</span>, gamma=<span class="number">256</span>)</span><br><span class="line">    circle_loss = criterion(inp_sp, inp_sn)</span><br><span class="line"></span><br><span class="line">    print(circle_loss)</span><br></pre></td></tr></table></figure>
<h1 id="jie-lun">结论</h1>
<p>本文对深度特征学习做出了两项深刻理解。第一，包括 triplet 损失和常用的分类损失函数在内的大多数损失函数具有统一的内在形式，它们都将类间相似度与类内相似度嵌入到相似性配对中进行优化。第二，在相似度配对内部，考虑各个相似度得分偏离理想状态的程度不同，应该给予它们不同的优化强度。将这两项理解联合起来，便得到 <code>Circle Loss</code>。</p>
<p>通过让每个相似性得分以不同的步调学习，<code>Circle Loss</code> 赋予深度特征学习的更灵活的优化途径，以及更明确的收敛目标；并且，它为两种基本学习范式（样本对和分类学习）提供了统一的解读以及统一的数学公式。</p>
<h1 id="shi-ji">实际</h1>
<ol>
<li>使用fasttext在文本分类中，使用该loss没有达到直接使用softmax + cross entropy 的效果差了0.07%左右。可能是我参数没有调好。</li>
<li>该loss需要花费额外的精力去进行调参，在实在没有办法的情况下，可以尝试使用该loss。</li>
</ol>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://blog.csdn.net/u013602059/article/details/105202917" target="_blank" rel="noopener">Circle Loss: A Unified Perspective of Pair Similarity Optimization 圆损失函数，统一优化视角，革新深度特征学习范式 CVPR 2020</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/优化算法</category>
      </categories>
  </entry>
  <entry>
    <title>latex</title>
    <url>/2020/06/06/latex/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/06/06/latex/timg.jpeg" alt></p>
<a id="more"></a>
<h1 id="an-zhuang-ji-pei-zhi-mac">安装及配置（mac）</h1>
<h2 id="an-zhuang">安装</h2>
<ol>
<li><a href="http://www.tug.org/mactex/mactex-download.html" target="_blank" rel="noopener">Downloading MacTex</a>，并进行安装。</li>
<li><a href="https://code.visualstudio.com/download" target="_blank" rel="noopener">Downloading VScode</a>,并进行安装。</li>
</ol>
<h2 id="pei-zhi">配置</h2>
<ol>
<li>
<p>打开 vscode 的扩展，搜索插件 LaTeX WorkShop 并进行安装。<img src="/2020/06/06/latex/image-20200606223402471.png" alt></p>
</li>
<li>
<p>安装完成后，即可编译英文 Tex 文件。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line">    Hello World.</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p>如新建<code>HelloWorld.tex</code>文件，选择<code>Build Latex Project</code> 即可得到如下图结果：</p>
<p><img src="/2020/06/06/latex/image-20200606223733559.png" alt></p>
</li>
<li>
<p>配置中文环境：LaTeX WorkShop 插件默认只提供<code>PDFLaTeX</code>，而中文编译需要<code>XeFLaTeX</code>，所以还需另行配置。</p>
<ol>
<li>
<p>依次选择<code>Code &gt; Preferences &gt; Settings</code>，搜索latex.tool，点击在settings.json中编辑。</p>
<p><img src="/2020/06/06/latex/image-20200606224100719.png" alt></p>
</li>
<li>
<p>修改对应代码如下，或者对相应部分进行替换</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">"latex-workshop.latex.tools": [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"xelatex"</span>,</span><br><span class="line">          <span class="attr">"command"</span>: <span class="string">"xelatex"</span>,</span><br><span class="line">          <span class="attr">"args"</span>: [</span><br><span class="line">            <span class="string">"-synctex=1"</span>,</span><br><span class="line">            <span class="string">"-interaction=nonstopmode"</span>,</span><br><span class="line">            <span class="string">"-file-line-error"</span>,</span><br><span class="line">            <span class="string">"%DOC%"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"pdflatex"</span>,</span><br><span class="line">          <span class="attr">"command"</span>: <span class="string">"pdflatex"</span>,</span><br><span class="line">          <span class="attr">"args"</span>: [</span><br><span class="line">            <span class="string">"-synctex=1"</span>,</span><br><span class="line">            <span class="string">"-interaction=nonstopmode"</span>,</span><br><span class="line">            <span class="string">"-file-line-error"</span>,</span><br><span class="line">            <span class="string">"%DOC%"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"latexmk"</span>,</span><br><span class="line">          <span class="attr">"command"</span>: <span class="string">"latexmk"</span>,</span><br><span class="line">          <span class="attr">"args"</span>: [</span><br><span class="line">            <span class="string">"-synctex=1"</span>,</span><br><span class="line">            <span class="string">"-interaction=nonstopmode"</span>,</span><br><span class="line">            <span class="string">"-file-line-error"</span>,</span><br><span class="line">            <span class="string">"-pdf"</span>,</span><br><span class="line">            <span class="string">"%DOC%"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"bibtex"</span>,</span><br><span class="line">          <span class="attr">"command"</span>: <span class="string">"bibtex"</span>,</span><br><span class="line">          <span class="attr">"args"</span>: [</span><br><span class="line">            <span class="string">"%DOCFILE%"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;</span><br><span class="line">      ],</span><br><span class="line"></span><br><span class="line">      "latex-workshop.latex.recipes": [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"XeLaTeX"</span>,</span><br><span class="line">          <span class="attr">"tools"</span>: [</span><br><span class="line">            <span class="string">"xelatex"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"PDFLaTeX"</span>,</span><br><span class="line">          <span class="attr">"tools"</span>: [</span><br><span class="line">            <span class="string">"pdflatex"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"latexmk"</span>,</span><br><span class="line">          <span class="attr">"tools"</span>: [</span><br><span class="line">            <span class="string">"latexmk"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"BibTeX"</span>,</span><br><span class="line">          <span class="attr">"tools"</span>: [</span><br><span class="line">            <span class="string">"bibtex"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"xelatex -&gt; bibtex -&gt; xelatex*2"</span>,</span><br><span class="line">          <span class="attr">"tools"</span>: [</span><br><span class="line">            <span class="string">"xelatex"</span>,</span><br><span class="line">            <span class="string">"bibtex"</span>,</span><br><span class="line">            <span class="string">"xelatex"</span>,</span><br><span class="line">            <span class="string">"xelatex"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"pdflatex -&gt; bibtex -&gt; pdflatex*2"</span>,</span><br><span class="line">          <span class="attr">"tools"</span>: [</span><br><span class="line">            <span class="string">"pdflatex"</span>,</span><br><span class="line">            <span class="string">"bibtex"</span>,</span><br><span class="line">            <span class="string">"pdflatex"</span>,</span><br><span class="line">            <span class="string">"pdflatex"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">    ],</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>新建Hello.tex 如下：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line">中文</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609133419903.png" alt></p>
</li>
</ol>
</li>
</ol>
<h1 id="latex-yu-fa">latex 语法</h1>
<h2 id="hello-world">Hello world</h2>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 这里是导言区</span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line">Hello, world!</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p>此处的第一行 <code>\documentclass{article}</code> 中包含了一个控制序列（或称命令/标记）。所谓控制序列，是以反斜杠 <code>\</code> 开头，以第一个<strong>空格或非字母</strong>的字符结束的一串文字。它们不被输出，但是他们会影响输出文档的效果。这里的控制序列是 <code>documentclass</code>，它后面紧跟着的 <code>{article}</code> 代表这个控制序列有一个必要的参数，该参数的值为 <code>article</code>。这个控制序列的作用，是调用名为 <code>article</code> 的文档类。</p>
<span class="label danger">注意，TeX 对控制序列的大小写是敏感的。</span>
<blockquote>
<p>部分控制序列还有被方括号 <code>[]</code> 包括的可选参数。</p>
<p>所谓文档类，即是 TeX 系统预设的（或是用户自定的）一些格式的集合。不同的文档类在输出效果上会有差别。</p>
</blockquote>
<p>处的第二行以 <code>%</code> 开头。TeX 以百分号 <code>%</code> 作为注释标记。具体来说，TeX 会忽略从 <code>%</code> 开始到当前行末尾的所有内容。这些内容不会被输出，也不影响最终排版效果，只供人类阅读。若要输出 <code>%</code> 字符本身，则需要在 <code>%</code> 之前加上反斜杠 <code>\</code> 进行转义（escape）。例如：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">今年的净利润为 20<span class="tag">\<span class="name">%</span></span>，比去年高。</span><br></pre></td></tr></table></figure>
<p>此处 <code>%</code> 被当做正常的百分号处理，其后的文字也将被正常输出。</p>
<p>在注释行之后出现了控制序列 <code>begin</code>。这个控制序列总是与 <code>end</code> 成对出现。这两个控制序列以及他们中间的内容被称为「环境」；它们之后的第一个必要参数总是<strong>一致的</strong>，被称为环境名。</p>
<p>只有在 <code>document</code> 环境中的内容，才会被正常输出到文档中去或是作为控制序列对文档产生影响。也就是说，在 <code>\end{document}</code> 之后插入任何内容都是无效的。</p>
<p>从 <code>\documentclass{article}</code> 开始到 <code>\begin{document}</code> 之前的部分被称为导言区。导言区是对整篇文档进行设置的区域——在导言区出现的控制序列，往往会影响整篇文档的格式。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">比如，通常在导言区设置页面大小、页眉页脚样式、章节标题样式等等。</span><br></pre></td></tr></table></figure>
<h2 id="zhong-ying-wen-hun-pai">中英文混排</h2>
<p>TeX 系统是高教授开发的。在 TeX 开发当初并没有考虑到亚洲文字的问题。因此早期的 TeX 系统并不能直接支持中文，必须要用其他工具先处理一下（或者是一些宏包之类的）。但是现在，XeTeX 原生支持 Unicode，并且可以方便地调用系统字体。至此，只需要使用几个简单的宏包，就能完成中文支持了。</p>
<p>所谓宏包，就是一系列控制序列的合集。这些控制序列太常用，以至于人们会觉得每次将他们写在导言区太过繁琐，于是将他们打包放在同一个文件中，成为所谓的宏包。<code>\usepackage{}</code> 可以用来调用宏包。</p>
<p>除去中文支持，中文的版式处理和标点则也是不小的挑战。好在 <code>CTeX</code> 宏集一次性解决了这些问题。<code>CTeX</code> 宏集的优势在于，它能适配于多种编译方式；在内部处理好了中文和中文版式的支持，隐藏了这些细节；并且，提供了不少中文用户需要的功能接口。</p>
<blockquote>
<p><code>CTeX</code> 宏集和 <code>CTeX</code> 套装是两个不同的东西。<code>CTeX</code> 宏集本质是 LaTeX 宏的集合，包含若干文档类（<code>.cls</code> 文件）和宏包（<code>.sty</code> 文件）。<code>CTeX</code> 套装是一个<strong>过时的</strong> TeX 系统。</p>
<p>新版 <code>CTeX</code> 宏集的默认能够自动检测用户的操作系统，并为之配置合适的字库。对于 Windows 用户、Mac OS X 用户和 Linux 用户，都无需做任何配置，就能使用 <code>CTeX</code> 宏集来排版中文。</p>
</blockquote>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;ctexart&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line">你好，world!</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609124748583.png" alt></p>
<p>相较于之前的例子，这份代码只有细微的差异：</p>
<ol>
<li>文档类从 <code>article</code> 变为 <code>ctexart</code>；</li>
<li>增加了文档类选项 <code>UTF8</code>。</li>
</ol>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line">你好，world!</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609124719918.png" alt></p>
<h2 id="zuo-zhe-biao-ti-ri-qi">作者、标题、日期</h2>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">title</span><span class="string">&#123;this is a title&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">author</span><span class="string">&#123;Jeffery&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">date</span><span class="string">&#123;\today&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">maketitle</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">% body</span></span><br><span class="line">你好，world!</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609122604811.png" alt></p>
<p>导言区复杂了很多，但和之前的文档主要的区别只有一处：定义了标题、作者、日期。</p>
<p>在 <code>document</code> 环境中，除了原本的<code>你好，world!</code>，还多了一个控制序列 <code>maketitle</code>。这个控制序列能将在导言区中定义的标题、作者、日期按照预定的格式展现出来。</p>
<blockquote>
<p>使用<code>titling</code>宏包可以修改上述默认格式。</p>
</blockquote>
<h2 id="zhang-jie-he-duan-luo">章节和段落</h2>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">title</span><span class="string">&#123;this is a title&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">author</span><span class="string">&#123;Jeffery&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">date</span><span class="string">&#123;\today&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">maketitle</span></span></span><br><span class="line"><span class="tag">\<span class="name">section</span><span class="string">&#123;你好中国&#125;</span></span></span><br><span class="line">中国在East Asia.</span><br><span class="line"><span class="tag">\<span class="name">subsection</span><span class="string">&#123;Hello Beijing&#125;</span></span></span><br><span class="line">北京是capital of China.</span><br><span class="line"><span class="tag">\<span class="name">subsubsection</span><span class="string">&#123;Hello Dongcheng District&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">paragraph</span><span class="string">&#123;Tian'anmen Square&#125;</span></span></span><br><span class="line">is in the center of Beijing</span><br><span class="line"><span class="tag">\<span class="name">subparagraph</span><span class="string">&#123;Chairman Mao&#125;</span></span></span><br><span class="line">is in the center of 天安门广场。</span><br><span class="line"><span class="tag">\<span class="name">subsection</span><span class="string">&#123;Hello 江苏&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">paragraph</span><span class="string">&#123;东南大学&#125;</span></span> is one of the best university in 江苏。</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609123108736.png" alt></p>
<p>在文档类 <code>article</code>/<code>ctexart</code> 中，定义了五个控制序列来调整行文组织结构。他们分别是</p>
<ul>
<li><code>\section{·}</code></li>
<li><code>\subsection{·}</code></li>
<li><code>\subsubsection{·}</code></li>
<li><code>\paragraph{·}</code></li>
<li><code>\subparagraph{·}</code></li>
</ul>
<blockquote>
<p>在<code>report</code>/<code>ctexrep</code>中，还有<code>\chapter{·}</code>；</p>
<p>在文档类<code>book</code>/<code>ctexbook</code>中，还定义了<code>\part{·}</code>。</p>
</blockquote>
<h2 id="cha-ru-mu-lu">插入目录</h2>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">title</span><span class="string">&#123;this is a title&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">author</span><span class="string">&#123;Jeffery&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">date</span><span class="string">&#123;\today&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="comment">% \tableofcontents 用于生成目录</span></span><br><span class="line"><span class="tag">\<span class="name">tableofcontents</span></span></span><br><span class="line"><span class="tag">\<span class="name">maketitle</span></span></span><br><span class="line"><span class="tag">\<span class="name">section</span><span class="string">&#123;你好中国&#125;</span></span></span><br><span class="line">中国在East Asia.</span><br><span class="line"><span class="tag">\<span class="name">subsection</span><span class="string">&#123;Hello Beijing&#125;</span></span></span><br><span class="line">北京是capital of China.</span><br><span class="line"><span class="tag">\<span class="name">subsubsection</span><span class="string">&#123;Hello Dongcheng District&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">paragraph</span><span class="string">&#123;Tian'anmen Square&#125;</span></span></span><br><span class="line">is in the center of Beijing</span><br><span class="line"><span class="tag">\<span class="name">subparagraph</span><span class="string">&#123;Chairman Mao&#125;</span></span></span><br><span class="line">is in the center of 天安门广场。</span><br><span class="line"><span class="tag">\<span class="name">subsection</span><span class="string">&#123;Hello 江苏&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">paragraph</span><span class="string">&#123;东南大学&#125;</span></span> is one of the best university in 江苏。</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609123511051.png" alt></p>
<h2 id="cha-ru-shu-xue-gong-shi">插入数学公式</h2>
<p>为了使用 AMS-LaTeX 提供的数学功能，需要在导言区加载 <code>amsmath</code> 宏包：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br></pre></td></tr></table></figure>
<h3 id="shu-xue-mo-shi">数学模式</h3>
<p>LaTeX 的数学模式有两种：行内模式 (inline) 和行间模式 (display)。前者在正文的行文中，插入数学公式；后者独立排列单独成行，并自动居中。</p>
<p>在行文中，使用 <code>$ ... $</code> 可以插入行内公式，使用 <code>\[ ... \]</code> 可以插入行间公式，如果需要对行间公式进行编号，则可以使用 <code>equation</code> 环境：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;equation&#125;</span></span></span><br><span class="line">...</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;equation&#125;</span></span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>行内公式也可以使用 <code>\(...\)</code> 或者 <code>\begin{math} ... \end{math}</code> 来插入，但略显麻烦。</p>
<p>无编号的行间公式也可以使用 <code>\begin{displaymath} ... \end{displaymath}</code> 或者 <code>\begin{equation*} ... \end{equation*}</code> 来插入，但略显麻烦。（<code>equation*</code> 中的 <code>*</code> 表示环境不编号）<br>
也有 plainTeX 风格的 <code>$$ ... $$</code> 来插入不编号的行间公式。但是在 LaTeX 中这样做会改变行文的默认行间距，不推荐。</p>
</blockquote>
<h3 id="shang-xia-biao">上下标</h3>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="comment">% 行内公式</span></span><br><span class="line">Einstein 's <span class="formula">$E=mc^2$</span>.</span><br><span class="line"><span class="comment">% 行间公式 不带编号</span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> E=mc^2. <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="comment">% 行间公式 带编号</span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;equation&#125;</span></span></span><br><span class="line">E=mc^2.</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;equation&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609124558745.png" alt></p>
<blockquote>
<p>行内公式和行间公式对标点的要求是不同的：行内公式的标点，应该放在数学模式的限定符之外，而行间公式则应该放在数学模式限定符之内。</p>
<p>数学模式中，需要表示上标，可以使用 <code>^</code> 来实现（下标则是 <code>_</code>）。<strong>它默认只作用于之后的一个字符</strong>，如果想对连续的几个字符起作用，请将这些字符用花括号 <code>{}</code> 括起来</p>
</blockquote>
<h3 id="gen-shi-yu-fen-shi">根式与分式</h3>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">sqrt</span><span class="string">&#123;x&#125;</span></span>$</span>, <span class="formula">$<span class="tag">\<span class="name">frac</span><span class="string">&#123;1&#125;</span><span class="string">&#123;2&#125;</span></span>$</span>.</span><br><span class="line"><span class="comment">% 根号</span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">sqrt</span><span class="string">&#123;x&#125;</span></span>, <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="comment">% 分式</span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">frac</span><span class="string">&#123;1&#125;</span><span class="string">&#123;2&#125;</span></span>. <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609125352216.png" alt></p>
<p>在行间公式和行内公式中，分式的输出效果是有差异的。如果要强制行内模式的分式显示为行间模式的大小，可以使用 <code>\dfrac</code>(变大), 反之可以使用 <code>\tfrac</code>。</p>
<blockquote>
<p>在行内写分式，推荐 <code>xfrac</code> 宏包提供的 <code>\sfrac</code> 命令的效果。</p>
<p>排版繁分式，使用 <code>\cfrac</code> 命令。</p>
</blockquote>
<h3 id="yun-suan-fu">运算符</h3>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="comment">% 加减</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">pm</span></span>$</span>  </span><br><span class="line"></span><br><span class="line"><span class="comment">%乘</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">times</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 除</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">div</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 点乘</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">cdot</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 交</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">cap</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 并</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">cup</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 大于等于</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">geq</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 小于等于</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">leq</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 不等于</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">neq</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 约等于</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">approx</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 恒等于</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">equiv</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 求和</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">sum</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 连乘</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">prod</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 极限</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">lim</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 积分</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">int</span></span>$</span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609130721090.png" alt></p>
<p>连加、连乘、极限、积分等大型运算符分别用 <code>\sum</code>, <code>\prod</code>, <code>\lim</code>, <code>\int</code> 生成。他们的上下标在行内公式中被压缩，以适应行高。我们可以用 <code>\limits</code> 和 <code>\nolimits</code> 来强制显式地指定是否压缩这些上下标。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="formula">$ <span class="tag">\<span class="name">sum</span></span>_&#123;i=1&#125;^n i<span class="tag">\<span class="name">quad</span></span> <span class="tag">\<span class="name">prod</span></span>_&#123;i=1&#125;^n $</span></span><br><span class="line"><span class="formula">$ <span class="tag">\<span class="name">sum</span></span><span class="tag">\<span class="name">limits</span></span> _&#123;i=1&#125;^n i<span class="tag">\<span class="name">quad</span></span> <span class="tag">\<span class="name">prod</span></span><span class="tag">\<span class="name">limits</span></span> _&#123;i=1&#125;^n $</span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">lim</span></span>_&#123;x<span class="tag">\<span class="name">to</span></span>0&#125;x^2 <span class="tag">\<span class="name">quad</span></span> <span class="tag">\<span class="name">int</span></span>_a^b x^2 dx <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">lim</span></span><span class="tag">\<span class="name">nolimits</span></span> _&#123;x<span class="tag">\<span class="name">to</span></span>0&#125;x^2<span class="tag">\<span class="name">quad</span></span> <span class="tag">\<span class="name">int</span></span><span class="tag">\<span class="name">nolimits</span></span>_a^b x^2 dx <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="comment">% 多重积分可以使用 `\iint`, `\iiint`, `\iiiint`, `\idotsint` 等命令输入。</span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">iint</span></span><span class="tag">\<span class="name">quad</span></span> <span class="tag">\<span class="name">iiint</span></span><span class="tag">\<span class="name">quad</span></span> <span class="tag">\<span class="name">iiiint</span></span><span class="tag">\<span class="name">quad</span></span> <span class="tag">\<span class="name">idotsint</span></span> <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609130934688.png" alt="image-20200609130934688"></p>
<h3 id="ding-jie-fu">定界符</h3>
<p>各种括号用 <code>()</code>, <code>[]</code>, <code>\{\}</code>, <code>\langle\rangle</code> 等命令表示；注意花括号通常用来输入命令和环境的参数，所以在数学公式中它们前面要加 <code>\</code>。因为 LaTeX 中 <code>|</code> 和 <code>\|</code> 的应用过于随意，amsmath 宏包推荐用 <code>\lvert\rvert</code> 和 <code>\lVert\rVert</code> 取而代之。</p>
<p>为了调整这些定界符的大小，amsmath 宏包推荐使用 <code>\big</code>, <code>\Big</code>, <code>\bigg</code>, <code>\Bigg</code> 等一系列命令放在上述括号前面调整大小。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">Biggl</span></span>(<span class="tag">\<span class="name">biggl</span></span>(<span class="tag">\<span class="name">Bigl</span></span>(<span class="tag">\<span class="name">bigl</span></span>((x)<span class="tag">\<span class="name">bigr</span></span>)<span class="tag">\<span class="name">Bigr</span></span>)<span class="tag">\<span class="name">biggr</span></span>)<span class="tag">\<span class="name">Biggr</span></span>) <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">Biggl</span><span class="string">[\biggl[\Bigl[\bigl[[x]</span></span><span class="tag">\<span class="name">bigr</span></span>]<span class="tag">\<span class="name">Bigr</span></span>]<span class="tag">\<span class="name">biggr</span></span>]<span class="tag">\<span class="name">Biggr</span></span>] <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">Biggl</span></span> <span class="tag">\<span class="name">&#123;</span></span><span class="tag">\<span class="name">biggl</span></span> <span class="tag">\<span class="name">&#123;</span></span><span class="tag">\<span class="name">Bigl</span></span> <span class="tag">\<span class="name">&#123;</span></span><span class="tag">\<span class="name">bigl</span></span> <span class="tag">\<span class="name">&#123;</span></span><span class="tag">\<span class="name">&#123;</span></span>x<span class="tag">\<span class="name">&#125;</span></span><span class="tag">\<span class="name">bigr</span></span> <span class="tag">\<span class="name">&#125;</span></span><span class="tag">\<span class="name">Bigr</span></span> <span class="tag">\<span class="name">&#125;</span></span><span class="tag">\<span class="name">biggr</span></span> <span class="tag">\<span class="name">&#125;</span></span><span class="tag">\<span class="name">Biggr</span></span><span class="tag">\<span class="name">&#125;</span></span> <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">Biggl</span></span><span class="tag">\<span class="name">langle</span></span><span class="tag">\<span class="name">biggl</span></span><span class="tag">\<span class="name">langle</span></span><span class="tag">\<span class="name">Bigl</span></span><span class="tag">\<span class="name">langle</span></span><span class="tag">\<span class="name">bigl</span></span><span class="tag">\<span class="name">langle</span></span><span class="tag">\<span class="name">langle</span></span> x</span><br><span class="line"><span class="tag">\<span class="name">rangle</span></span><span class="tag">\<span class="name">bigr</span></span><span class="tag">\<span class="name">rangle</span></span><span class="tag">\<span class="name">Bigr</span></span><span class="tag">\<span class="name">rangle</span></span><span class="tag">\<span class="name">biggr</span></span><span class="tag">\<span class="name">rangle</span></span><span class="tag">\<span class="name">Biggr</span></span><span class="tag">\<span class="name">rangle</span></span> <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">Biggl</span></span><span class="tag">\<span class="name">lvert</span></span><span class="tag">\<span class="name">biggl</span></span><span class="tag">\<span class="name">lvert</span></span><span class="tag">\<span class="name">Bigl</span></span><span class="tag">\<span class="name">lvert</span></span><span class="tag">\<span class="name">bigl</span></span><span class="tag">\<span class="name">lvert</span></span><span class="tag">\<span class="name">lvert</span></span> x</span><br><span class="line"><span class="tag">\<span class="name">rvert</span></span><span class="tag">\<span class="name">bigr</span></span><span class="tag">\<span class="name">rvert</span></span><span class="tag">\<span class="name">Bigr</span></span><span class="tag">\<span class="name">rvert</span></span><span class="tag">\<span class="name">biggr</span></span><span class="tag">\<span class="name">rvert</span></span><span class="tag">\<span class="name">Biggr</span></span><span class="tag">\<span class="name">rvert</span></span> <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">Biggl</span></span><span class="tag">\<span class="name">lVert</span></span><span class="tag">\<span class="name">biggl</span></span><span class="tag">\<span class="name">lVert</span></span><span class="tag">\<span class="name">Bigl</span></span><span class="tag">\<span class="name">lVert</span></span><span class="tag">\<span class="name">bigl</span></span><span class="tag">\<span class="name">lVert</span></span><span class="tag">\<span class="name">lVert</span></span> x</span><br><span class="line"><span class="tag">\<span class="name">rVert</span></span><span class="tag">\<span class="name">bigr</span></span><span class="tag">\<span class="name">rVert</span></span><span class="tag">\<span class="name">Bigr</span></span><span class="tag">\<span class="name">rVert</span></span><span class="tag">\<span class="name">biggr</span></span><span class="tag">\<span class="name">rVert</span></span><span class="tag">\<span class="name">Biggr</span></span><span class="tag">\<span class="name">rVert</span></span> <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609131349870.png" alt></p>
<h3 id="sheng-lue-hao">省略号</h3>
<p>省略号用 <code>\dots</code>, <code>\cdots</code>, <code>\vdots</code>, <code>\ddots</code> 等命令表示。</p>
<p><code>\dots</code> 和 <code>\cdots</code> 的纵向位置不同，前者一般用于有下标的序列。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> x_1,x_2,<span class="tag">\<span class="name">dots</span></span> ,x_n<span class="tag">\<span class="name">quad</span></span> 1,2,<span class="tag">\<span class="name">cdots</span></span> ,n<span class="tag">\<span class="name">quad</span></span></span><br><span class="line"><span class="tag">\<span class="name">vdots</span></span><span class="tag">\<span class="name">quad</span></span> <span class="tag">\<span class="name">ddots</span></span> <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609131538918.png" alt></p>
<h3 id="ju-zhen">矩阵</h3>
<p><code>amsmath</code> 的 <code>pmatrix</code>, <code>bmatrix</code>, <code>Bmatrix</code>, <code>vmatrix</code>, <code>Vmatrix</code> 等环境可以在矩阵两边加上各种分隔符。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">begin</span><span class="string">&#123;pmatrix&#125;</span></span> a&amp;b<span class="tag">\<span class="name">\</span></span>c&amp;d <span class="tag">\<span class="name">end</span><span class="string">&#123;pmatrix&#125;</span></span> <span class="tag">\<span class="name">quad</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;bmatrix&#125;</span></span> a&amp;b<span class="tag">\<span class="name">\</span></span>c&amp;d <span class="tag">\<span class="name">end</span><span class="string">&#123;bmatrix&#125;</span></span> <span class="tag">\<span class="name">quad</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;Bmatrix&#125;</span></span> a&amp;b<span class="tag">\<span class="name">\</span></span>c&amp;d <span class="tag">\<span class="name">end</span><span class="string">&#123;Bmatrix&#125;</span></span> <span class="tag">\<span class="name">quad</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;vmatrix&#125;</span></span> a&amp;b<span class="tag">\<span class="name">\</span></span>c&amp;d <span class="tag">\<span class="name">end</span><span class="string">&#123;vmatrix&#125;</span></span> <span class="tag">\<span class="name">quad</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;Vmatrix&#125;</span></span> a&amp;b<span class="tag">\<span class="name">\</span></span>c&amp;d <span class="tag">\<span class="name">end</span><span class="string">&#123;Vmatrix&#125;</span></span> <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609131725854.png" alt></p>
<p>使用 <code>smallmatrix</code> 环境，可以生成行内公式的小矩阵。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">Marry has a little matrix <span class="formula">$ ( <span class="tag">\<span class="name">begin</span><span class="string">&#123;smallmatrix&#125;</span></span> a&amp;b<span class="tag">\<span class="name">\</span></span>c&amp;d <span class="tag">\<span class="name">end</span><span class="string">&#123;smallmatrix&#125;</span></span> ) $</span>.</span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609131818971.png" alt></p>
<h3 id="duo-xing-gong-shi">多行公式</h3>
<p>有的公式特别长，我们需要手动为他们换行；有几个公式是一组，我们需要将他们放在一起；还有些类似分段函数，我们需要给它加上一个左边的花括号。</p>
<h4 id="chang-gong-shi">长公式</h4>
<h5 id="bu-dui-qi">不对齐</h5>
<p>无须对齐的长公式可以使用 <code>multline</code> 环境(如果不需要编号，可以使用 <code>multline*</code> 环境代替。)。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line">    <span class="tag">\<span class="name">begin</span><span class="string">&#123;multline&#125;</span></span></span><br><span class="line">        x = a+b+c+&#123;&#125; <span class="tag">\<span class="name">\</span></span></span><br><span class="line">        d+e+f+g</span><br><span class="line">    <span class="tag">\<span class="name">end</span><span class="string">&#123;multline&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609132032239.png" alt></p>
<h5 id="dui-qi">对齐</h5>
<p>需要对齐的公式，可以使用 <code>aligned</code> <em>次环境</em>来实现，它必须包含在数学环境之内。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line">    <span class="tag">\<span class="name">[</span></span><span class="tag">\<span class="name">begin</span><span class="string">&#123;aligned&#125;</span></span></span><br><span class="line">        x =&#123;&#125;&amp; a+b+c+&#123;&#125; <span class="tag">\<span class="name">\</span></span></span><br><span class="line">        &amp;d+e+f+g</span><br><span class="line">    <span class="tag">\<span class="name">end</span><span class="string">&#123;aligned&#125;</span></span><span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609132211903.png" alt></p>
<h4 id="gong-shi-zu">公式组</h4>
<p>无需对齐的公式组可以使用 <code>gather</code> 环境，需要对齐的公式组可以使用 <code>align</code> 环境。他们都带有编号，如果不需要编号可以使用带星花的版本。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;gather&#125;</span></span></span><br><span class="line">    a = b+c+d <span class="tag">\<span class="name">\</span></span></span><br><span class="line">    x = y+z</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;gather&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;align&#125;</span></span></span><br><span class="line">    a &amp;= b+c+d <span class="tag">\<span class="name">\</span></span></span><br><span class="line">    x &amp;= y+z</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;align&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609132406290.png" alt></p>
<h3 id="fen-duan-han-shu">分段函数</h3>
<p>分段函数可以用<code>cases</code>次环境来实现，它必须包含在数学环境之内。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> y= <span class="tag">\<span class="name">begin</span><span class="string">&#123;cases&#125;</span></span></span><br><span class="line">    -x,<span class="tag">\<span class="name">quad</span></span> x<span class="tag">\<span class="name">leq</span></span> 0 <span class="tag">\<span class="name">\</span></span></span><br><span class="line">    x,<span class="tag">\<span class="name">quad</span></span> x&gt;0</span><br><span class="line">    <span class="tag">\<span class="name">end</span><span class="string">&#123;cases&#125;</span></span> <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609132647535.png" alt></p>
<h3 id="shu-xue-gong-shi-gong-ju">数学公式工具</h3>
<ul>
<li><a href="https://mathpix.com/" target="_blank" rel="noopener">https://mathpix.com/</a> 通过热键呼出截屏，而后将截屏中的公式转换成 LaTeX 数学公式的代码。</li>
<li><a href="http://detexify.kirelabs.org/classify.html" target="_blank" rel="noopener">http://detexify.kirelabs.org/classify.html</a> 允许用户用鼠标在输入区绘制单个数学符号的样式，系统会根据样式返回对应的 LaTeX 代码（和所需的宏包）。这在查询不熟悉的数学符号时特别有用。</li>
</ul>
<h2 id="cha-ru-tu-pian">插入图片</h2>
<p>在 LaTeX 中插入图片，有很多种方式。最好用的应当属利用 <code>graphicx</code> 宏包提供的 <code>\includegraphics</code> 命令。比如你在你的 TeX 源文件同目录下，有名为 <code>a.jpg</code> 的图片，你可以用这样的方式将它插入到输出文档中：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;graphicx&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">includegraphics</span><span class="string">&#123;a.jpg&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609133258012.png" alt></p>
<p>图片很大，超过了输出文件的纸张大小，这时候可以用 <code>\includegraphics</code> 控制序列的可选参数来控制。比如</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;graphicx&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="comment">% 这样图片的宽度会被缩放至页面宽度的百分之六十，图片的总高度会按比例缩放。</span></span><br><span class="line"><span class="tag">\<span class="name">includegraphics</span><span class="string">[width = .6\textwidth]</span><span class="string">&#123;a.jpg&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609133717627.png" alt></p>
<h2 id="biao-ge">表格</h2>
<p><code>tabular</code> 环境提供了最简单的表格功能。它用 <code>\hline</code> 命令表示横线，在列格式中用 <code>|</code> 表示竖线；用 <code>&amp;</code> 来分列，用 <code>\\</code> 来换行；每列可以采用居左、居中、居右等横向对齐方式，分别用 <code>l</code>、<code>c</code>、<code>r</code> 来表示。（<a href="https://www.tablesgenerator.com/" target="_blank" rel="noopener">表格在线生成</a>）</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;graphicx&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;tabular&#125;</span><span class="string">&#123;|l|c|r|&#125;</span></span></span><br><span class="line">    <span class="tag">\<span class="name">hline</span></span></span><br><span class="line">   操作系统&amp; 发行版&amp; 编辑器<span class="tag">\<span class="name">\</span></span></span><br><span class="line">    <span class="tag">\<span class="name">hline</span></span></span><br><span class="line">   Windows &amp; MikTeX &amp; TexMakerX <span class="tag">\<span class="name">\</span></span></span><br><span class="line">    <span class="tag">\<span class="name">hline</span></span></span><br><span class="line">   Unix/Linux &amp; teTeX &amp; Kile <span class="tag">\<span class="name">\</span></span></span><br><span class="line">    <span class="tag">\<span class="name">hline</span></span></span><br><span class="line">   Mac OS &amp; MacTeX &amp; TeXShop <span class="tag">\<span class="name">\</span></span></span><br><span class="line">    <span class="tag">\<span class="name">hline</span></span></span><br><span class="line">   通用&amp; TeX Live &amp; TeXworks <span class="tag">\<span class="name">\</span></span></span><br><span class="line">    <span class="tag">\<span class="name">hline</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;tabular&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609134020824.png" alt></p>
<h2 id="fu-dong-ti">浮动体</h2>
<p>插图和表格通常需要占据大块空间，所以在文字处理软件中经常需要调整他们的位置。<code>figure</code> 和 <code>table</code> 环境可以自动完成这样的任务；这种自动调整位置的环境称作浮动体(float)。我们以 <code>figure</code> 为例。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;graphicx&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;figure&#125;</span><span class="string">[htbp]</span></span></span><br><span class="line">    <span class="tag">\<span class="name">centering</span></span></span><br><span class="line">    <span class="tag">\<span class="name">includegraphics</span><span class="string">[width = .4\textwidth]</span><span class="string">&#123;a.jpg&#125;</span></span></span><br><span class="line">    <span class="tag">\<span class="name">caption</span><span class="string">&#123;夏&#125;</span></span></span><br><span class="line">    <span class="tag">\<span class="name">label</span><span class="string">&#123;fig:myphoto&#125;</span></span></span><br><span class="line">    <span class="tag">\<span class="name">end</span><span class="string">&#123;figure&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/latex/image-20200609134518050.png" alt></p>
<blockquote>
<p><code>htbp</code> 选项用来指定插图的理想位置，这几个字母分别代表 here, top, bottom, float page，也就是就这里、页顶、页尾、浮动页（专门放浮动体的单独页面或分栏）。</p>
<p><code>\centering</code> 用来使插图居中；</p>
<p><code>\caption</code> 命令设置插图标题，LaTeX 会自动给浮动体的标题加上编号。</p>
<p><code>\label</code> 应该放在标题命令之后。</p>
</blockquote>
<h2 id="ban-mian-she-zhi">版面设置</h2>
<h3 id="ye-bian-ju">页边距</h3>
<p>设置页边距，推荐使用 <code>geometry</code> 宏包。比如希望，将纸张的长度设置为 20cm、宽度设置为 15cm、左边距 1cm、右边距 2cm、上边距 3cm、下边距 4cm，可以在导言区加上这样几行：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;geometry&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">geometry</span><span class="string">&#123;papersize=&#123;20cm,15cm&#125;</span></span>&#125;</span><br><span class="line"><span class="tag">\<span class="name">geometry</span><span class="string">&#123;left=1cm,right=2cm,top=3cm,bottom=4cm&#125;</span></span></span><br></pre></td></tr></table></figure>
<h3 id="ye-mei-ye-jiao">页眉页脚</h3>
<p>设置页眉页脚，推荐使用 <code>fancyhdr</code> 宏包。比如希望，在页眉左边写上我的名字，中间写上今天的日期，右边写上我的电话；页脚的正中写上页码；页眉和正文之间有一道宽为 0.4pt 的横线分割，可以在导言区加上如下几行：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;fancyhdr&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">pagestyle</span><span class="string">&#123;fancy&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">lhead</span><span class="string">&#123;\author&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">chead</span><span class="string">&#123;\date&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">rhead</span><span class="string">&#123;166xxxxxxxx&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">lfoot</span><span class="string">&#123;&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">cfoot</span><span class="string">&#123;\thepage&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">rfoot</span><span class="string">&#123;&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">renewcommand</span><span class="string">&#123;\headrulewidth&#125;</span><span class="string">&#123;0.4pt&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">renewcommand</span><span class="string">&#123;\headwidth&#125;</span><span class="string">&#123;\textwidth&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">renewcommand</span><span class="string">&#123;\footrulewidth&#125;</span><span class="string">&#123;0pt&#125;</span></span></span><br></pre></td></tr></table></figure>
<h3 id="shou-xing-suo-jin">首行缩进</h3>
<p>CTeX 宏集已经处理好了首行缩进的问题（自然段前空两格汉字宽度）。因此，使用 CTeX 宏集进行中西文混合排版时，不需要关注首行缩进的问题。</p>
<h3 id="xing-jian-ju">行间距</h3>
<p>可以通过 <code>setspace</code> 宏包提供的命令来调整行间距。比如在导言区添加如下内容，可以将行距设置为字号的 1.5 倍：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;setspace&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">onehalfspacing</span></span></span><br></pre></td></tr></table></figure>
<h3 id="duan-jian-ju">段间距</h3>
<p>可以通过修改长度 <code>\parskip</code> 的值来调整段间距。例如在导言区添加以下内容</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">addtolength</span><span class="string">&#123;\parskip&#125;</span><span class="string">&#123;.4em&#125;</span></span></span><br></pre></td></tr></table></figure>
<p>可以在原有的基础上，增加段间距 0.4em。如果需要减小段间距，只需将该数值改为负值即可。</p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://liam.page/2014/09/08/latex-introduction/" target="_blank" rel="noopener">一份其实很短的 LaTeX 入门文档</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/基本技能</category>
      </categories>
      <tags>
        <tag>latex</tag>
      </tags>
  </entry>
  <entry>
    <title>SVM</title>
    <url>/2020/06/03/machine_learning/SVM/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/06/03/machine_learning/SVM/00630Defly1g4w7oendezj30gz0aojrq.jpg" alt></p>
<a id="more"></a>
<h1 id="jiang-jiang-svm">讲讲SVM</h1>
<h2 id="yi-ge-guan-yu-svm-de-tong-hua-gu-shi">一个关于SVM的童话故事</h2>
<p>支持向量机（Support Vector Machine，SVM）是众多监督学习方法中十分出色的一种，几乎所有讲述经典机器学习方法的教材都会介绍。关于SVM，流传着一个关于天使与魔鬼的故事。</p>
<p>传说魔鬼和天使玩了一个游戏，魔鬼在桌上放了两种颜色的球。魔鬼让天使用一根木棍将它们分开。这对天使来说，似乎太容易了。天使不假思索地一摆，便完成了任务。魔鬼又加入了更多的球。随着球的增多，似乎有的球不能再被原来的木棍正确分开，如下图所示。</p>
<p><img src="/2020/06/03/machine_learning/SVM/00630Defgy1g4vn6ow8vtj30i40ddgpt.jpg" alt></p>
<p>SVM实际上是在为天使找到木棒的最佳放置位置，使得两边的球都离分隔它们的木棒足够远。依照SVM为天使选择的木棒位置，魔鬼即使按刚才的方式继续加入新球，木棒也能很好地将两类不同的球分开。</p>
<p>看到天使已经很好地解决了用木棒线性分球的问题，魔鬼又给了天使一个新的挑战，如下图所示。</p>
<p><img src="/2020/06/03/machine_learning/SVM/00630Defgy1g4vn9xl3pcj30iy0dswgr.jpg" alt></p>
<p>按照这种球的摆法，世界上貌似没有一根木棒可以将它们 完美分开。但天使毕竟有法力，他一拍桌子，便让这些球飞到了空中，然后凭借念力抓起一张纸片，插在了两类球的中间。从魔鬼的角度看这些 球，则像是被一条曲线完美的切开了。</p>
<p><img src="/2020/06/03/machine_learning/SVM/00630Defgy1g4vnbaltf7j30mo0ar77n.jpg" alt></p>
<p>后来，“无聊”的科学家们把这些球称为“数据”，把木棍称为“分类面”，找到最大间隔的木棒位置的过程称为“优化”，拍桌子让球飞到空中的念力叫“核映射”，在空中分隔球的纸片称为“分类超平面”。这便是SVM的童话故事。</p>
<h2 id="li-jie-svm-di-yi-ceng">理解SVM：第一层</h2>
<p>支持向量机，因其英文名为support vector machine，故一般简称SVM，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是<strong>间隔最大化</strong>，最终可转化为一个凸二次规划问题的求解。</p>
<p>**线性分类器：**给定一些数据点，它们分别属于两个不同的类，现在要找到一个线性分类器把这些数据分成两类。如果用x表示数据点，用y表示类别（y可以取1或者0，分别代表两个不同的类），一个线性分类器的学习目标便是要在n维的数据空间中找到一个超平面，这个超平面的方程可以表示为：<br>
\[
w^Tx+b=0
\]<br>
这个超平面可以用分类函数  \(f(x)=w^Tx+b\)表示，当\(f(x)\) 等于0的时候，\(x\)便是位于超平面上的点，而\(f(x)\)大于0的点对应 \(y=1\) 的数据点，\(f(x)\)小于0的点对应\(y=-1\)的点，如下图所示：</p>
<p><img src="/2020/06/03/machine_learning/SVM/00630Defly1g4w7oendezj30gz0aojrq.jpg" alt></p>
<h3 id="han-shu-jian-ge-yu-ji-he-jian-ge">函数间隔与几何间隔</h3>
<p>在分离超平面固定为\(𝑤^T𝑥+𝑏=0\)的时候，\(|wTx+b|\)表示点x到超平面的相对距离。通过观察\(w^Tx+b\)和\(y\)是否同号，我们判断分类是否正确。这里引入函数间隔的概念，定义函数间隔\(\gamma^{\prime}\) 为：<br>
\[
\gamma^{\prime}=y(w^Tx+b)
\]<br>
可以看到，就是感知机模型里面的误分类点到超平面距离的分子。对于训练集中\(m\)个样本点对应的\(m\)个函数间隔的最小值，就是整个训练集的函数间隔。</p>
<p>函数间隔并不能正常反应点到超平面的距离，当分子成比例的增长时，分母也是成倍增长。为了统一度量，需要对法向量\(w\)加上约束条件，这样就得到了几何间隔\(\gamma\),定义为：<br>
\[
\gamma=\frac{y\left(w^{T} x+b\right)}{\|w\|_{2}}=\frac{\gamma^{\prime}}{\|w\|_{2}}
\]<br>
几何间隔才是点到超平面的真正距离，感知机模型里用到的距离就是几何距离。</p>
<h3 id="zhi-chi-xiang-liang">支持向量</h3>
<p>在感知机模型中，可以找到多个可以分类的超平面将数据分开，并且优化时希望所有的点都被准确分类。但是实际上离超平面很远的点已经被正确分类，它对超平面的位置没有影响。我们最关心是那些离超平面很近的点，这些点很容易被误分类。如果可以让离超平面比较近的点尽可能的远离超平面，最大化几何间隔，那么分类效果会更好一些。SVM的思想起源正起于此。</p>
<p>如下图所示，分离超平面为\(w^Tx+b=0\)，如果所有的样本不光可以被超平面分开，还和超平面保持一定的函数距离（下图函数距离为1），那么这样的分类超平面是比感知机的分类超平面优的。可以证明，这样的超平面只有一个。和超平面平行的保持一定的函数距离的这两个超平面对应的向量，我们定义为支持向量，如下图虚线所示。</p>
<p><img src="/2020/06/03/machine_learning/SVM/1042406-20161124144326487-1331861308.jpg" alt></p>
<p>支持向量到超平面的距离为\(\frac{1}{||w||_2}\),两个支持向量之间的距离为\(\frac{2}{||w||_2}\)。</p>
<h3 id="mu-biao-han-shu-yu-you-hua">目标函数与优化</h3>
<p>SVM的模型是让所有点到超平面的距离大于一定的距离，也就是所有的分类点要在各自类别的支持向量两边。用数学式子表示为：<br>
\[
\begin{array}{ll}
\max &amp; \gamma=\frac{y\left(w^{T} x+b\right)}{\|w\|_{2}} \\ \text { s.t } &amp; y_{i}\left(w^{T} x_{i}+b\right)=\gamma^{\prime}_{i} \geq \gamma^{\prime}(i=1,2, \ldots m)
\end{array}
\]<br>
一般我们都取函数间隔\(\gamma\)为1，这样我们的优化函数定义为:<br>
\[
\begin{array}{ll}
\max &amp; \frac{1}{\|w\|_{2}}  \\ \text { s.t } &amp; y_{i}\left(w^{T} x_{i}+b\right) \geq 1(i=1,2, \ldots m)
\end{array}
\]<br>
也就是说，我们要在约束条件\(y_{i}\left(w^{T} x_{i}+b\right) \geq 1(i=1,2, \ldots m)\)下，最大化\(\frac{1}{||w||_2}\)。可以看出，这个感知机的优化方式不同，感知机是固定分母优化分子，而SVM是固定分子优化分母，同时加上了支持向量的限制。</p>
<p>由于\(\frac{1}{||w||_2}\)最大化等价于\(\frac{||w||_2^2}{2}\)最小化。这样SVM的优化函数等价于：<br>
\[
\begin{array}{ll}
\min &amp;\frac{1}{2}\|w\|_{2}^{2} \\ \text { s.t } &amp;y_{i}\left(w^{T} x_{i}+b\right) \geq 1(i=1,2, \ldots m)
\end{array}
\]<br>
由于目标函数\(\frac{||w||_2^2}{2}\)是凸函数，同时约束条件不等式是仿射的，根据凸优化理论，可以通过拉格朗日函数将优化目标转化为无约束的优化函数<br>
\[
\begin{array}{ll}
L(w, b, \alpha)&amp;=\frac{1}{2}\|w\|_{2}^{2}-\sum_{i=1}^{m} \alpha_{i}\left[y_{i}\left(w^{T} x_{i}+b\right)-1\right] \\ \text { s.t } &amp; \alpha_{i} \geq 0
\end{array}
\]<br>
由于引入了朗格朗日乘子，我们的优化目标变成：<br>
\[
\underbrace{\min }_{w, b} \underbrace{\max }_{\alpha_i \ge0} L(w, b, \alpha)
\]<br>
可以通过拉格朗日对偶将我们的优化问题转化为等价的对偶问题来求解。也就是说，现在要求的是<br>
\[
\underbrace{\max }_{\alpha_i \ge0} \underbrace{\min }_{w, b}  L(w, b, \alpha)
\]<br>
从上式中，可以先求优化函数对于\(𝑤\)和\(𝑏\)的极小值(这个极值可以通过对\(w\)和\(b\)分别求偏导数得到)。接着再求拉格朗日乘子\(\alpha\)的极大值。<br>
\[
\begin{aligned}
\frac{\partial L}{\partial w}=0 &amp; \Rightarrow w=\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i} \\
\frac{\partial L}{\partial b}=0 &amp; \Rightarrow \sum_{i=1}^{m} \alpha_{i} y_{i}=0
\end{aligned}
\]<br>
从上两式子可以看出，已经求得了\(w\)和\(\alpha\)的关系，只要后面接着能够求出优化函数极大化对应的\(\alpha\)，就可以求出\(w\)了，至于，由于上两式已经没有\(b\)，所以最后的\(b\)可以有多个。既然已经求出\(w\)和\(\alpha\)的关系，就可以带入优化函数\(L(w,b,\alpha)\)消去\(w\)。定义:</p>
<p>\[
\psi(\alpha)=\underbrace{\min }_{w,b} L(w, b, \alpha)
\]<br>
从上面可以看出，通过对\(w,b\)极小化以后，优化函数\(\psi(\alpha)\)仅仅只有\(\alpha\)向量做参数。只要能够(利用SMO算法)极大化\(\psi(\alpha)\)，就可以求出此时对应的\(\alpha\)，进而求出\(w,b\).</p>
<h3 id="zui-da-jian-ge-sun-shi-han-shu-hinge-loss">最大间隔损失函数Hinge loss</h3>
<p>SVM 求解使通过建立二次规划原始问题，引入拉格朗日乘子法，然后转换成对偶的形式去求解，这是一种理论非常充实的解法。这里换一种角度来思考，在机器学习领域，一般的做法是经验风险最小化 ，即构建假设函数为输入输出间的映射，然后采用损失函数来衡量模型的优劣。求得使损失最小化的模型即为最优的假设函数，采用不同的损失函数也会得到不同的机器学习算法。SVM采用的就是Hinge Loss，用于“最大间隔”分类。</p>
<p>Hinge Loss中文名叫合页损失函数，因为它的图像是这样的：</p>
<p><img src="/2020/06/03/machine_learning/SVM/v2-3c6aa9626ee8e4609b0d7c5712baf624_1440w.jpg" alt></p>
<p>hinge-loss的公式是：<br>
\[
\begin{array}{c}
\sum_{i=1}^{N}\left[1-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}+\lambda\|w\|^{2} \\
{[z]_{+}=\left\{\begin{array}{l}
z, z>0 \\
0 . z \leq 0
\end{array}\right.}
\end{array}
\]<br>
第一项是损失，第二项是正则化项。这个公式就是说 \(y_i(w * x_i+b)\) 大于1时loss为0， 否则loss为 \(1-y_i(w*x_i+b)\) 。对比感知机的损失函数\([-y_i(w*x_i+b)]\)  来说，hinge loss不仅要分类正确，而且置信度足够高的时候，损失才为0，对学习有更高的要求。对比感知机损失和hinge loss的图像，明显Hinge loss更加严格。</p>
<p><img src="/2020/06/03/machine_learning/SVM/v2-0884e199d2198ec716f3b09536b10213_r.jpg" alt="preview"></p>
<p>如下图中，点 \(x_4\)被分类正确了，但是它的损失不是0。其实这个道理和SVM中的Margin是一样的，不仅要分类正确，还要使得Margin最大化，所以说hinge loss的另外一种解释。</p>
<p><img src="/2020/06/03/machine_learning/SVM/v2-a04fdb43f41bd9e4fd06306558ab61f2_1440w.jpg" alt="img"></p>
<h2 id="shen-ru-svm-di-er-ceng">深入SVM：第二层</h2>
<h3 id="cong-xian-xing-ke-fen-dao-xian-xing-bu-ke-fen">从线性可分到线性不可分</h3>
<p>有时候本来数据的确是可分的，也就是说可以用 线性分类SVM的学习方法来求解，但是却因为混入了异常点，导致不能线性可分，比如下图，本来数据是可以按下面的实线来做超平面分离的，可由于一个橙色和一个蓝色的异常点导致我们没法用上述方法来分类。</p>
<p><img src="/2020/06/03/machine_learning/SVM/1042406-20161125104106409-1177897648.png" alt="img"></p>
<p>另外一种情况没有这么糟糕到不可分，但是会严重影响我们模型的泛化预测效果，比如下图，本来如果我们不考虑异常点，SVM的超平面应该是下图中的红色线所示，但是由于有一个蓝色的异常点，导致我们学习到的超平面是下图中的粗虚线所示，这样会严重影响我们的分类模型预测效果。</p>
<p><img src="/2020/06/03/machine_learning/SVM/1042406-20161125104737206-364720074.png" alt="img"></p>
<p>如何解决这些问题呢？SVM引入了软间隔最大化的方法来解决。回顾硬间隔最大化的条件：</p>
<p>\[
\begin{array}{ll}
\min &amp; \frac{1}{2}\|w\|_{2}^{2} \\ \text { s.t } &amp; y_{i}\left(w^{T} x_{i}+b\right) \geq 1(i=1,2, \ldots m)
\end{array}
\]<br>
接着如何软间隔最大化呢？</p>
<p>SVM对训练集里面的每个样本\((x_i,y_i)\)引入了一个松弛变量\(\xi_i \ge 0\)使函数间隔加上松弛变量大于等于1，也就是说：<br>
\[
y_{i}\left(w \cdot x_{i}+b\right) \geq 1-\xi_{i}
\]<br>
对比硬间隔最大化，可以看到对样本到超平面的函数距离的要求放松了，之前是一定要大于等于1，现在只需要加上一个大于等于0的松弛变量能大于等于1就可以了。当然，对于加入的松弛变量是有成本的，每一个松弛变量\(\xi_i\), 对应了一个代价\(\xi_i\)，这个就得到了的软间隔最大化的SVM学习条件如下：</p>
<p>\[
\begin{aligned}
&amp;\min \frac{1}{2}\|w\|_{2}^{2}+C \sum_{i=1}^{m} \xi_{i}\\
\text {s.t. } &amp; y_{i}\left(w^{T} x_{i}+b\right) \geq 1-\xi_{i}(i=1,2, \ldots m)\\
&amp;\xi_{i} \geq 0 \quad(i=1,2, \ldots m)
\end{aligned}
\]<br>
这里,\(C > 0\)为惩罚参数，可以理解为我们一般回归和分类问题正则化时候的参数。\(C\)越大，对误分类的惩罚越大，\(C\)越小，对误分类的惩罚越小。也就是说，我们希望\(\frac{||w||_2^2}{2}\)尽量小，误分类的点尽可能的少。\(C\)是协调两者关系的正则化惩罚系数。在实际应用中，需要调参来选择。</p>
<p><strong>如果数据中出现了离群点outliers，那么就可以使用松弛变量来解决。</strong></p>
<h3 id="he-han-shu-kernel">核函数Kernel</h3>
<p>事实上，大部分时候数据并不是线性可分的，这个时候满足这样条件的超平面就根本不存在。对于非线性的情况，SVM 的处理方法是选择一个核函数 κ(⋅,⋅) ，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题。</p>
<p>具体来说，**在线性不可分的情况下，支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开。**如图所示，一堆数据在二维空间无法划分，从而映射到三维空间里划分：</p>
<p><img src="/2020/06/03/machine_learning/SVM/00630Defly1g4w7t050xfj30nl0bajrv.jpg" alt></p>
<p><img src="/2020/06/03/machine_learning/SVM/quesbase6415311358438441728.gif" alt></p>
<p>通常人们会从一些常用的核函数中选择（根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数），例如：<strong>多项式核、高斯核、线性核。</strong></p>
<p>核函数简要概括，即以下三点：</p>
<ol>
<li>实际中，我们会经常遇到线性不可分的样例，此时，我们的常用做法是把样例特征映射到高维空间中去(映射到高维空间后，<strong>相关特征便被分开了</strong>，也就达到了分类的目的)；</li>
<li>但进一步，如果凡是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小是会高到可怕的。那咋办呢？</li>
<li>此时，核函数就隆重登场了，核函数的价值在于它虽然也是将特征进行从低维到高维的转换，但核函数能够<strong>事先在低维上进行计算</strong>，而将实质上的分类效果表现在了高维上，避免了直接在高维空间中的复杂计算。</li>
</ol>
<h3 id="zong-jie">总结</h3>
<p>SVM它本质上即是一个分类方法，用 \(w^T+b\) 定义分类函数，于是求\(w,b\)，为寻最大间隔，引出\(\frac{||w||^2_2}{2}\),继而引入拉格朗日因子，化为对拉格朗日乘子a的求解（求解过程中会涉及到一系列最优化或凸二次规划等问题），如此，求\(w,b\)与求\(a\)等价，而\(a\)的求解可以用一种快速学习算法SMO，至于核函数，是为处理非线性情况，若直接映射到高维计算恐维度爆炸，故在低维计算，等效高维表现。</p>
<h2 id="svm-de-ying-yong">SVM的应用</h2>
<p>SVM在很多诸如<strong>文本分类，图像分类，生物序列分析和生物数据挖掘，手写字符识别等领域有很多的应用</strong>。</p>
<h1 id="zhi-chi-xiang-liang-ji-yuan-shi-zui-you-hua-wen-ti-he-he-ye-sun-shi">支持向量机原始最优化问题和合页损失</h1>
<p>线性支持向量机原始最优化问题<br>
\[
\begin{array}{ll}
\min _{w, b, \xi} &amp; \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}   \\
\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N \\
&amp; \xi_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
\]<br>
等价于(合页损失)最优化问题：<br>
\[
\min _{w, b} \sum_{i=1}^{N}\left[1-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}+\lambda\|w\|^{2}
\]<br>
证明，令合页损失中：<br>
\[
1-y_{i}\left(w \cdot x_{i}+b\right)=\xi_{i}, \quad \xi_{i} \geqslant 0
\]<br>
则：\(y_i(w*x_i+b) \ge 1\).于是\(w,b,\xi\)满足支持向量机原始优化问题的约束条件。由\([1-y_i(w*x_i+b)]_+=[\xi_i]_+=\xi_i\)，所以合页损失可以写成:<br>
\[
\min _{w, b} \sum_{i=1}^{N} \xi_{i}+\lambda\|w\|^{2}
\]<br>
取：\(\lambda=\frac{1}{2C}\)，则：<br>
\[
\min _{w, b} \frac{1}{C}\left(\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}\right)
\]<br>
与原始问题等价。</p>
<h2 id="svm-de-yi-xie-wen-ti">SVM的一些问题</h2>
<ol>
<li>
<p>是否存在一组参数使SVM训练误差为0？</p>
<p>答：存在</p>
</li>
<li>
<p>训练误差为0的SVM分类器一定存在吗？</p>
<p>答：一定存在</p>
</li>
<li>
<p>加入松弛变量的SVM的训练误差可以为0吗？</p>
<p>答：使用SMO算法训练的线性分类器并不一定能得到训练误差为0的模型。这是由于优化目标改变了，并不再是使训练误差最小。</p>
</li>
<li>
<p><strong>带核的SVM为什么能分类非线性问题?</strong></p>
<p>答：核函数的本质是两个函数的內积，通过核函数将其隐射到高维空间，在高维空间非线性问题转化为线性问题, SVM得到超平面是高维空间的线性分类平面。其分类结果也视为低维空间的非线性分类结果, 因而带核的SVM就能分类非线性问题。</p>
</li>
<li>
<p><strong>如何选择核函数？</strong></p>
<ul>
<li>如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM；</li>
<li>如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数；</li>
<li>如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况。</li>
</ul>
</li>
</ol>
<h1 id="lr-he-svm-de-lian-xi-yu-qu-bie">LR和SVM的联系与区别</h1>
<h2 id="xiang-tong-dian">相同点</h2>
<ul>
<li>都是线性分类器。本质上都是求一个最佳分类超平面。</li>
<li>都是监督学习算法。</li>
<li>都是判别模型。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。</li>
</ul>
<h2 id="bu-tong-dian">不同点</h2>
<ul>
<li>LR是参数模型，svm是<strong>非参数模型</strong>，linear和rbf则是针对数据线性可分和不可分的区别；</li>
<li>从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。</li>
<li>SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。</li>
<li>逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。</li>
<li>logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。</li>
</ul>
<h1 id="xian-xing-fen-lei-qi-yu-fei-xian-xing-fen-lei-qi-de-qu-bie-yi-ji-you-lie">线性分类器与非线性分类器的区别以及优劣</h1>
<p>线性和非线性是针对模型参数和输入特征来讲的。比如输入\(x\)，模型\(y=ax+ax^2\) 那么就是非线性模型，如果输入是\(x\)和\(x^2\)则模型是线性的。</p>
<ul>
<li>
<p>线性分类器可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。</p>
<p>LR,贝叶斯分类，单层感知机、线性回归</p>
</li>
<li>
<p>非线性分类器效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好。</p>
<p>决策树、RF、GBDT、多层感知机</p>
</li>
</ul>
<p><strong>SVM两种都有（看线性核还是高斯核）</strong></p>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>LightGBM</title>
    <url>/2020/06/03/machine_learning/LightGBM/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/06/03/machine_learning/LightGBM/img_lightgbm.jpg" alt></p>
<a id="more"></a>
<h1 id="light-gbm-shi-shi-yao">LightGBM是什么</h1>
<p>不久前微软DMTK(分布式机器学习工具包)团队在GitHub上开源了性能超越其他boosting工具的LightGBM，在三天之内GitHub上被star了1000次，fork了200次。知乎上有近千人关注“如何看待微软开源的LightGBM？”问题，被评价为“速度惊人”，“非常有启发”，“支持分布式”，“代码清晰易懂”，“占用内存小”等。</p>
<p><a href="%5Bhttps://github.com/Microsoft/LightGBM%5D(https://github.com/Microsoft/LightGBM)">LightGBM （Light Gradient Boosting Machine）</a>是一个实现GBDT算法的框架，支持高效率的并行训练。</p>
<p>LightGBM在Higgs数据集上LightGBM比XGBoost快将近10倍，内存占用率大约为XGBoost的1/6，并且准确率也有提升。GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的GBDT算法是不能满足其需求的。</p>
<p>LightGBM提出的主要原因就<strong>是为了解决GBDT在海量数据遇到的问题</strong>，让GBDT可以更好更快地用于工业实践。</p>
<h2 id="light-gbm-zai-na-xie-di-fang-jin-xing-liao-you-hua-qu-bie-xg-boost">LightGBM在哪些地方进行了优化    (区别XGBoost)？</h2>
<ul>
<li>基于Histogram的决策树算法</li>
<li>带深度限制的Leaf-wise的叶子生长策略</li>
<li>直方图做差加速直接</li>
<li>支持类别特征(Categorical Feature)</li>
<li>Cache命中率优化</li>
<li>基于直方图的稀疏特征优化多线程优化。</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>预排序算法</th>
<th>LightGBM</th>
</tr>
</thead>
<tbody>
<tr>
<td>内存占用</td>
<td><code>2 * #feature * #data #4Bytes</code></td>
<td><code>#feature * #data #1Bytes</code></td>
</tr>
<tr>
<td>统计量累积</td>
<td><code>O(#feature * #data)</code></td>
<td><code>O(#feature * #data)</code></td>
</tr>
<tr>
<td>分割增益计算</td>
<td><code>O(#feature * #data)</code></td>
<td><code>O(#feature * #k)</code></td>
</tr>
<tr>
<td>直方图作差</td>
<td>N/A</td>
<td>加速1倍</td>
</tr>
<tr>
<td>直接支持类别特征</td>
<td>N/A</td>
<td>在Expo数据上加速8倍</td>
</tr>
<tr>
<td>Cache优化</td>
<td>N/A</td>
<td>在Higgs数据上加速40%</td>
</tr>
<tr>
<td>带深度限制的Leaf-wise的决策树算法</td>
<td>N/A</td>
<td>精度更好的模型</td>
</tr>
</tbody>
</table>
<h2 id="histogram-suan-fa">Histogram算法</h2>
<p>直方图算法的基本思想是先把连续的浮点特征值离散化成k个整数（其实又是分桶的思想，而这些桶称为bin，比如[0,0.1)→0, [0.1,0.3)→1），同时构造一个宽度为k的直方图。</p>
<p>在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</p>
<p><img src="/2020/06/03/machine_learning/LightGBM/histogram-1.png" alt></p>
<p>优点：</p>
<ol>
<li>最明显就是内存消耗的降低，直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用8位整型存储就足够了，内存消耗可以降低为原来的1/8。</li>
<li>在计算上的代价也大幅降低，预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次（k可以认为是常数），时间复杂度从<code>O(#data*#feature)</code>优化到<code>O(k*#features)</code>。</li>
</ol>
<h2 id="dai-shen-du-xian-zhi-de-leaf-wise-de-xie-zi-sheng-chang-ce-lue">带深度限制的Leaf-wise的叶子生长策略</h2>
<p>在XGBoost中，树是按层生长的，称为Level-wise tree growth，同一层的所有节点都做分裂，最后剪枝，如下图所示：</p>
<p><img src="https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155197509149646916.png" alt></p>
<p>Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。</p>
<p>在Histogram算法之上，LightGBM进行进一步的优化。首先它抛弃了大多数GBDT工具使用的按层生长 (level-wise)<br>
的决策树生长策略，而使用了带有深度限制的按叶子生长 (leaf-wise)算法。</p>
<p><img src="https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155197520844369289.png" alt></p>
<p>Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</p>
<h2 id="zhi-fang-tu-chai-jia-su">直方图差加速</h2>
<p>LightGBM另一个优化是Histogram（直方图）做差加速。一个容易观察到的现象：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。</p>
<p>利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。</p>
<h2 id="zhi-jie-zhi-chi-lei-bie-te-zheng">直接支持类别特征</h2>
<p>实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，转化到多维的0/1特征，降低了空间和时间的效率。而类别特征的使用是在实践中很常用的。基于这个考虑，LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。并在决策树算法上增加了类别特征的决策规则。在Expo数据集上的实验，相比0/1展开的方法，训练速度可以加速8倍，并且精度一致。据我们所知，LightGBM是第一个直接支持类别特征的GBDT工具。</p>
<h1 id="light-gbm-you-dian">LightGBM优点</h1>
<p>LightGBM （Light Gradient Boosting Machine）是一个实现GBDT算法的框架，支持高效率的并行训练，并且具有以下优点：</p>
<ul>
<li>更快的训练速度</li>
<li>更低的内存消耗</li>
<li>更好的准确率</li>
<li>分布式支持，可以快速处理海量数据</li>
</ul>
<h1 id="dai-ma-shi-xian">代码实现</h1>
<p>为了演示LightGBM在Python中的用法，本代码以sklearn包中自带的鸢尾花数据集为例，用lightgbm算法实现鸢尾花种类的分类任务。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span>  make_classification</span><br><span class="line"></span><br><span class="line">iris = load_iris()   <span class="comment"># 载入鸢尾花数据集</span></span><br><span class="line">data=iris.data</span><br><span class="line">target = iris.target</span><br><span class="line">X_train,X_test,y_train,y_test =train_test_split(data,target,test_size=<span class="number">0.2</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 加载你的数据</span></span><br><span class="line"><span class="comment"># print('Load data...')</span></span><br><span class="line"><span class="comment"># df_train = pd.read_csv('../regression/regression.train', header=None, sep='\t')</span></span><br><span class="line"><span class="comment"># df_test = pd.read_csv('../regression/regression.test', header=None, sep='\t')</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># y_train = df_train[0].values</span></span><br><span class="line"><span class="comment"># y_test = df_test[0].values</span></span><br><span class="line"><span class="comment"># X_train = df_train.drop(0, axis=1).values</span></span><br><span class="line"><span class="comment"># X_test = df_test.drop(0, axis=1).values</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建成lgb特征的数据集格式</span></span><br><span class="line">lgb_train = lgb.Dataset(X_train, y_train) <span class="comment"># 将数据保存到LightGBM二进制文件将使加载更快</span></span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)  <span class="comment"># 创建验证数据</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 将参数写成字典下形式</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'task'</span>: <span class="string">'train'</span>,</span><br><span class="line">    <span class="string">'boosting_type'</span>: <span class="string">'gbdt'</span>,  <span class="comment"># 设置提升类型</span></span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'regression'</span>, <span class="comment"># 目标函数</span></span><br><span class="line">    <span class="string">'metric'</span>: &#123;<span class="string">'l2'</span>, <span class="string">'auc'</span>&#125;,  <span class="comment"># 评估函数</span></span><br><span class="line">    <span class="string">'num_leaves'</span>: <span class="number">31</span>,   <span class="comment"># 叶子节点数</span></span><br><span class="line">    <span class="string">'learning_rate'</span>: <span class="number">0.05</span>,  <span class="comment"># 学习速率</span></span><br><span class="line">    <span class="string">'feature_fraction'</span>: <span class="number">0.9</span>, <span class="comment"># 建树的特征选择比例</span></span><br><span class="line">    <span class="string">'bagging_fraction'</span>: <span class="number">0.8</span>, <span class="comment"># 建树的样本采样比例</span></span><br><span class="line">    <span class="string">'bagging_freq'</span>: <span class="number">5</span>,  <span class="comment"># k 意味着每 k 次迭代执行bagging</span></span><br><span class="line">    <span class="string">'verbose'</span>: <span class="number">1</span> <span class="comment"># &lt;0 显示致命的, =0 显示错误 (警告), &gt;0 显示信息</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">print(<span class="string">'Start training...'</span>)</span><br><span class="line"><span class="comment"># 训练 cv and train</span></span><br><span class="line">gbm = lgb.train(params,lgb_train,num_boost_round=<span class="number">20</span>,valid_sets=lgb_eval,early_stopping_rounds=<span class="number">5</span>) <span class="comment"># 训练数据需要参数列表和数据集</span></span><br><span class="line"> </span><br><span class="line">print(<span class="string">'Save model...'</span>) </span><br><span class="line"> </span><br><span class="line">gbm.save_model(<span class="string">'model.txt'</span>)   <span class="comment"># 训练后保存模型到文件</span></span><br><span class="line"> </span><br><span class="line">print(<span class="string">'Start predicting...'</span>)</span><br><span class="line"><span class="comment"># 预测数据集</span></span><br><span class="line">y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration) <span class="comment">#如果在训练期间启用了早期停止，可以通过best_iteration方式从最佳迭代中获得预测</span></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line">print(<span class="string">'The rmse of prediction is:'</span>, mean_squared_error(y_test, y_pred) ** <span class="number">0.5</span>) <span class="comment"># 计算真实值和预测值之间的均方根误差</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>XGBoost</title>
    <url>/2020/06/01/machine_learning/XGBoost/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/06/01/machine_learning/XGBoost/u=2735111756,2432069868&amp;fm=26&amp;gp=0.jpg.png" alt></p>
<a id="more"></a>
<h1 id="shi-yao-shi-xg-boost">什么是XGBoost</h1>
<p>XGBoost是陈天奇等人开发的一个开源机器学习项目，高效地实现了GBDT算法并进行了算法和工程上的许多改进，被广泛应用在Kaggle竞赛及其他许多机器学习竞赛中并取得了不错的成绩。</p>
<p>说到XGBoost，不得不提GBDT(Gradient Boosting Decision Tree)。因为XGBoost本质上还是一个GBDT，但是力争<strong>把速度和效率发挥到极致</strong>，所以叫X (Extreme) GBoosted。包括前面说过，两者都是boosting方法。</p>
<h2 id="xg-boost-shu-de-ding-yi">XGBoost树的定义</h2>
<p>先来举个<strong>例子</strong>，我们要预测一家人对电子游戏的喜好程度，考虑到年轻和年老相比，年轻更可能喜欢电子游戏，以及男性和女性相比，男性更喜欢电子游戏，故先根据年龄大小区分小孩和大人，然后再通过性别区分开是男是女，逐一给各人在电子游戏喜好程度上打分，就这样，训练出了2棵树tree1和tree2，类似之前GBDT的原理，两棵树的结论累加起来便是最终的结论，所以小孩的预测分数就是两棵树中小孩所落到的结点的分数相加：2 + 0.9 = 2.9。爷爷的预测分数同理：-1 + （-0.9）= -1.9。具体如下图所示：</p>
<p><img src="/2020/06/01/machine_learning/XGBoost/quesbase64153438578739198433.png" alt></p>
<p>惊呼，这不是跟上文介绍的GBDT乃异曲同工么？</p>
<p>事实上，如果不考虑工程实现、解决问题上的一些差异，XGBoost与GBDT<strong>比较大的不同就是目标函数的定义</strong>。XGBoost的目标函数如下图所示：</p>
<p><img src="/2020/06/01/machine_learning/XGBoost/quesbase64153438580139159593.png" alt></p>
<p>其中：</p>
<ul>
<li>红色箭头所指向的L 即为损失函数（比如平方损失函数：\(l(y_i,y^i)=(y_i-y^i)^2\)</li>
<li>红色方框所框起来的是正则项（包括L1正则、L2正则）</li>
<li>红色圆圈所圈起来的为常数项</li>
<li>对于\(f(x)\)，XGBoost利用泰勒展开三项，做一个近似。<strong>\(f(x)\)表示的是其中一颗回归树。</strong></li>
</ul>
<p>XGBoost的<strong>核心算法思想</strong>，基本就是：</p>
<ol>
<li>不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数**\(f(x)\)**，去拟合上次预测的残差。</li>
<li>当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数</li>
<li>最后只需要将每棵树对应的分数加起来就是该样本的预测值。</li>
</ol>
<p>显然，我们的目标是要使得树群的预测值\(y_i^{\prime}\)尽量接近真实值\(y_i\)，而且有尽量大的泛化能力。类似之前GBDT的套路，XGBoost也是需要将多棵树的得分累加得到最终的预测得分（每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差）。</p>
<p><img src="/2020/06/01/machine_learning/XGBoost/quesbase64153438657261833493.png" alt></p>
<p>那接下来，如何选择每一轮加入什么 \(f\) 呢？答案是非常直接的，选取一个 \(f\) 来使得我们的目标函数尽量最大地降低。这里 \(f\) 可以使用泰勒展开公式近似。</p>
<h2 id="zheng-ze-xiang-shu-de-fu-za-du">正则项：树的复杂度</h2>
<p>XGBoost对树的复杂度包含了两个部分：</p>
<ul>
<li>一个是树里面叶子节点的个数T</li>
<li>一个是树上叶子节点的得分w的L2模平方（对w进行L2正则化，相当于针对每个叶结点的得分增加L2平滑，目的是为了避免过拟合）</li>
</ul>
<p><img src="https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64153438674199471483.png" alt></p>
<p>再来看一下XGBoost的目标函数（损失函数揭示训练误差 + 正则化定义复杂度）：</p>
<p>\[
L(\phi)=\sum_{i}l(y_i^{\prime}-y_i)+\sum_k\Omega(f_t)
\]<br>
正则化公式也就是目标函数的后半部分，对于上式而言，\(y_i^{\prime}\)是整个累加模型的输出，正则化项\(\sum \Omega (f_t)\)是则表示树的复杂度的函数，值越小复杂度越低，泛化能力越强。</p>
<h2 id="shu-gai-zen-yao-chang">树该怎么长</h2>
<p>从头到尾了解了xgboost如何优化、如何计算，但树到底长啥样，我们却一直没看到。很显然，一棵树的生成是由一个节点一分为二，然后不断分裂最终形成为整棵树。那么树怎么分裂的就成为了接下来我们要探讨的关键。对于一个叶子节点如何进行分裂，XGBoost作者在其原始论文中给出了一种分裂节点的方法：<strong>枚举所有不同树结构的贪心法</strong></p>
<p>不断地枚举不同树的结构，然后利用打分函数来寻找出一个最优结构的树，接着加入到模型中，不断重复这样的操作。这个寻找的过程使用的就是<strong>贪心算法</strong>。选择一个feature分裂，计算loss function最小值，然后再选一个feature分裂，又得到一个loss function最小值，你枚举完，找一个效果最好的，把树给分裂，就得到了小树苗。</p>
<p>总而言之，XGBoost使用了和CART回归树一样的想法，利用贪婪算法，遍历所有特征的所有特征划分点，不同的是使用的目标函数不一样。具体做法就是分裂后的目标函数值比单子叶子节点的目标函数的增益，同时为了限制树生长过深，还加了个阈值，只有当增益大于该阈值才进行分裂。从而继续分裂，形成一棵树，再形成一棵树，<strong>每次在上一次的预测基础上取最优进一步分裂/建树。</strong></p>
<h2 id="ru-he-ting-zhi-shu-de-xun-huan-sheng-cheng">如何停止树的循环生成</h2>
<p>凡是这种循环迭代的方式必定有停止条件，什么时候停止呢？简言之，设置树的最大深度、当样本权重和小于设定阈值时停止生长以防止过拟合。具体而言，则</p>
<ol>
<li>当引入的分裂带来的增益小于设定阀值的时候，可以忽略掉这个分裂，所以并不是每一次分裂loss function整体都会增加的，有点预剪枝的意思，阈值参数为（即正则项里叶子节点数T的系数）；</li>
<li>当树达到最大深度时则停止建立决策树，设置一个超参数max_depth，避免树太深导致学习局部样本，从而过拟合；</li>
<li>样本权重和小于设定阈值时则停止建树。什么意思呢，即涉及到一个超参数-最小的样本权重和min_child_weight，和GBM的 min_child_leaf 参数类似，但不完全一样。大意就是一个叶子节点样本太少了，也终止同样是防止过拟合；</li>
</ol>
<h1 id="xg-boost-yu-gbdt-you-shi-yao-bu-tong">XGBoost与GBDT有什么不同</h1>
<p>除了算法上与传统的GBDT有一些不同外，XGBoost还在工程实现上做了大量的优化。总的来说，两者之间的区别和联系可以总结成以下几个方面。</p>
<ol>
<li>GBDT是机器学习算法，XGBoost是该算法的工程实现。</li>
<li>在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。</li>
<li>GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代 价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。</li>
<li>传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。</li>
<li>传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。</li>
<li>传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略。</li>
</ol>
<h1 id="wei-shi-yao-xg-boost-yao-yong-tai-le-zhan-kai-you-shi-zai-na-li">为什么XGBoost要用泰勒展开，优势在哪里？</h1>
<p>XGBoost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式, 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了XGBoost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归。</p>
<h2 id="dai-ma-shi-xian">代码实现</h2>
<figure class="highlight nix"><table><tr><td class="code"><pre><span class="line"><span class="built_in">import</span> xgboost</span><br><span class="line"><span class="comment"># First XGBoost model for Pima Indians dataset</span></span><br><span class="line">from numpy <span class="built_in">import</span> loadtxt</span><br><span class="line">from xgboost <span class="built_in">import</span> XGBClassifier </span><br><span class="line">from sklearn.model_selection <span class="built_in">import</span> train_test_split</span><br><span class="line">from sklearn.metrics <span class="built_in">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line"><span class="attr">dataset</span> = loadtxt('pima-indians-diabetes.csv', <span class="attr">delimiter=",")</span></span><br><span class="line"><span class="comment"># split data into X and y</span></span><br><span class="line"><span class="attr">X</span> = dataset[:,<span class="number">0</span>:<span class="number">8</span>]</span><br><span class="line"><span class="attr">Y</span> = dataset[:,<span class="number">8</span>]</span><br><span class="line"><span class="comment"># split data into train and test sets</span></span><br><span class="line"><span class="attr">seed</span> = <span class="number">7</span></span><br><span class="line"><span class="attr">test_size</span> = <span class="number">0.33</span></span><br><span class="line">X_train, X_test, y_train, <span class="attr">y_test</span> = train_test_split(X, Y, <span class="attr">test_size=test_size,</span> <span class="attr">random_state=seed)</span></span><br><span class="line"><span class="comment"># fit model no training data</span></span><br><span class="line"><span class="attr">model</span> = XGBClassifier()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># make predictions for test data</span></span><br><span class="line"><span class="attr">y_pred</span> = model.predict(X_test)</span><br><span class="line"><span class="attr">predictions</span> = [round(value) for value <span class="keyword">in</span> y_pred]</span><br><span class="line"><span class="comment"># evaluate predictions</span></span><br><span class="line"><span class="attr">accuracy</span> = accuracy_score(y_test, predictions)</span><br><span class="line">print(<span class="string">"Accuracy: %.2f%%"</span> % (accuracy * <span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<h1 id="can-kao-wen-xian">参考文献</h1>
<ol>
<li><a href="https://blog.csdn.net/v_JULY_v/article/details/81410574" target="_blank" rel="noopener">通俗理解kaggle比赛大杀器xgboost</a></li>
<li><a href="https://www.cnblogs.com/cassielcode/p/12469053.html" target="_blank" rel="noopener">XGBoost20题</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>GBDT</title>
    <url>/2020/05/31/machine_learning/gbdt/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/31/machine_learning/gbdt/v2-e9d180e7afd45c9cee0c094039df4eb7_1200x500.jpg" alt></p>
<a id="more"></a>
<h1 id="gbdt-suan-fa-de-guo-cheng">GBDT算法的过程</h1>
<p>GBDT(Gradient Boosting Decision Tree)，全名叫梯度提升决策树，使用的是<strong>Boosting</strong>的思想。</p>
<p>GBDT利用最速下降法的近似方法来实现每一步的优化,用损失函数的<strong>负梯度</strong>作为回归问题中提升树算法中的残差的<strong>近似值</strong>,每一步以此来估计回归树叶结点区域以拟合残差的近似值。</p>
<h2 id="boosting-si-xiang">Boosting思想</h2>
<p>Boosting方法训练基分类器时采用串行的方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。</p>
<p>Bagging与Boosting的串行训练方式不同，Bagging方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。</p>
<h2 id="gbdt-yuan-li">GBDT原理</h2>
<p>GBDT的原理很简单，就是所有弱分类器的结果相加等于预测值，然后下一个弱分类器去拟合误差函数对预测值的残差(这个残差就是预测值与真实值之间的误差)。它里面的弱分类器的表现形式就是各棵树。</p>
<p>举一个非常简单的例子，比如我今年30岁了，但计算机或者模型GBDT并不知道我今年多少岁，那GBDT咋办呢？</p>
<ul>
<li>它会在第一个弱分类器（或第一棵树中）随便用一个年龄比如20岁来拟合，然后发现误差有10岁；</li>
<li>接下来在第二棵树中，用6岁去拟合剩下的损失，发现差距还有4岁；</li>
<li>接着在第三棵树中用3岁拟合剩下的差距，发现差距只有1岁了；</li>
<li>最后在第四课树中用1岁拟合剩下的残差，完美。</li>
<li>最终，四棵树的结论加起来，就是真实年龄30岁（实际工程中，gbdt是计算负梯度，用负梯度近似残差）。</li>
</ul>
<h1 id="wei-shi-yao-gbdt-ke-yi-yong-fu-ti-du-jin-si-can-chai">为什么gbdt可以用负梯度近似残差</h1>
<p>回归任务下，GBDT 在每一轮的迭代时对每个样本都会有一个预测值，此时的损失函数为均方差损失函数，<br>
\[
l(y_i,y^i)=\frac{1}{2}(y_i - y^i)^2
\]<br>
此时的负梯度是这样计算的<br>
\[
-[\frac{ \partial l(y_i,y^i)}{\partial y^i}] = (y_i-y^i)
\]<br>
所以，当损失函数选用均方损失函数是时，每一次拟合的值就是（真实值 - 当前模型预测的值），即残差。此时的变量是\(y^i\)，即“当前预测模型的值”，也就是对它求负梯度。</p>
<h1 id="ti-du-ti-sheng-he-ti-du-xia-jiang-de-qu-bie-he-lian-xi-shi-shi-yao">梯度提升和梯度下降的区别和联系是什么？</h1>
<p>下表是梯度提升算法和梯度下降算法的对比情况。</p>
<ul>
<li>相同之处：两者都是在每一轮迭代中，利用损失函数相对于模型的负梯度方向的信息来对当前模型进行更新。</li>
<li>不同之处：在梯度下降中，模型是以参数化形式表示，从而模型的更新等价于参数的更新。而在梯度提升中，模型并不需要进行参数化表示，而是直接定义在函数空间中，从而大大扩展了可以使用的模型种类。</li>
</ul>
<table>
<thead>
<tr>
<th>名称</th>
<th>学习空间</th>
<th>更新方式</th>
<th>损失函数</th>
</tr>
</thead>
<tbody>
<tr>
<td>梯度提升</td>
<td>函数空间\(F\)</td>
<td>\(F=F_{t-1}-\rho \nabla_F L \mid_{F=F_{t-1}}\)</td>
<td>\(L = \sum_{i}{l(y_i,F(x_i))}\)</td>
</tr>
<tr>
<td>梯度下降</td>
<td>参数空间\(W\)</td>
<td>\(W_t = w_{t-1}-\rho \nabla_wL\mid_{w=w_{t-1}}\)</td>
<td>\(L = \sum_{i}{l(y_i,F(x_i))}\)</td>
</tr>
</tbody>
</table>
<h1 id="strong-gbdt-strong-de-you-dian-he-ju-xian-xing-you-na-xie"><strong>GBDT</strong>的优点和局限性有哪些？</h1>
<h2 id="you-dian">优点</h2>
<ol>
<li>预测阶段的计算速度快，树与树之间可并行化计算。</li>
<li>在分布稠密的数据集上，泛化能力和表达能力都很好，这使得GBDT在Kaggle的众多竞赛中，经常名列榜首。</li>
<li>采用决策树作为弱分类器使得GBDT模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系。</li>
</ol>
<h2 id="ju-xian-xing">局限性</h2>
<ol>
<li>GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。</li>
<li>GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。</li>
<li>训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提高训练速度。</li>
</ol>
<h1 id="rf-sui-ji-sen-lin-yu-gbdt-zhi-jian-de-qu-bie-yu-lian-xi">RF(随机森林)与GBDT之间的区别与联系</h1>
<p><strong>相同点</strong>：</p>
<ul>
<li>都是由多棵树组成，最终的结果都是由多棵树一起决定。</li>
<li>RF和GBDT在使用CART树时，可以是分类树或者回归树。</li>
</ul>
<p><strong>不同点</strong>：</p>
<ul>
<li>组成随机森林的树可以并行生成，而GBDT是串行生成</li>
<li>随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和</li>
<li>随机森林对异常值不敏感，而GBDT对异常值比较敏感</li>
<li>随机森林是减少模型的方差，而GBDT是减少模型的偏差</li>
<li>随机森林不需要进行特征归一化。而GBDT<strong>则需要进行特征归一化</strong></li>
</ul>
<h1 id="dai-ma">代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="comment"># 训练数据</span></span><br><span class="line">train_feature = np.genfromtxt(<span class="string">"train_feat.txt"</span>,dtype=np.float32)</span><br><span class="line">num_feature = len(train_feature[<span class="number">0</span>])</span><br><span class="line">train_feature = pd.DataFrame(train_feature)</span><br><span class="line"></span><br><span class="line">train_label = train_feature.iloc[:, num_feature - <span class="number">1</span>]</span><br><span class="line">train_feature = train_feature.iloc[:, <span class="number">0</span>:num_feature - <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据</span></span><br><span class="line">test_feature = np.genfromtxt(<span class="string">"test_feat.txt"</span>,dtype=np.float32)</span><br><span class="line">num_feature = len(test_feature[<span class="number">0</span>])</span><br><span class="line">test_feature = pd.DataFrame(test_feature)</span><br><span class="line"></span><br><span class="line">test_label = test_feature.iloc[:, num_feature - <span class="number">1</span>]</span><br><span class="line">test_feature = test_feature.iloc[:, <span class="number">0</span>:num_feature - <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">gbdt = GradientBoostingRegressor(</span><br><span class="line">  loss = <span class="string">'ls'</span></span><br><span class="line">, learning_rate = <span class="number">0.1</span></span><br><span class="line">, n_estimators = <span class="number">100</span></span><br><span class="line">, subsample = <span class="number">1</span></span><br><span class="line">, min_samples_split = <span class="number">2</span></span><br><span class="line">, min_samples_leaf = <span class="number">1</span></span><br><span class="line">, max_depth = <span class="number">3</span></span><br><span class="line">, init = <span class="literal">None</span></span><br><span class="line">, random_state = <span class="literal">None</span></span><br><span class="line">, max_features = <span class="literal">None</span></span><br><span class="line">, alpha = <span class="number">0.9</span></span><br><span class="line">, verbose = <span class="number">0</span></span><br><span class="line">, max_leaf_nodes = <span class="literal">None</span></span><br><span class="line">, warm_start = <span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">gbdt.fit(train_feature, train_label)</span><br><span class="line">pred = gbdt.predict(test_feature)</span><br><span class="line">total_err = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(pred.shape[<span class="number">0</span>]):</span><br><span class="line">    print(<span class="string">'pred:'</span>, pred[i], <span class="string">' label:'</span>, test_label[i])</span><br><span class="line">print(<span class="string">'均方误差:'</span>, np.sqrt(((pred - test_label) ** <span class="number">2</span>).mean()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">pred: <span class="number">320.0008173984891</span>  label: <span class="number">320.0</span></span><br><span class="line">pred: <span class="number">360.99965033119537</span>  label: <span class="number">361.0</span></span><br><span class="line">pred: <span class="number">363.99928183902097</span>  label: <span class="number">364.0</span></span><br><span class="line">pred: <span class="number">336.0002344322584</span>  label: <span class="number">336.0</span></span><br><span class="line">pred: <span class="number">358.0000159974151</span>  label: <span class="number">358.0</span></span><br><span class="line">均方误差: <span class="number">0.0005218003748239915</span></span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>随机森林</title>
    <url>/2020/05/31/machine_learning/random-forest/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/31/machine_learning/random-forest/v2-88dddf356f60e384e3bae3561e8c19f4_1200x500.jpg" alt></p>
<a id="more"></a>
<h1 id="shi-yao-shi-sui-ji-sen-lin">什么是随机森林</h1>
<h2 id="bagging-si-xiang">Bagging思想</h2>
<p>Bagging是bootstrap aggregating。思想就是从总体样本当中随机取一部分样本进行训练，通过多次这样的结果，进行投票获取平均值作为结果输出，这就极大可能的避免了不好的样本数据，从而提高准确度。因为有些是不好的样本，相当于噪声，模型学入噪声后会使准确度不高。</p>
<p><strong>举个例子</strong>：</p>
<p>假设有1000个样本，如果按照以前的思维，是直接把这1000个样本拿来训练，但现在不一样，先抽取800个样本来进行训练，假如噪声点是这800个样本以外的样本点，就很有效的避开了。重复以上操作，提高模型输出的平均值。</p>
<h2 id="ji-cheng-fang-fa">集成方法</h2>
<h3 id="bagging">bagging</h3>
<p>装袋法的核心思想是构建多个相互独立的评估器，然后对其预测进行平均或多数表决原则来决定集成评估器的结果。</p>
<ol>
<li>评估器：相互独立，同时运行</li>
<li>抽样：有放回抽样</li>
<li>如何决定集成的结果：平均或者少数服从多数</li>
<li>目标：降低方差</li>
<li>基学习器过拟合：能够一定程度上解决基学习器过拟合的问题</li>
<li>基学习器学习能力弱：不是很有帮助</li>
<li>代表算法：随机森林</li>
</ol>
<h3 id="boosting">boosting</h3>
<p>提升方法中，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本进行预测，从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树。</p>
<ol>
<li>评估器：相互关联，按顺序依次构建，后建的模型在先建模型预测失败的样本上有更多的权重</li>
<li>抽样：有放回的采样，但会确认数据的权重，每次抽样都会给预测失败的样本更多的权重</li>
<li>如何决定集成的结果：加权平均，在训练集上表现好的模型会有更大的权重</li>
<li>目标：降低偏差，提高模型整体的精确度</li>
<li>基学习器过拟合：加剧过拟合问题</li>
<li>基学习器学习能力弱：提升模型表现</li>
<li>代表算法：GBDT,Adaboost</li>
</ol>
<h3 id="stacking">stacking</h3>
<p>Stacking模型是指将多种分类器组合在一起来取得更好表现的一种集成学习模型。一般情况下，Stacking模型分为两层。第一层中我们训练多个不同的模型，然后再以第一层训练的各个模型的输出作为输入来训练第二层的模型，以得到一个最终的输出。</p>
<h2 id="sui-ji-sen-lin">随机森林</h2>
<p>Random Forest(随机森林)是一种基于树模型的Bagging的优化版本，一棵树的生成肯定还是不如多棵树，因此就有了随机森林，解决决策树泛化能力弱的特点。(可以理解成三个臭皮匠顶过诸葛亮)</p>
<p>而同一批数据，用同样的算法只能产生一棵树，这时Bagging策略可以帮助我们产生不同的数据集。<strong>Bagging</strong>策略来源于bootstrap aggregation：从样本集（假设样本集\(N\)个数据点）中重采样选出\(n\)个样本（有放回的采样，样本数据点个数仍然不变为N），在所有样本上，对这\(n\)个样本建立分类器（ID3\C4.5\CART\SVM\LOGISTIC），重复以上两步\(m\)次，获得\(m\)个分类器，最后根据这\(m\)个分类器的投票结果，决定数据属于哪一类。</p>
<p><strong>每棵树的按照如下规则生成：</strong></p>
<ol>
<li>如果训练集大小为N，对于每棵树而言，<strong>随机</strong>且有放回地从训练集中的抽取N个训练样本，作为该树的训练集；</li>
<li>如果每个样本的特征维度为M，指定一个常数m&lt;&lt;M，<strong>随机</strong>地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；</li>
<li>每棵树都尽最大程度的生长，并且没有剪枝过程。</li>
</ol>
<p>一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。</p>
<p>总的来说就是随机选择样本数，随机选取特征，随机选择分类器，建立多颗这样的决策树，然后通过这几课决策树来投票，决定数据属于哪一类(<strong>投票机制有一票否决制、少数服从多数、加权多数</strong>)</p>
<h2 id="sui-ji-sen-lin-fen-lei-xiao-guo-de-ying-xiang-yin-su">随机森林分类效果的影响因素</h2>
<ul>
<li>森林中<strong>任意两棵树的相关性</strong>：相关性越大，错误率越大；</li>
<li>森林中<strong>每棵树的分类能力</strong>：每棵树的分类能力越强，整个森林的错误率越低。</li>
</ul>
<p>减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。</p>
<h2 id="sui-ji-sen-lin-you-shi-yao-you-que-dian">随机森林有什么优缺点</h2>
<p><strong>优点：</strong></p>
<ul>
<li>在当前的很多数据集上，相对其他算法有着很大的优势，表现良好。</li>
<li>它能够处理很高维度（feature很多）的数据，并且不用做特征选择(因为特征子集是随机选择的)。</li>
<li>在训练完后，它能够给出哪些feature比较重要。</li>
<li>训练速度快，容易做成并行化方法(训练时树与树之间是相互独立的)。</li>
<li>在训练过程中，能够检测到feature间的互相影响。</li>
<li>对于不平衡的数据集来说，它可以平衡误差。</li>
<li>如果有很大一部分的特征遗失，仍可以维持准确度。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>随机森林已经被证明在某些<strong>噪音较大</strong>的分类或回归问题上会过拟合。</li>
<li>对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。</li>
</ul>
<h2 id="sui-ji-sen-lin-ru-he-chu-li-que-shi-zhi">随机森林如何处理缺失值？</h2>
<p>根据随机森林创建和训练的特点，随机森林对缺失值的处理还是比较特殊的。</p>
<ul>
<li>首先，给缺失值预设一些估计值，比如数值型特征，选择其余数据的中位数或众数作为当前的估计值</li>
<li>然后，根据估计的数值，建立随机森林，把所有的数据放进随机森林里面跑一遍。记录每一组数据在决策树中一步一步分类的路径.</li>
<li>判断哪组数据和缺失数据路径最相似，引入一个相似度矩阵，来记录数据之间的相似度，比如有N组数据，相似度矩阵大小就是N*N</li>
<li>如果缺失值是类别变量，通过权重投票得到新估计值，如果是数值型变量，通过加权平均得到新的估计值，如此迭代，直到得到稳定的估计值。</li>
</ul>
<p>其实，该缺失值填补过程类似于推荐系统中采用协同过滤进行评分预测，先计算缺失特征与其他特征的相似度，再加权得到缺失值的估计，而随机森林中计算相似度的方法（数据在决策树中一步一步分类的路径）乃其独特之处。</p>
<h2 id="shi-yao-shi-oob-sui-ji-sen-lin-zhong-oob-shi-ru-he-ji-suan-de-ta-you-shi-yao-you-que-dian">什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？</h2>
<p><strong>OOB</strong>：</p>
<p>上面我们提到，构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob error（out-of-bag error）。</p>
<p>bagging方法中Bootstrap每次约有1/3的样本不会出现在Bootstrap所采集的样本集合中，当然也就没有参加决策树的建立，把这1/3的数据称为<strong>袋外数据oob（out of bag）</strong>,它可以用于取代测试集误差估计方法。</p>
<p><strong>袋外数据(oob)误差的计算方法如下：</strong></p>
<ul>
<li>对于已经生成的随机森林,用袋外数据测试其性能,假设袋外数据总数为O,用这O个袋外数据作为输入,带进之前已经生成的随机森林分类器,分类器会给出O个数据相应的分类</li>
<li>因为这O条数据的类型是已知的,则用正确的分类与随机森林分类器的结果进行比较,统计随机森林分类器分类错误的数目,设为X,则袋外数据误差大小=X/O</li>
</ul>
<p><strong>优缺点</strong>：</p>
<p>这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。</p>
<h2 id="dai-ma">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载波士顿房价数据集</span></span><br><span class="line">boston_house = load_boston()</span><br><span class="line"></span><br><span class="line">boston_feature_name = boston_house.feature_names</span><br><span class="line">boston_features = boston_house.data</span><br><span class="line">boston_target = boston_house.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看类的用法</span></span><br><span class="line">help(RandomForestRegressor)</span><br><span class="line">rgs = RandomForestRegressor(n_estimators=<span class="number">15</span>)  <span class="comment">##随机森林模型</span></span><br><span class="line">rgs = rgs.fit(boston_features, boston_target)</span><br><span class="line">rgs.predict(boston_features)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>随机森林</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2020/05/30/machine_learning/decision-tree/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/30/machine_learning/decision-tree/v2-39d109b46ea4f34d5efbf67edc11d57d_1440w.png" alt></p>
<a id="more"></a>
<h1 id="shi-yao-shi-jue-ce-shu">什么是决策树</h1>
<h2 id="jue-ce-shu-de-ji-ben-si-xiang">决策树的基本思想</h2>
<p><img src="/2020/05/30/machine_learning/decision-tree/00630Defly1g4q286viibj30pk0pfk09.jpg" alt></p>
<p>LR模型是一股脑儿的把所有特征塞入学习，决策树更像是编程语言中的if-else一样，去做条件判断，这是两者根本性的区别。主要优点是模型具有可读性，分类速度快。其主要围绕着两个问题：</p>
<ol>
<li>如何从数据表中找出最佳节点和最佳分枝？</li>
<li>如何让决策树停止生长，防止过拟合？</li>
</ol>
<h2 id="ce-lue">策略</h2>
<p>决策树学习本质上是从训练数据集中归纳出一组分类规则。我们需要的是一个与训练数据<strong>矛盾较小</strong>,同时具有很好的<strong>泛化能力</strong>的决策树。</p>
<h2 id="suan-fa">算法</h2>
<p>决策树学习算法包含<strong>特征选择</strong>，<strong>决策树的生成</strong>与<strong>决策树的剪枝过程</strong>。生成只考虑局部最优，剪枝则考虑全局最优。</p>
<h3 id="te-zheng-xuan-ze">特征选择</h3>
<p>如果利用一个特征进行分类的结果与随机分类的结果没有很大差别,则称这个特征是<strong>没有分类能力</strong>的.扔掉这样的特征对决策树学习的精度影响不大.</p>
<ol>
<li><strong>信息熵</strong>：熵是衡量<strong>随机变量不确定性</strong>的度量.熵越大,随机变量的不确定性就越大.信息熵是信息量的期望，\(\left.H(X)=-\sum_{x \in X} P(x) \log P(x)\right)\)</li>
<li><strong>条件熵</strong>：条件熵表示在已知随机变量X的条件下随机变量Y的不确定性.\(H(Y | X)=\sum_{i=1}^{n} p_{i} H\left(Y | X=x_{i}\right)\)</li>
<li><strong>信息增益</strong>：表示得知特征X的信息而使得类Y的信息的<strong>不确定性减少</strong>的程度.定义为集合D的经验熵与特征A在给定条件下D的经验条件熵之差\(g(D,A)=H(D)-H(D|A)\),也就是训练数据集中类与特征的<strong>互信息</strong>.</li>
<li><strong>信息增益算法</strong>:
<ol>
<li>计算数据集D的经验熵\(H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|_{\mathrm{log}_{2}}\left|C_{k}\right|}{|D|}\)。</li>
<li>计算特征A对数据集D的经验条件熵\(H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{\mu}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{k}\right|}{\left|D_{i}\right|}\)。</li>
<li>计算信息增益,选取信息增益最大的特征.</li>
</ol>
</li>
<li><strong>信息增益比</strong>:信息增益值的大小是相对于训练数据集而言的,并无绝对意义.使用信息增益比,\(g_{R}(D, A)=\frac{g(D, A)}{H(D)}\)可以对这一问题进行校正.</li>
</ol>
<h3 id="jue-ce-shu-de-sheng-cheng">决策树的生成</h3>
<h4 id="id-3-suan-fa">ID3 算法</h4>
<p>在根节点处计算信息熵，然后根据特征依次划分并计算其节点的信息熵，用 信息增益 = 根节点信息熵–属性节点的信息熵，根据信息增益进行降序排列，排在前面的就是第一个划分属性，其后依次类推，这就得到了决策树的形状，也就是怎么“长”了。</p>
<p><img src="/2020/05/30/machine_learning/decision-tree/image39e7b.png" alt></p>
<p><img src="/2020/05/30/machine_learning/decision-tree/image61cdc.png" alt></p>
<p><img src="/2020/05/30/machine_learning/decision-tree/image9e194.png" alt></p>
<p><img src="/2020/05/30/machine_learning/decision-tree/image09288.png" alt></p>
<p>信息增益存在一个问题：对可取值数目较多的属性有所偏好，例如：考虑将“编号”作为一个属性。为了解决这个问题，引出了另一个 算法C4.5。</p>
<h4 id="c-4-5-suan-fa">C4.5算法</h4>
<p>为了解决信息增益的问题，引入一个信息增益比：</p>
<p>\[
Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}
\]</p>
<p>其中：</p>
<p>\[
IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}
\]</p>
<p>特征a(比如身份id)的可能取值数目越多(即V越大)，则IV(a)的值通常就越大。<strong>信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数。特征取值个数较多时，惩罚参数较小（也就是除以一个较大的值）；特征个数较少时，惩罚参数较大（除以一个较小的值）</strong>。不过有一个缺点：信息增益率<strong>偏向取值较少的特征</strong>。</p>
<p>使用信息增益率：基于以上缺点，并不是直接选择信息增益率最大的特征，而是现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。</p>
<h4 id="strong-cart-suan-fa-strong"><strong>CART算法</strong></h4>
<ol>
<li>
<p>CART既可以用于<strong>分类也</strong>可以用于<strong>回归</strong>.它假设决策树是<strong>二叉树</strong>,内部结点特征的取值为&quot;是&quot;和&quot;否&quot;.递归地构建二叉树,对回归树用<strong>平方误差</strong>最小化准则,对分类数用<strong>基尼指数</strong>最小化准则.</p>
</li>
<li>
<p><strong>回归树的生成</strong>:</p>
<p>CART回归树是假设树为二叉树，通过不断将特征进行分裂。比如当前树结点是基于第j个特征值进行分裂的，设该特征值小于s的样本划分为左子树，大于s的样本划分为右子树。<br>
\[
R_{1}(j, s)=\left\{x | x^{(j)} \leqslant s\right\} \quad \quad R_{2}(j, s)=\left\{x | x^{(l)}>s\right\}
\]</p>
<p>而CART回归树实质上就是在该特征维度对样本空间进行划分，而这种空间划分的优化是一种NP难问题，因此，在决策树模型中是使用启发式方法解决。典型CART回归树产生的目标函数为：<br>
\[
\sum_{x_i \in R_m}{(y_i - f(x_i))^2}
\]<br>
因此，当我们为了求解最优的切分特征j和最优的切分点s，就转化为求解这么一个目标函数：<br>
\[
\min _{j, z}\left[\min _{c_1} \sum_{x \in R_{1}(j, x)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x \in R_{2}(j, x)}\left(y_{i}-c_{2}\right)^{2}\right]
\]<br>
所以我们只要遍历所有特征的的所有切分点，就能找到最优的切分特征和切分点。最终得到一棵回归树。</p>
</li>
<li>
<p><strong>基尼指数</strong>:假设有K个类,样本属于第k类的概率为\(p_k\),则概率分布的基尼指数为:\(\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}\),表示不确定性.在特征A的条件下集合D的基尼指数定义为\(\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)\),表示分割后集合D的不确定性.基尼指数越大,样本集合的<strong>不确定性</strong>也就越大.</p>
</li>
<li>
<p><strong>分类树的生成</strong></p>
<ol>
<li>从根结点开始,设结点的训练数据集为D,对每个特征A和其可能取的每个值a,计算A=a时的基尼指数,</li>
<li>选择<strong>基尼指数最小</strong>的特征及其对应的切分点作为<strong>最优特征</strong>与<strong>最优切分点</strong>,生成两个子结点</li>
<li>递归进行以上操作,直至满足<strong>停止条件</strong>.停止条件一般是结点中的样本个数小于阈值,或样本集的基尼指数小于阈值,或没有更多特征.</li>
</ol>
</li>
<li>
<p><strong>CART剪枝</strong></p>
<p>\(T_t\)表示以t为根结点的子树,\(|T_t|\)是\(T_t\)的叶结点个数.可以证明当\(\alpha=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}\)时,\(T_t\)与\(t\)有相同的损失函数值,且\(t\)的结点少,因此\(t\)比\(T_t\)更可取,对\(T_t\)进行剪枝.<strong>自下而上</strong>地对各内部结点t计算\(g(t)=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}\),并令\(a=min(g(t))\),<strong>自上而下</strong>地访问内部节点t,如果有\(g(t)=a\),进行剪枝,并对t以<strong>多数表决法</strong>决定其类,得到子树T,如此循环地生成一串<strong>子树序列</strong>,直到新生成的T是由根结点单独构成的树为止.利用<strong>交叉验证法</strong>在子树序列中选取最优子树.</p>
<p>如果是<strong>连续值</strong>的情况,一般用<strong>二分法</strong>作为结点来划分.</p>
</li>
</ol>
<h4 id="san-chong-bu-tong-de-jue-ce-shu">三种不同的决策树</h4>
<ul>
<li>
<p><strong>ID3</strong>：取值多的属性，更容易使数据更纯，其信息增益更大。训练得到的是一棵庞大且深度浅的树：不合理。</p>
</li>
<li>
<p><strong>C4.5</strong>：采用信息增益率替代信息增益。</p>
</li>
<li>
<p><strong>CART</strong>：以基尼系数替代熵，最小化不纯度，而不是最大化信息增益。</p>
</li>
</ul>
<h3 id="shu-chang-dao-shi-yao-shi-hou-ting">“树”长到什么时候停</h3>
<ul>
<li>当前结点包含的<strong>样本全属于同一类别</strong>，无需划分；</li>
<li>当前<strong>属性集为空</strong>，或是所有样本在所有属性上取值相同，无法划分；例如：所有的样本特征都是一样的，就造成无法划分了，训练集太单一。</li>
<li>当前结点包含的<strong>样本集合为空</strong>，不能划分。</li>
</ul>
<h3 id="jue-ce-shu-de-jian-zhi">决策树的剪枝</h3>
<ol>
<li>在学习时过多考虑如何提高对训练数据的正确分类,从而构建出过于复杂的决策树,产生<strong>过拟合</strong>现象.解决方法是对已生成的决策树进行简化,称为剪枝.</li>
<li>设树的叶结点个数为\(|T|\),每个叶结点有\(N_t\)个样本点,其中\(k\)类样本点有\(N_{tk}\)个,剪枝往往通过极小化决策树整体的损失函数\(C_{\alpha}(T)=\sum_{i=1}^{\pi} N_{i} H_{i}(T)+\alpha|T|\)来实现,其中经验熵\(H_{t}(T)=-\sum_{k} \frac{N_{a}}{N_{t}} \log \frac{N_{u}}{N_{t}}\).剪枝通过加入\(a|T|\)项来考虑模型复杂度,实际上就是用正则化的极大似然估计进行模型选择.</li>
<li><strong>剪枝算法</strong>:剪去某一子结点,如果生成的新的整体树的<strong>损失函数值</strong>小于原树,则进行剪枝,直到不能继续为止.具体可以由动态规划实现.</li>
</ol>
<h1 id="shu-xing-jie-gou-wei-shi-yao-bu-xu-yao-gui-yi-hua">树形结构为什么不需要归一化?</h1>
<p>根本原因在于：特征选择依赖于在某个特征值下的样本数量带来的信息增益，和特征值的大小无关。所以数值缩放不影响分裂点位置，对树模型的结构不造成影响。<br>
按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。</p>
<p>树模型是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。</p>
<p>既然树形结构（如决策树、RF）不需要归一化，那为何非树形结构比如Adaboost、SVM、LR、Knn、KMeans之类则需要归一化。</p>
<p>对于线性模型，特征值差别很大时，运用梯度下降的时候，损失等高线是椭圆形，需要进行多次迭代才能到达最优点。<br>
但是如果进行了归一化，那么等高线就是圆形的，促使SGD往原点迭代，从而导致需要的迭代次数较少。</p>
<h1 id="jue-ce-shu-ru-he-jian-zhi">决策树如何剪枝</h1>
<p>决策树的剪枝基本策略有 预剪枝 (Pre-Pruning) 和 后剪枝 (Post-Pruning)。</p>
<ul>
<li><strong>预剪枝</strong>：其中的核心思想就是，在每一次实际对结点进行进一步划分之前，先采用验证集的数据来验证如果划分是否能提高划分的准确性。如果不能，就把结点标记为叶结点并退出进一步划分；如果可以就继续递归生成节点。</li>
<li><strong>后剪枝</strong>：后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来泛化性能提升，则将该子树替换为叶结点。</li>
</ul>
<h1 id="dai-ma">代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#用于数据处理和分析的工具包</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment">#引入用于数据预处理/特征工程的工具包</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="comment">#import决策树建模包</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">adult_data = pd.read_csv(<span class="string">'./DecisionTree.csv'</span>)</span><br><span class="line">adult_data.info()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pandas</span>.<span class="title">core</span>.<span class="title">frame</span>.<span class="title">DataFrame</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">RangeIndex</span>:</span> <span class="number">32561</span> entries, <span class="number">0</span> to <span class="number">32560</span></span><br><span class="line">Data columns (total <span class="number">9</span> columns):</span><br><span class="line">workclass         <span class="number">32561</span> non-null object</span><br><span class="line">education         <span class="number">32561</span> non-null object</span><br><span class="line">marital-status    <span class="number">32561</span> non-null object</span><br><span class="line">occupation        <span class="number">32561</span> non-null object</span><br><span class="line">relationship      <span class="number">32561</span> non-null object</span><br><span class="line">race              <span class="number">32561</span> non-null object</span><br><span class="line">gender            <span class="number">32561</span> non-null object</span><br><span class="line">native-country    <span class="number">32561</span> non-null object</span><br><span class="line">income            <span class="number">32561</span> non-null object</span><br><span class="line">dtypes: object(<span class="number">9</span>)</span><br><span class="line">memory usage: <span class="number">2.2</span>+ MB</span><br><span class="line">    </span><br><span class="line">adult_data.shape</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">(<span class="number">32561</span>, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列</span></span><br><span class="line">adult_data.columns</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">Index([<span class="string">u'workclass'</span>, <span class="string">u'education'</span>, <span class="string">u'marital-status'</span>, <span class="string">u'occupation'</span>,</span><br><span class="line">       <span class="string">u'relationship'</span>, <span class="string">u'race'</span>, <span class="string">u'gender'</span>, <span class="string">u'native-country'</span>, <span class="string">u'income'</span>],</span><br><span class="line">      dtype=<span class="string">'object'</span>)</span><br><span class="line"><span class="comment"># 特征</span></span><br><span class="line">feature_columns = [<span class="string">u'workclass'</span>, <span class="string">u'education'</span>, <span class="string">u'marital-status'</span>, <span class="string">u'occupation'</span>, <span class="string">u'relationship'</span>, <span class="string">u'race'</span>, <span class="string">u'gender'</span>, <span class="string">u'native-country'</span>]</span><br><span class="line"><span class="comment"># 标签</span></span><br><span class="line">label_column = [<span class="string">'income'</span>]</span><br><span class="line"><span class="comment">#区分特征和目标列</span></span><br><span class="line">features = adult_data[feature_columns]</span><br><span class="line">label = adult_data[label_column]</span><br><span class="line"><span class="comment"># 把每一个离散的类别特征的取值用one-hot 表示（因为数据都是用字符串表示，需要转化成数字）</span></span><br><span class="line">features = pd.get_dummies(features)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化一个决策树分类器</span></span><br><span class="line">clf = tree.DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>, max_depth=<span class="number">4</span>)</span><br><span class="line"><span class="comment">#用决策树分类器拟合数据</span></span><br><span class="line">clf = clf.fit(features.values, label.values)</span><br><span class="line"><span class="comment"># inference</span></span><br><span class="line">clf.predict(features.values)</span><br></pre></td></tr></table></figure>
<h2 id="ke-shi-hua-jue-ce-shu">可视化决策树</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pydotplus</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display, Image</span><br><span class="line"></span><br><span class="line">dot_data = tree.export_graphviz(clf, </span><br><span class="line">                                out_file=<span class="literal">None</span>, </span><br><span class="line">                                feature_names=features.columns,</span><br><span class="line">                                class_names = [<span class="string">'&lt;=50k'</span>, <span class="string">'&gt;50k'</span>],</span><br><span class="line">                                filled = <span class="literal">True</span>,</span><br><span class="line">                                rounded =<span class="literal">True</span></span><br><span class="line">                               )</span><br><span class="line">                               </span><br><span class="line">graph = pydotplus.graph_from_dot_data(dot_data)</span><br><span class="line">display(Image(graph.create_png()))</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/30/machine_learning/decision-tree/download.png" alt></p>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>命名实体识别</title>
    <url>/2020/05/26/named_entity_recognition/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/26/named_entity_recognition/3c5ea3534a4143adbca52823c5b757f1.jpg" alt></p>
<a id="more"></a>
<h1 id="jian-jie">简介</h1>
<p>命名实体识别(Named Entity Recognition,NER)的主要任务是识别出文本中的人名、地名等专有名称和有意义的时间、日期等数量短语并加以归类。命名实体识别技术是信息抽取、信息检索、机器翻译、问答系统等多种自然语言处理技术必不可少的组成部分。</p>
<p>中文命名实体识别的难度要比英文的难度大（<strong>英文专有名词会大写</strong>）。</p>
<h1 id="yan-jiu-zhu-ti">研究主体</h1>
<p>命名实体是命名实体识别的研究主体,一般包括3大类(实体类、时间类和数字类)和7小类(人名、地名、机构名、时间、日期、货币和百分比)命名实体。实际研究中,命名实体的确切含义需要根据具体应用来确定。</p>
<h1 id="te-dian-ji-nan-dian">特点及难点</h1>
<p>评判一个命名实体是否被正确识别包括两个方面:</p>
<ol>
<li>实体的边界是否正确</li>
<li>实体的类型是否标注正确。</li>
</ol>
<p>英语中的命名实体具有比较明显的形态标志,如人名、地名等实体中的每个词的第一个字母要大写等,所以实体边界识别相对汉语来说比较容易,任务的重点是确定实体的类型。</p>
<p>和英语相比,汉语命名实体识别任务更加复杂,一方面由于分词等因素的影响难度较大,另一方面，其难点主要表现在如下几个方面:</p>
<ol>
<li>命名实体类型多样,数量众多,不断有新的命名实体涌现,如新的人名、地名等,<strong>难以建立大而全的</strong>姓氏库、名字库、地址库等<strong>数据库</strong>。</li>
<li>命名实体构成结构比较复杂,<strong>存在大量的嵌套、别名、缩略词等问题</strong>,没有严格的规律可以遵循,对这类命名实体识别的<strong>召回率</strong>相对偏低。</li>
<li><strong>存在大量的交叉和互相包含现象</strong>,组织名称中也存在大量的人名、地名、数字的现象,要正确标注这些命名实体类型,要涉及上下文语义层面的分析,这些都给命名实体的识别带来困难。</li>
<li>误差传播，<strong>分词、语法分析系统的可靠性</strong>也直接决定命名实体识别的有效性,使得中文命名实体识别更加困难。</li>
</ol>
<h1 id="zhu-yao-fang-fa">主要方法</h1>
<p>命名实体识别的主要技术方法分为:基于规则和词典的方法、无监督学习方法、监督学习方法（基于统计的方法、基于深度学习的方法）。</p>
<h2 id="ji-yu-gui-ze-he-ci-dian-de-fang-fa">基于规则和词典的方法</h2>
<ol>
<li>特定领域词典，其中还包括同义林词典；</li>
<li>句法词汇模板；</li>
<li>正则表达式；</li>
</ol>
<p>总的来说，当词汇表足够大时，基于规则的方法能够取得不错效果。但总结规则模板花费大量时间，且词汇表规模小，且实体识别结果普遍高精度、低召回。</p>
<h2 id="wu-jian-du-xue-xi-fang-fa">无监督学习方法</h2>
<p>主要是基于聚类的方法，根据文本相似度得到不同的簇，表示不同的实体类别组。常用到的特征或者辅助信息有词汇资源、语料统计信息（TF-IDF）、浅层语义信息等。</p>
<h2 id="jian-du-xue-xi-fang-fa">监督学习方法</h2>
<h3 id="ji-yu-tong-ji-de-fang-fa">基于统计的方法</h3>
<p>NER 任务可以是看作是 <strong>token 级别的多分类任务</strong>或<strong>序列标注任务</strong>。</p>
<p>**特征工程：**word 级别特征（词法特征、词性标注等），词汇特征（维基百科、DBpdia 知识），文档及语料级别特征。</p>
<p>**机器学习算法：**隐马尔可夫模型 HMM、决策树 DT、最大熵模型 MEM、最大熵马尔科夫模型 HEMM、支持向量机 SVM、条件随机场 CRF。</p>
<h4 id="yin-ma-er-ke-fu-mo-xing">隐马尔科夫模型</h4>
<p>隐马尔可夫模型描述由一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。隐马尔可夫模型由初始状态分布，状态转移概率矩阵以及观测概率矩阵所确定。</p>
<p>NER本质上可以看成是一种序列标注问题，在使用HMM解决NER这种序列标注问题的时候，我们所能观测到的是字组成的序列（观测序列），观测不到的是每个字对应的标注（状态序列）。对应的，HMM的<strong>三个要素</strong>可以解释为，<strong>初始状态分布</strong>就是每一个标注作为句子第一个字的标注的概率，<strong>状态转移概率矩阵</strong>就是由某一个标注转移到下一个标注的概率，<strong>观测概率矩阵</strong>就是指在某个标注下，生成某个词的概率。根据HMM的三个要素，可以定义如下的HMM模型:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HMM</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, N, M)</span>:</span></span><br><span class="line">        <span class="string">"""Args:</span></span><br><span class="line"><span class="string">            N: 状态数，这里对应存在的标注的种类</span></span><br><span class="line"><span class="string">            M: 观测数，这里对应有多少不同的字</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.N = N</span><br><span class="line">        self.M = M</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 状态转移概率矩阵 A[i][j]表示从i状态转移到j状态的概率</span></span><br><span class="line">        self.A = torch.zeros(N, N)</span><br><span class="line">        <span class="comment"># 观测概率矩阵, B[i][j]表示i状态下生成j观测的概率</span></span><br><span class="line">        self.B = torch.zeros(N, M)</span><br><span class="line">        <span class="comment"># 初始状态概率  Pi[i]表示初始时刻为状态i的概率</span></span><br><span class="line">        self.Pi = torch.zeros(N)</span><br></pre></td></tr></table></figure>
<p>HMM模型的训练过程对应隐马尔可夫模型的学习问题，根据训练数据利用<strong>最大似然</strong>的方法估计模型的三个要素：初始状态分布、状态转移概率矩阵以及观测概率矩阵。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HMM</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, N, M)</span>:</span></span><br><span class="line">        ....</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, word_lists, tag_lists, word2id, tag2id)</span>:</span></span><br><span class="line">        <span class="string">"""HMM的训练，即根据训练语料对模型参数进行估计,</span></span><br><span class="line"><span class="string">           因为我们有观测序列以及其对应的状态序列，所以我们</span></span><br><span class="line"><span class="string">           可以使用极大似然估计的方法来估计隐马尔可夫模型的参数</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            word_lists: 列表，其中每个元素由字组成的列表，如 ['担','任','科','员']</span></span><br><span class="line"><span class="string">            tag_lists: 列表，其中每个元素是由对应的标注组成的列表，如 ['O','O','B-TITLE', 'E-TITLE']</span></span><br><span class="line"><span class="string">            word2id: 将字映射为ID</span></span><br><span class="line"><span class="string">            tag2id: 字典，将标注映射为ID</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> len(tag_lists) == len(word_lists)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 估计转移概率矩阵</span></span><br><span class="line">        <span class="keyword">for</span> tag_list <span class="keyword">in</span> tag_lists:</span><br><span class="line">            seq_len = len(tag_list)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(seq_len - <span class="number">1</span>):</span><br><span class="line">                current_tagid = tag2id[tag_list[i]]</span><br><span class="line">                next_tagid = tag2id[tag_list[i+<span class="number">1</span>]]</span><br><span class="line">                self.A[current_tagid][next_tagid] += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 一个重要的问题：如果某元素没有出现过，该位置为0，这在后续的计算中是不允许的</span></span><br><span class="line">        <span class="comment"># 解决方法：我们将等于0的概率加上很小的数</span></span><br><span class="line">        self.A[self.A == <span class="number">0.</span>] = <span class="number">1e-10</span></span><br><span class="line">        self.A = self.A / self.A.sum(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 估计观测概率矩阵</span></span><br><span class="line">        <span class="keyword">for</span> tag_list, word_list <span class="keyword">in</span> zip(tag_lists, word_lists):</span><br><span class="line">            <span class="keyword">assert</span> len(tag_list) == len(word_list)</span><br><span class="line">            <span class="keyword">for</span> tag, word <span class="keyword">in</span> zip(tag_list, word_list):</span><br><span class="line">                tag_id = tag2id[tag]</span><br><span class="line">                word_id = word2id[word]</span><br><span class="line">                self.B[tag_id][word_id] += <span class="number">1</span></span><br><span class="line">        self.B[self.B == <span class="number">0.</span>] = <span class="number">1e-10</span></span><br><span class="line">        self.B = self.B / self.B.sum(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 估计初始状态概率</span></span><br><span class="line">        <span class="keyword">for</span> tag_list <span class="keyword">in</span> tag_lists:</span><br><span class="line">            init_tagid = tag2id[tag_list[<span class="number">0</span>]]</span><br><span class="line">            self.Pi[init_tagid] += <span class="number">1</span></span><br><span class="line">        self.Pi[self.Pi == <span class="number">0.</span>] = <span class="number">1e-10</span></span><br><span class="line">        self.Pi = self.Pi / self.Pi.sum()</span><br></pre></td></tr></table></figure>
<p>模型训练完毕之后，要利用训练好的模型进行解码，就是对给定的模型未见过的句子，求句子中的每个字对应的标注，针对解码问题，使用的是维特比（viterbi）算法。实现的细节如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HMM</span><span class="params">(object)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decoding</span><span class="params">(self, word_list, word2id, tag2id)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        使用维特比算法对给定观测序列求状态序列， 这里就是对字组成的序列,求其对应的标注。</span></span><br><span class="line"><span class="string">        维特比算法实际是用动态规划解隐马尔可夫模型预测问题，即用动态规划求概率最大路径（最优路径）</span></span><br><span class="line"><span class="string">        这时一条路径对应着一个状态序列</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 问题:整条链很长的情况下，十分多的小概率相乘，最后可能造成下溢</span></span><br><span class="line">        <span class="comment"># 解决办法：采用对数概率，这样源空间中的很小概率，就被映射到对数空间的大的负数</span></span><br><span class="line">        <span class="comment">#  同时相乘操作也变成简单的相加操作</span></span><br><span class="line">        A = torch.log(self.A)</span><br><span class="line">        B = torch.log(self.B)</span><br><span class="line">        Pi = torch.log(self.Pi)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化 维比特矩阵viterbi 它的维度为[状态数, 序列长度]</span></span><br><span class="line">        <span class="comment"># 其中viterbi[i, j]表示标注序列的第j个标注为i的所有单个序列(i_1, i_2, ..i_j)出现的概率最大值</span></span><br><span class="line">        seq_len = len(word_list)</span><br><span class="line">        viterbi = torch.zeros(self.N, seq_len)</span><br><span class="line">        <span class="comment"># backpointer是跟viterbi一样大小的矩阵</span></span><br><span class="line">        <span class="comment"># backpointer[i, j]存储的是 标注序列的第j个标注为i时，第j-1个标注的id</span></span><br><span class="line">        <span class="comment"># 等解码的时候，我们用backpointer进行回溯，以求出最优路径</span></span><br><span class="line">        backpointer = torch.zeros(self.N, seq_len).long()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.Pi[i] 表示第一个字的标记为i的概率</span></span><br><span class="line">        <span class="comment"># Bt[word_id]表示字为word_id的时候，对应各个标记的概率</span></span><br><span class="line">        <span class="comment"># self.A.t()[tag_id]表示各个状态转移到tag_id对应的概率</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 所以第一步为</span></span><br><span class="line">        start_wordid = word2id.get(word_list[<span class="number">0</span>], <span class="literal">None</span>)</span><br><span class="line">        Bt = B.t()</span><br><span class="line">        <span class="keyword">if</span> start_wordid <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 如果字不再字典里，则假设状态的概率分布是均匀的</span></span><br><span class="line">            bt = torch.log(torch.ones(self.N) / self.N)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            bt = Bt[start_wordid]</span><br><span class="line">        viterbi[:, <span class="number">0</span>] = Pi + bt</span><br><span class="line">        backpointer[:, <span class="number">0</span>] = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 递推公式：</span></span><br><span class="line">        <span class="comment"># viterbi[tag_id, step] = max(viterbi[:, step-1]* self.A.t()[tag_id] * Bt[word])</span></span><br><span class="line">        <span class="comment"># 其中word是step时刻对应的字</span></span><br><span class="line">        <span class="comment"># 由上述递推公式求后续各步</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1</span>, seq_len):</span><br><span class="line">            wordid = word2id.get(word_list[step], <span class="literal">None</span>)</span><br><span class="line">            <span class="comment"># 处理字不在字典中的情况</span></span><br><span class="line">            <span class="comment"># bt是在t时刻字为wordid时，状态的概率分布</span></span><br><span class="line">            <span class="keyword">if</span> wordid <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 如果字不再字典里，则假设状态的概率分布是均匀的</span></span><br><span class="line">                bt = torch.log(torch.ones(self.N) / self.N)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                bt = Bt[wordid]  <span class="comment"># 否则从观测概率矩阵中取bt</span></span><br><span class="line">            <span class="keyword">for</span> tag_id <span class="keyword">in</span> range(len(tag2id)):</span><br><span class="line">                max_prob, max_id = torch.max(</span><br><span class="line">                    viterbi[:, step<span class="number">-1</span>] + A[:, tag_id],</span><br><span class="line">                    dim=<span class="number">0</span></span><br><span class="line">                )</span><br><span class="line">                viterbi[tag_id, step] = max_prob + bt[tag_id]</span><br><span class="line">                backpointer[tag_id, step] = max_id</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 终止， t=seq_len 即 viterbi[:, seq_len]中的最大概率，就是最优路径的概率</span></span><br><span class="line">        best_path_prob, best_path_pointer = torch.max(</span><br><span class="line">            viterbi[:, seq_len<span class="number">-1</span>], dim=<span class="number">0</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 回溯，求最优路径</span></span><br><span class="line">        best_path_pointer = best_path_pointer.item()</span><br><span class="line">        best_path = [best_path_pointer]</span><br><span class="line">        <span class="keyword">for</span> back_step <span class="keyword">in</span> range(seq_len<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            best_path_pointer = backpointer[best_path_pointer, back_step]</span><br><span class="line">            best_path_pointer = best_path_pointer.item()</span><br><span class="line">            best_path.append(best_path_pointer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将tag_id组成的序列转化为tag</span></span><br><span class="line">        <span class="keyword">assert</span> len(best_path) == len(word_list)</span><br><span class="line">        id2tag = dict((id_, tag) <span class="keyword">for</span> tag, id_ <span class="keyword">in</span> tag2id.items())</span><br><span class="line">        tag_list = [id2tag[id_] <span class="keyword">for</span> id_ <span class="keyword">in</span> reversed(best_path)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tag_list</span><br></pre></td></tr></table></figure>
<h4 id="tiao-jian-sui-ji-chang">条件随机场</h4>
<p>HMM模型中存在两个假设，一是输出观察值之间严格独立（观测独立性假设），二是状态转移过程中当前状态只与前一状态有关（齐次马尔科夫性假设）。在命名实体识别的场景下，HMM认为观测到的句子中的每个字都是相互独立的，而且当前时刻的标注只与前一时刻的标注相关。但实际上，命名实体识别往往需要更多的特征，比如词性，词的上下文等等，同时当前时刻的标注应该与前一时刻以及后一时刻的标注都相关联。由于这两个假设的存在，显然HMM模型在解决命名实体识别的问题上是存在缺陷的。</p>
<p>而条件随机场就没有这种问题，它通过引入自定义的特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖，可以有效克服HMM模型面临的问题。</p>
<p>为了建立一个条件随机场，首先要定义一个<strong>特征函数集</strong>，该函数集内的每个特征函数都以标注序列作为输入，提取的特征作为输出。假设该函数集为：<br>
\[
\Phi\left(x_{1}, \ldots, x_{m}, s_{1}, \ldots, s_{m}\right) \in \mathbb{R}^{d}
\]<br>
其中 \(x=(x_1,x_2,\ldots,x_m)\)表示观测序列， \(s=(s_1,s_2,\ldots,s_m)\)表示状态序列。然后，条件随机场使用对数线性模型来计算给定观测序列下状态序列的条件概率：<br>
\[
p(s | x ; w)=\frac{\exp (w \cdot \Phi(x, s))}{\sum_{s^{\prime}} \exp \left(w \cdot \Phi\left(x, s^{\prime}\right)\right)}
\]<br>
其中 \(s^\prime\)是是所有可能的状态序列，\(w\)是条件随机场模型的参数，可以把它看成是每个特征函数的权重。CRF模型的训练其实就是对参数 <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]"> 的估计。假设有 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 个已经标注好的数据 <img src="https://www.zhihu.com/equation?tex=%7B%28x%5Ei%2C+s%5Ei%29%7D_%7Bi%3D1%7D%5En" alt="[公式]"> ，则其对数似然函数的正则化形式如下：<br>
\[
L(w)=\sum_{i=1}^{n} \log p\left(s^{i} | x^{i} ; w\right)-\frac{\lambda_{2}}{2}\|w\|_{2}^{2}-\lambda_{1}\|w\|_{1}
\]<br>
那么，最优参数 \(w^\star\) 就是：<br>
\[
w^{*}=\arg \max _{w \in \mathbb{R}^{d}} L\left(w\right)
\]<br>
解码采用维特比算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn_crfsuite <span class="keyword">import</span> CRF   <span class="comment"># 借助一个外部的库</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word2features</span><span class="params">(sent, i)</span>:</span></span><br><span class="line">    <span class="string">"""抽取单个字的特征"""</span></span><br><span class="line">    word = sent[i]</span><br><span class="line">    prev_word = <span class="string">"&lt;s&gt;"</span> <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> sent[i<span class="number">-1</span>]</span><br><span class="line">    next_word = <span class="string">"&lt;/s&gt;"</span> <span class="keyword">if</span> i == (len(sent)<span class="number">-1</span>) <span class="keyword">else</span> sent[i+<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 因为每个词相邻的词会影响这个词的标记</span></span><br><span class="line">    <span class="comment"># 所以我们使用：</span></span><br><span class="line">    <span class="comment"># 前一个词，当前词，后一个词，</span></span><br><span class="line">    <span class="comment"># 前一个词+当前词， 当前词+后一个词</span></span><br><span class="line">    <span class="comment"># 作为特征</span></span><br><span class="line">    features = &#123;</span><br><span class="line">        <span class="string">'w'</span>: word,</span><br><span class="line">        <span class="string">'w-1'</span>: prev_word,</span><br><span class="line">        <span class="string">'w+1'</span>: next_word,</span><br><span class="line">        <span class="string">'w-1:w'</span>: prev_word+word,</span><br><span class="line">        <span class="string">'w:w+1'</span>: word+next_word,</span><br><span class="line">        <span class="string">'bias'</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sent2features</span><span class="params">(sent)</span>:</span></span><br><span class="line">    <span class="string">"""抽取序列特征"""</span></span><br><span class="line">    <span class="keyword">return</span> [word2features(sent, i) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(sent))]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRFModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 algorithm=<span class="string">'lbfgs'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 c1=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 c2=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_iterations=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 all_possible_transitions=False</span></span></span><br><span class="line"><span class="function"><span class="params">                 )</span>:</span></span><br><span class="line"></span><br><span class="line">        self.model = CRF(algorithm=algorithm,</span><br><span class="line">                         c1=c1,</span><br><span class="line">                         c2=c2,</span><br><span class="line">                         max_iterations=max_iterations,</span><br><span class="line">                         all_possible_transitions=all_possible_transitions)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, sentences, tag_lists)</span>:</span></span><br><span class="line">        <span class="string">"""训练模型"""</span></span><br><span class="line">        features = [sent2features(s) <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">        self.model.fit(features, tag_lists)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, sentences)</span>:</span></span><br><span class="line">        <span class="string">"""解码,对给定句子预测其标注"""</span></span><br><span class="line">        features = [sent2features(s) <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">        pred_tag_lists = self.model.predict(features)</span><br><span class="line">        <span class="keyword">return</span> pred_tag_lists</span><br></pre></td></tr></table></figure>
<h3 id="strong-shen-du-xue-xi-de-fang-fa-strong"><strong>深度学习的方法</strong></h3>
<h4 id="fen-bu-shi-biao-shi">分布式表示</h4>
<ol>
<li>词级别表示：使用词嵌入方式，对不同语料进行训练，如生物医学领域PubMed、NYT 之类。</li>
<li>字符级别表示：字符嵌入主要可以降低 OOV 率。</li>
<li>混合信息表示：除了词级别表示、字符级别表示外，一些研究工作还嵌入了其他一些语义信息，如词汇相似 度、词性标注、分块、语义依赖、汉字偏旁、汉字拼音以及位置嵌入，词嵌入，段嵌入等。</li>
</ol>
<h4 id="shang-xia-wen-bian-ma">上下文编码</h4>
<p>卷积网络 CNN、循环网络 RNN、递归网络、Transformer</p>
<h5 id="cnn">CNN</h5>
<p>句子经过 embedding 层，一个 word 被表示为 N 维度的向量，随后整个句子表示使用卷积（通常为一维卷积）编码，进而得到每个 word 的局部特征，再使用最大池化操作得到整个句子的全局特征，可以直接将其送入解码层输出标签，也可以将其和局部特征向量一起送入解码层。</p>
<p><img src="/2020/05/26/named_entity_recognition/640.png" alt></p>
<h5 id="rnn">RNN</h5>
<p>LSTM也常常被用来解决序列标注问题。和HMM、CRF不同的是，LSTM是依靠神经网络超强的非线性拟合能力，在训练时将样本通过高维空间中的复杂非线性变换，学习到从样本到标注的函数，之后使用这个函数为指定的样本预测每个token的标注。LSTM比起CRF模型最大的好处就是<strong>简单粗暴</strong>，不需要做繁杂的特征工程，直接训练即可.</p>
<p>常用的循环神经网络包括 LSTM 和 GRU，在 NLP 中常使用双向网络 BiRNN，从左到右和从右到左两个方向提取问题特征。</p>
<p><img src="/2020/05/26/named_entity_recognition/640-20200527211629345.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_packed_sequence, pack_padded_sequence</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiLSTM</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, emb_size, hidden_size, out_size)</span>:</span></span><br><span class="line">        <span class="string">"""初始化参数：</span></span><br><span class="line"><span class="string">            vocab_size:字典的大小</span></span><br><span class="line"><span class="string">            emb_size:词向量的维数</span></span><br><span class="line"><span class="string">            hidden_size：隐向量的维数</span></span><br><span class="line"><span class="string">            out_size:标注的种类</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(BiLSTM, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, emb_size)</span><br><span class="line">        self.bilstm = nn.LSTM(emb_size, hidden_size,</span><br><span class="line">                              batch_first=<span class="literal">True</span>,</span><br><span class="line">                              bidirectional=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.lin = nn.Linear(<span class="number">2</span>*hidden_size, out_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, sents_tensor, lengths)</span>:</span></span><br><span class="line">        emb = self.embedding(sents_tensor)  <span class="comment"># [B, L, emb_size]</span></span><br><span class="line"></span><br><span class="line">        packed = pack_padded_sequence(emb, lengths, batch_first=<span class="literal">True</span>)</span><br><span class="line">        rnn_out, _ = self.bilstm(packed)</span><br><span class="line">        <span class="comment"># rnn_out:[B, L, hidden_size*2]</span></span><br><span class="line">        rnn_out, _ = pad_packed_sequence(rnn_out, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        scores = self.lin(rnn_out)  <span class="comment"># [B, L, out_size]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, sents_tensor, lengths, _)</span>:</span></span><br><span class="line">        <span class="string">"""解码"""</span></span><br><span class="line">        logits = self.forward(sents_tensor, lengths)  <span class="comment"># [B, L, out_size]</span></span><br><span class="line">        _, batch_tagids = torch.max(logits, dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> batch_tagid</span><br></pre></td></tr></table></figure>
<h5 id="rnn-yu-cnn-xiang-jie-he-de-fang-shi">RNN与CNN 相结合的方式</h5>
<p>BiLSTM-CNN 的网络结构，有 ID-CNNs 迭代膨胀卷积：</p>
<p><img src="/2020/05/26/named_entity_recognition/640-20200527211818522.png" alt></p>
<h5 id="lstm-crf">LSTM + CRF</h5>
<p>简单的LSTM的优点是能够通过<strong>双向</strong>的设置学习到观测序列（输入的字）之间的依赖，在训练过程中，LSTM能够根据目标自动提取观测序列的特征，但是缺点是无法学习到状态序列（输出的标注）之间的关系，而在命名实体识别任务中，标注之间是有一定的关系的，比如O类标注后面不会接一个I类标注，所以LSTM在解决NER这类序列标注任务时，虽然可以省去很繁杂的特征工程，但是也存在无法学习到标注上下文的缺点。</p>
<p>相反，CRF的优点就是能对隐含状态建模，学习状态序列的特点，但它的缺点是需要手动提取序列特征。所以一般的做法是，在LSTM后面再加一层CRF，以获得两者的优点。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiLSTM_CRF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, emb_size, hidden_size, out_size)</span>:</span></span><br><span class="line">        <span class="string">"""初始化参数：</span></span><br><span class="line"><span class="string">            vocab_size:字典的大小</span></span><br><span class="line"><span class="string">            emb_size:词向量的维数</span></span><br><span class="line"><span class="string">            hidden_size：隐向量的维数</span></span><br><span class="line"><span class="string">            out_size:标注的种类</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(BiLSTM_CRF, self).__init__()</span><br><span class="line">        <span class="comment"># 这里的BiLSTM就是LSTM模型部分所定义的BiLSTM模型</span></span><br><span class="line">        self.bilstm = BiLSTM(vocab_size, emb_size, hidden_size, out_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># CRF实际上就是多学习一个转移矩阵 [out_size, out_size] 初始化为均匀分布</span></span><br><span class="line">        self.transition = nn.Parameter(</span><br><span class="line">            torch.ones(out_size, out_size) * <span class="number">1</span>/out_size)</span><br><span class="line">        <span class="comment"># self.transition.data.zero_()</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, sents_tensor, lengths)</span>:</span></span><br><span class="line">        <span class="comment"># [B, L, out_size]</span></span><br><span class="line">        emission = self.bilstm(sents_tensor, lengths)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算CRF scores, 这个scores大小为[B, L, out_size, out_size]</span></span><br><span class="line">        <span class="comment"># 也就是每个字对应对应一个 [out_size, out_size]的矩阵</span></span><br><span class="line">        <span class="comment"># 这个矩阵第i行第j列的元素的含义是：上一时刻tag为i，这一时刻tag为j的分数</span></span><br><span class="line">        batch_size, max_len, out_size = emission.size()</span><br><span class="line">        crf_scores = emission.unsqueeze(</span><br><span class="line">            <span class="number">2</span>).expand(<span class="number">-1</span>, <span class="number">-1</span>, out_size, <span class="number">-1</span>) + self.transition.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> crf_scores</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, test_sents_tensor, lengths, tag2id)</span>:</span></span><br><span class="line">        <span class="string">"""使用维特比算法进行解码"""</span></span><br><span class="line">        start_id = tag2id[<span class="string">'&lt;start&gt;'</span>]</span><br><span class="line">        end_id = tag2id[<span class="string">'&lt;end&gt;'</span>]</span><br><span class="line">        pad = tag2id[<span class="string">'&lt;pad&gt;'</span>]</span><br><span class="line">        tagset_size = len(tag2id)</span><br><span class="line"></span><br><span class="line">        crf_scores = self.forward(test_sents_tensor, lengths)</span><br><span class="line">        device = crf_scores.device</span><br><span class="line">        <span class="comment"># B:batch_size, L:max_len, T:target set size</span></span><br><span class="line">        B, L, T, _ = crf_scores.size()</span><br><span class="line">        <span class="comment"># viterbi[i, j, k]表示第i个句子，第j个字对应第k个标记的最大分数</span></span><br><span class="line">        viterbi = torch.zeros(B, L, T).to(device)</span><br><span class="line">        <span class="comment"># backpointer[i, j, k]表示第i个句子，第j个字对应第k个标记时前一个标记的id，用于回溯</span></span><br><span class="line">        backpointer = (torch.zeros(B, L, T).long() * end_id).to(device)</span><br><span class="line">        lengths = torch.LongTensor(lengths).to(device)</span><br><span class="line">        <span class="comment"># 向前递推</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(L):</span><br><span class="line">            batch_size_t = (lengths &gt; step).sum().item()</span><br><span class="line">            <span class="keyword">if</span> step == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 第一个字它的前一个标记只能是start_id</span></span><br><span class="line">                viterbi[:batch_size_t, step,</span><br><span class="line">                        :] = crf_scores[: batch_size_t, step, start_id, :]</span><br><span class="line">                backpointer[: batch_size_t, step, :] = start_id</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                max_scores, prev_tags = torch.max(</span><br><span class="line">                    viterbi[:batch_size_t, step<span class="number">-1</span>, :].unsqueeze(<span class="number">2</span>) +</span><br><span class="line">                    crf_scores[:batch_size_t, step, :, :],     <span class="comment"># [B, T, T]</span></span><br><span class="line">                    dim=<span class="number">1</span></span><br><span class="line">                )</span><br><span class="line">                viterbi[:batch_size_t, step, :] = max_scores</span><br><span class="line">                backpointer[:batch_size_t, step, :] = prev_tags</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在回溯的时候我们只需要用到backpointer矩阵</span></span><br><span class="line">        backpointer = backpointer.view(B, <span class="number">-1</span>)  <span class="comment"># [B, L * T]</span></span><br><span class="line">        tagids = []  <span class="comment"># 存放结果</span></span><br><span class="line">        tags_t = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(L<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            batch_size_t = (lengths &gt; step).sum().item()</span><br><span class="line">            <span class="keyword">if</span> step == L<span class="number">-1</span>:</span><br><span class="line">                index = torch.ones(batch_size_t).long() * (step * tagset_size)</span><br><span class="line">                index = index.to(device)</span><br><span class="line">                index += end_id</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                prev_batch_size_t = len(tags_t)</span><br><span class="line"></span><br><span class="line">                new_in_batch = torch.LongTensor(</span><br><span class="line">                    [end_id] * (batch_size_t - prev_batch_size_t)).to(device)</span><br><span class="line">                offset = torch.cat(</span><br><span class="line">                    [tags_t, new_in_batch],</span><br><span class="line">                    dim=<span class="number">0</span></span><br><span class="line">                )  <span class="comment"># 这个offset实际上就是前一时刻的</span></span><br><span class="line">                index = torch.ones(batch_size_t).long() * (step * tagset_size)</span><br><span class="line">                index = index.to(device)</span><br><span class="line">                index += offset.long()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            tags_t = backpointer[:batch_size_t].gather(</span><br><span class="line">                dim=<span class="number">1</span>, index=index.unsqueeze(<span class="number">1</span>).long())</span><br><span class="line">            tags_t = tags_t.squeeze(<span class="number">1</span>)</span><br><span class="line">            tagids.append(tags_t.tolist())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># tagids:[L-1]（L-1是因为扣去了end_token),大小的liebiao</span></span><br><span class="line">        <span class="comment"># 其中列表内的元素是该batch在该时刻的标记</span></span><br><span class="line">        <span class="comment"># 下面修正其顺序，并将维度转换为 [B, L]</span></span><br><span class="line">        tagids = list(zip_longest(*reversed(tagids), fillvalue=pad))</span><br><span class="line">        tagids = torch.Tensor(tagids).long()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 返回解码的结果</span></span><br><span class="line">        <span class="keyword">return</span> tagids</span><br></pre></td></tr></table></figure>
<h5 id="di-gui-shen-jing-wang-luo">递归神经网络</h5>
<p>递归神经网络相较循环神经网络，最大区别是具有树状阶层结构。循环神经网络一个很好的特性是通过神经元循环结构处理变长序列，而对于具有树状或图结构的数据很难建模（如语法解析树）。还有一点特别在于其训练算法不同于常规的后向传播算法，而是采用 BPTS (Back Propagation Through Structure)。虽然递归神经网络理论上感觉效果不错，但实际应用中效果一般，并且很难训练。</p>
<p><img src="/2020/05/26/named_entity_recognition/640-20200527213037128.png" alt="img"></p>
<h5 id="transformer">Transformer</h5>
<p>transformer 在长距离文本依赖上相较 RNN 有更好的效果。</p>
<p><img src="/2020/05/26/named_entity_recognition/over_all.png" alt></p>
<h5 id="bert-lstm-crf">BERT+（LSTM）+CRF</h5>
<p>BERT中蕴含了大量的通用知识，利用预训练好的BERT模型，再用少量的标注数据进行FINETUNE是一种快速的获得效果不错的NER的方法。</p>
<h5 id="strong-yu-yan-mo-xing-strong"><strong>语言模型</strong></h5>
<ol>
<li>word2vec</li>
<li>Glove</li>
<li>fasttext</li>
<li>ELMO</li>
<li>BERT</li>
<li>GPT</li>
<li>GPT2</li>
<li>XLNET</li>
<li>ALBERT</li>
<li>RoBERTa</li>
</ol>
<h4 id="strong-jie-ma-ceng-strong"><strong>解码层</strong></h4>
<ol>
<li>MLP+softmax</li>
<li>CRF</li>
<li>RNN</li>
<li>Pointer Network</li>
</ol>
<p>1和2比较常见。3使用 RNN 解码，框架图如下所示。文中所述当前输出（并非隐藏层输出）经过 softmax 损失函数后输入至下一时刻 LSTM 单元，所以这是一个局部归一化模型。</p>
<p><img src="/2020/05/26/named_entity_recognition/640-20200527213946340.png" alt></p>
<p>使用指针网络解码，是将 NER 任务当作先识别“块”即实体范围，然后再对其进行分类。指针网络通常是在 Seq2seq 框架中，如下图所示。</p>
<p><img src="/2020/05/26/named_entity_recognition/640-20200527214031105.png" alt></p>
<h2 id="strong-qi-ta-yan-jiu-fang-xiang-de-ner-fang-fa-strong"><strong>其他研究方向的NER方法</strong></h2>
<ol>
<li>多任务学习 Multi-task Learning</li>
<li>深度迁移学习 Deep Transfer Learning</li>
<li>深度主动学习 Deep Active Learning</li>
<li>深度强化学习 Deep Reinforcement Learning</li>
<li>深度对抗学习 Deep Adversarial Learning</li>
<li>注意力机制 Neural Attention</li>
</ol>
<h1 id="shu-ju-ji">数据集</h1>
<ol>
<li>医疗数据集</li>
<li>kaggle数据集</li>
<li>BosonNLP命名实体识别数据</li>
</ol>
<h1 id="ping-ce-zhi-biao">评测指标</h1>
<p>NER 评测指标 P R F1 分为两类，这也是比赛和论文中通用评测方式：</p>
<ol>
<li><strong>Exact-match</strong>严格匹配，范围与类别都正确。其中 F1 值又可以分为 macro-averaged 和 micro-averaged，前者是按照不同实体类别计算 F1，然后取平均；后者是把所有识别结果合在一起，再计算 F1。这两者的区别在于实体类别数目不均衡，因为通常语料集中类别数量分布不均衡，模型往往对于大类别的实体学习较好。</li>
<li><strong>relaxed match</strong> 宽松匹配，简言之，可视为实体位置区间部分重叠，或位置正确类别错误的，都记为正确或按照匹配的位置区间大小评测。</li>
</ol>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/61227299" target="_blank" rel="noopener">NLP实战-中文命名实体识别</a></li>
<li><a href="https://blog.csdn.net/u014033218/article/details/89304699" target="_blank" rel="noopener">命名实体识别研究综述</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
      <tags>
        <tag>ner</tag>
        <tag>命名实体识别</tag>
      </tags>
  </entry>
  <entry>
    <title>逻辑回归与最大熵模型</title>
    <url>/2020/05/23/machine_learning/logistic_regression_and_maximum_entropy_mode/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/23/machine_learning/logistic_regression_and_maximum_entropy_mode/Logistic-Regression-learning.png" alt></p>
<a id="more"></a>
<h1 id="luo-ji-hui-gui">逻辑回归</h1>
<h2 id="shi-yao-shi-luo-ji-hui-gui">什么是逻辑回归</h2>
<p>逻辑回归是用来做分类算法的，线性回归，一般形式是\(y=w \ast x + b\) ，y的取值范围是\([- \infty, +\infty]\)，有这么多取值，把\(y\)的结果带入一个非线性变换的<strong>Sigmoid函数</strong>中，即可得到[0,1]之间取值范围的数\(S\)，\(S\)可以看成是一个概率值，如果设置概率阈值为0.5，那么\(S\)大于0.5可以看成是正样本，小于0.5看成是负样本，这样就可以进行分类了。</p>
<h2 id="shi-yao-shi-dui-shu-ji-lu">什么是对数几率</h2>
<p>如果某事件发生的概率是p,则该事件发生的<strong>几率</strong>(此处几率指该事件发生概率与不发生概率之比)是\(\frac{p}{1-p}\), <strong>对数几率</strong> 是\(log(\frac{p}{1-p})\),那么\(\log \frac{P(Y=1 | x)}{1-P(Y=1 | x)}=w \cdot x\)，也就是说在逻辑斯谛回归模型中,输出Y=1的对数几率是输入x的<strong>线性函数</strong>,线性函数值越接近正无穷,概率值就越接近1,反之则越接近0.</p>
<h2 id="shi-yao-shi-sigmoid-han-shu">什么是Sigmoid函数</h2>
<p>公式及图像如下：</p>
<p><img src="/2020/05/23/machine_learning/logistic_regression_and_maximum_entropy_mode/00630Defly1g4pvk2ctatj30cw0b63yq.jpg" alt></p>
<p>函数中\(t\)无论取什么值，其结果都在\([0,1]\)的区间内，一个分类问题就有两种答案，一种是“是”，一种是“否”，那0对应着“否”，1对应着“是”，假设分类的<strong>阈值</strong>是0.5，那么超过0.5的归为1分类，低于0.5的归为0分类，阈值是可以自己设定的。把\(w \ast x + b\)带入t中就得到了逻辑回归的一般模型方程：</p>
<p>\[
H_{w,b}(x)=\frac{1}{1+e^{(w \ast x+b)}}
\]<br>
结果可以理解为概率，概率大于0.5的属于1分类，概率小于0.5的属于0分类，这样就达到了分类的目的。</p>
<h2 id="sun-shi-han-shu-shi-shi-yao">损失函数是什么</h2>
<p>逻辑回归的损失函数是 <strong>log loss</strong>，也就是<strong>对数似然函数</strong>，函数公式如下：</p>
<p>\[
\operatorname{cost}\left(h_{\theta}(x), y\right)=\left\{\begin{aligned}
-\log \left(h_{\theta}(x)\right) &amp; \text { if } y=1 \\
-\log \left(1-h_{\theta}(x)\right) &amp; \text { if } y=0
\end{aligned}\right.
\]</p>
<p>公式中的 \(y=1\) 表示的是真实值为1时用第一个公式，真实 \(y=0\) 用第二个公式计算损失。当真实样本为\(y=1\)时，但\(h=0\)概率，那么\(log0=\infty\)，这就对模型最大的惩罚力度；当真实样本\(y=1\)，并且\(h=1\)时，那么\(log1=0\)，相当于没有惩罚，也就是没有损失，达到最优结果。</p>
<p>上面公式也可以表示成：<br>
\[
\begin{aligned} L(w) &amp;=\sum_{i=1}^{N}\left[y_{i} \log \pi\left(x_{i}\right)+\left(1-y_{i}\right)\log\left(1-\pi\left(x_{i}\right)\right)\right]\\ &amp;=\sum_{i=1}^{N}\left[y_{i} \log \frac{\pi\left(x_{i}\right)}{1-\pi\left(x_{i}\right)}+\log \left(1-\pi\left(x_{i}\right)\right)\right]\\&amp;=\sum_{i=1}^{N}\left[y_{i}\left(w \cdot x_{i}\right)-\log \left(1+\exp \left(w \cdot x_{i}\right)\right]\right.\end{aligned}
\]</p>
<p>最后按照梯度下降法一样，求解极小值点，得到想要的模型效果。</p>
<h2 id="ru-he-jin-xing-duo-xiang-luo-ji-hui-gui">如何进行多项逻辑回归</h2>
<p>当问题是多分类问题时,可以作如下推广:设Y有K类可能取值,\(P(Y=k | x)=\frac{\exp \left(w_{k} \cdot x\right)}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}, \quad k=1,2, \cdots, K-1 \quad P(Y=K | x)=\frac{1}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}\),实际上就是<strong>one-vs-all</strong>的思想,将其他所有类当作一个类,问题转换为二分类问题.</p>
<p>使用最大似然法衡量模型输出的概率与真实概率的差别，假设样本一共有N个，那么这组样本发生的总概率可以表示为：<br>
\[
P(\boldsymbol{W})=\prod_{n=1}^{N}\left(\frac{e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}}{\sum_{k^{\prime}}^{C} e^{\boldsymbol{w}_{k^{\prime}}^{T} \boldsymbol{x}_{n}}}\right)
\]<br>
对函数取对数再乘以-1，推导得到：<br>
\[
\begin{aligned}
F(\boldsymbol{W})=-\ln (P(\boldsymbol{W})) &amp;=\sum_{n=1}^{N} \ln \left(\frac{\sum_{k^{\prime}}^{C} e^{\boldsymbol{w}_{k^{\prime}}^{T} \boldsymbol{x}_{n}}}{e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}}\right) \\
&amp;=\sum_{n=1}^{N} \ln \left(\frac{e^{\boldsymbol{w}_{1}^{T} \boldsymbol{x}}+e^{\boldsymbol{w}_{2}^{T} \boldsymbol{x}}+\ldots+e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}+\ldots+e^{\boldsymbol{w}_{c}^{T} \boldsymbol{x}}}{e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}}\right) \\
&amp;=\sum_{n=1}^{N} \ln \left(1+\sum_{k \neq y_{n}} e^{\boldsymbol{w}_{k} \boldsymbol{x}_{n}-\boldsymbol{w}_{y_{n}} \boldsymbol{x}_{n}}\right)
\end{aligned}
\]</p>
<h2 id="luo-ji-hui-gui-you-shi-yao-you-dian">逻辑回归有什么优点</h2>
<ul>
<li>LR能以概率的形式输出结果，而非只是0,1判定。</li>
<li>LR的可解释性强，可控度高(你要给老板讲的嘛…)。</li>
<li>训练快，feature engineering之后效果赞。</li>
<li>因为结果是概率，可以做ranking model。</li>
</ul>
<h2 id="luo-ji-hui-gui-chang-yong-de-you-hua-fang-fa-you-na-xie">逻辑回归常用的优化方法有哪些</h2>
<h3 id="yi-jie-fang-fa-ti-du-xia-jiang">一阶方法：梯度下降</h3>
<p>梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。</p>
<h3 id="er-jie-fang-fa-niu-dun-fa-ni-niu-dun-fa">二阶方法：牛顿法、拟牛顿法</h3>
<p>牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。</p>
<p>实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。</p>
<p>缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。</p>
<p>拟牛顿法： 不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。</p>
<h2 id="luo-ji-si-te-hui-gui-wei-shi-yao-yao-dui-te-zheng-jin-xing-chi-san-hua">逻辑斯特回归为什么要对特征进行离散化</h2>
<ol>
<li>非线性！逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代；</li>
<li>速度快！稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</li>
<li>鲁棒性！离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</li>
<li>方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</li>
<li>稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；</li>
<li>简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</li>
</ol>
<h2 id="luo-ji-hui-gui-de-mu-biao-han-shu-zhong-zeng-da-l-1-zheng-ze-hua-hui-shi-shi-yao-jie-guo">逻辑回归的目标函数中增大L1正则化会是什么结果</h2>
<p>所有的参数w都会变成0。</p>
<h1 id="zui-da-shang">最大熵</h1>
<h2 id="strong-zui-da-shang-yuan-li-strong"><strong>最大熵原理</strong></h2>
<p>学习概率模型时,在所有可能的概率模型中,<strong>熵最大</strong>的模型是最好的模型.直观地,最大熵原理认为模型首先要满足已有的事实,即<strong>约束条件</strong>.在没有更多信息的情况下,那些不确定的部分都是&quot;<strong>等可能的</strong>&quot;.</p>
<h2 id="strong-zui-da-shang-mo-xing-strong"><strong>最大熵模型</strong></h2>
<p>给定训练数据集,可以确定联合分布P(X,Y)的经验分布\(\tilde{P}(X=x, Y=y)=\frac{v(X=x, Y=y)}{N}\)和边缘分布P(X)的经验分布\(\tilde{P}(X=x)=\frac{v(X=x)}{N}\),其中v表示频数,N表示样本容量.用<strong>特征函数\(f(x,y)\)</strong>=1描述x与y满足某一事实,可以得到特征函数关于P(X,Y)的经验分布的期望值和关于模型P(Y|X)与P(X)的经验分布的期望值,假设两者相等,就得到了<strong>约束条件</strong>\(\sum_{x, y} \tilde{P}(x) P(y | x) f(x, y)=\sum_{x, y} \tilde{P}(x, y) f(x, y)\).定义在条件概率分布P(Y|X)上的条件熵为\(H(P)=-\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)\),则<strong>条件熵最大</strong>的模型称为最大熵模型.</p>
<h2 id="strong-zui-da-shang-mo-xing-de-xue-xi-strong"><strong>最大熵模型的学习</strong></h2>
<p>就是求解最大熵模型的过程.等价于<strong>约束最优化问题</strong><br>
\[
\begin{aligned}
&amp;\max _{P_{e c}} H(P)=-\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)\\
&amp;\text { s.t. } \quad E_{p}\left(f_{i}\right)=E_{p}\left(f_{i}\right), \quad i=1,2, \cdots, n\\
     &amp;\sum_{y} P(y | x)=1
\end{aligned}
\]<br>
,将求最大值问题改为等价的求最小值问题<br>
\[
\begin{aligned}
&amp;\min _{R \in C}-H(P)=\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)\\
&amp;\text { s.t. } \quad E_{P}\left(f_{i}\right)-E_{\beta}\left(f_{i}\right)=0, \quad i=1,2, \cdots, n\\
&amp;\sum_{y} P(y | x)=1
\end{aligned}
\]</p>
<p>引入<strong>拉格朗日乘子</strong><br>
\[
\begin{aligned}
L(P, w) &amp; \equiv-H(P)+w_{0}\left(1-\sum_{y} P(y | x)\right)+\sum_{i=1}^{n} w_{i}\left(E_{p}\left(f_{i}\right)-E_{P}\left(f_{i}\right)\right) \\
&amp;=\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)+w_{0}\left(1-\sum_{y} P(y | x)\right) \\
&amp;+\sum_{i=1}^{n} w_{i}\left(\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P(y | x) f_{i}(x, y)\right)
\end{aligned}
\]<br>
将原始问题\(\min _{p \in C} \max _{w} L(P, w)\)转换为无约束最优化的<strong>对偶问题</strong>\(\max _{w} \min _{P \in \mathbf{C}} L(P, w)\).首先求解内部的<strong>极小化问题</strong>,即求\(L(P,W)\)对\(P(y|x)\)的偏导数.<br>
\[
\begin{aligned}
\frac{\partial L(P, w)}{\partial P(y | x)} &amp;=\sum_{x, y} \tilde{P}(x)(\log P(y | x)+1)-\sum_{y} w_{0}-\sum_{x, y}\left(\tilde{P}(x) \sum_{i=1}^{n} w_{i} f_{i}(x, y)\right) \\
&amp;=\sum_{x, y} \tilde{P}(x)\left(\log P(y | x)+1-w_{0}-\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
\end{aligned}
\]<br>
,并令偏导数等于0,解得\(Z_{w}(x)=\sum_{y} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)\).可以证明对偶函数等价于对数似然函数,那么对偶函数极大化等价于最大熵模型的<strong>极大似然估计</strong>\(L(w)=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)\).之后可以用最优化算法求解得到w.</p>
<h2 id="luo-ji-hui-gui-yu-zui-da-shang-mo-xing-de-gong-tong-dian">逻辑回归与最大熵模型的共同点</h2>
<p>最大熵模型与逻辑斯谛回归模型有类似的形式,它们又称为<strong>对数线性模型</strong>.模型学习就是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计.</p>
<h2 id="you-hua-suan-fa">优化算法</h2>
<p>似然函数是<strong>光滑的凸函数</strong>,因此多种最优化方法都适用.</p>
<ol>
<li><strong>改进的迭代尺度法(IIS)</strong>:假设当前的参数向量是w,如果能找到一种方法<strong>w-&gt;w+δ</strong>使对数似然函数值变大,就可以<strong>重复</strong>使用这一方法,直到找到最大值.</li>
<li>逻辑斯谛回归常应用梯度下降法,牛顿法或拟牛顿法.</li>
</ol>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>logistic</tag>
      </tags>
  </entry>
  <entry>
    <title>线性回归</title>
    <url>/2020/05/23/machine_learning/linear_regression/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/23/machine_learning/linear_regression/71505f8a532d4d5e9b259634ff1c99c3.jpeg" alt></p>
<a id="more"></a>
<h1 id="shi-yao-shi-xian-xing-hui-gui">什么是线性回归</h1>
<ul>
<li>线性：两个变量之间的关系是一次函数关系的——图象<strong>是直线</strong>，叫做线性。</li>
<li>非线性：两个变量之间的关系不是一次函数关系的——图象<strong>不是直线</strong>，叫做非线性。</li>
<li>回归：人们在测量事物的时候因为客观条件所限，求得的都是测量值，而不是事物真实的值，为了能够得到真实值，无限次的进行测量，最后通过这些测量数据计算<strong>回归到真实值</strong>，这就是回归的由来。</li>
</ul>
<p>#能够解决什么样的问题</p>
<p>对大量的观测数据进行处理，从而得到比较符合事物内部规律的数学表达式。寻找到数据与数据之间的规律所在，从而就可以模拟出结果，也就是对结果进行预测。解决的就是通过已知的数据得到未知的结果。例如：对房价的预测、判断信用评价、电影票房预估等。</p>
<h2 id="biao-da-shi-shi-shi-yao">表达式是什么</h2>
<p>\[
Y=wx+b
\]<br>
\(w\)叫做\(x\)的系数，\(b\)叫做偏置项。</p>
<h1 id="ru-he-ji-suan">如何计算</h1>
<h2 id="loss-function-mse">Loss Function–MSE</h2>
<p>\[
J=\frac{1}{2m}\sum^{i=1}_{m}(y^{'}-y)^2
\]</p>
<p>利用<strong>梯度下降法</strong>找到最小值点，也就是最小误差，最后把\(w\) 和\(b\) 给求出来。</p>
<h1 id="guo-ni-he-qian-ni-he-ru-he-jie-jue">过拟合、欠拟合如何解决</h1>
<p>使用正则化项，也就是给loss function加上一个参数项，正则化项有<strong>L1正则化、L2正则化、ElasticNet</strong>。加入这个正则化项好处：</p>
<ul>
<li>控制参数幅度，不让模型“无法无天”。</li>
<li>限制参数搜索空间</li>
<li>解决欠拟合与过拟合的问题。</li>
</ul>
<h2 id="shi-yao-shi-l-2-zheng-ze-hua-ling-hui-gui">什么是L2正则化(岭回归)</h2>
<p>方程：<br>
\[
J=J_0+\lambda\sum_{w}w^2
\]<br>
\(J_0\)表示上面的 loss function ，在loss function的基础上加入\(w\)参数的平方和乘以 \(\lambda\) ，假设：<br>
\[
L=\lambda({w_1}^2 + {w_2}^2)
\]<br>
回忆以前学过的单位元的方程：<br>
\[
x^2+y^2=1
\]<br>
和L2正则化项一样，此时的任务变成在\(L\)约束下求出\(J\)取最小值的解。求解\(J_0\)的过程可以画出等值线。同时L2正则化的函数L也可以在\(w_1w_2\)的二维平面上画出来。如下图：</p>
<p><img src="/2020/05/23/machine_learning/linear_regression/00630Defgy1g4ns9qha1nj308u089aav.jpg" alt></p>
<p>L表示为图中的黑色圆形，随着梯度下降法的不断逼近，与圆第一次产生交点，而这个交点很难出现在坐标轴上。这就说明了L2正则化不容易得到稀疏矩阵，同时为了求出损失函数的最小值，使得\(w_1\)和\(w_2\)无限接近于0，达到防止过拟合的问题。</p>
<h2 id="shi-yao-chang-jing-xia-yong-l-2-zheng-ze-hua">什么场景下用L2正则化</h2>
<p>只要数据线性相关，用LinearRegression拟合的不是很好，<strong>需要正则化</strong>，可以考虑使用岭回归(L2), 如果输入特征的维度很高,而且是稀疏线性关系的话， 岭回归就不太合适,考虑使用Lasso回归。</p>
<h2 id="shi-yao-shi-l-1-zheng-ze-hua-lasso-hui-gui">什么是L1正则化(Lasso回归)</h2>
<p>L1正则化与L2正则化的区别在于惩罚项的不同：<br>
\[
J=J_0+\lambda(|w_1|+|w_2|)
\]<br>
求解J0的过程可以画出等值线。同时L1正则化的函数也可以在w1w2的二维平面上画出来。如下图：</p>
<p><img src="/2020/05/23/machine_learning/linear_regression/00630Defgy1g4nse7rf9xj308u089gme.jpg" alt></p>
<p>惩罚项表示为图中的黑色棱形，随着梯度下降法的不断逼近，与棱形第一次产生交点，而这个交点很容易出现在坐标轴上。<strong>这就说明了L1正则化容易得到稀疏矩阵。</strong></p>
<h2 id="shi-yao-chang-jing-xia-shi-yong-l-1-zheng-ze-hua">什么场景下使用L1正则化</h2>
<p><strong>L1正则化(Lasso回归)可以使得一些特征的系数变小,甚至还使一些绝对值较小的系数直接变为0</strong>，从而增强模型的泛化能力 。对于高的特征数据,尤其是线性关系是稀疏的，就采用L1正则化(Lasso回归),或者是要在一堆特征里面找出主要的特征，那么L1正则化(Lasso回归)更是首选了。</p>
<h2 id="shi-yao-shi-elastic-net-hui-gui">什么是ElasticNet回归</h2>
<p><strong>ElasticNet综合了L1正则化项和L2正则化项</strong>，它的公式是:<br>
\[
min(\frac{1}{2m}[\sum_{i=1}^{m}({y_i}^{'}-y_i)^2+\lambda\sum_{j=1}^{n}\theta_j^2]+\lambda\sum_{j=1}^{n}|\theta|
\]</p>
<h2 id="shi-yao-chang-jing-xia-shi-yong-elastic-net-hui-gui">什么场景下使用ElasticNet回归</h2>
<p>ElasticNet在我们发现用Lasso回归太过(太多特征被稀疏为0),而岭回归也正则化的不够(回归系数衰减太慢)的时候，可以考虑使用ElasticNet回归来综合，得到比较好的结果。</p>
<h1 id="xian-xing-hui-gui-yao-qiu-yin-bian-liang-fu-cong-zheng-tai-fen-bu">线性回归要求因变量服从正态分布？</h1>
<p>我们假设线性回归的噪声服从均值为0的正态分布。 当噪声符合正态分布\(N(0,\delta^2)\)时，因变量则符合正态分布\(N(wx(i)+b,\delta^2)\)，其中预测函数\(y=wx(i)+b\)。这个结论可以由正态分布的概率密度函数得到。也就是说当噪声符合正态分布时，其因变量必然也符合正态分布。</p>
<p><strong>在用线性回归模型拟合数据之前，首先要求数据应符合或近似符合正态分布，否则得到的拟合函数不正确。</strong></p>
<h1 id="dai-ma-shi-xian">代码实现</h1>
<h2 id="shu-ju-jie-shao">数据介绍</h2>
<p>数据主要包括2014年5月至2015年5月美国King County的房屋销售价格以及房屋的基本信息。 数据分为训练数据和测试数据，分别保存在kc_train.csv和kc_test.csv两个文件中。 其中训练数据主要包括10000条记录，14个字段，主要字段说明如下：</p>
<blockquote>
<ol>
<li>第一列“销售日期”：2014年5月到2015年5月房屋出售时的日期</li>
<li>第二列“销售价格”：房屋交易价格，单位为美元，是目标预测值</li>
<li>第三列“卧室数”：房屋中的卧室数目</li>
<li>第四列“浴室数”：房屋中的浴室数目</li>
<li>第五列“房屋面积”：房屋里的生活面积</li>
<li>第六列“停车面积”：停车坪的面积</li>
<li>第七列“楼层数”：房屋的楼层数</li>
<li>第八列“房屋评分”：King County房屋评分系统对房屋的总体评分</li>
<li>第九列“建筑面积”：除了地下室之外的房屋建筑面积</li>
<li>第十列“地下室面积”：地下室的面积</li>
<li>第十一列“建筑年份”：房屋建成的年份</li>
<li>第十二列“修复年份”：房屋上次修复的年份</li>
<li>第十三列&quot;纬度&quot;：房屋所在纬度</li>
<li>第十四列“经度”：房屋所在经度。</li>
</ol>
</blockquote>
<p>测试数据主要包括3000条记录，13个字段，跟训练数据的不同是测试数据并不包括房屋销售价格，需要通过由训练数据所建立的模型以及所给的测试数据，得出测试数据相应的房屋销售价格预测值。</p>
<h2 id="bu-zou">步骤</h2>
<p><img src="/2020/05/23/machine_learning/linear_regression/687474703a2f2f7777772e7761696c69616e2e776f726b2f696d616765732f323031382f31322f31302f3132343030663535342e706e67.png" alt></p>
<blockquote>
<ol>
<li>选择合适的模型，对模型的好坏进行评估和选择。</li>
<li>对缺失的值进行补齐操作，可以使用均值的方式补齐数据，使得准确度更高。</li>
<li>数据的取值一般跟属性有关系，但世界万物的属性是很多的，有些值小，但不代表不重要，所有为了提高预测的准确度，统一数据维度进行计算，方法有特征缩放和归一法等。</li>
<li>数据处理好之后就可以进行调用模型库进行训练了。</li>
<li>使用测试数据进行目标函数预测输出，观察结果是否符合预期。或者通过画出对比函数进行结果线条对比。</li>
</ol>
</blockquote>
<h2 id="mo-xing-xuan-ze">模型选择</h2>
<p>一元线性回归<br>
\[
y:h(x)=wx+b
\]<br>
n元线性回归<br>
\[
y:h(x)=\sum^n_{i=1}w_ix^i+b=WX+b
\]<br>
这里，y表示要求的销售价格，x表示特征值。需要调用sklearn库来进行训练。</p>
<h2 id="shu-ju-chu-li">数据处理</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读取训练数据</span></span><br><span class="line">housing = pd.read_csv(<span class="string">'kc_train.csv'</span>)</span><br><span class="line"><span class="comment">#销售价格</span></span><br><span class="line">target=pd.read_csv(<span class="string">'kc_train2.csv'</span>)  </span><br><span class="line"><span class="comment">#测试数据</span></span><br><span class="line">t=pd.read_csv(<span class="string">'kc_test.csv'</span>)   </span><br><span class="line"></span><br><span class="line">train_set.info()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/23/machine_learning/linear_regression/image-20200524102417564.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">label = train_set[<span class="number">1</span>]</span><br><span class="line">train_set = train_set.drop([<span class="number">1</span>],axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#特征缩放</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">minmax_scaler=MinMaxScaler()</span><br><span class="line"><span class="comment">#进行拟合，内部参数会发生变化</span></span><br><span class="line">minmax_scaler.fit(train_set)   </span><br><span class="line">scaler_train_set=minmax_scaler.transform(train_set)</span><br><span class="line">scaler_test_set = minmax_scaler.transform(test_set)</span><br><span class="line">scaler_train_set=pd.DataFrame(scaler_train_set,columns=train_set.columns)</span><br><span class="line">scaler_test_set = pd.DataFrame(scaler_test_set,columns=train_set.columns)</span><br><span class="line">scaler_train_set.head()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/23/machine_learning/linear_regression/image-20200524103226890.png" alt></p>
<h2 id="xun-lian-mo-xing">训练模型</h2>
<p>使用sklearn库的线性回归函数进行调用训练。梯度下降法获得误差最小值。最后使用均方误差法来评价模型的好坏程度，并画图进行比较。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#选择基于梯度下降的线性回归模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">LR_reg=LinearRegression()</span><br><span class="line"><span class="comment">#进行拟合</span></span><br><span class="line">LR_reg.fit(scaler_housing,target)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#使用均方误差用于评价模型好坏</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">preds=LR_reg.predict(scaler_housing)   <span class="comment">#输入数据进行预测得到结果</span></span><br><span class="line">mse=mean_squared_error(preds,target)   <span class="comment">#使用均方误差来评价模型好坏，可以输出mse进行查看评价值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#绘图进行比较</span></span><br><span class="line">plot.figure(figsize=(<span class="number">10</span>,<span class="number">7</span>))       <span class="comment">#画布大小</span></span><br><span class="line">num=<span class="number">100</span></span><br><span class="line">x=np.arange(<span class="number">1</span>,num+<span class="number">1</span>)              <span class="comment">#取100个点进行比较</span></span><br><span class="line">plot.plot(x,target[:num],label=<span class="string">'target'</span>)      <span class="comment">#目标取值</span></span><br><span class="line">plot.plot(x,preds[:num],label=<span class="string">'preds'</span>)        <span class="comment">#预测取值</span></span><br><span class="line">plot.legend(loc=<span class="string">'upper right'</span>)  <span class="comment">#线条显示位置</span></span><br><span class="line">plot.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/23/machine_learning/linear_regression/687474703a2f2f7777772e7761696c69616e2e776f726b2f696d616765732f323031382f31322f31302f3132343039346539362e706e67.png" alt></p>
<p>这张结果对比图中就可以看出模型是否得到精确的目标函数，是否能够精确预测房价。如果想要预测test文件里的数据，那就把test文件里的数据进行读取，并且进行特征缩放，调用： <strong>LR_reg.predict(test)</strong> 就可以得到预测结果，并进行输出操作。</p>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch Bugs</title>
    <url>/2020/05/20/pytorch_bugs/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/20/pytorch_bugs/computer-bug-850x479.jpg" alt></p>
<a id="more"></a>
<h1 id="xian-qia-xiang-guan">显卡相关</h1>
<h2 id="cuda-error-invalid-device-ordinal">CUDA error: invalid device ordinal</h2>
<h3 id="bug-miao-shu">bug 描述</h3>
<p>这个问题是在有多张显卡的机器上，由于0卡被占用，而选择使用除0卡外的其他卡时出现的错误。</p>
<h3 id="yuan-yin">原因</h3>
<p>当<code>CUDA_VISIBLE_DEVICES</code>被赋值为单个值的时候(即使你使用的是1卡)，pytorch 会默认你的gpu_id为0。</p>
<p>所以如果想使用非0的单张卡，<code>CUDA_VISIBLE_DEVICES</code>应该被设置为多个值，比如<code>1,0</code>，这样主卡的id会被设置为1。从而解决问题。</p>
<h2 id="runtime-error-cu-dnn-error-cudnn-status-bad-param">RuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM</h2>
<h3 id="bug-miao-shu-1">bug 描述</h3>
<p>这个问题是因为在处理cnews 的数据的时候，序列长度太长，超出了缓存容量，解决方案是把rnn的输入tensor的float64转成了float32.</p>
<h1 id="shu-ju-lei-xing-bu-yi-zhi-xiang-guan-bugs">数据类型不一致相关bugs</h1>
<h2 id="expected-tensor-for-argument-1-indices-to-have-scalar-type-long-but-got-cuda-type-instead-while-checking-arguments-for-embedding">Expected tensor for argument #1 ‘indices’ to have scalar type Long; but got CUDAType instead (while checking arguments for embedding)</h2>
<h3 id="bug-miao-shu-2">bug 描述</h3>
<p>在使用nn.embedding ，forward的时候</p>
<h3 id="yuan-yin-1">原因</h3>
<p>由于对应的token_id是从使用<code>torch.from_numpy(np.array(token_ids))</code>转换过来的。所以token_ids 的类型从int转成了float，而nn.embedding的需要的输入是 int 或者 Long类型</p>
<h3 id="jie-jue">解决</h3>
<p><code>torch.from_numpy(np.array(token_ids))--&gt; torch.from_numpy(np.array(token_ids,**dtype=np.int**))</code></p>
<h1 id="yue-jie-wen-ti">越界问题</h1>
<h2 id="device-side-assert-triggered-unable-to-get-repr-for-xx">device-side assert triggered/unable to get repr for xx</h2>
<ol>
<li>在使用nn.Embedding()时，token_id出现负数，或者超出nn.Embedding的最大值会出现错误。</li>
<li>Bert 输入长度超过512</li>
</ol>
<h3 id="jie-jue-ban-fa">解决办法</h3>
<p>检查 token_id 值是否有越界。</p>
<h1 id="other">Other</h1>
<h2 id="bug-miao-shu-3">bug 描述</h2>
<p>torch.tensor(input_size,output_size)</p>
<p>tensor() takes 1 positional argument but 2 were given</p>
<h2 id="fen-xi">分析</h2>
<p>在Pytorch中，Tensor和tensor都用于生成新的张量。</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;  a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>])</span><br><span class="line">&gt;&gt;&gt; a=torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>区别：</p>
<p>torch.Tensor()是Python类，更明确的说，是默认张量类型torch.FloatTensor()的别名，torch.Tensor([1,2]) 会调用Tensor类的构造函数__init__，<strong>生成单精度浮点类型的张量</strong>。</p>
<p>torch.tensor()仅仅是Python的函数，<code>torch.tensor(data, dtype=None, device=None, requires_grad=False)</code>其中data可以是：list, tuple, array, scalar等类型。</p>
]]></content>
      <categories>
        <category>技术/pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>bugs</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构-屠龙14式</title>
    <url>/2020/05/15/data-structures-and-algorithms/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/15/data-structures-and-algorithms/0.png" alt></p>
<a id="more"></a>
<h1 id="shuang-zhi-zhen-mo-shi">双指针模式</h1>
<p><strong>基本思想</strong>:使用两个指针以不同的速度在数组或链表中移动。在处理<strong>循环链表</strong>或<strong>数组</strong>时，此方法非常有用。</p>
<p>通过以不同的速度移动（例如，在循环链表中），算法证明两个指针必然会相遇。 一旦两个指针都处于循环循环中，快速指针就应该捕获慢速指针。这种方法在解决有环的链表和数组时特别有用。</p>
<h2 id="kuai-man-zhi-zhen">快慢指针</h2>
<p>快慢指针一般都初始化指向链表的头结点 head，前进时快指针 fast 在前，慢指针 slow 在后，巧妙解决一些链表中的问题。</p>
<h3 id="pan-ding-lian-biao-zhong-shi-fou-han-you-huan">判定链表中是否含有环</h3>
<p>如果链表中不含环，那么这个指针最终会遇到空指针 null 表示链表到头了，可以判断该链表不含环。但是如果链表中含有环，那么这个指针就会陷入死循环，因为环形数组中没有 null 指针作为尾部节点。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">def has_cycle(head) &#123;</span><br><span class="line">    <span class="keyword">while</span> head:</span><br><span class="line">        head = head.next</span><br><span class="line">    <span class="keyword">return</span> false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>经典解法就是用两个指针，一个跑得快，一个跑得慢。如果不含有环，跑得快的那个指针最终会遇到 null，说明链表不含环；如果含有环，快指针最终会超慢指针一圈，和慢指针相遇，说明链表含有环。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">def has_cycle(head) &#123;</span><br><span class="line">    </span><br><span class="line">    fast = head</span><br><span class="line">    slow = head</span><br><span class="line">    <span class="keyword">while</span> fast <span class="keyword">and</span> fast.next:</span><br><span class="line">        fast = fast.next.next</span><br><span class="line">        slow = slow.next</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> fast == slow: </span><br><span class="line">    		<span class="keyword">return</span> true</span><br><span class="line">    <span class="keyword">return</span> false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="yi-zhi-lian-biao-zhong-han-you-huan-fan-hui-zhe-ge-huan-de-qi-shi-wei-zhi">已知链表中含有环，返回这个环的起始位置</h3>
<p><img src="/2020/05/15/data-structures-and-algorithms/fast_slow_pointer.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">def detect_cycle(head) &#123;</span><br><span class="line">    </span><br><span class="line">    fast = head</span><br><span class="line">    slow = head</span><br><span class="line">    <span class="keyword">while</span> fast <span class="keyword">and</span> fast.next:</span><br><span class="line">        fast = fast.next.next</span><br><span class="line">        slow = slow.next</span><br><span class="line">        <span class="keyword">if</span> fast == slow:</span><br><span class="line">        	<span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 上面的代码类似 hasCycle 函数</span></span><br><span class="line">    slow = head;</span><br><span class="line">    <span class="keyword">while</span> slow != fast:</span><br><span class="line">        fast = fast.next</span><br><span class="line">        slow = slow.next</span><br><span class="line">    <span class="keyword">return</span> slow</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当快慢指针相遇时，让其中任一个指针指向头节点，然后让它俩以相同速度前进，再次相遇时所在的节点位置就是环开始的位置。</p>
<p>第一次相遇时，假设慢指针 <code>slow</code> 走了 <code>k</code> 步，那么快指针 <code>fast</code> 一定走了 <code>2k</code> 步，也就是说比 <code>slow</code> 多走了 <code>k</code> 步（也就是环的长度）。</p>
<p><img src="/2020/05/15/data-structures-and-algorithms/fast_slow_pointer_2.png" alt></p>
<p>设相遇点距环的起点的距离为 <code>m</code>，那么环的起点距头结点 <code>head</code> 的距离为 <code>k - m</code>，也就是说如果从 <code>head</code> 前进 <code>k - m</code> 步就能到达环起点。如果从相遇点继续前进 <code>k - m</code> 步，也恰好到达环起点。</p>
<p><img src="/2020/05/15/data-structures-and-algorithms/fast_slow_pointer_3.png" alt></p>
<p>所以，只要把快慢指针中的任一个重新指向 <code>head</code>，然后两个指针同速前进，<code>k - m</code> 步后就会相遇，相遇之处就是环的起点了。</p>
<h3 id="xun-zhao-lian-biao-de-zhong-dian">寻找链表的中点</h3>
<p>让快指针一次前进两步，慢指针一次前进一步，当快指针到达链表尽头时，慢指针就处于链表的中间位置。当链表的长度是奇数时，slow 恰巧停在中点位置(此时fast.next=null)；如果长度是偶数，<span class="label danger">slow 最终的位置是中间偏右</span>(此时fast = null)：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> fast <span class="keyword">and</span> fast.next:</span><br><span class="line">    fast = fast.next.next</span><br><span class="line">    slow = slow.next</span><br><span class="line"></span><br><span class="line"><span class="comment"># slow 就在中间位置</span></span><br><span class="line"><span class="keyword">return</span> slow</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/15/data-structures-and-algorithms/image-20200630183112564.png" alt="image-20200630183112564"></p>
<blockquote>
<p>寻找链表中点的一个重要作用是对链表进行归并排序。</p>
<p>归并排序：求中点索引递归地把数组二分，最后合并两个有序数组。对于链表，合并两个有序链表是很简单的，难点就在于二分。</p>
</blockquote>
<h3 id="xun-zhao-lian-biao-de-dao-shu-di-k-ge-yuan-su">寻找链表的倒数第 k 个元素</h3>
<p>使用快慢指针，让快指针先走 k 步，然后快慢指针开始同速前进。这样当快指针走到链表末尾 null 时，慢指针所在的位置就是倒数第 k 个链表节点:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_k</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">  slow = head</span><br><span class="line">    fast = head</span><br><span class="line">    <span class="keyword">while</span> k &gt; <span class="number">0</span>:</span><br><span class="line">        fast = fast.next</span><br><span class="line">        k -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> fast :</span><br><span class="line">        slow = slow.next;</span><br><span class="line">        fast = fast.next;</span><br><span class="line">  <span class="keyword">return</span> slow;</span><br></pre></td></tr></table></figure>
<h3 id="ying-yong-chang-jing">应用场景</h3>
<ul>
<li>链表或数组循环</li>
<li>用于找中间元素</li>
<li>需要知道某个元素的位置或链表的总长度</li>
</ul>
<h3 id="ti-mu">题目</h3>
<ul class="contains-task-list">
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox0" checked="true" disabled="true"><label for="checkbox0"> <a href="https://leetcode-cn.com/problems/linked-list-cycle/" target="_blank" rel="noopener">环形链表（LEETCODE）</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox1" checked="true" disabled="true"><label for="checkbox1"> <a href="https://leetcode-cn.com/problems/intersection-of-two-linked-lists/" target="_blank" rel="noopener">相交链表（LEETCODE）</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox2" checked="true" disabled="true"><label for="checkbox2"> <a href="https://leetcode-cn.com/problems/linked-list-cycle-ii/" target="_blank" rel="noopener">环形链表入口节点（LEETCODE）</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox3" checked="true" disabled="true"><label for="checkbox3"> <a href="https://leetcode-cn.com/problems/lian-biao-zhong-dao-shu-di-kge-jie-dian-lcof/" target="_blank" rel="noopener">链表中倒数第k个节点</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox4" disabled="true"><label for="checkbox4"> <a href="https://leetcode-cn.com/problems/palindrome-linked-list/" target="_blank" rel="noopener">回文链表</a></label></div></p>
</li>
</ul>
<h2 id="zuo-you-zhi-zhen">左右指针</h2>
<p>左右指针在数组中实际是指两个索引值，一般初始化为 <code>left = 0, right = nums.length - 1</code>。</p>
<h3 id="er-fen-cha-zhao">二分查找</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span><span class="params">(nums, target)</span>:</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = len(nums) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        mid = left + (right - left) / <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] &gt; target:</span><br><span class="line">            right = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] == target:</span><br><span class="line">            <span class="comment"># 直接返回（适合列表中不存在重复元素）</span></span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<h3 id="liang-shu-he">两数和</h3>
<p><img src="/2020/05/15/data-structures-and-algorithms/2.png" alt></p>
<blockquote>
<p>只要数组有序，就应该想到双指针技巧。</p>
</blockquote>
<h3 id="fan-zhuan-shu-zu">反转数组</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">def reverse(nums) &#123;</span><br><span class="line">    left = <span class="number">0</span>;</span><br><span class="line">    right = len(nums) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        <span class="comment"># swap(nums[left], nums[right])</span></span><br><span class="line">        temp = nums[left]</span><br><span class="line">        nums[left] = nums[right]</span><br><span class="line">        nums[right] = temp</span><br><span class="line">        left += <span class="number">1</span></span><br><span class="line">    	right -= <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="hua-dong-chuang-kou-suan-fa">滑动窗口算法</h3>
<p>滑动窗口模式用于对给定数组或链表的特定窗口大小执行所需操作.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">slide_window</span><span class="params">(s,t)</span>:</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 窗口: 字典/list/set</span></span><br><span class="line">    window = defaultdict(int)</span><br><span class="line">    <span class="comment"># gold: 字典/list</span></span><br><span class="line">    need = defaultdict(int)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化need</span></span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> t:</span><br><span class="line">        <span class="comment"># need do something</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> right &lt; len(s):</span><br><span class="line">        <span class="comment"># 要添加的字符</span></span><br><span class="line">        c = s[right]</span><br><span class="line">        <span class="comment"># right 指针右移，扩大窗口</span></span><br><span class="line">        right += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对窗口内的数据进行更新操作 1</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判断左侧窗口是否要收缩</span></span><br><span class="line">        <span class="keyword">while</span> (window needs shrink):</span><br><span class="line">            <span class="comment"># d 是将移出窗口的字符</span></span><br><span class="line">            d = s[left]</span><br><span class="line">            <span class="comment"># 左移窗口</span></span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 进行窗口内数据的一系列更新操作 2（与1处保持对称）</span></span><br><span class="line">            <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<h3 id="ying-yong-chang-jing-1">应用场景</h3>
<ul>
<li>问题为排序数组或链表，并且需要满足某些约束的一组元素问题</li>
<li>数组中的元素集是一对，三元组，甚至是子数组</li>
</ul>
<h3 id="ti-mu-1">题目</h3>
<ul class="contains-task-list">
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox5" disabled="true"><label for="checkbox5"> [N-sum问题(leetcode)]</label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox6" checked="true" disabled="true"><label for="checkbox6"> <a href="https://leetcode-cn.com/problems/find-closest-lcci/" target="_blank" rel="noopener">单词距离</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox7" checked="true" disabled="true"><label for="checkbox7"> <a href="https://leetcode-cn.com/problems/reverse-string/" target="_blank" rel="noopener">反转字符串</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox8" checked="true" disabled="true"><label for="checkbox8"> <a href="https://leetcode-cn.com/problems/merge-sorted-array/" target="_blank" rel="noopener">合并两个有序数组</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox9" checked="true" disabled="true"><label for="checkbox9"> <a href="https://leetcode-cn.com/problems/valid-palindrome/" target="_blank" rel="noopener">验证回文串</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox10" disabled="true"><label for="checkbox10"> <a href="https://leetcode-cn.com/problems/binary-subarrays-with-sum/" target="_blank" rel="noopener">和相同的二元子数组</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox11" checked="true" disabled="true"><label for="checkbox11"> <a href="https://leetcode-cn.com/problems/longest-substring-without-repeating-characters/" target="_blank" rel="noopener">无重复字符的最长子串(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox12" disabled="true"><label for="checkbox12"> <a href="https://leetcode-cn.com/problems/trapping-rain-water/" target="_blank" rel="noopener">接雨水(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox13" disabled="true"><label for="checkbox13"> <a href="https://leetcode-cn.com/problems/minimum-size-subarray-sum/" target="_blank" rel="noopener">长度最小的子数组(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox14" disabled="true"><label for="checkbox14"> [输出一个排好序的数组的平方数组（简单）]</label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox15" disabled="true"><label for="checkbox15"> [3-Sum（中等）]</label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox16" disabled="true"><label for="checkbox16"> [比较两个字符是否相等，字符中包括得有退格键（中等）]</label></div></p>
</li>
</ul>
<h1 id="hua-dong-chuang-kou">滑动窗口</h1>
<p>滑动窗口模式用于对给定数组或链表的特定窗口大小执行所需操作，例如查找包含所有1的最长子序列。滑动窗口从第一个元素开始，每次向右移动一个元素并根据要解决的问题调整窗口的长度。在某些情况下，窗口的大小保持不变，而在其他情况下，大小会增大或缩小。</p>
<p><img src="/2020/05/15/data-structures-and-algorithms/1.png" alt></p>
<h2 id="ying-yong-chang-jing-2">应用场景</h2>
<ul>
<li>问题输入是线性数据结构，如链表、数组或字符串</li>
<li>题目要求查找最长/最短的子字符串、子数组或所需的值</li>
</ul>
<h2 id="ti-mu-2">题目</h2>
<ul class="contains-task-list">
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox17" checked="true" disabled="true"><label for="checkbox17"> <a href="https://www.nowcoder.com/practice/1624bc35a45c42c0bc17d17fa0cba788?tpId=13&amp;tqId=11217&amp;tPage=4&amp;rp=4&amp;ru=/ta/coding-interviews&amp;qru=/ta/coding-interviews/question-ranking" target="_blank" rel="noopener">滑动窗口的最大值(剑指offer)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox18" checked="true" disabled="true"><label for="checkbox18"> <a href="https://leetcode-cn.com/problems/sliding-window-median/" target="_blank" rel="noopener">滑动窗口中位数(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox19" checked="true" disabled="true"><label for="checkbox19"> <a href="https://leetcode-cn.com/problems/minimum-window-substring/" target="_blank" rel="noopener">最小覆盖子串(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox20" checked="true" disabled="true"><label for="checkbox20"> <a href="https://leetcode-cn.com/problems/find-all-anagrams-in-a-string/" target="_blank" rel="noopener">找到字符串中所有字母异位词(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox21" checked="true" disabled="true"><label for="checkbox21"> <a href="https://leetcode-cn.com/problems/longest-substring-without-repeating-characters/" target="_blank" rel="noopener">无重复字符的最长子串(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox22" checked="true" disabled="true"><label for="checkbox22"> <a href="https://leetcode-cn.com/problems/permutation-in-string/" target="_blank" rel="noopener">字符串的排列(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox23" checked="true" disabled="true"><label for="checkbox23"> <a href="https://leetcode-cn.com/problems/partition-labels/" target="_blank" rel="noopener">划分字母区间(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox24" checked="true" disabled="true"><label for="checkbox24"> <a href="https://leetcode-cn.com/problems/fruit-into-baskets/" target="_blank" rel="noopener">水果成篮(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox25" disabled="true"><label for="checkbox25"> <a href="https://leetcode-cn.com/problems/subarrays-with-k-different-integers/" target="_blank" rel="noopener">k个不同整数的子数组</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox26" disabled="true"><label for="checkbox26"> [窗口大小为K的最大子数组和]</label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox27" disabled="true"><label for="checkbox27"> [拥有K个不同的字母的最长子串]</label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox28" disabled="true"><label for="checkbox28"> [字符串的同字母异序词]</label></div></p>
</li>
</ul>
<h2 id="fen-xi-yu-zong-jie">分析与总结</h2>
<ol>
<li>考虑左右边界值的变化。</li>
<li>关注的元素是否有效（有效是指：元素的index是否在窗口范围内）</li>
</ol>
<figure class="highlight python"><figcaption><span>滑动窗口模板</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">slide_window</span><span class="params">(s,t)</span>:</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 窗口: 字典/list/set</span></span><br><span class="line">    window = defaultdict(int)</span><br><span class="line">    <span class="comment"># gold: 字典/list</span></span><br><span class="line">    need = defaultdict(int)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化need</span></span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> t:</span><br><span class="line">        <span class="comment"># need do something</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> right &lt; len(s):</span><br><span class="line">        <span class="comment"># 要添加的字符</span></span><br><span class="line">        c = s[right]</span><br><span class="line">        <span class="comment"># right 指针右移，扩大窗口</span></span><br><span class="line">        right += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对窗口内的数据进行更新操作 1</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判断左侧窗口是否要收缩</span></span><br><span class="line">        <span class="keyword">while</span> (window needs shrink):</span><br><span class="line">            <span class="comment"># d 是将移出窗口的字符</span></span><br><span class="line">            d = s[left]</span><br><span class="line">            <span class="comment"># 左移窗口</span></span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 进行窗口内数据的一系列更新操作 2（与1处保持对称）</span></span><br><span class="line">            <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<h1 id="qu-jian-he-bing">区间合并</h1>
<p>合并间隔模式是<strong>处理重叠间隔</strong>的有效技术。 在涉及间隔的许多问题中，你可以需要找到重叠间隔或合并间隔（如果它们重叠）。给定两个间隔aaa和bbb，可能存在6中不同的间隔交互情况：<br>
<img src="/2020/05/15/data-structures-and-algorithms/4.png" alt></p>
<h2 id="ying-yong-chang-jing-3">应用场景</h2>
<ul>
<li>要求生成仅具有互斥间隔的列表</li>
<li>出现“overlapping intervals”一词</li>
</ul>
<h2 id="ti-mu-3">题目</h2>
<ol>
<li><a href="https://leetcode-cn.com/problems/merge-intervals/" target="_blank" rel="noopener">合并区间（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/meeting-rooms-ii/" target="_blank" rel="noopener">会议室（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/range-module/" target="_blank" rel="noopener">Range模块（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/interval-list-intersections/" target="_blank" rel="noopener">区间列表的交集（LEETCODE）</a></li>
<li>区间交集（中等）</li>
<li>最大化CPU负载（困难）</li>
</ol>
<h1 id="xun-huan-pai-xu">循环排序</h1>
<p>循环排序模式描述了一种处理涉及包含给定范围内的数字的数组问题的有趣方法。其一次遍历数组一个数字，如果正在迭代的当前数字不是正确的索引，则将其与正确索引处的数字交换。</p>
<p><img src="/2020/05/15/data-structures-and-algorithms/5.png" alt></p>
<h2 id="ying-yong-chang-jing-4">应用场景</h2>
<ul>
<li>涉及给定范围内的数字的排序数组</li>
<li>要求在已排序/旋转的数组中找到缺失/重复/最小的数字</li>
</ul>
<h2 id="ti-mu-4">题目</h2>
<ol>
<li><a href="https://leetcode-cn.com/problems/missing-number/" target="_blank" rel="noopener">缺失数字（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/find-the-duplicate-number/" target="_blank" rel="noopener">寻找重复数（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/first-missing-positive/" target="_blank" rel="noopener">缺失的第一个正数（LEETCODE）</a></li>
<li>需要数组中没出现的数字 （简单）</li>
<li>寻找最小的没出现的正整数 （中等）</li>
</ol>
<h1 id="yuan-di-lian-biao-fan-zhuan">原地链表翻转</h1>
<p>在许多问题中，可能会要求我们反转链表的一组节点之间的链接。 通常，约束就是需要就地执行此操作，即使用现有节点对象而不使用额外内存。 这是上述模式有用的地方。</p>
<p>此模式一次反转一个节点，从一个指向链表头部的变量（当前）开始，一个变量（上一个）将指向已处理的上一个节点。 以锁步方式，将通过将当前节点指向前一个节点，然后再转到下一个节点来反转当前节点。 此外，更新变量“previous”以始终指向您已处理的上一个节点。<br>
<img src="/2020/05/15/data-structures-and-algorithms/6.png" alt></p>
<h2 id="ying-yong-chang-jing-5">应用场景</h2>
<ul>
<li>就地反转链表</li>
</ul>
<h2 id="ti-mu-5">题目</h2>
<ol>
<li><a href="https://leetcode-cn.com/problems/reverse-linked-list/" target="_blank" rel="noopener">反转链表（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/reverse-linked-list-ii/" target="_blank" rel="noopener">反转链表II（LEETCODE）</a></li>
</ol>
<h1 id="shu">树</h1>
<h2 id="shu-de-kuan-du-you-xian-sou-suo-tree-bfs">树的宽度优先搜索（Tree BFS）</h2>
<p>该模式基于广度优先搜索（BFS）技术来遍历树，并<span class="label danger">使用队列</span>在跳到下一层之前记录下该层的所有节点。使用这种方法可以有效地解决涉及以逐级顺序遍历树的任何问题。Tree BFS模式的基本思想是将根节点push到队列然后不断迭代直到队列为空。对于每次迭代，删除队列头部的节点并“访问”该节点。从队列中删除每个节点后，我们还将其所有子节点push进队列。</p>
<h3 id="ying-yong-chang-jing-6">应用场景</h3>
<ul>
<li>涉及到层序遍历树</li>
</ul>
<h3 id="ti-mu-6">题目</h3>
<ul class="contains-task-list">
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox29" checked="true" disabled="true"><label for="checkbox29"> <a href="https://leetcode-cn.com/problems/n-ary-tree-level-order-traversal/" target="_blank" rel="noopener">N叉树的层序遍历(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox30" checked="true" disabled="true"><label for="checkbox30"> <a href="https://leetcode-cn.com/problems/binary-tree-level-order-traversal/" target="_blank" rel="noopener">二叉树的层序遍历(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox31" checked="true" disabled="true"><label for="checkbox31"> <a href="https://leetcode-cn.com/problems/binary-tree-zigzag-level-order-traversal/" target="_blank" rel="noopener">二叉树的锯齿形层次遍历(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox32" checked="true" disabled="true"><label for="checkbox32"> <a href="https://leetcode-cn.com/problems/cong-shang-dao-xia-da-yin-er-cha-shu-lcof/" target="_blank" rel="noopener">从上到下打印二叉树(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox33" checked="true" disabled="true"><label for="checkbox33"> <a href="https://leetcode-cn.com/problems/cong-shang-dao-xia-da-yin-er-cha-shu-ii-lcof/" target="_blank" rel="noopener">从上到下打印二叉树 II(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox34" checked="true" disabled="true"><label for="checkbox34"> <a href="https://leetcode-cn.com/problems/cong-shang-dao-xia-da-yin-er-cha-shu-iii-lcof/" target="_blank" rel="noopener">从上到下打印二叉树 III(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox35" checked="true" disabled="true"><label for="checkbox35"> <a href="https://leetcode-cn.com/problems/find-largest-value-in-each-tree-row/" target="_blank" rel="noopener">在每个树行中找最大值(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox36" checked="true" disabled="true"><label for="checkbox36"> <a href="https://leetcode-cn.com/problems/list-of-depth-lcci/" target="_blank" rel="noopener">特定深度节点链表(leetcode)</a></label></div></p>
</li>
</ul>
<h3 id="dai-ma-mo-ban">代码模板</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">levelOrder</span><span class="params">(root)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :type root: TreeNode</span></span><br><span class="line"><span class="string">    :rtype: List[List[int]]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    q = []</span><br><span class="line">    res = []</span><br><span class="line">    q.append(root)</span><br><span class="line">    <span class="keyword">while</span> q:</span><br><span class="line">        temp_q = []</span><br><span class="line">        temp_res = []</span><br><span class="line">        <span class="keyword">while</span> q:</span><br><span class="line">            node = q.pop(<span class="number">0</span>)</span><br><span class="line">            temp_res.append(node.val)</span><br><span class="line">            <span class="keyword">if</span> node.left:</span><br><span class="line">                temp_q.append(node.left)</span><br><span class="line">            <span class="keyword">if</span> node.right:</span><br><span class="line">                temp_q.append(node.right)</span><br><span class="line">        q = temp_q</span><br><span class="line">        res.append(temp_res)</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"><span class="comment"># 如果需要层级信息，加上level</span></span><br></pre></td></tr></table></figure>
<h2 id="shu-de-shen-du-you-xian-sou-suo-tree-dfs">树的深度优先搜索（Tree DFS）</h2>
<p>基于深度优先搜索(DFS)技术来实现树的遍历。可以用递归（如果用迭代方式的话，需要用栈）来记录遍历过程中访问过的父节点。</p>
<p>该模式的运行方式是从根节点开始，如果该节点不是叶子节点，我们需要干三件事：</p>
<ol>
<li>需要区别我们是先处理根节点（pre-order，前序），处理孩子节点之间处理根节点（in-order，中序），还是处理完所有孩子再处理根节点（post-order，后序）。</li>
<li>递归处理当前节点的左右孩子。</li>
</ol>
<h3 id="ying-yong-chang-jing-7">应用场景</h3>
<ul>
<li>涉及树的先序、中序或者后续遍历问题</li>
<li>如果问题涉及搜索节点离叶子更近的目标</li>
</ul>
<h3 id="ti-mu-7">题目</h3>
<ul class="contains-task-list">
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox37" checked="true" disabled="true"><label for="checkbox37"> <a href="https://leetcode-cn.com/problems/sum-root-to-leaf-numbers/" target="_blank" rel="noopener">求根到叶子节点数字之和（leetcode）</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox38" checked="true" disabled="true"><label for="checkbox38"> <a href="https://leetcode-cn.com/problems/path-sum/" target="_blank" rel="noopener">路径总和(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox39" checked="true" disabled="true"><label for="checkbox39"> <a href="https://leetcode-cn.com/problems/path-sum-ii/" target="_blank" rel="noopener">路径总和II(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox40" checked="true" disabled="true"><label for="checkbox40"> <a href="https://leetcode-cn.com/problems/path-sum-iii/" target="_blank" rel="noopener">路径总和III(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox41" checked="true" disabled="true"><label for="checkbox41"> <a href="https://leetcode-cn.com/problems/maximum-depth-of-binary-tree/" target="_blank" rel="noopener">二叉树的最大深度（leetcode）</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox42" checked="true" disabled="true"><label for="checkbox42"> <a href="https://leetcode-cn.com/problems/minimum-depth-of-binary-tree/" target="_blank" rel="noopener">二叉树的最小深度(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox43" checked="true" disabled="true"><label for="checkbox43"> <a href="https://leetcode-cn.com/problems/leaf-similar-trees/" target="_blank" rel="noopener">叶子相似的树(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox44" checked="true" disabled="true"><label for="checkbox44"> <a href="https://leetcode-cn.com/problems/maximum-depth-of-n-ary-tree/" target="_blank" rel="noopener">N叉树的最大深度(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox45" disabled="true"><label for="checkbox45"> <a href="https://leetcode-cn.com/problems/construct-binary-tree-from-inorder-and-postorder-traversal/" target="_blank" rel="noopener">从中序与后序遍历序列构造二叉树（leetcode）</a></label></div></p>
</li>
</ul>
<h3 id="dai-ma">代码</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">res = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self,node,temp_res)</span>:</span></span><br><span class="line">    <span class="comment"># 出口</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 对当前节点的操作</span></span><br><span class="line">  <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 到达叶子节点</span></span><br><span class="line">    <span class="keyword">if</span> node.left <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> node.right <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        res.append(temp_res)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 左节点不为空</span></span><br><span class="line">    <span class="keyword">if</span> node.left:</span><br><span class="line">        dfs(node.left,copy.deepcopy(temp_res))</span><br><span class="line">    <span class="comment"># 有节点不为空</span></span><br><span class="line">    <span class="keyword">if</span> node.right:</span><br><span class="line">        dfs(node.right,copy.deepcopy(temp_res))</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="comment"># 双重递归        </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.status = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pathSum</span><span class="params">(self, root, target)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :type target: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> len(self.status)</span><br><span class="line">        self.dfs(root,target)</span><br><span class="line">        <span class="keyword">if</span> root.left:</span><br><span class="line">            self.pathSum(root.left,target)</span><br><span class="line">        <span class="keyword">if</span> root.right:</span><br><span class="line">            self.pathSum(root.right,target)</span><br><span class="line">        <span class="keyword">return</span> len(self.status)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self, node,target)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        target -= node.val</span><br><span class="line">        <span class="keyword">if</span> target == <span class="number">0</span>:</span><br><span class="line">            self.status.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> node.left:</span><br><span class="line">            self.dfs(node.left, target)</span><br><span class="line">        <span class="keyword">if</span> node.right:</span><br><span class="line">            self.dfs(node.right, target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 非递归解法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxDepth</span><span class="params">(self, root)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :type root: TreeNode</span></span><br><span class="line"><span class="string">    :rtype: int</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># return self.dfs(root,0)</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    stack = []</span><br><span class="line">    max_depth = <span class="number">0</span> <span class="comment"># 记录最优值</span></span><br><span class="line">    stack.append((root,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">while</span> stack:</span><br><span class="line">        node,depth = stack.pop()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> node.right:</span><br><span class="line">            stack.append((node.right,depth+<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">if</span> node.left:</span><br><span class="line">            stack.append((node.left,depth+<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 叶子</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        max_depth = max(depth,max_depth)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> max_depth</span><br></pre></td></tr></table></figure>
<h1 id="shuang-dui-mo-shi">双堆模式</h1>
<p>在许多问题中，给出了一系列元素，需要我们将其分成两部分。 为了解决这个问题，我们想要知道一个部分中的最小元素和另一个部分中的最大元素。 这种模式是解决此类问题的有效方法。</p>
<p>这种模式使用两个堆：找到最小元素的Min Heap和找到最大元素的Max Heap。 该模式的工作原理是将前半部分的数字存储在Max Heap中，这是因为我们希望在上半部分找到最大的数字。 然后将数字的后半部分存储在Min Heap中，因为我们希望在后半部分找到最小的数字。 在任何时候，可以从两个堆的顶部元素计算当前数字列表的中值。</p>
<h2 id="ying-yong-chang-jing-8">应用场景</h2>
<ul>
<li>优先队列，调度等情况</li>
<li>找到集合中的最小/最大/中值元素</li>
<li>有时，在以二叉树数据结构为特征的问题中很有用</li>
</ul>
<h2 id="ti-mu-8">题目</h2>
<ol>
<li><a href="https://leetcode-cn.com/problems/find-median-from-data-stream/" target="_blank" rel="noopener">数据流的中位数（LEETCODE）</a></li>
<li><a href="https://www.nowcoder.com/practice/1624bc35a45c42c0bc17d17fa0cba788?tpId=13&amp;tqId=11217&amp;tPage=4&amp;rp=4&amp;ru=/ta/coding-interviews&amp;qru=/ta/coding-interviews/question-ranking" target="_blank" rel="noopener">滑动窗口的最大值（剑指offer）</a></li>
</ol>
<h1 id="zi-ji-wen-ti-mo-shi">子集问题模式</h1>
<p>大量的编程面试问题涉及处理一组给定元素的排列和组合。 Subsets模式描述了一种有效的广度优先搜索（BFS）方法来处理所有这些问题。</p>
<p>例如给定一个数组 [1, 5, 3]，</p>
<ol>
<li>首先初始化一个空数组： [[ ]]</li>
<li>将第一个数字(1)添加到所有现有子集，以创建新的子集: [[], [1]]</li>
<li>继续添加[[], [1], [5], [1, 5]]</li>
<li>[[], [1], [5], [1, 5], [3], [1, 3], [5, 3], [1, 5, 3]]</li>
</ol>
<p><img src="/2020/05/15/data-structures-and-algorithms/10.png" alt></p>
<h2 id="ying-yong-chang-jing-9">应用场景</h2>
<ul>
<li>需要找到给定集合的组合或排列的问题</li>
</ul>
<h2 id="ti-mu-9">题目</h2>
<ol>
<li><a href="https://leetcode-cn.com/problems/subsets/" target="_blank" rel="noopener">子集系列（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/letter-case-permutation/" target="_blank" rel="noopener">字母大小写全排列（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/generalized-abbreviation/" target="_blank" rel="noopener">列举单词的全部缩写（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/word-subsets/" target="_blank" rel="noopener">单词子集（LEETCODE）</a></li>
</ol>
<h1 id="er-fen-bian-chong">二分变种</h1>
<p>无论何时给定排序数组，链表或矩阵，并要求查找某个元素，你可以使用的最佳算法是二分搜索。此模式描述了处理涉及二分搜索的所有问题的有效方法。这种模式的步骤是这样的：</p>
<ol>
<li>首先，算出左右端点的中点。最简单的方式是这样的：middle = (start + end) / 2。但这种计算方式有不小的概率会出现整数越界。因此一般都推荐另外这种写法：middle = start + (end — start) / 2</li>
<li>如果要找的目标改好和中点所在的数值相等，我们返回中点的下标就行</li>
<li>如果目标不等的话：我们就有两种移动方式了</li>
<li>如果目标比中点在的值小（key &lt; arr[middle]）：将下一步搜索空间放到左边（end = middle - 1）</li>
<li>如果比中点的值大，则继续在右边搜索，丢弃左边：left = middle + 1</li>
</ol>
<p><img src="/2020/05/15/data-structures-and-algorithms/11.png" alt></p>
<h2 id="ti-mu-10">题目</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox46" checked="true" disabled="true"><label for="checkbox46"> <a href="https://leetcode-cn.com/problems/binary-search/" target="_blank" rel="noopener">二分查找(leetcode)</a></label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox47" checked="true" disabled="true"><label for="checkbox47"> <a href="https://leetcode-cn.com/problems/search-in-rotated-sorted-array/" target="_blank" rel="noopener">搜索旋转排序数组（leetcode）</a></label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox48" checked="true" disabled="true"><label for="checkbox48"> <a href="https://leetcode-cn.com/problems/search-in-rotated-sorted-array-ii" target="_blank" rel="noopener">搜索旋转排序数组 II(leetcode)</a></label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox49" checked="true" disabled="true"><label for="checkbox49"> <a href="https://leetcode-cn.com/problems/find-first-and-last-position-of-element-in-sorted-array/" target="_blank" rel="noopener">在排序数组中查找元素的第一个和最后一个位置(leetcode)</a></label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox50" disabled="true"><label for="checkbox50"> <a href="https://leetcode-cn.com/problems/median-of-two-sorted-arrays/" target="_blank" rel="noopener">寻找两个有序数组的中位数（leetcode）</a></label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox51" checked="true" disabled="true"><label for="checkbox51"> <a href="https://leetcode-cn.com/problems/find-minimum-in-rotated-sorted-array/" target="_blank" rel="noopener">寻找旋转排序数组中的最小值（leetcode）</a></label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox52" disabled="true"><label for="checkbox52"> <a href="https://leetcode-cn.com/problems/find-minimum-in-rotated-sorted-array-ii" target="_blank" rel="noopener">寻找旋转排序数组中的最小值 II(leetcode)</a></label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox53" disabled="true"><label for="checkbox53"> 顺序未知的二分（可能翻转过了，简单）</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox54" disabled="true"><label for="checkbox54"> 无界排序数组的二分（中等）</label></div></li>
</ul>
<h2 id="dai-ma-mo-ban-1">代码模板</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span><span class="params">(nums, target)</span>:</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = len(nums) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        mid = left + (right - left) / <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] &gt; target:</span><br><span class="line">            right = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] == target:</span><br><span class="line">            <span class="comment"># 直接返回（适合列表中不存在重复元素）</span></span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search_left</span><span class="params">(nums, target)</span>:</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = len(nums) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        mid = left + (right - left) / <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] &gt; target:</span><br><span class="line">            right = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] == target:</span><br><span class="line">            <span class="comment"># 向左边界搜索,用right</span></span><br><span class="line">            right = mid <span class="number">-1</span></span><br><span class="line">    <span class="comment"># 检查是否越界</span></span><br><span class="line">    <span class="keyword">if</span> left &gt;= len(nums) <span class="keyword">or</span> <span class="keyword">not</span> nums[left] == target:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">return</span> left</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search_right</span><span class="params">(nums,target)</span>:</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = len(nums) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        mid = left + (right - left) / <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] &gt; target:</span><br><span class="line">            right = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] == target:</span><br><span class="line">            <span class="comment"># 向右边界搜索，用left</span></span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">    <span class="comment"># 检查是否越界</span></span><br><span class="line">    <span class="keyword">if</span> right &lt; <span class="number">0</span> <span class="keyword">or</span> <span class="keyword">not</span> nums[right] == target:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">return</span> right</span><br></pre></td></tr></table></figure>
<h1 id="top-k">Top K</h1>
<p>任何要求我们在给定集合中找到最大/最小/频繁“K”元素的问题都属于这种模式。</p>
<p>跟踪<code>K</code>元素的最佳数据结构是Heap。这种模式将利用Heap来解决从一组给定元素一次处理<code>K</code>元素的多个问题。大致思路是这样的：</p>
<ul>
<li>根据问题将<code>K</code>元素插入到最小堆或最大堆中</li>
<li>迭代剩余的数字，如果找到一个比堆中的数字大的数字，则删除该数字并插入较大的数字</li>
</ul>
<p><img src="/2020/05/15/data-structures-and-algorithms/12.png" alt></p>
<h2 id="ying-yong-chang-jing-10">应用场景</h2>
<ul>
<li>要求找到给定集合的最大/最小/频繁<code>K</code>元素；</li>
<li>要求对数组进行排序以找到确切的元素</li>
</ul>
<h2 id="ju-ge-li-zi">举个栗子</h2>
<ol>
<li><a href="https://leetcode-cn.com/problems/top-k-frequent-elements/" target="_blank" rel="noopener">前K个高频元素（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/top-k-frequent-words/" target="_blank" rel="noopener">前K个高频单词（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/permutation-sequence/" target="_blank" rel="noopener">第k个排列（LEETCODE）</a></li>
<li>前K大的数（简单）</li>
<li>前K个最常出现的数字（中等）</li>
</ol>
<h2 id="fen-xi-zong-jie">分析总结</h2>
<ol>
<li>当涉及到top k问题的时候，可以考虑一下<code>堆</code>这种数据结构<code>python: import heapq</code>，python构建最大堆，可以通过给list中的元素加 <code>负号</code> 的方式实现。</li>
</ol>
<h1 id="k-lu-gui-bing">K路归并</h1>
<p>K路归并能解决那些涉及到多组排好序的数组的问题。</p>
<p>该模式是这样的运行的：</p>
<ol>
<li>把每个数组中的第一个元素都加入最小堆中</li>
<li>取出堆顶元素（全局最小），将该元素放入排好序的结果集合里面</li>
<li>将刚取出的元素所在的数组里面的下一个元素加入堆</li>
<li>重复步骤2，3，直到处理完所有数字</li>
</ol>
<p><img src="/2020/05/15/data-structures-and-algorithms/13.png" alt></p>
<h2 id="ying-yong-chang-jing-11">应用场景</h2>
<ul>
<li>适用于排序的数组，列表或矩阵</li>
<li>问题要求合并排序列表，在排序列表中查找最小元素等</li>
</ul>
<h2 id="ti-mu-11">题目</h2>
<ol>
<li><a href="https://leetcode-cn.com/problems/merge-two-sorted-lists/" target="_blank" rel="noopener">合并两个有序链表（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/merge-k-sorted-lists/" target="_blank" rel="noopener">合并K个排序链表（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/ugly-number-ii/" target="_blank" rel="noopener">丑数系列（LEETCODE）</a></li>
<li>合并K个排好序的链表（中等）</li>
<li>K对数和最大（困难）</li>
</ol>
<h1 id="tuo-bu-pai-xu-mo-shi">拓扑排序模式</h1>
<p>拓扑排序用于查找彼此依赖的元素的线性排序。例如，如果事件“B”依赖于事件“A”，则“A”在拓扑排序中位于“B”之前。流程大概是这样的：</p>
<ol>
<li>初始化
<ul>
<li>借助于HashMap将图保存成邻接表形式。</li>
<li>找到所有的起点，用HashMap来帮助记录每个节点的入度</li>
</ul>
</li>
<li>创建图，找到每个节点的入度
<ul>
<li>利用输入，把图建好，然后遍历一下图，将入度信息记录在HashMap中</li>
</ul>
</li>
<li>找所有的起点
<ul>
<li>所有入度为0的节点，都是有效的起点，而且我们讲他们都加入到一个队列中</li>
</ul>
</li>
<li>排序
<ul>
<li>对每个起点，执行以下步骤
<ul>
<li>把它加到结果的顺序中</li>
<li>将其在图中的孩子节点取到</li>
<li>将其孩子的入度减少1</li>
<li>如果孩子的入度变为0，则改孩子节点成为起点，将其加入队列中</li>
</ul>
</li>
<li>重复上面4步过程，直到起点队列为空。</li>
</ul>
</li>
</ol>
<p><img src="/2020/05/15/data-structures-and-algorithms/14.png" alt></p>
<h2 id="ying-yong-chang-jing-12">应用场景</h2>
<ul>
<li>需要处理没有定向循环的图</li>
<li>要求按排序顺序更新所有对象</li>
<li>如果有一组遵循特定顺序的对象</li>
</ul>
<h2 id="ti-mu-12">题目</h2>
<ol>
<li><a href="https://leetcode-cn.com/problems/course-schedule/" target="_blank" rel="noopener">课程表系列（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/longest-increasing-path-in-a-matrix/" target="_blank" rel="noopener">矩阵中的最长递增路径（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/sequence-reconstruction/" target="_blank" rel="noopener">序列重建（LEETCODE）</a></li>
<li>任务执行顺序安排（中等）</li>
<li>树的最小高度（困难）</li>
</ol>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://medium.com/hackernoon/14-patterns-to-ace-any-coding-interview-question-c5bb3357f6ed" target="_blank" rel="noopener">14 Patterns to Ace Any Coding Interview Question</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/90664857" target="_blank" rel="noopener">码农找工作之：秒杀算法面试必须掌握的14种模式</a></li>
<li><a href="https://labuladong.gitbook.io/algo/" target="_blank" rel="noopener">labuladong的算法小抄</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>bert源码</title>
    <url>/2020/05/13/research/bert_code/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/13/research/bert_code/bert.jpg" alt></p>
<a id="more"></a>
<h1 id="jian-jie">简介</h1>
<p>BERT——来自<strong>Transformer的双向编码器表征</strong>。与最近的语言表征模型不同，BERT旨在基于<strong>所有层</strong>的<strong>左、右语境</strong>来预训练深度双向表征。BERT是首个在<strong>大批句子层面</strong>和<strong>token层面</strong>任务中取得当前最优性能的<strong>基于微调的表征模型</strong>，其性能超越许多使用任务特定架构的系统，刷新了11项NLP任务的最优性能记录。</p>
<h2 id="motivation">motivation</h2>
<p>作者认为现有的技术严重制约了预训练表征的能力，其主要局限在于语言模型是<strong>单向</strong>的，例如，OpenAI GPT使用的是<strong>从左到右</strong>的架构，其中<strong>每个token只能注意Transformer自注意力层中的先前token</strong>。这些局限对于<strong>句子层面的任务</strong>而言不是最佳选择，对于<strong>token级任务</strong>则可能是毁灭性的，<strong>因为在这种任务中，结合两个方向的语境至关重要</strong>BERT（Bidirectional Encoder Representations from Transformers）改进了<strong>基于微调的策略</strong>。</p>
<h2 id="solution">solution</h2>
<p>BERT提出一种新的<strong>预训练目标</strong>——<strong>遮蔽语言模型（masked language model，MLM）</strong>，来克服上文提到的单向局限。MLM<strong>随机遮蔽输入中的一些token</strong>，通过遮蔽词的语境来<strong>预测其原始词汇id</strong>。与从左到右的语言模型预训练不同，MLM目标<strong>允许表征融合左右两侧的语境</strong>，从而预训练一个深度<strong>双向Transformer</strong>。除了 MLM，还引入了一个**“下一句预测”（next sentence prediction）任务**，该任务<strong>联合预训练</strong>文本对表征。</p>
<h2 id="contribution">contribution</h2>
<ol>
<li>Bert模型的<strong>双向特性是最重要的一项新贡献</strong></li>
<li>BERT是首个在大批句子层面和token层面任务中取得当前最优性能的基于微调的表征模型，其性能超越许多使用任务特定架构的系统。证明了<strong>预训练表征</strong>可以<strong>消除对许多精心设计的任务特定架构的需求</strong>。</li>
</ol>
<p><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">论文地址</a></p>
<h1 id="mo-xing-jia-gou">模型架构</h1>
<p>BERT 旨在基于所有层的左、右语境来预训练深度双向表征。因此，预训练的 BERT 表征可以仅用一个额外的输出层进行微调，进而为很多任务创建当前最优模型，无需对任务特定架构做出大量修改。</p>
<h2 id="base-amp-large">Base &amp; Large</h2>
<blockquote>
<p>\(BERT_{Base}:L=12,H=768,A=12,Total Parameters=110M\)</p>
<p>\(BERT_{Large}:L=24,H=1024,A=16,Total Parameters=340M\)</p>
</blockquote>
<p>其中，\(L\):表示层数，\(H\):表示隐藏层的size，\(A\):表示自注意力head的个数。feed-forward的size为\(4H\),即\(H=768\)时为3072，\(H=1024\)时为4096。</p>
<h2 id="bert-amp-open-ai-gpt-amp-el-mo">BERT &amp; OpenAI GPT &amp; ELMo</h2>
<p>\(BERT_{Base}\)和OpenAI GPT的大小是一样的。BERT Transformer使用<strong>双向自注意力机制</strong>，而GPT Transformer使用受限的自注意力机制，导致每个token只能关注其左侧的语境。双向Transformer在文献中通常称为**“Transformer 编码器”<strong>，而只</strong>关注左侧语境的版本<strong>则因能用于文本生成而被称为</strong>“Transformer 解码器”**。</p>
<p><img src="/2020/05/13/research/bert_code/bert-gpt-transformer-elmo.png" alt></p>
<ul>
<li>BERT 使用双向Transformer</li>
<li>OpenAI GPT 使用从左到右的Transformer</li>
<li>ELMo 使用独立训练的从左到右和从右到左LSTM的级联来生成下游任务的特征。</li>
</ul>
<h2 id="pre-training-tasks">Pre-training Tasks</h2>
<h3 id="task-1-masked-lm">Task1:Masked LM</h3>
<p>BERT训练双向语言模型时以较小的概率把少量的词替成了Mask或者另一个随机的词。其目的在于使模型被迫增加对上下文的记忆。标准语言模型只能从左到右或从右到左进行训练，使得每个单词在多层上下文中<strong>间接</strong>地“see itself”。</p>
<p>为了训练一个深度双向表示，研究团队采用了一种简单的方法，即随机屏蔽（masking）部分输入token，然后只预测那些被屏蔽的token。论文将这个过程称为“masked LM”(MLM)。与masked token对应的最终隐藏向量被输入到词汇表上的输出softmax中。</p>
<p>在实验中，随机地Mask每个序列中15%的WordPiece token。</p>
<p>虽然这确实能获得双向预训练模型，但这种方法有两个缺点。</p>
<ul>
<li>
<p>缺点1：预训练和finetuning之间不匹配，因为在finetuning期间从未看到<code>[MASK]</code>token。</p>
<p>为了解决这个问题，并不总是用实际的<code>[MASK]</code>token替换被“masked”的词汇。使用训练数据生成器<strong>随机选择15％的token</strong>。例如在这个句子“my dog is hairy”中，它选择的token是“hairy”。然后，执行以下过程：</p>
<ul>
<li>80％的时间：<strong>用<code>[MASK]</code>标记替换单词</strong>，例如，<code>my dog is hairy → my dog is [MASK]</code></li>
<li>10％的时间：用一个<strong>随机的单词</strong>替换该单词，例如，<code>my dog is hairy → my dog is apple</code></li>
<li>10％的时间：<strong>保持单词不变</strong>，例如，<code>my dog is hairy → my dog is hairy</code>. 这样做的目的是将表示偏向于实际观察到的单词。</li>
</ul>
<p>由于Transformer encoder不知道它将被要求预测哪些单词或哪些单词已被随机单词替换，因此它<strong>被迫保持每个输入token的分布式上下文表示</strong>。此外，因为随机替换只发生在所有token的1.5％（即<strong>15％的10％</strong>），这不会损害模型的语言理解能力。</p>
</li>
<li>
<p>缺点2：每个batch只预测了15％的token，这表明模型可能需要更多的预训练步骤才能收敛。</p>
<p>MLM的收敛速度略慢于 left-to-right的模型（预测每个token），但MLM模型在实验上获得的提升远远超过增加的训练成本。</p>
</li>
</ul>
<h3 id="task-2-next-sentence-prediction">Task2:Next Sentence Prediction</h3>
<p>为了训练一个<strong>理解句子关系</strong>的模型，预先训练一个二分类的下一句测任务，这一任务可以从任何单语语料库中生成。具体地说，当选择句子A和B作为预训练样本时，B有50％的可能是A的下一个句子，也有50％的可能是来自语料库的随机句子。（实际bert后续的研究中，表明这个任务在理解句子关系表现的不是很理想。）例如：</p>
<figure class="highlight inform7"><table><tr><td class="code"><pre><span class="line">Input = <span class="comment">[CLS]</span> the <span class="keyword">man</span> went to <span class="comment">[MASK]</span> store <span class="comment">[SEP]</span> he bought a gallon <span class="comment">[MASK]</span> milk <span class="comment">[SEP]</span></span><br><span class="line">Label = IsNext</span><br><span class="line"></span><br><span class="line">Input = <span class="comment">[CLS]</span> the <span class="keyword">man</span> <span class="comment">[MASK]</span> to the store <span class="comment">[SEP]</span> penguin <span class="comment">[MASK]</span> <span class="keyword">are</span> flight ##less birds <span class="comment">[SEP]</span></span><br><span class="line">Label = NotNext</span><br></pre></td></tr></table></figure>
<h1 id="dai-ma-shi-xian-pytorch">代码实现（pytorch）</h1>
<p>参考<a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener">transformers</a></p>
<h2 id="yu-xun-lian-mo-xing">预训练模型</h2>
<p><a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">bert中文预训练模型下载</a>，解压缩之后，会有三个文件：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bert_config.json <span class="comment"># 模型配置文件</span></span><br><span class="line">pytorch_model.bin <span class="comment"># 预训练好的模型</span></span><br><span class="line">vocab.txt <span class="comment">#词汇表</span></span><br></pre></td></tr></table></figure>
<p>vocab.txt是模型的词典，这个文件会经常要用到。<em>bert_config.json</em>是BERT的配置(超参数)，比如网络的层数，通常不需要修改，如果自己显存小的话，也可以调小一下bert的层数。pytorch_model是预训练好的模型的模型参数，Fine-Tuning模型的初始值就是来自这个文件，然后根据不同的任务进行Fine-Tuning。</p>
<figure class="highlight"><figcaption><span>bert_config.json</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "attention_probs_dropout_prob": 0.1,  #乘法attention时，softmax后dropout概率 </span><br><span class="line">  "directionality": "bidi", </span><br><span class="line">  "hidden_act": "gelu",  #激活函数 </span><br><span class="line">  "hidden_dropout_prob": 0.1,  #隐藏层dropout概率 </span><br><span class="line">  "hidden_size": 768,  #隐藏单元数 (embedding_size)</span><br><span class="line">  "initializer_range": 0.02,   #初始化范围 </span><br><span class="line">  "intermediate_size": 3072,  #升维维度</span><br><span class="line">  "max_position_embeddings": 512,  #用于生成position_embedding。输入序列长度（seq_len）不能超过512</span><br><span class="line">  "num_attention_heads": 12,  #每个隐藏层中的attention head数 (则，每个head的embedding_size=768/12=64)</span><br><span class="line">  "num_hidden_layers": 12, #隐藏层数 </span><br><span class="line">  "pooler_fc_size": 768, </span><br><span class="line">  "pooler_num_attention_heads": 12, </span><br><span class="line">  "pooler_num_fc_layers": 3, </span><br><span class="line">  "pooler_size_per_head": 128, </span><br><span class="line">  "pooler_type": "first_token_transform", </span><br><span class="line">  "type_vocab_size": 2,  #segment_ids类别 [0,1] </span><br><span class="line">  "vocab_size": 21128 #词典中词数</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>vocab.txt中的部分内容</p>
<figure class="highlight plain"><figcaption><span>vocab.txt</span></figcaption><table><tr><td class="code"><pre><span class="line">馬</span><br><span class="line">高</span><br><span class="line">龍</span><br><span class="line">龸</span><br><span class="line">ﬁ</span><br><span class="line">ﬂ</span><br><span class="line">！</span><br><span class="line">（</span><br><span class="line">）</span><br><span class="line">，</span><br><span class="line">－</span><br><span class="line">．</span><br><span class="line">／</span><br><span class="line">：</span><br><span class="line">？</span><br><span class="line">～</span><br><span class="line">the</span><br><span class="line">of</span><br><span class="line">and</span><br><span class="line">in</span><br><span class="line">to</span><br></pre></td></tr></table></figure>
<h2 id="data">Data</h2>
<h3 id="step-1-read-example-from-file">step 1: read example from file</h3>
<figure class="highlight python"><figcaption><span>read_examples_from_file</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InputExample</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A single training/test example for token classification.</span></span><br><span class="line"><span class="string">	用于保存当个样本实例。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        guid: 样本id.</span></span><br><span class="line"><span class="string">        words: 文本序列，list类型：[word1,word2,...wordn]. </span></span><br><span class="line"><span class="string">        labels: 序列对应的label， This should be specified for train and dev examples, but not for test examples.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 由于语法糖@dataclass,会自动对该类添加__init__()函数。</span></span><br><span class="line">    guid: str</span><br><span class="line">    words: List[str]</span><br><span class="line">    labels: Optional[List[str]]</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_examples_from_file</span><span class="params">(data_dir, mode: Union[Split, str])</span> -&gt; List[InputExample]:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    data_dir: 原始数据所在的文件夹</span></span><br><span class="line"><span class="string">    model：‘train’(‘valid’)对应的数据会有label，‘test’数据的label为0</span></span><br><span class="line"><span class="string">    return：</span></span><br><span class="line"><span class="string">    	examples：list类型，包含train or valid or test 中的所有样本。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(mode, Split):</span><br><span class="line">        mode = mode.value</span><br><span class="line">    file_path = os.path.join(data_dir, <span class="string">f"<span class="subst">&#123;mode&#125;</span>.txt"</span>)</span><br><span class="line">    guid_index = <span class="number">1</span></span><br><span class="line">    examples = []</span><br><span class="line">    <span class="keyword">with</span> open(file_path, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        words = []</span><br><span class="line">        labels = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            <span class="keyword">if</span> line.startswith(<span class="string">"-DOCSTART-"</span>) <span class="keyword">or</span> line == <span class="string">""</span> <span class="keyword">or</span> line == <span class="string">"\n"</span>:</span><br><span class="line">                <span class="keyword">if</span> words:</span><br><span class="line">                    examples.append(InputExample(guid=<span class="string">f"<span class="subst">&#123;mode&#125;</span>-<span class="subst">&#123;guid_index&#125;</span>"</span>, words=words, labels=labels))</span><br><span class="line">                    guid_index += <span class="number">1</span></span><br><span class="line">                    words = []</span><br><span class="line">                    labels = []</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                splits = line.split(<span class="string">" "</span>)</span><br><span class="line">                words.append(splits[<span class="number">0</span>])</span><br><span class="line">                <span class="keyword">if</span> len(splits) &gt; <span class="number">1</span>:</span><br><span class="line">                    labels.append(splits[<span class="number">-1</span>].replace(<span class="string">"\n"</span>, <span class="string">""</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># Examples could have no label for mode = "test"</span></span><br><span class="line">                    labels.append(<span class="string">"O"</span>)</span><br><span class="line">        <span class="keyword">if</span> words:</span><br><span class="line">            examples.append(InputExample(guid=<span class="string">f"<span class="subst">&#123;mode&#125;</span>-<span class="subst">&#123;guid_index&#125;</span>"</span>, words=words, labels=labels))</span><br><span class="line">    <span class="keyword">return</span> examples</span><br></pre></td></tr></table></figure>
<h3 id="step-2-convert-example-into-feature">step 2:convert example into feature</h3>
<h4 id="basic-tokenizer">BasicTokenizer</h4>
<p>在转化成feature之前需要先了解一下bert的分词：</p>
<ol>
<li>对于文本序列中不想被切分开的词可以放到一个列表中，通过never_split传入分词函数中，就不会对列表中的文字进行分词。</li>
<li>中文序列标注任务需要注意：<code>'\t', '\n', '\r'，' '</code>会被替换成空白字符<code>' '</code>添加到文本中，英文文本对文本进行分词的时候又会根据空白字符来进行分词，导致空白字符会被直接清洗掉，这样就会导致中文序列标注的标签中的<code>'\t', '\n', '\r'，' '</code>在分词之后找不到对应的位置。</li>
</ol>
<figure class="highlight python"><figcaption><span>BasicTokenizer</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Runs basic tokenization (punctuation splitting, lower casing, etc.)."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, do_lower_case=True, never_split=None, tokenize_chinese_chars=True)</span>:</span></span><br><span class="line">        <span class="string">""" Constructs a BasicTokenizer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            **do_lower_case**: Whether to lower case the input.</span></span><br><span class="line"><span class="string">            **never_split**: (`optional`) List of token not to split.</span></span><br><span class="line"><span class="string">            **tokenize_chinese_chars**: (`optional`),是否对中文分词，默认是true</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> never_split <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            never_split = []</span><br><span class="line">        self.do_lower_case = do_lower_case</span><br><span class="line">        self.never_split = never_split</span><br><span class="line">        self.tokenize_chinese_chars = tokenize_chinese_chars</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text, never_split=None)</span>:</span></span><br><span class="line">        <span class="string">""" Basic Tokenization of a piece of text.</span></span><br><span class="line"><span class="string">            仅仅根据 空白字符 来进行分词</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            **never_split**: (`optional`)，对哪些文字不进行分词，[person,people,..],如果正常分词的话，person 会被切成per ##son，但是person如果被放入这个列表中，就不会被切分开</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        never_split = self.never_split + (never_split <span class="keyword">if</span> never_split <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> [])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 注意： 非法中文字符会被从文本中删除，以及‘\t’, ‘\n’, and ‘\r’，‘ ’会被替换成空白字符‘ ’添加到文本中，对文本进行分词的时候又会根据空白字符来进行分词，导致空白字符会被直接清洗掉，对于序列标注任务来说，会造成文本与标签对不齐的问题。</span></span><br><span class="line">        text = self._clean_text(text)</span><br><span class="line">        <span class="keyword">if</span> self.tokenize_chinese_chars:</span><br><span class="line">            <span class="comment"># 对中文字符按 字 进行分词，对中文分词过滤一遍之后在对英文字符进行分词。</span></span><br><span class="line">            text = self._tokenize_chinese_chars(text)</span><br><span class="line">            </span><br><span class="line">        orig_tokens = whitespace_tokenize(text) <span class="comment"># 先按照空白字符进行分词。</span></span><br><span class="line">        split_tokens = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> orig_tokens:</span><br><span class="line">            <span class="keyword">if</span> self.do_lower_case <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> never_split:</span><br><span class="line">                token = token.lower()</span><br><span class="line">                token = self._run_strip_accents(token)</span><br><span class="line">            split_tokens.extend(self._run_split_on_punc(token, never_split))</span><br><span class="line">        output_tokens = whitespace_tokenize(<span class="string">" "</span>.join(split_tokens))</span><br><span class="line">        <span class="keyword">return</span> output_tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run_strip_accents</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="string">"""Strips accents from a piece of text."""</span></span><br><span class="line">        text = unicodedata.normalize(<span class="string">"NFD"</span>, text)</span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">            cat = unicodedata.category(char)</span><br><span class="line">            <span class="keyword">if</span> cat == <span class="string">"Mn"</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            output.append(char)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>.join(output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run_split_on_punc</span><span class="params">(self, text, never_split=None)</span>:</span></span><br><span class="line">        <span class="string">"""Splits punctuation on a piece of text."""</span></span><br><span class="line">        <span class="keyword">if</span> never_split <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> text <span class="keyword">in</span> never_split:</span><br><span class="line">            <span class="keyword">return</span> [text]</span><br><span class="line">        chars = list(text)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        start_new_word = <span class="literal">True</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">while</span> i &lt; len(chars):</span><br><span class="line">            char = chars[i]</span><br><span class="line">            <span class="keyword">if</span> _is_punctuation(char):</span><br><span class="line">                output.append([char])</span><br><span class="line">                start_new_word = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> start_new_word:</span><br><span class="line">                    output.append([])</span><br><span class="line">                start_new_word = <span class="literal">False</span></span><br><span class="line">                output[<span class="number">-1</span>].append(char)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [<span class="string">""</span>.join(x) <span class="keyword">for</span> x <span class="keyword">in</span> output]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_tokenize_chinese_chars</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="string">"""对中文合法字符按字进行分割。"""</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">            cp = ord(char)</span><br><span class="line">            <span class="keyword">if</span> self._is_chinese_char(cp):</span><br><span class="line">                output.append(<span class="string">" "</span>)</span><br><span class="line">                output.append(char)</span><br><span class="line">                output.append(<span class="string">" "</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output.append(char)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>.join(output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_is_chinese_char</span><span class="params">(self, cp)</span>:</span></span><br><span class="line">        <span class="string">"""判断cp是否是中文合法字符</span></span><br><span class="line"><span class="string">        0x4e00-0x9fff cjk 统一字型 常用字 共 20992个（实际只定义到0x9fc3)</span></span><br><span class="line"><span class="string">        0x3400-0x4dff cjk 统一字型扩展表a 少用字 共 6656个</span></span><br><span class="line"><span class="string">        0x20000-0x2a6df cjk 统一字型扩展表b 少用字，历史上使用 共42720个</span></span><br><span class="line"><span class="string">        0xf900-0xfaff cjk 兼容字型 重复字，可统一变体，共同字 共512个</span></span><br><span class="line"><span class="string">        0x2f800-0x2fa1f cjk 兼容字型补遗 可统一变体 共544个</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> (</span><br><span class="line">            (cp &gt;= <span class="number">0x4E00</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x9FFF</span>)</span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x3400</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x4DBF</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x20000</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2A6DF</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2A700</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2B73F</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2B740</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2B81F</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2B820</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2CEAF</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0xF900</span> <span class="keyword">and</span> cp &lt;= <span class="number">0xFAFF</span>)</span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2F800</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2FA1F</span>)  <span class="comment">#</span></span><br><span class="line">        ):  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_clean_text</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="string">"""过滤掉非法字符，把`'\t', '\n', '\r'，' '`会被替换成空白字符`' '`"""</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">            cp = ord(char)</span><br><span class="line">            <span class="keyword">if</span> cp == <span class="number">0</span> <span class="keyword">or</span> cp == <span class="number">0xFFFD</span> <span class="keyword">or</span> _is_control(char):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> _is_whitespace(char):</span><br><span class="line">                output.append(<span class="string">" "</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output.append(char)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>.join(output)</span><br></pre></td></tr></table></figure>
<h4 id="wordpiece-tokenizer">WordpieceTokenizer</h4>
<p>经过<code>BasicTokenizer</code>处理成空格隔开的单词之后，还需要在经过WordpieceTokenizer对英文单词进行更细粒度的切分：</p>
<blockquote>
<p>input = “unaffable”</p>
<p>output = [“un”, “##aff”, “##able”]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordpieceTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Runs WordPiece tokenization."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab, unk_token, max_input_chars_per_word=<span class="number">100</span>)</span>:</span></span><br><span class="line">        self.vocab = vocab</span><br><span class="line">        self.unk_token = unk_token</span><br><span class="line">        self.max_input_chars_per_word = max_input_chars_per_word</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        For example:input = "unaffable"output = ["un", "##aff", "##able"]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          text: 经过`BasicTokenizer`分词之后，</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">          A list of wordpiece tokens.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        output_tokens = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> whitespace_tokenize(text):</span><br><span class="line">            chars = list(token)</span><br><span class="line">            <span class="keyword">if</span> len(chars) &gt; self.max_input_chars_per_word:</span><br><span class="line">                output_tokens.append(self.unk_token)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            is_bad = <span class="literal">False</span></span><br><span class="line">            start = <span class="number">0</span></span><br><span class="line">            sub_tokens = []</span><br><span class="line">            <span class="keyword">while</span> start &lt; len(chars):</span><br><span class="line">                end = len(chars)</span><br><span class="line">                cur_substr = <span class="literal">None</span></span><br><span class="line">                <span class="keyword">while</span> start &lt; end:</span><br><span class="line">                    substr = <span class="string">""</span>.join(chars[start:end])</span><br><span class="line">                    <span class="keyword">if</span> start &gt; <span class="number">0</span>:</span><br><span class="line">                        substr = <span class="string">"##"</span> + substr</span><br><span class="line">                    <span class="keyword">if</span> substr <span class="keyword">in</span> self.vocab: <span class="comment"># 如果在词汇表中存在该wordpiece</span></span><br><span class="line">                        cur_substr = substr</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    end -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> cur_substr <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    is_bad = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                sub_tokens.append(cur_substr)</span><br><span class="line">                start = end</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> is_bad:</span><br><span class="line">                output_tokens.append(self.unk_token)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output_tokens.extend(sub_tokens)</span><br><span class="line">        <span class="keyword">return</span> output_tokens</span><br></pre></td></tr></table></figure>
<h4 id="bert-tokenizer">BertTokenizer</h4>
<p><code>BertTokenizer</code>基于<code>WordPiece</code>，需要注意的地方是，对于中文文本来说，中文文本是没有空格分隔的文本，所以是需要在WordPiece之前do_basic_tokenize，也就是对于中文来说，需要<code>do_basic_tokenize=True</code>,不然对于没有分隔的中文字符会由于其长度超过词的最大长度，被修改为<code>[UNK]</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertTokenizer</span><span class="params">(PreTrainedTokenizer)</span>:</span></span><br><span class="line">    <span class="string">r"""</span></span><br><span class="line"><span class="string">    Constructs a BERT tokenizer. Based on WordPiece.</span></span><br><span class="line"><span class="string">    这是一个替换原文本中符号，检测元文本中的单词是否在预训练字典中，将单词替换成字典中对应的id，对文本的长度进行padding的一个类。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab_file:词汇表文件</span></span><br><span class="line"><span class="string">        do_lower_case:是否大写变小写，默认True</span></span><br><span class="line"><span class="string">        do_basic_tokenize:是否在WordPiece之前做basci tokenize,对于中文字符来说</span></span><br><span class="line"><span class="string">        never_split：哪些词不需要在进行更细粒度的切分，前提需要`do_basic_tokenize=True`，才会起作用。</span></span><br><span class="line"><span class="string">        unk_token：对于不在词汇表中的词汇被修改成unk_token，默认是 "[UNK]"，无特殊需要，使用默认就好。</span></span><br><span class="line"><span class="string">        sep_token：分隔句子的标识，默认是"[SEP]"，无特殊需要，使用默认就好。</span></span><br><span class="line"><span class="string">        pad_token:超出句子长度的内容被填补pad_token, 默认是"[PAD]"，无特殊需要，使用默认就好。</span></span><br><span class="line"><span class="string">        cls_token：句子分类，默认"[CLS]"，原用于NSP任务，可用该字符对应的输出向量做文本分类任务</span></span><br><span class="line"><span class="string">        mask_token：对文本进行mask的字符，用于MAKS LM 任务，被mask住的字符需要在训练阶段进行predict，默认是"[MASK]"，无特殊需要，使用默认就好。</span></span><br><span class="line"><span class="string">        tokenize_chinese_chars：是否对中文字符进行分词。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    vocab_files_names = VOCAB_FILES_NAMES <span class="comment"># 词汇表文件名</span></span><br><span class="line">    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP</span><br><span class="line">    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION</span><br><span class="line">    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        vocab_file,</span></span></span><br><span class="line"><span class="function"><span class="params">        do_lower_case=True,</span></span></span><br><span class="line"><span class="function"><span class="params">        do_basic_tokenize=True,</span></span></span><br><span class="line"><span class="function"><span class="params">        never_split=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        unk_token=<span class="string">"[UNK]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        sep_token=<span class="string">"[SEP]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        pad_token=<span class="string">"[PAD]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        cls_token=<span class="string">"[CLS]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        mask_token=<span class="string">"[MASK]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        tokenize_chinese_chars=True,</span></span></span><br><span class="line"><span class="function"><span class="params">        **kwargs</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">        super().__init__(</span><br><span class="line">            unk_token=unk_token,</span><br><span class="line">            sep_token=sep_token,</span><br><span class="line">            pad_token=pad_token,</span><br><span class="line">            cls_token=cls_token,</span><br><span class="line">            mask_token=mask_token,</span><br><span class="line">            **kwargs,</span><br><span class="line">        )</span><br><span class="line">        self.max_len_single_sentence = self.max_len - <span class="number">2</span>  <span class="comment"># take into account special tokens</span></span><br><span class="line">        self.max_len_sentences_pair = self.max_len - <span class="number">3</span>  <span class="comment"># roberta，take into account special tokens</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(vocab_file):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">"Can't find a vocabulary file at path '&#123;&#125;'. To load the vocabulary from a Google pretrained "</span></span><br><span class="line">                <span class="string">"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"</span>.format(vocab_file)</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 加载词汇表文件</span></span><br><span class="line">        self.vocab = load_vocab(vocab_file)</span><br><span class="line">        <span class="comment"># id 到 token 的映射</span></span><br><span class="line">        self.ids_to_tokens = collections.OrderedDict([(ids, tok) <span class="keyword">for</span> tok, ids <span class="keyword">in</span> self.vocab.items()])</span><br><span class="line">        self.do_basic_tokenize = do_basic_tokenize</span><br><span class="line">        <span class="keyword">if</span> do_basic_tokenize:</span><br><span class="line">            self.basic_tokenizer = BasicTokenizer(</span><br><span class="line">                do_lower_case=do_lower_case, never_split=never_split, tokenize_chinese_chars=tokenize_chinese_chars</span><br><span class="line">            )</span><br><span class="line">        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">vocab_size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_vocab</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> dict(self.vocab, **self.added_tokens_encoder)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        split_tokens = []</span><br><span class="line">        <span class="keyword">if</span> self.do_basic_tokenize:</span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):</span><br><span class="line">                <span class="keyword">for</span> sub_token <span class="keyword">in</span> self.wordpiece_tokenizer.tokenize(token):</span><br><span class="line">                    split_tokens.append(sub_token)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            split_tokens = self.wordpiece_tokenizer.tokenize(text)</span><br><span class="line">        <span class="keyword">return</span> split_tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_convert_token_to_id</span><span class="params">(self, token)</span>:</span></span><br><span class="line">        <span class="string">""" 把词映射到id"""</span></span><br><span class="line">        <span class="keyword">return</span> self.vocab.get(token, self.vocab.get(self.unk_token))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_convert_id_to_token</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="string">"""把token id 映射成词"""</span></span><br><span class="line">        <span class="keyword">return</span> self.ids_to_tokens.get(index, self.unk_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convert_tokens_to_string</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">        <span class="string">""" 英文：把wordpiece之后的tokens 还原成文本。没有处理中文的换行等"""</span></span><br><span class="line">        out_string = <span class="string">" "</span>.join(tokens).replace(<span class="string">" ##"</span>, <span class="string">""</span>).strip()</span><br><span class="line">        <span class="keyword">return</span> out_string</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_inputs_with_special_tokens</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span> -&gt; List[int]:</span></span><br><span class="line">        <span class="string">"""为分词后的句子添加special token：[CLS]、[SEP]</span></span><br><span class="line"><span class="string">        - single sequence: ``[CLS] X [SEP]``</span></span><br><span class="line"><span class="string">        - pair of sequences: ``[CLS] A [SEP] B [SEP]``</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> [self.cls_token_id] + token_ids_0 + [self.sep_token_id]</span><br><span class="line">        cls = [self.cls_token_id]</span><br><span class="line">        sep = [self.sep_token_id]</span><br><span class="line">        <span class="keyword">return</span> cls + token_ids_0 + sep + token_ids_1 + sep</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_special_tokens_mask</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span> -&gt; List[int]:</span></span><br><span class="line">        <span class="string">"""判断token_ids 是否已经添加过special token，用mask序列来表示：[0,1,1,1,1,0],0表示special token，返回mask 序列</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> already_has_special_tokens:</span><br><span class="line">            <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">"You should not supply a second sequence if the provided sequence of "</span></span><br><span class="line">                    <span class="string">"ids is already formated with special tokens for the model."</span></span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">return</span> list(map(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x <span class="keyword">in</span> [self.sep_token_id, self.cls_token_id] <span class="keyword">else</span> <span class="number">0</span>, token_ids_0))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> [<span class="number">1</span>] + ([<span class="number">0</span>] * len(token_ids_0)) + [<span class="number">1</span>] + ([<span class="number">0</span>] * len(token_ids_1)) + [<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> [<span class="number">1</span>] + ([<span class="number">0</span>] * len(token_ids_0)) + [<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_token_type_ids_from_sequences</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span> -&gt; List[int]:</span></span><br><span class="line">        <span class="string">"""根据序列创建 token type ids，用于表示句子分割，如果只有一个句子，返回全为0的列表，如果是两个句子，则表示成如下：</span></span><br><span class="line"><span class="string">            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1</span></span><br><span class="line"><span class="string">            | first sequence    | second sequence |</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        sep = [self.sep_token_id]</span><br><span class="line">        cls = [self.cls_token_id]</span><br><span class="line">        <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> len(cls + token_ids_0 + sep) * [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> len(cls + token_ids_0 + sep) * [<span class="number">0</span>] + len(token_ids_1 + sep) * [<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_vocabulary</span><span class="params">(self, vocab_path)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Save the sentencepiece vocabulary (copy original file) and special tokens file to a directory.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> os.path.isdir(vocab_path):</span><br><span class="line">            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[<span class="string">"vocab_file"</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            vocab_file = vocab_path</span><br><span class="line">        <span class="keyword">with</span> open(vocab_file, <span class="string">"w"</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> writer:</span><br><span class="line">            <span class="keyword">for</span> token, token_index <span class="keyword">in</span> sorted(self.vocab.items(), key=<span class="keyword">lambda</span> kv: kv[<span class="number">1</span>]):</span><br><span class="line">                <span class="keyword">if</span> index != token_index:</span><br><span class="line">                    logger.warning(</span><br><span class="line">                        <span class="string">"Saving vocabulary to &#123;&#125;: vocabulary indices are not consecutive."</span></span><br><span class="line">                        <span class="string">" Please check that the vocabulary is not corrupted!"</span>.format(vocab_file)</span><br><span class="line">                    )</span><br><span class="line">                    index = token_index</span><br><span class="line">                writer.write(token + <span class="string">"\n"</span>)</span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> (vocab_file,)</span><br></pre></td></tr></table></figure>
<p>由于BertTokenizer继承了PreTrainedTokenizer，对PreTrainedTokenizer感兴趣可以通过这个<a href="https://github.com/huggingface/transformers/blob/ef46ccb05c601f413a774d43524591816406778d/src/transformers/tokenization_utils.py#L693" target="_blank" rel="noopener">链接</a>在进行研究。</p>
<h4 id="input-feature">Input feature</h4>
<figure class="highlight python"><figcaption><span>convert_example_to_features</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InputFeatures</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    InputExample 对应的 feature，该类变量名称与model中的变量是对应的。</span></span><br><span class="line"><span class="string">    @dataclass 自动为该类添加初始化函数__init__().</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    input_ids: List[int] <span class="comment"># 输入样本的id</span></span><br><span class="line">    attention_mask: List[int] <span class="comment"># </span></span><br><span class="line">    <span class="comment"># 用来指示第几个句子，比如：</span></span><br><span class="line">    <span class="comment"># tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]</span></span><br><span class="line">    <span class="comment">#  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1</span></span><br><span class="line">    token_type_ids: Optional[List[int]] = <span class="literal">None</span> </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 由于是序列标注任务，所以label对应的也应该是一个序列。</span></span><br><span class="line">    label_ids: Optional[List[int]] = <span class="literal">None</span> </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="comment"># 实际上该函数没有处理中文分词中，换行（\n）,空格字符对齐的问题。    </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_examples_to_features</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    examples: List[InputExample], <span class="comment"># 样本集</span></span></span></span><br><span class="line"><span class="function"><span class="params">    label_list: List[str], <span class="comment"># 样本集对应的标签集</span></span></span></span><br><span class="line"><span class="function"><span class="params">    max_seq_length: int, <span class="comment"># 最大序列长度，但是不应该超过510（512应该包含[CLS],[SEP]两个字符）</span></span></span></span><br><span class="line"><span class="function"><span class="params">    tokenizer: PreTrainedTokenizer, <span class="comment"># 分词器</span></span></span></span><br><span class="line"><span class="function"><span class="params">    cls_token_at_end=False, <span class="comment"># [CLS]字符是否添加在序列最后，默认是放在序列最前面，False：[CLS] + A + [SEP] + B + [SEP]，True: A + [SEP] + B + [SEP] + [CLS]</span></span></span></span><br><span class="line"><span class="function"><span class="params">    cls_token=<span class="string">"[CLS]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    cls_token_segment_id=<span class="number">1</span>, <span class="comment"># `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)</span></span></span></span><br><span class="line"><span class="function"><span class="params">    sep_token=<span class="string">"[SEP]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    sep_token_extra=False, <span class="comment"># roberta 中会有extra sep token</span></span></span></span><br><span class="line"><span class="function"><span class="params">    pad_on_left=False, <span class="comment"># 是否在序列的左边进行pad</span></span></span></span><br><span class="line"><span class="function"><span class="params">    pad_token=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    pad_token_segment_id=<span class="number">0</span>, <span class="comment"># padding token_ids 的值为0</span></span></span></span><br><span class="line"><span class="function"><span class="params">    pad_token_label_id=<span class="number">-100</span>, <span class="comment"># 序列标注的tag，超出序列长度的部分，pad成：-100</span></span></span></span><br><span class="line"><span class="function"><span class="params">    sequence_a_segment_id=<span class="number">0</span>, <span class="comment"># 第一句对应的seg id：0  --&gt; type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1</span></span></span></span><br><span class="line"><span class="function"><span class="params">    mask_padding_with_zero=True, <span class="comment"># 对padding的部分，mask对应的值为0</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span> -&gt; List[InputFeatures]:</span></span><br><span class="line">        </span><br><span class="line">  <span class="comment"># label 到 index 的映射 ： [B,M,E,S]--&gt;[0,1,2,3]</span></span><br><span class="line">  label_map = &#123;label: i <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(label_list)&#125; </span><br><span class="line">  </span><br><span class="line">  features = []</span><br><span class="line">  <span class="keyword">for</span> (ex_index, example) <span class="keyword">in</span> enumerate(examples):</span><br><span class="line">      <span class="keyword">if</span> ex_index % <span class="number">10</span>_000 == <span class="number">0</span>:</span><br><span class="line">          logger.info(<span class="string">"Writing example %d of %d"</span>, ex_index, len(examples))</span><br><span class="line">  </span><br><span class="line">      tokens = []  <span class="comment"># 文本序列分词之后的列表:sentence --&gt; [word1,wor2,word3,....]</span></span><br><span class="line">      label_ids = [] <span class="comment"># 由于分词之后，label 和 tokens 会产生错位现象（[CLS],[SEP],空格,换行等字符导致的问题），需要重新和分词之后的内容对齐。</span></span><br><span class="line">      <span class="keyword">for</span> word, label <span class="keyword">in</span> zip(example.words, example.labels):</span><br><span class="line">          word_tokens = tokenizer.tokenize(word)</span><br><span class="line">  </span><br><span class="line">          <span class="keyword">if</span> len(word_tokens) &gt; <span class="number">0</span>:</span><br><span class="line">              tokens.extend(word_tokens)</span><br><span class="line">              label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - <span class="number">1</span>))</span><br><span class="line">  </span><br><span class="line">      <span class="comment"># Account for [CLS] and [SEP] with "- 2" and with "- 3" for RoBERTa.</span></span><br><span class="line">      special_tokens_count = tokenizer.num_special_tokens_to_add()</span><br><span class="line">      <span class="comment"># 先取出不添加special token的序列，再在这个序列的基础上，添加special token</span></span><br><span class="line">      <span class="keyword">if</span> len(tokens) &gt; max_seq_length - special_tokens_count:</span><br><span class="line">          tokens = tokens[: (max_seq_length - special_tokens_count)]</span><br><span class="line">          label_ids = label_ids[: (max_seq_length - special_tokens_count)]</span><br><span class="line">  </span><br><span class="line">      <span class="comment"># 在序列末尾添加[SEP]</span></span><br><span class="line">      tokens += [sep_token]</span><br><span class="line">      label_ids += [pad_token_label_id]</span><br><span class="line">      </span><br><span class="line">      <span class="comment"># roberta 的extra token</span></span><br><span class="line">      <span class="keyword">if</span> sep_token_extra:</span><br><span class="line">          <span class="comment"># roberta uses an extra separator b/w pairs of sentences</span></span><br><span class="line">          tokens += [sep_token]</span><br><span class="line">          label_ids += [pad_token_label_id]</span><br><span class="line">          </span><br><span class="line">      <span class="comment"># 第一句对应的seg id：0  --&gt; type_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1</span></span><br><span class="line">      segment_ids = [sequence_a_segment_id] * len(tokens)</span><br><span class="line">    </span><br><span class="line">      <span class="comment"># [CLS] 添加到句首还是句末</span></span><br><span class="line">      <span class="keyword">if</span> cls_token_at_end:</span><br><span class="line">          tokens += [cls_token]</span><br><span class="line">          label_ids += [pad_token_label_id]</span><br><span class="line">          segment_ids += [cls_token_segment_id]</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          tokens = [cls_token] + tokens</span><br><span class="line">          label_ids = [pad_token_label_id] + label_ids</span><br><span class="line">          segment_ids = [cls_token_segment_id] + segment_ids</span><br><span class="line">    </span><br><span class="line">      <span class="comment"># 把词转化成token id</span></span><br><span class="line">      input_ids = tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line">  </span><br><span class="line">      <span class="comment"># The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.</span></span><br><span class="line">      input_mask = [<span class="number">1</span> <span class="keyword">if</span> mask_padding_with_zero <span class="keyword">else</span> <span class="number">0</span>] * len(input_ids)</span><br><span class="line">  </span><br><span class="line">      <span class="comment"># 添加0padding到句子最大长度</span></span><br><span class="line">      padding_length = max_seq_length - len(input_ids)</span><br><span class="line">      <span class="comment"># 在句子的左边padding 0 还是在句子的右边padding 0</span></span><br><span class="line">      <span class="keyword">if</span> pad_on_left:</span><br><span class="line">          input_ids = ([pad_token] * padding_length) + input_ids</span><br><span class="line">          input_mask = ([<span class="number">0</span> <span class="keyword">if</span> mask_padding_with_zero <span class="keyword">else</span> <span class="number">1</span>] * padding_length) + input_mask</span><br><span class="line">          segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids</span><br><span class="line">          label_ids = ([pad_token_label_id] * padding_length) + label_ids</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          input_ids += [pad_token] * padding_length</span><br><span class="line">          input_mask += [<span class="number">0</span> <span class="keyword">if</span> mask_padding_with_zero <span class="keyword">else</span> <span class="number">1</span>] * padding_length</span><br><span class="line">          segment_ids += [pad_token_segment_id] * padding_length</span><br><span class="line">          label_ids += [pad_token_label_id] * padding_length</span><br><span class="line">  </span><br><span class="line">      <span class="keyword">assert</span> len(input_ids) == max_seq_length</span><br><span class="line">      <span class="keyword">assert</span> len(input_mask) == max_seq_length</span><br><span class="line">      <span class="keyword">assert</span> len(segment_ids) == max_seq_length</span><br><span class="line">      <span class="keyword">assert</span> len(label_ids) == max_seq_length</span><br><span class="line">  </span><br><span class="line">      <span class="keyword">if</span> ex_index &lt; <span class="number">5</span>:</span><br><span class="line">          logger.info(<span class="string">"*** Example ***"</span>)</span><br><span class="line">          logger.info(<span class="string">"guid: %s"</span>, example.guid)</span><br><span class="line">          logger.info(<span class="string">"tokens: %s"</span>, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> tokens]))</span><br><span class="line">          logger.info(<span class="string">"input_ids: %s"</span>, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> input_ids]))</span><br><span class="line">          logger.info(<span class="string">"input_mask: %s"</span>, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> input_mask]))</span><br><span class="line">          logger.info(<span class="string">"segment_ids: %s"</span>, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> segment_ids]))</span><br><span class="line">          logger.info(<span class="string">"label_ids: %s"</span>, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> label_ids]))</span><br><span class="line">  </span><br><span class="line">      <span class="keyword">if</span> <span class="string">"token_type_ids"</span> <span class="keyword">not</span> <span class="keyword">in</span> tokenizer.model_input_names:</span><br><span class="line">          segment_ids = <span class="literal">None</span></span><br><span class="line">  </span><br><span class="line">      features.append(</span><br><span class="line">          InputFeatures(</span><br><span class="line">              input_ids=input_ids, attention_mask=input_mask, token_type_ids=segment_ids, label_ids=label_ids</span><br><span class="line">          )</span><br><span class="line">      )</span><br><span class="line">  <span class="keyword">return</span> features</span><br></pre></td></tr></table></figure>
<h3 id="step-3-dataset">step 3:Dataset</h3>
<p>有了样本和特征，构建数据集类。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NerDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    features: List[InputFeatures]</span><br><span class="line">    <span class="comment"># 对于pad的部分不计算损失 </span></span><br><span class="line">    pad_token_label_id: int = nn.CrossEntropyLoss().ignore_index</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        data_dir: str,</span></span></span><br><span class="line"><span class="function"><span class="params">        tokenizer: PreTrainedTokenizer,</span></span></span><br><span class="line"><span class="function"><span class="params">        labels: List[str],</span></span></span><br><span class="line"><span class="function"><span class="params">        model_type: str,</span></span></span><br><span class="line"><span class="function"><span class="params">        max_seq_length: Optional[int] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">        overwrite_cache=False,</span></span></span><br><span class="line"><span class="function"><span class="params">        mode: Split = Split.train,</span></span></span><br><span class="line"><span class="function"><span class="params">        local_rank=<span class="number">-1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        )</span>:</span></span><br><span class="line">        <span class="comment"># 加载数据集文件</span></span><br><span class="line">        cached_features_file = os.path.join(data_dir, <span class="string">"cached_&#123;&#125;_&#123;&#125;_&#123;&#125;"</span>.format(mode.value, tokenizer.__class__.__name__, str(max_seq_length)), )</span><br><span class="line">        <span class="keyword">with</span> torch_distributed_zero_first(local_rank):</span><br><span class="line">        <span class="comment"># Make sure only the first process in distributed training processes the dataset,</span></span><br><span class="line">        <span class="comment"># and the others will use the cache.</span></span><br><span class="line">            <span class="keyword">if</span> os.path.exists(cached_features_file) <span class="keyword">and</span> <span class="keyword">not</span> overwrite_cache:</span><br><span class="line">                logger.info(<span class="string">f"Loading features from cached file <span class="subst">&#123;cached_features_file&#125;</span>"</span>)</span><br><span class="line">                self.features = torch.load(cached_features_file)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                logger.info(<span class="string">f"Creating features from dataset file at <span class="subst">&#123;data_dir&#125;</span>"</span>)</span><br><span class="line">                examples = read_examples_from_file(data_dir, mode)</span><br><span class="line">                <span class="comment"># TODO clean up all this to leverage built-in features of tokenizers</span></span><br><span class="line">                self.features = convert_examples_to_features(</span><br><span class="line">                    examples,</span><br><span class="line">                    labels,</span><br><span class="line">                    max_seq_length,</span><br><span class="line">                    tokenizer,</span><br><span class="line">                    cls_token_at_end=bool(model_type <span class="keyword">in</span> [<span class="string">"xlnet"</span>]),</span><br><span class="line">                    <span class="comment"># xlnet has a cls token at the end</span></span><br><span class="line">                    cls_token=tokenizer.cls_token,</span><br><span class="line">                    cls_token_segment_id=<span class="number">2</span> <span class="keyword">if</span> model_type <span class="keyword">in</span> [<span class="string">"xlnet"</span>] <span class="keyword">else</span> <span class="number">0</span>,</span><br><span class="line">                    sep_token=tokenizer.sep_token,</span><br><span class="line">                    sep_token_extra=bool(model_type <span class="keyword">in</span> [<span class="string">"roberta"</span>]),</span><br><span class="line">                    <span class="comment"># roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805</span></span><br><span class="line">                    pad_on_left=bool(tokenizer.padding_side == <span class="string">"left"</span>),</span><br><span class="line">                    pad_token=tokenizer.pad_token_id,</span><br><span class="line">                    pad_token_segment_id=tokenizer.pad_token_type_id,</span><br><span class="line">                    pad_token_label_id=self.pad_token_label_id,</span><br><span class="line">                )</span><br><span class="line">                <span class="comment"># 保存0卡上的数据到缓存</span></span><br><span class="line">                <span class="keyword">if</span> local_rank <span class="keyword">in</span> [<span class="number">-1</span>, <span class="number">0</span>]:</span><br><span class="line">                    logger.info(<span class="string">f"Saving features into cached file <span class="subst">&#123;cached_features_file&#125;</span>"</span>)</span><br><span class="line">                    torch.save(self.features, cached_features_file)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.features)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, i)</span> -&gt; InputFeatures:</span></span><br><span class="line">        <span class="keyword">return</span> self.features[i]</span><br></pre></td></tr></table></figure>
<h2 id="bert-model">BertModel</h2>
<p><img src="/2020/05/13/research/bert_code/BertModel.png" alt></p>
<p>从整体来看BertModel由三部分组成：BertEmbeddings、BertEncoder、BertPooler,需要注意<code>attention_mask和head_mask</code>的处理。</p>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>inputs，segment，mask'，position_ids'，head_mask'</code></p>
</li>
<li>
<p>输出：<code>元组 (最后一层的隐变量，最后一层第一个token的隐变量，最后一层的隐变量或每一层attentions 权重参数)</code></p>
</li>
<li>
<p>过程:<code>embedding-&gt;encoder-&gt;pooler</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertModel</span><span class="params">(BertPreTrainedModel)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__(config)</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        self.embeddings = BertEmbeddings(config)</span><br><span class="line">        self.encoder = BertEncoder(config)</span><br><span class="line">        self.pooler = BertPooler(config)</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_input_embeddings</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.embeddings.word_embeddings</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_input_embeddings</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        self.embeddings.word_embeddings = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_prune_heads</span><span class="params">(self, heads_to_prune)</span>:</span></span><br><span class="line">        <span class="string">""" E.g. &#123;1: [0, 2], 2: [2, 3]&#125; will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> layer, heads <span class="keyword">in</span> heads_to_prune.items():</span><br><span class="line">            self.encoder.layer[layer].attention.prune_heads(heads)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">        head_mask: head_mask has shape [num_heads] or [num_hidden_layers x num_heads]</span></span><br><span class="line"><span class="string">        encoder_hidden_states: encoder 的输出</span></span><br><span class="line"><span class="string">        encoder_attention_mask:</span></span><br><span class="line"><span class="string">        返回值：</span></span><br><span class="line"><span class="string">        last_hidden_state :[batch_size, sequence_length, hidden_size]，是序列在模型最后一层的输出的隐藏层</span></span><br><span class="line"><span class="string">        pooler_output :[batch_size, hidden_size]:[CLS]对应的隐状态的输出，由于这个token是用来做NSP任务的，这个输出通常不能很好的summary 整个序列的语义，如果想要获取整句话的语义通常需要对整个序列取平均或者池化。</span></span><br><span class="line"><span class="string">        hidden_states:[batch_size, sequence_length, hidden_size],embedding + output of each layer.</span></span><br><span class="line"><span class="string">        attention:[batch_size, num_heads, sequence_length, sequence_length],矩阵的权值是经过softmax的。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"You cannot specify both input_ids and inputs_embeds at the same time"</span>)</span><br><span class="line">        <span class="keyword">elif</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">elif</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"You have to specify either input_ids or inputs_embeds"</span>)</span><br><span class="line"></span><br><span class="line">        device = input_ids.device <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> inputs_embeds.device</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 全部mask成 1</span></span><br><span class="line">            attention_mask = torch.ones(input_shape, device=device)</span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 全表示成0（第一句）</span></span><br><span class="line">            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. attention_mask:[bs,seq_len] --&gt; [bs,num_heads,seq_len,seq_len]</span></span><br><span class="line">        <span class="comment"># 2. attention_value:1 --&gt; 0, 0--&gt;-10000,这样做的目的是为了和算出来的score（没有经过softmax）相加，相当于从序列中移除掉了mask的内容。</span></span><br><span class="line">        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, self.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.config.is_decoder <span class="keyword">and</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()</span><br><span class="line">            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length) <span class="comment"># [bs,seq_len]</span></span><br><span class="line">            <span class="keyword">if</span> encoder_attention_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 1. attention_mask:[bs,seq_len] --&gt; [bs,num_heads,seq_len,seq_len]</span></span><br><span class="line">            <span class="comment"># 2. attention_value:1 --&gt; 0, 0--&gt;-10000,这样做的目的是为了和算出来的score（没有经过softmax）相加，相当于从序列中移除掉了mask的内容。</span></span><br><span class="line">            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            encoder_extended_attention_mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># input head_mask： [num_heads] or [num_hidden_layers x num_heads]，1表示保留这个head。</span></span><br><span class="line">        <span class="comment"># 因为有多个head，每个head都需要mask序列中哪些是token，哪些是padding，所以需要转化head_mask为：[num_hidden_layers,bsz,num_heads,seq_length,seq_length]</span></span><br><span class="line">        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)</span><br><span class="line"></span><br><span class="line">        embedding_output = self.embeddings(</span><br><span class="line">            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds</span><br><span class="line">        )</span><br><span class="line">        encoder_outputs = self.encoder(</span><br><span class="line">            embedding_output,</span><br><span class="line">            attention_mask=extended_attention_mask,</span><br><span class="line">            head_mask=head_mask,</span><br><span class="line">            encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">            encoder_attention_mask=encoder_extended_attention_mask,</span><br><span class="line">        )</span><br><span class="line">        sequence_output = encoder_outputs[<span class="number">0</span>]</span><br><span class="line">        pooled_output = self.pooler(sequence_output)</span><br><span class="line">        <span class="comment"># add hidden_states and attentions if they are here</span></span><br><span class="line">        outputs = (sequence_output, pooled_output,) + encoder_outputs[<span class="number">1</span>:]  </span><br><span class="line">        <span class="keyword">return</span> outputs  <span class="comment"># sequence_output, pooled_output, (hidden_states), (attentions)</span></span><br></pre></td></tr></table></figure>
<h3 id="bert-embeddings">BertEmbeddings</h3>
<p>从Bert的论文中可以知道，Bert的词向量主要是由三个向量相加组合而成，分别是单词本身的向量，单词所在句子中位置的向量和句子所在单个训练文本中位置的向量。这样做的好处主要可以解决只有词向量时碰见多义词时模型预测不准的问题。</p>
<p><img src="/2020/05/13/research/bert_code/bert-input-representation.png" alt></p>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>inputs，segment'，position_ids'</code></p>
</li>
<li>
<p>输出：<code>words+position+segment的embedding</code></p>
</li>
<li>
<p>过程:<code>调用nn.Embedding构造words、position、segment的embedding -&gt; 三个embedding相加 -&gt; 规范化 LayerNorm（关联类BertLayerNorm）-&gt; dropout</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEmbeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Construct the embeddings from word, position and token_type embeddings.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)</span><br><span class="line">        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)</span><br><span class="line">        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)</span><br><span class="line">        </span><br><span class="line">        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">        device = input_ids.device <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> inputs_embeds.device</span><br><span class="line">        <span class="keyword">if</span> position_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 生成token 的 index 信息。[seq_len]</span></span><br><span class="line">            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)</span><br><span class="line">            position_ids = position_ids.unsqueeze(<span class="number">0</span>).expand(input_shape) <span class="comment">#[bs,seq_len]</span></span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs_embeds <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            inputs_embeds = self.word_embeddings(input_ids)</span><br><span class="line">        position_embeddings = self.position_embeddings(position_ids)</span><br><span class="line">        token_type_embeddings = self.token_type_embeddings(token_type_ids)</span><br><span class="line"></span><br><span class="line">        embeddings = inputs_embeds + position_embeddings + token_type_embeddings</span><br><span class="line">        <span class="comment"># BertLayerNorm = torch.nn.LayerNorm</span></span><br><span class="line">        embeddings = self.LayerNorm(embeddings)</span><br><span class="line">        embeddings = self.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure>
<p>从forward函数开始看，先有一个torch.arange函数。<code>torch.arange(seq_length, dtype=torch.long, device=input_ids.device)</code>来生成token 的index 信息,<span class="label danger">和transformer中的Positional Embedding不同，bert这里的positional embedding 需要通过训练进行学习，transformer中的positional embedding是使用公式计算出来的</span>(可以参考<a href="/2020/05/11/transformer/" title="Transformer">Transformer</a> 的Positional Embedding来进行比较)。此外，LayerNorm的数学表达为：\(y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\)，\(\gamma,\beta\)可以通过仿射变换来进行学习。</p>
<h3 id="bert-encoder">BertEncoder</h3>
<p>下面代码将原有的BertLayer一层一层剥开，如果想要输出每层的状态（output_hidden_states=True），则对hidden_states 进行累加，如果（output_hidden_states=False）则只与最后一层的状态相加。BertEncoder类，其实只是用来输出BertLayer类的状态的一个函数。真正模型内部的东西在BertLayer类中。</p>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states（由BetEmbeddings输出），attention_mask，head_mask</code></p>
</li>
<li>
<p>输出：<code>元组 (最后一层隐变量+每层隐变量) 或者 (最后一层attention+每一层attention)</code></p>
</li>
<li>
<p>过程:<code>调用modulelist类实例layer使得每一层输出（关联类BertLayer）-&gt; 保存所有层的attention输出 和 隐变量 -&gt; 返回元组，元组第一个是最后一层的attention或hidden，再往后是每层的。</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.output_attentions = config.output_attentions</span><br><span class="line">        self.output_hidden_states = config.output_hidden_states</span><br><span class="line">        self.layer = nn.ModuleList([BertLayer(config) <span class="keyword">for</span> _ <span class="keyword">in</span> range(config.num_hidden_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,)</span>:</span></span><br><span class="line">        all_hidden_states = ()</span><br><span class="line">        all_attentions = ()</span><br><span class="line">        <span class="comment"># 将原有的BertLayer一层一层剥开</span></span><br><span class="line">        <span class="comment"># 如果想要输出每层的状态（output_hidden_states=True），则对hidden_states 进行累加</span></span><br><span class="line">        <span class="comment"># 如果（output_hidden_states=False）则只与最后一层的状态相加。</span></span><br><span class="line">        <span class="keyword">for</span> i, layer_module <span class="keyword">in</span> enumerate(self.layer):</span><br><span class="line">            <span class="keyword">if</span> self.output_hidden_states:</span><br><span class="line">                all_hidden_states = all_hidden_states + (hidden_states,)</span><br><span class="line"></span><br><span class="line">            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask)</span><br><span class="line">            hidden_states = layer_outputs[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.output_attentions:</span><br><span class="line">                all_attentions = all_attentions + (layer_outputs[<span class="number">1</span>],)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add last layer</span></span><br><span class="line">        <span class="keyword">if</span> self.output_hidden_states:</span><br><span class="line">            all_hidden_states = all_hidden_states + (hidden_states,)</span><br><span class="line"></span><br><span class="line">        outputs = (hidden_states,)</span><br><span class="line">        <span class="keyword">if</span> self.output_hidden_states:</span><br><span class="line">            outputs = outputs + (all_hidden_states,)</span><br><span class="line">        <span class="keyword">if</span> self.output_attentions:</span><br><span class="line">            outputs = outputs + (all_attentions,)</span><br><span class="line">        <span class="keyword">return</span> outputs  <span class="comment"># last-layer hidden state, (all hidden states), (all attentions)</span></span><br></pre></td></tr></table></figure>
<h4 id="bert-layer">BertLayer</h4>
<p>BertLayer 由三部分组成：<code>BertAttention，BertIntermeidiate，BertOutput</code>。</p>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states（由上层BertLayer输出），attention_mask，head_mask</code></p>
</li>
<li>
<p>输出：<code>元组，(本层输出的隐变量，本层输出的attention)</code></p>
</li>
<li>
<p>过程:<code>调用attention得到attention_outputs -&gt; 取第一维attention_output[0]作为intermediate的参数 -&gt;调用intermediate-&gt; 调用output得到layer_output -&gt; layer_output 和 attention_outputs[1:]合并成元组返回</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.attention = BertAttention(config)</span><br><span class="line">        self.is_decoder = config.is_decoder</span><br><span class="line">        <span class="keyword">if</span> self.is_decoder:</span><br><span class="line">            self.crossattention = BertAttention(config)</span><br><span class="line">        self.intermediate = BertIntermediate(config)</span><br><span class="line">        self.output = BertOutput(config)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,)</span>:</span></span><br><span class="line">        self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)</span><br><span class="line">        attention_output = self_attention_outputs[<span class="number">0</span>]</span><br><span class="line">        outputs = self_attention_outputs[<span class="number">1</span>:]  <span class="comment"># add self attentions if we output attention weights</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.is_decoder <span class="keyword">and</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            cross_attention_outputs = self.crossattention(</span><br><span class="line">                attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask</span><br><span class="line">            )</span><br><span class="line">            attention_output = cross_attention_outputs[<span class="number">0</span>]</span><br><span class="line">            outputs = outputs + cross_attention_outputs[<span class="number">1</span>:]  <span class="comment"># add cross attentions if we output attention weights</span></span><br><span class="line"></span><br><span class="line">        intermediate_output = self.intermediate(attention_output)</span><br><span class="line">        layer_output = self.output(intermediate_output, attention_output)</span><br><span class="line">        outputs = (layer_output,) + outputs</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>从forward开始看，依次进入BertAttention，BertIntermediate和BertOutput这三个类。</p>
<h5 id="bert-attention">BertAttention</h5>
<p>这个类由两个类组成：<code>BertSelfAttention,BertSelfOutput</code>.</p>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>input_tensor(就是BertLayer的hidden_states)，mask，head_mask'</code></p>
</li>
<li>
<p>输出：<code>返回元组（attention_output，self_outputs[1:]）第一个是语义向量，第二个是概率</code></p>
</li>
<li>
<p>过程:<code>selfattention得到 self_outputs-&gt; 以self_outputs[0]作为参数调用selfoutput得到 attention_output-&gt; 返回元组（attention_output，self_outputs[1:]）第一个是语义向量，第二个是概率</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.self_att = BertSelfAttention(config)</span><br><span class="line">        self.output = BertSelfOutput(config)</span><br><span class="line">        self.pruned_heads = set()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prune_heads</span><span class="params">(self, heads)</span>:</span> <span class="comment"># </span></span><br><span class="line">        <span class="keyword">if</span> len(heads) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        mask = torch.ones(self.self_att.num_attention_heads, self.self_att.attention_head_size)</span><br><span class="line">        heads = set(heads) - self.pruned_heads  </span><br><span class="line">        <span class="keyword">for</span> head <span class="keyword">in</span> heads:</span><br><span class="line">            <span class="comment"># Compute how many pruned heads are before the head and move the index accordingly</span></span><br><span class="line">            head = head - sum(<span class="number">1</span> <span class="keyword">if</span> h &lt; head <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> h <span class="keyword">in</span> self.pruned_heads)</span><br><span class="line">            mask[head] = <span class="number">0</span></span><br><span class="line">        mask = mask.view(<span class="number">-1</span>).contiguous().eq(<span class="number">1</span>)</span><br><span class="line">        index = torch.arange(len(mask))[mask].long()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Prune linear layers</span></span><br><span class="line">        self.self_att.query = prune_linear_layer(self.self_att.query, index)</span><br><span class="line">        self.self_att.key = prune_linear_layer(self.self_att.key, index)</span><br><span class="line">        self.self_att.value = prune_linear_layer(self.self_att.value, index)</span><br><span class="line">        self.output.dense = prune_linear_layer(self.output.dense, index, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update hyper params and store pruned heads</span></span><br><span class="line">        self.self_att.num_attention_heads = self.self_att.num_attention_heads - len(heads)</span><br><span class="line">        self.self_att.all_head_size = self.self_att.attention_head_size * self.self_att.num_attention_heads</span><br><span class="line">        self.pruned_heads = self.pruned_heads.union(heads)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,)</span>:</span></span><br><span class="line">        self_outputs = self.self_att(</span><br><span class="line">            hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask</span><br><span class="line">        )</span><br><span class="line">        attention_output = self.output(self_outputs[<span class="number">0</span>], hidden_states)</span><br><span class="line">        outputs = (attention_output,) + self_outputs[<span class="number">1</span>:]  <span class="comment"># add attentions if we output them</span></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h6 id="bert-self-attention">BertSelfAttention</h6>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states(由BertLayer输出),mask，head_mask</code></p>
</li>
<li>
<p>输出：<code>返回元组（context_layer语义向量，attention_prob概率）第一个是语义向量，第二个是概率</code></p>
</li>
<li>
<p>过程:<code>self-attention 过程</code></p>
</li>
</ul>
<p><img src="/2020/05/13/research/bert_code/image-20200515100926049.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="keyword">if</span> config.hidden_size % config.num_attention_heads != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> hasattr(config, <span class="string">"embedding_size"</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">"The hidden size (%d) is not a multiple of the number of attention "</span></span><br><span class="line">                <span class="string">"heads (%d)"</span> % (config.hidden_size, config.num_attention_heads)</span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        self.output_attentions = config.output_attentions</span><br><span class="line">        <span class="comment"># 12</span></span><br><span class="line">        self.num_attention_heads = config.num_attention_heads</span><br><span class="line">        <span class="comment"># 64 = 768/12 </span></span><br><span class="line">        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)</span><br><span class="line">        <span class="comment"># 768 = 12 * 64</span></span><br><span class="line">        self.all_head_size = self.num_attention_heads * self.attention_head_size</span><br><span class="line"></span><br><span class="line">        self.query = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.key = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.value = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># x:[bs,seq_len,768]--&gt; x:[bs,seq_len, 12, 64]   multi-head</span></span><br><span class="line">        new_x_shape = x.size()[:<span class="number">-1</span>] + (self.num_attention_heads, self.attention_head_size)</span><br><span class="line">        x = x.view(*new_x_shape)</span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None, )</span>:</span></span><br><span class="line">        mixed_query_layer = self.query(hidden_states)</span><br><span class="line">    <span class="comment"># q,k,v</span></span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mixed_key_layer = self.key(encoder_hidden_states)</span><br><span class="line">            mixed_value_layer = self.value(encoder_hidden_states)</span><br><span class="line">            attention_mask = encoder_attention_mask</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mixed_key_layer = self.key(hidden_states)</span><br><span class="line">            mixed_value_layer = self.value(hidden_states)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># multi-head </span></span><br><span class="line">        query_layer = self.transpose_for_scores(mixed_query_layer) <span class="comment"># [bs,seq_len, 12, 64]</span></span><br><span class="line">        key_layer = self.transpose_for_scores(mixed_key_layer) <span class="comment"># [bs,seq_len, 12, 64]</span></span><br><span class="line">        value_layer = self.transpose_for_scores(mixed_value_layer) <span class="comment"># [bs,seq_len, 12, 64]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到权值矩阵</span></span><br><span class="line">        attention_scores = torch.matmul(query_layer, key_layer.transpose(<span class="number">-1</span>, <span class="number">-2</span>))</span><br><span class="line">        attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 用求和的方式，因为在attention_mask 中：1--&gt;0,0--&gt;-10000.</span></span><br><span class="line">            attention_scores = attention_scores + attention_mask</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对最后一个维度用 softmax 计算概率</span></span><br><span class="line">        attention_probs = nn.Softmax(dim=<span class="number">-1</span>)(attention_scores)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">        <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper .</span></span><br><span class="line">        attention_probs = self.dropout(attention_probs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="keyword">if</span> head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_probs = attention_probs * head_mask</span><br><span class="line"></span><br><span class="line">        context_layer = torch.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">        context_layer = context_layer.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        <span class="comment"># [bs,seq_len,12,64]--&gt; x:[bs,seq_len, 768]   把multi-head 再拼接回去</span></span><br><span class="line">        new_context_layer_shape = context_layer.size()[:<span class="number">-2</span>] + (self.all_head_size,)</span><br><span class="line">        context_layer = context_layer.view(*new_context_layer_shape)</span><br><span class="line"></span><br><span class="line">        outputs = (context_layer, attention_probs) <span class="keyword">if</span> self.output_attentions <span class="keyword">else</span> (context_layer,)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h6 id="bert-self-output">BertSelfOutput</h6>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states（由BertSelfAttention输出）, input_tensor（就是BertAttention的input_tensor，也就是BertSelfAttention的输入）</code></p>
</li>
<li>
<p>输出：<code>hidden_states</code></p>
</li>
<li>
<p>过程:<code>对hidden_states加一层dense -&gt; dropout -&gt; 得到的hidden_states与input_tensor相加做LayerNorm #这种做法说是为了避免梯度消失，也就是曾经的残差网络解决办法：output=output+Q</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfOutput</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, hidden_states, input_tensor)</span>:</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<h5 id="bert-intermediate">BertIntermediate</h5>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states(BertSelfOutput的输出)</code></p>
</li>
<li>
<p>输出：<code>hidden_states</code></p>
</li>
<li>
<p>过程:<code>对hidden_states加一层dense，向量输出大小为intermedia_size -&gt; 调用intermediate_act_fn，这个函数是由config.hidden_act得来，是gelu、relu、swish方法中的一个 #中间层存在的意义：推测是能够使模型从低至高学习到多层级信息，从表面信息到句法到语义。还有人研究说中间层的可迁移性更好。</code></p>
</li>
</ul>
<p>ACT2FN：激活函数:<code>{&quot;gelu&quot;: gelu, &quot;relu&quot;: torch.nn.functional.relu, &quot;swish&quot;: swish}</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertIntermediate</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)</span><br><span class="line">        <span class="keyword">if</span> isinstance(config.hidden_act, str):</span><br><span class="line">            <span class="comment"># ACT2FN = &#123;"gelu": gelu, "relu": torch.nn.functional.relu, "swish": swish&#125;</span></span><br><span class="line">            self.intermediate_act_fn = ACT2FN[config.hidden_act]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.intermediate_act_fn = config.hidden_act</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, hidden_states)</span>:</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.intermediate_act_fn(hidden_states)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<h5 id="bert-output">BertOutput</h5>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states（由BertAttention输出）, input_tensor</code></p>
</li>
<li>
<p>输出：<code>hidden_states</code></p>
</li>
<li>
<p>过程:<code>对hidden_states加一层dense ，由intermedia_size 又变回hidden_size -&gt; dropout -&gt; 得到的hidden_states与input_tensor相加做LayerNorm #这种做法说是为了避免梯度消失，也就是曾经的残差网络解决办法：output=output+Q。</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertOutput</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, hidden_states, input_tensor)</span>:</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<p>可以看到BertOutput是一个输入size是<code>config.intermediate_size</code>，输出size是<code>config.hidden_size</code>。又把size从BertIntermediate中的<code>config.intermediate_size</code>变回<code>config.hidden_size</code>。然后又接了一个Dropout和一个归一化。</p>
<h3 id="bert-pooler">BertPooler</h3>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>接收参数：<code>hidden_states（由BertAttention输出）, input_tensor</code></li>
<li>输出：<code>pooled_output</code></li>
<li>过程:<code>简单取第一个token -&gt; 加一层dense -&gt; Tanh激活函数输出</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertPooler</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.activation = nn.Tanh()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, hidden_states)</span>:</span></span><br><span class="line">        first_token_tensor = hidden_states[:, <span class="number">0</span>]</span><br><span class="line">        pooled_output = self.dense(first_token_tensor)</span><br><span class="line">        pooled_output = self.activation(pooled_output)</span><br><span class="line">        <span class="keyword">return</span> pooled_output</span><br></pre></td></tr></table></figure>
<h2 id="zong-jie">总结</h2>
<p>数据流：</p>
<ol>
<li>输入的数据首先经过<strong>BertEmbeddings</strong>类。在BertEmbeddings中将每个单词变为words_embeddings + position_embeddings +token_type_embeddings三项embeddings的和。</li>
<li>然后，把已经变为词向量的数据输入BertSelfAttention类中。BertSelfAttention类中是一个Multi-Head Attention（少一个Linear层）， 也就是说数据流入这个<strong>少一个Linear层的Multi-Head Attention</strong>。</li>
<li>之后，数据流入BertSelfOutput类。BertSelfOutput是一个Linear+Dropout+LayerNorm。<strong>补齐了BertSelfAttention中少的那个Linear层</strong>，并且进行一次LayerNorm。</li>
<li>再之后，数据经过BertIntermediate(Linear层+激活函数)和BertOutput(Linear+Dropout+LayerNorm)。这样整个Transformer的部分就算完成了。</li>
<li>最后，取出最后一层的<code>[CLS]</code>对应的向量，经过<code>(Linear+Tanh)</code>,得到pooled_out</li>
</ol>
<p>关于bert的训练优化参考<a href="https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py" target="_blank" rel="noopener">链接</a></p>
<p>其余类：</p>
<ol>
<li>
<p>BertConfig</p>
<p>保存BERT的各种参数配置</p>
</li>
<li>
<p>BertOnlyMLMHead<br>
使用mask 方法训练语言模型时用的，返回预测值<br>
过程：调用BertLMPredictionHead，返回的就是prediction_scores</p>
</li>
<li>
<p>BertLMPredictionHead<br>
decode功能<br>
过程：调用BertPredictionHeadTransform -&gt; linear层，输出维度是vocab_size</p>
</li>
<li>
<p>BertPredictionHeadTransform<br>
过程：dense -&gt; 激活(gelu or relu or swish) -&gt; LayerNorm</p>
</li>
<li>
<p>BertOnlyNSPHead<br>
NSP策略训练模型用的，返回0或1<br>
过程：添加linear层输出size是2，返回seq_relationship_score</p>
</li>
<li>
<p>BertPreTrainingHeads<br>
MLM和NSP策略都写在里面，输入的是Encoder的输出sequence_output, pooled_output<br>
返回（prediction_scores, seq_relationship_score）分别是MLM和NSP下的分值</p>
</li>
<li>
<p>BertPreTrainedModel<br>
从全局变量BERT_PRETRAINED_MODEL_ARCHIVE_MAP加载BERT模型的权重</p>
</li>
<li>
<p>BertForPreTraining<br>
计算score和loss<br>
通过BertPreTrainingHeads，得到prediction后计算loss，然后反向传播。</p>
</li>
<li>
<p>BertForMaskedLM<br>
只有MLM策略的loss</p>
</li>
<li>
<p>BertForNextSentencePrediction<br>
只有NSP策略的loss</p>
</li>
<li>
<p>BertForSequenceClassification<br>
计算句子分类任务的loss</p>
</li>
<li>
<p>BertForMultipleChoice<br>
计算句子选择任务的loss</p>
</li>
<li>
<p>BertForTokenClassification<br>
计算对token分类or标注任务的loss</p>
</li>
<li>
<p>BertForQuestionAnswering<br>
计算问答任务的loss</p>
</li>
</ol>
<h3 id="shi-yong-yu-xun-lian-mo-xing">使用预训练模型</h3>
<p>在BertModel class中有两个函数。get_pool_output表示获取每个batch第一个词的[CLS]表示结果。BERT认为这个词包含了整条语料的信息；适用于句子级别的分类问题。get_sequence_output表示BERT最终的输出结果,shape为[batch_size,seq_length,hidden_size]。可以直观理解为对每条语料的最终表示，适用于seq2seq问题。<br>
对于其它序列标注或生成任务，也可以使用 BERT 对应的输出信息作出预测，例如每一个时间步输出一个标注或词等。下图展示了 BERT 在 11 种任务中的微调方法，它们都只添加了一个额外的输出层。在下图中，Tok 表示不同的词、E 表示输入的嵌入向量、\(T_i\)表示第\(i\) 个词在经过 BERT 处理后输出的上下文向量。</p>
<p><img src="/2020/05/13/research/bert_code/bert-specific-models.png" alt></p>
<h1 id="can-kao">参考</h1>
<ol start="3">
<li><a href="http://fancyerii.github.io/2019/03/09/bert-codes/" target="_blank" rel="noopener">BERT代码阅读</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/56103665" target="_blank" rel="noopener">一起读Bert文本分类代码 </a></li>
<li><a href="https://daiwk.github.io/posts/nlp-bert.html#pytorch%E7%89%88%E6%9C%AC" target="_blank" rel="noopener">bert</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/75558363" target="_blank" rel="noopener">快速掌握BERT源代码（pytorch）</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
      <tags>
        <tag>transformer</tag>
        <tag>bert</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/2020/05/11/transformer/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/11/transformer/Cg-4jlOszDCIbmRNAAo_4amaeggAAOn7APf2hoACj_5550.jpg" alt></p>
<a id="more"></a>
<h1 id="jian-jie">简介</h1>
<p>自2017年提出以来，Transformer在众多自然语言处理问题中取得了非常好的效果。它不但训练速度更快，而且更适合建模长距离依赖关系，因此大有取代循环或卷积神经网络，一统自然语言处理的深度模型江湖之势。本文结合<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">《Attention is all you need》</a>论文与Harvard的代码<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">《Annotated Transformer》</a>深入理解transformer模型。</p>
<h1 id="transformer">Transformer</h1>
<p>Transformer的整体结构如下图所示，在Encoder和Decoder中都使用了Self-attention, Point-wise和全连接层。Encoder和decoder的大致结构分别如下图的左半部分和右半部分所示。</p>
<p><img src="/2020/05/11/transformer/over_all.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Helper: Construct a model from hyperparameters.</span></span><br><span class="line"><span class="string">    src_vocab: 输入词汇表大小</span></span><br><span class="line"><span class="string">    tgt_vocab:输出词汇表大小</span></span><br><span class="line"><span class="string">    N：堆叠个数</span></span><br><span class="line"><span class="string">    d_model:embedding_size</span></span><br><span class="line"><span class="string">    d_ff: 线形层输出维度</span></span><br><span class="line"><span class="string">    h:multi-head中 head 的个数</span></span><br><span class="line"><span class="string">    dropout：</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N), <span class="comment"># encoder</span></span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N), <span class="comment"># decoder</span></span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)), <span class="comment"># src_embed</span></span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)), <span class="comment"># tgt_embed</span></span><br><span class="line">        Generator(d_model, tgt_vocab) <span class="comment"># generator</span></span><br><span class="line">    		)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h2 id="embedding">Embedding</h2>
<p>transformer的输入是<strong>Word Embedding + Position Embedding</strong>。</p>
<h3 id="word-embedding">Word Embedding</h3>
<p>Word embedding在pytorch中通常用 nn.Embedding 实现，其权重矩阵通常有两种选择：</p>
<ol>
<li>使用 Pre-trained的<strong>Embeddings并freeze</strong>，这种情况下实际就是一个 Lookup Table。</li>
<li>对其进行随机初始化(当然也可以选择 Pre-trained 的结果)，但<strong>设为 Trainable</strong>。这样在 training 过程中不断地对 Embeddings 进行改进。</li>
</ol>
<p>transformer选择后者，代码实现如下：</p>
<figure class="highlight python"><figcaption><span>word_embedding</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model) <span class="comment"># vocab 词汇表大小</span></span><br><span class="line">        self.d_model = d_model  <span class="comment">#表示embedding的维度</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure>
<h3 id="positional-embedding">Positional Embedding</h3>
<p>在RNN中，对句子的处理是一个个word按顺序输入的。但在 Transformer 中，输入句子的所有word是同时处理的，没有考虑词的排序和位置信息。因此，Transformer 的作者提出了加入 <code>positional encoding</code>的方法来解决这个问题。<code>positional encoding</code>使得 Transformer 可以衡量 word 位置有关的信息。</p>
<p><strong>如何实现具有位置信息的encoding？</strong></p>
<p>作者提供了两种思路：</p>
<ul>
<li>通过训练学习 positional encoding 向量；</li>
<li>使用公式来计算 positional encoding向量。</li>
</ul>
<p>试验后发现两种选择的结果是相似的，所以采用了第2种方法，优点是不需要训练参数，而且即使<strong>在训练集中没有出现过的句子长度上也能用</strong>。Positional Encoding的公式如下：<br>
\[
\begin{aligned}
P E_{(p o s, 2 i)} &amp;=\sin \left(p o s / 10000^{2 i / d_{\text {model }}}\right) \\
P E_{(p o s, 2 i+1)} &amp;=\cos \left(p o s / 10000^{2 i / d_{\text {model }}}\right.
\end{aligned}
\]<br>
其中，\(pos\)指的是这个 word 在这个句子中的位置；\(2i\)指的是 embedding 词向量的偶数维度，\(2i+1\)指的是embedding 词向量的奇数维度。具体实现如下：</p>
<figure class="highlight python"><figcaption><span>PositionalEncoding</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>) <span class="comment"># [max_len, 1]</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * -(math.log(<span class="number">10000.0</span>) / d_model)) </span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term) <span class="comment"># 偶数列</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term) <span class="comment"># 奇数列</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)         <span class="comment"># [1, max_len, d_model]</span></span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe) <span class="comment"># 上述代码只需计算一次，然后放到寄存器里，随用随取。</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x) <span class="comment"># 据说dropout=0.1</span></span><br></pre></td></tr></table></figure>
<p><code>x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)</code> 这行代码表示；输入模型的整个Embedding是Word Embedding与Positional Embedding直接相加之后的结果。</p>
<p>为什么上面的两个公式能体现单词的相对位置信息呢？</p>
<p>下面一段代码取词向量的4个维度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在位置编码下方，将基于位置添加正弦波。对于每个维度，波的频率和偏移都不同。</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line">y = pe.forward(Variable(torch.zeros(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>))) <span class="comment"># [bs=1,seq_len=100,embed_size=20]</span></span><br><span class="line">plt.plot(np.arange(<span class="number">100</span>), y[<span class="number">0</span>, :, <span class="number">4</span>:<span class="number">8</span>].data.numpy())</span><br><span class="line">plt.legend([<span class="string">"dim %d"</span> %p <span class="keyword">for</span> p <span class="keyword">in</span> [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/11/transformer/position_embedding.png" alt></p>
<p>可以看到某个序列中不同位置的单词，在某一维度上的位置编码数值不一样，即同一序列的不同单词在单个纬度符合某个正弦或者余弦，可认为他们的具有相对关系。</p>
<h2 id="encoder">Encoder</h2>
<p>Encoder部分是由个层相同小Encoder Layer串联而成。小Encoder Layer可以简化为两个部分：</p>
<ol>
<li>Multi-Head Self Attention</li>
<li>Feed-Forward Network</li>
</ol>
<p><code>Multi-Head Self Attention</code> 和<code>Feed-Forward Network</code>之后都接了一层<code>Add</code> 和<code>Norm</code></p>
<p>示意图如下:</p>
<p><img src="/2020/05/11/transformer/encoder_layer.png" alt></p>
<h3 id="muti-head-attention">Muti-Head-Attention</h3>
<p>Multi-Head Self Attention 实际上是<strong>由h个Self Attention 层并行组成，原文中h=8</strong>。</p>
<h4 id="self-attention">Self-Attention</h4>
<p>self-attention的输入是序列词向量<code>x</code>。<code>x</code>经过一个线性变换得到<code>query(Q)</code>, <code>x</code>经过第二个线性变换得到<code>key(K)</code>,<code> x</code>经过第三个线性变换得到<code>value(V)</code>。也就是：</p>
<ul>
<li>Q = linear_q(x)</li>
<li>K = linear_k(x)</li>
<li>V = linear_v(x)</li>
</ul>
<p>即：</p>
<p><img src="/2020/05/11/transformer/qkv.png" alt></p>
<p>linear_k, linear_q, linear_v是相互独立、权重\(𝑊^𝑄,𝑊^𝐾,W^V\))是不同的，通过训练可得到。得到query(Q)，key(K)，value(V)之后按照下面的公式计算attention(Q, K, V)：<br>
\[
\text {Attention}(Q, K, V)=\text {Softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
\]<br>
<img src="/2020/05/11/transformer/attention.png" alt></p>
<p>这里<code>Z</code>就是<code>attention(Q, K, V)</code>，\(𝑑_𝑘=\frac{𝑑_{𝑚𝑜𝑑𝑒𝑙}}{ℎ}=\frac{512}{8}=64\)。</p>
<ol>
<li>
<p>为什么要用\(\sqrt{d_k}\) 对 \(𝑄𝐾^𝑇\)进行缩放呢？</p>
<p>\(d_k\)实际上是<code>Q/K/V</code>的最后一个维度，当\(d_k\)越大，\(QK^T\)就越大，可能会将softmax函数推入梯度极小的区域。</p>
</li>
<li>
<p>softmax之后值都介于0到1之间，可以理解成得到了 attention weights。然后基于这个 attention weights 对 V 求 weighted sum 值 Attention(Q, K, V)。</p>
</li>
</ol>
<p>Multi-Head-Attention 就是将<code>embedding</code>之后的X按维度\(𝑑_{𝑚𝑜𝑑𝑒𝑙}=512\) 切割成\(ℎ=8\)个，分别做self-attention之后再合并在一起。</p>
<p><img src="/2020/05/11/transformer/attention_1.png" alt></p>
<figure class="highlight python"><figcaption><span>Attention</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute 'Scaled Dot Product Attention</span></span><br><span class="line"><span class="string">    qkv :[batch, h, seq_len, embed_size/h(d_k)]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)  <span class="comment"># mask </span></span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)     <span class="comment"># dropout=0.1</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/11/transformer/multi_head.png" alt></p>
<figure class="highlight python"><figcaption><span>MultiHeadedAttention</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        实现MultiHeadedAttention。</span></span><br><span class="line"><span class="string">           输入的q，k，v是形状 [batch, seq_len, embed_size(d_model)]。</span></span><br><span class="line"><span class="string">           输出的x 的形状同上。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        <span class="comment">#    [batch, seq_len, embed_size] -&gt;[batch, h, seq_len, embed_size/h]</span></span><br><span class="line">        query, key, value = [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch.</span></span><br><span class="line">        <span class="comment">#    计算注意力attn 得到attn*v 与attn </span></span><br><span class="line">        <span class="comment">#    qkv :[batch, h, seq_len, embed_size/h] --&gt;</span></span><br><span class="line">        <span class="comment">#              x:[batch, h, seq_len, embed_size/h], attn[batch, h, seq_len, seq_len]</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) "Concat" using a view and apply a final linear.</span></span><br><span class="line">        <span class="comment">#     上一步的结果合并在一起还原成原始输入序列的形状</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)  <span class="comment"># 最后再过一个线性层</span></span><br></pre></td></tr></table></figure>
<h4 id="self-attention-de-you-dian">Self-Attention的优点</h4>
<ol>
<li>因为每个词都和周围所有词做attention，所以任意两个位置都相当于有直连线路，可捕获长距离依赖。</li>
<li>Attention的可解释性更好，根据Attention score可以知道一个词和哪些词的关系比较大。</li>
<li>易于并行化，当前层的Attention计算只和前一层的值有关，所以一层的所有节点可并行执行self-attention操作。计算效率高，一次Self-Attention只需要两次矩阵运算，速度很快。</li>
</ol>
<h3 id="add-amp-norm">Add &amp; Norm</h3>
<p><code>x</code> 序列经过<code>Multi-Head-Self-Attention</code> 之后实际经过一个<code>Add+Norm</code>层，再进入<code>feed-forward network(FFN)</code>，在<code>FFN</code>之后又经过一个<code>norm</code>再输入下一个encoder layer。几乎每个sub layer之后都会经过一个归一化，然后再加在原来的输入上。</p>
<figure class="highlight python"><figcaption><span>Norm</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features)) </span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features)) </span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Add</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="string">"Apply residual connection to any sublayer with the same size."</span></span><br><span class="line">        <span class="comment"># sublayer &lt;-- encoder layer</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure>
<h3 id="feed-forward-network">Feed-Forward Network</h3>
<p>Feed-Forward Network可以细分为有两层，第一层是一个线性激活函数，第二层是激活函数是ReLU。可以表示为：<br>
\[
F F N=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}
\]</p>
<figure class="highlight python"><figcaption><span>PositionwiseFeedForward</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Implements FFN equation.</span></span><br><span class="line"><span class="string">    positionwise体现每个单词上做linear，单词之间各不相同</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">      <span class="string">"""</span></span><br><span class="line"><span class="string">      d_model: embedding_size</span></span><br><span class="line"><span class="string">      d_ff: 线形层输出维度</span></span><br><span class="line"><span class="string">      """</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure>
<h3 id="zu-he-chu-encoder">组合出Encoder</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span><span class="params">(module, N)</span>:</span></span><br><span class="line">    <span class="string">"Produce N identical layers."</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)  <span class="comment"># sub layer</span></span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Pass the input (and mask) through each layer in turn."</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<h3 id="xiao-jie">小结</h3>
<p>总的来说Encoder 是由上述小encoder layer 6个串行叠加组成。encoder sub layer主要包含两个部分：</p>
<ul>
<li>SubLayer-1 做 Multi-Headed Attention</li>
<li>SubLayer-2 做 Feed Forward Neural Network</li>
</ul>
<h2 id="decoder">Decoder</h2>
<p>Decoder与Encoder有所不同，Encoder与Decoder的关系可以用下图描述：</p>
<p><img src="/2020/05/11/transformer/decoder.png" alt></p>
<figure class="highlight python"><figcaption><span>Decoder</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Generic N layer decoder with masking."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span> </span><br><span class="line">      	<span class="comment"># memory Encoder最后的输出。 </span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<p>Decoder 子结构（sub layer）：</p>
<p><img src="/2020/05/11/transformer/decoder_layer.png" alt></p>
<p>Decoder 也是N=6层堆叠的结构。被分为3个 SubLayer，Encoder与Decoder有三大主要的不同：</p>
<ol>
<li>Decoder SubLayer-1 使用的是 “Masked” Multi-Headed Attention 机制，防止为了模型看到要预测的数据，防止泄露。</li>
<li>SubLayer-2 是一个 Encoder-Decoder Multi-head Attention。</li>
<li>LinearLayer 和 SoftmaxLayer 作用于 SubLayer-3 的输出后面，来预测对应的 word 的 probabilities 。</li>
</ol>
<h3 id="mask-multi-head-attention">Mask-Multi-Head-Attention</h3>
<p>Mask 的目的是防止 Decoder “seeing the future”，防止提前知道预测的内容（ensures that the predictions for position \(i\) can depend only on the known outputs at positions less than \(i\).）,这也说明Transformer只是在Encoder阶段可以并行化，Decoder阶段需要一个个词顺序翻译，依然是<strong>串行</strong>的(参考Greedy Decoding代码部分。这里mask是一个下三角矩阵，对角线以及对角线左下都是1，其余都是0。</p>
<figure class="highlight python"><figcaption><span>subsequent_mask</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="string">""""Mask out subsequent positions.</span></span><br><span class="line"><span class="string">    mask后续的位置，返回[size, size]尺寸下三角Tensor</span></span><br><span class="line"><span class="string">    对角线及其左下角全是1，右上角全是0</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    <span class="comment"># np.triu()  Upper triangle of an array: 返回矩阵上三角</span></span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span> <span class="comment"># 等于0，返回的是矩阵的下三角</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.imshow(subsequent_mask(<span class="number">20</span>)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="/2020/05/11/transformer/subsequent_mask.png" alt></p>
<p>subsequent_mask 返回结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]], dtype=torch.uint8)</span><br></pre></td></tr></table></figure>
<p>当mask不为空的时候，attention计算需要将x做一个操作：scores = scores.masked_fill(mask == 0, -1e9)。即将mask==0的替换为-1e9,其余不变。</p>
<h3 id="encoder-decoder-multi-head-attention">Encoder-Decoder Multi-head Attention</h3>
<p>这部分和Multi-head Attention的区别是该层的输入来自encoder和上一次decoder的结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (right) for connections."</span></span><br><span class="line">      	<span class="comment"># 将decoder的三个Sublayer串联起来</span></span><br><span class="line">        m = memory <span class="comment"># 为encoder 最后的输出</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>
<h3 id="linear-and-softmax-to-produce-output-probabilities">Linear and Softmax to Produce Output Probabilities</h3>
<p>Decoder的最后一个部分是过一个linear layer将decoder的输出扩展到与vocabulary size一样的维度上。经过softmax 后，选择概率最高的一个word作为预测结果。假设我们有一个已经训练好的网络，在做预测时，步骤如下：</p>
<ol>
<li>给 decoder 输入 encoder 对整个句子 embedding 的结果和一个特殊的开始符号 。decoder 将产生预测，在例子中应该是 <code>I</code>。</li>
<li>给 decoder 输入 encoder 的 embedding 结果和 “ I”，在这一步 decoder 应该产生预测 <code>am</code>。</li>
<li>给 decoder 输入 encoder 的 embedding 结果和 “ I am”，在这一步 decoder 应该产生预测 <code>a</code>。</li>
<li>给 decoder 输入 encoder 的 embedding 结果和 “ I am a”，在这一步 decoder 应该产生预测 <code>student</code>。</li>
<li>给 decoder 输入 encoder 的 embedding 结果和 “I am a student”, decoder应该生成句子结尾的标记，decoder 应该输出 <code>&lt;/eos&gt;</code>。</li>
<li>然后 decoder 生成了 ，翻译完成。</li>
</ol>
<p>这部分的代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Define standard linear + softmax generation step."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">    		<span class="string">"""</span></span><br><span class="line"><span class="string">    		d_model:decoder 输出的embedding size</span></span><br><span class="line"><span class="string">    		vocab：目标的词汇表大小</span></span><br><span class="line"><span class="string">    		"""</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="encoder-decoder">EncoderDecoder</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many </span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed <span class="comment"># embedding</span></span><br><span class="line">        self.tgt_embed = tgt_embed <span class="comment"># embedding</span></span><br><span class="line">        self.generator = generator <span class="comment"># 用于生成翻译目标（Linear + softmax --&gt; prob）</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Take in and process masked src and target sequences."</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure>
<h2 id="full-model">Full Model</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout) <span class="comment"># linear(relu(linear(x)))</span></span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h1 id="training">Training</h1>
<p>这部分主要根据自己的理解对代码加注释</p>
<h2 id="batches-and-masking">Batches and Masking</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Batch</span>:</span></span><br><span class="line">    <span class="string">"Object for holding a batch of data with mask during training."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, src, trg=None, pad=<span class="number">0</span>)</span>:</span></span><br><span class="line">        self.src = src</span><br><span class="line">        self.src_mask = (src != pad).unsqueeze(<span class="number">-2</span>) <span class="comment"># 对超出句子长度部分mask</span></span><br><span class="line">        <span class="keyword">if</span> trg <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.trg = trg[:, :<span class="number">-1</span>]</span><br><span class="line">            self.trg_y = trg[:, <span class="number">1</span>:]</span><br><span class="line">            self.trg_mask = self.make_std_mask(self.trg, pad) <span class="comment"># mask to hide padding and future words</span></span><br><span class="line">            self.ntokens = (self.trg_y != pad).data.sum()</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_std_mask</span><span class="params">(tgt, pad)</span>:</span></span><br><span class="line">        <span class="string">"Create a mask to hide padding and future words."</span></span><br><span class="line">        tgt_mask = (tgt != pad).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">        tgt_mask = tgt_mask &amp; Variable(</span><br><span class="line">            subsequent_mask(tgt.size(<span class="number">-1</span>)).type_as(tgt_mask.data))</span><br><span class="line">        <span class="keyword">return</span> tgt_mask</span><br></pre></td></tr></table></figure>
<h2 id="training-loop">Training Loop</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(data_iter, model, loss_compute)</span>:</span></span><br><span class="line">    <span class="string">"Standard Training and Logging Function"</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    total_tokens = <span class="number">0</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    tokens = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(data_iter):</span><br><span class="line">        out = model.forward(batch.src, batch.trg, </span><br><span class="line">                            batch.src_mask, batch.trg_mask)</span><br><span class="line">        loss = loss_compute(out, batch.trg_y, batch.ntokens)</span><br><span class="line">        total_loss += loss</span><br><span class="line">        total_tokens += batch.ntokens</span><br><span class="line">        tokens += batch.ntokens</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">1</span>:</span><br><span class="line">            elapsed = time.time() - start</span><br><span class="line">            print(<span class="string">"Epoch Step: %d Loss: %f Tokens per Sec: %f"</span> %</span><br><span class="line">                    (i, loss / batch.ntokens, tokens / elapsed))</span><br><span class="line">            start = time.time()</span><br><span class="line">            tokens = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> total_loss / total_tokens</span><br></pre></td></tr></table></figure>
<h2 id="training-data">Training Data</h2>
<p>使用标准WMT 2014英语-德语数据集进行了训练，该数据集包含大约450万个句子对。 使用字节对的编码方法对句子进行编码，该编码具有大约37000个词的共享源-目标词汇表。 对于英语-法语，使用了WMT 2014 英语-法语数据集，该数据集由36M个句子组成，并将词分成32000个词片(Word-piece)的词汇表。句子对按照近似的序列长度进行批处理。每个训练批包含一组句子对，包含大约25000个源词和25000个目标词。</p>
<p>使用torch text来创建batch。在torchtext的一个函数中创建batch，确保填充到最大batch训练长度的大小不超过阈值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">global</span> max_src_in_batch, max_tgt_in_batch</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_size_fn</span><span class="params">(new, count, sofar)</span>:</span></span><br><span class="line">    <span class="string">"Keep augmenting batch and calculate total number of tokens + padding."</span></span><br><span class="line">    <span class="keyword">global</span> max_src_in_batch, max_tgt_in_batch</span><br><span class="line">    <span class="keyword">if</span> count == <span class="number">1</span>: </span><br><span class="line">        max_src_in_batch = <span class="number">0</span></span><br><span class="line">        max_tgt_in_batch = <span class="number">0</span></span><br><span class="line">    max_src_in_batch = max(max_src_in_batch,  len(new.src))</span><br><span class="line">    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + <span class="number">2</span>)</span><br><span class="line">    src_elements = count * max_src_in_batch</span><br><span class="line">    tgt_elements = count * max_tgt_in_batch</span><br><span class="line">    <span class="keyword">return</span> max(src_elements, tgt_elements)</span><br></pre></td></tr></table></figure>
<h2 id="optimizer">Optimizer</h2>
<p>选择Adam作为优化器，其参数为\(\beta_1 = 0.9, \beta_2=0.98,\epsilon=10^{-9}\)。根据<br>
\[
lrate = d_{model}^{- \frac{1}{2}} * min(step\_num^{- \frac{1}{2}},step\_num \cdot warmup\_steps^{-1.5})
\]<br>
在训练过程中改变了学习率。在<code>warm_up</code>中随步数线性地增加学习速率，随后与步数的反平方根成比例地减小它。预热<code>warmup_steps</code>为4000。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NoamOpt</span>:</span></span><br><span class="line">    <span class="string">"Optim wrapper that implements rate."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model_size, factor, warmup, optimizer)</span>:</span></span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self._step = <span class="number">0</span></span><br><span class="line">        self.warmup = warmup</span><br><span class="line">        self.factor = factor</span><br><span class="line">        self.model_size = model_size</span><br><span class="line">        self._rate = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"Update parameters and rate"</span></span><br><span class="line">        self._step += <span class="number">1</span></span><br><span class="line">        rate = self.rate()</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.optimizer.param_groups:</span><br><span class="line">            p[<span class="string">'lr'</span>] = rate</span><br><span class="line">        self._rate = rate</span><br><span class="line">        self.optimizer.step()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rate</span><span class="params">(self, step = None)</span>:</span></span><br><span class="line">        <span class="string">"Implement `lrate` above"</span></span><br><span class="line">        <span class="keyword">if</span> step <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            step = self._step</span><br><span class="line">        <span class="comment"># 对应上面公式</span></span><br><span class="line">        <span class="keyword">return</span> self.factor * (self.model_size ** (<span class="number">-0.5</span>) * min(step ** (<span class="number">-0.5</span>), step * self.warmup ** (<span class="number">-1.5</span>)))</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_std_opt</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">2</span>, <span class="number">4000</span>,torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">opts = [NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="literal">None</span>), </span><br><span class="line">        NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">8000</span>, <span class="literal">None</span>),</span><br><span class="line">        NoamOpt(<span class="number">256</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="literal">None</span>)]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">20000</span>), [[opt.rate(i) <span class="keyword">for</span> opt <span class="keyword">in</span> opts] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">20000</span>)])</span><br><span class="line">plt.legend([<span class="string">"512:4000"</span>, <span class="string">"512:8000"</span>, <span class="string">"256:4000"</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/11/transformer/warm_up.png" alt></p>
<p>也就是说：</p>
<ol>
<li>在embedding size相同的情况下，warm up步数越少，前期的学习率曲线越陡峭，学的越快。</li>
<li>在warm up步数相同的时候，embedding size 越小，前期的学习率曲线越陡峭，学的越快。</li>
</ol>
<h2 id="regularization">Regularization</h2>
<h3 id="label-smoothing">Label Smoothing</h3>
<h4 id="bei-jing-jie-shao">背景介绍</h4>
<p>在多分类训练任务中，输入经过神级网络的计算，会得到当前输入对应于各个类别的置信度分数，这些分数会被softmax进行归一化处理，最终得到当前输入属于每个类别的概率。<br>
\[
q_{i}=\frac{\exp \left(z_{i}\right)}{\sum_{j=1}^{K} \exp \left(z_{j}\right)}
\]<br>
之后在使用交叉熵函数来计算损失值：<br>
\[
\begin{aligned}
&amp;L o s s=-\sum_{i=1}^{K} p_{i} \log q_{i}\\
&amp;p_{i}=\left\{\begin{array}{l}
1, \text { if }(i=y) \\
0, i f(i \neq y)
\end{array}\right.
\end{aligned}
\]<br>
其中，i表示多类中的某一类。</p>
<p>最终在训练网络时，最小化预测概率和标签真实概率的交叉熵，从而得到最优的预测概率分布。在此过程中，为了达到最好的拟合效果，最优的预测概率分布为：<br>
\[
Z_{i}=\left\{\begin{array}{l}
+\infty, i f(i=y) \\
0, i f(i \neq y)
\end{array}\right.
\]<br>
也就是说，网络会驱使自身往正确标签和错误标签差值大的方向学习，在训练数据不足以表征所以的样本特征的情况下，就<strong>会导致网络过拟合</strong>。</p>
<h4 id="label-smoothing-yuan-li">label smoothing原理</h4>
<p>label smoothing的提出就是为了解决上述问题，是一种正则化的策略。其通过&quot;软化&quot;传统的one-hot类型标签，使得在计算损失值时能够有效抑制过拟合现象。label smoothing相当于减少真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。</p>
<p>1.label smoothing将真实概率分布作如下改变：<br>
\[
P_{i}=\left\{\begin{array}{l}
1, i f(i=y) \\
0, i f(i \neq y)
\end{array} \Longrightarrow  P_{i}=\left\{\begin{array}{l}
(1-\varepsilon), i f(i=y) \\
\frac{\varepsilon}{K-1}, i f(i \neq y)
\end{array}\right.\right.
\]<br>
其实更新后的分布就相当于往真实分布中加入了噪声，为了便于计算，该噪声服从简单的均匀分布。</p>
<p>2.与之对应，label smoothing将交叉熵损失函数作如下改变：<br>
\[
L o s s=-\sum_{i=1}^{K} p_{i} \log q_{i} \Longrightarrow \operatorname{Loss}_{i}=\left\{\begin{array}{l}
(1-\varepsilon)^{*} \operatorname{Loss}, \text {if}(i=y) \\
\varepsilon^{*} \operatorname{Loss}, \text {if}(i \neq y)
\end{array}\right.
\]<br>
3.与之对应，label smoothing将最优的预测概率分布作如下改变：<br>
\[
Z_{i}=\left\{\begin{array}{l}
+\infty, i f(i=y) \\
0, i f(i \neq y)
\end{array} \Longrightarrow Z_{i}=\left\{\begin{array}{l}
\log \frac{(k-1)(1-\varepsilon)}{\varepsilon+\alpha}, i f(i=y) \\
\alpha, i f(i \neq y)
\end{array}\right.\right.
\]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelSmoothing</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement label smoothing."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, padding_idx, smoothing=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(size_average=<span class="literal">False</span>)</span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, target)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>)) <span class="comment"># 减2，是减去目标序列的&lt;s&gt; &lt;e&gt;标识位置。</span></span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)</span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, Variable(true_dist, requires_grad=<span class="literal">False</span>))</span><br></pre></td></tr></table></figure>
<p>在训练期间，采用了值 \(\epsilon_{ls}=0.1\)的标签平滑。 这种做法提高了困惑度，因为模型变得更加不确定，但提高了准确性和BLEU分数。使用KL div loss实现标签平滑。 相比使用独热目标分布，其包含正确单词的置信度和整个词汇表中分布的其余平滑项。可以看到标签平滑的示例:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Example of label smoothing.</span></span><br><span class="line"><span class="comment"># embed_size = 5, padding_idx=0,smoothing=0.4</span></span><br><span class="line">crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.4</span>)</span><br><span class="line">predict = torch.FloatTensor([[<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">                             [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>], </span><br><span class="line">                             [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>]])</span><br><span class="line">v = crit(Variable(predict.log()), </span><br><span class="line">         Variable(torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the target distributions expected by the system.</span></span><br><span class="line">plt.imshow(crit.true_dist)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/11/transformer/label_smooth.png" alt></p>
<p>以紫色为例：0号标签应该以<code>-0.5</code>为目标，<code>1,2,3,4</code>以<code>1.5</code>为目标</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(x)</span>:</span></span><br><span class="line">    d = x + <span class="number">3</span> * <span class="number">1</span></span><br><span class="line">    predict = torch.FloatTensor([[<span class="number">0</span>, x / d, <span class="number">1</span> / d, <span class="number">1</span> / d, <span class="number">1</span> / d],</span><br><span class="line">                                 ])</span><br><span class="line">    <span class="comment">#print(predict)</span></span><br><span class="line">    <span class="keyword">return</span> crit(Variable(predict.log()),</span><br><span class="line">                 Variable(torch.LongTensor([<span class="number">1</span>]))).data[<span class="number">0</span>]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">100</span>), [loss(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>)])</span><br></pre></td></tr></table></figure>
<p>如果对给定的选择非常有信心，标签平滑实际上会开始惩罚模型。</p>
<p><img src="/2020/05/11/transformer/penate.png" alt></p>
<h2 id="examples">Examples</h2>
<h3 id="copy-task">Copy Task</h3>
<h4 id="synthetic-data">Synthetic Data</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_gen</span><span class="params">(V, batch, nbatches)</span>:</span></span><br><span class="line">    <span class="string">"Generate random data for a src-tgt copy task."</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nbatches):</span><br><span class="line">        data = torch.from_numpy(np.random.randint(<span class="number">1</span>, V, size=(batch, <span class="number">10</span>)))</span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        src = Variable(data, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        tgt = Variable(data, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> Batch(src, tgt, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="loss-computation">Loss Computation</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleLossCompute</span>:</span></span><br><span class="line">    <span class="string">"A simple loss compute and train function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, generator, criterion, opt=None)</span>:</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        self.criterion = criterion</span><br><span class="line">        self.opt = opt</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x, y, norm)</span>:</span></span><br><span class="line">        x = self.generator(x)</span><br><span class="line">        loss = self.criterion(x.contiguous().view(<span class="number">-1</span>, x.size(<span class="number">-1</span>)), </span><br><span class="line">                              y.contiguous().view(<span class="number">-1</span>)) / norm</span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">if</span> self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.opt.step()</span><br><span class="line">            self.opt.optimizer.zero_grad()</span><br><span class="line">        <span class="keyword">return</span> loss.data[<span class="number">0</span>] * norm</span><br></pre></td></tr></table></figure>
<h4 id="span-id-greedy-greedy-decoding-span"><span id="greedy">Greedy Decoding </span></h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Train the simple copy task.</span></span><br><span class="line">V = <span class="number">11</span></span><br><span class="line">criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line">model = make_model(V, V, N=<span class="number">2</span>)</span><br><span class="line">model_opt = NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">1</span>, <span class="number">400</span>,</span><br><span class="line">        torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    run_epoch(data_gen(V, <span class="number">30</span>, <span class="number">20</span>), model, </span><br><span class="line">              SimpleLossCompute(model.generator, criterion, model_opt))</span><br><span class="line">    model.eval()</span><br><span class="line">    print(run_epoch(data_gen(V, <span class="number">30</span>, <span class="number">5</span>), model, </span><br><span class="line">                    SimpleLossCompute(model.generator, criterion, <span class="literal">None</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greedy_decode</span><span class="params">(model, src, src_mask, max_len, start_symbol)</span>:</span></span><br><span class="line">    memory = model.encode(src, src_mask)</span><br><span class="line">    ys = torch.ones(<span class="number">1</span>, <span class="number">1</span>).fill_(start_symbol).type_as(src.data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_len<span class="number">-1</span>):</span><br><span class="line">        out = model.decode(memory, src_mask, </span><br><span class="line">                           Variable(ys), </span><br><span class="line">                           Variable(subsequent_mask(ys.size(<span class="number">1</span>))</span><br><span class="line">                                    .type_as(src.data)))</span><br><span class="line">        prob = model.generator(out[:, <span class="number">-1</span>])</span><br><span class="line">        _, next_word = torch.max(prob, dim = <span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat([ys, </span><br><span class="line">                        torch.ones(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ys</span><br><span class="line"></span><br><span class="line">model.eval()</span><br><span class="line">src = Variable(torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]]) )</span><br><span class="line">src_mask = Variable(torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>) )</span><br><span class="line">print(greedy_decode(model, src, src_mask, max_len=<span class="number">10</span>, start_symbol=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>result</span></figcaption><table><tr><td class="code"><pre><span class="line">  <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span>     <span class="number">4</span>     <span class="number">5</span>     <span class="number">6</span>     <span class="number">7</span>     <span class="number">8</span>     <span class="number">9</span>    <span class="number">10</span></span><br><span class="line">[torch.LongTensor of size <span class="number">1</span>x10]</span><br></pre></td></tr></table></figure>
<p>翻译的例子涉及GPU并行比较复杂，不做介绍。</p>
<h1 id="attention-visualization">Attention Visualization</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tgt_sent = trans.split()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(data, x, y, ax)</span>:</span></span><br><span class="line">    seaborn.heatmap(data, </span><br><span class="line">                    xticklabels=x, square=<span class="literal">True</span>, yticklabels=y, vmin=<span class="number">0.0</span>, vmax=<span class="number">1.0</span>, </span><br><span class="line">                    cbar=<span class="literal">False</span>, ax=ax)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    print(<span class="string">"Encoder Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.encoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data, sent, sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    print(<span class="string">"Decoder Self Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.decoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data[:len(tgt_sent), :len(tgt_sent)], tgt_sent, tgt_sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"Decoder Src Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.decoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data[:len(tgt_sent), :len(sent)],sent, tgt_sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="encoder-visualization">Encoder visualization</h2>
<div class="note info">
            <p>Encoder Layer 2</p>
          </div>
<p><img src="/2020/05/11/transformer/encoder_layer_2.png" alt></p>
<div class="note info">
            <p>Encoder Layer 2</p>
          </div>
<p><img src="/2020/05/11/transformer/encoder_layer_4.png" alt></p>
<div class="note info">
            <p>Encoder Layer 6</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511115252714.png" alt></p>
<p>同一行，比较不同的head,可以看出，不同的head，attention到的内容是各不相同的。</p>
<h2 id="decoder-visualization">Decoder visualization</h2>
<div class="note info">
            <p>decoder Self Layer 2 :<code>&lt;s&gt;</code>会attention到所有单词，单词大多会attention到自己。</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511120009561.png" alt></p>
<div class="note info">
            <p>decoder Src Layer 2</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511120705604.png" alt></p>
<div class="note info">
            <p>decoder Self Layer 4</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511120927309.png" alt></p>
<div class="note info">
            <p>decoder Src Layer 4</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511120946538.png" alt></p>
<div class="note info">
            <p>decoder Self Layer 6</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511121002344.png" alt></p>
<div class="note info">
            <p>decoder Src Layer 6</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511121021648.png" alt></p>
<p>上三角全黑：表示decoder是一个串行计算，每一个词只会attention到其前面的词。</p>
<h1 id="zong-jie">总结</h1>
<h2 id="tricks">tricks</h2>
<p>在训练过程中，模型没有收敛得很好时，Decoder预测产生的词很可能不是我们想要的。这个时候如果再把错误的数据再输给Decoder，就会越跑越偏。这个时候怎么办？</p>
<ul>
<li>在训练过程中可以使用 “teacher forcing”。因为我们知道应该预测的word是什么，那么可以给Decoder喂一个正确的结果作为输入。</li>
<li>除了选择最高概率的词 (greedy search)，还可以选择是比如 “Beam Search”，可以保留topK个预测的word。 Beam Search 方法不再是只得到一个输出放到下一步去训练了，我们可以设定一个值，拿多个值放到下一步去训练，这条路径的概率等于每一步输出的概率的乘积。</li>
</ul>
<h2 id="transformer-de-you-que-dian">Transformer的优缺点</h2>
<h3 id="you-dian">优点</h3>
<ol>
<li>每层计算复杂度比RNN要低。</li>
<li>可以进行<strong>并行计算</strong>。</li>
<li>从计算一个序列长度为n的信息要经过的路径长度来看, CNN需要增加卷积层数来扩大视野，RNN需要从1到n逐个进行计算，而Self-attention只需要一步矩阵计算就可以。Self-Attention可以比RNN更好地解决长时依赖问题。当然如果计算量太大，比如序列长度N大于序列维度D这种情况，也可以用窗口限制Self-Attention的计算数量。</li>
<li>从作者在附录中给出的栗子可以看出，Self-Attention模型更可解释，Attention结果的分布表明了该模型学习到了一些语法和语义信息。</li>
</ol>
<h3 id="que-dian">缺点</h3>
<ol>
<li>实践上：有些RNN轻易可以解决的问题transformer没做到，比如<strong>复制string</strong>，或者推理时碰到的sequence长度比训练时更长（因为<strong>碰到了没见过的position embedding</strong>）。</li>
<li>理论上：transformers不是computationally universal(图灵完备)，这种非RNN式的模型是非图灵完备的的，<strong>无法单独完成NLP中推理、决策等计算问题</strong>（包括使用transformer的bert模型等等）。</li>
</ol>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">The Annotated Transformer</a></li>
<li><a href="https://www.cnblogs.com/zingp/p/11696111.html" target="_blank" rel="noopener">深入理解Transformer及其源码</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>平滑方法</title>
    <url>/2020/05/08/%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://protao.github.io/img/2gramsmooth.png" alt></p>
<a id="more"></a>
<h3 id="jian-jie">简介</h3>
<p>在一个概率模型中，也就是 p(e)在 event space E 下的概率分布，模型很可能会用最大似然估计(MLE)：<br>
\[
P_{M L E}=\frac{c(x)}{\sum_{e} c(e)}
\]<br>
然而，由于并没有足够的数据，很多事件 \(x\) 并没有在训练数据中出现，也就是 \(c(x)=0\)，\(P_{MLE}=0\)这是有问题的，没有在训练数据中出现的数据，并不代表不会在测试数据中出现，如果没有考虑到数据稀疏性，模型就显得太简单了。</p>
<p>Data sparsity 是 smoothing 的最大原因。Chen &amp; Goodman 在1998 年提到过，几乎所有数据稀疏的场景下，smoothing 都可以帮助提高 performance，而数据稀疏性几乎是所有统计模型都会遇到的问题。而如果你有足够多的训练数据，所有的 parameters 都可以在没有 smoothing 的情况下被准确的估计，那么你总是可以扩展模型，如原来是 bigram，没有数据稀疏，完全可以扩展到 trigram 来提高 performance，如果还没有出现稀疏，就再往高层推，当 parameters 越来越多的时候，数据稀疏再次成为了问题，这时候，用合适的平滑方法可以得到更准确的模型。实际上，无论有多少的数据，平滑几乎总是可以以很小的代价来提高 performance。</p>
<h3 id="ping-hua-fang-fa">平滑方法</h3>
<h4 id="add-one-smoothing">add-one smoothing</h4>
<p>(也叫laplace smoothing),以bigram为例：</p>
<p>MLE estimate：<br>
\[
P_{M L E}\left(w_{i} | w_{i-1}\right)=\frac{c\left(w_{i-1} w_{i}\right)}{c\left(w_{i-1}\right)}
\]<br>
Add-one estimate:<br>
\[
P_{A d d-1}\left(w_{i} | w_{i-1}\right)=\frac{c\left(w_{i-1} w_{i}\right)+1}{c\left(w_{i-1}\right)+V}
\]<br>
其中，\(V\)表示词典大小。</p>
<p>假设语料库为：</p>
<blockquote>
<ol>
<li>john read moby dick</li>
<li>mary read a different book</li>
<li>she read a book by cher</li>
</ol>
</blockquote>
<p>那么，john read a book 这个句子的概率为：</p>
<p>\[
\begin{aligned}
p(\text { john read a book })&amp;=p(\text { john } |&lt;s>)  p(\text { read } | \text { john })  p(\text { a } | \text { read })  p(\text { book } | a )  p(&lt;e> | \text {book}) \\
&amp;=\frac{c(&lt;s>,john)}{\sum_{w} c(&lt;s>,w)} \frac{c(john,read)}{\sum_{w} c(john,w)} \frac{c(read,a)}{\sum_{w} c(read,w)} \frac{c(a,book)}{\sum_{w} c(a,w)} \frac{c(book,&lt;e>)}{\sum_{w} c(book,w)}\\
&amp;= \frac{1}{3} * \frac{1}{1} * \frac{2}{3} * \frac{1}{2} * \frac{1}{2} \\
&amp;\approx 0.006
\end{aligned}
\]</p>
<p>而，cher read a book 这个句子出现的概率是：</p>
<p>\[
\begin{aligned}
p(\text { cher read a book })&amp;=p(\text { cher } |&lt;s>)  p(\text { read } | \text { cher })  p(\text { a } | \text { read })  p(\text { book } | a )  p(&lt;e> | \text {book}) \\
&amp;=\frac{c(&lt;s>,cher)}{\sum_{w} c(&lt;s>,w)} \frac{c(cher,read)}{\sum_{w} c(cher,w)} \frac{c(read,a)}{\sum_{w} c(read,w)} \frac{c(a,book)}{\sum_{w} c(a,w)} \frac{c(book,&lt;e>)}{\sum_{w} c(book,w)}\\
&amp;= \frac{0}{3} * \frac{0}{1} * \frac{2}{3} * \frac{1}{2} * \frac{1}{2} \\
&amp;\approx 0
\end{aligned}
\]<br>
如果使用add-one smoothing：<br>
\[
\begin{aligned}
p(\text { john read a book })&amp;=p(\text { john } |&lt;s>)  p(\text { read } | \text { john })  p(\text { a } | \text { read })  p(\text { book } | a )  p(&lt;e> | \text {book}) \\
&amp;= \frac{1+1}{3+11} * \frac{1+1}{1+11} * \frac{2+1}{3+11} * \frac{1+1}{2+11} * \frac{1+1}{2+11} \\
&amp;\approx 0.0001
\end{aligned}
\]</p>
<p>\[
\begin{aligned}
p(\text { cher read a book })&amp;=p(\text { cher } |&lt;s>)  p(\text { read } | \text { cher })  p(\text { a } | \text { read })  p(\text { book } | a )  p(&lt;e> | \text {book}) \\
&amp;= \frac{0+1}{3+11} * \frac{0+1}{1+11} * \frac{2+1}{3+11} * \frac{1+1}{2+11} * \frac{1+1}{2+11} \\
&amp;\approx 0.00003
\end{aligned}
\]</p>
<p>加 1 平滑通常情况下是一种很糟糕的算法，与其他平滑方法相比显得非常差，可以把加 1 平滑用在其他任务中，如文本分类，或者非零计数没那么多的情况下。</p>
<h4 id="additive-smoothing">Additive smoothing</h4>
<p>对加 1 平滑的改进就是把 1 改成 \(\delta\)，且 \(0&lt;\delta\)<br>
\[
P_{A d d-\delta}\left(w_{i} | w_{i-1}\right)=\frac{c\left(w_{i-1} w_{i}\right)+\delta}{c\left(w_{i-1}\right)+\delta V}
\]<br>
可以把\(\delta\)看作是一个可以调节的超参数。</p>
<h4 id="good-turing-smoothing">Good-Turing smoothing</h4>
<p><strong>基本思想:</strong> 用观察计数较高的 N-gram 数量来重新估计概率量大小，并把它指派给那些具有零计数或较低计数的 N-gram.</p>
<p>举个例子：假设你在钓鱼，然后抓到了 18 条鱼，种类如下：10条鲤鱼, 3条黑鱼, 2条刀鱼, 1条鲨鱼, 1条草鱼, 1条鳗鱼，那么</p>
<ol>
<li>
<p>下一个钓到的鱼是鲨鱼的概率是多少？</p>
<p>1/18</p>
</li>
<li>
<p>下一条与是新鱼种（之前没有出现过）的概率是多少？</p>
<p>3/18(没有出现过的按出现过一次算)</p>
</li>
<li>
<p>下一条抓到的鱼是鲨鱼的概率是多少？</p>
<p>首先肯定的是概率小于1/18  --&gt; good turing</p>
</li>
</ol>
<p>在good turing 下,对参数的估计分两种情况：</p>
<ol>
<li>没有出现过的：\(P_{GT}=\frac{N_1}{N}\)</li>
<li>出现过的：\(P_{GT}=\frac{(c+1)N_{c+1}}{N_c*N}\)</li>
</ol>
<p>其中，\(N_c\)表示出现c次的单词个数,\(N\)代表样本总数。<br>
比如：</p>
<blockquote>
<p>sam i am i am sam i do not eat</p>
<p>sam：2次</p>
<p>i：3次</p>
<p>am：2次</p>
<p>do：1次</p>
<p>not：1次</p>
<p>eat：1次</p>
</blockquote>
<p>有：\(N_3=1,N_2=2,N_1=3\)</p>
<p>对于第3个问题：\(P_{GT}=\frac{(1+1)*1}{3*18}=\frac{1}{27}\).</p>
<h5 id="que-dian">缺点</h5>
<p>由公式：\(P_{GT}=\frac{(c+1)N_{c+1}}{N_c*N}\)在计算当前项\(c\) 的时候，需要计算到\(c+1\)的\(N_{c+1}\),若\(N_{c+1}=0\)，则会导致概率为0.</p>
<p>解决方法：可以利用机器学习中回归算法对\(N_1,N_2,\ldots,N_c,N_{c+1}\)进行拟合，进而得出\(N_{c+1}\)的值。</p>
<h4 id="interpolation">Interpolation</h4>
<p><strong>基本思想</strong>：在计算Trigram概率的同时，考虑Unigram，Bigram，Trigram出现的次数。</p>
<p>举个例子：有</p>
<p><code>C(in the kitchen)=0</code>,<code>C(the kitchen)=3</code>,<code>C(kitchen)=4</code>,<code>C(arboretum)=0</code>.在计算<code>p(kitchen|in the)</code>和<code>p(arboretum|in the)</code>的时候，从语料计算得出：\(p(kitchen|in,the)=p(arboretum|in,the)=0\),但是从经验上来讲\(p(kitchen|in,the)>p(arboretum|in,the)\)，因为kitchen要比arboretum常见的多。要实现这个，我们就希望把 bigram 和 unigram 结合起来，interpolate 就是这样一种方法。用线性差值把不同阶的 N-gram 结合起来，这里结合了 trigram，bigram 和 unigram。用 lambda 进行加权<br>
\[
\begin{aligned}
\mathrm{p}\left(\mathrm{w}_{\mathrm{n}} | \mathrm{w}_{\mathrm{n}-1}, \mathrm{w}_{\mathrm{n}-2}\right)=&amp; \lambda_{1} \mathrm{p}\left(\mathrm{w}_{\mathrm{n}} | \mathrm{w}_{\mathrm{n}-1}, \mathrm{w}_{\mathrm{n}-2}\right) \\
&amp;+\lambda_{2} \mathrm{p}\left(\mathrm{w}_{\mathrm{n}} | \mathrm{w}_{\mathrm{n}-1}\right) \\
&amp;+\lambda_{3} \mathrm{p}\left(\mathrm{w}_{\mathrm{n}}\right)\\
\sum_i\lambda_i=1
\end{aligned}
\]</p>
<h5 id="zen-yang-she-zhi-lambdas">怎样设置 lambdas？？？？</h5>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理基础</tag>
      </tags>
  </entry>
  <entry>
    <title>分词算法</title>
    <url>/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/2.png" alt></p>
<a id="more"></a>
<h3 id="fen-ci-jian-jie">分词简介</h3>
<p>中文分词算法是指将一个汉字序列切分成一个一个单独的词，与英文以空格作为天然的分隔符不同，中文字符在语义识别时，需要把数个字符组合成词，才能表达出真正的含义。分词算法通常应用于自然语言处理、搜索引擎、智能推荐等领域。</p>
<p>分词算法根据其核心思想主要分为两种，第一种是基于词典的分词，先把句子按照字典切分成词，再寻找词的最佳组合方式；第二种是基于字的分词，即由字构词，先把句子分成一个个字，再将字组合成词，寻找最优的切分策略，同时也可以转化成序列标注问题。</p>
<p>其分类大致可分为：</p>
<ol>
<li>
<p>基于匹配规则的方法</p>
<ul>
<li>正向最大匹配法(forward maximum matching method, FMM)</li>
<li>逆向最大匹配法(backward maximum matching method, BMM)</li>
<li>最短路径分词算法</li>
</ul>
</li>
<li>
<p>基于统计以及机器学习的分词方法</p>
<ul>
<li>基于N-gram语言模型的分词方法</li>
<li>基于HMM的分词方法</li>
<li>基于CRF的分词方法</li>
<li>基于词感知机的分词方法</li>
<li>基于深度学习的端到端的分词方法</li>
</ul>
</li>
</ol>
<p>基于规则匹配的分词通常会加入一些启发式规则，比如“正向/反向最大匹配”，“长词优先”等。</p>
<p>基于统计以及机器学习的分词方法，它们基于人工标注的词性和统计特征，对中文进行建模，即根据观测到的数据(标注好的语料)对模型参数进行训练，在分词阶段再通过模型计算各种分词出现的概率，将概率最大的分词结果作为最终结果。这类分词算法能很好处理歧义和未登录词问题，效果比基于规则匹配的方法效果好，但是需要大量的人工标注数据，以及较慢的分词速度。</p>
<p><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码及数据</a></p>
<h4 id="zhong-wen-fen-ci-de-ying-yong">中文分词的应用</h4>
<p>目前在自然语言处理技术中，中文处理技术比西文处理技术要落后很大一段距离，许多西文的处理方法中文不能直接采用，就是因为中文必需有分词这道工序。中文分词是其他中文信息处理的基础，搜索引擎只是中文分词的一个应用。其他的比如机器翻译（MT）、语音合成、自动分类、自动摘要、自动校对等等，都需要用到分词。因为中文需要分词，可能会影响一些研究，但同时也为一些企业带来机会，因为国外的计算机处理技术要想进入中国市场，首先也是要解决中文分词问题。在中文研究方面，相比外国人来说，中国人有十分明显的优势。</p>
<p>分词准确性对搜索引擎来说十分重要，但如果分词速度太慢，即使准确性再高，对于搜索引擎来说也是不可用的，因为搜索引擎需要处理数以亿计的网页，如果分词耗用的时间过长，会严重影响搜索引擎内容更新的速度。因此对于搜索引擎来说，分词的准确性和速度，二者都需要达到很高的要求。目前研究中文分词的大多是科研院校，清华、北大、中科院、北京语言学院、东北大学、IBM研究院、微软中国研究院等都有自己的研究队伍，而真正专业研究中文分词的商业公司除了海量科技以外，几乎没有。</p>
<h3 id="ji-yu-pi-pei-gui-ze-de-fang-fa">基于匹配规则的方法</h3>
<p>主要是人工建立词库也叫做词典，通过词典匹配的方式对句子进行划分。其实现简单高效，但是对未登陆词很难进行处理。主要有前向最大匹配法，后向最大匹配法以及双向最大匹配法。</p>
<h4 id="qian-xiang-zui-da-pi-pei-suan-fa">前向最大匹配算法</h4>
<p>前向最大匹配算法，是从待分词句子的左边向右边搜索，寻找词的最大匹配。规定一个词的最大长度，每次扫描的时候寻找当前开始的这个长度的词来和字典中的词匹配，如果没有找到，就缩短长度继续寻找，直到找到字典中的词或者成为单字。</p>
<h5 id="suan-fa-liu-cheng">算法流程</h5>
<ol>
<li>从前向后扫描字典，测试读入的子串是否在字典中</li>
<li>如果存在，则从输入中删除掉该子串，重新按照规则取子串，重复1</li>
<li>如果不存在于字典中，则从右向左减少子串长度，重复1</li>
</ol>
<h5 id="fen-ci-shi-li">分词实例：</h5>
<p>比如说输入 “北京大学生前来应聘”，假设词典中最长的单词为 5 个（MAX_LENGTH）。</p>
<ol>
<li>第一轮：取子串 “北京大学生”，正向取词，如果匹配失败，<strong>每次去掉匹配字段最后面的一个字</strong><br>
“北京大学生”，扫描 5 字词典，没有匹配，子串长度减 1 变为“北京大学”<br>
“北京大学”，扫描 4 字词典，有匹配，输出“北京大学”，输入变为“生前来应聘”</li>
<li>第二轮：取子串“生前来应聘”<br>
“生前来应聘”，扫描 5 字词典，没有匹配，子串长度减 1 变为“生前来应”<br>
“生前来应”，扫描 4 字词典，没有匹配，子串长度减 1 变为“生前来”<br>
“生前来”，扫描 3 字词典，没有匹配，子串长度减 1 变为“生前”<br>
“生前”，扫描 2 字词典，有匹配，输出“生前”，输入变为“来应聘””</li>
<li>第三轮：取子串“来应聘”<br>
“来应聘”，扫描 3 字词典，没有匹配，子串长度减 1 变为“来应”<br>
“来应”，扫描 2 字词典，没有匹配，子串长度减 1 变为“来”<br>
颗粒度最小为 1，直接输出“来”，输入变为“应聘”</li>
<li>第四轮：取子串“应聘”<br>
“应聘”，扫描 2 字词典，有匹配，输出“应聘”，输入变为“”<br>
输入长度为0，扫描终止</li>
</ol>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>数据准备：词库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_max_match</span><span class="params">(sentence, window_size, word_dict)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    前向最大匹配算法：</span></span><br><span class="line"><span class="string">    （1）扫描字典，测试读入的子串是否在字典中</span></span><br><span class="line"><span class="string">    （2）如果存在，则从输入中删除掉该子串，重新按照规则取子串，重复（1）</span></span><br><span class="line"><span class="string">    （3）如果不存在于字典中，则从右向左减少子串长度，重复（1）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param sentence: 待分词的句子</span></span><br><span class="line"><span class="string">    :param window_size: 词的最大长度</span></span><br><span class="line"><span class="string">    :param word_dict: 词典</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    seg_words = []  <span class="comment"># 存放分词结果</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> sentence:</span><br><span class="line">        <span class="keyword">for</span> word_len <span class="keyword">in</span> range(window_size, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> sentence[:word_len] <span class="keyword">in</span> word_dict:</span><br><span class="line">                <span class="comment"># 如果该词再字典中，将该词保存到分词结果中,并将其从句中切出来</span></span><br><span class="line">                seg_words.append(sentence[:word_len])</span><br><span class="line">                sentence = sentence[word_len:]</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 如果窗口中的词不在字典中，将第一个字切分出来</span></span><br><span class="line">            seg_words.append(sentence[:word_len])</span><br><span class="line">            sentence = sentence[word_len:]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'/'</span>.join(seg_words)</span><br></pre></td></tr></table></figure>
<h4 id="hou-xiang-zui-da-pi-pei-suan-fa">后向最大匹配算法</h4>
<p>在词典中从句尾向句首进行扫描，尽可能地选择与词典中最长单词匹配的词作为目标分词，然后进行下一次匹配。</p>
<p>在实践中，逆向最大匹配算法性能优于正向最大匹配算法。</p>
<h5 id="suan-fa-liu-cheng-1">算法流程</h5>
<ol>
<li>从后向前扫描字典，测试读入的子串是否在字典中</li>
<li>如果存在，则从输入中删除掉该子串，重新按照规则取子串，重复1</li>
<li>如果不存在于字典中，则从左向右减少子串长度，重复1</li>
</ol>
<h5 id="fen-ci-shi-li-1">分词实例</h5>
<p>输入 “北京大学生前来应聘”，假设词典中最长的单词为 5 个（MAX_LENGTH）。</p>
<ol>
<li>第一轮：取子串 “生前来应聘”，逆向取词，如果匹配失败，<strong>每次去掉匹配字段最前面的一个字</strong><br>
“生前来应聘”，扫描 5 字词典，没有匹配，字串长度减 1 变为“前来应聘”<br>
“前来应聘”，扫描 4 字词典，没有匹配，字串长度减 1 变为“来应聘”<br>
“来应聘”，扫描 3 字词典，没有匹配，字串长度减 1 变为“应聘”<br>
“应聘”，扫描 2 字词典，有匹配，输出“应聘”，输入变为“大学生前来”</li>
<li>第二轮：取子串“大学生前来”<br>
“大学生前来”，扫描 5 字词典，没有匹配，字串长度减 1 变为“学生前来”<br>
“学生前来”，扫描 4 字词典，没有匹配，字串长度减 1 变为“生前来”<br>
“生前来”，扫描 3 字词典，没有匹配，字串长度减 1 变为“前来”<br>
“前来”，扫描 2 字词典，有匹配，输出“前来”，输入变为“北京大学生”</li>
<li>第三轮：取子串“北京大学生”<br>
“北京大学生”，扫描 5 字词典，没有匹配，字串长度减 1 变为“京大学生”<br>
“京大学生”，扫描 4 字词典，没有匹配，字串长度减 1 变为“大学生”<br>
“大学生”，扫描 3 字词典，有匹配，输出“大学生”，输入变为“北京”</li>
<li>第四轮：取子串“北京”<br>
“北京”，扫描 2 字词典，有匹配，输出“北京”，输入变为“”<br>
输入长度为0，扫描终止</li>
</ol>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-1"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>数据准备：词库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_max_match</span><span class="params">(sentence, window_size, word_dict)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    后向最大匹配算法：</span></span><br><span class="line"><span class="string">    （1）从后向前扫描字典，测试读入的子串是否在字典中</span></span><br><span class="line"><span class="string">    （2）如果存在，则从输入中删除掉该子串，重新按照规则取子串，重复（1）</span></span><br><span class="line"><span class="string">    （3）如果不存在于字典中，则从右向左减少子串长度，重复（1）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param sentence: 待分词的句子</span></span><br><span class="line"><span class="string">    :param window_size: 词的最大长度</span></span><br><span class="line"><span class="string">    :param word_dict: 词典</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    seg_words = []</span><br><span class="line">    <span class="keyword">while</span> sentence:</span><br><span class="line">        <span class="keyword">for</span> word_len <span class="keyword">in</span> range(window_size, <span class="number">0</span>, <span class="number">-1</span>):  <span class="comment"># 每次去掉匹配字段最前面的一个字</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> sentence[len(sentence) - word_len:] <span class="keyword">in</span> word_dict:</span><br><span class="line">                <span class="comment"># 如果该词再字典中，将该词保存到分词结果中,并将其从句中切出来</span></span><br><span class="line">                seg_words.append(sentence[len(sentence) - word_len:])</span><br><span class="line">                sentence = sentence[:len(sentence) - word_len]</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 如果窗口中的词不在字典中，将最后一个字切分出来</span></span><br><span class="line">            seg_words.append(sentence[<span class="number">-1</span>:])</span><br><span class="line">            sentence = sentence[:<span class="number">-1</span>]</span><br><span class="line">    seg_words.reverse()</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'/'</span>.join(seg_words)</span><br></pre></td></tr></table></figure>
<h4 id="shuang-xiang-zui-da-pi-pei-fa">双向最大匹配法</h4>
<p>因为同一个句子，在机械分词中经常会出现多种分词的组合，因此需要进行歧义消除，来得到最优的分词结果。</p>
<p>以很常见的MMSEG机械分词算法为例，MMSEG在搜索引擎Solr中经常使用到，是一种非常可靠高效的分词算法。MMSEG消除歧义的规则有四个，它在使用中依次用这四个规则进行过滤，直到只有一种结果或者第四个规则使用完毕。这个四个规则分别是：</p>
<ol>
<li>
<p>最大匹配。选择“词组长度最大的”那个词组，然后选择这个词组的第一个词，作为切分出的第一个词，如对于“中国人民万岁”，匹配结果分别为：</p>
<ul>
<li>中/国/人</li>
<li>中国/人/民</li>
<li>中国/人民/万岁</li>
<li>中国人/民/万岁</li>
</ul>
<p>在这个例子“词组长度最长的”词组为后两个，因此选择了“中国人/民/万岁”中的“中国人”，或者“中国/人民/万岁”中的“中国”。</p>
</li>
<li>
<p>最大平均词语长度。经过规则1过滤后，如果剩余的词组超过1个，那就选择平均词语长度最大的那个(平均词长=词组总字数/词语数量)。比如“生活水平”，可能得到如下词组：</p>
<ul>
<li>生/活水/平 (4/3=1.33)</li>
<li>生活/水/平 (4/3=1.33)</li>
<li>生活/水平 (4/2=2)</li>
</ul>
<p>根据此规则，就可以确定选择“生活/水平”这个词组</p>
</li>
<li>
<p>词语长度的最小变化率。这个变化率一般可以由标准差来决定。比如对于“中国人民万岁”这个短语，可以计算：</p>
<ul>
<li>中国/人民/万岁(标准差=sqrt(((2-2)<sup>2+(2-2)</sup>2+(2-2^2))/3)=0)</li>
<li>中国人/民/万岁(标准差=sqrt(((2-3)<sup>2+(2-1)</sup>2+(2-2)^2)/3)=0.8165)</li>
</ul>
<p>于是选择“中国/人民/万岁”这个词组。</p>
</li>
<li>
<p>计算词组中的所有单字词词频的自然对数，然后将得到的值相加，取总和最大的词组。比如：</p>
<ul>
<li>设施/和服/务</li>
<li>设施/和/服务</li>
</ul>
<p>这两个词组中分别有“务”和“和”这两个单字词，假设“务”作为单字词时候的频率是5，“和”作为单字词时候的频率是10，对5和10取自然对数，然后取最大值者，所以取“和”字所在的词组，即“设施/和/服务”。</p>
</li>
</ol>
<p>在实际中根据需求，选择或制定相应的规则来改善分词的质量。</p>
<h5 id="suan-fa-liu-cheng-2">算法流程</h5>
<ol>
<li>比较正向最大匹配和逆向最大匹配结果</li>
<li>如果分词数量结果不同，那么取分词数量较少的那个</li>
<li>如果分词数量结果相同
<ul>
<li>分词结果相同，可以返回任何一个</li>
<li>分词结果不同，返回单字数比较少的那个</li>
</ul>
</li>
</ol>
<p>这种规则的出发点来自语言学上的启发：汉语中单字词的数量要远小于非单字词。因此，算法应当尽量减少结果中的单字，保留更多的完整词语。</p>
<h5 id="fen-ci-shi-li-2">分词实例</h5>
<p>正向匹配最终切分结果为：北京大学 / 生前 / 来 / 应聘，分词数量为 4，单字数为 1<br>
逆向匹配最终切分结果为：”北京/ 大学生/ 前来 / 应聘，分词数量为 4，单字数为 0<br>
逆向匹配单字数少，因此返回逆向匹配的结果。</p>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-2"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>数据准备：词库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fb_max_match</span><span class="params">(sentence,window_size,word_dict)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    1. 比较正向最大匹配和逆向最大匹配结果</span></span><br><span class="line"><span class="string">    2. 如果分词数量结果不同，那么取分词数量较少的那个</span></span><br><span class="line"><span class="string">    3. 如果分词数量结果相同</span></span><br><span class="line"><span class="string">       * 分词结果相同，可以返回任何一个</span></span><br><span class="line"><span class="string">       * 分词结果不同，返回单字数比较少的那个</span></span><br><span class="line"><span class="string">    :param sentence: 待分词的句子</span></span><br><span class="line"><span class="string">    :param window_size: 词的最大长度</span></span><br><span class="line"><span class="string">    :param word_dict: 词典</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    forward_seg = forward_max_match(sentence,window_size,word_dict)</span><br><span class="line">    backward_seg = backward_max_match(sentence,window_size,word_dict)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果分词结果不同，返回词数较小的分词结果</span></span><br><span class="line">    <span class="keyword">if</span> len(forward_seg) != len(backward_seg):</span><br><span class="line">        <span class="keyword">return</span> forward_seg <span class="keyword">if</span> len(forward_seg) &lt; len(backward_seg) <span class="keyword">else</span> backward_seg</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果分词结果词数相同，优先考虑返回包含单个字符最少的分词结果</span></span><br><span class="line">        forward_single_word_count = len([filter(<span class="keyword">lambda</span> x:len(x) == <span class="number">1</span>,forward_seg)])</span><br><span class="line">        backward_single_word_count = len([filter(<span class="keyword">lambda</span> x:len(x) == <span class="number">1</span>,backward_seg)])</span><br><span class="line">        <span class="keyword">if</span> forward_single_word_count != backward_single_word_count:</span><br><span class="line">            <span class="keyword">return</span> forward_seg <span class="keyword">if</span> forward_single_word_count &lt; backward_single_word_count <span class="keyword">else</span> backward_seg</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 否则，返回任意结果</span></span><br><span class="line">            <span class="keyword">return</span> forward_seg</span><br></pre></td></tr></table></figure>
<h5 id="que-dian">缺点：</h5>
<p>对歧义问题的解决有点弱</p>
<h4 id="n-zui-duan-lu-jing-fen-ci-suan-fa">(N-)最短路径分词算法</h4>
<h5 id="ji-ben-si-xiang">基本思想</h5>
<p>首先基于词典对文本进行全切分(改编版最大匹配)；然后基于词语的临接关系构建一个有向图；使用模型(比如一阶马尔科夫模型)对图里的边加权；最后使用最短路径算法求，从句首到句末质量最高(比如概率最大)的路径，就得到了分词结果。</p>
<h5 id="ju-li">举例</h5>
<p>最短路径分词算法首先将一句话中的所有词匹配出来，构成词图（有向无环图DAG），之后寻找从起始点到终点的最短路径作为最佳组合方式，以“他说的确实在理”为例，给出对这句话的3-最短路：<br>
<img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95.png" alt="avatar"></p>
<ol>
<li>
<p>构建有向图：根据一个已有词典构造出的有向无环图（全切分，根据临接关系构建有向图）。它将字串分为单个的字，每个字用图中相邻的两个结点表示，故对于长度为n的字串，需要n+1个结点。两节点间若有边，则表示两节点间所包含的所有结点构成的词，如图中结点2、3、4构成词“的确”。</p>
</li>
<li>
<p>计算权重：本例子中权重为1（为了简单），实际应用中，可以使用一阶马尔科夫模型进行权重计算。</p>
<ul>
<li>
<p>p(实，在)=p(实)*p(在|实)。概率取值越大，说明一个边出现的概率越大，这条边会提升所在分句结果的概率，由于最后要计算最短路径，需要构造一个连接权重与分局结果质量成反比的指标，因此对概率取了倒：weight=1/p(实,在)</p>
</li>
<li>
<p>这个概率可能非常小，得到的权重取值非常大。而我们后面在计算路径的长度时，会将若干个边的权重加起来，这时候有上溢出的可能性。避免上溢出的常用策略是取对数。：weight=log(1/p(实，在))。</p>
</li>
<li>
<p>概率的估算：概率就用频率来估计，p(实) = （“实”字在语料中出现的次数）/（语料的总词数），</p>
<p>p(在|实) = p(实在)/p(实)=(“实在”在语料中出现的次数)/(“实”在语料中出现的次数)</p>
</li>
</ul>
</li>
</ol>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-wei-te-bi-a"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码（维特比）</a></h5>
<p>数据准备：1、词库，2、词语之间的条件概率（用来计算路径权重）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用维特比算法求词图的最短路径</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(self, word_graph)</span>:</span></span><br><span class="line">    path_length_map = &#123;&#125;  <span class="comment"># 用于存储所有的路径，后面的邻接词语所在位置，以及对应的长度</span></span><br><span class="line">    word_graph = [[[<span class="string">"&lt;start&gt;"</span>, <span class="number">1</span>]]] + word_graph + [[[<span class="string">"&lt;end&gt;"</span>, <span class="number">-1</span>]]]</span><br><span class="line">    <span class="comment"># 这是一种比较简单的数据结构</span></span><br><span class="line">    path_length_map[(<span class="string">"&lt;start&gt;"</span>,)] = [<span class="number">1</span>, <span class="number">0</span>]  <span class="comment"># start处，后面的临接词语在列表的1处，路径长度是0,。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(word_graph)):</span><br><span class="line">        distance_from_start2current = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> len(word_graph[i]) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> former_path <span class="keyword">in</span> list(path_length_map.keys()):  <span class="comment"># path_length_map内容一直在变，需要深拷贝key,也就是已经积累的所有路径</span></span><br><span class="line">            <span class="comment"># 取出已经积累的路径，后面的临接词语位置，以及路径的长度。</span></span><br><span class="line">            [next_index_4_former_path, former_distance] = path_length_map[former_path]</span><br><span class="line">            former_word = former_path[<span class="number">-1</span>]</span><br><span class="line">            later_path = list(former_path)</span><br><span class="line">            <span class="keyword">if</span> next_index_4_former_path == i:  <span class="comment"># 如果这条路径的临接词语的位置就是当前索引</span></span><br><span class="line">                <span class="keyword">for</span> current_word <span class="keyword">in</span> word_graph[i]:  <span class="comment"># 遍历词图数据中，这个位置上的所有换选词语，然后与former_path拼接新路径</span></span><br><span class="line">                    current_word, next_index = current_word</span><br><span class="line">                    new_path = tuple(later_path + [current_word])  <span class="comment"># 只有int, string, tuple这种不可修改的数据类型可以hash，</span></span><br><span class="line">                    <span class="comment"># 也就是成为dict的key</span></span><br><span class="line">                    <span class="comment"># 计算新路径的长度</span></span><br><span class="line">                    new_path_len = former_distance + self.word_distance.get((former_word, current_word), <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">                    path_length_map[new_path] = [next_index, new_path_len]  <span class="comment"># 存储新路径后面的临接词语，以及路径长度</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 维特比的部分。选择到达当前节点的路径中，最短的那一条</span></span><br><span class="line">                    <span class="keyword">if</span> current_word <span class="keyword">in</span> distance_from_start2current:  <span class="comment"># 如果已经有到达当前词语的路径，需要择优</span></span><br><span class="line">                        <span class="keyword">if</span> distance_from_start2current[current_word][<span class="number">1</span>] &gt; new_path_len:  <span class="comment"># 如果当前新路径比已有的更短</span></span><br><span class="line">                            distance_from_start2current[current_word] = [new_path, new_path_len]  <span class="comment"># 用更短的路径数据覆盖原来的</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        distance_from_start2current[current_word] = [new_path, new_path_len]  <span class="comment"># 如果还没有这条路径，就记录它</span></span><br><span class="line">    shortest_path = distance_from_start2current[<span class="string">"&lt;end&gt;"</span>][<span class="number">0</span>]</span><br><span class="line">    shortest_path = shortest_path[<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> shortest_path</span><br></pre></td></tr></table></figure>
<h4 id="que-dian-1">缺点</h4>
<ol>
<li>对歧义和新词的处理不是很好，对词典中未出现的词没法进行处理。</li>
<li>分词效果取决于词典的质量。</li>
</ol>
<h3 id="ji-yu-tong-ji-yi-ji-ji-qi-xue-xi-de-fen-ci-fang-fa">基于统计以及机器学习的分词方法</h3>
<h4 id="ji-yu-n-gram-yu-yan-mo-xing-de-fen-ci-fang-fa">基于N-gram语言模型的分词方法</h4>
<h5 id="jian-jie">简介</h5>
<p>n-gram模型，称为N元模型，可用于定义字符串中的距离，也可用于中文的分词；该模型假设第n个词的出现只与前面n-1个词相关，与其他词都不相关，整个语句的概率就是各个词出现概率的乘积；而这些概率，利用语料，统计同时出现相关词的概率次数计算得到；常用的模型是Bi-gram和Tri-gram模型。<br>
假设一个字符串s由m个词组成，因此我们需要计算出\(p(w_1,w_2,\ldots,w_m)\)的概率，根据概率论中的链式法则得到如下：<br>
\[
p(w_1,w_2,\ldots,w_m) = p(w_1)*p(w_2|w_1)*p(w_3|w_2,w_1)\ldots p(w_m|w_{m-1},\ldots,w_2,w_1)=\prod_{i}{p(w_i|w_1,w_2,\ldots,w_{i-1})}
\]<br>
那么下面的问题是如何计算上面每一个概率，比如\(p(w_1,w_2,w_3,w_4,w_5)\)，一种比较直观的计算就是计数然后用除法：<br>
\[
p(w_5|w_1,w_2,w_3,w_4) = \frac{Count(w_1,w_2,w_3,w_4,w_5)}{Count(w_1,w_2,w_3,w_4)}
\]</p>
<p>直接计算这个概率的难度有点大：</p>
<ol>
<li>
<p>直接这样计算会导致参数空间过大。</p>
<p>一个语言模型的参数就是所有的这些条件概率，试想按上面方式计算\(p(w_5|w_1,w_2,w_3,w_4)\),这里\(w_5\)有词典大小取值的可能，记词典大小：\(|V|\)，则该模型的参数个数是\(|V|^5\)，而且这还不包含\(P(w_4|w_1,w_2,w_3)\)的个数，可以看到这样去计算条件概率会使语言模型参数个数过多而无法使用。</p>
</li>
<li>
<p>数据稀疏严重。我的理解是像上面那样计数计算，比如计数分子\(w_1,w_2,w_3,w_4,w_5\),在我们所能见的文本中出现的次数是很小的，这样计算的结果是过多的条件概率会等于0，因为我们根本没有看到足够的文本来统计！假设一个语料库中单词的数量为\(|V|\)个，一个句子由\(n\)个词组成，那么每个单词都可能有\(|V|\)个取值，那么由这些词组成的\(n\)元组合的数目为\(|V|^n\)种，也就是说，组合数会随着\(n\)的增加而呈现指数级别的增长，随着\(n\)的增加，语料数据库能够提供的数据是非常有限的，除非有海量的各种类型的语料数据，否则还有大量的\(n\)元组合都没有在语料库中出现过（即由\(n\)个单词所组成的一些句子根本就没出现过，可以理解为很多的\(n\)元组所组成的句子不能被人很好地理解）也就是说依据最大似然估计得到的概率将会是0，模型可能仅仅能够计算寥寥几个句子。怎么解决呢？</p>
</li>
</ol>
<p>解决參数空间过大的问题。引入了马尔科夫假设：**随意一个词出现的概率只与它前面出现的有限的一个或者几个词有关。**假设第\(w_i\)个词语只与它前面的\(n\)个词语相关，这样我们就得到前面的条件概率计算简化如下<br>
\[
p(w_i|w_{i-1},\ldots,w_2,w_1) \approx p(w_i|w_{i-1},\ldots,w_{i-n})\\
p(w_1,w_2,\ldots,w_m) \approx \prod_{i} p(w_i|w_{i-1},\ldots,w_{i-n})
\]</p>
<p>当n=1，即一元模型（Uni-gram）,即\(w_i\)与它前面的0个词相关，即\(w_i\)不与任何词相关，每一个词都是相互独立的：<br>
\[
P\left(w_{1}, w_{2}, \cdots, w_{m}\right)=\prod_{i=1}^{m} P\left(w_{i}\right)
\]<br>
当n=2，即二元模型（Bi-gram）,此时\(w_i\)与它前面1个词相关：<br>
\[
P\left(w_{1}, w_{2}, \cdots, w_{m}\right)=\prod_{i=1}^{m} P\left(w_{i} | w_{i-1}\right)
\]<br>
当n=3时，即三元模型（Tri-gram）,此时\(w_i\)与它前面2个词相关：<br>
\[
P\left(w_{1}, w_{2}, \cdots, w_{m}\right)=\prod_{i=1}^{m} P\left(w_{i} | w_{i-2} w_{i-1}\right)
\]<br>
一般来说，N元模型就是假设当前词的出现概率只与它前面的N-1个词有关。而这些概率参数都是可以通过大规模语料库来计算。<strong>在实践中用的最多的就是bigram和trigram了，高于四元的用的非常少，由于训练它须要更庞大的语料，并且数据稀疏严重，时间复杂度高，精度却提高的不多。</strong></p>
<h5 id="can-shu-gu-ji">参数估计</h5>
<p>要计算出模型中的条件概率，这些条件概率也称为模型的参数，得到这些参数的过程称为训练。用最大似然性估计计算下面的条件概率：<br>
\[
P\left(w_{i} | w_{i-1}\right)=\frac{c\left(w_{i-1}, w_{i}\right)}{c\left(w_{i-1}\right)}
\]<br>
一元语言模型中：句子概率定义为：\(P\left(s\right)=\prod_{i=1}^{m} P\left(w_{i}\right)\),这个式子成立的条件是有一个假设，就是条件无关假设，我们认为每个词都是条件无关的。这里的参数种类是一种 \(P(w_n)\),但是参数实例有\(|V|\)个(V是词典大小),我们应该如何得到每个参数实例的值。用的是极大似然估计法。比如训练语料是:&quot;星期五早晨，我特意起了个大早，为的就是看看早晨的天空。&quot;那么我们的字典为：星 期 五 早 晨 ，我 特 意 起 了 个 大 早 为 的 就 是 看 天 空 。 22个不同词，每个词语的概率直接用极大似然估计法估计得到。如：p(星) = 1/27，p(期) = 1/27。于是需要存储学习得到的模型参数，一个向量，22维，每个维度保存着每个单词的概率值。当需要计算：p(我看看早晨的天空)=p(我)p(看)p(看)p(早)p(晨)p(的)p(天)p(空)=\(\frac{1}{27}*\frac{1}{27}*\frac{1}{27}\ldots *\frac{1}{27}\)，可以直接计算出来。于是只要将每句话拆开为每个单词然后用累积形式运算，这样就能算出每句话的概率。缺点是：不包含语序信息。</p>
<p>二元语言模型中：为了计算对应的二元模型的参数，即\(P(w_i | w_{i-1})\)，要先计数即\(c(w_{i-1},w_i)\)，然后计数\(c(w_{i-1})\),再用除法可得到这些条件概率.可以看到对于\(c(w_{i-1},w_i)\)来说，\(w_{i-1}\)有语料库词典大小（记作\(|V|\)）的可能取值，\(w_i\)也是，所以\(c(w_{i-1},w_i)\)要计算的个数有\(|V|^2\)。</p>
<p>\(c(w_{i-1},w_i)\)计数结果如下:</p>
<table>
<thead>
<tr>
<th></th>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>english</th>
</tr>
</thead>
<tbody>
<tr>
<td>i</td>
<td>5</td>
<td>827</td>
<td>0</td>
<td>9</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>want</td>
<td>2</td>
<td>0</td>
<td>608</td>
<td>1</td>
<td>6</td>
<td>6</td>
<td>5</td>
<td>1</td>
</tr>
<tr>
<td>to</td>
<td>2</td>
<td>0</td>
<td>4</td>
<td>686</td>
<td>2</td>
<td>0</td>
<td>6</td>
<td>211</td>
</tr>
<tr>
<td>eat</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>16</td>
<td>2</td>
<td>42</td>
<td>0</td>
</tr>
<tr>
<td>chinese</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>82</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>food</td>
<td>15</td>
<td>0</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>4</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>lunch</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>spend</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>\(c(w_{i-1})\)的计数如下：</p>
<table>
<thead>
<tr>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>english</th>
</tr>
</thead>
<tbody>
<tr>
<td>2533</td>
<td>927</td>
<td>2417</td>
<td>746</td>
<td>158</td>
<td>1093</td>
<td>341</td>
<td>278</td>
</tr>
</tbody>
</table>
<p>那么二元模型的参数计算结果如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>english</th>
</tr>
</thead>
<tbody>
<tr>
<td>i</td>
<td>0.002</td>
<td>0.33</td>
<td>0</td>
<td>0.0036</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.00079</td>
</tr>
<tr>
<td>want</td>
<td>0.0022</td>
<td>0</td>
<td>0.66</td>
<td>0.0011</td>
<td>0.0065</td>
<td>0.0065</td>
<td>0.0054</td>
<td>0.0011</td>
</tr>
<tr>
<td>to</td>
<td>0.00083</td>
<td>0</td>
<td>0.0017</td>
<td>0.28</td>
<td>0.00083</td>
<td>0</td>
<td>0.0025</td>
<td>0.087</td>
</tr>
<tr>
<td>eat</td>
<td>0</td>
<td>0</td>
<td>0.0027</td>
<td>0</td>
<td>0.021</td>
<td>0.0027</td>
<td>0.056</td>
<td>0</td>
</tr>
<tr>
<td>chinese</td>
<td>0.0063</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.52</td>
<td>0.0063</td>
<td>0</td>
</tr>
<tr>
<td>food</td>
<td>0.014</td>
<td>0</td>
<td>0.0014</td>
<td>0</td>
<td>0.00092</td>
<td>0.0037</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>lunch</td>
<td>0.0059</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.0029</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>spend</td>
<td>0.0036</td>
<td>0</td>
<td>0.0036</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>比如计算其中的P(want | i) = 0.33如:\(p(want|i) = \frac{c(i,want)}{c(i)}=\frac{827}{2533} \approx0.33\),针对这个语料库的二元模型建立好了后，可以计算目标，即一个句子的概率了，一个例子如下：</p>
<p>\(p(s)=p(i want english food) = p(i|&lt;start>)*p(want|i)*p(english|want)*p(food|english)*p(&lt;end>|food) \approx 0.000031\)</p>
<p>该二元模型所捕捉到的一些实际信息:</p>
<ul>
<li>\(p(english|want) = 0.0011,p(chinese|want)=0.0065\),want chinese 的概率更高，这和真实世界情况相对应，因为chinese food 比 English food 更受欢迎。</li>
<li>\(p(to|want)=0.66\), want to 的概率很高，反映了语法特性</li>
<li>\(p(food|to)=0\) ，to food 概率为0，因为这种搭配不常见</li>
<li>\(p(want|spend)=0\), spend want 概率为0，因为这样违反了语法。</li>
</ul>
<p>常常在对数空间里面计算概率，原因有两个：</p>
<ol>
<li>防止溢出，如果计算的句子很长，最后得到的结果将非常小，甚至会溢出，比如计算得到的概率是0.001，那么假设以10为底取对数的结果就是-3，这样就不会溢出。</li>
<li>对数空间里面加法可以代替乘法，因为log(p1p2) = logp1 + logp2，而在计算机内部，显然加法比乘法执行更快！</li>
</ol>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-3"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>数据准备：unigram:(word, freq),bigram:(word1,word2,freq) 语料数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="comment"># global parameter</span></span><br><span class="line">DELIMITER = <span class="string">" "</span>  <span class="comment"># 分词之后的分隔符</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DNASegment</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.word1_dict_prob = &#123;&#125;  <span class="comment"># 记录概率,1-gram</span></span><br><span class="line">        self.word1_dict_count = &#123;&#125;  <span class="comment"># 记录词频,1-gram</span></span><br><span class="line">        self.word1_dict_count[<span class="string">"&lt;S&gt;"</span>] = <span class="number">8310575403</span>  <span class="comment"># 开始的&lt;S&gt;的个数</span></span><br><span class="line"></span><br><span class="line">        self.word2_dict_prob = &#123;&#125;  <span class="comment"># 记录概率,2-gram</span></span><br><span class="line">        self.word2_dict_count = &#123;&#125;  <span class="comment"># 记录词频,2-gram</span></span><br><span class="line"></span><br><span class="line">        self.gmax_word_length = <span class="number">0</span></span><br><span class="line">        self.all_freq = <span class="number">0</span>  <span class="comment"># 所有词的词频总和,1-gram的</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 估算未出现的词的概率,根据beautiful data里面的方法估算</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_unkonw_word_prob</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> math.log(<span class="number">10.</span> / (self.all_freq * <span class="number">10</span> ** len(word)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得片段的概率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_prob</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> self.word1_dict_prob:  <span class="comment"># 如果字典包含这个词</span></span><br><span class="line">            prob = self.word1_dict_prob[word]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            prob = self.get_unkonw_word_prob(word)</span><br><span class="line">        <span class="keyword">return</span> prob</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得两个词的转移概率(bigram)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_trans_prob</span><span class="params">(self, first_word, second_word)</span>:</span></span><br><span class="line">        trans_word = first_word + <span class="string">" "</span> + second_word</span><br><span class="line">        <span class="comment"># print trans_word</span></span><br><span class="line">        <span class="keyword">if</span> trans_word <span class="keyword">in</span> self.word2_dict_count:</span><br><span class="line">            trans_prob = math.log(self.word2_dict_count[trans_word] / self.word1_dict_count[first_word])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            trans_prob = self.get_word_prob(second_word)</span><br><span class="line">        <span class="keyword">return</span> trans_prob</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 寻找node的最佳前驱节点</span></span><br><span class="line">    <span class="comment"># 方法为寻找所有可能的前驱片段</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_best_pre_node</span><span class="params">(self, sequence, node, node_state_list)</span>:</span></span><br><span class="line">        <span class="comment"># 如果node比最大词长小，取的片段长度以node的长度为限</span></span><br><span class="line">        max_seg_length = min([node, self.gmax_word_length])</span><br><span class="line">        pre_node_list = []  <span class="comment"># 前驱节点列表</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得所有的前驱片段，并记录累加概率</span></span><br><span class="line">        <span class="keyword">for</span> segment_length <span class="keyword">in</span> range(<span class="number">1</span>, max_seg_length + <span class="number">1</span>):</span><br><span class="line">            segment_start_node = node - segment_length</span><br><span class="line">            segment = sequence[segment_start_node:node]  <span class="comment"># 获取片段</span></span><br><span class="line"></span><br><span class="line">            pre_node = segment_start_node  <span class="comment"># 取该片段，则记录对应的前驱节点</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> pre_node == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 如果前驱片段开始节点是序列的开始节点，</span></span><br><span class="line">                <span class="comment"># 则概率为&lt;S&gt;转移到当前词的概率</span></span><br><span class="line">                <span class="comment"># segment_prob = self.get_word_prob(segment)</span></span><br><span class="line">                segment_prob = self.get_word_trans_prob(<span class="string">"&lt;S&gt;"</span>, segment)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 如果不是序列开始节点，按照二元概率计算</span></span><br><span class="line">                <span class="comment"># 获得前驱片段的前一个词</span></span><br><span class="line">                pre_pre_node = node_state_list[pre_node][<span class="string">"pre_node"</span>]</span><br><span class="line">                pre_pre_word = sequence[pre_pre_node:pre_node]</span><br><span class="line">                segment_prob = self.get_word_trans_prob(pre_pre_word, segment)</span><br><span class="line"></span><br><span class="line">            pre_node_prob_sum = node_state_list[pre_node][<span class="string">"prob_sum"</span>]  <span class="comment"># 前驱节点的概率的累加值</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 当前node一个候选的累加概率值</span></span><br><span class="line">            candidate_prob_sum = pre_node_prob_sum + segment_prob</span><br><span class="line"></span><br><span class="line">            pre_node_list.append((pre_node, candidate_prob_sum))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 找到最大的候选概率值</span></span><br><span class="line">        (best_pre_node, best_prob_sum) = max(pre_node_list, key=<span class="keyword">lambda</span> d: d[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> (best_pre_node, best_prob_sum)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最大概率分词</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mp_seg</span><span class="params">(self, sequence)</span>:</span></span><br><span class="line">        sequence = sequence.strip()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化</span></span><br><span class="line">        node_state_list = []  <span class="comment"># 记录节点的最佳前驱，index就是位置信息</span></span><br><span class="line">        <span class="comment"># 初始节点，也就是0节点信息</span></span><br><span class="line">        ini_state = &#123;&#125;</span><br><span class="line">        ini_state[<span class="string">"pre_node"</span>] = <span class="number">-1</span>  <span class="comment"># 前一个节点</span></span><br><span class="line">        ini_state[<span class="string">"prob_sum"</span>] = <span class="number">0</span>  <span class="comment"># 当前的概率总和</span></span><br><span class="line">        node_state_list.append(ini_state)</span><br><span class="line">        <span class="comment"># 字符串概率为2元概率</span></span><br><span class="line">        <span class="comment"># P(a b c) = P(a|&lt;S&gt;)P(b|a)P(c|b)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 逐个节点寻找最佳前驱节点</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> range(<span class="number">1</span>, len(sequence) + <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 寻找最佳前驱，并记录当前最大的概率累加值</span></span><br><span class="line">            (best_pre_node, best_prob_sum) = self.get_best_pre_node(sequence, node, node_state_list)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 添加到队列</span></span><br><span class="line">            cur_node = &#123;&#125;</span><br><span class="line">            cur_node[<span class="string">"pre_node"</span>] = best_pre_node</span><br><span class="line">            cur_node[<span class="string">"prob_sum"</span>] = best_prob_sum</span><br><span class="line">            node_state_list.append(cur_node)</span><br><span class="line">            <span class="comment"># print "cur node list",node_state_list</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># step 2, 获得最优路径,从后到前</span></span><br><span class="line">        best_path = []</span><br><span class="line">        node = len(sequence)  <span class="comment"># 最后一个点</span></span><br><span class="line">        best_path.append(node)</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            pre_node = node_state_list[node][<span class="string">"pre_node"</span>]</span><br><span class="line">            <span class="keyword">if</span> pre_node == <span class="number">-1</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            node = pre_node</span><br><span class="line">            best_path.append(node)</span><br><span class="line">        best_path.reverse()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># step 3, 构建切分</span></span><br><span class="line">        word_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(best_path) - <span class="number">1</span>):</span><br><span class="line">            left = best_path[i]</span><br><span class="line">            right = best_path[i + <span class="number">1</span>]</span><br><span class="line">            word = sequence[left:right]</span><br><span class="line">            word_list.append(word)</span><br><span class="line"></span><br><span class="line">        seg_sequence = DELIMITER.join(word_list)</span><br><span class="line">        <span class="keyword">return</span> seg_sequence</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载词典，为词\t词频的格式</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initial_dict</span><span class="params">(self, gram1_file, gram2_file)</span>:</span></span><br><span class="line">        <span class="comment"># 读取unigram文件</span></span><br><span class="line">        dict_file = open(gram1_file, <span class="string">"r"</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> dict_file:</span><br><span class="line">            sequence = line.strip()</span><br><span class="line">            key = sequence.split(<span class="string">'\t'</span>)[<span class="number">0</span>]</span><br><span class="line">            value = float(sequence.split(<span class="string">'\t'</span>)[<span class="number">1</span>])</span><br><span class="line">            self.word1_dict_count[key] = value</span><br><span class="line">        <span class="comment"># 计算频率</span></span><br><span class="line">        self.all_freq = sum(self.word1_dict_count.values())  <span class="comment"># 所有词的词频</span></span><br><span class="line">        self.gmax_word_length = max(len(key) <span class="keyword">for</span> key <span class="keyword">in</span> self.word1_dict_count.keys())</span><br><span class="line">        self.gmax_word_length = <span class="number">20</span></span><br><span class="line">        self.all_freq = <span class="number">1024908267229.0</span></span><br><span class="line">        <span class="comment"># 计算1gram词的概率</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> self.word1_dict_count:</span><br><span class="line">            self.word1_dict_prob[key] = math.log(self.word1_dict_count[key] / self.all_freq)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取2_gram_file，同时计算转移概率</span></span><br><span class="line">        dict_file = open(gram2_file, <span class="string">"r"</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> dict_file:</span><br><span class="line">            sequence = line.strip()</span><br><span class="line">            key = sequence.split(<span class="string">'\t'</span>)[<span class="number">0</span>]</span><br><span class="line">            value = float(sequence.split(<span class="string">'\t'</span>)[<span class="number">1</span>])</span><br><span class="line">            first_word = key.split(<span class="string">" "</span>)[<span class="number">0</span>]</span><br><span class="line">            second_word = key.split(<span class="string">" "</span>)[<span class="number">1</span>]</span><br><span class="line">            self.word2_dict_count[key] = float(value)</span><br><span class="line">            <span class="keyword">if</span> first_word <span class="keyword">in</span> self.word1_dict_count:</span><br><span class="line">                <span class="comment"># 取自然对数</span></span><br><span class="line">                self.word2_dict_prob[key] = math.log(value / self.word1_dict_count[first_word])  </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.word2_dict_prob[key] = self.word1_dict_prob[second_word]</span><br></pre></td></tr></table></figure>
<h5 id="zong-jie">总结</h5>
<p><strong>列举出所有可能的分词方式，再分别计算句子概率，选择概率最大的作为最终分词结果。穷举法，速度慢。</strong></p>
<p>缺点：</p>
<ol>
<li>N-grams有一些不足，因为语言存在一个长距离依赖关系，比如：“The computer which I had just put into the machine room on the fifth floor crashed.”假如要预测最后一个词语crashed出现的概率，如果采用二元模型，那么crashed与floor实际关联可能性应该非常小，相反的，这句子的主语computer与crashed的相关性很大，但是n-grams并没有捕捉到这个信息。</li>
<li>一个词是由前一个或者几个词决定的，这样可以去除一部分歧义问题，但是n-gram模型还是基于马尔科夫模型的，其基本原理就是无后效性，就是后续的节点的状态不影响前面的状态，就是先前的分词形式一旦确定，无论后续跟的是什么词，都不会再有变化，这在现实中显然是不成立的。。因此就有一些可以考虑到后续词的算法，如crf等方法。</li>
</ol>
<h4 id="ji-yu-hmm-de-fen-ci-suan-fa">基于HMM的分词算法</h4>
<h5 id="yin-ma-er-ke-fu-mo-xing-hmm">隐马尔科夫模型(HMM)</h5>
<p>隐马尔可夫模型是关于<strong>时序</strong>的概率模型,描述由一个隐藏的马尔可夫链随机生成不可观测的<strong>状态序列</strong>,再由各个状态生成一个观测而产生<strong>观测随机序列</strong>的过程.HMM是一种生成式模型，它的理论基础是朴素贝叶斯，本质上就类似于我们将朴素贝叶斯在单样本分类问题上的应用推广到序列样本分类问题上。</p>
<h6 id="mo-xing-biao-shi">模型表示</h6>
<p>设Q是所有可能的状态的集合\(Q=\left\{q_{1}, q_{2}, \cdots, q_{N}\right\}\),V是所有可能的观测的集合\(V=\left\{v_{1}, v_{2}, \cdots, v_{M}\right\}\),I是长度为T的状态序列\(I=\left(i_{1}, i_{2}, \cdots, i_{T}\right)\), O是对应的观测序列\(O=\left(o_{1}, o_{2}, \cdots, o_{T}\right)\),</p>
<ol>
<li>A是<strong>状态转移概率矩阵</strong>\(A=\left[a_{i j}\right]_{N \times N}\),\(a_{ij}\)表示在时刻t处于状态\(q_i\)的条件下在时刻t+1转移到状态\(q_j\)的概率.</li>
<li>B是<strong>观测概率矩阵</strong> \(B=\left[b_{j}(k)\right]_{N \times M}\),\(b_{ij}\)是在时刻t处于状态\(q_j\)的条件下生成观测\(v_k\)的概率.</li>
<li>\(\pi\)是<strong>初始状态概率向量</strong>\(\pi=\pi(x)\),\(\pi_i\)表示时刻t=1处于状态qi的概率.</li>
</ol>
<p>隐马尔可夫模型由初始状态概率向量\(pi\),状态转移概率矩阵A以及观测概率矩阵B确定.\(\pi\)和A决定即隐藏的<strong>马尔可夫链</strong>,生成不可观测的<strong>状态序列</strong>.B决定如何从状态生成观测,与状态序列综合确定了<strong>观测序列</strong>.因此,隐马尔可夫模型可以用<strong>三元符号</strong>表示\(\lambda = (A,B,\pi)\)</p>
<h6 id="liang-ge-ji-ben-jia-she">两个基本假设</h6>
<ol>
<li><strong>齐次马尔可夫性假设</strong>:假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态.</li>
<li><strong>观测独立性假设</strong>:假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态.</li>
</ol>
<h6 id="san-ge-ji-ben-wen-ti">三个基本问题</h6>
<p><strong>1. 概率计算问题</strong></p>
<p>给定模型\(\lambda = (A,B,\pi)\)和观测序列,\(O=\left(o_{1}, o_{2}, \cdots, o_{T}\right)\)计算在模型\(\lambda\)下观测序列O出现的概率\(P(O|λ)\).</p>
<ol>
<li>直接计算：列举所有可能长度为T的状态序列,求各个状态序列I与观测序列O的联合概率,但计算量太大,实际操作不可行.</li>
<li><strong>前向算法</strong>：定义到时刻t部分观测序列为\(o_1\)~\(o_t\)且状态为\(q_i\)的概率为<strong>前向概率</strong>,记作\(\alpha_{t}(i)=P\left(o_{1}, o_{2}, \cdots, o_{t}, i_{t}=q_{i} | \lambda\right)\).初始化前向概率\(\alpha_{1}(i)=\pi_{i} b_{i}\left(o_{1}\right), \quad i=1,2, \cdots, N\)，递推，对\(t=1\) ~ \(T-1\),\(\alpha_{t+1}(i)=\left[\sum_{j=1}^{N} \alpha_{t}(j) a_{j i}\right] b_{i}\left(o_{t+1}\right), \quad i=1,2, \cdots, N\),得到\(P(O | \lambda)=\sum_{i=1}^{N} \alpha_{T}(i)\)减少计算量的原因在于每一次计算直接引用前一个时刻的计算结果,避免重复计算.</li>
<li><strong>后向算法</strong>:定义在时刻t状态为\(q_i\)的条件下,从t+1到T的部分观测序列为\(o_{i+1}\)~\(o_T\)的概率为<strong>后向概率</strong>,记作\(\beta_{t}(i)=P\left(o_{t+1}, o_{t+2}, \cdots, o_{r} | i_{t}=q_{i}, \lambda\right)\).初始化后向概率\(\beta_{r}(i)=1, \quad i=1,2, \cdots, N\),递推,对\(t=T-1\)~\(1\)\(\beta_{t}(i)=\sum_{j=1}^{N} a_{i j} b_{j}\left(o_{i+1}\right) \beta_{i+1}(j), \quad i=1,2, \cdots, N\),得到\(P(O | \lambda)=\sum_{i=1}^{N} \pi_{i} b_{i}\left(o_{1}\right) \beta_{1}(i)\)</li>
</ol>
<p><strong>2. 学习算法</strong></p>
<p>已知观测序列\(O=(o_1,o_2, \cdots,o_r)\),估计模型\(\lambda = (A,B,\pi)\),的参数,使得在该模型下观测序列概率\(p(O|\lambda)\)最大.根据训练数据是否包括观察序列对应的状态序列分别由监督学习与非监督学习实现.</p>
<ol>
<li>
<p>监督学习：估计转移概率\(\hat{a}_{i j}=\frac{A_{i j}}{\sum_{j=1}^{N} A_{i j}}, \quad i=1,2, \cdots, N ; \quad j=1,2, \cdots, N\) 和观测概率\(\hat{b}_{j}(k)=\frac{B_{j k}}{\sum_{k=1}^{M} B_{j k}}, \quad j=1,2, \cdots, N, k=1,2, \cdots, M\).初始状态概率\(\pi_i\)的估计为S个样本中初始状态为\(q_i\)的频率.</p>
</li>
<li>
<p><strong>非监督学习(Baum-Welch算法)</strong>:将观测序列数据看作观测数据O,状态序列数据看作不可观测的<strong>隐数据</strong>I.首先确定完全数据的对数似然函数\(log p(O,I|\lambda)\),求Q函数</p>
</li>
</ol>
<p>\[
   \begin{aligned}
   Q(\lambda, \bar{\lambda})=&amp; \sum_{t} \log \pi_{i_1} P(O, I | \bar{\lambda}) \\
   &amp;+\sum_{I}\left(\sum_{i=1}^{I-1} \log a_{i_t,i_{t+1}}\right) P(O, I | \bar{\lambda}) \\
   &amp;+\sum_{I}\left(\sum_{i=1}^{T} \log b_{i_t}\left(o_{t}\right)\right) P(O, I | \bar{\lambda})
   \end{aligned}
\]</p>
<p>,用拉格朗日乘子法极大化Q函数求模型参数\(\pi_{i}=\frac{P\left(O, i_{1}=i | \bar{\lambda}\right)}{P(O | \bar{\lambda})}\),\(a_{i j}=\frac{\sum_{i=1}^{T-1} P\left(O, i_{t}=i, i_{t+1}=j | \bar{\lambda}\right)}{\sum_{t=1}^{T-1} P\left(O, i_{t}=i | \bar{\lambda}\right)}\),\(b_{j}(k)=\frac{\sum_{i=1}^{T} P\left(O, i_{t}=j | \bar{\lambda}\right) I\left(o_{i}=v_{k}\right)}{\sum_{i=1}^{T} P\left(O, i_{t}=j | \bar{\lambda}\right)}\),</p>
<p><strong>3. 预测问题</strong></p>
<p>也称为解码问题.已知模型\(\lambda = (A,B,\pi)\)和观测序列\(O=(O_1,O_2,\cdots,O_T)\),求对给定观测序列条件概率\(P(I|O)\)最大的状态序列\(I=(i_1,i_2,\cdots,i_T)\)</p>
<ol>
<li>
<p><strong>近似算法</strong>: 在每个时刻t选择在该时刻最有可能出现的状态\(i_t^*\),从而得到一个状态序列作为预测的结果.优点是<strong>计算简单</strong>,缺点是不能保证状态序列整体是最有可能的状态序列</p>
</li>
<li>
<p><strong>维特比算法</strong>:用<strong>动态规划</strong>求概率最大路径,这一条路径对应着一个状态序列.从t=1开始,递推地计算在时刻t状态为i的各条部分路径的最大概率,直至得到时刻t=T状态为i的各条路径的最大概率.时刻t=T的最大概率即为<strong>最优路径</strong>的概率\(P^\star\),最优路径的<strong>终结点</strong>\(i_t^\star\)也同时得到,之后从终结点开始由后向前逐步求得<strong>结点</strong>,得到最优路径.</p>
</li>
</ol>
<h5 id="jian-jie-1">简介</h5>
<p>在分词算法中，分词对应着三大问题中的预测问题（解码问题），隐马尔可夫经常用作能够发现新词的算法，通过海量的数据学习，能够将人名、地名、互联网上的新词等一一识别出来，具有广泛的应用场景。</p>
<p>其分词过程:</p>
<p><img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/hmm_seg.png" alt="avatar"></p>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-4"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>数据准备：分好词的词库</p>
<p>step 1:通过统计语料库中词的频次，计算三个概率：初始状态概率start，状态转移概率矩阵trans，发射概率emit。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HmmTrain</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.line_index = <span class="number">-1</span></span><br><span class="line">        self.char_set = set()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init</span><span class="params">(self)</span>:</span>  <span class="comment"># 初始化字典</span></span><br><span class="line">        trans_dict = &#123;&#125;  <span class="comment"># 存储状态转移概率</span></span><br><span class="line">        emit_dict = &#123;&#125;  <span class="comment"># 发射概率(状态-&gt;词语的条件概率)</span></span><br><span class="line">        Count_dict = &#123;&#125;  <span class="comment"># 存储所有状态序列 ，用于归一化分母</span></span><br><span class="line">        start_dict = &#123;&#125;  <span class="comment"># 存储状态的初始概率</span></span><br><span class="line">        state_list = [<span class="string">'B'</span>, <span class="string">'M'</span>, <span class="string">'E'</span>, <span class="string">'S'</span>]  <span class="comment"># 状态序列</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> state <span class="keyword">in</span> state_list:</span><br><span class="line">            trans_dict[state] = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> state1 <span class="keyword">in</span> state_list:</span><br><span class="line">                trans_dict[state][state1] = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> state <span class="keyword">in</span> state_list:</span><br><span class="line">            start_dict[state] = <span class="number">0.0</span></span><br><span class="line">            emit_dict[state] = &#123;&#125;</span><br><span class="line">            Count_dict[state] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(trans_dict) #&#123;'B': &#123;'B': 0.0, 'S': 0.0, 'M': 0.0, 'E': 0.0&#125;, 'S': &#123;'B': 0.0, 'S': 0.0, 'M': 0.0, 'E': 0.0&#125;,。。。&#125;</span></span><br><span class="line">        <span class="comment"># print(emit_dict) # &#123;'B': &#123;&#125;, 'S': &#123;&#125;, 'M': &#123;&#125;, 'E': &#123;&#125;&#125;</span></span><br><span class="line">        <span class="comment"># print(start_dict) # &#123;'B': 0.0, 'S': 0.0, 'M': 0.0, 'E': 0.0&#125;</span></span><br><span class="line">        <span class="comment"># print(Count_dict) # &#123;'B': 0, 'S': 0, 'M': 0, 'E': 0&#125;</span></span><br><span class="line">        <span class="keyword">return</span> trans_dict, emit_dict, start_dict, Count_dict</span><br><span class="line"></span><br><span class="line">    <span class="string">'''保存模型'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_model</span><span class="params">(self, word_dict, model_path)</span>:</span></span><br><span class="line">        f = open(model_path, <span class="string">'w'</span>)</span><br><span class="line">        f.write(str(word_dict))</span><br><span class="line">        f.close()</span><br><span class="line"></span><br><span class="line">    <span class="string">'''词语状态转换'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_status</span><span class="params">(self, word)</span>:</span>  <span class="comment"># 根据词语，输出词语对应的SBME状态</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        S:单字词</span></span><br><span class="line"><span class="string">        B:词的开头</span></span><br><span class="line"><span class="string">        M:词的中间</span></span><br><span class="line"><span class="string">        E:词的末尾</span></span><br><span class="line"><span class="string">        能 ['S']</span></span><br><span class="line"><span class="string">        前往 ['B', 'E']</span></span><br><span class="line"><span class="string">        科威特 ['B', 'M', 'E']</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        word_status = []</span><br><span class="line">        <span class="keyword">if</span> len(word) == <span class="number">1</span>:</span><br><span class="line">            word_status.append(<span class="string">'S'</span>)</span><br><span class="line">        <span class="keyword">elif</span> len(word) == <span class="number">2</span>:</span><br><span class="line">            word_status = [<span class="string">'B'</span>, <span class="string">'E'</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            M_num = len(word) - <span class="number">2</span></span><br><span class="line">            M_list = [<span class="string">'M'</span>] * M_num</span><br><span class="line">            word_status.append(<span class="string">'B'</span>)</span><br><span class="line">            word_status.extend(M_list)</span><br><span class="line">            word_status.append(<span class="string">'E'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> word_status</span><br><span class="line"></span><br><span class="line">    <span class="string">'''基于人工标注语料库，训练发射概率，初始状态， 转移概率'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, train_filepath, trans_path, emit_path, start_path)</span>:</span></span><br><span class="line">        trans_dict, emit_dict, start_dict, Count_dict = self.init()</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> open(train_filepath):</span><br><span class="line">            self.line_index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            line = line.strip()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            char_list = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(line)):</span><br><span class="line">                <span class="keyword">if</span> line[i] == <span class="string">" "</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                char_list.append(line[i])</span><br><span class="line"></span><br><span class="line">            self.char_set = set(char_list)  <span class="comment"># 训练预料库中所有字的集合</span></span><br><span class="line"></span><br><span class="line">            word_list = line.split(<span class="string">" "</span>)</span><br><span class="line">            line_status = []  <span class="comment"># 统计状态序列</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> word_list:</span><br><span class="line">                line_status.extend(self.get_word_status(word))  <span class="comment"># 一句话对应一行连续的状态</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> len(char_list) == len(line_status):</span><br><span class="line">                <span class="comment"># print(word_list) # ['但', '从', '生物学', '眼光', '看', '就', '并非', '如此', '了', '。']</span></span><br><span class="line">                <span class="comment"># print(line_status) # ['S', 'S', 'B', 'M', 'E', 'B', 'E', 'S', 'S', 'B', 'E', 'B', 'E', 'S', 'S']</span></span><br><span class="line">                <span class="comment"># print('******')</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(len(line_status)):</span><br><span class="line">                    <span class="keyword">if</span> i == <span class="number">0</span>:  <span class="comment"># 如果只有一个词，则直接算作是初始概率</span></span><br><span class="line">                        start_dict[line_status[<span class="number">0</span>]] += <span class="number">1</span>  <span class="comment"># start_dict记录句子第一个字的状态，用于计算初始状态概率</span></span><br><span class="line">                        Count_dict[line_status[<span class="number">0</span>]] += <span class="number">1</span>  <span class="comment"># 记录每一个状态的出现次数</span></span><br><span class="line">                    <span class="keyword">else</span>:  <span class="comment"># 统计上一个状态到下一个状态，两个状态之间的转移概率</span></span><br><span class="line">                        trans_dict[line_status[i - <span class="number">1</span>]][line_status[i]] += <span class="number">1</span>  <span class="comment"># 用于计算转移概率</span></span><br><span class="line">                        Count_dict[line_status[i]] += <span class="number">1</span></span><br><span class="line">                        <span class="comment"># 统计发射概率</span></span><br><span class="line">                        <span class="keyword">if</span> char_list[i] <span class="keyword">not</span> <span class="keyword">in</span> emit_dict[line_status[i]]:</span><br><span class="line">                            emit_dict[line_status[i]][char_list[i]] = <span class="number">0.0</span></span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            emit_dict[line_status[i]][char_list[i]] += <span class="number">1</span>  <span class="comment"># 用于计算发射概率</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(emit_dict)#&#123;'S': &#123;'否': 10.0, '昔': 25.0, '直': 238.0, '六': 1004.0, '殖': 17.0, '仗': 36.0, '挪': 15.0, '朗': 151.0</span></span><br><span class="line">        <span class="comment"># print(trans_dict)#&#123;'S': &#123;'S': 747969.0, 'E': 0.0, 'M': 0.0, 'B': 563988.0&#125;, 'E': &#123;'S': 737404.0, 'E': 0.0, 'M': 0.0, 'B': 651128.0&#125;,</span></span><br><span class="line">        <span class="comment"># print(start_dict) #&#123;'S': 124543.0, 'E': 0.0, 'M': 0.0, 'B': 173416.0&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 进行归一化</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> start_dict:  <span class="comment"># 状态的初始概率</span></span><br><span class="line">            start_dict[key] = start_dict[key] * <span class="number">1.0</span> / self.line_index</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> trans_dict:  <span class="comment"># 状态转移概率</span></span><br><span class="line">            <span class="keyword">for</span> key1 <span class="keyword">in</span> trans_dict[key]:</span><br><span class="line">                trans_dict[key][key1] = trans_dict[key][key1] / Count_dict[key]</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> emit_dict:  <span class="comment"># 发射概率(状态-&gt;词语的条件概率)</span></span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> emit_dict[key]:</span><br><span class="line">                emit_dict[key][word] = emit_dict[key][word] / Count_dict[key]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(emit_dict)#&#123;'S': &#123;'否': 6.211504202703743e-06, '昔': 1.5528760506759358e-05, '直': 0.0001478338000243491,</span></span><br><span class="line">        <span class="comment"># print(trans_dict)#&#123;'S': &#123;'S': 0.46460125869921165, 'E': 0.0, 'M': 0.0, 'B': 0.3503213832274479&#125;,</span></span><br><span class="line">        <span class="comment"># print(start_dict)#&#123;'S': 0.41798844132394497, 'E': 0.0, 'M': 0.0, 'B': 0.5820149148537713&#125;</span></span><br><span class="line">        self.save_model(trans_dict, trans_path)</span><br><span class="line">        self.save_model(emit_dict, emit_path)</span><br><span class="line">        self.save_model(start_dict, start_path)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> trans_dict, emit_dict, start_dict</span><br></pre></td></tr></table></figure>
<p>Step 2: 使用维特比算法，求解概率最大路径</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HmmCut</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,start_model_path,trans_model_path,emit_model_path)</span>:</span></span><br><span class="line">        self.prob_trans = self.load_model(trans_model_path)</span><br><span class="line">        self.prob_emit = self.load_model(emit_model_path)</span><br><span class="line">        self.prob_start = self.load_model(start_model_path)</span><br><span class="line"></span><br><span class="line">    <span class="string">'''加载模型'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_model</span><span class="params">(self, model_path)</span>:</span></span><br><span class="line">        f = open(model_path, <span class="string">'r'</span>)</span><br><span class="line">        a = f.read()</span><br><span class="line">        word_dict = eval(a)</span><br><span class="line">        f.close()</span><br><span class="line">        <span class="keyword">return</span> word_dict</span><br><span class="line"></span><br><span class="line">    <span class="string">'''verterbi算法求解'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(self, obs, states, start_p, trans_p, emit_p)</span>:</span>  <span class="comment"># 维特比算法（一种递归算法）</span></span><br><span class="line">        <span class="comment"># 算法的局限在于训练语料要足够大，需要给每个词一个发射概率,.get(obs[0], 0)的用法是如果dict中不存在这个key,则返回0值</span></span><br><span class="line">        V = [&#123;&#125;]</span><br><span class="line">        path = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">            V[<span class="number">0</span>][y] = start_p[y] * emit_p[y].get(obs[<span class="number">0</span>], <span class="number">0</span>)  <span class="comment"># 在位置0，以y状态为末尾的状态序列的最大概率</span></span><br><span class="line">            path[y] = [y]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, len(obs)):</span><br><span class="line">            V.append(&#123;&#125;)</span><br><span class="line">            newpath = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">                state_path = ([(V[t - <span class="number">1</span>][y0] * trans_p[y0].get(y, <span class="number">0</span>) * emit_p[y].get(obs[t], <span class="number">0</span>), y0) <span class="keyword">for</span> y0 <span class="keyword">in</span> states <span class="keyword">if</span> V[t - <span class="number">1</span>][y0] &gt; <span class="number">0</span>])</span><br><span class="line">                <span class="keyword">if</span> state_path == []:</span><br><span class="line">                    (prob, state) = (<span class="number">0.0</span>, <span class="string">'S'</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    (prob, state) = max(state_path)</span><br><span class="line">                V[t][y] = prob</span><br><span class="line">                newpath[y] = path[state] + [y]</span><br><span class="line"></span><br><span class="line">            path = newpath  <span class="comment"># 记录状态序列</span></span><br><span class="line">        (prob, state) = max([(V[len(obs) - <span class="number">1</span>][y], y) <span class="keyword">for</span> y <span class="keyword">in</span> states])  <span class="comment"># 在最后一个位置，以y状态为末尾的状态序列的最大概率</span></span><br><span class="line">        <span class="keyword">return</span> (prob, path[state])  <span class="comment"># 返回概率和状态序列</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分词主控函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cut</span><span class="params">(self, sent)</span>:</span></span><br><span class="line">        prob, pos_list = self.viterbi(sent, (<span class="string">'B'</span>, <span class="string">'M'</span>, <span class="string">'E'</span>, <span class="string">'S'</span>), self.prob_start, self.prob_trans, self.prob_emit)</span><br><span class="line">        seglist = list()</span><br><span class="line">        word = list()</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(len(pos_list)):</span><br><span class="line">            <span class="keyword">if</span> pos_list[index] == <span class="string">'S'</span>:</span><br><span class="line">                word.append(sent[index])</span><br><span class="line">                seglist.append(word)</span><br><span class="line">                word = []</span><br><span class="line">            <span class="keyword">elif</span> pos_list[index] <span class="keyword">in</span> [<span class="string">'B'</span>, <span class="string">'M'</span>]:</span><br><span class="line">                word.append(sent[index])</span><br><span class="line">            <span class="keyword">elif</span> pos_list[index] == <span class="string">'E'</span>:</span><br><span class="line">                word.append(sent[index])</span><br><span class="line">                seglist.append(word)</span><br><span class="line">                word = []</span><br><span class="line">        seglist = [<span class="string">''</span>.join(tmp) <span class="keyword">for</span> tmp <span class="keyword">in</span> seglist]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> seglist</span><br></pre></td></tr></table></figure>
<p>如果画出状态转移图像，发现状态只能在层间点转移，转移路径上的分数为概率，求一条概率最大的路径。这可以用维特比算法计算，本质上就是一个动态规划</p>
<p><img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/hmm_viterbi.png" alt="avatar"></p>
<h5 id="zong-jie-1">总结</h5>
<p><strong>从三方面比较HMM和N-gram。</strong></p>
<ol>
<li>HMM模型是一个生成模型，区别于N-gram语言模型，HMM没有直接对给定观测值后状态的分布 \(P(S|O)\)（<em>O</em> 代表观测序列）进行建模，而是对状态序列本身的分布\(P(S)\)和给定状态后观测值的分布 \(p(O|S)\)建模 ；</li>
<li>学习过程与N-gram相同，HMM在有监督学习的情况下，使用极大似然估计参数；</li>
<li>预测时，HMM采用维特比算法。</li>
</ol>
<h4 id="crf-fen-ci">CRF分词</h4>
<h5 id="tiao-jian-sui-ji-chang-crf">条件随机场CRF</h5>
<p>条件随机场基于概率无向图模型，利用最大团理论，对随机变量的联合分布\(P(Y)\)进行建模。</p>
<p>在序列标注任务中的条件随机场，往往是指<strong>线性链条件随机场</strong>，这时，在条件概率模型\(P(Y|X)\)中，\(Y\)是输出变量，表示标记序列（或状态序列），\(X\)是输入变量，表示需要标注的观测序列。学习时，利用训练数据集通过极大似然估计或正则化的极大似然估计得到条件概率模型\(p(Y|X)\)；预测时，对于给定的输入序列x，求出条件概率\(p(y|x)\)最大的输出序列y.</p>
<h6 id="tiao-jian-sui-ji-chang-de-can-shu-hua-xing-shi">条件随机场的参数化形式</h6>
<p>\[
P(y|x)=\frac{1}{Z(x)} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, j} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right) 
\\
Z(x)=\sum_{y} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, j} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right)
\]</p>
<h5 id="tiao-jian-sui-ji-chang-fen-ci-fang-fa">条件随机场分词方法</h5>
<p>条件随机场和隐马尔可夫一样，也是使用BMES四个状态位来进行分词。以如下句子为例：</p>
<p>中 国 是 泱 泱 大 国<br>
B  B  B  B  B  B  B<br>
M M M M M M M<br>
E  E  E  E  E  E  E<br>
S  S  S  S  S  S  S</p>
<p>条件随机场解码就是在以上由标记组成的数组中搜索一条最优的路径。</p>
<p>要把每一个字(即观察变量)对应的每一个状态BMES(即标记变量)的概率都求出来。例如对于观察变量“国”，当前标记变量为E，前一个观察变量为“中”，前一个标记变量为B，则：t(B, E, ‘国’) 对应到条件随机场里相邻标记变量\((y_{i-1},y_i)\)的势函数。s(E, ‘国’) 对应到条件随机场里单个标记变量\(y_i\)对应的势函数\(s_l(y_i,x,i)\)。t(B, E, ‘国’), s(E, ‘国’)相应的权值\(λ_k,\mu_l\)， 都是由条件随机场用大量的标注语料训练出来。因此分词的标记识别就是求对于各个观察变量，它们的标记变量(BMES)状态序列的概率最大值，即求：的概率组合最大值。这个解法与隐马尔可夫类似，可以用viterbi算法求解。</p>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-5"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>使用crf<ins>实现对模型的训练，crf</ins> 安装、数据格式及模板参数请参考：<a href="https://jeffery0628.github.io/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/" target="_blank" rel="noopener">序列标注</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> CRFPP</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRFModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model=<span class="string">'model_name'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 类初始化</span></span><br><span class="line"><span class="string">        :param model: 模型名称</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_tagger</span><span class="params">(self, tag_data)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 添加语料</span></span><br><span class="line"><span class="string">        :param tag_data: 数据</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        word_str = tag_data.strip()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(self.model):</span><br><span class="line">            print(<span class="string">'模型不存在,请确认模型路径是否正确!'</span>)</span><br><span class="line">            exit()</span><br><span class="line">        tagger = CRFPP.Tagger(<span class="string">"-m &#123;&#125; -v 3 -n2"</span>.format(self.model))</span><br><span class="line">        tagger.clear()</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> word_str:</span><br><span class="line">            tagger.add(word)</span><br><span class="line">        tagger.parse()</span><br><span class="line">        <span class="keyword">return</span> tagger</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">text_mark</span><span class="params">(self, tag_data, begin=<span class="string">'B'</span>, middle=<span class="string">'I'</span>, end=<span class="string">'E'</span>, single=<span class="string">'S'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        文本标记</span></span><br><span class="line"><span class="string">        :param tag_data: 数据</span></span><br><span class="line"><span class="string">        :param begin: 开始标记</span></span><br><span class="line"><span class="string">        :param middle: 中间标记</span></span><br><span class="line"><span class="string">        :param end: 结束标记</span></span><br><span class="line"><span class="string">        :param single: 单字结束标记</span></span><br><span class="line"><span class="string">        :return result: 标记列表</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        tagger = self.add_tagger(tag_data)</span><br><span class="line">        size = tagger.size()</span><br><span class="line">        tag_text = <span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, size):</span><br><span class="line">            word, tag = tagger.x(i, <span class="number">0</span>), tagger.y2(i)</span><br><span class="line">            <span class="keyword">if</span> tag <span class="keyword">in</span> [begin, middle]:</span><br><span class="line">                tag_text += word</span><br><span class="line">            <span class="keyword">elif</span> tag <span class="keyword">in</span> [end, single]:</span><br><span class="line">                tag_text += word + <span class="string">"*&amp;*"</span></span><br><span class="line">        result = tag_text.split(<span class="string">'*&amp;*'</span>)</span><br><span class="line">        result.pop()</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crf_test</span><span class="params">(self, tag_data, separator=<span class="string">'/'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: crf测试</span></span><br><span class="line"><span class="string">        :param tag_data:</span></span><br><span class="line"><span class="string">        :param separator:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        result = self.text_mark(tag_data)</span><br><span class="line">        data = separator.join(result)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crf_learn</span><span class="params">(self, filename)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 训练模型</span></span><br><span class="line"><span class="string">        :param filename: 已标注数据源</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        crf_bash = <span class="string">"crf_learn -f 3 -c 4.0 data/template.txt &#123;&#125; &#123;&#125;"</span>.format(filename, self.model)</span><br><span class="line">        process = subprocess.Popen(crf_bash.split(), stdout=subprocess.PIPE)</span><br><span class="line">        output = process.communicate()[<span class="number">0</span>]</span><br><span class="line">        print(output.decode(encoding=<span class="string">'utf-8'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    crf_model = CRFModel(model=<span class="string">'data/model_crf'</span>)</span><br><span class="line">    crf_model.crf_learn(filename=<span class="string">'data/train_file_crf.txt'</span>)</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    print(crf_model.crf_test(tag_data=<span class="string">'我们一定要战胜敌人，我们认为它们都是纸老虎。'</span>))</span><br></pre></td></tr></table></figure>
<h5 id="tiao-jian-sui-ji-chang-fen-ci-de-you-que-dian">条件随机场分词的优缺点</h5>
<p>条件随机场分词是一种精度很高的分词方法，它比隐马尔可夫的精度要高，是因为隐马尔可夫假设观察变量\(x_i\)只与当前状态\(y_i\)有关，而与其它状态\(y_{i-1}\)，\(y_{i+1}\)无关;而条件随机场假设了当前观察变量\(x_i\)与上下文相关，如 ，就是考虑到上一个字标记状态为B时，当前标记状态为E并且输出“国”字的概率。因此通过上下文的分析，条件随机场分词会提升到更高的精度。但因为复杂度比较高，条件随机场一般训练代价都比较大。</p>
<h4 id="jie-gou-hua-gan-zhi-ji-fen-ci-suan-fa">结构化感知机分词算法</h4>
<p>利用隐马尔科夫模型实现基于序列标注的中文分词器，效果并不理想。事实上，隐马尔可夫模型假设人们说的话仅仅取决于一个隐藏的BMES序列，这个假设太单纯了，不符合语言规律。语言不是由这么简单的标签序列生成，语言含有更多特征，而隐马尔科夫模型没有捕捉到。<strong>隐马弥可夫模型能捕捉的特征仅限于两种: 其一，前一个标签是什么；其二，当前字符是什么</strong>。为了利用更多的特征，线性模型( linear model )应运而生。线性模型由两部分构成: 一系列用来提取特征的特征函数\(\phi\)，以及相应的权重向量\(w\)。</p>
<h5 id="gan-zhi-ji-suan-fa">感知机算法</h5>
<p>感知机算法是一种迭代式的算法：在训练集上运行多个迭代，每次读入一个样本，执行预测，将预测结果与正确答案进行对比，计算误差，根据误差更新模型参数，再次进行训练，直到误差最小为止。</p>
<ul>
<li><strong>损失函数</strong>: 从数值优化的角度来讲，迭代式机器学习算法都在优化(减小)一个损失函数。损失函数 J(w) 用来衡量模型在训练集上的错误程度，自变量是模型参数 \(w\)，因变量是一个标量，表示模型在训练集上的损失的大小。</li>
<li><strong>梯度下降</strong>: 给定样本，其特征向量 \(x\) 只是常数，对 \(J(w)\) 求导，得到一个梯度向量 \(\Delta w\)，它的反方向一定是当前位置损失函数减小速度最快的方向。如果参数点 \(w\) 反方向移动就会使损失函数减小，叫梯度下降。</li>
<li><strong>学习率</strong>: 梯度下降的步长叫做学习率。</li>
<li><strong>随机梯度下降</strong>(SGD): 如果算法每次迭代随机选取部分样本计算损失函数的梯度，则称为随机梯度下降。</li>
</ul>
<p>假如数据本身线性不可分，感知机损失函数不会收敛，每次迭代分离超平面都会剧烈振荡。这时可以对感知机算法打补丁，使用投票感知机或平均感知机。</p>
<ol>
<li>
<p><strong>投票感知机</strong>：每次迭代的模型都保留，准确率也保留，预测时，每个模型都给出自己的结果，乘以它的准确率加权平均值作为最终结果。</p>
</li>
<li>
<p>投票感知机要求存储多个模型及加权，计算开销较大，更实际的做法是取多个模型的权重的平均，这就是<strong>平均感知机</strong>。</p>
</li>
</ol>
<h5 id="jie-gou-hua-yu-ce-wen-ti">结构化预测问题</h5>
<p>自然语言处理问题大致可分为两类，一种是分类问题，另一种就是结构化预测问题，序列标注只是结构化预测的一个特例，对感知机稍作拓展，分类器就能支持结构化预测。<br>
信息的层次结构特点称作结构化。<strong>那么结构化预测</strong>(structhre，prediction)则是预测对象结构的一类监督学习问题。相应的模型训练过程称作<strong>结构化学习</strong>(stutured laming )。分类问题的预测结果是一个决策边界， 回归问题的预测结果是一个实数标量，而结构化预测的结果则是一个完整的结构。<br>
自然语言处理中有许多任务是结构化预测，比如序列标注预测结构是一整个序列，句法分析预测结构是一棵句法树，机器翻译预测结构是一段完整的译文。这些结构由许多部分构成，最小的部分虽然也是分类问题(比如中文分词时每个字符分类为{B,M,E,S} ),但必须考虑结构整体的合理程度。</p>
<h6 id="jie-gou-hua-yu-ce-yu-xue-xi-liu-cheng">结构化预测与学习流程</h6>
<p>结构化预测的过程就是给定一个模型 λ 及打分函数 score，利用打分函数给一些备选结构打分，选择分数最高的结构作为预测输出，公式如下:<br>
\[
\hat{y}=\arg \max _{y \in Y} \operatorname{score}({\lambda}(x, y))
\]<br>
其中，Y 是备选结构的集合。既然结构化预测就是搜索得分最高的结构 y，那么结构化学习的目标就是想方设法让正确答案 y 的得分最高。不同的模型有不同的算法，对于线性模型，训练算法为结构化感知机。</p>
<h5 id="jie-gou-hua-gan-zhi-ji-suan-fa">结构化感知机算法</h5>
<p>要让线性模型支持结构化预测，必须先设计打分函数。打分函数的输入有两个缺一不可的参数: 特征 \(x\) 和结构\(y\)。但之前的线性模型的“打分函数”只接受一个自变量 \(x\)。做法是定义新的特征函数\(\phi (x,y)\)，把结构 \(y\) 也作为一种特征，输出新的“结构化特征向量”。新特征向量与权重向量做点积后，就得到一个标量，将其作为分数:<br>
\[
\operatorname{score}(x, y)=w \cdot \phi(x, y)
\]<br>
打分函数有了，取分值最大的结构作为预测结果，得到结构化预测函数:<br>
\[
\hat{y}=\arg \max _{y \in Y}(w \cdot \phi(x, y))
\]<br>
预测函数与线性分类器的决策函数很像，都是权重向量点积特征向量。那么感知机算法也可以拓展复用，得到线性模型的结构化学习算法:</p>
<ol>
<li>读入样本 \((x,y)\)，进行结构化预测\(\hat{y}=\arg \max _{y \in Y}(w \cdot \phi(x, y))\)</li>
<li>与正确答案相比，若不相等，则更新参数: 奖励正确答案触发的特征函数的权重，否则进行惩罚:\(w \leftarrow w+\phi\left(x^{(i)}, y\right)-\phi\left(x^{(i)}, \hat{y}\right)\)</li>
<li>调整学习率:\(\boldsymbol{w} \leftarrow \boldsymbol{w}+\alpha\left(\phi\left(\boldsymbol{x}^{(i)}, \boldsymbol{y}\right)-\phi\left(\boldsymbol{x}^{(i)}, \hat{\boldsymbol{y}}\right)\right)\)</li>
</ol>
<h6 id="jie-gou-hua-gan-zhi-ji-yu-gan-zhi-ji-suan-fa-bi-jiao">结构化感知机与感知机算法比较</h6>
<ul>
<li>结构化感知机修改了特征向量。</li>
<li>结构化感知机的参数更新赏罚分明。</li>
</ul>
<h6 id="jie-gou-hua-gan-zhi-ji-yu-xu-lie-biao-zhu">结构化感知机与序列标注</h6>
<p>序列标注最大的结构特点就是标签相互之间的依赖性，这种依赖性利用初始状态概率想俩狗和状态转移概率矩阵体系那，那么对于结构化感知机，就可以使用<strong>转移特征</strong>来表示:<br>
\[
\phi_{k}\left(y_{t-1}, y_{t}\right)=\left\{\begin{array}{ll}
1, &amp; y_{t-1}=s_{i}, and , y_{t}=s_{j} \\
0
\end{array} \quad i=0, \cdots, N ; j=1, \cdots, N\right.
\]<br>
其中，\(y_t\)为序列第 t 个标签，\(s_i\)为标注集第 i 种标签，N 为标注集大小。</p>
<p><strong>状态特征</strong>，类似于隐马尔可夫模型的发射概率矩阵，状态特征只与当前的状态有关，与之前的状态无关:<br>
\[
\phi_{i}\left(x_{i}, y_{i}\right)=\left\{\begin{array}{l}
1 \\
0
\end{array}\right.
\]<br>
于是，结构化感知机的特征函数就是转移特征和状态特征的合集:<br>
\[
\phi=\left[\phi_{k} ; \phi_{l}\right] \quad k=1, \cdots, N^{2}+N ; l=N^{2}+N+1, \cdots
\]<br>
基于以上公式，统一用打分函数来表示:<br>
\[
\operatorname{score}(\boldsymbol{x}, \boldsymbol{y})=\sum_{t=1}^{T} \boldsymbol{w} \cdot \phi\left(y_{t-1}, y_{t}, \boldsymbol{x}_{t}\right)
\]<br>
有了打分公式，就可以利用维特比算法求解得分最高的序列。</p>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-6"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CPTTrain</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, segment, train)</span>:</span></span><br><span class="line">        self.__char_type = &#123;&#125;</span><br><span class="line">        data_path = <span class="string">"data"</span></span><br><span class="line">        <span class="keyword">for</span> ind, name <span class="keyword">in</span> enumerate([<span class="string">"punc"</span>, <span class="string">"alph"</span>, <span class="string">"date"</span>, <span class="string">"num"</span>]):</span><br><span class="line">            fn = data_path + <span class="string">"/"</span> + name</span><br><span class="line">            <span class="keyword">if</span> os.path.isfile(fn):</span><br><span class="line">                <span class="keyword">for</span> line <span class="keyword">in</span> open(fn, <span class="string">"r"</span>):</span><br><span class="line">                    self.__char_type[line.strip()] = ind</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">"can't open"</span>, fn)</span><br><span class="line">                exit()</span><br><span class="line"></span><br><span class="line">        self.__train_insts = <span class="literal">None</span>  <span class="comment"># all instances for training.</span></span><br><span class="line">        self.__feats_weight = <span class="literal">None</span>  <span class="comment"># ["b", "m", "e", "s"][all the features] --&gt; weight.</span></span><br><span class="line">        self.__words_num = <span class="literal">None</span>  <span class="comment"># total words num in all the instances.</span></span><br><span class="line">        self.__insts_num = <span class="literal">None</span>  <span class="comment"># namley the sentences' num.</span></span><br><span class="line">        self.__cur_ite_ID = <span class="literal">None</span>  <span class="comment"># current iteration index.</span></span><br><span class="line">        self.__cur_inst_ID = <span class="literal">None</span>  <span class="comment"># current index_th instance.</span></span><br><span class="line">        self.__real_inst_ID = <span class="literal">None</span>  <span class="comment"># the accurate index in training instances after randimizing.</span></span><br><span class="line">        self.__last_update = <span class="literal">None</span>  <span class="comment"># ["b".."s"][feature] --&gt; [last_update_ite_ID, last_update_inst_ID]</span></span><br><span class="line">        self.__feats_weight_sum = <span class="literal">None</span>  <span class="comment"># sum of ["b".."s"][feature] from begin to end.</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> segment <span class="keyword">and</span> train <span class="keyword">or</span> <span class="keyword">not</span> segment <span class="keyword">and</span> <span class="keyword">not</span> train:</span><br><span class="line">            print(<span class="string">"there is only a True and False in segment and train"</span>)</span><br><span class="line">            exit()</span><br><span class="line">        <span class="keyword">elif</span> train:</span><br><span class="line">            self.Train = self.__Train</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.__LoadModel()</span><br><span class="line">            self.Segment = self.__Segment</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__LoadModel</span><span class="params">(self)</span>:</span></span><br><span class="line">        model = <span class="string">"data/avgmodel"</span></span><br><span class="line">        print(<span class="string">"load"</span>, model, <span class="string">"..."</span>)</span><br><span class="line">        self.__feats_weight = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> os.path.isfile(model):</span><br><span class="line">            start = time.clock()</span><br><span class="line">            self.__feats_weight = pickle.load(open(model, <span class="string">"rb"</span>))</span><br><span class="line">            end = time.clock()</span><br><span class="line">            print(<span class="string">"It takes %d seconds"</span> % (end - start))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"can't open"</span>, model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Train</span><span class="params">(self, corp_file_name, max_train_num, max_ite_num)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.__LoadCorp(corp_file_name, max_train_num):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        starttime = time.clock()</span><br><span class="line"></span><br><span class="line">        self.__feats_weight = &#123;&#125;</span><br><span class="line">        self.__last_update = &#123;&#125;</span><br><span class="line">        self.__feats_weight_sum = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> self.__cur_ite_ID <span class="keyword">in</span> range(max_ite_num):</span><br><span class="line">            <span class="keyword">if</span> self.__Iterate():</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        self.__SaveModel()</span><br><span class="line">        endtime = time.clock()</span><br><span class="line">        print(<span class="string">"total iteration times is %d seconds"</span> % (endtime - starttime))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__GenerateFeats</span><span class="params">(self, inst)</span>:</span></span><br><span class="line">        inst_feat = []</span><br><span class="line">        <span class="keyword">for</span> ind, [c, tag, t] <span class="keyword">in</span> enumerate(inst):</span><br><span class="line">            inst_feat.append([])</span><br><span class="line">            <span class="keyword">if</span> t == <span class="number">-1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># Cn</span></span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">-2</span>, <span class="number">3</span>):</span><br><span class="line">                inst_feat[<span class="number">-1</span>].append(<span class="string">"C%d==%s"</span> % (n, inst[ind + n][<span class="number">0</span>]))</span><br><span class="line">            <span class="comment"># CnCn+1</span></span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">-2</span>, <span class="number">2</span>):</span><br><span class="line">                inst_feat[<span class="number">-1</span>].append(<span class="string">"C%dC%d==%s%s"</span> % (n, n + <span class="number">1</span>, inst[ind + n][<span class="number">0</span>], inst[ind + n + <span class="number">1</span>][<span class="number">0</span>]))</span><br><span class="line">            <span class="comment"># C-1C1</span></span><br><span class="line">            inst_feat[<span class="number">-1</span>].append(<span class="string">"C-1C1==%s%s"</span> % (inst[ind - <span class="number">1</span>][<span class="number">0</span>], inst[ind + <span class="number">1</span>][<span class="number">0</span>]))</span><br><span class="line">            <span class="comment"># Pu(C0)</span></span><br><span class="line">            inst_feat[<span class="number">-1</span>].append(<span class="string">"Pu(%s)==%d"</span> % (c, int(t == <span class="number">0</span>)))</span><br><span class="line">            <span class="comment"># T(C-2)T(C-1)T(C0)T(C1)T(C2)</span></span><br><span class="line">            inst_feat[<span class="number">-1</span>].append(<span class="string">"T-2...2=%d%d%d%d%d"</span> % (</span><br><span class="line">            inst[ind - <span class="number">2</span>][<span class="number">2</span>], inst[ind - <span class="number">1</span>][<span class="number">2</span>], inst[ind][<span class="number">2</span>], inst[ind + <span class="number">1</span>][<span class="number">2</span>], inst[ind + <span class="number">2</span>][<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> inst_feat</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__SaveModel</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># the last time to sum all the features.</span></span><br><span class="line">        norm = float(self.__cur_ite_ID + <span class="number">1</span>) * self.__insts_num</span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> self.__feats_weight_sum:</span><br><span class="line">            last_ite_ID = self.__last_update[feat][<span class="number">0</span>]</span><br><span class="line">            last_inst_ID = self.__last_update[feat][<span class="number">1</span>]</span><br><span class="line">            c = (self.__cur_ite_ID - last_ite_ID) * self.__insts_num + self.__cur_inst_ID - last_inst_ID</span><br><span class="line">            self.__feats_weight_sum[feat] += self.__feats_weight[feat] * c</span><br><span class="line">            self.__feats_weight_sum[feat] = self.__feats_weight_sum[feat] / norm</span><br><span class="line"></span><br><span class="line">        pickle.dump(self.__feats_weight_sum, open(<span class="string">"data/avgmodel"</span>, <span class="string">"wb"</span>))</span><br><span class="line">        self.__train_insts = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__LoadCorp</span><span class="params">(self, corp_file_name, max_train_num)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(corp_file_name):</span><br><span class="line">            print(<span class="string">"can't open"</span>, corp_file_name)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        self.__train_insts = []</span><br><span class="line">        self.__words_num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> ind, line <span class="keyword">in</span> enumerate(open(corp_file_name, <span class="string">"r"</span>)):</span><br><span class="line">            <span class="keyword">if</span> max_train_num &gt; <span class="number">0</span> <span class="keyword">and</span> ind &gt;= max_train_num:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            self.__train_insts.append(self.__PreProcess(line.strip()))</span><br><span class="line">            self.__words_num += len(self.__train_insts[<span class="number">-1</span>]) - <span class="number">4</span></span><br><span class="line">        self.__insts_num = len(self.__train_insts)</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"number of total insts is"</span>, self.__insts_num)</span><br><span class="line">        print(<span class="string">"number of total characters is"</span>, self.__words_num)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__PreProcess</span><span class="params">(self, sent)</span>:</span></span><br><span class="line">        inst = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            inst.append([<span class="string">"&lt;s&gt;"</span>, <span class="string">"s"</span>, <span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sent.split():</span><br><span class="line">            rt = word.rpartition(<span class="string">"/"</span>)</span><br><span class="line">            t = self.__char_type.get(rt[<span class="number">0</span>], <span class="number">4</span>)</span><br><span class="line">            inst.append([rt[<span class="number">0</span>], rt[<span class="number">2</span>], t])  <span class="comment"># [c, tag, t]</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            inst.append([<span class="string">"&lt;s&gt;"</span>, <span class="string">"s"</span>, <span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> inst</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Segment</span><span class="params">(self, src)</span>:</span></span><br><span class="line">        <span class="string">"""suppose there is one sentence once."""</span></span><br><span class="line">        inst = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            inst.append([<span class="string">"&lt;s&gt;"</span>, <span class="string">"s"</span>, <span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> src:</span><br><span class="line">            inst.append([c, <span class="string">""</span>, self.__char_type.get(c, <span class="number">4</span>)])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            inst.append([<span class="string">"&lt;s&gt;"</span>, <span class="string">"s"</span>, <span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        feats = self.__GenerateFeats(inst)</span><br><span class="line">        tags = self.__DPSegment(inst, feats)</span><br><span class="line"></span><br><span class="line">        rst = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(tags) - <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">if</span> tags[i] <span class="keyword">in</span> [<span class="string">"s"</span>, <span class="string">"b"</span>]:</span><br><span class="line">                rst.append(inst[i][<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                rst[<span class="number">-1</span>] += inst[i][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">" "</span>.join(rst)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Iterate</span><span class="params">(self)</span>:</span></span><br><span class="line">        start = time.clock()</span><br><span class="line">        print(<span class="string">"%d th iteration"</span> % self.__cur_ite_ID)</span><br><span class="line"></span><br><span class="line">        train_list = random.sample(range(self.__insts_num), self.__insts_num)</span><br><span class="line">        error_sents_num = <span class="number">0</span></span><br><span class="line">        error_words_num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> self.__cur_inst_ID, self.__real_inst_ID <span class="keyword">in</span> enumerate(train_list):</span><br><span class="line">            num = self.__TrainInstance()</span><br><span class="line">            error_sents_num += <span class="number">1</span> <span class="keyword">if</span> num &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            error_words_num += num</span><br><span class="line"></span><br><span class="line">        st = <span class="number">1</span> - float(error_sents_num) / self.__insts_num</span><br><span class="line">        wt = <span class="number">1</span> - float(error_words_num) / self.__words_num</span><br><span class="line"></span><br><span class="line">        end = time.clock()</span><br><span class="line">        print(<span class="string">"sents accuracy = %f%%, words accuracy = %f%%, it takes %d seconds"</span> % (st * <span class="number">100</span>, wt * <span class="number">100</span>, end - start))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> error_sents_num == <span class="number">0</span> <span class="keyword">and</span> error_words_num == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__TrainInstance</span><span class="params">(self)</span>:</span></span><br><span class="line">        cur_inst = self.__train_insts[self.__real_inst_ID]</span><br><span class="line">        feats = self.__GenerateFeats(cur_inst)</span><br><span class="line"></span><br><span class="line">        seg = self.__DPSegment(cur_inst, feats)</span><br><span class="line">        <span class="keyword">return</span> self.__Correct(seg, feats)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__DPSegment</span><span class="params">(self, inst, feats)</span>:</span></span><br><span class="line">        num = len(inst)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get all position's score.</span></span><br><span class="line">        value = [&#123;&#125; <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, num - <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> [<span class="string">"b"</span>, <span class="string">"m"</span>, <span class="string">"e"</span>, <span class="string">"s"</span>]:</span><br><span class="line">                value[i][t] = self.__GetScore(i, t, feats)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># find optimal path.</span></span><br><span class="line">        tags = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]</span><br><span class="line">        best = [<span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]  <span class="comment"># best[i]: [i, i + length(i)) is optimal segment.</span></span><br><span class="line">        length = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num - <span class="number">2</span> - <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">for</span> dis <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">                <span class="keyword">if</span> i + dis &gt; num - <span class="number">2</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                cur_score = best[i + dis]</span><br><span class="line">                self.__Tag(i, i + dis, tags)</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(i, i + dis):</span><br><span class="line">                    cur_score += value[k][tags[k]]</span><br><span class="line">                <span class="keyword">if</span> length[i] <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> cur_score &gt; best[i]:</span><br><span class="line">                    best[i] = cur_score</span><br><span class="line">                    length[i] = dis</span><br><span class="line"></span><br><span class="line">        i = <span class="number">2</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; num - <span class="number">2</span>:</span><br><span class="line">            self.__Tag(i, i + length[i], tags)</span><br><span class="line">            i += length[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tags</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__GetScore</span><span class="params">(self, pos, t, feats)</span>:</span></span><br><span class="line">        pos_feats = feats[pos]</span><br><span class="line">        score = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> pos_feats:</span><br><span class="line">            score += self.__feats_weight.get(feat + <span class="string">"=&gt;"</span> + t, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Tag</span><span class="params">(self, f, t, tags)</span>:</span></span><br><span class="line">        <span class="string">"""tag the sequence tags in the xrange of [f, t)"""</span></span><br><span class="line">        <span class="keyword">if</span> t - f == <span class="number">1</span>:</span><br><span class="line">            tags[f] = <span class="string">"s"</span></span><br><span class="line">        <span class="keyword">elif</span> t - f &gt;= <span class="number">2</span>:</span><br><span class="line">            tags[f], tags[t - <span class="number">1</span>] = <span class="string">"b"</span>, <span class="string">"e"</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(f + <span class="number">1</span>, t - <span class="number">1</span>):</span><br><span class="line">                tags[i] = <span class="string">"m"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Correct</span><span class="params">(self, tags, feats)</span>:</span></span><br><span class="line">        updates = &#123;&#125;</span><br><span class="line">        cur_inst = self.__train_insts[self.__real_inst_ID]</span><br><span class="line">        error_words_num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(cur_inst) - <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">if</span> tags[i] == cur_inst[i][<span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            error_words_num += <span class="number">1</span></span><br><span class="line">            pos_feats = feats[i]</span><br><span class="line">            target = cur_inst[i][<span class="number">1</span>]</span><br><span class="line">            mine = tags[i]</span><br><span class="line">            <span class="keyword">for</span> feat <span class="keyword">in</span> pos_feats:</span><br><span class="line">                updates[feat + <span class="string">"=&gt;"</span> + target] = updates.get(feat + <span class="string">"=&gt;"</span> + target, <span class="number">0.0</span>) + <span class="number">1</span></span><br><span class="line">                updates[feat + <span class="string">"=&gt;"</span> + mine] = updates.get(feat + <span class="string">"=&gt;"</span> + mine, <span class="number">0.0</span>) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        self.__Update(updates)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> error_words_num</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Update</span><span class="params">(self, updates)</span>:</span></span><br><span class="line">        <span class="comment"># update the features weight.</span></span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> updates:</span><br><span class="line">            pair = self.__last_update.get(feat, [<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">            last_ite_ID = pair[<span class="number">0</span>]</span><br><span class="line">            last_inst_ID = pair[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            c = (self.__cur_ite_ID - last_ite_ID) * self.__insts_num + self.__cur_inst_ID - last_inst_ID</span><br><span class="line">            self.__feats_weight_sum[feat] = self.__feats_weight_sum.get(feat, <span class="number">0</span>) + c * self.__feats_weight.get(feat, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            self.__feats_weight[feat] = self.__feats_weight.get(feat, <span class="number">0</span>) + updates[feat]</span><br><span class="line">            self.__last_update[feat] = [self.__cur_ite_ID, self.__cur_inst_ID]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"></span><br><span class="line">    train = CPTTrain(train=<span class="literal">True</span>, segment=<span class="literal">False</span>)</span><br><span class="line">    train.Train(<span class="string">"data/msr_train.txt"</span>, max_train_num=<span class="number">1000000</span>, max_ite_num=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    srcs = [<span class="string">"夏天的清晨"</span>,</span><br><span class="line">            <span class="string">"“人们常说生活是一部教科书，而血与火的战争更是不可多得的教科书，她确实是名副其实的‘我的大学’。"</span>,</span><br><span class="line">            <span class="string">"夏天的清晨夏天看见猪八戒和嫦娥了。"</span>,</span><br><span class="line">            <span class="string">"海运业雄踞全球之首，按吨位计占世界总数的１７％。"</span>]</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"avg"</span>)</span><br><span class="line">    seg = CPTTrain(train=<span class="literal">False</span>, segment=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> src <span class="keyword">in</span> srcs:</span><br><span class="line">        print(seg.Segment(src))</span><br></pre></td></tr></table></figure>
<h4 id="ji-yu-shen-du-xue-xi-de-duan-dao-duan-de-fen-ci-fang-fa">基于深度学习的端到端的分词方法</h4>
<p>在中文分词上，基于神经网络的方法，往往使用「字向量 + BiLSTM + CRF」模型，利用神经网络来学习特征，将传统 CRF 中的人工特征工程量降到最低</p>
<p>BiLSTM、BiLSTM+CRF、BiLSTM+CNN+CRF、BERT、BERT+CRF、BERT+BiLSTM、BERT+BiLSTM+CRF，由于以上模型均可对序列标注任务进行建模求解，所以均可拿来做中文分词。以比较典型的「字向量 + BiLSTM （+CNN）+ CRF」模型为例：</p>
<p>BiLSTM融合两组学习方向相反（一个按句子顺序，一个按句子逆序）的LSTM层，能够在理论上实现当前词即包含历史信息、又包含未来信息，更有利于对当前词进行标注。虽然依赖于神经网络强大的非线性拟合能力，理论上我们已经能够学习出不错的模型。但是，BiLSTM只考虑了标签上的上下文信息。对于序列标注任务来说，当前位置的标签\(y_t\)与前一个位置\(y_{t-1}\)、后一个位置\(y_{t+1}\)都有潜在的关系。例如，“东南大学欢迎您”被标注为“东/S 南/M 大/M 学/E 欢/B 迎/E 您/S”，由分词的标注规则可知，B标签后只能接M和E，BiLSTM没有利用这种标签之间的上下文信息。因此，就有人提出了在模型后接一层CRF层，用于在整个序列上学习最优的标签序列：</p>
<p><img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/bilstm_crf.jpeg" alt="avatar"></p>
<p>BiLSTM+CNN+CRF：对于分词任务，当前词的标签基本上只与前几个和和几个词有关联。BiLSTM在学习较长句子时，可能因为模型容量问题丢弃一些重要信息，因此在模型中加了一个CNN层，用于提取当前词的局部特征。（不知效果怎样？？！！）</p>
<p><img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/bilstm_cnn_crf.png" alt="avatar"></p>
<h5 id="mo-xing-bi-jiao">模型比较</h5>
<p>Trained and tested with pku dataset</p>
<p>CRF</p>
<table>
<thead>
<tr>
<th style="text-align:left">Template</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">crf_template</td>
<td style="text-align:center">0.938</td>
<td style="text-align:center">0.923</td>
<td style="text-align:center">0.931</td>
</tr>
</tbody>
</table>
<p>Bi-LSTM</p>
<table>
<thead>
<tr>
<th style="text-align:left">Structure</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">emb256_hid256_l3</td>
<td style="text-align:center">0.9252</td>
<td style="text-align:center">0.9237</td>
<td style="text-align:center">0.9243</td>
</tr>
</tbody>
</table>
<p>Bi-LSTM + CRF</p>
<table>
<thead>
<tr>
<th style="text-align:left">Structure</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">emb256_hid256_l3</td>
<td style="text-align:center">0.9343</td>
<td style="text-align:center">0.9336</td>
<td style="text-align:center">0.9339</td>
</tr>
</tbody>
</table>
<p>BERT + Bi-LSTM</p>
<table>
<thead>
<tr>
<th style="text-align:left">Structure</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">emb768_hid512_l2</td>
<td style="text-align:center">0.9698</td>
<td style="text-align:center">0.9650</td>
<td style="text-align:center">0.9646</td>
</tr>
</tbody>
</table>
<h5 id="dai-ma">代码</h5>
<p>以BERT+RNN+CRF为例，可根据需求进行改动。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> TorchCRF <span class="keyword">import</span> CRF </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertRNNCRF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_tags, rnn_type, bert_path, bert_train, seg_vocab_size,hidden_dim, n_layers, bidirectional, batch_first,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout, restrain)</span>:</span></span><br><span class="line">        super(BertRNNCRF, self).__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="comment"># 对bert进行训练</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.bert.named_parameters():</span><br><span class="line">            param.requires_grad = bert_train</span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                               hidden_size=hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        self.crf = CRF(num_tags, batch_first=<span class="literal">True</span>, restrain_matrix=restrain, loss_side=<span class="number">2.5</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            self.fc_tags = nn.Linear(hidden_dim*<span class="number">2</span>, num_tags)</span><br><span class="line">            <span class="comment"># self.fc_tags = nn.Linear(768, num_tags)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.fc_tags = nn.Linear(hidden_dim, num_tags)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, context, seq_len,max_seq_len, mask_bert)</span>:</span></span><br><span class="line">        <span class="comment"># context    输入的句子</span></span><br><span class="line">        <span class="comment"># mask_bert  对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        bert_sentence, bert_cls = self.bert(context, attention_mask=mask_bert)</span><br><span class="line">        <span class="comment"># sentence_len = bert_sentence.shape[1]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># bert_cls = bert_cls.unsqueeze(dim=1).repeat(1, sentence_len, 1)</span></span><br><span class="line">        <span class="comment"># bert_sentence = bert_sentence + bert_cls</span></span><br><span class="line">        encoder_out, sorted_seq_lengths, desorted_indices = self.prepare_pack_padded_sequence(bert_sentence, seq_len)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(encoder_out, sorted_seq_lengths,</span><br><span class="line">                                                            batch_first=self.batch_first)</span><br><span class="line">        <span class="comment"># self.rnn.flatten_parameters()</span></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        <span class="comment"># output = [ batch size,sent len, hidden_dim * bidirectional]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first,total_length=max_seq_len)</span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        out = self.fc_tags(self.dropout(output.contiguous()))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out[:, <span class="number">1</span>:, :]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_pack_padded_sequence</span><span class="params">(self, inputs_words, seq_lengths, descending=True)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param device:</span></span><br><span class="line"><span class="string">        :param inputs_words:</span></span><br><span class="line"><span class="string">        :param seq_lengths:</span></span><br><span class="line"><span class="string">        :param descending:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        sorted_seq_lengths, indices = torch.sort(seq_lengths, descending=descending)</span><br><span class="line">        _, desorted_indices = torch.sort(indices, descending=<span class="literal">False</span>)</span><br><span class="line">        sorted_inputs_words = inputs_words[indices]</span><br><span class="line">        <span class="keyword">return</span> sorted_inputs_words, sorted_seq_lengths, desorted_indices</span><br></pre></td></tr></table></figure>
<h3 id="fen-ci-zhong-de-nan-ti">分词中的难题</h3>
<p>中文是一种十分复杂的语言，让计算机理解中文语言更是困难。在中文分词过程中，有两大难题一直没有完全突破。</p>
<h4 id="1-qi-yi-qie-fen-wen-ti">1、歧义切分问题</h4>
<p>歧义是指同样的一句话，可能有两种或者更多的切分方法。</p>
<ul>
<li>交集歧义：“结合成”，结合/成，结/合成</li>
<li>组合歧义：“起身”，他站起/身/来。 他明天/起身/去北京。</li>
<li>混合型歧义：同时具备交集歧义和组合歧义的特点。1，这篇文章写得太平淡了。2，这墙抹的太平了。3，即使太平时期也不应该放松警惕。“太平淡”是交集型，“太平”是组合型。</li>
</ul>
<p>交集歧义相对组合歧义来说是还算比较容易处理，组合歧义需要根据整个句子来判断。（特征：上下文语义分析，韵律分析，语气，重音，停顿）</p>
<h4 id="2-wei-deng-lu-ci">2、未登录词</h4>
<p>未登录词（生词，新词）：1.已有的词表中没有收录的词。2.已有的训练语料中未曾出现的词（集外词OOV）。</p>
<p>类型：</p>
<ol>
<li>新出现的普通词汇：博客，超女，给力。</li>
<li>专有名词：人名、地名、组织名、时间、数字表达、</li>
<li>专业名词和研究领域名词：苏丹红，禽流感</li>
<li>其他专用名词：新出现的产品名，电影，书记。</li>
</ol>
<p>对于真实数据来说，未登录词对分词精度的影响远远超过了歧义切分。未登录词是很难处理的问题，而且这些又正好是人们经常使用的词，因此对于搜索引擎来说，分词系统中的未登录词识别十分重要。目前未登录词识别准确率已经成为评价一个分词系统好坏的重要标志之一。</p>
<h3 id="can-kao">参考</h3>
<ol>
<li>
<p><a href="https://blog.csdn.net/selinda001/article/details/79345072" target="_blank" rel="noopener">中文分词引擎 java 实现 — 正向最大、逆向最大、双向最大匹配法</a></p>
</li>
<li>
<p><a href="https://www.cnblogs.com/sxron/articles/6391926.html" target="_blank" rel="noopener">中文分词算法总结</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/Zh823275484/article/details/87878512" target="_blank" rel="noopener">基于n-gram模型的中文分词</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/qq_27825451/article/details/102457058" target="_blank" rel="noopener">详解语言模型NGram及困惑度Perplexity</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/wangliang_f/article/details/17532633" target="_blank" rel="noopener">分词学习(3)，基于ngram语言模型的n元分词</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/33261835" target="_blank" rel="noopener">中文分词算法简介</a></p>
</li>
<li>
<p><a href="https://www.jianshu.com/p/715fa597c6bc" target="_blank" rel="noopener">NLP：分词算法综述</a></p>
</li>
<li>
<p><a href="https://github.com/NLP-LOVE/Introduction-NLP/blob/master/chapter/5.%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%88%86%E7%B1%BB%E4%B8%8E%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8.md" target="_blank" rel="noopener">感知机分类与序列标注</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>自然语言处理基础</category>
      </categories>
      <tags>
        <tag>自然语言处理基础</tag>
        <tag>自然语言处理</tag>
        <tag>分词算法</tag>
      </tags>
  </entry>
  <entry>
    <title>文本纠错</title>
    <url>/2020/04/24/text_correction/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/04/24/text_correction/v2-cd4b3605ecff9b1018015673f2f03fb3_1200x500.jpg" alt></p>
<a id="more"></a>
<h1 id="jian-jie">简介</h1>
<h2 id="chang-jian-cuo-wu-lei-xing">常见错误类型</h2>
<ol>
<li>谐音字词，如 配副眼睛-配副眼镜</li>
<li>混淆音字词，如 流浪织女-牛郎织女</li>
<li>字词顺序颠倒，如 伍迪艾伦-艾伦伍迪</li>
<li>字词补全，如 爱有天意-假如爱有天意</li>
<li>形似字错误，如 高梁-高粱</li>
<li>中文拼音全拼，如 xingfu-幸福</li>
<li>中文拼音缩写，如 sz-深圳</li>
<li>语法错误，如 想象难以-难以想象</li>
</ol>
<p>针对不同业务场景，这些问题并不一定全部存在，比如输入法中需要处理前四种，搜索引擎需要处理所有类型，语音识别后文本纠错只需要处理前两种， 其中’形似字错误’主要针对五笔或者笔画手写输入等。</p>
<h2 id="jiu-cuo-fang-fa">纠错方法</h2>
<p>纠错算法分为两个方向：基于规则、深度模型</p>
<h3 id="strong-gui-ze-de-jie-jue-si-lu-strong"><strong>规则的解决思路</strong></h3>
<p>中文纠错分为两步走，第一步是错误检测，第二步是错误纠正；</p>
<p>错误检测部分先通过结巴中文分词器切词，由于句子中含有错别字，所以切词结果往往会有切分错误的情况，这样从字粒度和词粒度两方面检测错误， 整合这两种粒度的疑似错误结果，形成疑似错误位置候选集；</p>
<p>错误纠正部分，是遍历所有的疑似错误位置，并使用音似、形似词典替换错误位置的词，然后通过语言模型计算句子困惑度，对所有候选集结果比较并排序，得到最优纠正词。</p>
<h3 id="strong-shen-du-mo-xing-de-jie-jue-si-lu-strong"><strong>深度模型的解决思路</strong></h3>
<p>端到端的深度模型可以避免人工提取特征，减少人工工作量，RNN序列模型对文本任务拟合能力强，rnn_attention在英文文本纠错比赛中取得第一名成绩，证明应用效果不错；</p>
<p>CRF会计算全局最优输出节点的条件概率，对句子中特定错误类型的检测，会根据整句话判定该错误，阿里参赛2016中文语法纠错任务并取得第一名，证明应用效果不错；</p>
<p>seq2seq模型是使用encoder-decoder结构解决序列转换问题，目前在序列转换任务中（如机器翻译、对话生成、文本摘要、图像描述）使用最广泛、效果最好的模型之一。</p>
<h1 id="bian-ji-ju-chi">编辑距离</h1>
<h2 id="jian-jie-1">简介</h2>
<p>编辑距离的经典应用就是用于拼写检错，如果用户输入的词语不在词典中，自动从词典中找出编辑距离小于某个数n的单词，让用户选择正确的那一个，n通常取到2或者3。</p>
<p>字符串A到B的<strong>编辑距离</strong>是指，只用插入、删除和替换三种操作，最少需要多少步可以把A变成B。例如，从FAME到GATE需要两步（两次替换），从GAME到ACM则需要三步（删除G和E再添加C）。</p>
<h2 id="dai-ma">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minDistance_recursive</span><span class="params">(self, word1, word2)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        递归解法</span></span><br><span class="line"><span class="string">        :type word1: str</span></span><br><span class="line"><span class="string">        :type word2: str</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> word1 <span class="keyword">or</span> <span class="keyword">not</span> word2:</span><br><span class="line">            <span class="comment"># 递归出口：其中一个字符串为空，返回非空字符串的长度</span></span><br><span class="line">            <span class="keyword">return</span> (len(word1) <span class="keyword">or</span> len(word2))</span><br><span class="line">        <span class="keyword">if</span> word1[<span class="number">-1</span>] == word2[<span class="number">-1</span>]:</span><br><span class="line">            <span class="comment"># 如果两个字符串最后一个字符相同</span></span><br><span class="line">            <span class="keyword">return</span> self.minDistance(word1[:<span class="number">-1</span>], word2[:<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> + min(self.minDistance(word1[:<span class="number">-1</span>], word2), self.minDistance(word1, word2[:<span class="number">-1</span>]),</span><br><span class="line">                       self.minDistance(word1[:<span class="number">-1</span>], word2[:<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minDistance_dp</span><span class="params">(self,word1,word2)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        动态规划解法</span></span><br><span class="line"><span class="string">        :param word1:</span></span><br><span class="line"><span class="string">        :param word2:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> word1 <span class="keyword">or</span> <span class="keyword">not</span> word2:</span><br><span class="line">            <span class="keyword">return</span> (len(word1) <span class="keyword">or</span> len(word2))</span><br><span class="line"></span><br><span class="line">        dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(word2)+<span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(word1)+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dp 初始化</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word1)+<span class="number">1</span>):</span><br><span class="line">            dp[i][<span class="number">0</span>] = i</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word2)+<span class="number">1</span>):</span><br><span class="line">            dp[<span class="number">0</span>][i] = i</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(word1)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,len(word2)+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> word1[i<span class="number">-1</span>] == word2[j<span class="number">-1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="number">1</span> + min(dp[i<span class="number">-1</span>][j],dp[i][j<span class="number">-1</span>],dp[i<span class="number">-1</span>][j<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[len(word1)][len(word2)]</span><br></pre></td></tr></table></figure>
<p><a href="https://leetcode-cn.com/problems/edit-distance/solution/bian-ji-ju-chi-by-leetcode-solution/" target="_blank" rel="noopener">leetcode题解</a></p>
<h1 id="pin-xie-zi-dong-jiu-cuo-xi-tong">拼写自动纠错系统</h1>
<p>给定一待纠错词w,需要从一系列候选词中选出一最可能的词c。也就是：\(argmax(p(c|w))\), c in 候选词表。根据贝叶斯原理，\(p(c|w) = p(w|c) * p(c) / p(w)\). 又有对任意可能的c,p(w)一样，故也就是求使\(argmax(p(w|c) * p(c))\)成立的c.</p>
<h2 id="candidate-model">Candidate Model:</h2>
<p>利用编辑距离小于1或者编辑距离小于2，来构造初步候选集，然后利用语料库，过滤初步候选集，得到符合规范的候选词。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits1</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="string">"All edits that are one edit away from `word`."</span></span><br><span class="line">    letters    = <span class="string">'abcdefghijklmnopqrstuvwxyz'</span></span><br><span class="line">    splits     = [(word[:i], word[i:])    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word) + <span class="number">1</span>)]</span><br><span class="line">    deletes    = [L + R[<span class="number">1</span>:]               <span class="keyword">for</span> L, R <span class="keyword">in</span> splits <span class="keyword">if</span> R]</span><br><span class="line">    transposes = [L + R[<span class="number">1</span>] + R[<span class="number">0</span>] + R[<span class="number">2</span>:] <span class="keyword">for</span> L, R <span class="keyword">in</span> splits <span class="keyword">if</span> len(R)&gt;<span class="number">1</span>]</span><br><span class="line">    replaces   = [L + c + R[<span class="number">1</span>:]           <span class="keyword">for</span> L, R <span class="keyword">in</span> splits <span class="keyword">if</span> R <span class="keyword">for</span> c <span class="keyword">in</span> letters]</span><br><span class="line">    inserts    = [L + c + R               <span class="keyword">for</span> L, R <span class="keyword">in</span> splits <span class="keyword">for</span> c <span class="keyword">in</span> letters]</span><br><span class="line">    <span class="keyword">return</span> set(deletes + transposes + replaces + inserts)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits2</span><span class="params">(word)</span>:</span> </span><br><span class="line">  <span class="string">"编辑距离小于2的候选词"</span></span><br><span class="line">  <span class="keyword">return</span> (e2 <span class="keyword">for</span> e1 <span class="keyword">in</span> edits1(word) <span class="keyword">for</span> e2 <span class="keyword">in</span> edits1(e1))</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">known</span><span class="params">(words)</span>:</span> </span><br><span class="line">  <span class="string">"在语料中过滤出复合拼写规范的（正确）的词"</span></span><br><span class="line">  <span class="keyword">return</span> set(w <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> w <span class="keyword">in</span> corpus)</span><br></pre></td></tr></table></figure>
<h2 id="language-model">Language Model</h2>
<p>候选集合中的词再语料库中的出现概率。比如：the 在英文文本中出现的概率率为7%，则有：p(the)=0.07</p>
<p>,可以通过统计语料库中\(p(c)\) = 词c出现的频次/所有词的出现次数之和。（unigram，bigram）</p>
<h2 id="error-model">Error Model</h2>
<p>当想要输入the却意外拼写成teh的概率:p(teh|the)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">candidates</span><span class="params">(word)</span>:</span> </span><br><span class="line">    <span class="keyword">return</span> known([word]) <span class="keyword">or</span> known(edits1(word)) <span class="keyword">or</span> known(edits2(word)) <span class="keyword">or</span> [word]</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">correction</span><span class="params">(word)</span>:</span> </span><br><span class="line">    <span class="keyword">return</span> max(candidates(word), key=P)</span><br></pre></td></tr></table></figure>
<h2 id="selection-mechanism">Selection Mechanism</h2>
<p>从正确的拼写候选集中选择最有可能的正确结果，argmax</p>
<h2 id="zong-jie">总结</h2>
<ol>
<li>利用编辑距离构造正确结果候选集</li>
<li>利用language model 和error model 来计算概率。</li>
<li>从正确结果候选集中选择概率最大的正确拼写。</li>
</ol>
<h3 id="can-kao">参考</h3>
<ol>
<li>
<p><a href="https://cloud.tencent.com/developer/article/1435917" target="_blank" rel="noopener">中文文本纠错算法走到多远了？</a></p>
</li>
<li>
<p><a href="https://github.com/shibing624/pycorrector" target="_blank" rel="noopener">pycorrect</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>文本纠错</tag>
      </tags>
  </entry>
  <entry>
    <title>关键词抽取</title>
    <url>/2020/04/21/keyword_extraction/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/04/21/keyword_extraction/gjc01.png" alt></p>
<a id="more"></a>
<h1 id="tf-idf-ti-qu-guan-jian-ci">TF-IDF 提取关键词</h1>
<h2 id="jian-jie">简介</h2>
<p>TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率).TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一份文件在一个语料库中的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。换句话说就是：<strong>一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章。</strong></p>
<blockquote>
<p>TF-IDF 算法主要适用于英文，中文首先要分词，分词后要解决多词一义，以及一词多义问题，这两个问题通过简单的tf-idf方法不能很好的解决。于是就有了后来的词嵌入方法，用向量来表征一个词。</p>
</blockquote>
<h2 id="tf-idf">TF-IDF</h2>
<h3 id="tf">TF</h3>
<p>表示词条（关键字）在文本中出现的次数（frequency）(一般不用)。TF 背后的隐含的假设是，查询关键字中的单词应该相对于其他单词更加重要，而文档的重要程度，也就是相关度，与单词在文档中出现的次数成正比。比如，“Car” 这个单词在文档 A 里出现了 5 次，而在文档 B 里出现了 20 次，那么 TF 计算就认为文档 B 可能更相关。</p>
<h4 id="bian-chong-yi-tong-guo-dui-shu-han-shu-bi-mian-tf-xian-xing-zeng-chang">变种一:通过对数函数避免 TF 线性增长</h4>
<p>理由：虽然我们一般认为一个文档包含查询关键词多次相对来说表达了某种相关度，但这样的关系很难说是线性的。以 “Car Insurance” 为例，文档 A 可能包含 “Car” 这个词 100 次，而文档 B 可能包含 200 次，是不是说文档 B 的相关度就是文档 A 的 2 倍呢？其实，当这种频次超过了某个阈值之后，这个 TF 也就没那么有区分度了。</p>
<p><strong>用 Log，也就是对数函数，对 TF 进行变换，就是一个不让 TF 线性增长的技巧</strong>。具体来说，人们常常用 1+Log(TF) 这个值来代替原来的 TF 取值。在这样新的计算下，假设 “Car” 出现一次，新的值是 1，出现 100 次，新的值是 \(log 100=5.6\)，而出现 200 次，新的值是\(log 200 = 6.3\)。很明显，这样的计算保持了一个平衡，既有区分度，但也不至于完全线性增长。</p>
<h4 id="bian-chong-er-biao-zhun-hua-jie-jue-chang-wen-dang-duan-wen-dang-wen-ti">变种二：标准化解决长文档、短文档问题</h4>
<p>经典的计算并没有考虑 “长文档” 和“短文档”的区别。一个文档 A 有 3,000 个单词，一个文档 B 有 250 个单词，很明显，即便 “Car” 在这两个文档中都同样出现过 20 次，也不能说这两个文档都同等相关。<strong>对 TF 进行 “标准化”（Normalization），特别是根据文档的最大 TF 值进行的标准化，成了另外一个比较常用的技巧</strong>。<br>
\[
tf_{ij} = \frac{n_{i,j}}{\sum_k n_{k,j}}
\]<br>
其中\(n_{i,j}\)是词\(w_i\)在文档\(d_j\)中出现的次数，分母是文档\(d_j\)中所有词汇出现的次数总和。</p>
<h3 id="idf">IDF</h3>
<p>仅有 TF 不能比较完整地描述文档的相关度。因为语言的因素，有一些单词可能会比较自然地在很多文档中反复出现，比如英语中的 “The”、“An”、“But” 等等。这些词大多起到了链接语句的作用，是保持语言连贯不可或缺的部分。然而，如果我们要搜索 “How to Build A Car” 这个关键词，其中的 “How”、“To” 以及 “A” 都极可能在绝大多数的文档中出现，这个时候 TF 就无法帮助我们区分文档的相关度了。</p>
<p>逆文档频率的思路很简单，就是我们需要去 “惩罚”（Penalize）那些出现在太多文档中的单词。</p>
<h4 id="bian-chong-san-dui-shu-han-shu-chu-li-idf">变种三：对数函数处理 IDF</h4>
<p>某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。<br>
\[
idf_w = log{\frac{|D|}{|j:t_i \in d_j|}}
\]</p>
<p>其中，\(|D|\)表示语料库中文档总数，分母为包含词w的文档数+1，分母加1是为了避免出现分母为零的情况。</p>
<p>样做的好处就是，第一，使用了文档总数来做标准化，很类似上面提到的标准化的思路；第二，利用对数来达到非线性增长的目的。</p>
<h4 id="bian-chong-si-cha-xun-ci-ji-wen-dang-xiang-liang-biao-zhun-hua">变种四：查询词及文档向量标准化</h4>
<p>对查询关键字向量，以及文档向量进行标准化，使得这些向量能够不受向量里有效元素多少的影响，也就是不同的文档可能有不同的长度。在线性代数里，可以把向量都标准化为一个单位向量的长度。这个时候再进行点积运算，就相当于在原来的向量上进行余弦相似度的运算。所以，另外一个角度利用这个规则就是直接在多数时候进行余弦相似度运算，以代替点积运算。</p>
<h3 id="tf-idf-1">TF-IDF</h3>
<p>某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语(关键词)。<br>
\[
TF-IDF = TF * IDF
\]</p>
<h2 id="tf-idf-de-ying-yong">TF-IDF 的应用</h2>
<ol>
<li>搜索引擎</li>
<li>关键词提取</li>
<li>文本相似性</li>
<li>文本摘要</li>
</ol>
<h2 id="dai-ma">代码</h2>
<figure class="highlight python"><figcaption><span>tf-idf</span><a href="https://github.com/jeffery0628/keyword_extraction" target="_blank" rel="noopener">github</a></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf_idf</span><span class="params">(document, corpus)</span>:</span>  <span class="comment"># 计算TF-IDF,并返回字典</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param document: 计算document里面每个词的tfidf值，document为文本分词后的形式，</span></span><br><span class="line"><span class="string">    如:[6 月 19 日 2012 年度 中国 爱心 城市 公益活动 新闻 发布会 在京举行]</span></span><br><span class="line"><span class="string">    如果是对一篇文档进行关键词提取，则需要对文档进行分句，把每句话看成一个document，corpus则存放的是整篇文档分词后的所有句子（句子为分词后的结果）。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param corpus:  corpus为所有问当分词后的列表：[document1,document2,document3,...]</span></span><br><span class="line"><span class="string">    :return:dict类型，按照tfidf值从大到小排序： orderdict[word] = tfidf_value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    word_tfidf = &#123;&#125;</span><br><span class="line">    <span class="comment"># 计算词频</span></span><br><span class="line">    freq_words = Counter(document)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> freq_words:</span><br><span class="line">        <span class="comment"># 计算TF：某个词在文章中出现的次数/文章总词数</span></span><br><span class="line">        tf = freq_words[word] / len(document)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算IDF：log(语料库的文档总数/(包含该词的文档数+1))</span></span><br><span class="line">        idf = math.log(len(corpus) / (wordinfilecount(word, corpus) + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算每个词的TFIDF值</span></span><br><span class="line">        tfidf = tf * idf  <span class="comment"># 计算TF-IDF</span></span><br><span class="line">        word_tfidf[word] = tfidf</span><br><span class="line"></span><br><span class="line">    orderdic = sorted(word_tfidf.items(), key=<span class="keyword">lambda</span> item: item[<span class="number">1</span>], reverse=<span class="literal">True</span>)  <span class="comment"># 给字典排序</span></span><br><span class="line">    <span class="keyword">return</span> orderdic</span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    stop_words_path = <span class="string">r'stop_words.txt'</span>  <span class="comment"># 停用词表路径</span></span><br><span class="line">    stop_words = get_stopwords(stop_words_path)  <span class="comment"># 获取停用词表列表</span></span><br><span class="line"></span><br><span class="line">    documents_dir = <span class="string">'data'</span></span><br><span class="line">    filelist = get_documents(documents_dir)  <span class="comment"># 获取文件列表</span></span><br><span class="line"></span><br><span class="line">    corpus = get_corpus(filelist, stop_words)  <span class="comment"># 建立语料库</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx,document <span class="keyword">in</span> enumerate(corpus):</span><br><span class="line">        word_tfidf = tf_idf(document, corpus)  <span class="comment"># 计算TF-IDF</span></span><br><span class="line">        <span class="comment"># 输出前十关键词</span></span><br><span class="line">        print(<span class="string">'document &#123;&#125;,top 10 key words&#123;&#125;'</span>.format(idx+<span class="number">1</span>,word_tfidf[:<span class="number">10</span>]))</span><br></pre></td></tr></table></figure>
<p>使用scikit-learn 计算tfidf</p>
<figure class="highlight python"><figcaption><span>tf-idf</span><a href="https://github.com/jeffery0628/keyword_extraction" target="_blank" rel="noopener">github</a></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer, TfidfVectorizer</span><br><span class="line"><span class="comment"># 对语料进行稍微处理</span></span><br><span class="line">corpus = [*map(<span class="keyword">lambda</span> x:<span class="string">" "</span>.join(x), corpus)]</span><br><span class="line">tfidf_model = TfidfVectorizer()</span><br><span class="line">tfidf_matrix = tfidf_model.fit_transform(corpus)  <span class="comment"># 计算每个词的tfidf值</span></span><br><span class="line">words = tfidf_model.get_feature_names()<span class="comment"># 所有词的集合</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(corpus)):</span><br><span class="line">  	word_tfidf = &#123;&#125;</span><br><span class="line">  	<span class="keyword">for</span> j <span class="keyword">in</span> range(len(words)):</span><br><span class="line">        word_tfidf[words[j]] = tfidf_matrix[i, j]</span><br><span class="line">        word_tfidf = sorted(word_tfidf.items(),key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>],reverse=<span class="literal">True</span>)</span><br><span class="line">        print(<span class="string">'document &#123;&#125;,top 10 key words&#123;&#125;'</span>.format(i+<span class="number">1</span>, word_tfidf[:<span class="number">10</span>]))</span><br></pre></td></tr></table></figure>
<h1 id="text-rank-ti-qu-guan-jian-ci">TextRank 提取关键词</h1>
<h2 id="jian-jie-1">简介</h2>
<p>TF-IDF 算法对有多段文本的关键词提取非常有效，但是对于单篇或者文档分割较少的文本表现得不是特别好。如果需要提取关键词的语句只有一句话，那么基于TF-IDF可以知道，所有关键词的重要度都为0(因为IDF值为0)，这种情况下使用TextRank是比较好的选择。</p>
<p>TextRank是一种基于图排序的算法，基本思想是(来源于PageRank)：通过把文本分割成若干组成单元(单词、句子)并建立图模型，利用投票机制对文本中的重要成分进行排序，<strong>仅利用单篇文档本身的信息就可以实现关键词提取、文本摘要</strong>。和 LDA、HMM 等模型不同, TextRank不需要事先对多篇文档进行学习训练, 因其简洁有效而得到广泛应用。</p>
<h2 id="page-rank">PageRank</h2>
<h3 id="page-rank-de-jian-hua-mo-xing">PageRank 的简化模型</h3>
<p>假设一共有 4 个网页 A、B、C、D。它们之间的链接信息如图所示：</p>
<p><img src="/2020/04/21/keyword_extraction/pagerank.png" alt="avatar"></p>
<p>出链指的是链接出去的链接。入链指的是链接进来的链接。比如图中 A 有 2 个入链，3 个出链。简单来说，一个网页的影响力 = 所有入链集合的页面的加权影响力之和，用公式表示为：</p>
<p>\[
P R(u)=\sum_{v \in B_{u}} \frac{P R(v)}{L(v)}
\]<br>
u 为待评估的页面， \(B_{u}\) 为页面 \(u\) 的入链集合。针对入链集合中的任意页面 \(v\)，它能给 \(u\) 带来的影响力是其自身的影响力 \(PR(v)\) 除以 \(v\) 页面的出链数量，即页面 \(v\) 把影响力 \(PR(v)\) 平均分配给了它的出链，这样统计所有能给 \(u\) 带来链接的页面 \(v\)，得到的总和就是网页 \(u\) 的影响力，即为 \(PR(u)\)。所以出链会给被链接的页面赋予影响力，当我们统计了一个网页链出去的数量，也就是统计了这个网页的跳转概率。</p>
<p>在例子中，A 有三个出链分别链接到了 B、C、D 上。那么当用户访问 A 的时候，就有跳转到 B、C 或者 D 的可能性，跳转概率均为 1/3。B 有两个出链，链接到了 A 和 D 上，跳转概率为 1/2。这样，我们可以得到 A、B、C、D 这四个网页的转移矩阵 M：</p>
<p>\[
M=\left[\begin{array}{cccc}
0 &amp; 1 / 2 &amp; 1 &amp; 0 \\
1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\
1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\
1 / 3 &amp; 1 / 2 &amp; 0 &amp; 0
\end{array}\right]
\]<br>
假设 A、B、C、D 四个页面的初始影响力都是相同的，即：<br>
\[
w_{0}=\left[\begin{array}{l}
1 / 4 \\
1 / 4 \\
1 / 4 \\
1 / 4
\end{array}\right]
\]<br>
当进行第一次转移之后，各页面的影响力 \(w_{1}\) 变为：<br>
\[
w_1=Mw_0=\left[\begin{array}{cccc}
0 &amp; 1 / 2 &amp; 1 &amp; 0 \\
1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\
1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\
1 / 3 &amp; 1 / 2 &amp; 0 &amp; 0
\end{array}\right]\left[\begin{array}{c}
1 / 4 \\
1 / 4 \\
1 / 4 \\
1 / 4
\end{array}\right]=\left[\begin{array}{c}
9 / 24 \\
5 / 24 \\
5 / 24 \\
5 / 24
\end{array}\right]
\]<br>
然后再用转移矩阵乘以 \(w_{1}\) 得到 \(w_{2}\) 结果，直到第 \(n\) 次迭代后 \(w_{n}\) 影响力不再发生变化，可以收敛到 \((0.3333,0.2222,0.2222,0.2222\)，也就是对应着 A、B、C、D 四个页面最终平衡状态下的影响力。能看出 A 页面相比于其他页面来说权重更大，也就是 PR 值更高。而 B、C、D 页面的 PR 值相等。</p>
<h3 id="deng-ji-xie-lu-rank-leak">等级泄露（Rank Leak）</h3>
<p>如果一个网页没有出链，就像是一个黑洞一样，吸收了其他网页的影响力而不释放，最终会导致其他网页的 PR 值为 0。</p>
<p><img src="/2020/04/21/keyword_extraction/rank_leak.png" alt="avatar"></p>
<h3 id="deng-ji-chen-mei-rank-sink">等级沉没（Rank Sink）</h3>
<p>如果一个网页只有出链，没有入链，计算的过程迭代下来，会导致<strong>这个网页</strong>的 PR 值为 0（也就是不存在公式中的 V）。</p>
<p><img src="/2020/04/21/keyword_extraction/rank_sink.png" alt="avatar"></p>
<h3 id="jie-jue-fang-an-sui-ji-liu-lan">解决方案：随机浏览</h3>
<p>为了解决简化模型中存在的等级泄露和等级沉没的问题，拉里·佩奇提出了 PageRank 的随机浏览模型。他假设了这样一个场景：用户并不都是按照跳转链接的方式来上网，还有一种可能是不论当前处于哪个页面，都有概率访问到其他任意的页面，比如说用户就是要直接输入网址访问其他页面，虽然这个概率比较小。</p>
<p>所以他定义了阻尼因子 d，这个因子代表了用户按照跳转链接来上网的概率，通常可以取一个固定值 0.85，而 1-d=0.15 则代表了用户不是通过跳转链接的方式来访问网页的，比如直接输入网址。<br>
\[
P R(u)=\frac{1-d}{N}+d \sum_{v \in B_{u}} \frac{P R(v)}{L(v)}
\]</p>
<p>其中 \(N\) 为网页总数，这样我们又可以重新迭代网页的权重计算了，加入了阻尼因子 \(d\)，一定程度上解决了等级泄露和等级沉没的问题。通过数学定理也可以证明，最终 PageRank 随机浏览模型是可以收敛的，也就是可以得到一个稳定正常的 PR 值。</p>
<h2 id="text-rank">TextRank</h2>
<p>TextRank通过词之间的相邻关系构建网络，然后用PageRank迭代计算每个节点的rank值，排序rank值即可得到关键词。PageRank迭代计算公式如下：</p>
<p>\[
P R\left(V_{i}\right)=(1-d)+d * \sum_{j \in \operatorname{In}\left(V_{i}\right)} \frac{1}{\left|O u t\left(V_{j}\right)\right|} P R\left(V_{j}\right)
\]<br>
其中，d:表示阻尼系数，一般为0.85。\(V_i\)：表示图中任一节点。\(In(V_i)\):表示指向顶点\(V_i\)的所有顶点的集合。\(|Out(V_j)|\)：表示由顶点\(V_j\)连接出去的所有顶点集合个数。\(PR(V_i)\)：表示顶点\(V_i\)的最终排序权重。(与pagerank公式基本一致。)</p>
<p>网页之间的链接关系可以用图表示，那么怎么把一个句子（可以看作词的序列）构建成图呢？TextRank将某一个词与其前面的N个词、以及后面的N个词均具有图相邻关系（类似于N-gram语法模型）。具体实现：设置一个长度为N的滑动窗口，所有在这个窗口之内的词都视作词结点的相邻结点；则TextRank构建的词图为无向图。下图给出了由一个文档构建的词图（去掉了停用词并按词性做了筛选）：</p>
<p><img src="/2020/04/21/keyword_extraction/textrank.png" alt="avatar"></p>
<p>考虑到不同词对可能有不同的共现（co-occurrence），TextRank将共现作为无向图边的权值。那么，TextRank的迭代计算公式如下：<br>
\[
W S\left(V_{i}\right)=(1-d)+d * \sum_{j \in \operatorname{In}\left(V_{i}\right)} \frac{w_{ij}}{\left|O u t\left(V_{j}\right)\right|} W S\left(V_{j}\right)
\]<br>
该公式仅仅比PageRank多了一个权重项\(w_{ij}\)，用来表示两个节点之间的边连接有不同的重要程度。</p>
<h3 id="text-rank-guan-jian-ci-duan-yu-ti-qu-suan-fa">TextRank 关键词(短语)提取算法</h3>
<ol>
<li>把给定的文本\(T\)按照完整句子进行分割，即\(T = [S_1,S_2,\ldots,S_n]\).</li>
<li>文本\(T\)中每个句子\(S_i\)，进行分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词，如名词、动词、形容词，即\(S_i = [t_{i,1},t_{i,2},\ldots,t_{i,\pi}]\),其中\(t_{ij}\)是保留后的候选关键词。</li>
<li>构建候选关键词图\(G = (V,E)\)，其中\(V\)为节点集，由步骤2生成的候选关键词组成，然后采用共现关系（co-occurrence）构造任两点之间的边，两个节点之间存在边仅当它们对应的词汇在长度为K的窗口中共现，K表示窗口大小，即最多共现K个单词。</li>
<li>根据上面公式，迭代传播各节点的权重，直至收敛。</li>
<li>对节点权重进行倒序排序，从而得到最重要的T个单词，作为候选关键词。</li>
<li>由步骤5得到最重要的T个单词，在原始文本中进行标记，若形成相邻词组，则组合成多词关键词。</li>
</ol>
<h3 id="dai-ma-1">代码</h3>
<figure class="highlight python"><figcaption><span>textrank</span><a href="https://github.com/jeffery0628/keyword_extraction" target="_blank" rel="noopener">github</a></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">textrank_graph</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.graph = defaultdict(list) <span class="comment"># key:[(),(),(),...] 如：是 [('是', '全国', 1), ('是', '调查', 1), ('是', '失业率', 1), ('是', '城镇', 1)]</span></span><br><span class="line">        self.d = <span class="number">0.85</span>  <span class="comment"># d是阻尼系数，一般设置为0.85</span></span><br><span class="line">        self.min_diff = <span class="number">1e-5</span>  <span class="comment"># 设定收敛阈值</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加节点之间的边</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addEdge</span><span class="params">(self, start, end, weight)</span>:</span></span><br><span class="line">        self.graph[start].append((start, end, weight))</span><br><span class="line">        self.graph[end].append((end, start, weight))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 节点排序</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rank</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 一共有14个节点</span></span><br><span class="line">        print(len(self.graph))</span><br><span class="line">        <span class="comment"># 默认初始化权重</span></span><br><span class="line">        weight_deault = <span class="number">1.0</span> / (len(self.graph) <span class="keyword">or</span> <span class="number">1.0</span>)</span><br><span class="line">        <span class="comment"># nodeweight_dict, 存储节点的权重</span></span><br><span class="line">        nodeweight_dict = defaultdict(float)</span><br><span class="line">        <span class="comment"># outsum，存储节点的出度权重</span></span><br><span class="line">        outsum_node_dict = defaultdict(float)</span><br><span class="line">        <span class="comment"># 根据图中的边，更新节点权重</span></span><br><span class="line">        <span class="keyword">for</span> node, out_edge <span class="keyword">in</span> self.graph.items():</span><br><span class="line">            <span class="comment"># 是 [('是', '全国', 1), ('是', '调查', 1), ('是', '失业率', 1), ('是', '城镇', 1)]</span></span><br><span class="line">            nodeweight_dict[node] = weight_deault <span class="comment"># 初始化节点权重</span></span><br><span class="line">            outsum_node_dict[node] = sum((edge[<span class="number">2</span>] <span class="keyword">for</span> edge <span class="keyword">in</span> out_edge), <span class="number">0.0</span>) <span class="comment"># 统计node节点的出度</span></span><br><span class="line">        <span class="comment"># 初始状态下的textrank重要性权重</span></span><br><span class="line">        sorted_keys = sorted(self.graph.keys())</span><br><span class="line">        <span class="comment"># 设定迭代次数，</span></span><br><span class="line">        step_dict = [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">1000</span>):</span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> sorted_keys:</span><br><span class="line">                s = <span class="number">0</span></span><br><span class="line">                <span class="comment"># 计算公式：(edge_weight/outsum_node_dict[edge_node])*node_weight[edge_node]</span></span><br><span class="line">                <span class="keyword">for</span> e <span class="keyword">in</span> self.graph[node]:</span><br><span class="line">                    s += e[<span class="number">2</span>] / outsum_node_dict[e[<span class="number">1</span>]] * nodeweight_dict[e[<span class="number">1</span>]]</span><br><span class="line">                <span class="comment"># 计算公式：(1-d) + d*s</span></span><br><span class="line">                nodeweight_dict[node] = (<span class="number">1</span> - self.d) + self.d * s</span><br><span class="line">            step_dict.append(sum(nodeweight_dict.values()))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> abs(step_dict[step] - step_dict[step - <span class="number">1</span>]) &lt;= self.min_diff:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 利用Z-score进行权重归一化，也称为离差标准化，是对原始数据的线性变换，使结果值映射到[0 - 1]之间。</span></span><br><span class="line">        <span class="comment"># 先设定最大值与最小值均为系统存储的最大值和最小值</span></span><br><span class="line">        (min_rank, max_rank) = (sys.float_info[<span class="number">0</span>], sys.float_info[<span class="number">3</span>])</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> nodeweight_dict.values():</span><br><span class="line">            <span class="keyword">if</span> w &lt; min_rank:</span><br><span class="line">                min_rank = w</span><br><span class="line">            <span class="keyword">if</span> w &gt; max_rank:</span><br><span class="line">                max_rank = w</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> n, w <span class="keyword">in</span> nodeweight_dict.items(): <span class="comment"># 归一化</span></span><br><span class="line">            nodeweight_dict[n] = (w - min_rank / <span class="number">10.0</span>) / (max_rank - min_rank / <span class="number">10.0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nodeweight_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextRank</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.candi_pos = [<span class="string">'n'</span>, <span class="string">'v'</span>, <span class="string">'a'</span>] <span class="comment"># 关键词的词性：名词，动词，形容词</span></span><br><span class="line">        self.span = <span class="number">5</span> <span class="comment"># 窗口大小</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract_keywords</span><span class="params">(self, text, num_keywords)</span>:</span></span><br><span class="line">        g = textrank_graph()</span><br><span class="line">        cm = defaultdict(int)</span><br><span class="line">        word_list = [[word.word, word.flag] <span class="keyword">for</span> word <span class="keyword">in</span> pseg.cut(text)] <span class="comment"># 使用jieba分词并且对词性进行标注</span></span><br><span class="line">        <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(word_list): <span class="comment"># 该循环用于统计在窗口范围内，词的共现次数</span></span><br><span class="line">            <span class="keyword">if</span> word[<span class="number">1</span>][<span class="number">0</span>] <span class="keyword">in</span> self.candi_pos <span class="keyword">and</span> len(word[<span class="number">0</span>]) &gt; <span class="number">1</span>: <span class="comment">#</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, i + self.span):</span><br><span class="line">                    <span class="keyword">if</span> j &gt;= len(word_list):<span class="comment"># 防止下标越界</span></span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    <span class="keyword">if</span> word_list[j][<span class="number">1</span>][<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> self.candi_pos <span class="keyword">or</span> len(word_list[j][<span class="number">0</span>]) &lt; <span class="number">2</span>: <span class="comment"># 排除词性不在关键词词性列表中的词或者词长度小于2的词</span></span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    pair = tuple((word[<span class="number">0</span>], word_list[j][<span class="number">0</span>]))</span><br><span class="line">                    cm[(pair)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> terms, w <span class="keyword">in</span> cm.items():</span><br><span class="line">            g.addEdge(terms[<span class="number">0</span>], terms[<span class="number">1</span>], w)</span><br><span class="line">        nodes_rank = g.rank()</span><br><span class="line">        nodes_rank = sorted(nodes_rank.items(), key=<span class="keyword">lambda</span> asd:asd[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nodes_rank[:num_keywords]</span><br></pre></td></tr></table></figure>
<h1 id="lda-latent-dirichlet-allocation-ti-qu-guan-jian-ci">LDA (Latent Dirichlet Allocation) 提取关键词</h1>
<h2 id="jian-jie-2">简介</h2>
<p>TF-IDF 和 TextRank 两种算法更多反映的是文本的统计信息，对于文本之间的语义关系考虑得比较少。LDA是一种能够体文本语义关系的关键词提取方法。</p>
<blockquote>
<p>二项分布（Binomial Distribution），即重复n次的伯努利试验（Bernoulli Experiment），用\(\xi\)表示随机试验的结果。如果事件发生的概率是P,则不发生的概率\(q=1-p\)，\(N\)次独立重复试验中发生K次的概率是\(P(\xi=K)= C(n,k) * p^k * (1-p)^{n-k}\)，其中\(C(n, k) =\frac{n!}{(k!(n-k)!)}\). 期望：\(E(ξ)=np\),方差：\(D(ξ)=npq\)其中\(q=1-p\)</p>
<p>多项分布（Multinomial Distribution）：多项式分布是二项式分布的推广。二项分布的典型例子是扔硬币，硬币正面朝上概率为p, 重复扔n次硬币，k次为正面的概率即为一个二项分布概率。把二项分布公式推广至多种状态，就得到了多项分布。<br>
某随机实验如果有k个可能结局\(A_1,A_2,\ldots,A_k\)，分别将他们的出现次数记为随机变量\(X_1,X_2,\ldots,X_k\)，它们的概率分布分别是\(p_1,p_2,\ldots,p_k\)那么在\(n\)次采样的总结果中，\(A_1\)出现\(n_1\)次、\(A_2\)出现\(n_2\)次,\(\dots\),\(A_k\)出现\(n_k\)次的这种事件的出现概率\(P\)有下面公式：<br>
\[
P\left(X_{1}=n_{1}, \cdots, X_{k}=n_{k}\right)=\left\{\begin{array}{ll}
\frac{n !}{n_{1} ! \cdots n_{k} !} p_{1}^{n_{1}} \cdots p_{k}^{n_{k}} &amp; , \sum_{i=1}^{k} n_{i}=n \\
0 &amp; , \text { otherwise }
\end{array}\right.
\]<br>
Beta分布与Dirichlet分布的定义域均为[0,1]，在实际使用中，通常将两者作为概率的分布，Beta分布描述的是单变量分布，Dirichlet分布描述的是多变量分布，因此，Beta分布可作为二项分布的先验概率，Dirichlet分布可作为多项分布的先验概率。</p>
</blockquote>
<p>在主题模型中，主题表示一个概念，表现为一系列相关的单词，是这些单词的条件概率。形象来说，主题就是一个桶，里面装了出现概率较高的单词，这些单词与这个主题有很强的相关性。</p>
<h2 id="li-lun">理论</h2>
<p>怎样才能生成主题？对文章的主题应该怎么分析？这是主题模型要解决的问题。</p>
<p>首先，可以用生成模型来看文档和主题这两件事。所谓生成模型，就是说，我们认为**一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”**这样一个过程得到的。那么，如果我们要生成一篇文档，它里面的每个词语出现的概率为：<br>
\[
p(W|D) = \sum_T P(W|T)P(T|D)
\]<br>
其中，\(W\)表示词，\(T\)表示主题，\(D\)表示文档。</p>
<p><img src="/2020/04/21/keyword_extraction/lda.png" alt="avatar"></p>
<p>其中”文档-词语”矩阵表示每个单词在每个文档中的词频，即出现的概率；”主题-词语”矩阵表示每个主题中每个单词的出现概率；”文档-主题”矩阵表示每个主题在每个文档中出现的概率。</p>
<p>单词对于主题的概率和主题对于文档的概率，可以通过Gibbs采样法（？？？）来进行概率的计算。</p>
<p>主题\(T_k\)下各个词\(W_i\)的权重计算公式：<br>
\[
P(W_i|T_k) = \frac{C_{ik}+\beta}{\sum_{i=1}^N{C_{ik}+N*\beta}} = \phi_i^{t=k} 
\]<br>
其中，\(w_i\)：表示单词集合中的任一单词。\(T_k\):表示主题集合中任一主题。\(P(w_i|T_k)\):表示在主题为\(k\)时，单词\(i\)出现的概率，其简记为\(\phi_i^{t=k}\)，\(C_{ik}\):表示语料库中单词\(i\)被赋予主题\(k\)的次数。\(N\):表示词汇表的大小。\(\beta\)：表示超参数。</p>
<p>文档\(D_m\)下各个词\(T_k\)的权重计算公式：<br>
\[
P(T_k|D_m) = \frac{C_{km}+\alpha}{\sum^K_{k=1}C_{km}+K*\alpha}=\theta^m_{t=k}
\]<br>
其中，\(D_m\):表示文档集合中任一文档。\(T_k\):表示主题集合中任一主题。\(P(T_k|D_m)\):表示语料库中文档m中单词被赋予主题\(k\)的次数。\(K\)：表示主题的数量。\(\alpha\)表示超参数。</p>
<p>得到了指定文档下某主题出现的概率，以及指定主题下、某单词出现的概率。那么由联合概率分布可以知道，对于指定文档某单词出现的概率：<br>
\[
P(W_i|D_m) = \sum_{k=1}^K{\phi_i^{t=k}*\theta_{t=k}^m}
\]<br>
基于上述公式，可以计算出单词\(i\)对于文档\(m\)的主题重要性。</p>
<p>但是由于在LDA主题概率模型中，所有的词汇都会以一定的概率出现在每个主题，所以这样会导致最终计算的单词对于文档的主题重要性区分度受影响。为了避免这种情况，一般会将单词相对于主题概率小于一定阈值的概率置为0.</p>
<h2 id="dai-ma-2">代码</h2>
<figure class="highlight python"><figcaption><span>LDA</span><a href="https://github.com/jeffery0628/keyword_extraction" target="_blank" rel="noopener">github</a></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 主题模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TopicModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 三个传入参数：处理后的数据集，关键词数量，具体模型（LSI、LDA），主题数量</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, doc_list, keyword_num, model=<span class="string">'LDA'</span>, num_topics=<span class="number">4</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 使用gensim的接口，将文本转为向量化表示</span></span><br><span class="line">        <span class="comment"># 先构建词空间</span></span><br><span class="line">        self.dictionary = corpora.Dictionary(doc_list)</span><br><span class="line">        <span class="comment"># 使用BOW模型向量化 (token_id,freq)</span></span><br><span class="line">        corpus = [self.dictionary.doc2bow(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> doc_list]  <span class="comment"># (token_id,freq)</span></span><br><span class="line">        <span class="comment"># 对每个词，根据tf-idf进行加权，得到加权后的向量表示</span></span><br><span class="line">        self.tfidf_model = models.TfidfModel(corpus)</span><br><span class="line">        self.tfidf_corpus = self.tfidf_model[corpus]</span><br><span class="line"></span><br><span class="line">        self.keyword_num = keyword_num</span><br><span class="line">        self.num_topics = num_topics</span><br><span class="line">        <span class="comment"># 选择加载胡模型</span></span><br><span class="line">        <span class="keyword">if</span> model == <span class="string">'LSI'</span>:</span><br><span class="line">            self.model = self.train_lsi()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.model = self.train_lda()</span><br><span class="line">        <span class="comment"># 得到数据集的主题-词分布</span></span><br><span class="line">        word_dic = self.word_dictionary(doc_list)</span><br><span class="line">        self.wordtopic_dic = self.get_wordtopic(word_dic)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 向量化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">doc2bowvec</span><span class="params">(self, word_list)</span>:</span></span><br><span class="line">        vec_list = [<span class="number">1</span> <span class="keyword">if</span> word <span class="keyword">in</span> word_list <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> word <span class="keyword">in</span> self.dictionary]</span><br><span class="line">        print(<span class="string">"vec_list"</span>, vec_list)</span><br><span class="line">        <span class="keyword">return</span> vec_list</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 词空间构建方法和向量化方法，在没有gensim接口时的一般处理方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word_dictionary</span><span class="params">(self, doc_list)</span>:</span></span><br><span class="line">        dictionary = []</span><br><span class="line">        <span class="comment"># 2及变1及结构</span></span><br><span class="line">        <span class="keyword">for</span> doc <span class="keyword">in</span> doc_list:</span><br><span class="line">            <span class="comment"># extend he append 方法有何异同 容易出错</span></span><br><span class="line">            dictionary.extend(doc)</span><br><span class="line"></span><br><span class="line">        dictionary = list(set(dictionary))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dictionary</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到数据集的主题 - 词分布</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_wordtopic</span><span class="params">(self, word_dic)</span>:</span></span><br><span class="line">        wordtopic_dic = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> word_dic:</span><br><span class="line">            singlist = [word]</span><br><span class="line">            <span class="comment"># 计算每个词胡加权向量</span></span><br><span class="line">            word_corpus = self.tfidf_model[self.dictionary.doc2bow(singlist)]</span><br><span class="line">            <span class="comment"># 计算每个词de主题向量</span></span><br><span class="line">            word_topic = self.model[word_corpus]</span><br><span class="line">            wordtopic_dic[word] = word_topic</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> wordtopic_dic</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_lsi</span><span class="params">(self)</span>:</span></span><br><span class="line">        lsi = models.LsiModel(self.tfidf_corpus, id2word=self.dictionary, num_topics=self.num_topics)</span><br><span class="line">        <span class="keyword">return</span> lsi</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_lda</span><span class="params">(self)</span>:</span></span><br><span class="line">        lda = models.LdaModel(self.tfidf_corpus, id2word=self.dictionary, num_topics=self.num_topics)</span><br><span class="line">        <span class="keyword">return</span> lda</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算词的分布和文档的分布的相似度，取相似度最高的keyword_num个词作为关键词</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_simword</span><span class="params">(self, word_list)</span>:</span></span><br><span class="line">        <span class="comment"># 文档的加权向量</span></span><br><span class="line">        sentcorpus = self.tfidf_model[self.dictionary.doc2bow(word_list)]</span><br><span class="line">        <span class="comment"># 文档主题 向量</span></span><br><span class="line">        senttopic = self.model[sentcorpus]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># senttopic [(0, 0.03457821), (1, 0.034260772), (2, 0.8970413), (3, 0.034119748)]</span></span><br><span class="line">        <span class="comment"># 余弦相似度计算</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">calsim</span><span class="params">(l1, l2)</span>:</span></span><br><span class="line">            a, b, c = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">            <span class="keyword">for</span> t1, t2 <span class="keyword">in</span> zip(l1, l2):</span><br><span class="line">                x1 = t1[<span class="number">1</span>]</span><br><span class="line">                x2 = t2[<span class="number">1</span>]</span><br><span class="line">                a += x1 * x1</span><br><span class="line">                b += x1 * x1</span><br><span class="line">                c += x2 * x2</span><br><span class="line">            sim = a / math.sqrt(b * c) <span class="keyword">if</span> <span class="keyword">not</span> (b * c) == <span class="number">0.0</span> <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line">            <span class="keyword">return</span> sim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算输入文本和每个词的主题分布相似度</span></span><br><span class="line">        sim_dic = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> self.wordtopic_dic.items():</span><br><span class="line">            <span class="comment"># 还是计算每个再本文档中的词  和文档的相识度</span></span><br><span class="line">            <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> word_list:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment">#</span></span><br><span class="line">            sim = calsim(v, senttopic)</span><br><span class="line">            sim_dic[k] = sim</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> sorted(sim_dic.items(), key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:self.keyword_num]:</span><br><span class="line">            print(k, v)</span><br><span class="line">        print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topic_extract</span><span class="params">(word_list, model, pos=False, keyword_num=<span class="number">10</span>)</span>:</span></span><br><span class="line">    doc_list = load_data(pos)</span><br><span class="line">    topic_model = TopicModel(doc_list, keyword_num, model=model)</span><br><span class="line">    topic_model.get_simword(word_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textrank_extract</span><span class="params">(text, pos=False, keyword_num=<span class="number">10</span>)</span>:</span></span><br><span class="line">    textrank = analyse.textrank</span><br><span class="line">    keywords = textrank(text, keyword_num)</span><br><span class="line">    <span class="comment"># 输出抽取出的关键词</span></span><br><span class="line">    <span class="keyword">for</span> keyword <span class="keyword">in</span> keywords:</span><br><span class="line">        print(keyword + <span class="string">"/ "</span>, end=<span class="string">''</span>)</span><br><span class="line">    print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># test</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    text = <span class="string">'6月19日,《2012年度“中国爱心城市”公益活动新闻发布会》在京举行。'</span> + \</span><br><span class="line">           <span class="string">'中华社会救助基金会理事长许嘉璐到会讲话。基金会高级顾问朱发忠,全国老龄'</span> + \</span><br><span class="line">           <span class="string">'办副主任朱勇,民政部社会救助司助理巡视员周萍,中华社会救助基金会副理事长耿志远,'</span> + \</span><br><span class="line">           <span class="string">'重庆市民政局巡视员谭明政。晋江市人大常委会主任陈健倩,以及10余个省、市、自治区民政局'</span> + \</span><br><span class="line">           <span class="string">'领导及四十多家媒体参加了发布会。中华社会救助基金会秘书长时正新介绍本年度“中国爱心城'</span> + \</span><br><span class="line">           <span class="string">'市”公益活动将以“爱心城市宣传、孤老关爱救助项目及第二届中国爱心城市大会”为主要内容,重庆市'</span> + \</span><br><span class="line">           <span class="string">'、呼和浩特市、长沙市、太原市、蚌埠市、南昌市、汕头市、沧州市、晋江市及遵化市将会积极参加'</span> + \</span><br><span class="line">           <span class="string">'这一公益活动。中国雅虎副总编张银生和凤凰网城市频道总监赵耀分别以各自媒体优势介绍了活动'</span> + \</span><br><span class="line">           <span class="string">'的宣传方案。会上,中华社会救助基金会与“第二届中国爱心城市大会”承办方晋江市签约,许嘉璐理'</span> + \</span><br><span class="line">           <span class="string">'事长接受晋江市参与“百万孤老关爱行动”向国家重点扶贫地区捐赠的价值400万元的款物。晋江市人大'</span> + \</span><br><span class="line">           <span class="string">'常委会主任陈健倩介绍了大会的筹备情况。'</span></span><br><span class="line"></span><br><span class="line">    pos = <span class="literal">False</span></span><br><span class="line">    seg_list = seg_to_list(text, pos)</span><br><span class="line">    filter_list = word_filter(seg_list, pos)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'LDA模型结果：'</span>)</span><br><span class="line">    topic_extract(filter_list, <span class="string">'LDA'</span>, pos)</span><br></pre></td></tr></table></figure>
<h3 id="lda-bu-zou-zong-jie">LDA 步骤总结</h3>
<p>数据集处理</p>
<ol>
<li>先构建词空间  Dictionary(4064 unique tokens: [‘上将’, ‘专门’, ‘乘客’, ‘仪式’, ‘体验’]…)</li>
<li>使用BOW模型向量化   corpus [[(0, 1), (1, 1), (2, 2), (3, 1),。。</li>
<li>对每个词，根据tf-idf进行加权，得到加权后的向量表示</li>
</ol>
<p>根据数据集获得模型</p>
<ol start="4">
<li>得到数据集的主题-词分布  model (得到每个词的向量）（文档转列表 再转集合去重，再转列表）{‘白血病’: [(0, 0.1273009), (1, 0.6181468), (2, 0.12732704), (3, 0.12722531)], ‘婴儿’: [。。。</li>
<li>求文档的分布:词》向量》tf/idf加权》同第4步得到文档的分布向量 [(0, 0.033984687), (1, 0.033736005), (2, 0.8978361), (3, 0.03444325)]</li>
<li>计算余弦距离得到结果</li>
</ol>
<h2 id="zong-jie">总结</h2>
<ol>
<li>
<p>为什么要用浅层语义分析？</p>
<p>我觉着一方面是考虑文本的语义信息。还有就是用词袋模型所表示的单词-文本矩阵一方面存在数据稀疏的问题，另一方面就是词本身一词多义和多词一义现象在进行文本相似度计算的时候未必能够准确的表达两个文本的语义相似度。</p>
</li>
<li>
<p>潜在语义分析算法</p>
<ol>
<li>矩阵奇异值分解算法</li>
<li>非负矩阵分解算法</li>
</ol>
</li>
</ol>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://www.cnblogs.com/jpcflyer/p/11180263.html" target="_blank" rel="noopener">机器学习经典算法之PageRank</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/41091116" target="_blank" rel="noopener">通俗易懂理解——TF-IDF与TextRank</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>key word extraction</tag>
      </tags>
  </entry>
  <entry>
    <title>bert系列</title>
    <url>/2020/04/13/research/bert%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/Sesame_japan_cast_white_bg.jpg" alt></p>
<a id="more"></a>
<h2 id="bert-ji-qi-hou-xu-mo-xing">BERT 及其后续模型</h2>
<p>了解这些后续的模型，方便在后续的研究和应用中来更好的选择模型。</p>
<h3 id="bert">BERT</h3>
<p>BERT模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/bert.jpg" alt></p>
<p>对比OpenAI GPT(Generative pre-trained transformer)，BERT是双向的Transformer block连接；就像单向rnn和双向rnn的区别，直觉上来讲效果会好一些。</p>
<p>对比ELMo，虽然都是“双向”，但目标函数其实是不同的。ELMo是分别以 \(P(w_i|w_1,\cdots,w_{i-1})\) 和\(P(w_i|w_{i+1},\cdots,w_n)\)作为目标函数，独立训练处两个representation然后拼接，而BERT则是以\(P(w_i|w_1,\cdots,w_{i-1},w_{i+1}\cdots,w_n)\)  作为目标函数训练LM。</p>
<h4 id="task-1-mlm">Task 1: MLM</h4>
<p>由于BERT需要通过上下文信息，来预测中心词的信息，同时又不希望模型提前看见中心词的信息，因此提出了一种 Masked Language Model 的预训练方式，即随机从输入预料上 mask 掉一些单词，然后通过的上下文预测该单词，类似于一个完形填空任务。</p>
<p>在预训练任务中，15%的 Word Piece 会被mask，这15%的 Word Piece 中，80%的时候会直接替换为 [Mask] ，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token</p>
<ul>
<li>没有100%mask的原因
<ul>
<li>如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词</li>
</ul>
</li>
<li>加入10%随机token的原因
<ul>
<li>Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘</li>
<li>另外编码器不知道哪些词需要预测的，哪些词是错误的，因此被迫需要学习每一个token的表示向量</li>
</ul>
</li>
<li>另外，每个batchsize只有15%的单词被mask的原因，是因为性能开销的问题，双向编码器比单项编码器训练要更慢</li>
</ul>
<h4 id="task-2-nsp">Task 2: NSP</h4>
<p>仅仅一个MLM任务是不足以让 BERT 解决阅读理解等句子关系判断任务的，因此添加了额外的一个预训练任务，即 Next Sequence Prediction。</p>
<p>具体任务即为一个句子关系判断任务，即判断句子B是否是句子A的下文，如果是的话输出’IsNext‘，否则输出’NotNext‘。</p>
<p>训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在图4中的[CLS]符号中.</p>
<h4 id="shu-ru">输入</h4>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/bert_input.jpg" alt></p>
<ul>
<li>Token Embeddings：即传统的词向量层，每个输入样本的首字符需要设置为[CLS]，可以用于之后的分类任务，若有两个不同的句子，需要用[SEP]分隔，且最后一个字符需要用[SEP]表示终止</li>
<li>Segment Embeddings：为[0,1]序列，用来在NSP任务中区别两个句子，便于做句子关系判断任务</li>
<li>Position Embeddings：与Transformer中的位置向量不同，BERT中的位置向量是直接训练出来的</li>
</ul>
<h4 id="fine-tunninng">Fine-tunninng</h4>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/bert_finetune.jpg" alt></p>
<p>对于不同的下游任务，我们仅需要对BERT不同位置的输出进行处理即可，或者直接将BERT不同位置的输出直接输入到下游模型当中。具体的如下所示：</p>
<ul>
<li>对于情感分析等单句分类任务，可以直接输入单个句子（不需要[SEP]分隔双句），将[CLS]的输出直接输入到分类器进行分类</li>
<li>对于句子对任务（句子关系判断任务），需要用[SEP]分隔两个句子输入到模型中，然后同样仅须将[CLS]的输出送到分类器进行分类</li>
<li>对于问答任务，将问题与答案拼接输入到BERT模型中，然后将答案位置的输出向量进行二分类并在句子方向上进行softmax（只需预测开始和结束位置即可）</li>
<li>对于命名实体识别任务，对每个位置的输出进行分类即可，如果将每个位置的输出作为特征输入到CRF将取得更好的效果。</li>
</ul>
<h4 id="you-dian">优点</h4>
<ul>
<li>考虑双向信息（上下文信息）</li>
<li>不用考虑很长的时序问题（梯度弥散，梯度爆炸）：long-term dependency</li>
</ul>
<h4 id="que-dian">缺点</h4>
<ul>
<li>BERT的预训练任务MLM使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务(训练数据缺乏mask)</li>
<li>缺乏生成能力</li>
<li>另外，BERT没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计。（朴素贝叶斯，naive）</li>
<li>由于最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）</li>
<li>适合处理自然语义理解类任务(NLU)，而不适合自然语言生成类任务(NLG)</li>
</ul>
<h4 id="torch-shi-yong">torch使用</h4>
<ol>
<li>导入相关库</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> transformers <span class="keyword">as</span> ppb <span class="comment"># pytorch transformers</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>导入预训练好的 DistilBERT 模型与分词器</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, <span class="string">'distilbert-base-uncased'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Want BERT instead of distilBERT? Uncomment the following line:</span></span><br><span class="line"><span class="comment">#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')# </span></span><br><span class="line"></span><br><span class="line">Load pretrained model/tokenizertokenizer = tokenizer_class.from_pretrained(pretrained_weights)</span><br><span class="line">model = model_class.from_pretrained(pretrained_weights)</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>tokenize：分词+[cls]+[sep]<br>
<img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/bert_tokenize.png" alt="avatar"></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.encode(<span class="string">'a visually stunning rumination on love'</span>, add_special_tokens=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>padding</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_len = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tokenized.values:</span><br><span class="line"> <span class="keyword">if</span> len(i) &gt; max_len:</span><br><span class="line">     max_len = len(i)</span><br><span class="line">padded = np.array([i + [<span class="number">0</span>]*(max_len-len(i)) <span class="keyword">for</span> i <span class="keyword">in</span> tokenized.values])</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>masking</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">attention_mask = np.where(padded != <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ol start="6">
<li>使用bert进行编码</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># last_hidden_states = [batch_size, max_sentence_len, 768]</span></span><br><span class="line">    last_hidden_states = model(input_ids, attention_mask=attention_mask)</span><br></pre></td></tr></table></figure>
<p><a href="http://fancyerii.github.io/2019/03/09/bert-codes/#dataprocessor" target="_blank" rel="noopener">bert代码阅读</a></p>
<hr>
<p>谷歌对BERT进行了改版，下面将对改版后的bert进行学习，对比改版前后主要的相似点和不同点，以便可以选择在研究或应用中使用哪一种。提出的几种方法改进BERT的预测指标或计算速度，但是始终达不到两者兼顾。<strong>XLNet和RoBERTa改善了性能，而DistilBERT提高了推理速度</strong>。</p>
<h3 id="strong-bert-wwm-strong"><strong>BERT-WWM</strong></h3>
<p><a href>论文</a></p>
<h4 id="jian-jie">简介</h4>
<p>wwm 即 Whole Word Masking（对全词进行Mask）。相比于bert的改进是用Mask标签替换一个完整的词而不是字，中文和英文不同，英文最小的token是一个单词，而中文中最小的token却是字，词是由一个或多个字组成，且每个词之间没有明显的分割，包含更多信息的是词，对全词mask就是对整个词都通过mask进行掩码。<br>
<img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/bert_wwm_exam.png" alt></p>
<h4 id="strong-bert-wwm-ext-strong"><strong>Bert-wwm-ext</strong></h4>
<p>它是BERT-wwm的一个升级版，相比于BERT-wwm的改进是增加了训练数据集同时也增加了训练步数。<br>
BERT-wwm-ext主要是有两点改进：<br>
1）预训练数据集做了增加，达到5.4B；<br>
2）训练步数增大，训练第一阶段1M步，训练第二阶段400K步。</p>
<p><a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">模型下载</a></p>
<h3 id="xl-net">XLNet</h3>
<p>XLNet作为bert的升级模型，主要在以下三个方面进行了优化:</p>
<ul>
<li>采用AR模型替代AE模型，解决mask带来的负面影响</li>
<li>双流注意力机制</li>
<li>引入transformer-xl</li>
</ul>
<h4 id="ar-yu-ae-yu-yan-mo-xing">AR与AE语言模型</h4>
<p>目前主流的nlp预训练模型包括两类 Auto Regressive (<strong>AR</strong>) Language Model 与Auto Encoding (AE) Language Model。</p>
<h5 id="ar-mo-xing">AR模型</h5>
<p>AR模型的主要任务在于评估语料的概率分布，例如，给定一个序列 \(X=(x_1, \cdots ,x_T)\) ，AR模型就是在计算其极大似然估计\(p(X)=\prod_{t=1}^Tp(x_t∣x_{&lt;t})\)即已知\(x_t\)之前的序列，预测\(x_t\) 的值，当然也可以反着来\(p(X)=\prod_{t=1}^Tp(x_t∣x_{>t})\)即已知\(x_t\)之后的序列，预测\(x_t\)的值。AR模型一个很明显的缺点就是：模型是单向的，我们更希望的是根据上下文来预测目标，而不单是上文或者下文，之前open AI提出的GPT就是采用的AR模式，包括GPT2.0也是该模式。</p>
<p>优点：</p>
<ol>
<li>具备生成能力</li>
<li>考虑了相关性</li>
<li>无监督</li>
<li>严格的数学表达</li>
</ol>
<p>缺点：</p>
<ol>
<li>单向的</li>
<li>（难考虑 long term dependency）</li>
</ol>
<h5 id="ae-mo-xing">AE模型</h5>
<p>AE模型采用的就是以上下文的方式，最典型的成功案例就是bert。简单回顾下bert的预训练阶段，预训练包括了两个任务，Masked Language Model与Next Sentence Prediction，Next Sentence Prediction即判断两个序列的推断关系，Masked Language Model采用了一个标志位[MASK]来随机替换一些词，再用[MASK]的上下文来预测[MASK]的真实值，bert的最大问题也是处在这个MASK的点，因为在微调阶段，没有MASK这就导致预训练和微调数据的不统一，从而引入了一些人为误差。</p>
<p>在xlnet中，采用了<strong>AR模型</strong>，但是怎么解决这个上下文的问题呢？</p>
<h4 id="pai-lie-yu-yan-mo-xing-wei-liao-kao-lu-shang-xia-wen-xin-xi">排列语言模型(为了考虑上下文信息)</h4>
<p>为了解决上文提到的问题，作者提出了排列语言模型，该模型不再对传统的AR模型的序列的值按顺序进行建模，而是最大化所有可能的序列的因式分解顺序的期望对数似然，这句话可能有点不好理解，举个栗子，假如我们有一个序列\([1,2,3,4]\)，如果我们的预测目标是3，对于传统的AR模型来说，结果是\(p(3)=\prod^3_{t=1}p(3|x_{&lt;t})\)，如果采用本文的方法，先对该序列进行因式分解，最终会有24种排列方式，下图是其中可能的四种情况，对于第一种情况因为3的左边没有其他的值，所以该情况无需做对应的计算，第二种情况3的左边还包括了2与4，所以得到的结果是\(p(3)=p(3|2)p(3|2,4)\)，后续的情况类似，这样处理过后不但保留了序列的上下文信息，也<strong>避免了采用mask标记位，巧妙的改进了bert与传统AR模型的缺点</strong>。<br>
<img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/xlnet.png" alt></p>
<h5 id="ru-he-xun-lian">如何训练？</h5>
<ol>
<li>假如句子长度为20，则会产生\(20\)!种排列，在这\(20\)!种排列中随机抽样，得到训练样本。</li>
<li>为获得丰富的上下文语义信息，对抽取的样本最后的几个单词进行预测。</li>
</ol>
<h4 id="ji-yu-mu-biao-gan-zhi-biao-zheng-de-strong-shuang-liu-zi-zhu-yi-li-strong-wei-liao-jie-jue-wei-zhi-xin-xi">基于目标感知表征的<strong>双流自注意力</strong>(为了解决位置信息)</h4>
<p>虽然排列语言模型能满足目前的目标，但是对于普通的transformer结构来说是存在一定的问题的，为什么这么说呢，假设我们要求这样的一个对数似然，\(p_\theta(X_{z_t}|x_{z_{&lt;t}})\)，如果采用标准的softmax的话，那么<br>
\[
p_{\theta}\left(X_{z_t} | x_{z_{&lt;t}}\right)=\frac{\exp \left(e(x)^{T} h_{\theta}\left(x_{z_{&lt;t}}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{T} h_{\theta}\left(x_{z_{&lt;t}}\right)\right)}
\]<br>
其中\(h_\theta (x_{z_{&lt;t}})\)表示的是添加了mask后的transformer的输出值，可以发现\(h_\theta (x_{z_{&lt;t}})\)并不依赖于其要预测的内容的位置信息，因为无论预测目标的位置在哪里，因式分解后得到的所有情况都是一样的，并且transformer的权重对于不同的情况是一样的，因此无论目标位置怎么变都能得到相同的分布结果，如下图所示，假如我们的序列index表示为[1,2,3]，对于目标2与3来说，其因式分解后的结果是一样的，那么经过transformer之后得到的结果肯定也是一样的。<br>
<img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/123.png" alt></p>
<p>这就导致模型没法得到正确的表述，为了解决这个问题，论文中提出来新的分布计算方法，来实现目标位置感知<br>
\[
p_{\theta}\left(X_{zt}=x | x_{z&lt;t}\right)=\frac{\exp \left(e(x)^{T} g_{\theta}\left(x_{z&lt;t}, z_{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{T} g_{\theta}\left(x_{z&lt;t}, z_{t}\right)\right)}
\]<br>
其中\(g_{\theta}\left(x_{z_{&lt;t}}, z_{t}\right)\)是新的表示形式，并且把位置信息\(z_t\)作为了其输入。<br>
这个新的表示形式，论文把该方法称为Two-Stream Self-Attention，双流自注意力，该机制需要解决了两个问题:</p>
<ul>
<li>如果目标是预测\(x_{z_t}\)，\(g_\theta (x_{z&lt;t},z_t)\)那么只能有其位置信息\(z_t\) 而不能包含内容信息\(x_{z_t}\)</li>
<li>如果目标是预测其他tokens即\(x_{z_j}\)那么应该包含\(x_{z_t}\)的内容信息这样才有完整的上下文信息</li>
</ul>
<p>传统的transformer并不满足这样的需求，因此作者采用了两种表述来代替原来的表述，这也是为什么称为<strong>双流</strong>的原因:</p>
<ul>
<li>
<p>content representation内容表述，即\(h_\theta (x_{z_{\leq t}})\)下文用\(h_{z_t}\)表示，该表述和传统的transformer一样，同时编码了上下文和\(x_{z_t}\)自身<br>
\[
h_{z_{t}}^{(m)}=\text {Attention}\left(Q=h_{z_{t}}^{(m-1)}, K V=h_{z \leq t}^{(m-1)} ; \theta\right)
\]<br>
<img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/content_representation.png" alt></p>
</li>
<li>
<p>query representation查询表述，即\(g_\theta (x_{z_{&lt;t}})\)，下文用\(g_{z_t}\) 表示，该表述包含上下文的内容信息\(x_{z_{&lt;t}}\) 和目标的位置信息\(z_t\)，但是不包括目标的内容信息\(x_{z_{t}}\) ，从图中可以看到，K与V的计算并没有包括Q，自然也就无法获取到目标的内容信息，但是目标的位置信息在计算Q的时候保留了下来，<br>
\[
g_{z_{t}}^{(m)}=\text {Attention}\left(Q=g_{z_{t}}^{(m-1)}, K V=h_{z&lt;t}^{(m-1)} ; \theta\right)
\]</p>
</li>
</ul>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/query_representation.png" alt></p>
<p>总的计算过程，首先，第一层的查询流是随机初始化了一个向量即\(g_i^{0}=w\)，内容流是采用的词向量即\(h_i^{0}=e(x_i)\)，self-attention的计算过程中两个流的网络权重是共享的，最后在微调阶段，只需要简单的把query stream移除，只采用content stream即可。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/two_stream.png" alt></p>
<h4 id="yin-ru-transformer-xl">引入transformer XL</h4>
<p>作者还将transformer-xl的两个最重要的技术点应用了进来，即片段循环机制与相对位置编码。</p>
<h5 id="pian-duan-xun-huan-ji-zhi">片段循环机制</h5>
<p>ransformer-xl的提出主要是为了解决超长序列的依赖问题，对于普通的transformer由于有一个最长序列的超参数控制其长度（如果不对文本控制，则会极大消耗内存，因为在计算score矩阵的时候，会开辟\(n^2\)大小的矩阵，对于一篇文本，长度为\(10^5\),则需要开辟\(10^5 * 10^5 = 10^10\)大小的score矩阵，极其消耗内存。），对于特别长的序列就会导致丢失一些信息，transformer-xl就能解决这个问题。我们看个例子，假设我们有一个长度为1000的序列，如果我们设置transformer的最大序列长度是100，那么这个1000长度的序列需要计算十次，并且每一次的计算都没法考虑到每一个段之间的关系，如果采用transformer-xl，首先取第一个段进行计算，然后把得到的结果的隐藏层的值进行缓存，第二个段计算的过程中，把缓存的值拼接起来再进行计算（RNN）。该机制不但能保留长依赖关系还能加快训练，因为每一个前置片段都保留了下来，不需要再重新计算，在transformer-xl的论文中，经过试验其速度比transformer快了1800倍。<br>
在xlnet中引入片段循环机制其实也很简单，只需要在计算KV的时候做简单的修改，其中\(\tilde{h}^{m-1}\)表示的是缓存值。<br>
\[
h_{z_{t}}^{(m)}=\text {Attention}\left(Q=h_{z_{t}}^{(m-1)}, K V=\left[\tilde{h}^{(m-1)}, h_{z \leq t}^{(m-1)}\right] ; \theta\right)
\]</p>
<h5 id="xiang-dui-wei-zhi-bian-ma">相对位置编码</h5>
<p>bert的position embedding采用的是绝对位置编码，但是绝对位置编码在transformer-xl中有一个致命的问题，因为没法区分到底是哪一个片段里的，这就导致了一些位置信息的损失，这里被替换为了transformer-xl中的相对位置编码。假设给定一对位置\(i\)与\(j\)，如果\(i\)与\(j\)是同一个片段里的那么我们令这个片段编码\(s_{ij}=s_+\)，如果不在一个片段里则令这个片段编码为\(s_{ij}=s_-\)，这个值是在训练的过程中得到的，也是用来计算attention weight时候用到的，在传统的transformer中\(\text {attention weight}= softmax(\frac{Q⋅K}{d}V)\)，在引入相对位置编码后，首先要计算出\(a_{ij}=(q_i+b)^T_{s_{sj}}\)，其中\(b\)也是一个需要训练得到的偏执量，最后把得到的\(a_ij\)与传统的transformer的weight相加从而得到最终的attention weight。</p>
<p><a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_xlnet.py" target="_blank" rel="noopener">源码</a></p>
<h4 id="zong-jie">总结</h4>
<p>在实验中发现，xlnet更多还是在长文本的阅读理解类的任务上提升更明显一些，这也很符合上文中介绍的这些优化点，在实际工业场景中，机器翻译、本文摘要类的任务应该会有更好的效果，当然在其他的文本分类、自然语言推理等任务上xlnet也有一定的效果提升。nlp领域的模型目前已经完全采用了pretrain+fine tuning的模式，GPT2.0的单向模式在增大训练语料的情况下效果就已经超越了bert，可想而知该领域的研究还有很大的上升空间</p>
<h3 id="ro-ber-ta">RoBERTa</h3>
<h4 id="jian-jie-1">简介</h4>
<p>在XLNet全面超越Bert后没多久，Facebook提出了RoBERTa（a Robustly Optimized BERT Pretraining Approach）。再度在多个任务上达到SOTA。</p>
<p>它在模型层面没有改变Google的Bert，<strong>改变的只是预训练的方法</strong>。</p>
<h4 id="dong-ji">动机</h4>
<p>Yinhan Liu等人[6]认为超参数的选择对最终结果有重大影响，为此他们提出了BERT预训练的重复研究，其中包括对超参数调整和训练集大小的影响的仔细评估。<strong>最终，他们发现了BERT的训练不足</strong>，并提出了一种改进的模型来训练BERT模型。</p>
<h4 id="gai-jin">改进</h4>
<h5 id="jing-tai-masking-vs-dong-tai-masking">静态Masking vs 动态Masking</h5>
<p>原来Bert对每一个序列随机选择15%的Tokens替换成[MASK]，为了消除与下游任务的不匹配，还对这15%的Tokens进行（1）80%的概率替换成[MASK]；（2）10%的概率不变；（3）10%的概率替换成其他词。但整个训练过程，这15%的Tokens一旦被选择就不再改变，也就是说从一开始随机选择了这15%的Tokens，之后的N个epoch里都不再改变了。这就叫做静态Masking。</p>
<p>而RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。这就叫做动态Masking。作者在只将静态Masking改成动态Masking，其他参数不变的情况下做了实验，动态Masking确实能提高性能。</p>
<h5 id="with-nsp-vs-without-nsp">with NSP vs without NSP</h5>
<p>原本的Bert为了捕捉句子之间的关系，使用了NSP任务进行预训练，就是输入一对句子A和B，判断这两个句子是否是连续的。在训练的数据中，50%的B是A的下一个句子，50%的B是随机抽取的。</p>
<p>而RoBERTa去除了NSP，而是每次输入连续的多个句子，直到最大长度512（可以跨文章）。这种训练方式叫做（FULL - SENTENCES），而原来的Bert每次只输入两个句子。实验表明在MNLI这种推断句子关系的任务上RoBERTa也能有更好性能。（？？？？）</p>
<h5 id="geng-da-de-mini-batch">更大的mini-batch</h5>
<p>原本的BERTbase 的batch size是256，训练1M个steps。RoBERTa的batch size为8k。为什么要用更大的batch size呢？作者借鉴了在机器翻译中，用更大的batch size配合更大学习率能提升模型优化速率和模型性能的现象，并且也用实验证明了确实Bert还能用更大的batch size。</p>
<h5 id="geng-duo-de-shu-ju-geng-chang-de-xun-lian-shi-jian">更多的数据，更长的训练时间</h5>
<p>借鉴XLNet用了比Bert多10倍的数据，RoBERTa也用了更多的数据。性能确实再次彪升。当然，也需要配合更长时间的训练。</p>
<h5 id="byte-pair-encoding-bpe-zi-fu-bian-ma">Byte-Pair Encoding (BPE)字符编码</h5>
<p>使用Sennrich[8]等人提出的Byte-Pair Encoding (BPE)字符编码，它是字符级和单词级表示之间的混合体，可以处理自然语言语料库中常见的大词汇，避免训练数据出现更多的“[UNK]”标志符号，从而影响预训练模型的性能。其中，“[UNK]”标记符表示当在BERT自带字典vocab.txt找不到某个字或者英文单词时，则用“[UNK]”表示。</p>
<h3 id="span-bert">SpanBERT</h3>
<p><a href="https://arxiv.org/pdf/1907.10529.pdf" target="_blank" rel="noopener">论文</a></p>
<h4 id="jian-jie-2">简介</h4>
<p>在许多 NLP 任务中都涉及对多个文本分词间关系的推理。例如，在抽取式问答任务中，在回答问题“Which NFL team won Super Bown 50?”时，判断“Denver Broncos” 是否属于“NFL team”是非常重要的步骤。相比于在已知“Broncos”预测“Denver”的情况，直接预测“Denver Broncos”难度更大，这意味着这类分词对自监督任务提出了更多的挑战。</p>
<p>SpanBert对 BERT 模型进行了如下改进：</p>
<ol>
<li>没有segment embedding，只有一个长的句子，类似RoBERTa。</li>
<li><strong>Span Masking</strong>:对随机的邻接分词（span）而非随机的单个词语（token）添加mask；</li>
<li>MLM+SBO:通过使用分词边界的表示来预测被添加mask的分词的内容，不再依赖分词内单个 token 的表示。</li>
</ol>
<p>SpanBERT 能够对分词进行更好地表示和预测。该模型和 BERT 在mask机制和训练目标上存在差别。首先，SpanBERT 不再对随机的单个 token 添加mask，而是对随机对邻接分词添加mask。其次，SpanBert提出了一个新的训练目标 span-boundary objective (SBO) 进行模型训练。通过对分词添加mask，作者能够使模型依据其所在语境预测整个分词。另外，SBO 能使模型在边界词中存储其分词级别的信息，使得模型的调优更佳容易。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/spanbert.png" alt></p>
<p>模型使用边界词 was和 to来预测分词中的每个单词。</p>
<p>为了搭建 SpanBERT ，作者首先构建了一个 BERT 模型的并进行了微调，SpanBERT的表现优于原始 BERT 模型。在搭建baseline的时候，作者发现对单个部分进行预训练的效果，比使用 next sentence prediction (NSP) 目标对两个长度为一半的部分进行训练的效果更优，在下游任务中表现尤其明显。因此，作者在经过调优的 BERT 模型的顶端对模型进行了改进。</p>
<h4 id="span-masking">Span Masking(???)</h4>
<p>对于每一个单词序列 \(X = (x_1, \ldots , x_n )\)，作者通过迭代地采样文本的分词选择单词，直到达到掩膜要求的大小（例如 X 的 15%），并形成 X 的子集 Y。在每次迭代中，作者首先从几何分布 \(l \sim Geo(p)\)中采样得到分词的长度，该几何分布是偏态分布，偏向于较短的分词。之后，作者随机（均匀地）选择分词的起点。</p>
<p>根据几何分布，先随机选择一段（span）的长度，之后再根据均匀分布随机选择这一段的起始位置，最后按照长度遮盖。作者设定几何分布取 p=0.2，并裁剪最大长度只能是 10（不应当是长度 10 以上修剪，而应当为丢弃），利用此方案获得平均采样长度分布。因此分词的平均长度为 3.8 。作者还测量了词语（word）中的分词程度，使得添加掩膜的分词更长。下图展示了分词掩膜长度的分布情况。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/spanbert_fenci.png" alt></p>
<p>和在 BERT 中一样，作者将 Y 的规模设定为 X 的15%，其中 80% 使用 [MASK] 进行替换，10% 使用随机单词替换，10%保持不变。与之不同的是，作者是在分词级别进行的这一替换，而非将每个单词单独替换。</p>
<h4 id="fen-ci-bian-jie-mu-biao-sbo">分词边界目标(SBO)???</h4>
<p>分词选择模型一般使用其边界词创建一个固定长度的分词表示。为了于该模型相适应，作者希望结尾分词的表示的总和与中间分词的内容尽量相同。为此，作者引入了 SBO ，其仅使用观测到的边界词来预测带掩膜的分词的内容。</p>
<p>具体做法是，在训练时取 Span 前后边界的两个词，值得指出，这两个词不在 Span 内，然后<strong>用这两个词向量加上 Span 中被遮盖掉词的位置向量，来预测原词</strong>。<br>
\[
\mathbf{y}_{i}=f\left(\mathbf{x}_{s-1}, \mathbf{x}_{e+1}, \mathbf{p}_{i}\right)
\]<br>
详细做法是将词向量和位置向量拼接起来，作者使用一个两层的前馈神经网络作为表示函数，该网络使用 GeLu 激活函数，并使用层正则化：</p>
<p>\[
\begin{aligned}
\mathbf{h} &amp;=\text { LayerNorm }\left(\operatorname{GeLU}\left(W_{1} \cdot\left[\mathbf{x}_{s-1} ; \mathbf{x}_{e+1} ; \mathbf{p}_{i}\right]\right)\right) \\
f(\cdot) &amp;=\text { LayerNorm }\left(\operatorname{GeLU}\left(W_{2} \cdot \mathbf{h}\right)\right)
\end{aligned}
\]<br>
作者使用向量表示\(y_i\)来预测\(x_i\)，并和 MLM 一样使用交叉熵作为损失函数，就是 SBO 目标的损失，之后将这个损失和 BERT 的 **Mased Language Model （MLM）**的损失加起来，一起用于训练模型。<br>
\[
\mathcal{L}(\text { football })=\mathcal{L}_{\mathrm{MLM}}\left(\mathbf{x}_{7}\right)+\mathcal{L}_{\mathrm{SBO}}\left(\mathbf{x}_{4}, \mathbf{x}_{9}, \mathbf{p}_{7}\right)
\]</p>
<h4 id="dan-xu-lie-xun-lian">单序列训练</h4>
<p>SpanBERT 没用 Next Sentence Prediction (NSP) 任务，而是直接用 Single-Sequence Training，也就是不加入 NSP 任务来判断是否两句是上下句，直接用一句来训练，片段长度最多为512个单词。</p>
<h4 id="xi-jie">细节</h4>
<ol>
<li>训练时用了 <strong>Dynamic Masking</strong> 而不是像 BERT 在预处理时做 静态Mask。</li>
<li><strong>取消 BERT 中随机采样短句的策略</strong></li>
<li>对Adam优化器中的一些参数做了改变。</li>
</ol>
<h5 id="mask-ji-zhi">mask机制</h5>
<p>作者在子单词、完整词语、命名实体、名词短语和随机分词方面进行了比较：发现使用随机分词掩膜机制效果更优。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/spanbert_token.png" alt></p>
<h5 id="fu-zhu-mu-biao">辅助目标</h5>
<p>使用 SBO 替换 NSP 并使用单序列进行预测的效果更优。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/spanbert_a_o.png" alt></p>
<h4 id="zong-jie-1">总结</h4>
<p>SpanBERT是基于分词的预训练模型，在多个评测任务中的得分都超越了 BERT 且在分词选择类任务中的提升尤其明显。</p>
<h3 id="ernie">ERNIE</h3>
<p><a href="https://arxiv.org/pdf/1905.07129" target="_blank" rel="noopener">论文</a></p>
<h4 id="jian-jie-3">简介</h4>
<p>无论是Elmo、GPT, 还是能力更强的 BERT 模型，其建模对象主要聚焦在原始语言信号上，较少利用语义知识单元建模。这个问题在中文方面尤为明显，例如，BERT 在处理中文语言时，通过预测汉字进行建模，模型很难学出更大语义单元的完整语义表示。例如，对于乒 [mask] 球，清明上 [mask] 图，[mask] 颜六色这些词，BERT 模型通过字的搭配，很容易推测出掩码的字信息，但没有显式地对语义概念单元 (如乒乓球、清明上河图) 以及其对应的语义关系进行建模。如果能够让模型学习到海量文本中蕴含的潜在知识，势必会进一步提升各个 NLP 任务效果。</p>
<p>ERNIE 模型通过建模海量数据中的实体概念等先验语义知识，学习真实世界的语义关系。具体来说，ERNIE 模型通过对词、实体等语义单元的掩码，使得模型学习完整概念的语义表示。相较于 BERT 学习原始语言信号，ERNIE 直接对先验语义知识单元进行建模，增强了模型语义表示能力。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/ernie.png" alt></p>
<p>在 BERT 模型中，通过『哈』与『滨』的局部共现，即可判断出『尔』字，模型没有学习与『哈尔滨』相关的知识。而 ERNIE 通过学习词与实体的表达，使模型能够建模出『哈尔滨』与『黑龙江』的关系，学到『哈尔滨』是『黑龙江』的省会以及『哈尔滨』是个冰雪城市。</p>
<h4 id="mo-xing">模型</h4>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/ernie_ar.png" alt></p>
<p>模型上主要的改进是在bert的后段加入了实体向量和经过bert编码后的向量拼接，另外在输出时多加了实体自编码的任务，从而帮助模型注入实体知识信息。</p>
<h5 id="t-encoder">T-Encoder</h5>
<p>这部分就是纯粹的bert结构，在该部分模型中主要负责对输入句子（token embedding, segment embedding和positional embedding）进行编码.</p>
<h5 id="k-encoder">K-Encoder</h5>
<p>引入实体信息，使用了TransE训练实体向量，再通过多头Attention进行编码，然后通过实体对齐技术，将实体向量加入到对应实体的第一个token编码后的向量上。（例如姚明是一个实体，在输入时会被分割成姚、明，最后在这部分引入的姚明这个实体的向量会被加入到“姚”这个字经过bert之后的向量上去）<br>
\[
\begin{aligned}
\left\{\tilde{\boldsymbol{w}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{w}}_{n}^{(i)}\right\} &amp;=\mathrm{MH}-\mathrm{ATT}\left(\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}\right) \\
\left\{\tilde{\boldsymbol{e}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{e}}_{m}^{(i)}\right\} &amp;=\mathrm{MH}-\mathrm{ATT}\left(\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}\right)
\end{aligned}
\]<br>
由于直接拼接后向量维度会与其他未拼接的向量维度不同，所以加入information fusion layer，另外考虑到后面的实体自编码任务，所以这里在融合信息之后，有实体向量加入的部分需要另外多输出一个实体向量。</p>
<p>加入实体信息之后的融合输出过程：<br>
\[
\begin{aligned}
\boldsymbol{h}_{j} &amp;=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{W}}_{e}^{(i)} \tilde{\boldsymbol{e}}_{k}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\
\boldsymbol{w}_{j}^{(i)} &amp;=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \\
\boldsymbol{e}_{k}^{(i)} &amp;=\sigma\left(\boldsymbol{W}_{e}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{e}^{(i)}\right)
\end{aligned}
\]<br>
未加入实体信息之后的融合输出过程：<br>
\[
\begin{aligned}
\boldsymbol{h}_{j} &amp;=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\
\boldsymbol{w}_{j}^{(i)} &amp;=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right)
\end{aligned}
\]</p>
<h5 id="shi-ti-zi-bian-ma">实体自编码???</h5>
<p>为了更好地使用实体信息，作者在这里多加入了一个预训练任务 entity auto-encoder(dEA)。<br>
\[
p\left(e_{j} | w_{i}\right)=\frac{\exp \left(\operatorname{linear}\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{j}\right)}{\sum_{k=1}^{m} \exp \left(\operatorname{linear}\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{k}\right)}
\]<br>
该预训练任务和bert相似，按以下方式进行训练：</p>
<ol>
<li>15% 的情况下：用 [MASK] 替换被选择的单词，例如，my dog is hairy → my dog is [MASK]</li>
<li>5% 的情况下：用一个随机单词替换被选择的单词，例如，my dog is hairy → my dog is apple</li>
<li>80% 的情况下：保持被选择的单词不变，例如，my dog is hairy → my dog is hairy</li>
</ol>
<h5 id="extra-task">Extra-task</h5>
<p>除了正常的任务之外，ERNIE引入了两个新任务Entity Typing和Relation Classification</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/ernie_extra_task.png" alt></p>
<h5 id="zong-jie-2">总结</h5>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/ernie_experiment.png" alt></p>
<p>ERNIE在通用任务其实相比bert优势不大，尽管文章提到了ERNIE更鲁棒以及GLUE的评估对ERNIE不是很友好，在增加复杂度的同时，并没有取得期待的效果。ERNIE提供了一种很好的实体信息引入思路，并且其新提出的预训练方法也给希望将bert这一模型引入关系抽取领域提供了很好的例子。</p>
<h3 id="mt-dnn-yu-xun-lian-duo-ren-wu">MT-DNN（预训练+多任务）</h3>
<p><a href="https://arxiv.org/pdf/1901.11504.pdf" target="_blank" rel="noopener">论文</a>,<a href="https://github.com/namisan/mt-dnn" target="_blank" rel="noopener">源码</a></p>
<h4 id="jian-jie-4">简介</h4>
<p>学习文本的向量空间表示，是许多自然语言理解(NLU)任务的基础。两种流行的方法是<strong>多任务学习</strong>和<strong>语言模型的预训练</strong>。MT-DNN通过提出一种新的多任务深度神经网络（MT-DNN）来综合两种方法的优点。</p>
<p><strong>预训练</strong>，如BERT、GPT等，就不多说了。</p>
<p><strong>多任务</strong>学习( MTL )的灵感来自于人的学习活动。在人类的学习活动中，人们经常应用从以前的任务中学到的知识来帮助学习新的任务。例如，在学习滑冰这件事情上，一个知道如何滑雪的人比什么都不知道的人容易。同样，**联合学习多个(相关)**任务也很有用，这样在一个任务中学习到的知识有益于其他任务。对于MTL（Multi-task Learning，多任务学习）来说，其优点有两个：1）弥补了有些任务的数据不足问题；2）有正则的作用，防止模型过拟合。</p>
<p><strong>两者结合</strong>（强强联手！）：MT-DNN认为，MTL和pretrain有很好的互补作用，那么是不是可以结合一下，发挥两者的作用。更具体的就是，先用BERT进行pretrain，然后用MTL进行finetune，这就形成了MT-DNN。可见，与BERT的不同在于finetune的过程，这里用MTL作为目标。</p>
<h4 id="mo-xing-1">模型</h4>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/mtdnn.png" alt></p>
<p>表征学习 MT-DNN 模型的架构。下面的网络层在所有任务中都共享，上面的两层是针对特定任务。输入\(X\)(一句话或句子对)首先表征为一个序列的嵌入向量，在\(l_1\)  中每个词对应一个向量。然后 Transformer 编码器捕捉每个单词的语境信息并在\(l_2\)中生成共享的语境嵌入向量。最后，针对每个任务，特定任务层生成特定任务的表征，而后是分类、相似性打分、关联排序等必需的操作。</p>
<h5 id="duo-ren-wu">多任务</h5>
<p>MT-DNN是结合了4种类型的NLU任务：单句分类、句子对分类、文本相似度打分和相关度排序。</p>
<ul>
<li>单句分类：CoLA任务是预测英语句子是否合乎语法，SST-2任务预测电影评论是正向还是负向。</li>
<li>文本相似度：这是一个回归任务。对于给定的句子对，模型计算二者之间的相似度。在GLUE中只有STS-B这个任务是处理文本相似度。</li>
<li>成对文本分类（文本蕴含）：对于给定的句子对，推理两个句子之间的关系。RET和MNLI是语言推理任务，推理句子之间是否存在蕴含关系、矛盾的关系或者中立关系。QQP和MRPC是预测句子是否语义等价。</li>
<li>相关性排序：给定一个问题和一系列候选答案，模型根据问题对所有候选答案进行排序。QNLI是斯坦福问答数据集的一个版本，任务是预测候选答案中是否包含对问题的正确答案。尽管这是一个二分类任务，但我们依旧把它当作排序任务，因为模型重排了候选答案，将正确答案排在更前。</li>
</ul>
<h6 id="dan-ju-fen-lei">单句分类</h6>
<p>用[CLS]的表征作为特征，设为\(x\)，则对于单句的分类任务，直接在后面接入一个分类层即可.<br>
\[
P_{r}(c | X)=\operatorname{softmax}\left(W^{T} \cdot x\right)
\]<br>
loss是交叉熵损失：<br>
\[
-\sum_{c} I(X, c) \log \left(P_{r}(c | X)\right)
\]</p>
<h6 id="ju-zi-xiang-si-du">句子相似度</h6>
<p>将两句话pack后送进去，得到的[CLS]的表征，可拿出来计算分数：<br>
\[
\operatorname{sim}\left(X_{1}, X_{2}\right)=\operatorname{sigmoid}\left(w^{T} \cdot x\right)
\]<br>
loss是MSE：<br>
\[
\left(y-\operatorname{Sim}\left(X_{1}, X_{2}\right)\right)^{2}
\]</p>
<h6 id="ju-zi-dui-fen-lei">句子对分类</h6>
<p>对这个任务不熟悉。。。</p>
<h6 id="xiang-guan-xing-pai-xu">相关性排序</h6>
<p>先计算两个句子之间的相似度，输入两个句子pack，采用[CLS]的输出作为表征。<br>
\[
\operatorname{Rel}(Q, A)=g\left(w^{T} \cdot x\right)
\]<br>
loss采用排序损失：<br>
\[
\begin{aligned}
&amp;-\sum_{Q, A^{+}} P_{r}\left(A^{+} | Q\right) \\
P_{r}\left(A^{+} | Q\right) &amp;=\frac{\exp \left(\gamma \operatorname{Rel}\left(Q, A^{+}\right)\right)}{\sum_{A^{\prime} \in A} \exp \left(\gamma \operatorname{Rel}\left(Q, A^{\prime}\right)\right)}
\end{aligned}
\]</p>
<h5 id="xun-lian-guo-cheng">训练过程</h5>
<p>MT-DNN 的训练程序包含两个阶段：预训练和多任务微调。</p>
<ol>
<li>预训练阶段遵循 BERT 模型的方式。lexicon encoder和Transformer encoder参数的学习是通过两个无监督预测任务：掩码语言建模(masked language modeling)和下一句预测(next sentence pre-<br>
diction)。</li>
<li>在多任务微调阶段，使用基于minibatch的随机梯度下降（SGD）来学习模型参数（也就是，所有共享层和任务特定层的参数），算法流程如下图所示。每个epoch，选择一个mini-batch \(b_t\)(在9个GLUE任务中)，再对特定任务\(k\)进行模型参数的更新。这近似地优化所有多任务的和。</li>
</ol>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/mtdnn_al.png" alt></p>
<h3 id="distil-bert">DistilBERT</h3>
<p><a href="https://arxiv.org/pdf/1910.01108" target="_blank" rel="noopener">论文</a></p>
<h4 id="dong-ji-1">动机</h4>
<p>预训练的语言模型正变得日益庞大，这些庞大的模型尽管能够带来准确率的提升，但是在这些预训练模型投入使用的过程中，往往需要对模型进行微调，而这需要大量的资源，而且模型投入使用的时候，由于计算量巨大，模型处理数据的时延过长。一种比较好的解决方案是对模型进行压缩。</p>
<h4 id="mo-xing-ya-suo-de-fang-fa">模型压缩的方法</h4>
<ol>
<li>
<p>量化</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/quantization.jpg" alt></p>
</li>
</ol>
<p>量化方法意味着降低模型权重的精度。比如K-means 量化方法：对模型权重矩阵W进行分组，分成n簇，然后把权重矩阵里的值转化成\(1,\cdots,n\)的正数。通过这种方式把矩阵中的32位的float型转化成只有8位（或者1位，binarizing matrix）的整数。</p>
<h5 id="cai-jian-pruning">裁剪（Pruning）</h5>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/Pruning.png" alt></p>
<h6 id="weight-pruning">weight pruning</h6>
<p>把权重矩阵中数值较大的值设置为0，从而构造稀疏矩阵。（理论上稀疏矩阵乘法速度快于normal(danse)矩阵乘法。）</p>
<h6 id="removing-neurons">Removing neurons</h6>
<p>直接删除权重矩阵中不重要的的某一行或某一列。（删除和fine-tune交替进行，这样剩下的神经元在一定程度上可以弥补被删除的神经元）</p>
<h6 id="removing-weight-matrices">Removing weight matrices</h6>
<p>[10]从big transformers中直接删除对accuracy贡献不大的整个attention head。但是这种方式不能保证能够并行加速，因为相邻的权重矩阵的size变的不同了。</p>
<h5 id="zhi-shi-zheng-liu">知识蒸馏</h5>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/Knowledge-distillation.jpg" alt></p>
<p>DistilBert使用模型蒸馏技术：这是一种能够将大型模型（被称为「老师」）压缩为较小模型（即「学生」）的技术。</p>
<p><a href="https://blog.csdn.net/nature553863/article/details/80568658" target="_blank" rel="noopener">知识蒸馏方法</a></p>
<h6 id="zhi-shi-zheng-liu-qian-yi-fan-hua-neng-li">知识蒸馏：迁移泛化能力</h6>
<p>知识蒸馏是一种压缩技术，要求对小型模型进行训练，以使其拥有类似于大型模型的行为特征。在监督学习领域，我们在训练分类模型时往往会利用对数似然信号实现概率最大化（logits 的 softmax），进而预测出正确类。在大多数情况下，性能良好的模型能够利用具有高概率的正确类预测输出分布，同时其它类的发生概率则接近于零。</p>
<p><strong>但是，某些“接近于零”的概率要比其它概率更大，这在一定程度上反映出模型的泛化能力</strong>。例如，把普通椅子误认为扶手椅虽然属于错误，但这种错误远比将其误认为蘑菇来得轻微。这种不确定性，有时被称为“暗知识”。也可以从另一个角度来理解蒸馏——用于防止模型对预测结果太过确定（类似于标签平滑）。</p>
<p>在语言建模当中，可以通过查看词汇表中的分布轻松观察到这种不确定性。下图为 BERT 对《卡萨布兰卡》电影当中经典台词下一句用词的猜测：</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/next_word.png" alt></p>
<p>BERT 提出的 20 大高概率用词猜测结果。语言模型确定了两个可能性最高的选项（day 与 life），接下来的词汇相比之下概率要低得多。</p>
<h6 id="ru-he-fu-zhi-zhe-xie-an-zhi-shi">如何复制这些“暗知识”</h6>
<p>在训练当中，通过训练学生网络，来模拟老师网络的全部输出分布（也就是知识），也就是通过匹配输出分布的方式训练学生网络，从而实现与老师网络相同的泛化方式，即通过软目标（老师概率）将交叉熵从老师处传递给学生。<br>
\[
L=-\sum_{i} t_{i} * \log \left(s_{i}\right)
\]<br>
其中 \(t_i\)为来自老师的 \(logit\)，\(s_i\) 为学生的 \(logit\)。以老师的行为\(t_i\)作为标签，让学生去学习老师的行为。**这个损失函数属于更丰富的训练信号，因为单一示例要比单一硬目标拥有更高的强制约束效果。**为了进一步揭示分类结果的质量，Hinton 等人提出了 softmax 温度的概念：<br>
\[
p_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}
\]<br>
T 为温度参数。当\(T \rightarrow 0\)时，分布变为 Kronecker（相当于独热目标矢量）；当 \(T \rightarrow +\infty\)时，则变为均匀分布。在训练过程中，将相同的温度参数应用于学生与老师网络，即可进一步为每个训练示例揭示更多信号。T 被设置为 1 即为标准 Softmax。</p>
<p>在蒸馏方面，使用 Kullback-Leibler 损失函数，因为其拥有相同的优化效果：<br>
\[
K L(p \| q)=\mathbb{E}_{p}\left(\log \left(\frac{p}{q}\right)\right)=\sum_{i} p_{i} * \log \left(p_{i}\right)-\sum_{i} p_{i} * \log \left(q_{i}\right)
\]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line">KD_loss = nn.KLDivLoss(reduction=<span class="string">'batchmean'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kd_step</span><span class="params">(teacher: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">            student: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">            temperature: float,</span></span></span><br><span class="line"><span class="function"><span class="params">            inputs: torch.tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            optimizer: Optimizer)</span>:</span></span><br><span class="line">    teacher.eval()</span><br><span class="line">    student.train()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        logits_t = teacher(inputs=inputs) <span class="comment"># 老师的输出</span></span><br><span class="line">    logits_s = student(inputs=inputs) <span class="comment"># 学生的输出</span></span><br><span class="line">    <span class="comment"># 计算kd_loss</span></span><br><span class="line">    loss = KD_loss(input=F.log_softmax(logits_s/temperature, dim=<span class="number">-1</span>),</span><br><span class="line">                   target=F.softmax(logits_t/temperature, dim=<span class="number">-1</span>))</span><br><span class="line">    </span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<p>利用老师信号，能够训练出一套较小的语言模型 DistilBERT，属于 BERT 的监督产物。</p>
<h4 id="mo-xing-jie-gou">模型结构</h4>
<p>DistilBERT想较于BERT删除了 token-type 嵌入与 pooler（用于下一句分类任务），但其余部分架构保持不变，而层数也减少至原本的二分之一。总体而言，蒸馏模型 DistilBERT 在总体参数数量上约为 BERT 的一半，但在 GLUE 语言理解基准测试中能够保留 95% 的 BERT 性能表现。</p>
<ol>
<li><strong>为什么不降低隐藏层的大小？</strong><br>
将 768 减至 512 ，意味着总参数量约下降至原本的二分之一。在现代框架当中，大多数运算都经过高度优化，而且张量的最终维度（隐藏维度）的变化会对 Transformer 架构（线性分层与层规范化）中的大部分运算产生小幅影响。实验中发现，层数对于推理时间的影响要远高于隐藏层的大小。因此，更小并不代表着一定更快。</li>
</ol>
<p>训练子网络的核心不只是建立架构，还要求为子网络找到正确的初始化方式以实现收敛。 DistilBERT 使用bert的参数进行初始化，将层数削减一半，并采用相同的隐藏大小。DistilBERT还用到了最近 RoBERTa 论文当中提到的一些训练技巧，这也再次证明 BERT 模型的训练方式对其最终表现有着至关重要的影响。与 RoBERTa 类似，对 DIstilBERT 进行大批次训练，使用梯度累积（每批最多 4000 个例子）、配合动态遮挡并删除了下一句预测目标。</p>
<h4 id="zong-jie-3">总结</h4>
<p>DistilBERT 的表现：保留了BERT  95% 以上的性能，同时将参数减少了 40%。推理时间方面，DistilBERT 比 BERT 快 60%，体积比 BERT 小 60%。</p>
<h3 id="albert">ALBERT</h3>
<p><a href="%5BPDF%5D(https://arxiv.org/pdf/1909.11942)">论文</a></p>
<h4 id="jian-jie-5">简介</h4>
<p>自BERT的成功以来，预训练模型都采用了很大的参数量以取得更好的模型表现。但是模型参数量越来越大也带来了很多问题，比如对算力要求越来越高、模型需要更长的时间去训练、甚至有些情况下参数量更大的模型表现却更差。为了解决目前预训练模型参数量过大的问题，albert提出了两种能够大幅减少预训练模型参数量的方法，此外还提出用Sentence-order prediction（SOP）任务代替BERT中的Next-sentence prediction（NSP）任务。</p>
<h4 id="mo-xing-2">模型</h4>
<h5 id="qian-ru-xiang-liang-yin-shi-fen-jie-factorized-embedding-parameterization">嵌入向量因式分解（Factorized embedding parameterization）</h5>
<p>在BERT、XLNet、RoBERTa等模型中，由于模型结构的限制，WordePiece embedding的大小\(E\) 总是与隐层大小\(H\)相同，即 \(E \equiv H\) 。从建模的角度考虑，词嵌入学习的是单词与上下文无关的表示，而隐层则是学习与上下文相关的表示。显然后者更加复杂，需要更多的参数，也就是说模型应当增大隐层大小\(H\)，或者说满足 \(H \gg E\)。但实际上词汇表的大小\(V\)通常非常大，如果\(E=H\)的话，增加隐层大小\(H\)后将会使embedding matrix的维度\(V \times E\)非常巨大。</p>
<p>因此ALBERT想要打破\(E\)与\(H\) 之间的绑定关系，从而减小模型的参数量，同时提升模型表现。具体做法是将embedding matrix分解为两个大小分别为\(V \times  E\) 和\(E \times H\) 矩阵，也就是说先将单词投影到一个低维的embedding空间 \(E\) ，再将其投影到高维的隐藏空间\(H\)  。这使得embedding matrix的维度从\(O(V \times H)\)  减小到\(O(v \times E + E \times H)\) 。当 \(H \gg E\)时，参数量减少非常明显。在实现时，随机初始化\(V\times E\)和\(E\times H\)的矩阵，计算某个单词的表示需用一个单词的one-hot向量乘以\(V\times E\)维的矩阵（也就是lookup），再用得到的结果乘\(E\times H\)维的矩阵即可。两个矩阵的参数通过模型学习。</p>
<p>从下图实验结果可见，对于不共享参数的情况，\(E\)几乎是与大越好；而共享参数之后， \(E\)太大反而会使模型表现变差，\(E=128\)模型表现最好，因此ALBERT的默认参数设置中\(E=128\).</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/albert_embedding.png" alt></p>
<p>考虑到ALBERT-base的 \(h=768\)，那么 \(E=768\)时，模型应该可以看作没有减少embedding参数量的情况。而不共享参数的实验结果表明此时模型表现更好，那么似乎说明了Factorized embedding在一定程度上降低了模型的表现。</p>
<h5 id="kua-ceng-can-shu-gong-xiang-can-shu-liang-jian-shao-zhu-yao-gong-xiang">跨层参数共享（参数量减少主要共享）</h5>
<p>另一个减少参数量的方法就是层之间的参数共享，即多个层使用相同的参数。参数共享有三种方式：只共享feed-forward network的参数、只共享attention的参数、共享全部参数。ALBERT默认是共享全部参数的。实验表明加入参数共享之后，每一层的输入embedding和输出embedding的L2距离和余弦相似度都比BERT稳定了很多。这证明参数共享能够使模型参数更加稳定。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/cross.png" alt></p>
<p>如下图所示，可以看出参数共享几乎也是对模型结果有负面影响的。但是考虑到其能够大幅削减参数量，并且对结果影响不是特别大，因此权衡之下选择了参数共享。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/albert_crosslayer.png" alt></p>
<h5 id="ju-jian-lian-guan-xing">句间连贯性</h5>
<ul>
<li><strong>NSP</strong>：下一句预测， 正样本=上下相邻的2个句子，负样本=随机2个句子</li>
<li><strong>SOP</strong>：句子顺序预测，正样本=正常顺序的2个相邻句子，负样本=调换顺序的2个相邻句子</li>
</ul>
<p>NSP任务过于简单，只要模型发现两个句子的主题不一样就行了，所以SOP预测任务能够让模型学习到更多的信息。</p>
<p>除了减少模型参数外，还对BERT的预训练任务Next-sentence prediction (NSP)进行了改进。在BERT中，NSP任务的正例是文章中连续的两个句子，而负例则是从两篇文档中各选一个句子构造而成。在先前的研究中，已经证明NSP是并不是一个合适的预训练任务。作者推测其原因是模型在判断两个句子的关系时不仅考虑了两个句子之间的连贯性（coherence），还会考虑到两个句子的话题（topic）。而两篇文档的话题通常不同，模型会更多的通过话题去分析两个句子的关系，而不是句子间的连贯性，这使得NSP任务变成了一个相对简单的任务。</p>
<p>因此本文提出了Sentence-order prediction (SOP)来取代NSP。具体来说，其正例与NSP相同，但负例是通过选择一篇文档中的两个连续的句子并将它们的顺序交换构造的。这样两个句子就会有相同的话题，模型学习到的就更多是句子间的连贯性。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/albert_sop.png" alt></p>
<p>如上实验结果所示，如果不使用NSP或SOP作为预训练任务的话，模型在NSP和SOP两个任务上表现都很差；如果使用NSP作为预训练任务的话，模型确实能很好的解决NSP问题，但是在SOP问题上表现却很差，几乎相当于随机猜，因此说明NSP任务确实很难学到句子间的连贯性；而如果用SOP作为预训练任务，则模型也可以较好的解决NSP问题，同时模型在下游任务上表现也更好。说明SOP确实是更好的预训练任务。</p>
<h4 id="qi-ta">其他</h4>
<h5 id="e-wai-de-xun-lian-shu-ju-he-dropout">额外的训练数据和dropout</h5>
<p>ALBERT训练时还加入了XLNet和RoBERTa训练时用的额外数据，实验表明加入额外数据（W additional data）确实会提升模型表现。此外，作者还观察到模型似乎一直没有过拟合数据，因此去除了Dropout，从对比试验可以看出，去除Dropout（W/O Dropout）后模型表现确实更好。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/albert_dropout.png" alt></p>
<h4 id="zong-jie-4">总结</h4>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/albert_bert.png" alt></p>
<p>ALBERT的训练速度明显比BERT快，ALBERT-xxlarge的表现更是全方面超过了BERT。</p>
<p>本文有两个小细节可以学习，一个是在模型不会过拟合的情况下不使用dropout；另一个则是warm-start，即在训练深层网络（例如12层）时，可以先训练浅层网络（例如6层），再在其基础上做fine-tune，这样可以加快深层模型的收敛。</p>
<h3 id="tiny-bert">TinyBERT</h3>
<p><a href="https://arxiv.org/abs/1909.10351" target="_blank" rel="noopener">论文</a></p>
<h4 id="jian-jie-6">简介</h4>
<p>在 NLP 领域，BERT 由于模型过于庞大，单个样本计算一次的开销动辄上百毫秒，很难应用到实际生产中。中科技大学、华为诺亚方舟实验室的研究者提出了 TinyBERT，这是一种为基于 transformer 的模型专门设计的知识蒸馏方法，模型大小为 BERT 的 13.3%，推理速度是 BERT 的 9.4 倍，而且性能没有出现明显下降。</p>
<p>目前主流的几种蒸馏方法大概分成 1. 利用 transformer 结构蒸馏, 2. 利用其它简单的结构比如 BiLSTM 等蒸馏。由于 BiLSTM 等结构简单，且一般是用 BERT 最后一层的输出结果进行蒸馏，不能学到 transformer 中间层的信息，对于复杂的语义匹配任务，效果有点不尽人意。</p>
<p>基于 transformer 结构的蒸馏方法目前比较出名的有微软的 BERT-PKD (Patient Knowledge Distillation for BERT)，huggingface 的 DistilBERT，以及华为的 TinyBERT。他们的基本思路都是减少 transformer encoding 的层数和 hidden size 大小，实现细节上各有不同，主要差异体现在 loss 的设计上。</p>
<h4 id="mo-xing-3">模型</h4>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/tinyBert.jpg" alt></p>
<p>整个 TinyBERT 的 loss 设计分为三部分：</p>
<h5 id="embedding-layer-distillation">Embedding-layer Distillation</h5>
<p>\[
\mathcal{L}_{\mathrm{embd}}=\operatorname{MSE}\left(\boldsymbol{E}^{S} \boldsymbol{W}_{e}, \boldsymbol{E}^{T}\right)
\]<br>
其中，\(E^s \in R^{l \times ds}\),\(E^T \ in R^{l \times dt}\)分别表示 student 网络的 embedding 和 teacher 网络的 embedding. 其中 l 代表 sequence length, ds 代表 student embedding 维度， dt 代表 teacher embedding 维度。由于 student 网络的 embedding 层通常较 teacher 会变小以获得更小的模型和加速，所以 \(W_e\) 是一个 \(d_s \times d_t\)维的可训练的线性变换矩阵，把 student 的 embedding 投影到 teacher embedding 所在的空间。最后再算 MSE，得到 embedding loss.</p>
<h5 id="transformer-layer-distillation">Transformer-layer Distillation</h5>
<p>TinyBERT 的 transformer 蒸馏采用隔 k 层蒸馏的方式。举个例子，teacher BERT 一共有 12 层，若是设置 student BERT 为 4 层，就是每隔 3 层计算一个 transformer loss. 映射函数为 g(m) = 3 * m(???), m 为 student encoder 层数。具体对应为 student 第 1 层 transformer 对应 teacher 第 3 层，第 2 层对应第 6 层，第 3 层对应第 9 层，第 4 层对应第 12 层。每一层的 transformer loss 又分为两部分组成，attention based distillation 和 hidden states based distillation.</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/transormers_distillation.jpg" alt></p>
<h6 id="attention-based-loss">Attention based loss</h6>
<p>\[
\mathcal{L}_{\mathrm{attn}}=\frac{1}{h} \sum_{i=1}^{h} \operatorname{MSE}\left(\boldsymbol{A}_{i}^{S}, \boldsymbol{A}_{i}^{T}\right)
\]</p>
<p>其中，\(A_i \in R^{l \times l}\) \(h\)代表attention的头数，\(l\)代表输入长度，\(A_i^S\)代表student网络第i个attention头的attention score矩阵。\(A_i^T\)代表teacher网络的第i个attention头的attention score 矩阵。为什么要做这个损失？在What Does BERT Look At? An Analysis of BERT’s Attention [12]中研究了attention 权重到底学到了什么，实验发现与语义还有语法相关的词比如第一个动词宾语，第一个介词宾语，以及[CLS], [SEP], 逗号等 token，有很高的注意力权重。为了确保这部分信息能被 student 网络学到，TinyBERT 在 loss 设计中加上了 student 和 teacher 的 attention matrix 的 MSE。这样语言知识可以很好的从 teacher BERT 转移到 student BERT.</p>
<h6 id="hidden-states-based-distillation">hidden states based distillation</h6>
<p>\[
\mathcal{L}_{\mathrm{hidn}}=\mathrm{MSE}\left(\boldsymbol{H}^{S} \boldsymbol{W}_{h}, \boldsymbol{H}^{T}\right)
\]</p>
<p>其中，\(H^S \in R^{l \times ds}\),\(H^T \in R^{l \times dt}\)分别是 student transformer 和 teacher transformer 的隐层输出。和 embedding loss 同理，\(W_h\)把\(H^S\)投影到\(H^T\)所在的空间。</p>
<h6 id="prediction-layer-distillation">Prediction-Layer Distillation</h6>
<p>\[
\mathcal{L}_{\text {pred }}=-\operatorname{softmax}\left(\boldsymbol{z}^{T}\right) \cdot \log \_ \left(\boldsymbol{z}^{S} / t\right)
\]</p>
<p>其中 t 是 temperature value，暂时设为 1.除了模仿中间层的行为外，这一层用来模拟 teacher 网络在 predict 层的表现。具体来说，这一层计算了 teacher 输出的概率分布和 student 输出的概率分布的 softmax 交叉熵。这一层的实现和具体任务相关.</p>
<p>prediction loss 有很多变化。</p>
<ol>
<li>在 TinyBERT 中，这个 loss 是 teacher BERT 预测的概率和 student BERT 预测概率的 softmax 交叉熵.</li>
<li>在 BERT-PKD 模型中，这个 loss 是 teacher BERT 和 student BERT 的交叉熵和 student BERT 和 hard target( one-hot)的交叉熵的加权平均。</li>
</ol>
<p>直接用 hard target loss，效果比使用 teacher student softmax 交叉熵下降 5-6 个点。因为 softmax 比 one-hot 编码了更多概率分布的信息。并且softmax cross-entropy loss 容易发生不收敛的情况，把 softmax 交叉熵改成 MSE, 收敛效果变好，但泛化效果变差。这是因为使用 softmax cross-entropy 需要学到整个概率分布，更难收敛，因为拟合了 teacher BERT 的概率分布，有更强的泛化性。MSE 对极值敏感，收敛的更快，但泛化效果不如前者。</p>
<p>所以总的loss：<br>
\[
\mathcal{L}_{\text {model }}=\sum_{m=0}^{M+1} \lambda_{m} \mathcal{L}_{\text {layer }}\left(S_{m}, T_{g(m)}\right)
\]<br>
其中<br>
\[
\mathcal{L}_{\text {layer }}\left(S_{m}, T_{g(m)}\right)=\left\{\begin{array}{ll}
\mathcal{L}_{\text {embd }}\left(S_{0}, T_{0}\right), &amp; m=0 \\
\mathcal{L}_{\text {hidn }}\left(S_{m}, T_{g(m)}\right)+\mathcal{L}_{\text {attn }}\left(S_{m}, T_{g(m)}\right), &amp; M \geq m>0 \\
\mathcal{L}_{\text {pred }}\left(S_{M+1}, T_{N+1}\right), &amp; m=M+1
\end{array}\right.
\]</p>
<h4 id="liang-jie-duan-xue-xi-kuang-jia">两阶段学习框架</h4>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/two_step.jpg" alt></p>
<p>类似于原生的 BERT 先 pre-train, 根据具体任务再 fine-tine。TinyBERT 先在 general domain 数据集上用未经微调的 BERT 充当教师蒸馏出一个 base 模型，在此基础上，具体任务通过数据增强，利用微调后的 BERT 再进行重新执行蒸馏。这种两阶段的方法给 TinyBERT 提供了像 BERT 一样的泛化能力。</p>
<h4 id="zong-jie-5">总结</h4>
<p>TinyBERT 作为一种蒸馏方法，能有效的提取 BERT transformer 结构中丰富的语意信息，在不牺牲性能的情况下，速度能获得 8 到 9 倍的提升。</p>
<h3 id="fast-bert">FastBert</h3>
<h4 id="jian-jie-7">简介</h4>
<p>FastBERT的创新点很容易理解，就是在每层Transformer后都去预测样本标签，如果某样本预测结果的置信度很高，就不用继续计算了。论文把这个逻辑称为样本自适应机制（Sample-wise adaptive mechanism），就是自适应调整每个样本的计算量，容易的样本通过一两层就可以预测出来，较难的样本则需要走完全程。</p>
<p>举个例子，比如 text_a = ‘北京鲜花快递’ text_b = ‘北京鲜花速递’ 这种case可能浅层的bert就已经能够很好的算出其相关性打分，所以算完两层bert的打分和12层的结果基本一致，而text_a = ‘北京鲜花快递’ text_b = ‘有没有哪个地方卖花卉，而且包送，位置北京’ 这种case可能需要深层的bert去抽取两边的语义特征，并计算其是否匹配，所以在inference阶段就需要算完12层。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/fastbert.jpg" alt></p>
<p>作者将原BERT模型称为主干（Backbone），每个分类器称为分支（Branch）。这里的分支Classifier都是最后一层的分类器蒸馏来的，作者将这称为自蒸馏（Self-distillation）。就是在预训练和精调阶段都只更新主干参数，精调完后<strong>freeze主干参数，用分支分类器（图中的student）蒸馏主干分类器（图中的teacher）的概率分布</strong>。之所以叫自蒸馏，是因为之前的蒸馏都是用两个模型去做，一个模型学习另一个模型的知识，而FastBERT是自己（分支）蒸馏自己（主干）的知识。值得注意的是，蒸馏时需要freeze主干部分，保证pretrain和finetune阶段学习的知识不被影响，仅用brach 来尽可能的拟合teacher的分布。同时，使用自蒸馏还有一点重要的好处，就是<strong>不再依赖于标注数据</strong>。蒸馏的效果可以通过源源不断的无标签数据来提升。</p>
<h4 id="zong-jie-6">总结</h4>
<p>FastBERT通过提前输出简单样本的预测结果，减少模型的计算负担，从而提高推理速度。虽然每层都多了一个分类器，但分类器的计算量也比Transformer小了两个数量级，对速度影响较小。后续的分支自蒸馏也设计的比较巧妙，可以利用无监督数据不断提升分支分类器的效果。</p>
<h3 id="zong-jie-7">总结</h3>
<p>性能：</p>
<ol>
<li>BERT-WWM</li>
<li>XLNet（长文本表现更好一些）</li>
<li>RoBERTa（动态mask、without NSP、更大batch，更多数据，更长训练时间）</li>
<li>SpanBERT(基于分词的预训练模型)</li>
<li>MT-DNN(预训练+多任务)</li>
<li>ERNIR（引入实体信息）</li>
</ol>
<p>效率：</p>
<ol>
<li>DistilBert（知识蒸馏）</li>
<li>ALBert（因式分解、跨层权重共享）</li>
<li>TinyBert（知识蒸馏）</li>
</ol>
<h3 id="can-kao">参考</h3>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/46652512" target="_blank" rel="noopener">【NLP】Google BERT详解</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/81157740" target="_blank" rel="noopener">带你读论文丨8篇论文梳理BERT相关模型进展与反思</a></li>
<li><a href="https://blog.csdn.net/u012526436/article/details/93196139" target="_blank" rel="noopener">最通俗易懂的XLNET详解</a></li>
<li><a href="https://www.jianshu.com/p/eddf04ba8545" target="_blank" rel="noopener">改进版的RoBERTa到底改进了什么？</a></li>
<li><a href="https://www.bilibili.com/video/av73657563/" target="_blank" rel="noopener">【AI模型】最通俗易懂的XLNet详解</a></li>
<li>Liu Y, Ott M, Goyal N, et al. Roberta: A robustly optimized bert pretraining approach[J]. arXiv preprint arXiv:1907.11692, 2019</li>
<li>Sennrich R, Haddow B, Birch A. Neural machine translation of rare words with subword units[J]. arXiv preprint arXiv:1508.07909, 2015.</li>
<li><a href="http://fancyerii.github.io/2019/03/09/bert-codes/#dataprocessor" target="_blank" rel="noopener">bert代码阅读</a></li>
<li><a href="https://www.infoq.cn/article/STabowUeFupgc4gRqRQj" target="_blank" rel="noopener">更小、更快、更便宜、更轻量：开源 DistilBERT，BERT 的精简版本</a></li>
<li>Michel, P., Levy, O., &amp; Neubig, G. (2019). Are Sixteen Heads Really Better than One? Retrieved from <a href="https://arxiv.org/abs/1905.10650" target="_blank" rel="noopener">https://arxiv.org/abs/1905.10650</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/94359189" target="_blank" rel="noopener">比 Bert 体积更小速度更快的 TinyBERT</a></li>
<li><a href="https://arxiv.org/abs/1906.04341" target="_blank" rel="noopener">https://arxiv.org/abs/1906.04341</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/87562926" target="_blank" rel="noopener">【论文阅读】ALBERT</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/59436589" target="_blank" rel="noopener">中文任务全面超越BERT：百度正式发布NLP预训练模型ERNIE</a></li>
<li><a href="https://www.jianshu.com/p/5e12e6edbd59" target="_blank" rel="noopener">BERT泛读系列（三）—— ERNIE: Enhanced Language Representation with Informative Entities论文笔记</a></li>
<li><a href="https://blog.csdn.net/ljp1919/article/details/90269059" target="_blank" rel="noopener">文献阅读：MT-DNN模型</a></li>
<li><a href="https://blog.csdn.net/Magical_Bubble/article/details/89517709" target="_blank" rel="noopener">MT-DNN解读(论文 + PyTorch源码)</a></li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>BERT-WWM</tag>
        <tag>XLNet</tag>
        <tag>RoBERTa</tag>
        <tag>SpanBERT</tag>
        <tag>ernie</tag>
        <tag>MT-DNN</tag>
        <tag>DistillBERT</tag>
        <tag>ALBERT</tag>
        <tag>TinyBERT</tag>
      </tags>
  </entry>
  <entry>
    <title>文本多标签分类</title>
    <url>/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>对于事件抽取中单句多事件的问题，打算用文本多标签分类来进行解决。</p>
<h3 id="nan-dian">难点</h3>
<ol>
<li>类标数量不确定，有些样本可能只有一个类标，有些样本的类标可能高达几十甚至上百个。</li>
<li>类标之间相互依赖，例如包含蓝天类标的样本很大概率上包含白云，如何解决类标之间的依赖性问题也是一大难点。</li>
<li>多标签的训练集比较难以获取。</li>
</ol>
<a id="more"></a>
<h3 id="fang-fa">方法</h3>
<p>前有很多关于多标签的学习算法，依据解决问题的角度，这些算法可以分为两大类:一是基于问题转化的方法，二是基于算法适用的方法。基于问题转化的方法是转化问题数据，使之使用现有算法；基于算法适用的方法是指针对某一特定的算法进行扩展，从而能够处理多标记数据，改进算法，适用数据。</p>
<h4 id="strong-ji-yu-wen-ti-zhuan-hua-de-fang-fa-strong"><strong>基于问题转化的方法</strong></h4>
<p>基于问题转化的方法中有的考虑标签之间的关联性，有的不考虑标签的关联性。最简单的不考虑关联性的算法将多标签中的每一个标签当成是单标签，对每一个标签实施常见的分类算法。具体而言，在传统机器学习的模型中对每一类标签做二分类，可以使用SVM、DT、Naive Bayes、DT、Xgboost等算法；在深度学习中，对每一类训练一个文本分类模型(如：textCNN、textRNN等)。考虑多标签的相关性时候可以将上一个输出的标签当成是下一个标签分类器的输入。在传统机器学习模型中可以使用<strong>分类器链</strong>，在这种情况下，第一个分类器只在输入数据上进行训练，然后每个分类器都在输入空间和链上的所有之前的分类器上进行训练。让我们试着通过一个例子来理解这个问题。在下面给出的数据集里，我们将X作为输入空间，而Y作为标签。在分类器链中，这个问题将被转换成4个不同的标签问题，就像下面所示。黄色部分是输入空间，白色部分代表目标变量。</p>
<p><img src="/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/1.png" alt="avatar"></p>
<p>在深度学习中，于输出层加上一个时序模型，将每一时刻输入的数据序列中加入上一时刻输出的结果值。在获得文章的整体语义(Text feature vector)后，将Text feature vector输入到一个RNN的序列中作为初始值，每一时刻输入是上一时刻的输出。从某种程度上来说，下图所示的模型将多标签任务当成了序列生成任务来处理。</p>
<p><img src="/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/cnn_rnn.png" alt="avatar"></p>
<p>除了将标签分开看之外，还有将标签统一来看(Label Powerset)。在这方面，我们将问题转化为一个多类问题，一个多类分类器在训练数据中发现的所有唯一的标签组合上被训练。让我们通过一个例子来理解它。</p>
<p><img src="/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/multi_one.png" alt="avatar"></p>
<p>在这一点上，我们发现x1和x4有相同的标签。同样的，x3和x6有相同的标签。因此，标签powerset将这个问题转换为一个单一的多类问题，如下所示。</p>
<p><img src="/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/multi_two.png" alt="avatar"></p>
<p>因此，标签powerset给训练集中的每一个可能的标签组合提供了一个独特的类。转化为单标签后就可以使用SVM、textCNN、textRNN等分类算法训练模型了。</p>
<p>感觉Label Powerset只适合标签数少的数据，一旦标签数目太多(假设有n个)，使用Label Powerset后可能的数据集将分布在[0,\(2^{n-1}\)]空间内，数据会很稀疏。</p>
<h4 id="strong-ji-yu-suan-fa-gua-yong-de-fang-fa-strong"><strong>基于算法适用的方法</strong></h4>
<p>改变算法来直接执行多标签分类，而不是将问题转化为不同的问题子集。在传统机器学习模型中适用于的多标签分类模型有:kNN多标签版本MLkNN，SVM的多标签版本Rank-SVM等。在深度学习中常常是修改多分类模型的输出层，使其适用于多标签的分类，如：在输出层对每一个标签的输出值使用sigmod函数进行2分类(标签之间无关联信息)。或者使用一般的CNN提取句子的语义信息，但考虑到句子表示的不同部分在句子分类的过程中会起不同的作用，故在进行分类时候使用了Attention机制(见图)，使得在预测每一类时候句子的不同部分的表示起不同的作用。</p>
<p><img src="/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/3.png" alt="avatar"></p>
<h3 id="zi-yuan">资源</h3>
<ol>
<li><a href="https://ai.baidu.com/tech/nlp/doctagger" target="_blank" rel="noopener">百度AI开放平台(文章标签)</a></li>
<li><a href="https://github.com/inspirehep/magpie" target="_blank" rel="noopener">Magpie(多标签分类工具)</a></li>
<li><a href="https://biendata.com/competition/zhihu/" target="_blank" rel="noopener">2017知乎“看山杯”比赛</a></li>
<li>[英文新闻多 标签分类数据集](<a href="https://drive.google.com/file/d/18-JOCIj9v5bZCrn9CIsk23W4wyhroCp_/view?usp=sharing" target="_blank" rel="noopener">https://drive.google.com/file/d/18-JOCIj9v5bZCrn9CIsk23W4wyhroCp_/view?usp=sharing</a></li>
<li>[网上可采集的、质量较高的多标签分类数据集(<a href="https://movie.douban.com/" target="_blank" rel="noopener">豆瓣电影评论</a>)</li>
<li>GitHub上比较完善的<a href="https://github.com/brightmart/text_classification" target="_blank" rel="noopener">文本多标签分类项目</a></li>
<li>NLPCC2018，task6和tesk8均可以当成是多标签的分类任务；</li>
</ol>
<h3 id="can-kao">参考</h3>
<ol>
<li><a href="https://www.cnblogs.com/cxf-zzj/p/10049613.html" target="_blank" rel="noopener">多标签分类(multi-label classification)综述</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>序列标注</title>
    <url>/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>后面需要补充序列标注在分词、词性标注、命名实体识别等相关任务的代码实践(走过的坑。。)及相关理论知识。</p>
<h3 id="tiao-jian-sui-ji-chang-crf">条件随机场(CRF)</h3>
<h4 id="crf-yong-yu-xu-lie-biao-zhu-wen-ti">crf 用于序列标注问题</h4>
<p>在 CRF 的序列标注问题中，要计算的是条件概率：<br>
\[
P\left(y_{1}, \ldots, y_{n} | x_{1}, \ldots, x_{n}\right)=P\left(y_{1}, \ldots, y_{n} | \boldsymbol{x}\right), \quad \boldsymbol{x}=\left(x_{1}, \ldots, x_{n}\right) \tag{1}
\]</p>
<a id="more"></a>
<p>为了得到这个概率的估计，CRF 做了两个假设：</p>
<h5 id="strong-jia-she-yi-gai-fen-bu-shi-zhi-shu-zu-fen-bu-strong"><strong>假设一：该分布是指数族分布。</strong></h5>
<p>这个假设意味着存在函数 \(f(y_1,…,y_n;x)\)，使得：<br>
\[
P\left(y_{1}, \ldots, y_{n} | \boldsymbol{x}\right)=\frac{1}{Z(\boldsymbol{x})} \exp \left(f\left(y_{1}, \ldots, y_{n} ; \boldsymbol{x}\right)\right) \tag{2}
\]<br>
其中 Z(x) 是归一化因子，因为这个是条件分布，所以归一化因子跟 x 有关。这个 f 函数可以视为一个打分函数，打分函数取指数并归一化后就得到概率分布。</p>
<h5 id="strong-jia-she-er-shu-chu-zhi-jian-de-guan-lian-jin-fa-sheng-zai-xiang-lin-wei-zhi-bing-qie-guan-lian-shi-zhi-shu-jia-xing-de-strong"><strong>假设二：输出之间的关联仅发生在相邻位置，并且关联是指数加性的。</strong></h5>
<p>这个假设意味着 f(y1,…,yn;x) 可以更进一步简化为：<br>
\[
\begin{aligned}
f\left(y_{1}, \ldots, y_{n} ; \boldsymbol{x}\right)=h\left(y_{1} ; \boldsymbol{x}\right) &amp;+g\left(y_{1}, y_{2} ; \boldsymbol{x}\right)+h\left(y_{2} ; \boldsymbol{x}\right)+\ldots \\
&amp;+g\left(y_{n-1}, y_{n} ; \boldsymbol{x}\right)+h\left(y_{n} ; \boldsymbol{x}\right)
\end{aligned} \tag{3}
\]<br>
这也就是说，现在我们只需要对每一个标签和每一个相邻标签对分别打分，然后将所有打分结果求和得到总分。</p>
<h5 id="xian-xing-lian-crf">线性链crf</h5>
<p>尽管已经做了大量简化，但一般来说，(3)式所表示的概率模型还是过于复杂，难以求解。<strong>于是考虑到当前深度学习模型中，RNN 或者层叠 CNN 等模型已经能够比较充分捕捉各个 y 与输出 x 的联系</strong>，因此，不妨考虑函数 g 跟 x 无关，那么：<br>
\[
\begin{aligned}
f\left(y_{1}, \ldots, y_{n} ; \boldsymbol{x}\right)=h\left(y_{1} ; \boldsymbol{x}\right) &amp;+g\left(y_{1}, y_{2}\right)+h\left(y_{2} ; \boldsymbol{x}\right)+\ldots \\
&amp;+g\left(y_{n-1}, y_{n}\right)+h\left(y_{n} ; \boldsymbol{x}\right)
\end{aligned}
\]<br>
这时候 g 实际上就是一个有限的、待训练的参数矩阵而已，而单标签的打分函数 \(h(y_i;x\)) 可以通过 RNN 或者 CNN 来建模。因此，该模型是可以建立的，其中概率分布变为：<br>
\[
P\left(y_{1}, \ldots, y_{n} | x\right)=\frac{1}{Z(x)} \exp \left(h\left(y_{1} ; x\right)+\sum_{k=1}^{n-1} g\left(y_{k}, y_{k+1}\right)+h\left(y_{k+1} ; x\right)\right)
\]</p>
<h5 id="gui-yi-hua-yin-zi">归一化因子</h5>
<p>为了训练 CRF 模型，用最大似然方法，也就是用：\(-\log P\left(y_{1}, \ldots, y_{n} | x\right)\)，作为损失函数，可以算出它等于：<br>
\[
-\left(h\left(y_{1} ; \boldsymbol{x}\right)+\sum_{k=1}^{n-1} g\left(y_{k}, y_{k+1}\right)+h\left(y_{k+1} ; \boldsymbol{x}\right)\right)+\log Z(\boldsymbol{x})
\]<br>
其中第一项是原来概率式的<strong>分子</strong>的对数，它是对目标的序列的打分，虽然它看上去挺迂回的，但是并不难计算。真正的难度在于<strong>分母</strong>的对数 \(logZ(x)\) 这一项。</p>
<p>归一化因子，在物理上也叫配分函数，在这里它需要我们对所有可能的路径的打分进行指数求和，而我们前面已经说到，这样的路径数是指数量级的（k^n），因此直接来算几乎是不可能的。</p>
<p>事实上，<strong>归一化因子难算，几乎是所有概率图模型的公共难题</strong>。幸运的是，在 CRF 模型中，由于我们只考虑了临近标签的联系（马尔可夫假设），因此我们可以递归地算出归一化因子，这使得原来是指数级的计算量降低为线性级别。</p>
<p>具体来说，我们将计算到时刻 \(t\) 的归一化因子记为\(Z_t\)，并将它分为 \(k\) 个部分：<br>
\[
Z_{t}=Z_{t}^{(1)}+Z_{t}^{(2)}+\cdots+Z_{t}^{(k)}
\]<br>
分别是截止到当前时刻 \(t\) 中、以标签 \(1,…,k\) 为终点的所有路径的得分指数和。那么，我们可以递归地计算：</p>
<p>\[
\begin{aligned}
Z_{t+1}^{(1)} &amp;=\left(Z_{t}^{(1)} G_{11}+Z_{t}^{(2)} G_{21}+\cdots+Z_{t}^{(k)} G_{k 1}\right) h_{t+1}(1 | x) \\
Z_{t+1}^{(2)} &amp;=\left(Z_{t}^{(1)} G_{12}+Z_{t}^{(2)} G_{22}+\cdots+Z_{t}^{(k)} G_{k 2}\right) h_{t+1}(2 | x) \\
&amp; \vdots \\
Z_{t+1}^{(k)} &amp;=\left(Z_{t}^{(1)} G_{1 k}+Z_{t}^{(2)} G_{2 k}+\cdots+Z_{t}^{(k)} G_{k k}\right) h_{t+1}(k | x)
\end{aligned}
\]<br>
它可以简单写为矩阵形式：<br>
\[
\mathbf{Z}_{t+1}=\mathbf{Z}_{t} \boldsymbol{G} \otimes H\left(y_{t+1} | \boldsymbol{x}\right) 
\]<br>
其中,\(Z_{t}=\left[Z_{t}^{(1)}, \ldots, Z_{t}^{(k)}\right]\),而 G 是对 g(yi,yj) (状态转移)各个元素取指数后的矩阵，即\(G=e^{g(y i, y j)}\),而\(H(y_{t+1}|x)\),是编码模型\(h(y_{t+1}|x)\),（RNN、CNN等）对位置 t+1 的各个标签的打分的指数，即\(H\left(y_{t+1} | x\right)=e^{h\left(y_{t+1} | x\right)}\)，也是一个向量。\(Z_tG\) 这一步是矩阵乘法，得到一个向量，而 \(⊗\) 是两个向量的逐位对应相乘。</p>
<p><img src="/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/decode-crf.png" alt="avatar"></p>
<p>从t到t+1时刻的计算，包括转移概率和j+1节点本身的概率</p>
<h5 id="dong-tai-gui-hua">动态规划</h5>
<p>写出损失函数 \(-logP(y_1,…,y_n|x)\) 后，就可以完成模型的训练了，因为目前的深度学习框架都已经带有自动求导的功能，只要我们能写出可导的 loss，就可以帮我们完成优化过程了。</p>
<p>那么剩下的最后一步，就是模型训练完成后，如何根据输入找出最优路径来。跟前面一样，这也是一个从 \(k^n\) 条路径中选最优的问题，而同样地，因为马尔可夫假设的存在，它可以转化为一个动态规划问题，用 viterbi 算法解决，计算量正比于 n。</p>
<p>动态规划在本博客已经出现了多次了，<strong>它的递归思想就是：一条最优路径切成两段，那么每一段都是一条（局部）最优路径</strong>。</p>
<h5 id="dai-ma-shi-xian">代码实现</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List, Optional</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Conditional random field.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This module implements a conditional random field [LMP01]_. The forward computation</span></span><br><span class="line"><span class="string">    of this class computes the log likelihood of the given sequence of tags and</span></span><br><span class="line"><span class="string">    emission score tensor. This class also has `~CRF.decode` method which finds</span></span><br><span class="line"><span class="string">    the best tag sequence given an emission score tensor using `Viterbi algorithm`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_tags: Number of tags.</span></span><br><span class="line"><span class="string">        batch_first: Whether the first dimension corresponds to the size of a minibatch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        start_transitions (`~torch.nn.Parameter`): Start transition score tensor of size</span></span><br><span class="line"><span class="string">            ``(num_tags,)``.</span></span><br><span class="line"><span class="string">        end_transitions (`~torch.nn.Parameter`): End transition score tensor of size</span></span><br><span class="line"><span class="string">            ``(num_tags,)``.</span></span><br><span class="line"><span class="string">        transitions (`~torch.nn.Parameter`): Transition score tensor of size</span></span><br><span class="line"><span class="string">            ``(num_tags, num_tags)``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. [LMP01] Lafferty, J., McCallum, A., Pereira, F. (2001).</span></span><br><span class="line"><span class="string">       "Conditional random fields: Probabilistic models for segmenting and</span></span><br><span class="line"><span class="string">       labeling sequence data". *Proc. 18th International Conf. on Machine</span></span><br><span class="line"><span class="string">       Learning*. Morgan Kaufmann. pp. 282–289.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_tags: int, batch_first: bool = False)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="keyword">if</span> num_tags &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f'invalid number of tags: <span class="subst">&#123;num_tags&#125;</span>'</span>)</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.num_tags = num_tags</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.start_transitions = nn.Parameter(torch.empty(num_tags))</span><br><span class="line">        self.end_transitions = nn.Parameter(torch.empty(num_tags))</span><br><span class="line">        self.transitions = nn.Parameter(torch.empty(num_tags, num_tags))</span><br><span class="line"></span><br><span class="line">        self.reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span><span class="params">(self)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""Initialize the transition parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The parameters will be initialized randomly from a uniform distribution</span></span><br><span class="line"><span class="string">        between -0.1 and 0.1.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        nn.init.uniform_(self.start_transitions, <span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">        nn.init.uniform_(self.end_transitions, <span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">        nn.init.uniform_(self.transitions, <span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span> -&gt; str:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f'<span class="subst">&#123;self.__class__.__name__&#125;</span>(num_tags=<span class="subst">&#123;self.num_tags&#125;</span>)'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            self,</span></span></span><br><span class="line"><span class="function"><span class="params">            emissions: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            tags: torch.LongTensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            mask: Optional[torch.ByteTensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">            reduction: str = <span class="string">'sum'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">"""Compute the conditional log likelihood of a sequence of tags given emission scores.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            emissions (`~torch.Tensor`): Emission score tensor of size</span></span><br><span class="line"><span class="string">                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,</span></span><br><span class="line"><span class="string">                ``(batch_size, seq_length, num_tags)`` otherwise.</span></span><br><span class="line"><span class="string">            tags (`~torch.LongTensor`): Sequence of tags tensor of size</span></span><br><span class="line"><span class="string">                ``(seq_length, batch_size)`` if ``batch_first`` is ``False``,</span></span><br><span class="line"><span class="string">                ``(batch_size, seq_length)`` otherwise.</span></span><br><span class="line"><span class="string">            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``</span></span><br><span class="line"><span class="string">                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.</span></span><br><span class="line"><span class="string">            reduction: Specifies  the reduction to apply to the output:</span></span><br><span class="line"><span class="string">                ``none|sum|mean|token_mean``. ``none``: no reduction will be applied.</span></span><br><span class="line"><span class="string">                ``sum``: the output will be summed over batches. ``mean``: the output will be</span></span><br><span class="line"><span class="string">                averaged over batches. ``token_mean``: the output will be averaged over tokens.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            `~torch.Tensor`: The log likelihood. This will have size ``(batch_size,)`` if</span></span><br><span class="line"><span class="string">            reduction is ``none``, ``()`` otherwise.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self._validate(emissions, tags=tags, mask=mask)</span><br><span class="line">        <span class="keyword">if</span> reduction <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">'none'</span>, <span class="string">'sum'</span>, <span class="string">'mean'</span>, <span class="string">'token_mean'</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f'invalid reduction: <span class="subst">&#123;reduction&#125;</span>'</span>)</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            mask = torch.ones_like(tags, dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            emissions = emissions.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            tags = tags.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            mask = mask.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        numerator = self._compute_score(emissions, tags, mask)</span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        denominator = self._compute_normalizer(emissions, mask)</span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        llh = numerator - denominator</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> reduction == <span class="string">'none'</span>:</span><br><span class="line">            <span class="keyword">return</span> llh</span><br><span class="line">        <span class="keyword">if</span> reduction == <span class="string">'sum'</span>:</span><br><span class="line">            <span class="keyword">return</span> llh.sum()</span><br><span class="line">        <span class="keyword">if</span> reduction == <span class="string">'mean'</span>:</span><br><span class="line">            <span class="keyword">return</span> llh.mean()</span><br><span class="line">        <span class="keyword">assert</span> reduction == <span class="string">'token_mean'</span></span><br><span class="line">        <span class="keyword">return</span> llh.sum() / mask.type_as(emissions).sum()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, emissions: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">               mask: Optional[torch.ByteTensor] = None)</span> -&gt; List[List[int]]:</span></span><br><span class="line">        <span class="string">"""Find the most likely tag sequence using Viterbi algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            emissions (`~torch.Tensor`): Emission score tensor of size</span></span><br><span class="line"><span class="string">                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,</span></span><br><span class="line"><span class="string">                ``(batch_size, seq_length, num_tags)`` otherwise.</span></span><br><span class="line"><span class="string">            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``</span></span><br><span class="line"><span class="string">                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            List of list containing the best tag sequence for each batch.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self._validate(emissions, mask=mask)</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            mask = emissions.new_ones(emissions.shape[:<span class="number">2</span>], dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            emissions = emissions.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            mask = mask.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self._viterbi_decode(emissions, mask)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_validate</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            self,</span></span></span><br><span class="line"><span class="function"><span class="params">            emissions: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            tags: Optional[torch.LongTensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">            mask: Optional[torch.ByteTensor] = None)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="keyword">if</span> emissions.dim() != <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f'emissions must have dimension of 3, got <span class="subst">&#123;emissions.dim()&#125;</span>'</span>)</span><br><span class="line">        <span class="keyword">if</span> emissions.size(<span class="number">2</span>) != self.num_tags:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f'expected last dimension of emissions is <span class="subst">&#123;self.num_tags&#125;</span>, '</span></span><br><span class="line">                <span class="string">f'got <span class="subst">&#123;emissions.size(<span class="number">2</span>)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> tags <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> emissions.shape[:<span class="number">2</span>] != tags.shape:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">'the first two dimensions of emissions and tags must match, '</span></span><br><span class="line">                    <span class="string">f'got <span class="subst">&#123;tuple(emissions.shape[:<span class="number">2</span>])&#125;</span> and <span class="subst">&#123;tuple(tags.shape)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> emissions.shape[:<span class="number">2</span>] != mask.shape:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">'the first two dimensions of emissions and mask must match, '</span></span><br><span class="line">                    <span class="string">f'got <span class="subst">&#123;tuple(emissions.shape[:<span class="number">2</span>])&#125;</span> and <span class="subst">&#123;tuple(mask.shape)&#125;</span>'</span>)</span><br><span class="line">            no_empty_seq = <span class="keyword">not</span> self.batch_first <span class="keyword">and</span> mask[<span class="number">0</span>].all()</span><br><span class="line">            no_empty_seq_bf = self.batch_first <span class="keyword">and</span> mask[:, <span class="number">0</span>].all()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> no_empty_seq <span class="keyword">and</span> <span class="keyword">not</span> no_empty_seq_bf:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">'mask of the first timestep must all be on'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_compute_score</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            self, emissions: torch.Tensor, tags: torch.LongTensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            mask: torch.ByteTensor)</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="comment"># emissions: (seq_length, batch_size, num_tags)</span></span><br><span class="line">        <span class="comment"># tags: (seq_length, batch_size)</span></span><br><span class="line">        <span class="comment"># mask: (seq_length, batch_size)</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.dim() == <span class="number">3</span> <span class="keyword">and</span> tags.dim() == <span class="number">2</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.shape[:<span class="number">2</span>] == tags.shape</span><br><span class="line">        <span class="keyword">assert</span> emissions.size(<span class="number">2</span>) == self.num_tags</span><br><span class="line">        <span class="keyword">assert</span> mask.shape == tags.shape</span><br><span class="line">        <span class="keyword">assert</span> mask[<span class="number">0</span>].all()</span><br><span class="line"></span><br><span class="line">        seq_length, batch_size = tags.shape</span><br><span class="line">        mask = mask.type_as(emissions)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Start transition score and first emission</span></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        score = self.start_transitions[tags[<span class="number">0</span>]]</span><br><span class="line">        score += emissions[<span class="number">0</span>, torch.arange(batch_size), tags[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, seq_length):</span><br><span class="line">            <span class="comment"># Transition score to next tag, only added if next timestep is valid (mask == 1)</span></span><br><span class="line">            <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">            score += self.transitions[tags[i - <span class="number">1</span>], tags[i]] * mask[i]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Emission score for next tag, only added if next timestep is valid (mask == 1)</span></span><br><span class="line">            <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># End transition score</span></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        seq_ends = mask.long().sum(dim=<span class="number">0</span>) - <span class="number">1</span></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        last_tags = tags[seq_ends, torch.arange(batch_size)]</span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        score += self.end_transitions[last_tags]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_compute_normalizer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            self, emissions: torch.Tensor, mask: torch.ByteTensor)</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="comment"># emissions: (seq_length, batch_size, num_tags)</span></span><br><span class="line">        <span class="comment"># mask: (seq_length, batch_size)</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.dim() == <span class="number">3</span> <span class="keyword">and</span> mask.dim() == <span class="number">2</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.shape[:<span class="number">2</span>] == mask.shape</span><br><span class="line">        <span class="keyword">assert</span> emissions.size(<span class="number">2</span>) == self.num_tags</span><br><span class="line">        <span class="keyword">assert</span> mask[<span class="number">0</span>].all()</span><br><span class="line"></span><br><span class="line">        seq_length = emissions.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Start transition score and first emission; score has size of</span></span><br><span class="line">        <span class="comment"># (batch_size, num_tags) where for each batch, the j-th column stores</span></span><br><span class="line">        <span class="comment"># the score that the first timestep has tag j</span></span><br><span class="line">        <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">        score = self.start_transitions + emissions[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, seq_length):</span><br><span class="line">            <span class="comment"># Broadcast score for every possible next tag</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags, 1)</span></span><br><span class="line">            broadcast_score = score.unsqueeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Broadcast emission score for every possible current tag</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, 1, num_tags)</span></span><br><span class="line">            broadcast_emissions = emissions[i].unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute the score tensor of size (batch_size, num_tags, num_tags) where</span></span><br><span class="line">            <span class="comment"># for each sample, entry at row i and column j stores the sum of scores of all</span></span><br><span class="line">            <span class="comment"># possible tag sequences so far that end with transitioning from tag i to tag j</span></span><br><span class="line">            <span class="comment"># and emitting</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags, num_tags)</span></span><br><span class="line">            next_score = broadcast_score + self.transitions + broadcast_emissions</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Sum over all possible current tags, but we're in score space, so a sum</span></span><br><span class="line">            <span class="comment"># becomes a log-sum-exp: for each sample, entry i stores the sum of scores of</span></span><br><span class="line">            <span class="comment"># all possible tag sequences so far, that end in tag i</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">            next_score = torch.logsumexp(next_score, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Set score to the next score if this timestep is valid (mask == 1)</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">            score = torch.where(mask[i].unsqueeze(<span class="number">1</span>), next_score, score)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># End transition score</span></span><br><span class="line">        <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">        score += self.end_transitions</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Sum (log-sum-exp) over all possible tags</span></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        <span class="keyword">return</span> torch.logsumexp(score, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_viterbi_decode</span><span class="params">(self, emissions: torch.FloatTensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                        mask: torch.ByteTensor)</span> -&gt; List[List[int]]:</span></span><br><span class="line">        <span class="comment"># emissions: (seq_length, batch_size, num_tags)</span></span><br><span class="line">        <span class="comment"># mask: (seq_length, batch_size)</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.dim() == <span class="number">3</span> <span class="keyword">and</span> mask.dim() == <span class="number">2</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.shape[:<span class="number">2</span>] == mask.shape</span><br><span class="line">        <span class="keyword">assert</span> emissions.size(<span class="number">2</span>) == self.num_tags</span><br><span class="line">        <span class="keyword">assert</span> mask[<span class="number">0</span>].all()</span><br><span class="line"></span><br><span class="line">        seq_length, batch_size = mask.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Start transition and first emission</span></span><br><span class="line">        <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">        score = self.start_transitions + emissions[<span class="number">0</span>]</span><br><span class="line">        history = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># score is a tensor of size (batch_size, num_tags) where for every batch,</span></span><br><span class="line">        <span class="comment"># value at column j stores the score of the best tag sequence so far that ends</span></span><br><span class="line">        <span class="comment"># with tag j</span></span><br><span class="line">        <span class="comment"># history saves where the best tags candidate transitioned from; this is used</span></span><br><span class="line">        <span class="comment"># when we trace back the best tag sequence</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Viterbi algorithm recursive case: we compute the score of the best tag sequence</span></span><br><span class="line">        <span class="comment"># for every possible next tag</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, seq_length):</span><br><span class="line">            <span class="comment"># Broadcast viterbi score for every possible next tag</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags, 1)</span></span><br><span class="line">            broadcast_score = score.unsqueeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Broadcast emission score for every possible current tag</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, 1, num_tags)</span></span><br><span class="line">            broadcast_emission = emissions[i].unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute the score tensor of size (batch_size, num_tags, num_tags) where</span></span><br><span class="line">            <span class="comment"># for each sample, entry at row i and column j stores the score of the best</span></span><br><span class="line">            <span class="comment"># tag sequence so far that ends with transitioning from tag i to tag j and emitting</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags, num_tags)</span></span><br><span class="line">            next_score = broadcast_score + self.transitions + broadcast_emission</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Find the maximum score over all possible current tag</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">            next_score, indices = next_score.max(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Set score to the next score if this timestep is valid (mask == 1)</span></span><br><span class="line">            <span class="comment"># and save the index that produces the next score</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">            score = torch.where(mask[i].unsqueeze(<span class="number">1</span>), next_score, score)</span><br><span class="line">            history.append(indices)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># End transition score</span></span><br><span class="line">        <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">        score += self.end_transitions</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Now, compute the best path for each sample</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        seq_ends = mask.long().sum(dim=<span class="number">0</span>) - <span class="number">1</span></span><br><span class="line">        best_tags_list = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(batch_size):</span><br><span class="line">            <span class="comment"># Find the tag which maximizes the score at the last timestep; this is our best tag</span></span><br><span class="line">            <span class="comment"># for the last timestep</span></span><br><span class="line">            _, best_last_tag = score[idx].max(dim=<span class="number">0</span>)</span><br><span class="line">            best_tags = [best_last_tag.item()]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># We trace back where the best last tag comes from, append that to our best tag</span></span><br><span class="line">            <span class="comment"># sequence, and trace it back again, and so on</span></span><br><span class="line">            <span class="keyword">for</span> hist <span class="keyword">in</span> reversed(history[:seq_ends[idx]]):</span><br><span class="line">                best_last_tag = hist[idx][best_tags[<span class="number">-1</span>]]</span><br><span class="line">                best_tags.append(best_last_tag.item())</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Reverse the order because we start from the last timestep</span></span><br><span class="line">            best_tags.reverse()</span><br><span class="line">            best_tags_list.append(best_tags)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> best_tags_list</span><br></pre></td></tr></table></figure>
<h4 id="zhu-zheng-softmax-he-crf-de-yi-tong">逐帧 softmax 和 CRF 的异同?</h4>
<p><strong>逐帧softmax</strong></p>
<p>CRF 主要用于序列标注问题，可以简单理解为是<strong>给序列中的每一帧都进行分类</strong>，既然是分类，很自然想到将这个序列用 CNN 或者 RNN 进行编码后，接一个全连接层用 softmax 激活，如下图所示：</p>
<p><img src="/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/encoder.png" alt="avatar"></p>
<p>然而，<strong>逐帧softmax并没有直接考虑输出的上下文关联</strong>。比如：当我们设计标签时，比如用 s、b、m、e 的 4 个标签来做字标注法的分词，目标输出序列本身会带有一些上下文关联，如 s 后面就不能接 m 和 e，等等。逐标签 softmax 并没有考虑这种输出层面的上下文关联，所以它意味着把这些关联放到了编码层面，希望模型能自己学到这些内容，但有时候会“强模型所难”。</p>
<p>而 CRF 则更直接一点，它<strong>将输出层面的关联分离了出来</strong>，这使得模型在学习上更为“从容”,CRF在输出端显式地考虑了上下文关联.</p>
<p><img src="/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/encoder-crf.png" alt="avatar"></p>
<p>如果仅仅是引入输出的关联，还不仅仅是 CRF 的全部，CRF 的真正精巧的地方，是它以路径为单位，考虑的是路径的概率。</p>
<p>假如一个输入有 n 帧，每一帧的标签有 k 中可能性，那么理论上就有\(k^n\)中不同的输入。我们将它用如下的网络图进行简单的可视化。在下图中，每个点代表一个标签的可能性，点之间的连线表示标签之间的关联，而每一种标注结果，都对应着图上的一条完整的路径。</p>
<p><img src="/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/crf-v.png" alt="avatar"></p>
<p>而在序列标注任务中，我们的正确答案是一般是唯一的。比如“今天天气不错”，如果对应的分词结果是“今天/天气/不/错”，那么目标输出序列就是 bebess，除此之外别的路径都不符合要求。</p>
<p>换言之，在序列标注任务中，我们的研究的基本单位应该是路径，我们要做的事情，是从 \(k^n\) 条路径选出正确的一条，那就意味着，如果将它视为一个分类问题，那么将是 \(k^n\) 类中选一类的分类问题。</p>
<p>这就是逐帧 softmax 和 CRF 的根本不同了：<strong>前者将序列标注看成是 n 个 k 分类问题，后者将序列标注看成是 1 个$ k^n$ 分类问题</strong>。</p>
<h4 id="crf">CRF++</h4>
<h5 id="jian-jie">简介</h5>
<p>CRF++ 是C++ 实现的CRF 工具。</p>
<p><a href="https://taku910.github.io/crfpp/" target="_blank" rel="noopener">文档</a></p>
<h5 id="an-zhuang">安装</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/taku910/crfpp.git</span><br><span class="line">cd crfpp</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>如果make的时候发生找不到winmain.h的错误: 可以下面这种方式修复：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sed -i <span class="string">'/#include "winmain.h"/d'</span> crf_test.cpp</span><br><span class="line">sed -i <span class="string">'/#include "winmain.h"/d'</span> crf_learn.cpp</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>后面需要用到Python 使用训练好的模型所以也一起安装CRFPP, cd python 目录下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cd python</span><br><span class="line">python setup.py build</span><br><span class="line">sudo python setup.py install</span><br></pre></td></tr></table></figure>
<p>然后在Python 或者Ipython 里输入 <code>import CRFPP</code> 如果发生如下错误</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ImportError: libcrfpp.so<span class="number">.0</span>: cannot open shared object file: No such file <span class="keyword">or</span> directory</span><br></pre></td></tr></table></figure>
<p>可用下面的方法解决</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/ld.so.conf</span><br><span class="line"><span class="comment"># 添加</span></span><br><span class="line">include /usr/local/lib</span><br><span class="line"><span class="comment"># 保存后加载一下</span></span><br><span class="line">sudo /sbin/ldconfig -v</span><br></pre></td></tr></table></figure>
<h5 id="xun-lian-shu-ju-ge-shi">训练数据格式</h5>
<p>对于训练数据，首先需要多列，但不能不一致，既在一个文件里有的行是两列，有的行是三列（这种格式是错误的）；其次第一列代表的是需要标注的<strong>字或词</strong>，最后一列是输出位标记tag，\如果有额外的特征，例如词性什么的，可以加到中间列里，所以训练集或者测试集的文件最少要有两列。以分词举例，标签为BMES</p>
<p>东	N	B<br>
南	N	M<br>
大	N	M<br>
学	N	E<br>
欢	V	B<br>
迎	V	E<br>
您	N	S</p>
<p>其中第3列是标签，也是测试文件中需要预测的结果，有BMES 4种状态。第二列是词性，不是必须的。</p>
<h5 id="te-zheng-mo-ban">特征模板</h5>
<p>每一行模板生成一组状态特征函数，数量是\(L*N\) 个，\(L\)是标签状态数是4个。\(N\)是此行模板在训练集上展开后的唯一样本数，在这个例子中，第一列的唯一字数是7个，所以有\(L*N = 4*7=28\)。例如：U01:%x[0,0]，生成如下28个函数：</p>
<figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line">func1 = <span class="keyword">if</span> (output = B and feature=U01:<span class="string">"东"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func2 = <span class="keyword">if</span> (output = M and feature=U01:<span class="string">"东"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func3 = <span class="keyword">if</span> (output = E and feature=U01:<span class="string">"东"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func4 = <span class="keyword">if</span> (output = S and feature=U01:<span class="string">"东"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func5 = <span class="keyword">if</span> (output = B and feature=U01:<span class="string">"南"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">...</span><br><span class="line">func25 = <span class="keyword">if</span> (output = B and feature=U01:<span class="string">"你"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func26 = <span class="keyword">if</span> (output = M and feature=U01:<span class="string">"你"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func27 = <span class="keyword">if</span> (output = E and feature=U01:<span class="string">"你"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func28 = <span class="keyword">if</span> (output = S and feature=U01:<span class="string">"你"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>这些函数经过训练后，其权值表示函数内文字对应该标签的概率（形象说法，概率和可大于1）。</p>
<h6 id="unigram">unigram</h6>
<p>U00:%x[-2,0]<br>
U01:%x[-1,0]<br>
U02:%x[0,0]<br>
U03:%x[1,0]<br>
U04:%x[2,0]<br>
U05:%x[-2,0]/%x[-1,0]/%x[0,0]<br>
U06:%x[-1,0]/%x[0,0]/%x[1,0]<br>
U07:%x[0,0]/%x[1,0]/%x[2,0]<br>
U08:%x[-1,0]/%x[0,0]<br>
U09:%x[0,0]/%x[1,0]</p>
<p>这是CRF<ins>例子中给出的模板，一共有９个模板，先看第一个模板，表示当前词和前面的第二个词组成的特征，以‘小明今天穿了一件红色上衣’为例，符合CRF</ins>处理格式的这句话应该变成如下形式：<br>
小　Ｂ<br>
明　Ｉ<br>
今　Ｂ   &lt;-- current word<br>
天　Ｉ<br>
穿　Ｓ<br>
了　Ｓ<br>
一　Ｂ<br>
件　Ｉ<br>
红　Ｂ<br>
色　Ｉ<br>
上　Ｂ<br>
衣　Ｉ<br>
假设我们有三个标记tag，Ｂ（表示一个词的开头那个字），Ｉ（表示一个词的结尾那个字），Ｓ（表示单个字的词），先看第一个模板Ｕ00:%x[-2,0],第一个模板产生的特征如下： 如果当前词是‘今’，那-2位置对应的字就是‘小’， 每个特征对应的字如下：</p>
<table>
<thead>
<tr>
<th>特征模板</th>
<th>意义</th>
<th>代表特征</th>
</tr>
</thead>
<tbody>
<tr>
<td>U00:%x[-2,0]</td>
<td>-2行，0列</td>
<td>小</td>
</tr>
<tr>
<td>U01:%x[-1,0]</td>
<td>-1行，0列</td>
<td>明</td>
</tr>
<tr>
<td>U02:%x[0,0]</td>
<td>0行，0列</td>
<td>今</td>
</tr>
<tr>
<td>U03:%x[1,0]</td>
<td>1行，0列</td>
<td>天</td>
</tr>
<tr>
<td>U04:%x[2,0]</td>
<td>2行，0列</td>
<td>穿</td>
</tr>
<tr>
<td>U05:%x[-2,0]/%x[-1,0]/%x[0,0]</td>
<td>-2行0列、-1行0列与0行0列的组合</td>
<td>小/明/今</td>
</tr>
<tr>
<td>U06:%x[-1,0]/%x[0,0]/%x[1,0]</td>
<td>-1行0列、0行0列与1行0列的组合</td>
<td>明/今/天</td>
</tr>
<tr>
<td>U07:%x[0,0]/%x[1,0]/%x[2,0]</td>
<td>0行0列、1行0列与2行0列的组合</td>
<td>今/天/穿</td>
</tr>
<tr>
<td>U08:%x[-1,0]/%x[0,0]</td>
<td>-1行0列与0行0列的组合</td>
<td>明/今</td>
</tr>
<tr>
<td>U09:%x[0,0]/%x[1,0]</td>
<td>0行0列与1行0列的组合</td>
<td>今/天</td>
</tr>
</tbody>
</table>
<p>根据第一个模板U00:%x[-2,0]能得到的转移特征函数如下：<br>
<code>func1=if(output=B and feature='U00:小' )　return 1 else return 0 </code><br>
其中output=B 指的是当前词（字）的预测标记，也就是’今‘的预测标记，每个模板会把所有可能的标记输出都列一遍，然后通过训练确定每种标记的权重，合理的标记在训练样本中出现的次数多，对应的权重就高，不合理的标记在训练样本中出现的少，对应的权重就少，但是在利用模板生成转移特征函数是会把所有可能的特征函数都列出来，由模型通过训练决定每个特征的重要程度。<br>
<code>func2=if(output=M and feature=’U00:小’) return 1 else return 0 </code><br>
<code>func3=if(output=E and feature=’U00:小’) return 1 else return 0 </code><br>
<code>func4=if(output=S and feature=’U00:小) return 1 else return 0 </code><br>
得到4个特征函数之后当前这个字’今‘的特征函数利用第一个模板就得到全部了，然后扫描下一个字‘天‘，以’天‘字作为当前字预测这个字的标记tag,同样会得到4个特征函数：<br>
<code>func5=if(output=B and feature=’U00:明’) return 1 else return 0 </code><br>
<code>func6=if(output=M and feature=’U00:明’) return 1 else return 0 </code><br>
<code>func7=if(output=E and feature=’U00:明’) return 1 else return 0 </code><br>
<code>func7=if(output=S and feature=’U00:明’) return 1 else return 0 </code><br>
特征函数中的feature指的是当前词的-2位置对应的词或对应的词的特征，因为在这里没有其他特征了，所以用字本身做特征，有的会有词性，那feature就会是’明‘这个字对应的词性而不是字本身了。 这个feature的作用就是确定模板所确定的当前词和临近词</p>
<h6 id="bigram">bigram</h6>
<p>与Unigram不同的是，Bigram类型模板生成的函数会多一个参数：上个节点的标签 \(y_{i-1}\)。生成函B01:%x[0,0] (当前词为“东南大学欢迎您” 的“您”)：<code>func1 = if (prev_output = E and output = S and feature=B01:&quot;您&quot;) return 1 else return 0</code></p>
<p>这样，每行模板则会生成 \(L*L*N\) 个特征函数。经过训练后，这些函数的权值反映了上一个节点的标签对当前节点的影响。(L表示隐状态的个数，如BMES为四个隐状态，N表示训练语料的大小。)</p>
<p>trigam理论上也是可行的，考虑前\(y_{i-2},y_{i-1}\)对\(y_i\)的影响，不过crf++并没有实现，这实际和二阶HMM模型类似的，有论文详细描述过其求解，比较复杂，但是其效果提升并未多大，所以一般不使用。</p>
<h6 id="chan-sheng-te-zheng-han-shu">产生特征函数</h6>
<p>训练阶段将遍历数据逐一产生特征函数，写成伪码过程为：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">m：样本个数</span><br><span class="line">k：模板个数</span><br><span class="line">templates：一组特征模板</span><br><span class="line">genFeatureFunc：根据模板和序列x，y和位置i产生特征函数</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">    <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; k; j++)&#123;</span><br><span class="line">        genFeatureFunc(templates[j],x,y,i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="te-zheng-han-shu">特征函数</h6>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span> B</span><br><span class="line"><span class="number">16</span> U00:-</span><br><span class="line"><span class="number">20</span> U00:<span class="number">0</span></span><br><span class="line"><span class="number">24</span> U00:<span class="number">1</span></span><br><span class="line"><span class="number">28</span> U00:<span class="number">2</span></span><br><span class="line"><span class="number">32</span> U00:<span class="number">3</span></span><br><span class="line"><span class="number">36</span> U00:<span class="number">4</span></span><br><span class="line"><span class="number">40</span> U00:<span class="number">5</span></span><br><span class="line"><span class="number">44</span> U00:<span class="number">6</span></span><br><span class="line"><span class="number">48</span> U00:<span class="number">7</span></span><br><span class="line"><span class="number">52</span> U00:<span class="number">8</span></span><br><span class="line"><span class="number">56</span> U00:<span class="number">9</span></span><br><span class="line"><span class="number">60</span> U00:_B<span class="number">-1</span></span><br><span class="line"><span class="number">64</span> U00:_B<span class="number">-2</span></span><br><span class="line">……</span><br><span class="line"><span class="number">17404</span> U01:厨</span><br><span class="line"><span class="number">17408</span> U01:去</span><br><span class="line"><span class="number">17412</span> U01:县</span><br><span class="line"><span class="number">17416</span> U01:参</span><br><span class="line"><span class="number">17420</span> U01:又</span><br><span class="line"><span class="number">17424</span> U01:叉</span><br><span class="line"><span class="number">17428</span> U01:及</span><br><span class="line"><span class="number">17432</span> U01:友</span><br><span class="line"><span class="number">17436</span> U01:双</span><br><span class="line"><span class="number">17440</span> U01:反</span><br><span class="line"><span class="number">17444</span> U01:发</span><br><span class="line"><span class="number">17448</span> U01:叔</span><br><span class="line"><span class="number">17452</span> U01:取</span><br><span class="line"><span class="number">17456</span> U01:受</span><br><span class="line">……</span><br><span class="line"><span class="number">77800</span> U05:_B<span class="number">-1</span>/一/个</span><br><span class="line"><span class="number">107540</span> U05:一/方/面</span><br><span class="line"><span class="number">107544</span> U05:一/无/所</span><br><span class="line"><span class="number">107548</span> U05:一/日/三</span><br><span class="line"><span class="number">107552</span> U05:一/日/为</span><br><span class="line"><span class="number">107556</span> U05:一/日/之</span><br><span class="line">……</span><br><span class="line"><span class="number">566536</span> U06:万/吨/_B+<span class="number">1</span></span><br><span class="line">……</span><br><span class="line"><span class="number">2159864</span> U09:ｖ/ｅ</span><br></pre></td></tr></table></figure>
<p>按照[id] [参数o]的格式排列，注意到id不是连续的，而是隔了四个（状态s隐藏起来了），这表示这四个标签（BMES）和公共的参数o组合成了四个特征函数。特别的，0-15为BEMS转移到BEMS的转移函数，也就是f(s’, s, o=null)。_B-1表示句子第一个单词前面的一个单词，_B+1表示末尾后面的一个单词.</p>
<h6 id="te-zheng-han-shu-quan-zhi">特征函数权值</h6>
<p>后面的小数依id顺序对应每个特征函数的权值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">9.0491453814148901</span></span><br><span class="line"><span class="number">7.0388286231971700</span></span><br><span class="line"><span class="number">-7.2545558164093009</span></span><br><span class="line"><span class="number">5.2799470769112835</span></span><br><span class="line"><span class="number">-8.5333633546653758</span></span><br><span class="line"><span class="number">-5.3549190735606933</span></span><br><span class="line"><span class="number">5.2575182675282477</span></span><br><span class="line"><span class="number">-5.4259109736696054</span></span><br></pre></td></tr></table></figure>
<p>比如说有一个句子“商品和服务”，对于每个字都按照上述模板生成一系列U特征函数的参数代入，得到一些类似010101的函数返回值，乘上这些函数的权值求和，就得到了各个标签的分数，由大到小代表输出这些标签的可能性。至于B特征函数（这里特指简单的f(s’, s)），在Viterbi后向解码的时候，前一个标签确定了后就可以代入当前的B特征函数，计算出每个输出标签的分数，再次求和排序即可。</p>
<h5 id="xun-lian-ji-yu-ce">训练及预测</h5>
<p>CRF++的训练命令一般格式如下：<br>
<code>crf_learn  -f 3 -c 4.0 template train.data model -t</code><br>
其中，template为模板文件，train.data为训练语料，-t表示可以得到一个model文件和一个model.txt文件，其他可选参数说明如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">-f, –freq=INT使用属性的出现次数不少于INT(默认为<span class="number">1</span>)</span><br><span class="line">-m, –maxiter=INT设置INT为LBFGS的最大迭代次数 (默认<span class="number">10</span>k)</span><br><span class="line">-c, –cost=FLOAT    设置FLOAT为代价参数，过大会过度拟合 (默认<span class="number">1.0</span>)</span><br><span class="line">-e, –eta=FLOAT设置终止标准FLOAT(默认<span class="number">0.0001</span>)</span><br><span class="line">-C, –convert将文本模式转为二进制模式</span><br><span class="line">-t, –textmodel为调试建立文本模型文件</span><br><span class="line">-a, –algorithm=(CRF|MIRA)    选择训练算法，默认为CRF-L2</span><br><span class="line">-p, –thread=INT线程数(默认<span class="number">1</span>)，利用多个CPU减少训练时间</span><br><span class="line">-H, –shrinking-size=INT    设置INT为最适宜的跌代变量次数 (默认<span class="number">20</span>)</span><br><span class="line">-v, –version显示版本号并退出</span><br><span class="line">-h, –help显示帮助并退出</span><br></pre></td></tr></table></figure>
<p>在训练过程中，会输出一些信息，其意义如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iter：迭代次数。当迭代次数达到maxiter时，迭代终止</span><br><span class="line">terr：标记错误率</span><br><span class="line">serr：句子错误率</span><br><span class="line">obj：当前对象的值。当这个值收敛到一个确定值的时候，训练完成</span><br><span class="line">diff：与上一个对象值之间的相对差。当此值低于eta时，训练完成</span><br></pre></td></tr></table></figure>
<p>在训练完模型后，我们可以使用训练好的模型对新数据进行预测，预测命令格式如下：<br>
<code>crf_test -m model seg_word_predict.data &gt; predict.txt</code></p>
<p><code>-m model</code>表示使用我们刚刚训练好的model模型，预测的数据文件为seg_word_predict.data, <code>&gt; predict.txt</code>表示将预测后的数据写入到predict.txt中。</p>
<h6 id="dai-ma">代码</h6>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> CRFPP</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRFModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model=<span class="string">'model_name'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 类初始化</span></span><br><span class="line"><span class="string">        :param model: 模型名称</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_tagger</span><span class="params">(self, tag_data)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 添加语料</span></span><br><span class="line"><span class="string">        :param tag_data: 数据</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        word_str = tag_data.strip()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(self.model):</span><br><span class="line">            print(<span class="string">'模型不存在,请确认模型路径是否正确!'</span>)</span><br><span class="line">            exit()</span><br><span class="line">        tagger = CRFPP.Tagger(<span class="string">"-m &#123;&#125; -v 3 -n2"</span>.format(self.model))</span><br><span class="line">        tagger.clear()</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> word_str:</span><br><span class="line">            tagger.add(word)</span><br><span class="line">        tagger.parse()</span><br><span class="line">        <span class="keyword">return</span> tagger</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">text_mark</span><span class="params">(self, tag_data, begin=<span class="string">'B'</span>, middle=<span class="string">'I'</span>, end=<span class="string">'E'</span>, single=<span class="string">'S'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        文本标记</span></span><br><span class="line"><span class="string">        :param tag_data: 数据</span></span><br><span class="line"><span class="string">        :param begin: 开始标记</span></span><br><span class="line"><span class="string">        :param middle: 中间标记</span></span><br><span class="line"><span class="string">        :param end: 结束标记</span></span><br><span class="line"><span class="string">        :param single: 单字结束标记</span></span><br><span class="line"><span class="string">        :return result: 标记列表</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        tagger = self.add_tagger(tag_data)</span><br><span class="line">        size = tagger.size()</span><br><span class="line">        tag_text = <span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, size):</span><br><span class="line">            word, tag = tagger.x(i, <span class="number">0</span>), tagger.y2(i)</span><br><span class="line">            <span class="keyword">if</span> tag <span class="keyword">in</span> [begin, middle]:</span><br><span class="line">                tag_text += word</span><br><span class="line">            <span class="keyword">elif</span> tag <span class="keyword">in</span> [end, single]:</span><br><span class="line">                tag_text += word + <span class="string">"*&amp;*"</span></span><br><span class="line">        result = tag_text.split(<span class="string">'*&amp;*'</span>)</span><br><span class="line">        result.pop()</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crf_test</span><span class="params">(self, tag_data, separator=<span class="string">'/'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: crf测试</span></span><br><span class="line"><span class="string">        :param tag_data:</span></span><br><span class="line"><span class="string">        :param separator:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        result = self.text_mark(tag_data)</span><br><span class="line">        data = separator.join(result)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crf_learn</span><span class="params">(self, filename)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 训练模型</span></span><br><span class="line"><span class="string">        :param filename: 已标注数据源</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        crf_bash = <span class="string">"crf_learn -f 3 -c 4.0 api/template.txt &#123;&#125; &#123;&#125;"</span>.format(filename, self.model)</span><br><span class="line">        process = subprocess.Popen(crf_bash.split(), stdout=subprocess.PIPE)</span><br><span class="line">        output = process.communicate()[<span class="number">0</span>]</span><br><span class="line">        print(output.decode(encoding=<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<h3 id="can-kao">参考</h3>
<ol>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/37163081" target="_blank" rel="noopener">简明条件随机场CRF介绍 | 附带纯Keras实现</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/qq_37667364/article/details/82919560" target="_blank" rel="noopener">CRF++/CRF/条件随机场的特征函数模板</a></p>
</li>
<li>
<p><a href="https://www.hankcs.com/nlp/the-crf-model-format-description.html" target="_blank" rel="noopener">CRF++模型格式说明</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>序列标注</tag>
        <tag>条件随机场</tag>
      </tags>
  </entry>
  <entry>
    <title>文本分类</title>
    <url>/2020/03/29/text_classification/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/03/29/text_classification/QQ%E6%88%AA%E5%9B%BE20180116105758_%E5%89%AF%E6%9C%AC.png" alt></p>
<a id="more"></a>
<h1 id="xiang-mu-jie-shao">项目介绍</h1>
<ol>
<li>
<p>项目使用深度学习模型进行文本分类，所使用的模型主要包括：FastText，TextCNN，DPCNN，RNN系列(RNN，LSTM，GRU)，RNN-Attention，TextRCNN，HAN，Bert，BertCNN，BertRNN，BertRCNN,XLNet。</p>
</li>
<li>
<p>方法部分对每个模型及其结构给出简要介绍，并附上pytorch代码实现。</p>
</li>
<li>
<p>实验部分所采用的的数据集：weibo_senti_100k情感分类(二分类)，cnews新闻十分类，____文本多标签分类。</p>
</li>
</ol>
<p><strong>数据下载</strong>：微博情感分类数据在<a href="https://github.com/jeffery0628/text_classification" target="_blank" rel="noopener">github仓库</a>中给出, <a href="https://pan.baidu.com/s/1OOTK374IZ1DHnz5COUcfbw" target="_blank" rel="noopener">cnews新闻数据</a>  密码:hf6o, <a href>____文本多标签数据</a></p>
<p><strong>词向量下载</strong>：<a href="https://github.com/Embedding/Chinese-Word-Vectors" target="_blank" rel="noopener">词向量</a></p>
<p><strong>预训练模型下载</strong>：<a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">中文预训练bert模型下载</a>,<a href="https://github.com/ymcui/Chinese-XLNet" target="_blank" rel="noopener">中文预训练XLNet下载</a></p>
<p><strong>项目仓库地址</strong>：<a href="https://github.com/jeffery0628/text_classification" target="_blank" rel="noopener">中文文本分类</a></p>
<p>如出现数学公式乱码以及图片问题，请移步<a href="https://jeffery0628.github.io/" target="_blank" rel="noopener">github.io</a>来获得更好的阅读体验。</p>
<p>最后，欢迎star！</p>
<h1 id="jian-jie">简介</h1>
<p>文本分类在文本处理中是很重要的一个模块，它的应用也非常广泛，比如：新闻分类、简历分类、邮件分类、办公文档分类、区域分类等诸多方面，还能够实现文本过滤，从大量文本中快速识别和过滤出符合特殊要求的信息。。它和其他的分类没有本质的区别，核心方法为首先提取分类数据的特征，然后选择最优的匹配，从而分类。但是文本也有自己的特点，根据文本的特点，文本分类的一般流程为：1.预处理；2.文本表示及特征选择；3.构造分类器；4.分类。</p>
<p>通常来讲，文本分类任务是指在给定的分类体系中，将文本指定分到某个或某几个类别中。被分类的对象有短文本，例如句子、标题、商品评论等等，长文本，如文章等。分类体系一般人工划分，例如：1）政治、体育、军事 2）正能量、负能量 3）好评、中性、差评。此外，还有文本多标签分类，比如一篇博客的标签可以同时是：自然语言处理，文本分类等。因此，对应的分类模式可以分为：二分类、多分类以及多标签分类问题。</p>
<p><img src="/2020/03/29/text_classification/595c46f937d92.png" alt></p>
<ol>
<li>对文本分类的研究可以追溯到二十世纪五十年代，当时主要依据特定的人工规则进行文本分类。</li>
<li>到二十世纪九十年代，统计机器学习 (Statistical machine learning) 成为主流，一些统计机器学习方法，比如支持向量机和朴素贝叶斯等分类方法在文本分类中取得了非常高的分类准确率。然而，统计机器学习方法首先需要进行特征工程工作，该工作需要深入理解业务需求，并且非常耗时耗力。</li>
<li>随着大数据量和图形处理单元强计算力的支持，深度学习近年来发展迅速，与统计机器学习方法相比，深度学习方法可以自动提取特征，使得人们将注意力更多地集中在数据和模型上。</li>
</ol>
<h1 id="fang-fa">方法</h1>
<p>这里我们根据是否使用深度学习方法将文本分类主要分为一下两个大类：</p>
<ul>
<li>基于机器学习的文本分类（不涉及实现以及实验结果的比较）。</li>
<li>基于深度学习的文本分类。</li>
</ul>
<h2 id="ji-yu-ji-qi-xue-xi-de-wen-ben-fen-lei">基于机器学习的文本分类</h2>
<p>90年代后互联网在线文本数量增长和机器学习学科的兴起，逐渐形成了一套解决大规模文本分类问题的经典玩法，这个阶段的主要套路是人工特征工程+浅层分类模型。整个文本分类问题就拆分成了<strong>特征工程</strong>和<strong>分类器</strong>两部分。</p>
<h3 id="te-zheng-gong-cheng">特征工程</h3>
<p>特征工程也就是将文本表示为计算机可以识别的、能够代表该文档特征的特征矩阵的过程。在基于传统机器学习的文本分类中，通常将特征工程分为<strong>文本预处理、特征提取、文本表示</strong>等三个部分。</p>
<h4 id="wen-ben-yu-chu-li">文本预处理</h4>
<p>文本预处理过程是提取文本中的关键词来表示文本的过程。中文文本预处理主要包括文本分词和去停用词两个阶段。文本分词，是因为很多研究表明特征粒度为词粒度远好于字粒度（因为大部分分类算法不考虑词序信息，基于字粒度显然损失了过多<code>n-gram</code>信息）。具体到中文分词，不同于英文有天然的空格间隔，需要设计复杂的分词算法。传统分词算法主要有基于字符串匹配的正向/逆向/双向最大匹配；基于理解的句法和语义分析消歧；基于统计的互信息/CRF方法(<code>WordEmbedding+Bi-LSTM+CRF</code>方法逐渐成为主流)。 而停用词是文本中一些高频的代词、连词、介词等对文本分类无意义的词，通常维护一个停用词表，特征提取过程中删除停用表中出现的词，本质上属于特征选择的一部分。</p>
<h4 id="te-zheng-ti-qu">特征提取</h4>
<p>特征提取包括<strong>特征选择</strong>和<strong>特征权重计算</strong>两部分。 特征选择的基本思路是根据某个评价指标独立的对原始特征项（词项）进行评分排序，从中选择得分最高的一些特征项，过滤掉其余的特征项。常用的评价有：文档频率、互信息、信息增益、χ²统计量等。特征权重计算主要是经典的TF-IDF方法及其扩展方法。</p>
<h4 id="wen-ben-biao-shi">文本表示</h4>
<p>文本表示的目的是把文本预处理后的转换成计算机可理解的方式，是决定文本分类质量最重要的部分。</p>
<p><img src="/2020/03/29/text_classification/image-20200520093625468.png" alt></p>
<h5 id="ci-dai-fa">词袋法</h5>
<p>忽略其词序和语法，句法，将文本仅仅看做是一个词集合。若词集合共有NN个词，每个文本表示为一个<code>N</code>维向量，元素为<code>0/1</code>，表示该文本是否包含对应的词。<code>( 0, 0, 0, 0, .... , 1, ... 0, 0, 0, 0)</code>。一般来说词库量至少都是百万级别，因此词袋模型有个两个最大的问题：高纬度、高稀疏性。</p>
<h5 id="n-gram-ci-dai-mo-xing">n-gram 词袋模型</h5>
<p>与词袋模型类似，考虑了局部的顺序信息，但是向量的维度过大，基本不采用。如果词集合大小为<code>N</code>，则bi-gram的单词总数为\(n^2\)向量空间模型。</p>
<h5 id="xiang-liang-kong-jian-mo-xing">向量空间模型</h5>
<p>以词袋模型为基础，向量空间模型通过特征选择降低维度，通过特征权重计算增加稠密性。</p>
<h3 id="fen-lei-qi">分类器</h3>
<p>大部分机器学习方法都在文本分类领域有所应用，比如朴素贝叶斯分类算法、KNN、SVM、最大熵、GBDT/XGBoost等等。</p>
<h2 id="ji-yu-shen-du-xue-xi-de-wen-ben-fen-lei">基于深度学习的文本分类</h2>
<h3 id="fast-text">FastText</h3>
<h4 id="jian-jie-1">简介</h4>
<p>fastText是Facebook于2016年开源的一个词向量计算和文本分类工具,<a href="https://arxiv.org/pdf/1607.01759.pdf" target="_blank" rel="noopener">论文地址</a>,其<strong>特点</strong>就是<strong>fast</strong>。在文本分类任务中，fastText（浅层网络）往往能取得和深度网络相媲美的精度，却在训练时间上比深度网络快许多数量级。在标准的多核CPU上， 在10分钟之内能够训练10亿词级别语料库的词向量，在1分钟之内能够分类有着30万多类别的50多万句子。</p>
<p>fastText是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：</p>
<ol>
<li>fastText在保持高精度的情况下加快了训练速度和测试速度</li>
<li>fastText不需要预训练好的词向量，fastText会自己训练词向量</li>
<li>fastText两个重要的优化：Hierarchical Softmax、N-gram</li>
</ol>
<h4 id="fast-text-mo-xing-jia-gou">fastText模型架构</h4>
<p>fastText模型架构和word2vec中的CBOW很相似， 不同之处是fastText预测标签而CBOW预测的是中间词，即模型架构类似但是模型的任务不同：</p>
<p><img src="/2020/03/29/text_classification/image-20200520093324964.png" alt></p>
<p>word2vec将上下文关系转化为多分类任务，进而训练逻辑回归模型。通常的文本数据中，词库少则数万，多则百万，在训练中直接训练多分类逻辑回归并不现实。word2vec中提供了两种针对大规模多分类问题的优化手段， negative sampling 和hierarchical softmax。在优化中，negative sampling 只更新少量负面类，从而减轻了计算量。hierarchical softmax 将词库表示成前缀树，从树根到叶子的路径可以表示为一系列二分类器，一次多分类计算的复杂度从|V|降低到了树的高度。<br>
fastText模型架构:其中\(x_1,x_2,\ldots,x_{N−1},x_N\)表示一个文本中的n-gram向量，每个特征是词向量的平均值。</p>
<p><img src="/2020/03/29/text_classification/image-20200520093359191.png" alt></p>
<h4 id="que-dian">缺点：</h4>
<blockquote>
<p>我不喜欢这类电影，但是喜欢这一个。</p>
<p>我喜欢这类电影，但是不喜欢这一个。</p>
</blockquote>
<p><strong>这样的两句句子经过词向量平均以后已经送入单层神经网络的时候已经完全一模一样了，分类器不可能分辨出这两句话的区别</strong>，只有添加n-gram特征以后才可能有区别。因此，在实际应用的时候需要对数据有足够的了解,然后在选择模型。</p>
<h4 id="mo-xing-dai-ma">模型代码</h4>
<p><img src="/2020/03/29/text_classification/FastText_network_structure.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FastText</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, output_dim, word_embedding, freeze)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(self.embedding_size, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,_, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line">        pooled = F.avg_pool2d(embedded, (embedded.shape[<span class="number">1</span>], <span class="number">1</span>)).squeeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># pooled = [batch size, embedding_dim]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(pooled)</span><br></pre></td></tr></table></figure>
<h3 id="text-cnn">TextCNN</h3>
<h4 id="jian-jie-2">简介</h4>
<p><strong>Yoon Kim</strong>在论文<a href="https://arxiv.org/abs/1408.5882" target="_blank" rel="noopener">(2014 EMNLP) Convolutional Neural Networks for Sentence Classification</a>提出TextCNN。将卷积神经网络CNN应用到文本分类任务，利用多个不同size的kernel来提取句子中的关键信息，从而能够更好地捕捉局部相关性。</p>
<h4 id="wang-luo-jie-gou">网络结构</h4>
<p><img src="/2020/03/29/text_classification/textcnn.png" alt></p>
<h4 id="yuan-li">原理</h4>
<p>TextCNN的详细过程原理图如下：</p>
<p><img src="/2020/03/29/text_classification/textcnndetail.png" alt></p>
<p>TextCNN详细过程：</p>
<ul>
<li>Embedding：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点。</li>
<li>Convolution：然后经过 <code>kernel_sizes=(2,3,4) </code>的一维卷积层，每个kernel_size 有两个输出 channel。</li>
<li>MaxPolling：第三层是一个<code>1-max pooling</code>层，这样不同长度句子经过pooling层之后都能变成定长的表示。</li>
<li>FullConnection and Softmax：最后接一层全连接的 softmax 层，输出每个类别的概率。</li>
</ul>
<h4 id="que-dian-1">缺点</h4>
<p>TextCNN模型最大的问题也是这个全局的max pooling丢失了结构信息，因此很难去发现文本中的转折关系等复杂模式。针对这个问题，可以尝试k-max pooling做一些优化，k-max pooling针对每个卷积核都不只保留最大的值，他保留前k个最大值，并且保留这些值出现的顺序，也即按照文本中的位置顺序来排列这k个最大值。在某些比较复杂的文本上相对于1-max pooling会有提升。</p>
<h4 id="dai-ma">代码</h4>
<p><img src="/2020/03/29/text_classification/TextCNN_network_structure.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_filters, filter_sizes, output_dim, dropout, word_embedding, freeze)</span>:</span></span><br><span class="line">        <span class="comment"># n_filter 每个卷积核的个数</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line">        self.convs = nn.ModuleList(</span><br><span class="line">            [nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=n_filters, kernel_size=(fs, self.embedding_size)) <span class="keyword">for</span> fs <span class="keyword">in</span></span><br><span class="line">             filter_sizes])</span><br><span class="line">        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, _, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [batch size, sent len]</span></span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line">        embedded = embedded.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># embedded = [batch size, 1, sent len, emb dim]</span></span><br><span class="line">        conved = [F.relu(conv(embedded)).squeeze(<span class="number">3</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs]</span><br><span class="line">        <span class="comment"># conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]</span></span><br><span class="line">        pooled = [F.max_pool1d(conv, conv.shape[<span class="number">2</span>]).squeeze(<span class="number">2</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> conved]</span><br><span class="line">        <span class="comment"># pooled_n = [batch size, n_filters]</span></span><br><span class="line">        cat = self.dropout(torch.cat(pooled, dim=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># cat = [batch size, n_filters * len(filter_sizes)]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(cat)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN1d</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_filters, filter_sizes, output_dim, dropout, word_embedding, freeze)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line">        self.convs = nn.ModuleList(</span><br><span class="line">            [nn.Conv1d(in_channels=self.embedding_size, out_channels=n_filters, kernel_size=fs) <span class="keyword">for</span> fs <span class="keyword">in</span> filter_sizes])</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, _, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [batch size, sent len]</span></span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line">        embedded = embedded.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># embedded = [batch size, emb dim, sent len]</span></span><br><span class="line">        conved = [F.relu(conv(embedded)) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs]</span><br><span class="line">        <span class="comment"># conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]</span></span><br><span class="line">        pooled = [F.max_pool1d(conv, conv.shape[<span class="number">2</span>]).squeeze(<span class="number">2</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> conved]</span><br><span class="line">        <span class="comment"># pooled_n = [batch size, n_filters]</span></span><br><span class="line">        cat = self.dropout(torch.cat(pooled, dim=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># cat = [batch size, n_filters * len(filter_sizes)]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(cat)</span><br></pre></td></tr></table></figure>
<h3 id="dpcnn">DPCNN</h3>
<h4 id="jian-jie-3">简介：</h4>
<p>ACL2017年中，腾讯AI-lab提出了Deep Pyramid Convolutional Neural Networks for Text Categorization(<a href="https://ai.tencent.com/ailab/media/publications/ACL3-Brady.pdf" target="_blank" rel="noopener">DPCNN</a>)。论文中提出了一种基于word-level级别的网络-DPCNN，由于TextCNN 不能通过卷积获得文本的长距离依赖关系，而论文中DPCNN通过不断加深网络，可以抽取长距离的文本依赖关系。实验证明在不增加太多计算成本的情况下，增加网络深度就可以获得最佳的准确率。‍</p>
<h4 id="wang-luo-jie-gou-1">网络结构</h4>
<p><img src="/2020/03/29/text_classification/DPCNN.jpg" alt></p>
<h5 id="region-embedding">Region embedding</h5>
<p>作者将TextCNN的包含多尺寸卷积滤波器的卷积层的卷积结果称之为<code>Region embedding</code>，意思就是对一个文本区域/片段（比如<code>3-gram</code>）进行一组卷积操作后生成的embedding。<br>
卷积操作有两种选择：</p>
<ol>
<li>保留词序：也就是设置一组<code>size=3*D</code>的二维卷积核对<code>3-gram</code>进行卷积（其中D是word embedding维度）</li>
<li>不保留词序（即使用词袋模型），即首先对<code>3-gram</code>中的3个词的embedding取均值得到一个size=D的向量，然后设置一组size=D的一维卷积核对该<code>3-gram</code>进行卷积。</li>
</ol>
<p>TextCNN里使用的是保留词序的做法，而DPCNN使用的是词袋模型的做法，DPCNN作者认为前者做法更容易造成过拟合，后者的性能却跟前者差不多。</p>
<h4 id="juan-ji-he-quan-lian-jie-de-quan-heng">卷积和全连接的权衡</h4>
<p>产生<code>region embedding</code>后，按照经典的TextCNN的做法的话，就是从每个特征图中挑选出最有代表性的特征，也就是直接应用全局最大池化层，这样就生成了这段文本的特征向量,假如卷积滤波器的size有3，4，5这三种，每种size包含100个卷积核，那么当然就会产生3<em>100幅特征图，然后将max-over-time-pooling操作应用到每个特征图上，于是文本的特征向量即3</em>100=300维。<br>
TextCNN这样做的意义本质上与<code>词袋模型(n-gram)+weighting+NB/MaxEnt/SVM</code>的经典文本分类模型没本质区别，只不过one-hot表示到word embedding表示的转变避免了词袋模型遭遇的数据稀疏问题。TextCNN本质上收益于词向量的引入带来的近义词有相近向量表示的bonus，同时TextCNN可以较好的利用词向量中近义关系。<strong>经典模型里难以学习的远距离信息在TextCNN中依然难以学习</strong>。</p>
<h5 id="deng-chang-juan-ji">等长卷积</h5>
<p>假设输入的序列长度为\(n\)，卷积核大小为\(m\)，步长为\(s\),输入序列两端各填补\(p\)个零,那么该卷积层的输出序列为\(\frac{(n-m+2p)}{s}+1\)。</p>
<ol>
<li>窄卷积:步长\(s=1\),两端不补零，即\(p=0\)，卷积后输出长度为\(n-m+1\)。</li>
<li>宽卷积:步长\(s=1\),两端补零\(p=m-1\)，卷积后输出长度\(n+m-1\)。</li>
<li>等长卷积: 步长\(s=1\),两端补零\(p=(m-1)/2\)，卷积后输出长度为\(n\)。</li>
</ol>
<p>将输入输出序列的第n个embedding称为第n个词位，那么这时size为n的卷积核产生的等长卷积的意义就是将输入序列的每个词位及其左右\(\frac{n-1}{2}\)个词的上下文信息压缩为该词位的embedding，产生了每个词位的被上下文信息修饰过的更高level更加准确的语义。想要克服TextCNN的缺点，捕获长距离模式，显然就要用到深层CNN。</p>
<p>直接等长卷积堆等长卷积会让每个词位包含进去越来越多，越来越长的上下文信息，这种方式会让网络层数变得非常非常非常深，但是这种方式太笨重。不过，既然等长卷积堆等长卷积会让每个词位的embedding描述语义描述的更加丰富准确，可以适当的堆两层来提高词位embedding的表示的丰富性。<br>
<img src="/2020/03/29/text_classification/equal_cnn.png" alt></p>
<h5 id="gu-ding-feature-map-de-shu-liang">固定feature map的数量</h5>
<p>在表示好每个词位的语义后，很多邻接词或者邻接<code>ngram</code>的词义是可以合并，例如“小明 人 不要 太好”中的“不要”和“太好”虽然语义本来离得很远，但是作为邻接词“不要太好”出现时其语义基本等价为“很好”，完全可以把“不要”和“太好”的语义进行合并。同时，合并的过程完全可以在原始的embedding space中进行的，原文中直接把“不要太好”合并为“很好”是很可以的，完全没有必要动整个语义空间。<br>
实际上，相比图像中这种从“点、线、弧”这种low-level特征到“眼睛、鼻子、嘴”这种high-level特征的明显层次性的特征区分，文本中的特征进阶明显要扁平的多，即从单词（1gram）到短语再到3gram、4gram的升级，其实很大程度上均满足“语义取代”的特性。而图像中就很难发生这种“语义取代”现象。因此，DPCNN与ResNet很大一个不同就是，<strong>在DPCNN中固定死了feature map的数量</strong>，也就是固定住了embedding space的维度（为了方便理解，以下简称语义空间），使得网络有可能让整个邻接词（邻接ngram）的合并操作在原始空间或者与原始空间相似的空间中进行（当然，网络在实际中会不会这样做是不一定的，只是提供了这么一种条件）。也就是说，整个网络虽然形状上来看是深层的，但是从语义空间上来看完全可以是扁平的。而ResNet则是不断的改变语义空间，使得图像的语义随着网络层的加深也不断的跳向更高level的语义空间。</p>
<h5 id="strong-chi-hua-strong"><strong>池化</strong></h5>
<p>每经过一个\(size=3,stride=2\)的池化层(简称\(1/2\)池化层)，序列的长度就被压缩成了原来的一半。这样同样是\(size=3\)的卷积核，每经过一个\(1/2\)池化层后，其能感知到的文本片段就比之前长了一倍。例如之前是只能感知3个词位长度的信息，经过1/2池化层后就能感知6个词位长度的信息，这时把1/2池化层和size=3的卷积层组合起来如图：<br>
<img src="/2020/03/29/text_classification/dpcnn_pooling.png" alt></p>
<h5 id="can-chai-lian-jie">残差连接</h5>
<p>在初始化深度CNN时，往往各层权重都是初始化为一个很小的值，这就导致最开始的网络中，后续几乎每层的输入都是接近0，这时网络的输出自然是没意义的，而这些小权重同时也阻碍了梯度的传播，使得网络的初始训练阶段往往要迭代好久才能启动。同时，就算网络启动完成，由于深度网络中仿射矩阵近似连乘，训练过程中网络也非常容易发生梯度爆炸或弥散问题（虽然由于非共享权重，深度CNN网络比RNN网络要好点）。<br>
针对深度CNN网络的梯度弥散问题ResNet中提出的<code>shortcut-connection/skip-connection/residual-connection</code>（残差连接）就是一种非常简单、合理、有效的解决方案。<br>
<img src="/2020/03/29/text_classification/dpcnn_resnet.png" alt><br>
既然每个block的输入在初始阶段容易是0而无法激活，那么直接用一条线把region embedding层连接到每个block的输入乃至最终的池化层/输出层。有了shortcut后，梯度就可以忽略卷积层权重的削弱，从shortcut一路无损的传递到各个block，直至网络前端，从而极大的缓解了梯度消失问题。</p>
<h4 id="dai-ma-1">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DPCNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,num_filters, num_classes,word_embedding, freeze)</span>:</span></span><br><span class="line">        super(DPCNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line"></span><br><span class="line">        self.conv_region = nn.Conv2d(<span class="number">1</span>, num_filters, (<span class="number">3</span>, self.embedding_size), stride=<span class="number">1</span>)</span><br><span class="line">        self.conv = nn.Conv2d(num_filters, num_filters, (<span class="number">3</span>, <span class="number">1</span>), stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.max_pool = nn.MaxPool2d(kernel_size=(<span class="number">3</span>, <span class="number">1</span>), stride=<span class="number">2</span>)</span><br><span class="line">        self.padding1 = nn.ZeroPad2d((<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># top bottom</span></span><br><span class="line">        self.padding2 = nn.ZeroPad2d((<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>))  <span class="comment"># bottom</span></span><br><span class="line"></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.fc = nn.Linear(num_filters, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,_, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text [batch_size,seq_len]</span></span><br><span class="line">        x = self.embedding(text)  <span class="comment"># x=[batch_size,seq_len,embedding_dim]</span></span><br><span class="line">        x = x.unsqueeze(<span class="number">1</span>)  <span class="comment"># [batch_size, 1, seq_len, embedding_dim]</span></span><br><span class="line">        x = self.conv_region(x)  <span class="comment"># x = [batch_size, num_filters, seq_len-3+1, 1]</span></span><br><span class="line"></span><br><span class="line">        x = self.padding1(x)  <span class="comment"># [batch_size, num_filters, seq_len, 1]</span></span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv(x)  <span class="comment"># [batch_size, num_filters, seq_len-3+1, 1]</span></span><br><span class="line">        x = self.padding1(x)  <span class="comment"># [batch_size, num_filters, seq_len, 1]</span></span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv(x)  <span class="comment"># [batch_size, num_filters, seq_len-3+1, 1]</span></span><br><span class="line">        <span class="keyword">while</span> x.size()[<span class="number">2</span>] &gt;= <span class="number">2</span>:</span><br><span class="line">            x = self._block(x)  <span class="comment"># [batch_size, num_filters,1,1]</span></span><br><span class="line">        x = x.squeeze()  <span class="comment"># [batch_size, num_filters]</span></span><br><span class="line">        x = self.fc(x)  <span class="comment"># [batch_size, 1]</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_block</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.padding2(x)</span><br><span class="line">        px = self.max_pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.padding1(px)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line"></span><br><span class="line">        x = self.padding1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Short Cut</span></span><br><span class="line">        x = x + px</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="rnn-xi-lie">RNN系列</h3>
<h4 id="rnn">RNN</h4>
<p>通过将前一时刻的运算结果添加到当前的运算中，从而实现了“考虑上文信息”的功能。</p>
<p><img src="/2020/03/29/text_classification/rnn.png" alt></p>
<p>RNN可以考虑上文的信息，那么如何将下文的信息也添加进去呢？这就是BiRNN要做的事情。</p>
<p><img src="/2020/03/29/text_classification/birnn.png" alt></p>
<h4 id="lstm">LSTM</h4>
<p>因为RNN存在梯度弥散和梯度爆炸的问题，所以RNN很难完美地处理具有长期依赖的信息。既然仅仅依靠一条线连接后面的神经元不足以解决问题，那么就再加一条线(这条线实现的功能是把rnn中的累乘变成了累加)，这就是LSTM。</p>
<p>LSTM的关键在于细胞的状态和穿过细胞的线，细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流动保持不变会变得容易。</p>
<p><img src="/2020/03/29/text_classification/lstm_c.png" alt></p>
<p>在LSTM中，门可以实现选择性的让信息通过，主要通过一个sigmoid的神经层和一个逐点相乘的操作来实现。LSTM通过三个这样的门结构来实现信息的保护和控制，分别是遗忘门（forget gate）、输入门（input gate）与输出门（output gate）。</p>
<h5 id="strong-yi-wang-men-strong"><strong>遗忘门</strong></h5>
<p>在LSTM中的第一步是决定从细胞状态中丢弃什么信息。这个决定通过一个称为遗忘门的结构来完成。遗忘门会读取\(h_{t-1}\)和\(x_t\)，输出一个0到1之间的数值给细胞的状态\(c_{t-1}\)中的数字。1表示完全保留，0表示完全舍弃。</p>
<p><img src="/2020/03/29/text_classification/lstm_forget_gate.png" alt></p>
<h5 id="strong-shu-ru-men-strong"><strong>输入门</strong></h5>
<p>遗忘门决定让多少新的信息加入到cell的状态中来。实现这个需要两个步骤：</p>
<ol>
<li>
<p>首先一个叫“input gate layer”的sigmoid层决定哪些信息需要更新；一个tanh层生成一个向量，用来更新状态C。</p>
<p><img src="/2020/03/29/text_classification/input_gate.png" alt></p>
</li>
<li>
<p>把 1 中的两部分联合起来，对cell的状态进行更新，我们把旧的状态与\(f_t\)相乘，丢弃掉我们确定需要丢弃的信息，接着加上\(i_t * \tilde{C}_{t}\)</p>
</li>
</ol>
<h5 id="shu-chu-men">输出门</h5>
<p>最终，我们需要确定输出什么值，这个输出将会基于我们的细胞的状态，但是也是一个过滤后的版本。首先，我们通过一个sigmoid层来确定细胞状态的哪些部分将输出出去。接着，我们把细胞状态通过tanh进行处理（得到一个-1到1之间的值）并将它和sigmoid门的输出相乘，最终我们仅仅会输出我们确定输出的部分。</p>
<p><img src="/2020/03/29/text_classification/output_gate.png" alt></p>
<h5 id="gong-shi">公式</h5>
<p><img src="/2020/03/29/text_classification/lstm_all.png" alt></p>
<h4 id="gru">GRU</h4>
<p>在LSTM中引入了三个门函数：输入门、遗忘门和输出门来控制输入值、记忆值和输出值。而在GRU模型中只有两个门：分别是更新门和重置门。</p>
<p><img src="/2020/03/29/text_classification/GRU.png" alt></p>
<p>图中的\(z_t\)和\(r_t\)分别表示更新门和重置门。更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多。重置门控制前一状态有多少信息被写入到当前的候选集 \(\tilde{h}_{t}\)上，重置门越小，前一状态的信息被写入的越少。</p>
<p>LSTM和CRU都是通过各种门函数来将重要特征保留下来，这样就保证了在long-term传播的时候也不会丢失。此外GRU相对于LSTM少了一个门函数，因此在参数的数量上也是要少于LSTM的，所以整体上GRU的训练速度要快于LSTM的。</p>
<h4 id="dai-ma-2">代码</h4>
<p><img src="/2020/03/29/text_classification/TextBiRNN_network_structure.png" alt="TextBiRNN_network_structure"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RnnModel</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, hidden_dim, output_dim, n_layers,bidirectional, dropout,word_embedding, freeze,batch_first=True)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.embedding_size,</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.embedding_size,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.embedding_size,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(hidden_dim * n_layers, output_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,_, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># 按照句子长度从大到小排序</span></span><br><span class="line">        text, sorted_seq_lengths, desorted_indices = prepare_pack_padded_sequence(text, text_lengths)</span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        embedded = self.dropout(self.embedding(text))</span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_seq_lengths, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># output (seq_len, batch, num_directions * hidden_size)</span></span><br><span class="line">            <span class="comment"># hidden (num_layers * num_directions, batch, hidden_size)</span></span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line">        <span class="comment"># 把句子序列再调整成输入时的顺序</span></span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        <span class="comment"># output = [batch_size,seq_len,hidden_dim * num_directionns ]</span></span><br><span class="line">        batch_size, max_seq_len,hidden_dim = output.shape</span><br><span class="line">        hidden = torch.mean(torch.reshape(hidden,[batch_size,<span class="number">-1</span>,hidden_dim]),dim=<span class="number">1</span>)</span><br><span class="line">        output = torch.sum(output,dim=<span class="number">1</span>)</span><br><span class="line">        fc_input = self.dropout(output + hidden)</span><br><span class="line">        out = self.fc(fc_input)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h4 id="self-attention">Self-Attention</h4>
<p><img src="/2020/03/29/text_classification/self-attention.png" alt></p>
<ol>
<li>Encode所有输入序列,得到对应的\(h_1,h_2, \cdots ,h_T\)(T为输入序列长度)</li>
<li>Decode输出目标\(y_t\)之前，会将上一步输出的隐藏状态\(S_{t-1}\)与之前encode好的\(h_1,h_2,\cdots,h_T\)进行比对，计算相似度（\(e_{t,j}=a(s_{t-1},h_j)\)）,\(h_j\)为之前第j个输入encode得到的隐藏向量，a为任意一种计算相似度的方式</li>
<li>然后通过softmax，即\(a_{t,j}=\frac{exp(e_{t,j})}{\sum^{T_x}_{k=1}exp(e_{t,k})}\)将之前得到的各个部分的相关系数进行归一化，得到\(a_{t,1},a_{t,2},\cdots,a_{t,T}\)</li>
<li>在对输入序列的隐藏层进行相关性加权求和得到此时decode需要的context vector ：</li>
</ol>
<h4 id="rnn-attenton">Rnn-Attenton</h4>
<p><img src="/2020/03/29/text_classification/TextAttBiRNN_network_structure.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RnnAttentionModel</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, hidden_dim, output_dim, n_layers,bidirectional, dropout,word_embedding, freeze, batch_first=True)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.embedding_size,</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.embedding_size,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.embedding_size,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        self.tanh1 = nn.Tanh()</span><br><span class="line">        self.tanh2 = nn.Tanh()</span><br><span class="line">        <span class="comment"># self.u = nn.Parameter(torch.Tensor(self.hidden_dim * 2,self.hidden_dim*2))</span></span><br><span class="line">        self.w = nn.Parameter(torch.randn(hidden_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="keyword">if</span> bidirectional:</span><br><span class="line">            self.w = nn.Parameter(torch.randn(hidden_dim * <span class="number">2</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">            self.fc = nn.Linear(hidden_dim * <span class="number">2</span>, output_dim)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.w = nn.Parameter(torch.randn(hidden_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line">            self.fc = nn.Linear(hidden_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,_, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># 按照句子长度从大到小排序</span></span><br><span class="line">        text, sorted_seq_lengths, desorted_indices = prepare_pack_padded_sequence(text, text_lengths)</span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        embedded = self.dropout(self.embedding(text))</span><br><span class="line">        <span class="comment"># embedded = [batch size,sent len,  emb dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_seq_lengths, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        <span class="comment"># output = [sent len, batch size, hidden dim * num_direction]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line">        <span class="comment"># 把句子序列再调整成输入时的顺序</span></span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        <span class="comment"># attention</span></span><br><span class="line">        <span class="comment"># M = [sent len, batch size, hidden dim * num_direction]</span></span><br><span class="line">        <span class="comment"># M = self.tanh1(output)</span></span><br><span class="line">        alpha = F.softmax(torch.matmul(self.tanh1(output), self.w), dim=<span class="number">0</span>).unsqueeze(<span class="number">-1</span>)  <span class="comment"># dim=0表示针对文本中的每个词的输出softmax</span></span><br><span class="line">        output_attention = output * alpha</span><br><span class="line"></span><br><span class="line">        batch_size, max_seq_len,hidden_dim = output.shape</span><br><span class="line">        hidden = torch.mean(torch.reshape(hidden,[batch_size,<span class="number">-1</span>,hidden_dim]),dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        output_attention = torch.sum(output_attention, dim=<span class="number">1</span>)</span><br><span class="line">        output = torch.sum(output, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        fc_input = self.dropout(output + output_attention + hidden)</span><br><span class="line">        <span class="comment"># fc_input = self.dropout(output_attention)</span></span><br><span class="line">        out = self.fc(fc_input)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h3 id="text-rcnn">TextRCNN</h3>
<h4 id="jian-jie-4">简介</h4>
<p>RNN和CNN作为文本分类问题的主要模型架构，都存在各自的优点及局限性。RNN擅长处理序列结构，能够考虑到句子的上下文信息，但RNN属于“biased model”，一个句子中越往后的词重要性越高，这有可能影响最后的分类结果，因为对句子分类影响最大的词可能处在句子任何位置。CNN属于无偏模型，能够通过最大池化获得最重要的特征，但是CNN的滑动窗口大小不容易确定，选的过小容易造成重要信息丢失，选的过大会造成巨大参数空间。为了解决二者的局限性，<a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552" target="_blank" rel="noopener">RCNN</a>这篇文章提出了一种新的网络架构，用双向循环结构获取上下文信息，这比传统的基于窗口的神经网络更能减少噪声，而且在学习文本表达时可以大范围的保留词序。其次使用最大池化层获取文本的重要部分，自动判断哪个特征在文本分类过程中起更重要的作用。</p>
<h4 id="mo-xing-jie-gou">模型结构</h4>
<p><img src="/2020/03/29/text_classification/rcnn.png" alt></p>
<h4 id="word-representation-learning">Word Representation Learning</h4>
<p>作者提出将单词的左上下文、右上下文、单词本身结合起来作为单词表示。作者使用了双向RNN来分别提取句子的上下文信息。公式如下:<br>
\[
\begin{array}{l}
c_{l}\left(w_{i}\right)=f\left(W^{(l)} c_{l}\left(w_{i-1}\right)+W^{(s l)} e\left(w_{i-1}\right)\right)  \\
c_{r}\left(w_{i}\right)=f\left(W^{(r)} c_{r}\left(w_{i+1}\right)+W^{(s r)} e\left(w_{i+1}\right)\right)
\end{array}
\]<br>
其中，\(c_l(w_i)\)代表单词\(w_i\)的左上下文，\(c_l(w_i)\)由上一个单词的左上下文\(c_l\)和\(c_l(w_{i-1})\)上一个单词的词嵌入向量 \(e(w_{i-1})\)计算得到，如公式（1）所示，所有句子第一个单词的左侧上下文使用相同的共享参数\(c_l(w_1)\)。 \(W^{(l)},W^{(sl)}\)用于将上一个单词的左上下文语义和上一个单词的语义结合到单词 \(w_i\)的左上下文表示中。右上下文的处理与左上下文完全相同，同样所有句子最后一个单词的右侧上下文使用相同的共享参数\(c_r(w_n)\)。 得到句子中每个单词的左上下文表示和右上下文表示后，就可以定义单词  \(w_i\)的表示如下<br>
\[
\boldsymbol{x}_{i}=\left[\boldsymbol{c}_{l}\left(w_{i}\right) ; \boldsymbol{e}\left(w_{i}\right) ; \boldsymbol{c}_{r}\left(w_{i}\right)\right]
\]</p>
<p>实际就是单词\(w_i\)，单词的词嵌入表示向量 \(e(w_i)\)以及单词的右上下文向量\(c_e(w_i)\) 的拼接后的结果。得到\(w_i\)的表示\(x_i\)后，就可以输入激活函数得到\(w_i\)的潜在语义向量 \(y_i^{(2)}\) 。<br>
\[
\boldsymbol{y}_{i}^{(2)}=\tanh \left(W^{(2)} \boldsymbol{x}_{i}+\boldsymbol{b}^{(2)}\right)
\]</p>
<h4 id="text-representation-learning">Text Representation Learning</h4>
<p>经过卷积层后，获得了所有词的表示，首先对其进行最大池化操作，最大池化可以帮助找到句子中最重要的潜在语义信息。<br>
\[
\boldsymbol{y}^{(3)}=\max _{i=1}^{n} \boldsymbol{y}_{i}^{(2)}
\]<br>
然后经过全连接层得到文本的表示，最后通过softmax层进行分类。<br>
\[
\begin{aligned}
&amp;\boldsymbol{y}^{(4)}=W^{(4)} \boldsymbol{y}^{(3)}+\boldsymbol{b}^{(4)}\\
&amp;p_{i}=\frac{\exp \left(\boldsymbol{y}_{i}^{(4)}\right)}{\sum_{k=1}^{n} \exp \left(\boldsymbol{y}_{k}^{(4)}\right)}
\end{aligned}
\]</p>
<h4 id="dai-ma-3">代码</h4>
<p><img src="/2020/03/29/text_classification/RCNN_network_structure.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RCNNModel</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, hidden_dim, output_dim, n_layers,bidirectional, dropout,word_embedding, freeze, batch_first=True)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.embedding_size ,</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.embedding_size ,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.embedding_size ,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.fc_cat = nn.Linear(hidden_dim * n_layers + self.embedding_size,self.embedding_size)</span><br><span class="line">        self.fc = nn.Linear(self.embedding_size, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,_, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># 按照句子长度从大到小排序</span></span><br><span class="line">        text, sorted_seq_lengths, desorted_indices = prepare_pack_padded_sequence(text, text_lengths)</span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        embedded = self.dropout(self.embedding(text))</span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_seq_lengths, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># packed_output</span></span><br><span class="line">        <span class="comment"># hidden [n_layers * bi_direction,batch_size,hidden_dim]</span></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        <span class="comment"># output [sent len, batch_size * n_layers * bi_direction]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line">        <span class="comment"># 把句子序列再调整成输入时的顺序</span></span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        <span class="comment"># output = [batch_size,seq_len,hidden_dim * num_directionns ]</span></span><br><span class="line">        batch_size, max_seq_len,hidden_dim = output.shape</span><br><span class="line">        <span class="comment"># 拼接左右上下文信息</span></span><br><span class="line">        output = torch.tanh(self.fc_cat(torch.cat((output, embedded), dim=<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">        output = torch.transpose(output,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">        output = F.max_pool1d(output,max_seq_len).squeeze().contiguous()</span><br><span class="line">        output = self.fc(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h3 id="han">HAN</h3>
<p><img src="/2020/03/29/text_classification/HAN.png" alt></p>
<p>整个网络结构包括五个部分：</p>
<ol>
<li>词序列编码器</li>
<li>基于词级的注意力层</li>
<li>句子编码器</li>
<li>基于句子级的注意力层</li>
<li>分类</li>
</ol>
<p>整个网络结构由双向GRU网络和注意力机制组合而成。</p>
<h4 id="ci-xu-lie-bian-ma-qi">词序列编码器</h4>
<p>给定一个句子中的单词\(w_{it}\)，其中 \(i\) 表示第\(i\) 个句子，\(t\) 表示第 \(t\) 个词。通过一个词嵌入矩阵 \(W_e\) 将单词转换成向量表示，具体如下所示：<br>
\[
x_{it}=W_e w_{it}
\]</p>
<p>利用双向GRU实现的整个编码流程：<br>
\[
\begin{aligned}
x_{i t} &amp;=W_{e} w_{i t}, t \in[1, T] \\
\overrightarrow{h}_{i t} &amp;=\overrightarrow{\operatorname{GRU}}\left(x_{i t}\right), t \in[1, T] \\
\overleftarrow{h}_{i t} &amp;=\overleftarrow{\operatorname{GRU}}\left(x_{i t}\right), t \in[T, 1] \\
{h}_{i t} &amp;= [\overrightarrow{h}_{i t},\overleftarrow{h}_{i t} ]
\end{aligned}
\]</p>
<h4 id="ci-ji-de-zhu-yi-li-ceng">词级的注意力层</h4>
<p>但是对于一句话中的单词，并不是每一个单词对分类任务都是有用的，比如在做文本的情绪分类时，可能我们就会比较关注“很好”、“伤感”这些词。为了能使循环神经网络也能自动将“注意力”放在这些词汇上，作者设计了基于单词的注意力层的具体流程如下：<br>
\[
\begin{aligned}
u_{i t} &amp;=\tanh \left(W_{w} h_{i t}+b_{w}\right) \\
\alpha_{i t} &amp;=\frac{\exp \left(u_{i t}^{\top} u_{w}\right)}{\sum_{t} \exp \left(u_{i t}^{\top} u_{w}\right)} \\
s_{i} &amp;=\sum_{t} \alpha_{i t} h_{i t}
\end{aligned}
\]<br>
上面式子中，\(u_{it}\) 是 \(h_{it}\) 的隐层表示，\(a_{it}\) 是经 softmax 函数处理后的归一化权重系数，\(u_w\)是一个随机初始化的向量，之后会作为模型的参数一起被训练，\(s_i\) 就是我们得到的第 i 个句子的向量表示。</p>
<h4 id="ju-zi-bian-ma-qi">句子编码器</h4>
<p>句子编码器也是基于双向GRU实现编码的，<br>
\[
\begin{aligned}
&amp;\overrightarrow{h}_{i}=\overrightarrow{\operatorname{GRU}}\left(s_{i}\right), i \in[1, L]\\
&amp;\overleftarrow{h}_{i}=\overleftarrow{\operatorname{GRU}}\left(s_{i}\right), t \in[L, 1]
\end{aligned}
\]<br>
公式和词编码类似，最后的 \(h_i\) 也是通过拼接得到的.</p>
<h4 id="ju-zi-ji-zhu-yi-li-ceng">句子级注意力层</h4>
<p>注意力层的流程如下，和词级的一致:<br>
\[
\begin{aligned}
u_{i} &amp;=\tanh \left(W_{s} h_{i}+b_{s}\right) \\
\alpha_{i} &amp;=\frac{\exp \left(u_{i}^{\top} u_{s}\right)}{\sum_{i} \exp \left(u_{i}^{\top} u_{s}\right)} \\
v &amp;=\sum_{i} \alpha_{i} h_{i}
\end{aligned}
\]<br>
最后得到的向量\(v\) 就是文档的向量表示，这是文档的高层表示。接下来就可以用可以用这个向量表示作为文档的特征。</p>
<h4 id="fen-lei">分类</h4>
<p>使用最常用的softmax分类器对整个文本进行分类了<br>
\[
p=\operatorname{softmax}\left(W_{c} v+b_{c}\right)
\]<br>
损失函数<br>
\[
L=-\sum_{d} \log p_{d j}
\]</p>
<h4 id="dai-ma-4">代码</h4>
<p><img src="/2020/03/29/text_classification/HAN_network_structure.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HierAttNet</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,rnn_type, word_hidden_size, sent_hidden_size, num_classes, word_embedding,n_layers,bidirectional,batch_first,freeze,dropout)</span>:</span></span><br><span class="line">        super(HierAttNet, self).__init__()</span><br><span class="line">        self.word_embedding = word_embedding</span><br><span class="line">        self.word_hidden_size = word_hidden_size</span><br><span class="line">        self.sent_hidden_size = sent_hidden_size</span><br><span class="line">        self.word_att_net = WordAttNet(rnn_type,word_embedding, word_hidden_size,n_layers,bidirectional,batch_first,dropout,freeze)</span><br><span class="line">        self.sent_att_net = SentAttNet(rnn_type,sent_hidden_size, word_hidden_size,n_layers,bidirectional,batch_first,dropout, num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, batch_doc, _, text_lengths)</span>:</span></span><br><span class="line">        output_list = []</span><br><span class="line">        <span class="comment"># ############################ 词级 #########################################</span></span><br><span class="line">        <span class="keyword">for</span> idx,doc <span class="keyword">in</span> enumerate(batch_doc):</span><br><span class="line">            <span class="comment"># 把一篇文档拆成多个句子</span></span><br><span class="line">            doc = doc[:text_lengths[idx]]</span><br><span class="line">            doc_list = doc.cpu().numpy().tolist()</span><br><span class="line">            sep_index = [i <span class="keyword">for</span> i, num <span class="keyword">in</span> enumerate(doc_list) <span class="keyword">if</span> num == self.word_embedding.stoi[<span class="string">'[SEP]'</span>]]</span><br><span class="line">            sentence_list = []</span><br><span class="line">            <span class="keyword">if</span> sep_index:</span><br><span class="line">                pre = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> cur <span class="keyword">in</span> sep_index:</span><br><span class="line">                    sentence_list.append(doc_list[pre:cur])</span><br><span class="line">                    pre = cur</span><br><span class="line"></span><br><span class="line">                sentence_list.append(doc_list[cur:])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sentence_list.append(doc_list)</span><br><span class="line">            max_sentence_len = len(max(sentence_list,key=<span class="keyword">lambda</span> x:len(x)))</span><br><span class="line">            seq_lens = []</span><br><span class="line">            input_token_ids = []</span><br><span class="line">            <span class="keyword">for</span> sent <span class="keyword">in</span> sentence_list:</span><br><span class="line">                cur_sent_len = len(sent)</span><br><span class="line">                seq_lens.append(cur_sent_len)</span><br><span class="line">                input_token_ids.append(sent+[self.word_embedding.stoi[<span class="string">'PAD'</span>]]*(max_sentence_len-cur_sent_len))</span><br><span class="line">            input_token_ids = torch.LongTensor(np.array(input_token_ids)).to(batch_doc.device)</span><br><span class="line">            seq_lens = torch.LongTensor(np.array(seq_lens)).to(batch_doc.device)</span><br><span class="line">            word_output, hidden = self.word_att_net(input_token_ids,seq_lens)</span><br><span class="line">            <span class="comment"># word_output = [bs,hidden_size]</span></span><br><span class="line">            output_list.append(word_output)</span><br><span class="line"></span><br><span class="line">        max_doc_sent_num = len(max(output_list,key=<span class="keyword">lambda</span> x: len(x)))</span><br><span class="line">        batch_sent_lens = []</span><br><span class="line">        batch_sent_inputs = []</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ############################ 句子级 #########################################</span></span><br><span class="line">        <span class="keyword">for</span> doc <span class="keyword">in</span> output_list:</span><br><span class="line">            cur_doc_sent_len = len(doc)</span><br><span class="line">            batch_sent_lens.append(cur_doc_sent_len)</span><br><span class="line">            expand_doc = torch.cat([doc,torch.zeros(size=((max_doc_sent_num-cur_doc_sent_len),len(doc[<span class="number">0</span>]))).to(doc.device)],dim=<span class="number">0</span>)</span><br><span class="line">            batch_sent_inputs.append(expand_doc.unsqueeze(dim=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        batch_sent_inputs = torch.cat(batch_sent_inputs, <span class="number">0</span>)</span><br><span class="line">        batch_sent_lens = torch.LongTensor(np.array(batch_sent_lens)).to(doc.device)</span><br><span class="line">        output = self.sent_att_net(batch_sent_inputs,batch_sent_lens)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h3 id="bert">Bert</h3>
<h4 id="bert-1">BERT</h4>
<p><img src="/2020/03/29/text_classification/bert_gpt_elmo.png" alt></p>
<h5 id="task-1-mlm">Task 1: MLM</h5>
<p>由于BERT需要通过上下文信息，来预测中心词的信息，同时又不希望模型提前看见中心词的信息，因此提出了一种 Masked Language Model 的预训练方式，即随机从输入预料上 mask 掉一些单词，然后通过的上下文预测该单词，类似于一个完形填空任务。</p>
<p>在预训练任务中，15%的 Word Piece 会被mask，这15%的 Word Piece 中，80%的时候会直接替换为 [Mask] ，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token</p>
<ul>
<li>没有100%mask的原因
<ul>
<li>如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词</li>
</ul>
</li>
<li>加入10%随机token的原因
<ul>
<li>Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘</li>
<li>另外编码器不知道哪些词需要预测的，哪些词是错误的，因此被迫需要学习每一个token的表示向量</li>
</ul>
</li>
<li>另外，每个batchsize只有15%的单词被mask的原因，是因为性能开销的问题，双向编码器比单项编码器训练要更慢</li>
</ul>
<h5 id="task-2-nsp">Task 2: NSP</h5>
<p>仅仅一个MLM任务是不足以让 BERT 解决阅读理解等句子关系判断任务的，因此添加了额外的一个预训练任务，即 Next Sequence Prediction。</p>
<p>具体任务即为一个句子关系判断任务，即判断句子B是否是句子A的下文，如果是的话输出’IsNext‘，否则输出’NotNext‘。</p>
<p>训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在图4中的[CLS]符号中</p>
<h5 id="shu-ru">输入</h5>
<ul>
<li>Token Embeddings：即传统的词向量层，每个输入样本的首字符需要设置为[CLS]，可以用于之后的分类任务，若有两个不同的句子，需要用[SEP]分隔，且最后一个字符需要用[SEP]表示终止</li>
<li>Segment Embeddings：为[0,1][0,1]序列，用来在NSP任务中区别两个句子，便于做句子关系判断任务</li>
<li>Position Embeddings：与Transformer中的位置向量不同，BERT中的位置向量是直接训练出来的</li>
</ul>
<h5 id="fine-tunninng">Fine-tunninng</h5>
<p>对于不同的下游任务，我们仅需要对BERT不同位置的输出进行处理即可，或者直接将BERT不同位置的输出直接输入到下游模型当中。具体的如下所示：</p>
<ul>
<li>对于情感分析等单句分类任务，可以直接输入单个句子（不需要[SEP]分隔双句），将[CLS]的输出直接输入到分类器进行分类</li>
<li>对于句子对任务（句子关系判断任务），需要用[SEP]分隔两个句子输入到模型中，然后同样仅须将[CLS]的输出送到分类器进行分类</li>
<li>对于问答任务，将问题与答案拼接输入到BERT模型中，然后将答案位置的输出向量进行二分类并在句子方向上进行softmax（只需预测开始和结束位置即可）</li>
<li>对于命名实体识别任务，对每个位置的输出进行分类即可，如果将每个位置的输出作为特征输入到CRF将取得更好的效果。</li>
</ul>
<h5 id="que-dian-2">缺点</h5>
<ul>
<li>BERT的预训练任务MLM使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务</li>
<li>另外，BERT没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计</li>
<li>由于最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）</li>
<li>适合处理自然语义理解类任务(NLU)，而不适合自然语言生成类任务(NLG)</li>
</ul>
<h5 id="dai-ma-5">代码</h5>
<h6 id="bert-2">Bert</h6>
<figure class="highlight python"><figcaption><span>bert</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bert</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bert_path, num_classes, word_embedding, trained=True)</span>:</span></span><br><span class="line">        super(Bert, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="comment"># 不对bert进行训练</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.bert.parameters():</span><br><span class="line">            param.requires_grad = trained</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>], num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, context, bert_masks, seq_lens)</span>:</span></span><br><span class="line">        <span class="comment"># context  输入的句子序列</span></span><br><span class="line">        <span class="comment"># seq_len  句子长度</span></span><br><span class="line">        <span class="comment"># mask     对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        <span class="comment"># cls [batch_size, 768]</span></span><br><span class="line">        <span class="comment"># sentence [batch size,sen len,  768]</span></span><br><span class="line">        sentence, cls = self.bert(context, attention_mask=bert_masks)</span><br><span class="line">        sentence = torch.sum(sentence,dim=<span class="number">1</span>)</span><br><span class="line">        out = self.fc(sentence)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h6 id="bert-cnn">BertCNN</h6>
<figure class="highlight python"><figcaption><span>BertCNN</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertCNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bert_path, num_filters, hidden_size, filter_sizes, dropout, num_classes, word_embedding,</span></span></span><br><span class="line"><span class="function"><span class="params">                 trained=True)</span>:</span></span><br><span class="line">        super(BertCNN, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.bert.parameters():</span><br><span class="line">            param.requires_grad = trained</span><br><span class="line">        self.convs = nn.ModuleList(</span><br><span class="line">            [nn.Conv2d(<span class="number">1</span>, num_filters, (k, hidden_size)) <span class="keyword">for</span> k <span class="keyword">in</span> filter_sizes])</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.fc_cnn = nn.Linear(num_filters * len(filter_sizes), num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conv_and_pool</span><span class="params">(self, x, conv)</span>:</span></span><br><span class="line">        x = F.relu(conv(x)).squeeze(<span class="number">3</span>)</span><br><span class="line">        x = F.max_pool1d(x, x.size(<span class="number">2</span>)).squeeze(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, context, bert_masks, seq_len)</span>:</span></span><br><span class="line">        <span class="comment"># context    输入的句子</span></span><br><span class="line">        <span class="comment"># mask  对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        encoder_out, text_cls = self.bert(context, attention_mask=bert_masks)</span><br><span class="line">        out = encoder_out.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        out = torch.cat([self.conv_and_pool(out, conv) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs], <span class="number">1</span>)</span><br><span class="line">        out = self.dropout(out)</span><br><span class="line">        out = self.fc_cnn(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h6 id="bert-rnn">BertRNN</h6>
<figure class="highlight python"><figcaption><span>BertRNN</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, bert_path, hidden_dim, n_layers, bidirectional, batch_first, word_embedding,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout, num_classes, trained)</span>:</span></span><br><span class="line">        super(BertRNN, self).__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.bert.parameters():</span><br><span class="line">            param.requires_grad = trained</span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                               hidden_size=hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.fc_rnn = nn.Linear(hidden_dim * <span class="number">2</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, bert_masks, seq_lens)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        <span class="comment"># context    输入的句子</span></span><br><span class="line">        <span class="comment"># mask  对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        bert_sentence, bert_cls = self.bert(text, attention_mask=bert_masks)</span><br><span class="line">        sentence_len = bert_sentence.shape[<span class="number">1</span>]</span><br><span class="line">        bert_cls = bert_cls.unsqueeze(dim=<span class="number">1</span>).repeat(<span class="number">1</span>, sentence_len, <span class="number">1</span>)</span><br><span class="line">        bert_sentence = bert_sentence + bert_cls</span><br><span class="line">        <span class="comment"># 按照句子长度从大到小排序</span></span><br><span class="line">        bert_sentence, sorted_seq_lengths, desorted_indices = prepare_pack_padded_sequence(bert_sentence, seq_lens)</span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(bert_sentence, sorted_seq_lengths,</span><br><span class="line">                                                            batch_first=self.batch_first)</span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output = [ batch size,sent len, hidden_dim * bidirectional]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        batch_size, max_seq_len, hidden_dim = output.shape</span><br><span class="line">        hidden = torch.mean(torch.reshape(hidden, [batch_size, <span class="number">-1</span>, hidden_dim]), dim=<span class="number">1</span>)</span><br><span class="line">        output = torch.sum(output, dim=<span class="number">1</span>)</span><br><span class="line">        fc_input = self.dropout(output + hidden)</span><br><span class="line">        out = self.fc_rnn(fc_input)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h6 id="bert-rcnn">BertRCNN</h6>
<figure class="highlight python"><figcaption><span>BertRCNN</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertRCNN</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, bert_path, hidden_dim, n_layers, bidirectional, dropout, num_classes, word_embedding,</span></span></span><br><span class="line"><span class="function"><span class="params">                 trained, batch_first=True)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.bert.parameters():</span><br><span class="line">            param.requires_grad = trained</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.maxpool = nn.MaxPool1d()</span></span><br><span class="line">        self.fc = nn.Linear(hidden_dim * n_layers, num_classes)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, bert_masks, seq_lens)</span>:</span></span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        <span class="comment"># context    输入的句子</span></span><br><span class="line">        <span class="comment"># mask  对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        bert_sentence, bert_cls = self.bert(text, attention_mask=bert_masks)</span><br><span class="line">        sentence_len = bert_sentence.shape[<span class="number">1</span>]</span><br><span class="line">        bert_cls = bert_cls.unsqueeze(dim=<span class="number">1</span>).repeat(<span class="number">1</span>, sentence_len, <span class="number">1</span>)</span><br><span class="line">        bert_sentence = bert_sentence + bert_cls</span><br><span class="line">        <span class="comment"># 按照句子长度从大到小排序</span></span><br><span class="line">        bert_sentence, sorted_seq_lengths, desorted_indices = prepare_pack_padded_sequence(bert_sentence, seq_lens)</span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(bert_sentence, sorted_seq_lengths,</span><br><span class="line">                                                            batch_first=self.batch_first)</span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        batch_size, max_seq_len, hidden_dim = output.shape</span><br><span class="line">        out = torch.transpose(output.relu(), <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        out = F.max_pool1d(out, max_seq_len).squeeze()</span><br><span class="line">        out = self.fc(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h6 id="xlnet">xlnet</h6>
<figure class="highlight python"><figcaption><span>xlnet</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XLNet</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, xlnet_path, num_classes, word_embedding, trained=True)</span>:</span></span><br><span class="line">        super(XLNet, self).__init__()</span><br><span class="line">        self.xlnet = XLNetModel.from_pretrained(xlnet_path)</span><br><span class="line">        <span class="comment"># 不对bert进行训练</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.xlnet.parameters():</span><br><span class="line">            param.requires_grad = trained</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(self.xlnet.d_model, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, context, xlnet_masks, seq_lens)</span>:</span></span><br><span class="line">        <span class="comment"># context  输入的句子序列</span></span><br><span class="line">        <span class="comment"># seq_len  句子长度</span></span><br><span class="line">        <span class="comment"># mask     对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        <span class="comment"># cls [batch_size, 768]</span></span><br><span class="line">        <span class="comment"># sentence [batch size,sen len,  768]</span></span><br><span class="line">        sentence_encoder = self.xlnet(context, attention_mask=xlnet_masks)</span><br><span class="line">        sentence_encoder = torch.sum(sentence_encoder[<span class="number">0</span>], dim=<span class="number">1</span>)</span><br><span class="line">        out = self.fc(sentence_encoder)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h4 id="bert-config-json">bert_config.json</h4>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "attention_probs_dropout_prob": 0.1, #乘法attention时，softmax后dropout概率 </span><br><span class="line">  "directionality": "bidi", </span><br><span class="line">  "hidden_act": "gelu",   #激活函数 </span><br><span class="line">  "hidden_dropout_prob": 0.1, #隐藏层dropout概率 </span><br><span class="line">  "hidden_size": 768, #隐藏单元数 </span><br><span class="line">  "initializer_range": 0.02, #初始化范围 </span><br><span class="line">  "intermediate_size": 3072, #升维维度</span><br><span class="line">  "max_position_embeddings": 512, #一个大于seq_length的参数，用于生成position_embedding</span><br><span class="line">  "num_attention_heads": 12,#每个隐藏层中的attention head数 </span><br><span class="line">  "num_hidden_layers": 2, #隐藏层数 </span><br><span class="line">  "pooler_fc_size": 768, </span><br><span class="line">  "pooler_num_attention_heads": 12, </span><br><span class="line">  "pooler_num_fc_layers": 3, </span><br><span class="line">  "pooler_size_per_head": 128, </span><br><span class="line">  "pooler_type": "first_token_transform", </span><br><span class="line">  "type_vocab_size": 2, #segment_ids类别 [0,1] </span><br><span class="line">  "vocab_size": 21128#词典中词数</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="dui-bi">对比</h1>
<h2 id="wei-bo-qing-gan-fen-lei">微博情感分类</h2>
<h3 id="shu-ju">数据</h3>
<p>weibo_senti_100k：共119988条数据，正例：59993,负例59995</p>
<p>句子最大长度：260，最小长度：3，平均长度：66.04</p>
<p>部分样例:</p>
<table>
<thead>
<tr>
<th>label</th>
<th>review</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]</td>
</tr>
<tr>
<td>1</td>
<td>@张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心，酒店都全部OK啦。</td>
</tr>
<tr>
<td>1</td>
<td>姑娘都羡慕你呢…还有招财猫高兴……//@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢//@李欣芸SharonLee:大佬范儿[书呆子]</td>
</tr>
<tr>
<td>1</td>
<td>美~~~~~[爱你]</td>
</tr>
<tr>
<td>1</td>
<td>梦想有多大，舞台就有多大![鼓掌]</td>
</tr>
<tr>
<td>0</td>
<td>今天真冷啊，难道又要穿棉袄了[晕]？今年的春天真的是百变莫测啊[抓狂]</td>
</tr>
<tr>
<td>0</td>
<td>[衰][衰][衰]像给剥了皮的蛇</td>
</tr>
<tr>
<td>0</td>
<td>酒驾的危害，这回是潜水艇。//@上海译文丁丽洁:[泪]</td>
</tr>
<tr>
<td>0</td>
<td>积压了这么多的枕边书，没一本看完了的，现在我读书的最佳地点尽然是公交车[晕]</td>
</tr>
<tr>
<td>0</td>
<td>[泪]错过了……</td>
</tr>
</tbody>
</table>
<p><a href="https://github.com/Embedding/Chinese-Word-Vectors" target="_blank" rel="noopener">词向量下载</a></p>
<p><a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">中文预训练bert模型下载</a></p>
<h3 id="fen-xi-ji-bi-jiao">分析及比较</h3>
<p>分成三类进行比较：</p>
<ol>
<li>FastText，TextCNN，DPCNN，RNN，RNN-Attention，TextRCNN(词向量/字向量)</li>
<li>RNN，LSTM，GRU，RNN-Attention</li>
<li>Bert，BertCNN，BertRNN，BertRCNN</li>
</ol>
<h4 id="fast-text-text-cnn-dpcnn-rnn-rnn-attention-text-rcnn">FastText，TextCNN，DPCNN，RNN，RNN-Attention，TextRCNN</h4>
<p>训练集上（词向量）的表现：</p>
<p><img src="/2020/03/29/text_classification/image-20200520140510805.png" alt></p>
<p>训练集上的速度：</p>
<p><img src="/2020/03/29/text_classification/image-20200520140848340.png" alt></p>
<p>验证集上的表现：</p>
<p><img src="/2020/03/29/text_classification/image-20200520135717244.png" alt></p>
<p>验证集上的速度</p>
<p><img src="/2020/03/29/text_classification/image-20200520140931559.png" alt="image-20200520140931559"></p>
<ol>
<li>从验证集上的表现来看，rcnn的表现比较稳定，0.985，但是其训练以及预测速度却是最慢的一个。FastText是速度最快的一个，在验证集上也能取得0.961的表现。可以根据任务的需求进行取舍。</li>
<li>对于FastText 来说，embedding的训练一定要打开。</li>
<li>对于过拟合现象，可以通过freeze word embedding 来缓解。</li>
<li>是使用字向量还是词向量？
<ul>
<li>对于FastText来说，使用词向量会比使用字向量精确度高出1%左右。原因就向上面FastText的缺点所述部分，使用词向量的时候会添加额外的<code>n-gram</code>信息。</li>
<li>对于TextCNN来说，网络自身就能够提取<code>n-gram</code>特征，如果再使用词向量，对于短文本来说，句子信息被压缩，容易出现过拟合现象（DPCNN同样出现过拟合）。在短文本的数据集上，TextCNN还是使用字向量比较好。</li>
<li>对于rnn来说，本身就存在梯度弥散和梯度爆炸的问题，所以使用词向量，使得句子序列会变长，会加剧这个问题。对于lstm来说也是同样的。</li>
<li>对于rcnn来说，使用词向量还是字向量基本没有任何区别。</li>
<li>对于加attention的rnn，每个时间步会attention到整个序列的word embedding，所以词向量或者字向量带来的影响并不明显。</li>
</ul>
</li>
</ol>
<h4 id="rnn-lstm-gru">RNN，LSTM，GRU</h4>
<p>训练集和验证集上的表现：</p>
<p><img src="/2020/03/29/text_classification/image-20200520144027325.png" alt></p>
<p>速度比较：</p>
<p><img src="/2020/03/29/text_classification/image-20200520144139492.png" alt></p>
<ol>
<li>速度上相差无几，能用lstm就用lstm把。</li>
<li>不要仅仅使用rnn最后输出的hidden来做分类。（如果只使用hidden来做分类，准确度50%.）</li>
<li>是使用sum求和来获取整句话的语义还是使用mean来获取整句话的语义其实影响不大。</li>
<li>在rnn上使用attention 精度上会略有提升，但是相比于速度的下降，感觉有些得不偿失，如果追求精度可以加上attention。</li>
</ol>
<h4 id="bert-bert-cnn-bert-rnn-bert-rcnn">Bert，BertCNN，BertRNN，BertRCNN</h4>
<p><img src="/2020/03/29/text_classification/image-20200521150254593.png" alt></p>
<ol>
<li>通常来说bert的模型的train不用打开，如果打开，在bert后面接的层的学习率应大于bert学习率一两个数量级，使得后面的层得到充分的训练。</li>
<li>bert模型本身就可以达到98.3%左右的精确度，在后面添加其他模型看不出效果。</li>
</ol>
<h2 id="cnews-xin-wen-shi-fen-lei-jie-guo-bi-jiao">Cnews新闻十分类结果比较</h2>
<h3 id="shu-ju-1">数据</h3>
<p>类别：‘体育’ ‘娱乐’ ‘家居’ ‘房产’ ‘教育’ ‘时尚’ ‘时政’ ‘游戏’ ‘科技’ ‘财经’<br>
训练集：50000 条数据，最大长度：27467，最小长度：8，类别个数：10,平均长度：913.31<br>
验证集：5000 条数据，最大长度：10919，最小长度：15，类别个数：10<br>
测试集：10000 条数据，最大长度：14720，最小长度：13，类别个数：10</p>
<p>使用数据集<a href="https://pan.baidu.com/s/1OOTK374IZ1DHnz5COUcfbw" target="_blank" rel="noopener">cnews</a>  密码:hf6o</p>
<p>训练集部分样例及每个类别的统计：</p>
<table>
<thead>
<tr>
<th>label</th>
<th>text</th>
<th>数量</th>
</tr>
</thead>
<tbody>
<tr>
<td>体育</td>
<td>黄蜂vs湖人首发：科比带伤战保罗 加索尔救赎之战 新浪体育讯北京时间4月27日，NBA季后赛首轮洛杉矶湖人主场迎战新奥尔良黄蜂，此前的比赛中，双方战成2-2平，因此本场比赛对于两支球队来说都非常重要，赛前双方也公布了首发阵容：湖人队：费舍尔、科比、阿泰斯特、加索尔、拜纳姆黄蜂队：保罗、贝里内利、阿里扎、兰德里、奥卡福[新浪NBA官方微博][新浪NBA湖人新闻动态微博][新浪NBA专题]<a href="%E6%96%B0%E6%B5%AA%E4%BD%93%E8%82%B2">黄蜂vs湖人图文直播室</a></td>
<td>5000</td>
</tr>
<tr>
<td>娱乐</td>
<td>皮克斯首部3D动画《飞屋历险记》预告发布(图)视频：动画片《飞屋历险记》先行版43秒预告新浪娱乐讯 迪士尼、皮克斯2009暑期3D动画力作《飞屋历险记》(Up)发布预告片，虽然这款预告片仅有43秒，并且只出现了被汽球吊起来的房屋，但门前老爷爷卡尔的一声“下午好”着实让人忍俊不禁。该片由《怪兽电力公司》导演彼特·道格特(Pete Docter)执导，曾在《海底总动员》、《料理鼠王》担任编剧的皮克斯老班底鲍勃-派特森(Bob Peterson)亦将在本片担任共同导演，献出自己的导演处女作。《飞屋历险记》同时会是皮克斯有史以来第一部以3-D电影呈现的里程碑作品，之后皮克斯的所有影片都将制作成立体电影。《飞屋历险记》讲述了一老一少的冒险旅程。78岁的老翁卡尔·弗雷德里克森(Carl Fredricksen)一生中都梦想着能环游世界、出没于异境险地体验，却平淡地渡过了一生。在他生活的最后阶段，卡而仿佛在命运的安排下，带着8岁的亚裔小鬼头Russell一同踏上了冒险的旅程。这对一老一小的奇特组合肩并肩闯荡江湖，共同经历了荒野跋涉、丛林野兽与反派坏蛋的狙击。田野/文</td>
<td>5000</td>
</tr>
<tr>
<td>家居</td>
<td>橱柜价格关键在计价方式 教你如何挑选买过橱柜的人都知道，橱柜的计价很复杂，商家的报价方式也不尽相同，那么哪种计价方式对消费者更有利？在计价过程中应该注意哪些问题？消费者在购买橱柜之前一定要了解清楚。 橱柜的主要计价方式——延米计价和单元柜体计价 现在市场上橱柜主要有两种计价方式——延米计价和单元柜体计价。 延米计价是指地柜和吊柜各一米的总价(有些还包含台面)。在此基础上，如果有局部区域只要地柜不要吊柜，就会按就会按“2/8”或“4/6”的比例折算。如某橱柜材料的延米价为2000元/延米，某顾客做2米的吊柜、4米的地柜，则吊柜价=2000X0.4X2=1600元，地柜价=2000X0.6X4=4800元(此吊柜、地柜价按4/6的比例计算)，再加上所选台面、配件、电器等附加费用即为整套橱柜的价格。 延米报价有许多不合理之处，水槽、燃气灶、嵌入式电器等部分所需门板很少，但仍按延米来算价，对消费者来说很不划算。例如一款1000元/延米的橱柜，一个水槽约0.8米长，但消费者还是要按1000元的单价乘以0.8米付费，这个实际上只是几块材料简单组合的水槽柜需要消费者花800元，而同样材质、同样大小的水槽柜仅需400元左右，二者价格相差数百元。 按延米计价，所有的配件费用都是在原有的基础上增加，虽然有些厂家宣称抽屉、拉篮不加钱，但其实那是最基本的配置，一旦顾客要求调整方案，就会要多加钱，此外不足一米的部分要按一米计价，因此对顾客来说，如此计价会多花不少冤枉钱。 “单元柜体计价”是国际惯例的橱柜计价方式，是按每一个组成厨柜的单元柜价格进行计算，然后加出总价。具体为：某吊柜单价×个数+某地柜单价×个数……。利用单元柜体计价，更为合理。举个例子说，外观相同的柜体，抽屉数量、五金件、托架数量如果不同，在以延米计价时，商家往往只给消费者最简单、最省成本的产品。而按单元柜体计价，一款尺寸相同的抽屉柜可按不同配置报出不同价格：同样是一款30cm宽、66cm高的单体柜，如果门改成弧型，是多少钱；如果抽屉里加上可拆装的金属篮，是多少钱；如果抽屉的侧板是木质的多少钱……把橱柜的每个细节都分解开来，消费者可以在预算之内把可有可无的配置省掉，把钱花在自己更需要的功能上。 两全其美报价方式——延米计价和单元柜体计价相结合 现在中国橱柜市场上仍普遍采用延米计价，但进口品牌及国内一些大品牌橱柜都采用单元柜体计价方式，如德宝·西克曼、海尔等品牌即是采用单元柜体计价方式。不过德宝·西克曼厨柜的工作人员介绍到，如果一开始就用单元柜体计价来进行报价，不够直观，同时为了便于顾客进行比较，他们会用延米计价给顾客所选定的材料进行一个初始报价，让顾客对自己的厨房装修要花多少钱心里大概有个底。在对厨房进行量尺后，设计师会按照顾客的需求，设计出厨房效果图。这时，销售人员会按单元柜体计价给顾客进行一个报价。对于每一种标准柜体都有相应的报价，顾客实际用到几组柜子，将这些柜子价格累加，再加上台面及其他相关费用，便是整个橱柜的价格。</td>
<td>5000</td>
</tr>
<tr>
<td>房产</td>
<td>冯仑：继续增持高GDP城市商业地产确立商业地产投资战略不久的万通地产(企业专区,旗下楼盘)(600246)，今年上半年遭遇了业绩下滑。公司昨日公布的半年报显示，其商业物业在报告期内实现的营业收入同比下降33.71%，营业利润率比上年同期下降47.29个百分点。不过，公司董事长冯仑日前表示，依然看好人均GDP8000美元以上城市的商业地产，万通将继续增加高GDP城市的商业地产；计划用5-10年，商业物业收入占比达到30%-50%。逆向运作地产投资冯仑指出，根据历史经验，GDP的增长、城市化的增长，和房地产物业形态有一定关系，即人均GDP在8000美元以下时，住宅是市场的核心，主流产品都将围绕住宅展开。目前，在中国的城市中，人均GDP8000美元的城市大约有十个，大部分省会城市依然在3000美元至5000美元之间，因此，未来5-10年，中国房地产市场的产品结构仍然是以住宅为主。 冯仑认为，万通地产从现在开始扩大商业地产的比重，在目前的市场中，是一种逆向运作的思维，但符合长期趋势。他指出，在人均GDP达到8000美元的经济实体中，商用不动产会成为地产业的主角。以美国为例，商业地产的市场规模大约是住宅的两倍。中国商业地产未来的市场空间很大。根据万通地产的发展战略，除了在环渤海区域内发展住宅以外，还会重点发展商业不动产。未来，公司业务结构将逐步调整，商用地产的收入会逐年增加；今后，公司商业物业收入将占到整体营业收入的一半左右。对于目前商业地产面临的不景气局面，万通地产董事会秘书程晓?指出，公司战略不会因市场的短期波动而改变，公司将继续加大商用物业项目的投资力度，以营运带动开发，以财务安排的多样化实施商用物业投资。改变商业模式冯仑表示，就房地产开发模式而言，过去两百年主要经历了三次变化，即从“地主加工头”到“厂长加资本家”，再到“导演加制片”。目前，国内多数地产商的开发模式属于“地主加工头”和“厂长加资本家”的阶段；而商业地产的开发模式，不能停留在这两个阶段。所谓“导演加制片”模式，即由专业的房地产投资和资产管理公司负责运营商业地产项目，实现收入的多元化。而这种模式需要相应的金融创新产品支持。业内人士指出，房地产金融领域内的REITS、抵押贷款等金融产品体系的完善，将支持商用地产在一个多元化的不动产经营环境中快速的成长。而商业模式的改变需要较长一段时间。数据显示，香港主流房地产企业在人均GDP10000美元的时候开始逐步发展商业地产，先后经过13-15年确立起新的商业模式。其中，长江实业经过13年的发展，商业地产在业务机构的比重占到30%，新鸿基则经过15年的调整，商业地产比重占到50%。SOHO中国(企业专区,旗下楼盘)董事长潘石屹也指出，现在的市场虽然在调整，不过也给从事商业地产开发的企业提供了良好机会和平台，应及时在地域、开发物业的品种、品牌的建设、销售和持有物业的比重四个方面做出调整。 我要评论</td>
<td>5000</td>
</tr>
<tr>
<td>教育</td>
<td>2010年6月英语六级考试考后难度调查2010年6月大学英语六级考试将于19日下午举行，欢迎各位考生在考试后参加难度调查，发表你对这次考试的看法。点击进入论坛，参与考后大讨论</td>
<td>5000</td>
</tr>
<tr>
<td>时尚</td>
<td>组图：萝莉潮人示范春季复古实穿导语：萝莉潮人示范春季复古实穿，在乍暖还寒的初春，有的甜美、有的优雅、有的性感，但无论是哪种风格都给人强烈的视觉冲击力，在这个缤纷的春季更加脱俗动人。</td>
<td>5000</td>
</tr>
<tr>
<td>时政</td>
<td>香港特区行政长官曾荫权将离港休假中新社香港八月七日电 香港特区行政长官曾荫权将于八月九日至十五日离港休假。特区政府发言人七日透露，曾荫权离港期间，八月九日由特区财政司司长曾俊华署理行政长官职务；八月十日至十五日由政务司司长唐英年署理行政长官职务。(完)</td>
<td>5000</td>
</tr>
<tr>
<td>游戏</td>
<td>全国人大常委会将对59件法律相关条文作出修改新华社快讯：全国人大常委会27日表决通过了关于修改部分法律的决定，对59件法律的相关条文作出修改。</td>
<td>5000</td>
</tr>
<tr>
<td>科技</td>
<td>入门级时尚卡片机 尼康S220套装仅1150尼康S220延续了S系列纤巧超薄的机身设计，采用铝合金材质打造，表面质地细腻，不易沾染指纹。S220拥有紫色、深蓝、洋红、水晶绿和柔银五款靓丽颜色可供选择。</td>
<td>5000</td>
</tr>
</tbody>
</table>
<h3 id="fen-xi-bi-jiao">分析比较</h3>
<p>训练集上的表现（字向量）（序列长度：2000）：</p>
<p><img src="/2020/03/29/text_classification/image-20200526204848210.png" alt="各种模型"></p>
<p><img src="/2020/03/29/text_classification/image-20200526205351509.png" alt="rnn/cnn-attention/fast_text"></p>
<p><img src="/2020/03/29/text_classification/image-20200526205849399.png" alt="speed"></p>
<p><img src="/2020/03/29/text_classification/image-20200526210508245.png" alt="蓝色为字向量/绿色为词向量"></p>
<ol>
<li>rnn在面对长文本时直接崩溃了（感觉自己说了一个废话）。再给rnn加上attention之后，rnn得到了救赎，但是效果和FastText基本持平。由此推断lstm+attention 能够获得一个相对较好的结果（如果不考虑速度的话）。</li>
<li>从speed图来看，最快的当然是FastText，可以并行的CNN处于第二梯队，最后的是RNN系列的模型。</li>
<li>词向量还是字向量：
<ol>
<li>对于FastText来说，在长文本上，词向量的表现要远远好于字向量。</li>
<li>可能是文本长度过长了吧，实验结果表明：在长文本数据上，词向量的表现要好于字向量，对于长文本分类来说，整句话的语义要高于某些特定的词吧。</li>
<li>对于rnn系列来说，毫无疑问，词向量的表现远好于字向量，因为句子序列长度变短了。</li>
<li>rcnn模型一般来说不会比rnn，或者cnn表现差。</li>
</ol>
</li>
<li>xlnet由于显存的原因，序列长度只取到1500（只要你显存够，在一定意义上来说是解决了bert的长度限制的问题）。但是在11g单卡上，batch_size=2，跑完一步，FastText可以跑一个epoch。不知道知识蒸馏效果会怎样。</li>
</ol>
<h2 id="wen-ben-duo-biao-qian-fen-lei">文本多标签分类</h2>
<p>（先挖一个坑）</p>
<h2 id="fen-xi">分析</h2>
<h4 id="chang-duan-wen-ben-fen-lei-de-bi-jiao">长短文本分类的比较</h4>
<p>对于词嵌入技术的文本表示，短文本和长文本表示上没有差别，此时分类效果的优劣主要在分类模型和训练数据上。</p>
<p>对于数据而言：随着文本越长，语义的重要性就越高，在文本很短的情况下，语义的重要性就很小，比如：“今天 天气 怎么样”，“今天 怎么样 天气”，“怎么样 天气 今天”。你甚至不必要考虑句子是否通顺，基本上可以当一句话处理，没有第二个意思。但是随着文本越来越长，比如512个字符，颠倒一下可能就要归为两类了。</p>
<p>对于模型而言：对于短文本，CNN配合Max-pooling池化(如TextCNN模型)速度快，而且效果也很好。因为短文本上的关键词比较容易找到，而且Max-pooling会直接过滤掉模型认为不重要特征。具体工作机制是：卷积窗口沿着长度为n的文本一个个滑动，类似于n-gram机制对文本切词，然后和文本中的每个词进行相似度计算，因为后面接了个Max-pooling，因此只会保留和卷积核最相近的词。微博数据集属于情感分类，为了判断句子的情感极性，只需要让分类器能识别出“不开心”这类词是个负极性的词，“高兴”、“开心”等这类词是正极性的词，其他词是偏中性词就可以了。因此，当把该句子中的各个词条输入给模型去分类时，并不需要去“瞻前顾后”，因此使用一个关注局部的前馈神经网络往往表现更佳。虽然Attention也突出了重点特征，但是难以过滤掉所有低分特征。但是对于长文本直接用CNN就不行了，TextCNN会比HAN模型泛化能力差很多。<strong>如果在TextCNN前加一层LSTM，这样效果可以提升很大</strong>。</p>
<h4 id="wei-shi-yao-chang-wen-ben-fen-lei-de-shi-yan-zhong-cnn-he-rnn-mei-you-la-kai-chai-ju">为什么长文本分类的实验中，cnn 和 rnn 没有拉开差距？</h4>
<p>cnn和rnn的精度都很高，分析主要还是分类的文章规则性比较强，且属于特定领域，词量不多，类别差异可能比较明显。</p>
<h1 id="wen-ben-fen-lei-tricks">文本分类tricks</h1>
<h2 id="fen-ci-qi">分词器</h2>
<p><strong>分词器所分出的词与词向量表中的token粒度match是更重要的事情</strong></p>
<h2 id="yi-zhi-yu-xun-lian-ci-xiang-liang-de-fen-ci-qi">已知预训练词向量的分词器</h2>
<p>像word2vec、glove、fasttext这些官方release的预训练词向量都会公布相应训练语料的信息，包括预处理策略如分词，这种情况下直接使用官方的训练该词向量所使用的分词器，此分词器在下游任务的表现十之八九会比其他花里胡哨的分词器好用。</p>
<h2 id="bu-zhi-dao-yu-xun-lian-ci-xiang-liang-de-fen-ci-qi">不知道预训练词向量的分词器</h2>
<p>这时就需要去“猜”一下分词器。怎么猜呢？<br>
首先，拿到预训练词向量表后，去里面search一些特定词汇比如一些网站、邮箱、成语、人名等，英文里还有n’t等，看看训练词向量使用的分词器是把它们分成什么粒度。<br>
然后跑几个分词器，看看哪个分词器的粒度跟他最接近就用哪个，如果不放心，就放到下游任务里跑跑看。</p>
<p>最理想的情况是：先确定最适合当前任务数据集的分词器，再使用同分词器产出的预训练词向量。如果无法满足理想情况，则需要自己在下游任务训练集或者大量同分布无监督语料上训练的词向量更有利于进一步压榨模型的性能。</p>
<h2 id="guan-yu-zhong-wen-zi-xiang-liang">关于中文字向量</h2>
<p>预训练中文字向量的时候，把窗口开大一些，不要直接使用word-level的窗口大小，效果会比随机初始化的字向量明显的好。</p>
<h2 id="shu-ju-ji-zao-sheng-hen-yan-zhong">数据集噪声很严重</h2>
<p>里噪声严重有两种情况。对于数据集D(X, Y)，一种是X内部噪声很大（比如文本为口语化表述或由互联网用户生成），一种是Y的噪声很大（一些样本被明显的错误标注，一些样本人也很难定义是属于哪一类，甚至具备类别二义性）。</p>
<h2 id="x-nei-bu-zao-sheng-hen-da">X内部噪声很大</h2>
<p>法一：直接将模型的输入变成char-level（中文中就是字的粒度），然后train from scratch（不使用预训练词向量）去跟word-level的对比一下，如果char-level的明显的效果好，那么短时间之内就直接基于char-level去做模型。</p>
<p>法二：使用特殊超参的FastText去训练一份词向量：<br>
一般来说fasttext在英文中的char ngram的窗口大小一般取值3～6，但是在处理中文时，如果我们的目的是为了去除输入中的噪声，那么我们可以把这个窗口限制为1～2，这种小窗口有利于模型去捕获错别字（比如，我们打一个错误词的时候，一般都是将其中的一个字达成同音异形的另一个字），比如word2vec学出来的“似乎”的最近词可能是“好像”，然而小ngram窗口fasttext学出来的“似乎”最近词则很有可能是“是乎”等内部包含错别字的词，这样就一下子让不太过分的错别字构成的词们又重新回到了一起，甚至可以一定程度上对抗分词器产生的噪声（把一个词切分成多个字）。</p>
<h2 id="y-de-zao-sheng-hen-da">Y的噪声很大</h2>
<p>首先忽略这个噪声，强行的把模型尽可能好的训出来。然后让训练好的模型去跑训练集和开发集，取出训练集中的错误样本和开发集中那些以很高的置信度做出错误决策的样本（比如以99%的把握把一个标签为0的样本预测为1），然后去做这些bad cases的分析，如果发现错误标注有很强的规律性，则直接撸一个脚本批量纠正一下（只要确保纠正后的标注正确率比纠正前明显高就行）。<br>
如果没有什么规律，但是发现模型高置信度做错的这些样本大部分都是标注错误的话，就直接把这些样本都删掉，常常也可以换来性能的小幅提升，毕竟测试集都是人工标注的，困难样本和错标样本不会太多。</p>
<h2 id="baseline-xuan-yong-cnn-huan-shi-rnn">baseline选用CNN还是RNN？</h2>
<p>看数据集，如果感觉数据集里很多很强的ngram可以直接帮助生成正确决策，那就CNN。<br>
如果感觉很多case都是那种需要把一个句子看完甚至看两三遍才容易得出正确tag，那就RNN。<br>
还可以CNN、RNN的模型都跑出来简单集成一下。</p>
<h2 id="dropout-jia-zai-na-li">Dropout加在哪里</h2>
<p>word embedding层后、pooling层后、FC层（全联接层）后。</p>
<h2 id="er-fen-lei">二分类</h2>
<p>二分类问题不一定要用sigmoid作为输出层的激活函数，尝试一下包含俩类别的softmax。可能多一条分支就多一点信息，实践中常常带来零点几个点的提升。</p>
<h2 id="yang-ben-lei-bie-bu-jun-heng-wen-ti">样本类别不均衡问题</h2>
<p>如果正负样本比小于9:1的话，继续做深度模型调超参，决策阈值也完全不用手调。但是，如果经常一个batch中完全就是同一个类别的样本，或者一些类别的样本经过好多batch都难遇到一个的话，均衡就非常非常有必要了。</p>
<h2 id="zui-hou">最后</h2>
<ol>
<li>别太纠结文本截断长度使用120还是150</li>
<li>别太纠结对性能不敏感的超参数带来的开发集性能的微小提升</li>
<li>别太纠结未登陆词的embedding是初始化成全0还是随机初始化，别跟PAD共享embedding就行</li>
<li>别太纠结优化器用Adam还是MomentumSGD，如果跟SGD的感情还不深，无脑Adam，最后再用MomentumSGD跑几遍</li>
<li>还是不会用tricks但是就是想跑出个好结果，bert大力出奇迹。</li>
</ol>
<h1 id="zong-jie">总结</h1>
<p>复杂的模型未必会有很好的结果，简单模型效果未必不理想，没必要一味追求深度学习、复杂模型什么的。选什么样的模型还是要根据数据来的。同一类问题，不同的数据效果差异很大，不要小看任何一类问题，例如分类，我们通常觉得它很简单，但有些数据并非你所想。</p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://blog.csdn.net/feilong_csdn/article/details/88655927" target="_blank" rel="noopener">fastText原理和文本分类实战，看这一篇就够了</a></li>
<li><a href="https://blog.csdn.net/asialee_bird/article/details/88813385#%E4%B8%80%E3%80%81%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0" target="_blank" rel="noopener">TextCNN文本分类（keras实现）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/55263066" target="_blank" rel="noopener">浅谈基于深度学习的文本分类问题</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2018-06-22-4" target="_blank" rel="noopener">从DPCNN出发，撩一下深层word-level文本分类模型</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2019-01-24-5" target="_blank" rel="noopener">文本分类有哪些论文中很少提及却对性能有重要影响的tricks？</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247484682&amp;idx=1&amp;sn=51520138eed826ec891ac8154ee550f9&amp;chksm=970c2ddca07ba4ca33ee14542cff0457601bb16236f8edc1ff0e617051d1f063354dda0f8893&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">从前馈到反馈：解析循环神经网络（RNN）及其tricks</a></li>
<li>[<a href="http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%81%9A%E5%A5%BD%E9%95%BF%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%B3%95%E5%BE%8B%E6%96%87%E4%B9%A6%E6%99%BA%E8%83%BD%E5%8C%96" target="_blank" rel="noopener">达观数据曾彦能：如何用深度学习做好长文本分类与法律文书智能化处理</a>](<a href="http://www.52nlp.cn/tag/%E9%95%BF%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB" target="_blank" rel="noopener">http://www.52nlp.cn/tag/长文本分类</a>)</li>
<li><a href="https://www.zhihu.com/question/326770917/answer/698646465" target="_blank" rel="noopener">短文本分类和长文本分类的模型如何进行选择？</a></li>
<li><a href="https://www.pianshen.com/article/4319299677/" target="_blank" rel="noopener">NLP实践九：HAN原理与文本分类实践</a></li>
<li><a href="https://www.jianshu.com/p/56061b8f463a" target="_blank" rel="noopener">NLP之文本分类</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>FastText</tag>
      </tags>
  </entry>
  <entry>
    <title>BatchNormalization</title>
    <url>/2020/03/28/BatchNormalization/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>机器学习领域有个很重要的假设：<strong>IID独立同分布假设</strong>，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。</p>
<p>那BatchNorm的作用是什么呢？<strong>BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。</strong></p>
<a id="more"></a>
<h3 id="internal-covariate-shift-wen-ti">Internal Covariate Shift 问题</h3>
<p><strong>如果ML系统实例集合中的输入值X的分布老是变，这不符合IID假设</strong>，网络模型很难<strong>稳定的学规律</strong>,对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就是<strong>在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层</strong>。</p>
<p>然后提出了BatchNorm的基本思想：能不能<strong>让每个隐层节点的激活输入分布固定下来呢</strong>？这样就避免了“Internal Covariate Shift”问题了。</p>
<p>BN不是凭空拍脑袋拍出来的好点子，它是有启发来源的：之前的研究表明如果在图像处理中对输入图像进行白化（Whiten）操作的话——所谓<strong>白化</strong>，<strong>就是对输入数据分布变换到0均值，单位方差的正态分布</strong>——那么神经网络会较快收敛，那么BN作者就开始推论了：图像是深度神经网络的输入层，做白化能加快收敛，那么其实对于深度网络来说，其中某个隐层的神经元是下一层的输入，意思是其实深度神经网络的每一个隐层都是输入层，不过是相对下一层来说而已，那么能不能对每个隐层都做白化呢？这就是启发BN产生的原初想法，而BN也确实就是这么做的，<strong>可以理解为对深层神经网络每个隐层神经元的激活值做简化版本的白化操作。</strong></p>
<h3 id="batch-norm-de-ben-zhi-si-xiang">BatchNorm的本质思想</h3>
<p>BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的<strong>激活输入值</strong>（就是那个x=WU+B，U是输入）<strong>随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近</strong>（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这<strong>导致反向传播时低层神经网络的梯度消失</strong>，这是训练深层神经网络收敛越来越慢的<strong>本质原因</strong>，<strong>而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布</strong>，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是<strong>这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</strong></p>
<p><strong>对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题</strong>。因为梯度一直都能保持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也就是说收敛地快。BN说到底就是这么个机制，方法很简单，道理很深刻。</p>
<h3 id="xun-lian-jie-duan-ru-he-zuo-batch-norm">训练阶段如何做BatchNorm</h3>
<p>假设对于一个深层神经网络来说，其中两层结构如下：</p>
<img src="/2020/03/28/BatchNormalization/bn1.png" alt="avatar">
<p>要对每个隐层神经元的激活值做BN，可以想象成每个隐层又加上了一层BN操作层，它位于X=WU+B激活值获得之后，非线性函数变换之前，其图示如下：</p>
<p><img src="/2020/03/28/BatchNormalization/bn2.png" alt="avatar"></p>
<p>对于Mini-Batch SGD来说，一次训练过程里面包含m个训练实例，其具体BN操作就是对于隐层内每个神经元的激活值来说，进行如下变换：<br>
\[
\hat{x}^{(k)}=\frac{x^{(k)}-E\left[x^{(k)}\right]}{\sqrt{\operatorname{Var}\left[x^{(k)}\right]}}
\]<br>
某个神经元对应的原始的激活x通过减去mini-Batch内m个实例获得的m个激活x求得的均值E(x)并除以求得的方差Var(x)来进行转换。</p>
<p>经过这个<strong>变换后某个神经元的激活x形成了均值为0，方差为1的正态分布，目的是把值往后续要进行的非线性变换的线性区拉动，增大导数值，增强反向传播信息流动性，加快训练收敛速度。 但是这样会导致网络表达能力下降，为了防止这一点，每个神经元增加两个调节参数（scale和shift），这两个参数是通过训练来学习到的，用来对变换后的激活反变换，使得网络表达能力增强，即对变换后的激活进行如下的scale和shift操作，这其实是变换的反操作：</strong><br>
\[
y^{(k)}=\gamma^{(k)} \hat{x}^{(k)}+\beta^{(k)}
\]<br>
算法描述：</p>
<p><img src="/2020/03/28/BatchNormalization/bn3.png" alt="avatar"></p>
<h3 id="tui-li-jie-duan-ru-he-zuo-batch-norm">推理阶段如何做BatchNorm</h3>
<p>BN在训练的时候可以根据Mini-Batch里的若干训练实例进行激活数值调整，但是在推理（inference）的过程中，很明显输入就只有一个实例，看不到Mini-Batch其它实例，那么这时候怎么对输入做BN呢？因为很明显一个实例是没法算实例集合求出的均值和方差的。这可如何是好？</p>
<p>既然没有从Mini-Batch数据里可以得到的统计量，那就想其它办法来获得这个统计量，就是均值和方差。可以用从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的均值和方差统计量，因为本来就打算用全局的统计量，只是因为计算量等太大所以才会用Mini-Batch这种简化方式的，那么在推理的时候直接用全局统计量即可。</p>
<p>决定了获得统计量的数据范围，那么接下来的问题是如何获得均值和方差的问题。很简单，因为每次做Mini-Batch训练时，都会有那个Mini-Batch里m个训练实例获得的均值和方差，现在要全局统计量，只要把每个Mini-Batch的均值和方差统计量记住，然后对这些均值和方差求其对应的数学期望即可得出全局统计量<br>
\[
\begin{aligned}
&amp;E[x] \leftarrow E_{\mathrm{B}}\left[\mu_{\mathrm{B}}\right] \\
&amp;\operatorname{Var}[x] \leftarrow \frac{m}{m-1} E_{\mathrm{B}}\left[\sigma_{\mathrm{B}}^{2}\right]
\end{aligned}
\]<br>
有了均值和方差，每个隐层神经元也已经有对应训练好的Scaling参数和Shift参数，就可以在推导的时候对每个神经元的激活数据计算NB进行变换了，在推理过程中进行BN采取如下方式：<br>
\[
y=\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\varepsilon}} \cdot x+\left(\beta-\frac{\gamma \cdot E[x]}{\sqrt{\operatorname{Var}[x]+\varepsilon})}\right)
\]<br>
这个公式其实和训练时<br>
\[
y^{(k)}=\gamma^{(k)} \hat{x}^{(k)}+\beta^{(k)}
\]<br>
是等价的，通过简单的合并计算推导就可以得出这个结论。那么为啥要写成这个变换形式呢？作者这么写的意思是：在实际运行的时候，按照这种变体形式可以减少计算量，为啥呢？因为对于每个隐层节点来说：<br>
\[
\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\varepsilon}} \frac{\gamma \cdot E[x]}{\sqrt{\operatorname{Var}[x]+\varepsilon}}
\]<br>
都是固定值，这样这两个值可以事先算好存起来，在推理的时候直接用就行了，这样比原始的公式每一步骤都现算少了除法的运算过程，乍一看也没少多少计算量，但是如果隐层节点个数多的话节省的计算量就比较多了。</p>
<h3 id="zong-jie">总结</h3>
<ol>
<li><strong>不仅仅极大提升了训练速度，收敛过程大大加快</strong></li>
<li><strong>还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果</strong></li>
<li>另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等。总而言之，经过这么简单的变换，带来的好处多得很，这也是为何现在BN这么快流行起来的原因。</li>
</ol>
<h3 id="can-kao">参考</h3>
<ol>
<li><a href="https://cloud.tencent.com/developer/article/1157136" target="_blank" rel="noopener">深度学习】深入理解Batch Normalization批标准化</a></li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>batch normalization</tag>
      </tags>
  </entry>
  <entry>
    <title>数据预处理</title>
    <url>/2020/03/28/data_preprocessing/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/03/28/data_preprocessing/image-20200528110918927.png" alt></p>
<a id="more"></a>
<h1 id="shu-ju-qing-xi">数据清洗</h1>
<h2 id="jian-jie">简介</h2>
<p>要获得优秀的模型，首先需要清洗数据。在拟合机器学习或统计模型之前，我们通常需要清洗数据。用杂乱数据训练出的模型无法输出有意义的结果。</p>
<p>数据清洗：从记录集、表或数据库中检测和修正（或删除）受损或不准确记录的过程。它识别出数据中不完善、不准确或不相关的部分，并替换、修改或删除这些脏乱的数据。</p>
<p>整个清洗流程以kaggle的<a href="https://www.kaggle.com/c/sberbank-russian-housing-market/data" target="_blank" rel="noopener">Sberbank 俄罗斯房地产价值预测竞赛数据</a>为例。</p>
<h2 id="cha-kan-shu-ju">查看数据</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import packages</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.mlab <span class="keyword">as</span> mlab</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="comment"># 使用自带的样式进行美化</span></span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> figure</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment"># 设置默认参数</span></span><br><span class="line">matplotlib.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">12</span>,<span class="number">8</span>)</span><br><span class="line"><span class="comment"># 关闭copywarning  据说速度会更快</span></span><br><span class="line">pd.options.mode.chained_assignment = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># read the data</span></span><br><span class="line">df = pd.read_csv(<span class="string">'sberbank.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape and data types of the data</span></span><br><span class="line">print(df.shape)</span><br><span class="line">print(df.dtypes)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">(<span class="number">30471</span>, <span class="number">292</span>)</span><br><span class="line">id                      int64</span><br><span class="line">timestamp              object</span><br><span class="line">full_sq                 int64</span><br><span class="line">life_sq               float64</span><br><span class="line">floor                 float64</span><br><span class="line">                       ...   </span><br><span class="line">mosque_count_5000       int64</span><br><span class="line">leisure_count_5000      int64</span><br><span class="line">sport_count_5000        int64</span><br><span class="line">market_count_5000       int64</span><br><span class="line">price_doc               int64</span><br><span class="line">Length: <span class="number">292</span>, dtype: object</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># select numeric columns</span></span><br><span class="line">df_numeric = df.select_dtypes(include=[np.number])</span><br><span class="line">numeric_cols = df_numeric.columns.values</span><br><span class="line">print(numeric_cols)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">[<span class="string">'id'</span> <span class="string">'full_sq'</span> <span class="string">'life_sq'</span> <span class="string">'floor'</span> <span class="string">'max_floor'</span> <span class="string">'material'</span> <span class="string">'build_year'</span></span><br><span class="line"> <span class="string">'num_room'</span> <span class="string">'kitch_sq'</span> <span class="string">'state'</span> <span class="string">'area_m'</span> <span class="string">'raion_popul'</span> <span class="string">'green_zone_part'</span></span><br><span class="line"> <span class="string">'indust_part'</span> <span class="string">'children_preschool'</span> <span class="string">'preschool_quota'</span></span><br><span class="line"> <span class="string">'preschool_education_centers_raion'</span> <span class="string">'children_school'</span> <span class="string">'school_quota'</span></span><br><span class="line"> <span class="string">'school_education_centers_raion'</span> <span class="string">'school_education_centers_top_20_raion'</span></span><br><span class="line"> <span class="string">'hospital_beds_raion'</span> <span class="string">'healthcare_centers_raion'</span>...]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># select non numeric columns</span></span><br><span class="line">df_non_numeric = df.select_dtypes(exclude=[np.number])</span><br><span class="line">non_numeric_cols = df_non_numeric.columns.values</span><br><span class="line">print(non_numeric_cols)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">[<span class="string">'timestamp'</span> <span class="string">'product_type'</span> <span class="string">'sub_area'</span> <span class="string">'culture_objects_top_25'</span></span><br><span class="line"> <span class="string">'thermal_power_plant_raion'</span> <span class="string">'incineration_raion'</span> <span class="string">'oil_chemistry_raion'</span></span><br><span class="line"> <span class="string">'radiation_raion'</span> <span class="string">'railroad_terminal_raion'</span> <span class="string">'big_market_raion'</span></span><br><span class="line"> <span class="string">'nuclear_reactor_raion'</span> <span class="string">'detention_facility_raion'</span> <span class="string">'water_1line'</span></span><br><span class="line"> <span class="string">'big_road1_1line'</span> <span class="string">'railroad_1line'</span> <span class="string">'ecology'</span>]</span><br></pre></td></tr></table></figure>
<p>从以上结果中，我们可以看到该数据集共有 30,471 行、292 列，还可以辨别特征属于数值变量还是分类变量。这些都是有用的信息。</p>
<h2 id="que-shi-shu-ju">缺失数据</h2>
<h3 id="re-tu">热图</h3>
<p>当特征数量较少时，可以通过热图对缺失数据进行可视化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cols = df.columns[:<span class="number">30</span>] <span class="comment"># first 30 columns</span></span><br><span class="line">colours = [<span class="string">'#000099'</span>, <span class="string">'#ffff00'</span>] <span class="comment"># specify the colours - yellow is missing. blue is not missing.</span></span><br><span class="line">sns.heatmap(df[cols].isnull(), cmap=sns.color_palette(colours))</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/output_4_1.png" alt></p>
<p>上图展示了前 30 个特征的缺失数据模式。横轴表示特征名，纵轴表示观察值/行数，黄色表示缺失数据，蓝色表示非缺失数据。例如，图中特征 life_sq 在多个行中存在缺失值。而特征 floor 只在第 7000 行左右出现零星缺失值。</p>
<h3 id="bai-fen-bi-lie-biao">百分比列表</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># if it's a larger dataset and the visualization takes too long can do this.</span></span><br><span class="line"><span class="comment"># % of missing.</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">    pct_missing = np.mean(df[col].isnull())</span><br><span class="line">    print(<span class="string">'&#123;&#125; - &#123;&#125;%'</span>.format(col, round(pct_missing*<span class="number">100</span>)))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">id - <span class="number">0.0</span>%</span><br><span class="line">timestamp - <span class="number">0.0</span>%</span><br><span class="line">full_sq - <span class="number">0.0</span>%</span><br><span class="line">life_sq - <span class="number">21.0</span>%</span><br><span class="line">floor - <span class="number">1.0</span>%</span><br><span class="line">big_market_raion - <span class="number">0.0</span>%</span><br><span class="line">nuclear_reactor_raion - <span class="number">0.0</span>%</span><br><span class="line">detention_facility_raion - <span class="number">0.0</span>%</span><br><span class="line">full_all - <span class="number">0.0</span>%</span><br><span class="line">male_f - <span class="number">0.0</span>%</span><br><span class="line">hospital_beds_raion - <span class="number">47.0</span>%</span><br><span class="line">raion_build_count_with_material_info - <span class="number">16.0</span>%</span><br><span class="line">build_count_block - <span class="number">16.0</span>%</span><br><span class="line">build_count_wood - <span class="number">16.0</span>%</span><br><span class="line">build_count_frame - <span class="number">16.0</span>%</span><br><span class="line">build_count_brick - <span class="number">16.0</span>%</span><br><span class="line">build_count_monolith - <span class="number">16.0</span>%</span><br><span class="line">build_count_panel - <span class="number">16.0</span>%</span><br><span class="line">build_count_foam - <span class="number">16.0</span>%</span><br><span class="line">build_count_slag - <span class="number">16.0</span>%</span><br><span class="line">build_count_mix - <span class="number">16.0</span>%</span><br><span class="line">raion_build_count_with_builddate_info - <span class="number">16.0</span>%</span><br><span class="line">build_count_before_1920 - <span class="number">16.0</span>%</span><br><span class="line">build_count_1921<span class="number">-1945</span> - <span class="number">16.0</span>%</span><br><span class="line">build_count_1946<span class="number">-1970</span> - <span class="number">16.0</span>%</span><br><span class="line">build_count_1971<span class="number">-1995</span> - <span class="number">16.0</span>%</span><br><span class="line">build_count_after_1995 - <span class="number">16.0</span>%</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>输出展示了每个特征的缺失值百分比。具体而言，可以从输出中看到特征 life_sq 有 21% 的缺失数据，而特征 floor 仅有 1% 的缺失数据。该列表有效地总结了每个特征的缺失数据百分比情况，是对热图可视化的补充。</p>
<h3 id="zhi-fang-tu">直方图</h3>
<p>在存在很多特征时，缺失数据直方图也不失为一种有效方法。要想更深入地了解观察值中的缺失值模式，我们可以用直方图的形式进行可视化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># first create missing indicator for features with missing data</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">    missing = df[col].isnull()</span><br><span class="line">    num_missing = np.sum(missing)</span><br><span class="line">  <span class="comment"># 如果该列存在缺失值，对该列中每一行是否缺失进行标记</span></span><br><span class="line">    <span class="keyword">if</span> num_missing &gt; <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'created missing indicator for: &#123;&#125;'</span>.format(col))</span><br><span class="line">        df[<span class="string">'&#123;&#125;_ismissing'</span>.format(col)] = missing</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># then based on the indicator, plot the histogram of missing values</span></span><br><span class="line">ismissing_cols = [col <span class="keyword">for</span> col <span class="keyword">in</span> df.columns <span class="keyword">if</span><span class="string">'ismissing'</span><span class="keyword">in</span> col]</span><br><span class="line"><span class="comment"># 统计一行中，有多少缺失值</span></span><br><span class="line">df[<span class="string">'num_missing'</span>] = df[ismissing_cols].sum(axis=<span class="number">1</span>)</span><br><span class="line">df[<span class="string">'num_missing'</span>].value_counts().reset_index().sort_values(by=<span class="string">'index'</span>).plot.bar(x=<span class="string">'index'</span>, y=<span class="string">'num_missing'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/output_6_2.png" alt></p>
<p>直方图可以帮助在 30,471 个观察值中识别缺失值状况。例如，从上图中可以看到，超过 6000 个观察值不存在缺失值，接近 4000 个观察值具备一个缺失值。</p>
<h3 id="ru-he-chu-li-que-shi-shu-ju">如何处理缺失数据</h3>
<h4 id="shan-chu-guan-cha-zhi">删除观察值</h4>
<p>在统计学中，该方法叫做成列删除(listwise deletion)，需要丢弃包含缺失值的整列观察值。只有在确定缺失数据无法提供信息时，才可以执行该操作。否则，我们应当考虑其他解决方案。</p>
<p>例如，从缺失数据直方图中，我们可以看到只有少量观察值的缺失值数量超过 35。因此，可以创建一个新的数据集 df_less_missing_rows，该数据集删除了缺失值数量超过 35 的观察值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># drop rows with a lot of missing values.</span></span><br><span class="line">ind_missing = df[df[<span class="string">'num_missing'</span>] &gt; <span class="number">35</span>].index</span><br><span class="line">df_less_missing_rows = df.drop(ind_missing, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="diu-qi-te-zheng">丢弃特征</h4>
<p>只在确定某个特征无法提供有用信息时才丢弃它。</p>
<p>例如，从缺失数据百分比列表中，我们可以看到 hospital_beds_raion 具备较高的缺失值百分比——47%，因此我们丢弃这一整个特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hospital_beds_raion has a lot of missing. If we want to drop.</span></span><br><span class="line">cols_to_drop = [<span class="string">'hospital_beds_raion'</span>]</span><br><span class="line">df_less_hos_beds_raion = df.drop(cols_to_drop, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="tian-chong-que-shi-shu-ju">填充缺失数据</h4>
<p>当特征是数值变量时，执行缺失数据填充。对同一特征的其他非缺失数据取平均值或中位数，用这个值来替换缺失值。当特征是分类变量时，用众数（最频值）来填充缺失值。以特征 life_sq 为例，可以用特征中位数来替换缺失值。（或者用回归的方式填补缺失值）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># replace missing values with the median.</span></span><br><span class="line">med = df[<span class="string">'life_sq'</span>].median()</span><br><span class="line">print(med)</span><br><span class="line">df[<span class="string">'life_sq'</span>] = df[<span class="string">'life_sq'</span>].fillna(med)</span><br></pre></td></tr></table></figure>
<p>其他方式填补缺失数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 调包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认用均值添补</span></span><br><span class="line">imp_mean = SimpleImputer()</span><br><span class="line">imp_mean = imp_mean.fit_transform(Age) </span><br><span class="line"></span><br><span class="line"><span class="comment">#用中位数填补</span></span><br><span class="line">imp_median = SimpleImputer(strategy=<span class="string">"median"</span>)  </span><br><span class="line">imp_median = imp_median.fit_transform(Age) </span><br><span class="line"></span><br><span class="line"><span class="comment">#用0填补</span></span><br><span class="line">imp_0 = SimpleImputer(strategy=<span class="string">"constant"</span>,fill_value=<span class="number">0</span>) </span><br><span class="line">imp_0 = imp_0.fit_transform(Age)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在这里我们使用中位数填补Age </span></span><br><span class="line">data.loc[:,<span class="string">"Age"</span>] = imp_median</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用Pandas和Numpy进行填补其实更加简单</span></span><br><span class="line">data.loc[:,<span class="string">"Age"</span>] = data.loc[:,<span class="string">"Age"</span>].fillna(data.loc[:,<span class="string">"Age"</span>].median())</span><br></pre></td></tr></table></figure>
<p>可以对所有数值特征一次性应用同样的填充策略。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># impute the missing values and create the missing value indicator variables for each numeric column.</span></span><br><span class="line">df_numeric = df.select_dtypes(include=[np.number])</span><br><span class="line">numeric_cols = df_numeric.columns.values</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> numeric_cols:</span><br><span class="line">    missing = df[col].isnull()</span><br><span class="line">    num_missing = np.sum(missing)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># only do the imputation for the columns that have missing values.</span></span><br><span class="line">    <span class="keyword">if</span> num_missing &gt; <span class="number">0</span>:  </span><br><span class="line">        print(<span class="string">'imputing missing values for: &#123;&#125;'</span>.format(col))</span><br><span class="line">        df[<span class="string">'&#123;&#125;_ismissing'</span>.format(col)] = missing</span><br><span class="line">        med = df[col].median()</span><br><span class="line">        df[col] = df[col].fillna(med)</span><br></pre></td></tr></table></figure>
<p>对所有分类特征一次性应用众数填充策略。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># impute the missing values and create the missing value indicator variables for each non-numeric column.</span></span><br><span class="line">df_non_numeric = df.select_dtypes(exclude=[np.number])</span><br><span class="line">non_numeric_cols = df_non_numeric.columns.values</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> non_numeric_cols:</span><br><span class="line">    missing = df[col].isnull()</span><br><span class="line">    num_missing = np.sum(missing)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># only do the imputation for the columns that have missing values.</span></span><br><span class="line">    <span class="keyword">if</span> num_missing &gt; <span class="number">0</span>:  </span><br><span class="line">        print(<span class="string">'imputing missing values for: &#123;&#125;'</span>.format(col))</span><br><span class="line">        df[<span class="string">'&#123;&#125;_ismissing'</span>.format(col)] = missing</span><br><span class="line">        </span><br><span class="line">        top = df[col].describe()[<span class="string">'top'</span>] <span class="comment"># impute with the most frequent value.</span></span><br><span class="line">        df[col] = df[col].fillna(top)</span><br></pre></td></tr></table></figure>
<h4 id="ti-huan-que-shi-zhi">替换缺失值</h4>
<p>对于分类特征，可以添加新的带值类别，如 _MISSING_。对于数值特征，可以用特定值（如-999）来替换缺失值。这样，我们就可以保留缺失值，使之提供有价值的信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># categorical</span></span><br><span class="line">df[<span class="string">'sub_area'</span>] = df[<span class="string">'sub_area'</span>].fillna(<span class="string">'_MISSING_'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># numeric</span></span><br><span class="line">df[<span class="string">'life_sq'</span>] = df[<span class="string">'life_sq'</span>].fillna(<span class="number">-999</span>)</span><br></pre></td></tr></table></figure>
<h2 id="bu-gui-ze-shu-ju-yi-chang-zhi">不规则数据（异常值）</h2>
<p>异常值指与其他观察值具备显著差异的数据，它们可能是真的异常值也可能是错误。</p>
<h3 id="zhi-fang-tu-xiang-xing-tu">直方图+箱形图</h3>
<h4 id="zhi-fang-tu-1">直方图</h4>
<p>当特征是数值变量时，使用直方图和箱形图来检测异常值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># histogram of life_sq.</span></span><br><span class="line">df[<span class="string">'life_sq'</span>].hist(bins=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/21B8CEEC-A71F-4D53-9A70-A539685A1B39.png" alt></p>
<p>由于数据中可能存在异常值，因此图中数据高度偏斜。</p>
<p>比较正常的情况</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">'floor'</span>].hist(bins=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/722E5EFF-CD89-4994-90A4-7AA1ADE60CA5.png" alt></p>
<h4 id="xiang-xing-tu">箱形图</h4>
<p>为了进一步研究特征，看一下箱形图。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># box plot.</span></span><br><span class="line">df.boxplot(column=[<span class="string">'life_sq'</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/924F9994-B7CF-478E-B4BA-68CE136FE990.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># box plot.</span></span><br><span class="line">df.boxplot(column=[<span class="string">'floor'</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/65762A44-008B-4239-9024-97A9CE86EFD3.png" alt></p>
<h3 id="miao-shu-xing-tong-ji">描述性统计</h3>
<p>对于数值特征，当异常值过于独特时，箱形图无法显示该值。因此，可以查看其描述统计学。<br>
例如，对于特征 life_sq，可以看到其最大值是 7478，而上四分位数（数据的第 75 个百分位数据）是 43。因此值 7478 是异常值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">'life_sq'</span>].describe()</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">count    <span class="number">24088.000000</span></span><br><span class="line">mean        <span class="number">34.403271</span></span><br><span class="line">std         <span class="number">52.285733</span></span><br><span class="line">min          <span class="number">0.000000</span></span><br><span class="line"><span class="number">25</span>%         <span class="number">20.000000</span></span><br><span class="line"><span class="number">50</span>%         <span class="number">30.000000</span></span><br><span class="line"><span class="number">75</span>%         <span class="number">43.000000</span></span><br><span class="line">max       <span class="number">7478.000000</span></span><br><span class="line">Name: life_sq, dtype: float64</span><br></pre></td></tr></table></figure>
<h3 id="tiao-xing-tu">条形图</h3>
<p>当特征是<strong>分类变量</strong>时，可以使用条形图来了解其类别和分布。</p>
<p>例如，特征 ecology 具备合理的分布。但如果某个类别「other」仅有一个值，则它就是异常值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># barchart -  distributionofacategoricalvariabledf['ecology'].value_counts().plot.bar()</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/9F67A55F-2E88-4704-87FF-89DB72CE37FB.png" alt></p>
<p>其他方法：还有很多方法可以找出异常值，如散点图、z 分数和聚类。</p>
<h3 id="ru-he-chu-li-yi-chang-zhi">如何处理异常值</h3>
<p>处理异常值的方法与处理缺失值有些类似：要么丢弃，要么修改，要么保留。</p>
<h2 id="bu-bi-yao-shu-ju">不必要数据</h2>
<p>处理异常值的方法与处理缺失值有些类似：要么丢弃，要么修改，要么保留。</p>
<h3 id="bu-bi-yao-shu-ju-lei-xing-1-xin-xi-bu-zu-zhong-fu">不必要数据类型1：信息不足/重复</h3>
<p>有时一个特征不提供信息，是因为它拥有太多具备相同值的行。需要了解重复特征背后的原因。当它们的确无法提供有用信息时，我们就可以丢弃它。</p>
<p>可以为具备高比例相同值的特征创建一个列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_rows = len(df.index)</span><br><span class="line">low_information_cols = [] <span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">    cnts = df[col].value_counts(dropna=<span class="literal">False</span>)</span><br><span class="line">    top_pct = (cnts/num_rows).iloc[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> top_pct &gt; <span class="number">0.95</span>:</span><br><span class="line">        low_information_cols.append(col)</span><br><span class="line">        print(<span class="string">'&#123;0&#125;: &#123;1:.5f&#125;%'</span>.format(col, top_pct*<span class="number">100</span>))</span><br><span class="line">        print(cnts)</span><br><span class="line">        print()</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">oil_chemistry_raion: <span class="number">99.02858</span>%</span><br><span class="line">no     <span class="number">30175</span></span><br><span class="line">yes      <span class="number">296</span></span><br><span class="line">Name: oil_chemistry_raion, dtype: int64</span><br><span class="line"></span><br><span class="line">railroad_terminal_raion: <span class="number">96.27187</span>%</span><br><span class="line">no     <span class="number">29335</span></span><br><span class="line">yes     <span class="number">1136</span></span><br><span class="line">Name: railroad_terminal_raion, dtype: int64</span><br><span class="line"></span><br><span class="line">nuclear_reactor_raion: <span class="number">97.16780</span>%</span><br><span class="line">no     <span class="number">29608</span></span><br><span class="line">yes      <span class="number">863</span></span><br><span class="line">Name: nuclear_reactor_raion, dtype: int64</span><br><span class="line"></span><br><span class="line">big_road1_1line: <span class="number">97.43691</span>%</span><br><span class="line">no     <span class="number">29690</span></span><br><span class="line">yes      <span class="number">781</span></span><br><span class="line">Name: big_road1_1line, dtype: int64</span><br></pre></td></tr></table></figure>
<h3 id="bu-bi-yao-shu-ju-lei-xing-2-bu-xiang-guan">不必要数据类型2：不想关</h3>
<p>数据需要为项目提供有价值的信息。如果特征与项目试图解决的问题无关，则这些特征是不相关数据。当这些特征无法服务于项目目标时，删除之。</p>
<h3 id="bu-bi-yao-shu-ju-lei-xing-3-fu-zhi">不必要数据类型3：复制</h3>
<p>复制数据即，观察值存在副本。复制数据有两个主要类型。</p>
<p>复制数据类型 1：基于所有特征，这种复制发生在观察值内所有特征的值均相同的情况下，很容易找出。</p>
<p>需要先删除数据集中的唯一标识符 id，然后删除复制数据得到数据集 df_dedupped。对比 df 和 df_dedupped 这两个数据集的形态，找出复制行的数量，删除这些复制数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># we know that column 'id' is unique, but what if we drop it?</span></span><br><span class="line">df_dedupped = df.drop(<span class="string">'id'</span>, axis=<span class="number">1</span>).drop_duplicates()</span><br><span class="line"></span><br><span class="line"><span class="comment"># there were duplicate rows</span></span><br><span class="line">print(df.shape)</span><br><span class="line">print(df_dedupped.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">(<span class="number">30471</span>, <span class="number">344</span>)</span><br><span class="line">(<span class="number">30461</span>, <span class="number">343</span>)</span><br><span class="line"><span class="comment"># 可以发现，有 10 行是完全复制的观察值。</span></span><br></pre></td></tr></table></figure>
<p>复制数据类型 2：基于关键特征</p>
<p>例如，相同使用面积、相同价格、相同建造年限的两次房产交易同时发生的概率接近零。</p>
<p>我们可以设置一组关键特征作为唯一标识符，比如 timestamp、full_sq、life_sq、floor、build_year、num_room、price_doc。然后基于这些特征检查是否存在复制数据，删除这些复制数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">key = [<span class="string">'timestamp'</span>, <span class="string">'full_sq'</span>, <span class="string">'life_sq'</span>, <span class="string">'floor'</span>, <span class="string">'build_year'</span>, <span class="string">'num_room'</span>, <span class="string">'price_doc'</span>]</span><br><span class="line">df.fillna(<span class="number">-999</span>).groupby(key)[<span class="string">'id'</span>].count().sort_values(ascending=<span class="literal">False</span>).head(<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">timestamp   full_sq  life_sq  floor  build_year  num_room  price_doc</span><br><span class="line"><span class="number">2014</span><span class="number">-12</span><span class="number">-09</span>  <span class="number">40</span>       <span class="number">-999.0</span>   <span class="number">17.0</span>   <span class="number">-999.0</span>       <span class="number">1.0</span>      <span class="number">4607265</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2014</span><span class="number">-04</span><span class="number">-15</span>  <span class="number">134</span>       <span class="number">134.0</span>   <span class="number">1.0</span>     <span class="number">0.0</span>         <span class="number">3.0</span>      <span class="number">5798496</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2013</span><span class="number">-08</span><span class="number">-30</span>  <span class="number">40</span>       <span class="number">-999.0</span>   <span class="number">12.0</span>   <span class="number">-999.0</span>       <span class="number">1.0</span>      <span class="number">4462000</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2012</span><span class="number">-09</span><span class="number">-05</span>  <span class="number">43</span>       <span class="number">-999.0</span>   <span class="number">21.0</span>   <span class="number">-999.0</span>      <span class="number">-999.0</span>    <span class="number">6229540</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2013</span><span class="number">-12</span><span class="number">-05</span>  <span class="number">40</span>       <span class="number">-999.0</span>   <span class="number">5.0</span>    <span class="number">-999.0</span>       <span class="number">1.0</span>      <span class="number">4414080</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2014</span><span class="number">-12</span><span class="number">-17</span>  <span class="number">62</span>       <span class="number">-999.0</span>   <span class="number">9.0</span>    <span class="number">-999.0</span>       <span class="number">2.0</span>      <span class="number">6552000</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2013</span><span class="number">-05</span><span class="number">-22</span>  <span class="number">68</span>       <span class="number">-999.0</span>   <span class="number">2.0</span>    <span class="number">-999.0</span>      <span class="number">-999.0</span>    <span class="number">5406690</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2012</span><span class="number">-08</span><span class="number">-27</span>  <span class="number">59</span>       <span class="number">-999.0</span>   <span class="number">6.0</span>    <span class="number">-999.0</span>      <span class="number">-999.0</span>    <span class="number">4506800</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2013</span><span class="number">-04</span><span class="number">-03</span>  <span class="number">42</span>       <span class="number">-999.0</span>   <span class="number">2.0</span>    <span class="number">-999.0</span>      <span class="number">-999.0</span>    <span class="number">3444000</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2015</span><span class="number">-03</span><span class="number">-14</span>  <span class="number">62</span>       <span class="number">-999.0</span>   <span class="number">2.0</span>    <span class="number">-999.0</span>       <span class="number">2.0</span>      <span class="number">6520500</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2014</span><span class="number">-01</span><span class="number">-22</span>  <span class="number">46</span>        <span class="number">28.0</span>    <span class="number">1.0</span>     <span class="number">1968.0</span>      <span class="number">2.0</span>      <span class="number">3000000</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2012</span><span class="number">-10</span><span class="number">-22</span>  <span class="number">61</span>       <span class="number">-999.0</span>   <span class="number">18.0</span>   <span class="number">-999.0</span>      <span class="number">-999.0</span>    <span class="number">8248500</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2013</span><span class="number">-09</span><span class="number">-23</span>  <span class="number">85</span>       <span class="number">-999.0</span>   <span class="number">14.0</span>   <span class="number">-999.0</span>       <span class="number">3.0</span>      <span class="number">7725974</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2013</span><span class="number">-06</span><span class="number">-24</span>  <span class="number">40</span>       <span class="number">-999.0</span>   <span class="number">12.0</span>   <span class="number">-999.0</span>      <span class="number">-999.0</span>    <span class="number">4112800</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2015</span><span class="number">-03</span><span class="number">-30</span>  <span class="number">41</span>        <span class="number">41.0</span>    <span class="number">11.0</span>    <span class="number">2016.0</span>      <span class="number">1.0</span>      <span class="number">4114580</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2013</span><span class="number">-12</span><span class="number">-18</span>  <span class="number">39</span>       <span class="number">-999.0</span>   <span class="number">6.0</span>    <span class="number">-999.0</span>       <span class="number">1.0</span>      <span class="number">3700946</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2013</span><span class="number">-08</span><span class="number">-29</span>  <span class="number">58</span>        <span class="number">58.0</span>    <span class="number">13.0</span>    <span class="number">2013.0</span>      <span class="number">2.0</span>      <span class="number">5764128</span>      <span class="number">1</span></span><br><span class="line">            <span class="number">50</span>        <span class="number">33.0</span>    <span class="number">2.0</span>     <span class="number">1972.0</span>      <span class="number">2.0</span>      <span class="number">8150000</span>      <span class="number">1</span></span><br><span class="line">            <span class="number">52</span>        <span class="number">30.0</span>    <span class="number">9.0</span>     <span class="number">2006.0</span>      <span class="number">2.0</span>      <span class="number">10000000</span>     <span class="number">1</span></span><br><span class="line"><span class="number">2013</span><span class="number">-08</span><span class="number">-30</span>  <span class="number">38</span>        <span class="number">17.0</span>    <span class="number">15.0</span>    <span class="number">2004.0</span>      <span class="number">1.0</span>      <span class="number">6400000</span>      <span class="number">1</span></span><br><span class="line">Name: id, dtype: int64</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 找到了 16 条复制数据。</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># drop duplicates based on an subset of variables.</span></span><br><span class="line">key = [<span class="string">'timestamp'</span>, <span class="string">'full_sq'</span>, <span class="string">'life_sq'</span>, <span class="string">'floor'</span>, <span class="string">'build_year'</span>, <span class="string">'num_room'</span>, <span class="string">'price_doc'</span>]</span><br><span class="line">df_dedupped2 = df.drop_duplicates(subset=key)</span><br><span class="line"></span><br><span class="line">print(df.shape)</span><br><span class="line">print(df_dedupped2.shape)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">(<span class="number">30471</span>, <span class="number">344</span>)</span><br><span class="line">(<span class="number">30455</span>, <span class="number">344</span>)</span><br></pre></td></tr></table></figure>
<h2 id="bu-yi-zhi-shu-ju">不一致数据</h2>
<p>在拟合模型时，数据集遵循特定标准也是很重要的一点。我们需要使用不同方式来探索数据，找出不一致数据。大部分情况下，这取决于观察和经验。不存在运行和修复不一致数据的既定代码。</p>
<h3 id="strong-bu-yi-zhi-shu-ju-lei-xing-1-da-xie-strong"><strong>不一致数据类型 1：大写</strong></h3>
<p>在类别值中混用大小写是一种常见的错误。这可能带来一些问题，因为 Python 分析对大小写很敏感。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">'sub_area'</span>].value_counts(dropna=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">Poselenie Sosenskoe               <span class="number">1776</span></span><br><span class="line">Nekrasovka                        <span class="number">1611</span></span><br><span class="line">Poselenie Vnukovskoe              <span class="number">1372</span></span><br><span class="line">Poselenie Moskovskij               <span class="number">925</span></span><br><span class="line">Poselenie Voskresenskoe            <span class="number">713</span></span><br><span class="line">                                  ... </span><br><span class="line">Molzhaninovskoe                      <span class="number">3</span></span><br><span class="line">Poselenie Shhapovskoe                <span class="number">2</span></span><br><span class="line">Poselenie Kievskij                   <span class="number">2</span></span><br><span class="line">Poselenie Mihajlovo-Jarcevskoe       <span class="number">1</span></span><br><span class="line">Poselenie Klenovskoe                 <span class="number">1</span></span><br><span class="line">Name: sub_area, Length: <span class="number">146</span>, dtype: int64</span><br><span class="line"><span class="comment"># 存储了不同地区的名称，看起来非常标准化。但是，有时候相同特征内存在不一致的大小写使用情况。「Poselenie Sosenskoe」和「pOseleNie sosenskeo」指的是相同的地区。</span></span><br><span class="line"><span class="comment"># 为了避免这个问题，可以将所有字母设置为小写（或大写）。</span></span><br><span class="line"><span class="comment"># make everything lower case.</span></span><br><span class="line">df[<span class="string">'sub_area_lower'</span>] = df[<span class="string">'sub_area'</span>].str.lower()</span><br><span class="line">df[<span class="string">'sub_area_lower'</span>].value_counts(dropna=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h3 id="strong-bu-yi-zhi-shu-ju-lei-xing-2-ge-shi-strong"><strong>不一致数据类型 2：格式</strong></h3>
<p>需要执行的另一个标准化是数据格式。比如将特征从字符串格式转换为 DateTime 格式。特征 timestamp 在表示日期时是字符串格式。</p>
<p>处理格式不一致的数据，可以使用以下代码进行格式转换，并提取日期或时间值。然后，就可以很容易地用年或月的方式分析交易量数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">'timestamp_dt'</span>] = pd.to_datetime(df[<span class="string">'timestamp'</span>], format=<span class="string">'%Y-%m-%d'</span>)</span><br><span class="line">df[<span class="string">'year'</span>] = df[<span class="string">'timestamp_dt'</span>].dt.year</span><br><span class="line">df[<span class="string">'month'</span>] = df[<span class="string">'timestamp_dt'</span>].dt.month</span><br><span class="line">df[<span class="string">'weekday'</span>] = df[<span class="string">'timestamp_dt'</span>].dt.weekday</span><br><span class="line"></span><br><span class="line">print(df[<span class="string">'year'</span>].value_counts(dropna=<span class="literal">False</span>))</span><br><span class="line">print()</span><br><span class="line">print(df[<span class="string">'month'</span>].value_counts(dropna=<span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="number">2014</span>    <span class="number">13662</span></span><br><span class="line"><span class="number">2013</span>     <span class="number">7978</span></span><br><span class="line"><span class="number">2012</span>     <span class="number">4839</span></span><br><span class="line"><span class="number">2015</span>     <span class="number">3239</span></span><br><span class="line"><span class="number">2011</span>      <span class="number">753</span></span><br><span class="line">Name: year, dtype: int64</span><br><span class="line"></span><br><span class="line"><span class="number">12</span>    <span class="number">3400</span></span><br><span class="line"><span class="number">4</span>     <span class="number">3191</span></span><br><span class="line"><span class="number">3</span>     <span class="number">2972</span></span><br><span class="line"><span class="number">11</span>    <span class="number">2970</span></span><br><span class="line"><span class="number">10</span>    <span class="number">2736</span></span><br><span class="line"><span class="number">6</span>     <span class="number">2570</span></span><br><span class="line"><span class="number">5</span>     <span class="number">2496</span></span><br><span class="line"><span class="number">9</span>     <span class="number">2346</span></span><br><span class="line"><span class="number">2</span>     <span class="number">2275</span></span><br><span class="line"><span class="number">7</span>     <span class="number">1875</span></span><br><span class="line"><span class="number">8</span>     <span class="number">1831</span></span><br><span class="line"><span class="number">1</span>     <span class="number">1809</span></span><br><span class="line">Name: month, dtype: int64</span><br></pre></td></tr></table></figure>
<h3 id="strong-bu-yi-zhi-shu-ju-lei-xing-3-lei-bie-zhi-strong"><strong>不一致数据类型 3：类别值</strong></h3>
<p>分类特征的值数量有限。有时由于拼写错误等原因可能出现其他值。需要观察特征来找出类别值不一致的情况。</p>
<p>举例来说：city 的值被错误输入为「torontoo」和「tronto」，其实二者均表示「toronto」（正确值）。</p>
<p>识别它们的一种简单方式是模糊逻辑（或编辑距离）。该方法可以衡量使一个值匹配另一个值需要更改的字母数量（距离）。</p>
<p>已知这些类别应仅有四个值：「toronto」、「vancouver」、「montreal」和「calgary」。计算所有值与单词「toronto」（和「vancouver」）之间的距离，我们可以看到疑似拼写错误的值与正确值之间的距离较小，因为它们只有几个字母不同。</p>
<figure class="highlight prolog"><table><tr><td class="code"><pre><span class="line">from nltk.metrics import edit_distance</span><br><span class="line"></span><br><span class="line">df_city_ex = pd.<span class="symbol">DataFrame</span>(data=&#123;<span class="string">'city'</span>: [<span class="string">'torontoo'</span>, <span class="string">'toronto'</span>, <span class="string">'tronto'</span>, <span class="string">'vancouver'</span>, <span class="string">'vancover'</span>, <span class="string">'vancouvr'</span>, <span class="string">'montreal'</span>, <span class="string">'calgary'</span>]&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df_city_ex[<span class="string">'city_distance_toronto'</span>] = df_city_ex[<span class="string">'city'</span>].map(lambda x: edit_distance(x, <span class="string">'toronto'</span>))</span><br><span class="line">df_city_ex[<span class="string">'city_distance_vancouver'</span>] = df_city_ex[<span class="string">'city'</span>].map(lambda x: edit_distance(x, <span class="string">'vancouver'</span>))</span><br><span class="line">df_city_ex</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/image-20200528145223629.png" alt></p>
<p>处理类别值不一致的数据，可以设置标准将这些拼写错误转换为正确值。</p>
<p>例如，下列代码规定所有值与「toronto」的距离在 2 个字母以内。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">msk = df_city_ex[<span class="string">'city_distance_toronto'</span>] &lt;= <span class="number">2</span></span><br><span class="line">df_city_ex.loc[msk, <span class="string">'city'</span>] = <span class="string">'toronto'</span></span><br><span class="line"></span><br><span class="line">msk = df_city_ex[<span class="string">'city_distance_vancouver'</span>] &lt;= <span class="number">2</span></span><br><span class="line">df_city_ex.loc[msk, <span class="string">'city'</span>] = <span class="string">'vancouver'</span></span><br><span class="line"></span><br><span class="line">df_city_ex</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/image-20200528145327127.png" alt></p>
<h3 id="strong-bu-yi-zhi-shu-ju-lei-xing-4-di-zhi-strong"><strong>不一致数据类型 4：地址</strong></h3>
<p>由于人们往数据库中输入数据时通常不会遵循标准格式，所以地址处理是一个比较难的问题。</p>
<p>处理地址不一致的数据，可以用浏览的方式可以找出混乱的地址数据。即便有时我们看不出什么问题，也可以运行代码执行标准化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># no address column in the housing dataset. So create one to show the code.</span></span><br><span class="line">df_add_ex = pd.DataFrame([<span class="string">'123 MAIN St Apartment 15'</span>, <span class="string">'123 Main Street Apt 12   '</span>, <span class="string">'543 FirSt Av'</span>, <span class="string">'  876 FIRst Ave.'</span>], columns=[<span class="string">'address'</span>])</span><br><span class="line">df_add_ex</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/image-20200528150318273.png" alt></p>
<p>处理地址不一致的数据，运行以下代码将所有字母转为小写，删除空格，删除句号，并将措辞标准化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_add_ex[<span class="string">'address_std'</span>] = df_add_ex[<span class="string">'address'</span>].str.lower()</span><br><span class="line">df_add_ex[<span class="string">'address_std'</span>] = df_add_ex[<span class="string">'address_std'</span>].str.strip() <span class="comment"># remove leading and trailing whitespace.</span></span><br><span class="line">df_add_ex[<span class="string">'address_std'</span>] = df_add_ex[<span class="string">'address_std'</span>].str.replace(<span class="string">'\\.'</span>, <span class="string">''</span>) <span class="comment"># remove period.</span></span><br><span class="line">df_add_ex[<span class="string">'address_std'</span>] = df_add_ex[<span class="string">'address_std'</span>].str.replace(<span class="string">'\\bstreet\\b'</span>, <span class="string">'st'</span>) <span class="comment"># replace street with st.</span></span><br><span class="line">df_add_ex[<span class="string">'address_std'</span>] = df_add_ex[<span class="string">'address_std'</span>].str.replace(<span class="string">'\\bapartment\\b'</span>, <span class="string">'apt'</span>) <span class="comment"># replace apartment with apt.</span></span><br><span class="line">df_add_ex[<span class="string">'address_std'</span>] = df_add_ex[<span class="string">'address_std'</span>].str.replace(<span class="string">'\\bav\\b'</span>, <span class="string">'ave'</span>) <span class="comment"># replace apartment with apt.</span></span><br><span class="line"></span><br><span class="line">df_add_ex</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/image-20200528150416408.png" alt></p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://mp.weixin.qq.com/s/TpiLd94iet902WK6SEjHrQ" target="_blank" rel="noopener">数据缺失、混乱、重复怎么办？最全数据清洗指南让你所向披靡</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习总结</title>
    <url>/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="zhi-shi-dian">知识点</h3>
<h4 id="shen-jing-yuan-gan-zhi-qi">神经元(感知器)</h4>
<ol>
<li>感知器是一种人工<strong>神经元</strong>.它接受几个<strong>二进制输出</strong>并产生一个<strong>二进制输入</strong>.如果引入<strong>权重</strong>和<strong>阈值</strong>,那么感知器的参数可以表示为:\(f(x)=sign(wx+b)\)</li>
<li>感知器是<strong>单输出</strong>的,但这个单输出可以被用于<strong>多个</strong>其它感知器的输入.</li>
<li>感知器可以很容易地计算基本的<strong>逻辑功能</strong>,如<strong>与</strong>,<strong>或</strong>,<strong>与非</strong>.所以感知器网络可以计算任何逻辑功能</li>
<li>使感知器能够自动调整权重和偏置的<strong>学习算法</strong>是神经网络有别于传统逻辑门的关键.</li>
</ol>
<h4 id="s-xing-shen-jing-yuan">S型神经元</h4>
<ol>
<li>网络中单个感知器上权重或偏置的<strong>微小改动</strong>可能会引起<strong>输出翻转</strong>,从而导致其余网络的行为改变.所以<strong>逐步修改</strong>权重和偏置来让输出接近期望很困难,所以引入了<strong>S型神经元(逻辑神经元)</strong></li>
<li>S型神经元和感知器类似,但是权重和偏置的微小改动只引起输出的<strong>微小变化</strong>.S型神经元的输入可以是<strong>0和1中的任意值</strong>,输出是\(σ(wx+b)\),其中\(σ\)被称为s型函数(逻辑函数).</li>
<li>σ函数是阶跃函数的<strong>平滑</strong>版本.这意味着权重和偏置的微小变化会产生一个微小的输出变化,\(\Delta output \approx \sum_{j} \frac{\partial \text { output }}{\partial w_{j}} \Delta w_{j}+\frac{\partial \text { output }}{\partial b} \Delta b\),这意味着输出的变化是权重和偏置的变化的<strong>线性函数</strong>.</li>
</ol>
<a id="more"></a>
<h4 id="ti-du-xia-jiang-suan-fa">梯度下降算法</h4>
<ol>
<li>
<p>神经网络中经常需要<strong>大量</strong>变量,因此采用<strong>梯度下降法</strong>来计算代价函数最小值:重复计算梯度,然后沿着<strong>相反</strong>的方向移动\(v \rightarrow v^{\prime}=v-\eta \nabla C\)</p>
</li>
<li>
<p>假如需要计算每个训练输入的梯度值,训练速度会相当缓慢.因此引入了<strong>随机梯度下降</strong>,只计算小批量数据(mini-batch)的梯度值来求平均值<br>
\[
\begin{aligned}
w_{k} \rightarrow w_{k}^{\prime} &amp;=w_{k}-\frac{\eta}{m} \sum_{j} \frac{\partial C_{X_{j}}}{\partial w_{k}} \\
b_{l} \rightarrow b_{l}^{\prime} &amp;=b_{l}-\frac{\eta}{m} \sum_{j} \frac{\partial C_{X_{j}}}{\partial b_{l}}
\end{aligned}
\]</p>
</li>
<li>
<p>随机机梯度下降不断地选定小批量数据来进行训练,直到用完了所有的训练输入,就称为完成了一个<strong>迭代期(epoch)</strong>,然后会开始一个新的迭代期.</p>
</li>
</ol>
<h4 id="fan-xiang-chuan-bo-bp-suan-fa">反向传播(BP)算法</h4>
<h5 id="shen-jing-wang-luo-zhong-shi-yong-ju-zhen-kuai-su-ji-suan-shu-chu-de-fang-fa">神经网络中使用矩阵快速计算输出的方法</h5>
<ol>
<li>\(w^l_{jk}\)表示从第\((l-1)\)层的第\(k\)个神经元到第\(l\)层的第\(j\)个神经元的链接上的<strong>权重</strong>,\(b^l_j\)表示在第l层第j个神经元的<strong>偏置</strong>.\(a^l_j\)表示第l层第j个神经元的<strong>激活值</strong>.由此得到了激活值之间的关系\(a_{j}^{l}=\sigma\left(\sum_{k} w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\right)\)</li>
<li>对每一层l定义<strong>权重矩阵</strong>(其中第j行第k列的元素是\(w^l_{jk}\),<strong>偏置向量</strong>(每个元素是\(b^l_j\))和<strong>激活向量</strong>(每个元素是\(a^l_j\)).所以上式可以改写为\(a^{l}=\sigma\left(w^{l} a^{l-1}+b^{l}\right)\),其中σ是<strong>向量化函数</strong>(作用σ到向量中的每个元素).中间量\(z^{l} \equiv w^{l} a^{l-1}+b^{l}\)称为<strong>带权输</strong>入(每个元素是第l层第j个神经元的激活函数的带权输入**)**.</li>
</ol>
<h5 id="guan-yu-dai-jie-han-shu-de-liang-ge-jia-she">关于代价函数的两个假设</h5>
<p><strong>反向传播</strong>的目标是计算代价函数关于w和b的<strong>偏导数</strong>.为了让反向传播可行,需要作出两个主要假设</p>
<ol>
<li>代价函数可以被写成一个在每个训练样本x上的代价函数的<strong>均值</strong>\(C=\frac{1}{n} \sum_{x} C_{x}\)</li>
<li>代价可以写成神经网络输出的函数,以二次代价函数为例\(C=\frac{1}{2}\left\|y-a^{L}\right\|^{2}=\frac{1}{2} \sum_{j}\left(y_{j}-a_{j}^{L}\right)^{2}\)</li>
</ol>
<h5 id="hadamard-cheng-ji-s-t">Hadamard乘积,s⊙t</h5>
<p>假设s和t是两个同样维度的向量,那么s⊙t表示<strong>按元素的乘积</strong>,称为<strong>Hadamard乘积</strong></p>
<h5 id="fan-xiang-chuan-bo-de-si-ge-ji-ben-fang-cheng">反向传播的四个基本方程</h5>
<p>在第l层第j个神经元上的<strong>误差</strong>,被定义为\(\delta_{j}^{l} \equiv \frac{\partial C}{\partial z_{j}^{l}}\)</p>
<p><strong>四个基本方程</strong></p>
<ol>
<li>
<p>输出层误差的方程: \(\delta_{j}^{L} = \frac{\partial C}{\partial a^L_j}\frac{\partial a^L_j}{\partial z^l_j} = \frac{\partial C}{\partial a_{j}^{L}} \sigma^{\prime}\left(z_{j}^{L}\right)\)</p>
<p>右式第⼀个项 \(\frac{\partial C}{\partial a^L_j}\) 表⽰代价随着 \(j^{th}\)输出激活值的变化⽽ 变化的速度。假如 \(C\) 不太依赖⼀个特定的输出神经元 \(j\)，那么\(\delta^L_j\)就会很⼩，这也是我们想要的效果。右式第⼆项 \(\sigma^\prime(Z^L_j)\)刻画了在\(z^L_j\) 处激活函数 \(\sigma\) 变化的速度。</p>
</li>
<li>
<p>使用下一层的误差来表示当前层的误差:\(\delta^{l}=\left(\left(w^{l+1}\right)^{T} \delta^{l+1}\right) \odot \sigma^{\prime}\left(z^{l}\right)\)</p>
<p>由递推公式<br>
\[
\begin{aligned}
\frac{\partial  C}{\partial  z^l_j} &amp;=\delta^l_j \\
&amp; = \sum_i{\frac{\partial  C}{\partial  z^{l+1}_i}\frac{\partial  z^{l+1}_i}{\partial  z^l_j}} \\ 
&amp;=\sum_i{\delta ^{l+1}_i \frac{\partial  z^{l+1}_i}{\partial  a^l_j}\frac{\partial  a^l_j}{\partial  z^l_j}} \\ 
&amp;=\sum_i{\delta^{l+1}_i w^{l+1}_{ji} \sigma \prime (z^l_j)} \\
&amp;=(w^{l+1}_j)^T\delta^{l+1}\sigma \prime(z^l_j) \\
&amp;=(w^{l+1})^T \sigma^{l+1}\bigodot \sigma \prime (z^l)
\end{aligned}
\]</p>
</li>
<li>
<p>代价函数关于网络中任意偏置的改变率:\(\frac{\partial C}{\partial b_{j}^{l}}=\delta_{j}^{l}\)</p>
</li>
<li>
<p>代价函数关于任何一个权重的改变率:\(\frac{\partial C}{\partial w_{j k}^{l}}=a_{k}^{l-1} \delta_{j}^{l}\)（因为\(z^l = w^la^{l-1}+b^l\)）</p>
</li>
</ol>
<h5 id="suan-fa-miao-shu">算法描述</h5>
<p>反向传播方程给出了一种计算代价函数梯度的方法:</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/bp.png" alt="avatar"></p>
<ol>
<li><strong>输入x</strong>:为输入层设置对应的激活值\(a1\)</li>
<li><strong>前向传播</strong>:对从前往后对每层计算相应的\(z=wa+b\)和\(a=σ(z)\)</li>
<li><strong>输出层误差</strong>:根据BP1计算误差向量</li>
<li><strong>反向误差传播</strong>:对后往前根据BP2对每层计算误差向量</li>
<li><strong>输出</strong>:根据BP3和BP4计算代价函数的梯度.</li>
</ol>
<h5 id="dai-ma">代码</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">    <span class="string">"""Return a tuple "(nabla_b, nabla_w)" representing the</span></span><br><span class="line"><span class="string">    gradient for the cost function C_x.</span></span><br><span class="line"><span class="string">    "nabla_b" and "nabla_w" are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">    to "self.biases" and "self.weights"."""</span></span><br><span class="line"></span><br><span class="line">    nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">    nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># feedforward</span></span><br><span class="line">    activation = x</span><br><span class="line">    activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">    zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">      z = np.dot(w, activation)+b</span><br><span class="line">      zs.append(z)</span><br><span class="line">      activation = sigmoid(z)</span><br><span class="line">      activations.append(activation)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward pass</span></span><br><span class="line">    delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * sigmoid_prime(zs[<span class="number">-1</span>])</span><br><span class="line">    nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">    nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line"><span class="comment"># differently to the notation in Chapter 2 of the book.</span></span><br><span class="line"><span class="comment"># Here,l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line"><span class="comment"># second-last layer, and so on.It's a renumbering of the</span></span><br><span class="line"><span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line"><span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">    	z = zs[-l]</span><br><span class="line">      sp = sigmoid_prime(z)</span><br><span class="line">    	delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">    	nabla_b[-l] = delta</span><br><span class="line">    	nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span><br><span class="line">    <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></span><br><span class="line">  <span class="string">"""Return the vector of partial derivatives \partial C_x /</span></span><br><span class="line"><span class="string">  \partial a for the output activations."""</span></span><br><span class="line">  	<span class="keyword">return</span> (output_activations-y)</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line"><span class="string">"""The sigmoid function."""</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line"><span class="string">"""Derivative of the sigmoid function."""</span></span><br><span class="line">  <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure>
<h4 id="jiao-cha-shang-dai-jie-han-shu">交叉熵代价函数</h4>
<p>人类通常在<strong>犯错比较明显</strong>的时候学习的<strong>速度最快</strong>.而神经元在这种<strong>饱和</strong>情况下学习很有<strong>难度</strong>(也就是偏导数很小),这是因为<strong>二次代价函数</strong>关于权重和偏置的<strong>偏导数</strong>是\(\frac{\partial C}{\partial w}=(a-y) \sigma^{\prime}(z) x=a \sigma^{\prime}(z)\)和\(\frac{\partial C}{\partial b}=(a-y) \sigma^{\prime}(z)=a \sigma^{\prime}(z)\)而σ函数的<strong>导数</strong>在接近0和1时都很小。</p>
<p>解决上述问题的方法是引入<strong>交叉熵代价函数</strong>:\(C=-\frac{1}{n} \sum_{x}[y \ln a+(1-y) \ln (1-a)]\)</p>
<ol>
<li>它是<strong>非负</strong>的,并且当实际输出接近目标值时它<strong>接近0</strong>,因此可以作为代价函数.</li>
<li>它关于权重的<strong>偏导数</strong>是\(\frac{\partial C}{\partial w_{j}}=\frac{1}{n} \sum_{x} x_{j}(\sigma(z)-y)\),也就是误差越大,学习速度越快.</li>
<li>如果输出层是<strong>线性神经元</strong>,那么<strong>二次代价函数</strong>不再会导致学习速度下降的问题,可以选用.如果输出神经元是<strong>S型神经元</strong>,<strong>交叉熵</strong>一般都是更好的选择.</li>
<li>对于多分类交叉熵代价函数为：\(C=-\sum_{j=1}^{T} y_{j} \log {p_j}\),</li>
</ol>
<h4 id="guo-ni-he">过拟合</h4>
<ol>
<li>最好的降低过拟合的方式就是<strong>增加训练样本量</strong>,但训练数据其实是很<strong>难得</strong>的资源.</li>
<li><strong>规范化</strong>也是一种缓解过拟合的技术.效果是让网络倾向于学习<strong>小一点的权重</strong>,它是寻找小的权重和最小化原始代价函数之间的折中,相对重要性由λ控制.
<ol>
<li><strong>L2规范化(权重衰减)<strong>的想法是增加一个额外的</strong>规范化项</strong>：\(\frac{\lambda}{2 n} \sum_{w} w^{2}\)到代价函数上.其中λ是<strong>规范化参数</strong>,注意规范化项里<strong>不包含偏置</strong>.</li>
<li><strong>L1规范化</strong>的规范化项为：\(\frac{\lambda}{n} \sum_{w}|w|\)在L1规范化中,权重通过一个常量<strong>向0</strong>进行缩小,在L2规范化中,权重通过一个和w成<strong>比例</strong>的量进行缩小.所以L1规范化倾向于<strong>聚集</strong>网络的权重在<strong>相对少量</strong>的高重要度连接上.</li>
</ol>
</li>
<li><strong>dropout</strong></li>
<li><strong>人为扩展训练数据</strong></li>
</ol>
<h4 id="diao-can">调参</h4>
<ol>
<li><strong>学习速率η</strong></li>
<li><strong>早停</strong></li>
<li><strong>规范化参数λ</strong></li>
<li><strong>小批量数据大小</strong></li>
<li><strong>网格搜索(grid search)</strong></li>
</ol>
<h3 id="xun-huan-shen-jing-wang-luo">循环神经网络</h3>
<h4 id="rnn">RNN</h4>
<p>对于前馈神经网络来说其公式：<br>
\[
O=f(W*X+b)
\]<br>
其中\(W\)和\(b\)是模型的参数，\(X\)是当前的输入，\(f(·)\)是激活函数，\(*\)是矩阵乘法，\(O\)是当前的输出。即输出等于输入经过线性与非线性映射后的结果。</p>
<p>而RNN通过将前一时刻的运算结果添加到当前的运算中，从而实现了“考虑上文信息”的功能。<br>
\[
\mathrm{O}_{\mathrm{t}}=\mathrm{f}\left(\mathrm{X} * \mathrm{W}+\mathrm{O}_{\mathrm{t}-1} * \mathrm{V}+\mathrm{b}\right)
\]<br>
其中\(W,V,b\)是模型的参数，下标\(t\)代表当前的序列位置／时间点，\(t-1\)代表上个位置/上个时间点，\(X\)是当前的输入，\(f(·)\)是激活函数，\(*\)是矩阵乘法，\(O\)是模型输出。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/rnn.png" alt="avatar"></p>
<p>BiRnn:RNN可以考虑上文的信息，那么如何将下文的信息也添加进去呢？这就是BiRNN要做的事情。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/birnn.png" alt="avatar"></p>
<h5 id="rnn-cun-zai-de-wen-ti">RNN存在的问题</h5>
<p>首先，从RNN的前向过程来看，可以认为它只有一层权重矩阵W（先不管V）。由此可见从深层的角度去看RNN的前向过程，就可以认为RNN是<strong>各个层的权重矩阵相同</strong>的深层网络。忽略V和激活函数，就可以近似的认为网络一共有T层（T等于序列的长度），那么第t层的输出就是连乘t次W，也就是\(W^t\)！,由于<strong>矩阵可以用它的特征值矩阵和特征向量矩阵去近似</strong>，即<br>
\[
W = V diag(\lambda) V^{-1}  \\
W^t = (V diag(\lambda) V^{-1})(V diag(\lambda) V^{-1})\cdots(V diag(\lambda) V^{-1})=V{ diag(\lambda)}^t V^{-1}
\]<br>
也就是说，**特征值矩阵中的每个特征值都会随着t的增大发生指数级变化！**所以某个特征值大于1时，就容易导致这个维度的值爆炸性增长；当特征值小于1时，会就会导致这个维度的值指数级速度衰减为0！</p>
<p>前向过程如此，误差反向传播的过程也必然近似为输出层的误差会乘以\(W^t\)来得到倒数第t层的梯度，然而由于刚各个维度不是指数级衰减就是指数级爆炸，很容易看出当更新RNN的靠前的层的时候（即离着输出层较远的那些层，或者说与序列末端离得远的位置），计算出的梯度要么大的吓人，要么小的忽略。小的忽略的值不会对W的更新有任何指导作用，大的吓人的值一下子就把W中的某些值弄的大的吓人了，害得前面的优化都白做了。</p>
<p>一个很简单的想法是进行<strong>梯度截断</strong>，在优化RNN的参数的时候，给梯度值设置一个上限。但是这样显然就会导致模型难以再顾及很靠前的历史信息了，因此理论上RNN可以保存任意长的历史信息来辅助当前时间点的决策，然而由于在优化时（训练RNN时），梯度无法准确合理的传到很靠前的时间点，因此RNN实际上只能记住并不长的序列信息（在NLP中，经验上认为序列大于30的时候，RNN基本记不住多于30的部分，而从多于10的位置开始就变得记性很差了），因此RNN相比前馈网络，可以记住的历史信息要长一些，但是无法记住长距离的信息（比如段落级的序列甚至篇章级的序列，用RNN就鸡肋了）。</p>
<h4 id="lstm">LSTM</h4>
<p>因为RNN存在梯度弥散和梯度爆炸的问题，所以RNN很难完美地处理具有长期依赖的信息。既然仅仅依靠一条线连接后面的神经元不足以解决问题，那么就再加一条线好了，这就是LSTM。</p>
<p>LSTM的关键在于细胞的状态和穿过细胞的线，细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流动保持不变会变得容易。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/lstm_c.png" alt="avatar"></p>
<p>在LSTM中，门可以实现选择性的让信息通过，主要通过一个sigmoid的神经层和一个逐点相乘的操作来实现。LSTM通过三个这样的门结构来实现信息的保护和控制，分别是遗忘门（forget gate）、输入门（input gate）与输出门（output gate）。</p>
<h5 id="strong-yi-wang-men-strong"><strong>遗忘门</strong></h5>
<p>在LSTM中的第一步是决定从细胞状态中丢弃什么信息。这个决定通过一个称为遗忘门的结构来完成。遗忘门会读取\(h_{t-1}\)和\(x_t\)，输出一个0到1之间的数值给细胞的状态\(c_{t-1}\)中的数字。1表示完全保留，0表示完全舍弃。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/lstm_forget_gate.png" alt="avatar"></p>
<h5 id="strong-shu-ru-men-strong"><strong>输入门</strong></h5>
<p>遗忘门决定让多少新的信息加入到cell的状态中来。实现这个需要两个步骤：</p>
<ol>
<li>首先一个叫“input gate layer”的sigmoid层决定哪些信息需要更新；一个tanh层生成一个向量，用来更新状态C。</li>
</ol>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/input_gate.png" alt="avatar"></p>
<ol start="2">
<li>把 1 中的两部分联合起来，对cell的状态进行更新，我们把旧的状态与\(f_t\)相乘，丢弃掉我们确定需要丢弃的信息，接着加上\(i_t * \tilde{C}_{t}\)</li>
</ol>
<h5 id="shu-chu-men">输出门</h5>
<p>最终，我们需要确定输出什么值，这个输出将会基于我们的细胞的状态，但是也是一个过滤后的版本。首先，我们通过一个sigmoid层来确定细胞状态的哪些部分将输出出去。接着，我们把细胞状态通过tanh进行处理（得到一个-1到1之间的值）并将它和sigmoid门的输出相乘，最终我们仅仅会输出我们确定输出的部分。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/output_gate.png" alt="avatar"></p>
<h5 id="gong-shi">公式</h5>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/lstm_all.png" alt="avatar"></p>
<h4 id="gru">GRU</h4>
<p>在LSTM中引入了三个门函数：输入门、遗忘门和输出门来控制输入值、记忆值和输出值。而在GRU模型中只有两个门：分别是更新门和重置门。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/GRU.png" alt="avatar"></p>
<p>图中的\(z_t\)和\(r_t\)分别表示更新门和重置门。更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多。重置门控制前一状态有多少信息被写入到当前的候选集 \(\tilde{h}_{t}\)上，重置门越小，前一状态的信息被写入的越少。</p>
<p>LSTM和GRU都是通过各种门函数来将重要特征保留下来，这样就保证了在long-term传播的时候也不会丢失。此外GRU相对于LSTM少了一个门函数，因此在参数的数量上也是要少于LSTM的，所以整体上GRU的训练速度要快于LSTM的。</p>
<h3 id="self-attention">Self-Attention</h3>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention.png" alt="avatar"></p>
<p><strong>核心思想</strong>：在decoding阶段对input中的信息赋予不同权重。</p>
<ol>
<li>Encode所有输入序列,得到对应的\(h_1,h_2, \cdots ,h_T\)(T为输入序列长度)</li>
<li>Decode输出目标\(y_t\)之前，会将上一步输出的隐藏状态\(S_{t-1}\)与之前encode好的\(h_1,h_2,\cdots,h_T\)进行比对，计算相似度（\(e_{t,j}=a(s_{t-1},h_j)\)）,\(h_j\)为之前第j个输入encode得到的隐藏向量，a为任意一种计算相似度的方式</li>
<li>然后通过softmax，即\(a_{t,j}=\frac{exp(e_{t,j})}{\sum^{T_x}_{k=1}exp(e_{t,k})}\)将之前得到的各个部分的相关系数进行归一化，得到\(a_{t,1},a_{t,2},\cdots,a_{t,T}\)</li>
<li>在对输入序列的隐藏层进行相关性加权求和得到此时decode需要的context vector ：\(c_j=\sum^{T_x}_{j=1}a_{i,j}h_j\)</li>
</ol>
<h3 id="ci-xiang-liang-mo-xing">词向量模型</h3>
<p>onehot编码方式的缺点：</p>
<ol>
<li>它是一个词袋模型，不考虑词与词之间的顺序（文本中词的顺序信息也是很重要的）</li>
<li>它假设词与词相互独立（在大多数情况下，词与词是相互影响的）</li>
<li>它得到的特征是离散稀疏的，实际应用中，面临着巨大的维度灾难问题</li>
</ol>
<p>将高维词向量嵌入到一个低维空间。word2vec是词嵌入方式的一种。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/word_embedding.png" alt="avatar"></p>
<p>主要解决两个问题：</p>
<ol>
<li>一个是统计语言模型里关注的条件概率\(𝑝(𝑤𝑡|𝑐𝑜𝑛𝑡𝑒𝑥𝑡)\)的计算</li>
<li>一个是向量空间模型里关注的词向量的表达</li>
</ol>
<h4 id="word-2-vec">word2vec</h4>
<h5 id="strong-c-bo-w-mo-xing-strong"><strong>CBoW模型</strong></h5>
<p>CBOW是已知上下文，估算当前词语的语言模型。其学习目标是最大化对数似然函数：<br>
\[
\mathcal{L}=\sum_{w \in \mathcal{C}} \log p(w | \text {Context}(w))
\]<br>
其中，w表示语料库C中任意一个词。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/CBOW.png" alt="avatar"></p>
<ol>
<li><strong>输入层</strong>是上下文的词语的词向量（在训练CBOW模型，词向量只是个副产品，确切来说，是CBOW模型的一个参数。训练开始的时候，词向量是个随机值，随着训练的进行不断被更新）。</li>
<li><strong>投影层</strong>对其求和，所谓求和，就是简单的向量加法。</li>
<li><strong>输出层</strong>输出最可能的w。由于语料库中词汇量是固定的|C|个，所以上述过程其实可以看做一个多分类问题。给定特征，从|C|个分类中挑一个。</li>
</ol>
<p>对于神经网络模型多分类，最朴素的做法是softmax回归：<br>
\[
h_{\theta}\left(x^{(i)}\right)=\left[\begin{array}{c}
p\left(y^{(i)}=1 | x^{(i)} ; \theta\right) \\
p\left(y^{(i)}=2 | x^{(i)} ; \theta\right) \\
\vdots \\
p\left(y^{(i)}=k | x^{(i)} ; \theta\right)
\end{array}\right]=\frac{1}{\sum_{j=1}^{k} e^{\theta_{j}^{T} x^{(i)}}}\left[\begin{array}{c}
e^{\theta_{1}^{T} x^{(i)}} \\
e^{\theta_{2}^{T} x^{(i)}} \\
\vdots \\
e^{\theta_{k}^{T} x^{(i)}}
\end{array}\right]
\]</p>
<h5 id="skip-gram-mo-xing">skip-gram模型</h5>
<p><strong>target word对context的预测中学习word vector</strong><br>
\[
p(\text {Context}(w) | w)=\prod_{u \in \text {Context}(w)} p(u | w)
\]</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/skipgram.png" alt="avatar"></p>
<p>如何将Skip-gram模型的前向计算过程写成数学形式，我们得到：<br>
\[
p\left(w_{o} | w_{i}\right)=\frac{e^{U_{o} \cdot V_{i}}}{\sum_{j} e^{U_{j} V_{i}}}
\]</p>
<p>其中，其中，\(V_i\)是Embedding层矩阵里的列向量，也被称为\(w_i\)的input vector。\(U_j\)是softmax层矩阵里的行向量，也被称为\(w_i\)的output vector。因此，Skip-gram模型的本质是<strong>计算输入word的input vector与目标word的output vector之间的余弦相似度，并进行softmax归一化</strong>。我们要学习的模型参数正是这两类词向量。</p>
<p>然而，直接对词典里的V个词计算相似度并归一化，显然是一件极其耗时的impossible mission。引入了两种优化算法：<strong>层次Softmax（Hierarchical Softmax）<strong>和</strong>负采样（Negative Sampling）</strong>。</p>
<h5 id="hierarchical-softmax">Hierarchical Softmax</h5>
<p>softmax回归需要对语料库中每个词语（类）都计算一遍输出概率并进行归一化，在几十万词汇量的语料上无疑是令人头疼的。在SVM中的多分类，其多分类是由二分类组合而来的：</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/svm_hierarchical.gif" alt="avatar"></p>
<p>这是一种二叉树结构，应用到word2vec中被作者称为Hierarchical Softmax：</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/cbow_hierarchical.jpg" alt="avatar"></p>
<p>上图输出层的树形结构即为Hierarchical Softmax。非叶子节点相当于一个神经元（感知机，逻辑斯谛回归就是感知机的输出代入\(f(x)=\frac{1}{1+e^x}\)，二分类决策输出1或0，分别代表向下左转或向下右转；每个叶子节点代表语料库中的一个词语，于是每个词语都可以被01唯一地编码，并且其编码序列对应一个事件序列，于是我们可以计算条件概率\(p(w|Context(x))\)。</p>
<p>在开始计算之前，还是得引入一些符号：</p>
<ol>
<li>\(p^w\)从根结点出发到达w对应叶子结点的路径.</li>
<li>\(l^w\)路径中包含结点的个数</li>
<li>\(p^w_1,p^w_2,\cdots,p^w_{l^w}\)路径\(p^w\)中的各个节点</li>
<li>\(d^w_2,d^w_3,\cdots,d^w_{l^w} \in {0,1}\)词w的编码，\(d^w_j\)表示路径\(p^w\)第j个节点对应的编码（根节点无编码）</li>
<li>\(\theta^w_1,\theta^w_2,\cdots,\theta^w_{l^w-1} \in R^m\)路径\(p^w\)中非叶节点对应的参数向量</li>
</ol>
<p>于是可以给出\(w\)的条件概率：<br>
\[
p(w | \text { Context }(w))=\prod_{j=2}^{l^{w}} p\left(d_{j}^{w} | \mathbf{x}_{w}, \theta_{j-1}^{w}\right)
\]</p>
<p>这是个简单明了的式子，从根节点到叶节点经过了\(l^w-1\)个节点，编码从下标2开始（根节点无编码），对应的参数向量下标从1开始（根节点为1）。其中，每一项是一个逻辑斯谛回归：</p>
<p>\[
p\left(d_{j}^{w} | \mathbf{x}_{w}, \theta_{j-1}^{w}\right)=\left\{\begin{array}{ll}
\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right), &amp; d_{j}^{w}=0 \\
1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right), &amp; d_{j}^{w}=1
\end{array}\right.
\]<br>
考虑到d只有0和1两种取值，我们可以用指数形式方便地将其写到一起：<br>
\[
p\left(d_{j}^{w} | \mathbf{x}_{w}, \theta_{j-1}^{w}\right)=\left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{1-d^w_{j}} \cdot\left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{d^w_{j}}
\]<br>
我们的目标函数取对数似然：<br>
\[
\mathcal{L}=\sum_{w \in \mathcal{C}} \log p(w | \text { Context }(w))
\]<br>
将\(p(w|Context(w))\)代入上式，有<br>
\[
\begin{aligned}
\mathcal{L} &amp;=\sum_{w \in \mathcal{C}} \log \prod_{j=2}^{l^{w}}\left\{\left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{1-d_{j}^{v}} \cdot\left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{d_{j}}\right\} \\
&amp;=\sum_{w \in \mathcal{C}} \sum_{j=2}^{l^{w}}\left\{\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]+d_{j}^{w} \cdot \log \left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]\right\}
\end{aligned}
\]<br>
这也很直白，连乘的对数换成求和。不过还是有点长，我们把每一项简记为：<br>
\[
\mathcal{L}(w, j)=\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]+d_{j}^{w} \cdot \log \left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]
\]<br>
怎么最大化对数似然函数呢？分别最大化每一项即可（这应该是一种近似，最大化某一项不一定使整体增大，具体收敛的证明还不清楚）。怎么最大化每一项呢？先求函数对每个变量的偏导数，对每一个样本，代入偏导数表达式得到函数在该维度的增长梯度，然后让对应参数加上这个梯度，函数在这个维度上就增长了。</p>
<p>每一项有两个参数，一个是每个节点的参数向量\(\theta^w_{j-1}\)，另一个是输出层的输入\(X_w\)，我们分别对其求偏导数：<br>
\[
\frac{\partial \mathcal{L}(w, j)}{\partial \theta_{j-1}^{w}}=\frac{\partial}{\partial \theta_{j-1}^{w}}\left\{\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]+d_{j}^{w} \cdot \log \left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]\right\}
\]<br>
因为sigmoid函数的导数有个很棒的形式：<br>
\[
\sigma^{\prime}(x)=\sigma(x)[1-\sigma(x)]
\]<br>
于是代入上上式得到：<br>
\[
\left(1-d_{j}^{w}\right)\left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right] \mathbf{x}_{w}-d_{j}^{w} \sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right) \mathbf{x}_{w}
\]<br>
合并同类项得到：<br>
\[
\left[1-d_{j}^{w}-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right] \mathbf{x}_{w}
\]<br>
于是\(\theta^w_{j-1}\)的更新表达式就得到了：<br>
\[
\theta_{j-1}^{w}:=\theta_{j-1}^{w}+\eta\left[1-d_{j}^{w}-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right] \mathbf{x}_{w}
\]<br>
再来\(X_w\)的偏导数，注意到\(\mathcal{L}(w, j)=\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]+d_{j}^{w} \cdot \log \left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]\)中\(X_w\)和\(\theta^w_{j-1}\)是对称的，所有直接将\(\theta^w_{j-1}\)的偏导数中的\(\theta^w_{j-1}\)替换为\(X_w\)，得到关于\(X_w\)的偏导数：</p>
<p>\[
\frac{\partial \mathcal{L}(w, j)}{\partial \mathbf{x}_{w}}=\left[1-d_{j}^{w}-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right] \theta_{j-1}^{w}
\]</p>
<p>不过\(X_w\)是上下文的词向量的和，不是上下文单个词的词向量。怎么把这个更新量应用到单个词的词向量上去呢？word2vec采取的是直接将\(X_w\)的更新量整个应用到每个单词的词向量上去：<br>
\[
\mathbf{v}(\tilde{w}):=\mathbf{v}(\tilde{w})+\eta \sum_{j=2}^{l^{w}} \frac{\partial \mathcal{L}(w, j)}{\partial \mathbf{x}_{w}}, \quad \tilde{w} \in \text { Context }(w)
\]<br>
其中，\(V(\tilde w)\)代表上下文中某一个单词的词向量。</p>
<h4 id="glove">glove</h4>
<p>CBOW和skip-gram虽然可以很好地进行词汇类比，但是因为这两种方法是基于一个个局部的上下文窗口方法，因此，没有有效地利用全局的词汇共现统计信息。为了克服全局矩阵分解和局部上下文窗口的缺陷，GloVe方法基于全局词汇共现的统计信息来学习词向量，从而将统计信息与局部上下文窗口方法的优点都结合起来，并发现其效果确实得到了提升。</p>
<h5 id="li-zi">例子</h5>
<p>假设i=ice,j=steam并对k取不同的词汇，如“solid”，“gas”，“water”，“fashion”，根据上面的定义，我们分别计算他们的概率\(P(k∣ice)\)、\(P(k∣steam)\)，并计算两者的比率\(P(k∣ice)/P(k∣steam)\)，可以发现，对于“solid”，其出现在“ice”上下文的概率应该比较大，出现在“steam”上下文的概率应该比较小，因此，他们的比值应该是一个比较大的数，在下表中是8.9，而对于“gas”，出现在“ice”上下文的概率应该比较小，而出现在“steam”上下文的概率应该比较大，因此，两者的比值应该是一个比较小的数，在下表中是\(8.5×10^{-2}\) ，而对于“water、fashion”这两个词汇，他们与“ice”和steam“的相关性应该比较小，因此，他们的比值应该都是接近1。因此，这样来看可以发现，比值\(P(k∣ice)/P(k∣steam)\)在一定程度上可以反映词汇之间的相关性，当相关性比较低时，其值应该在1附近，当相关性比较高时，比值应该偏离1比较远。<br>
<img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/glove.png" alt="avatar"></p>
<h5 id="yuan-li">原理</h5>
<p>基于这样的思想，作者提出了这样一种猜想，能不能通过训练词向量，使得词向量经过某种函数计算之后可以得到上面的比值，具体如下：<br>
\[
F\left(w_{i}, w_{j}, \tilde{w}_{k}\right)=\frac{P_{i k}}{P_{j k}}
\]</p>
<p>其中，\(w_i\),\(w_j\),\(\tilde{w}_{k}\) 为词汇i,j,k对应的词向量，其维度都为d，而\(P_{ik}\)，\(P_{jk}\) 则可以直接通过语料计算得到，这里F为一个未知的函数。由于词向量都是在一个线性向量空间，因此，可以对\(w_i\),\(w_j\)进行差分，将上式转变为如下：<br>
\[
F\left(w_{i}-w_{j}, \tilde{w}_{k}\right)=\frac{P_{i k}}{P_{j k}}
\]<br>
由于上式中左侧括号中是两个维度为d的词向量，而右侧是一个标量，因此，很容易会想到向量的内积，因此，上式可以进一步改变为:<br>
\[
F\left(\left(w_{i}-w_{j}\right)^{T} \tilde{w}_{k}\right)=F\left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\right)=\frac{P_{i k}}{P_{j k}}
\]<br>
由于上式中左侧是一种减法，而右侧是一种除法，很容易联想到指数计算，因此，可以把F限定为指数函数，此时有：<br>
\[
\exp \left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\right)=\frac{\exp \left(w_{i}^{T} w_{k}\right)}{\exp \left(w_{j}^{T} w_{k}\right)}=\frac{P_{i k}}{P_{j k}}
\]<br>
因此，此时只要确保等式两边分子分母相等即可，即：<br>
\[
\exp \left(w_{i}^{T} w_{k}\right)=P_{i k}, \exp \left(w_{j}^{T} w_{k}\right)=P_{j k}
\]<br>
进一步的，可以转化为对语料中的所有词汇，考察\(exp(w^T_iw_k)\)=\(P_{ik}\)=\(\frac{X_{ik}}{X_i}\) ，即：<br>
\[
w_{i}^{T} w_{k}=\log \left(\frac{X_{i k}}{X_{i}}\right)=\log X_{i k}-\log X_{i}
\]<br>
由于上式左侧\(w^T_iw_k\)中，调换i和k的值不会改变其结果，即具有对称性，因此，为了确保等式右侧也具备对称性，引入了两个偏置项，即<br>
\[
w_{i}^{T} w_{k}=\log X_{i k}-b_{i}-b_{k}
\]<br>
此时，\(logX_i\)已经包含在\(bi\)当中。因此，此时模型的目标就转化为通过学习词向量的表示，使得上式两边尽量接近，因此，可以通过计算两者之间的平方差来作为目标函数，即：<br>
\[
J=\sum_{i, k=1}^{V}\left(w_{i}^{T} \tilde{w}_{k}+b_{i}+b_{k}-\log X_{i k}\right)^{2}
\]<br>
但是这样的目标函数有一个缺点，就是对所有的共现词汇都是采用同样的权重，因此，作者对目标函数进行了进一步的修正，通过语料中的词汇共现统计信息来改变他们在目标函数中的权重，具体如下：<br>
\[
J=\sum_{i, k=1}^{V} f\left(X_{i k}\right)\left(w_{i}^{T} \tilde{w}_{k}+b_{i}+b_{k}-\log X_{i k}\right)^{2}
\]<br>
这里\(V\)表示词汇的数量，并且权重函数\(f\)必须具备以下的特性</p>
<ul>
<li>\(f(0)=0\)，当词汇共现的次数为0时，此时对应的权重应该为0。</li>
<li>f(x)必须是一个非减函数，这样才能保证当词汇共现的次数越大时，其权重不会出现下降的情况。</li>
<li>对于那些太频繁的词，\(f(x)\)应该能给予他们一个相对小的数值，这样才不会出现过度加权。</li>
</ul>
<p>综合以上三点特性，作者提出了下面的权重函数：<br>
\[
f(x)=\left\{\begin{array}{cc}
\left(x / x_{\max }\right)^{\alpha} &amp; \text { if } x&lt;x_{\max } \\
1 &amp; \text { otherwise }
\end{array}\right.
\]</p>
<h5 id="zong-jie">总结</h5>
<ol>
<li>Glove综合了全局词汇共现的统计信息和局部窗口上下文方法的优点，可以说是两个主流方法的一种综合，但是相比于全局矩阵分解方法，由于GloVe不需要计算那些共现次数为0的词汇，因此，可以极大的减少计算量和数据的存储空间。</li>
<li>但是GloVe把语料中的词频共现次数作为词向量学习逼近的目标，当语料比较少时，有些词汇共现的次数可能比较少，笔者觉得可能会出现一种误导词向量训练方向的现象。</li>
</ol>
<h4 id="elmo">ELMO</h4>
<h5 id="zhen-dui-de-wen-ti">针对的问题</h5>
<p>word2vec中，只有apple一个词向量，无法对一词多义进行建模。</p>
<h5 id="yuan-li-1">原理</h5>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/EMLO.png" alt="avatar"></p>
<p>ELMo的主要做法是先训练一个完整的语言模型，再用这个语言模型去处理需要训练的文本，生成相应的词向量，所以在文中一直强调ELMo的模型对同一个字在不同句子中能生成不同的词向量。</p>
<p>在进行有监督的NLP任务时，可以将ELMo直接当做特征拼接到具体任务模型的词向量输入或者是模型的最高层表示上。不像传统的词向量，每一个词只对应一个词向量，ELMo利用预训练好的双向语言模型，然后根据具体输入从该语言模型中可以得到上下文依赖的当前词表示（<strong>对于不同上下文的同一个词的表示是不一样的</strong>），再当成特征加入到具体的NLP有监督模型里。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/emlo_embedding.png" alt="avatar"></p>
<p>EMLO使用的是一个<strong>双向的LSTM语言模型</strong>，由一个前向和一个后向语言模型构成，目标函数就是取这两个方向语言模型的最大似然。</p>
<p>前向LSTM结构：<br>
\[
p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} | t_{1}, t_{2}, \ldots, t_{k-1}\right)
\]<br>
反向LSTM结构：<br>
\[
p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} | t_{k+1}, t_{k+2}, \ldots, t_{N}\right)
\]<br>
最大似然函数：<br>
\[
\sum_{k=1}^{N}\left(\log p\left(t_{k} | t_{1}, t_{2}, \ldots, t_{k-1}\right)+\log p\left(t_{k} | t_{k+1}, t_{k+2}, \ldots, t_{N}\right)\right)
\]</p>
<h5 id="zong-jie-1">总结</h5>
<ol>
<li>ELMo的假设前提一个词的词向量不应该是固定的，所以在一词多意方面ELMo的效果一定比word2vec要好。</li>
<li>word2vec的学习词向量的过程是通过中心词的上下窗口去学习，学习的范围太小了，而ELMo在学习语言模型的时候是从整个语料库去学习的，而后再通过语言模型生成的词向量就相当于基于整个语料库学习的词向量，更加准确代表一个词的意思。</li>
</ol>
<h4 id="gpt">GPT</h4>
<p>用Transformer网络代替了LSTM作为语言模型来更好的捕获长距离语言结构。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/gpt_1.png" alt="avatar"></p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/gpt_2.png" alt="avatar"></p>
<h5 id="masked-attention">Masked Attention</h5>
<p>在transformer中，Encoder因为要编码整个句子，所以每个词都需要考虑上下文的关系。所以每个词在计算的过程中都是可以看到句子中所有的词的。但是Decoder与Seq2Seq中的解码器类似，每个词都只能看到前面词的状态，所以是一个单向的Self-Attention结构。Masked Attention的实现也非常简单，只要在普通的Self Attention的Softmax步骤之前，与按位乘上一个下三角矩阵M就好了</p>
<p>attention：<br>
\[
\begin{aligned}
Q &amp;=X W_{Q} \\
K &amp;=X W_{K} \\
V &amp;=X W_{V} \\
\text {Attention}(Q, K, V) &amp;=\operatorname{Softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
\end{aligned}
\]<br>
masked-attention<br>
\[
\text { Attention }(Q, K, V)=\operatorname{Softmax}\left(\frac{Q K^{T} \Theta M}{\sqrt{d_{k}}}\right) V
\]</p>
<h4 id="bert">BERT</h4>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/bert_gpt_elmo.png" alt="avatar"></p>
<h5 id="task-1-mlm">Task 1: MLM</h5>
<p>由于BERT需要通过上下文信息，来预测中心词的信息，同时又不希望模型提前看见中心词的信息，因此提出了一种 Masked Language Model 的预训练方式，即随机从输入预料上 mask 掉一些单词，然后通过的上下文预测该单词，类似于一个完形填空任务。</p>
<p>在预训练任务中，15%的 Word Piece 会被mask，这15%的 Word Piece 中，80%的时候会直接替换为 [Mask] ，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token</p>
<ul>
<li>没有100%mask的原因
<ul>
<li>如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词</li>
</ul>
</li>
<li>加入10%随机token的原因
<ul>
<li>Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘</li>
<li>另外编码器不知道哪些词需要预测的，哪些词是错误的，因此被迫需要学习每一个token的表示向量</li>
</ul>
</li>
<li>另外，每个batchsize只有15%的单词被mask的原因，是因为性能开销的问题，双向编码器比单项编码器训练要更慢</li>
</ul>
<h5 id="task-2-nsp">Task 2: NSP</h5>
<p>仅仅一个MLM任务是不足以让 BERT 解决阅读理解等句子关系判断任务的，因此添加了额外的一个预训练任务，即 Next Sequence Prediction。</p>
<p>具体任务即为一个句子关系判断任务，即判断句子B是否是句子A的下文，如果是的话输出’IsNext‘，否则输出’NotNext‘。</p>
<p>训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在图4中的[CLS]符号中</p>
<h5 id="shu-ru">输入</h5>
<ul>
<li>Token Embeddings：即传统的词向量层，每个输入样本的首字符需要设置为[CLS]，可以用于之后的分类任务，若有两个不同的句子，需要用[SEP]分隔，且最后一个字符需要用[SEP]表示终止</li>
<li>Segment Embeddings：为[0,1]序列，用来在NSP任务中区别两个句子，便于做句子关系判断任务</li>
<li>Position Embeddings：与Transformer中的位置向量不同，BERT中的位置向量是直接训练出来的</li>
</ul>
<h5 id="fine-tunninng">Fine-tunninng</h5>
<p>对于不同的下游任务，我们仅需要对BERT不同位置的输出进行处理即可，或者直接将BERT不同位置的输出直接输入到下游模型当中。具体的如下所示：</p>
<ul>
<li>对于情感分析等单句分类任务，可以直接输入单个句子（不需要[SEP]分隔双句），将[CLS]的输出直接输入到分类器进行分类</li>
<li>对于句子对任务（句子关系判断任务），需要用[SEP]分隔两个句子输入到模型中，然后同样仅须将[CLS]的输出送到分类器进行分类</li>
<li>对于问答任务，将问题与答案拼接输入到BERT模型中，然后将答案位置的输出向量进行二分类并在句子方向上进行softmax（只需预测开始和结束位置即可）</li>
<li>对于命名实体识别任务，对每个位置的输出进行分类即可，如果将每个位置的输出作为特征输入到CRF将取得更好的效果。</li>
</ul>
<h5 id="que-dian">缺点</h5>
<ul>
<li>BERT的预训练任务MLM使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务</li>
<li>另外，BERT没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计</li>
<li>由于最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）</li>
<li>适合处理自然语义理解类任务(NLU)，而不适合自然语言生成类任务(NLG)</li>
</ul>
<h5 id="torch-shi-yong">torch使用</h5>
<ol>
<li>导入相关库</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> transformers <span class="keyword">as</span> ppb <span class="comment"># pytorch transformers</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>导入预训练好的 DistilBERT 模型与分词器</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, <span class="string">'distilbert-base-uncased'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Want BERT instead of distilBERT? Uncomment the following line:</span></span><br><span class="line"><span class="comment">#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')# </span></span><br><span class="line"></span><br><span class="line">Load pretrained model/tokenizertokenizer = tokenizer_class.from_pretrained(pretrained_weights)</span><br><span class="line">model = model_class.from_pretrained(pretrained_weights)</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>tokenize：分词+[cls]+[sep]<br>
<img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/bert_tokenize.png" alt="avatar"></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.encode(<span class="string">"a visually stunning rumination on love"</span>, add_special_tokens=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>padding</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">   max_len = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tokenized.values:</span><br><span class="line">    <span class="keyword">if</span> len(i) &gt; max_len:</span><br><span class="line">        max_len = len(i)</span><br><span class="line">padded = np.array([i + [<span class="number">0</span>]*(max_len-len(i)) <span class="keyword">for</span> i <span class="keyword">in</span> tokenized.values])</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>masking</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">attention_mask = np.where(padded != <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ol start="6">
<li>使用bert进行编码</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># last_hidden_states = [batch_size, max_sentence_len, 768]</span></span><br><span class="line">    last_hidden_states = model(input_ids, attention_mask=attention_mask)</span><br></pre></td></tr></table></figure>
<h3 id="transformer">transformer</h3>
<p>无论是RNN还是CNN，在处理NLP任务时都有缺陷。CNN是其先天的卷积操作不很适合序列化的文本，RNN是其没有并行化，很容易超出内存限制（比如50tokens长度的句子就会占据很大的内存）。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/transformer.png" alt="avatar"></p>
<h4 id="wen-ti">问题</h4>
<ol>
<li>如何实现并行计算同时缩短依赖距离？采用自注意机制</li>
<li>如何向CNN一样考虑多通道信息？采用多头注意力</li>
<li>自注意力机制损失了位置信息，如何补偿? 位置嵌入</li>
<li>后面的层中位置信息消散？ 残差连接</li>
</ol>
<h4 id="self-attention-1">self-attention</h4>
<img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention-1.png" alt="avatar">
<h5 id="bing-xing-hua">并行化</h5>
<img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention-2.png" alt="avatar" style="zoom:67%;">
<img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention-3.png" alt="avatar" style="zoom: 67%;">
<img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention-4.png" alt="avatar" style="zoom: 67%;">
<h5 id="multi-head-self-attention">multi-head self-attention</h5>
<img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/multi-head.png" alt="avatar" style="zoom:67%;">
<img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/multi-head-1.png" alt="avatar">
<h4 id="encoder">encoder</h4>
<p>Encoder由N=6个相同的layer组成，layer指的就是上图左侧的单元，最左边有个“Nx”，这里是x6个。每个Layer由两个sub-layer组成，分别是multi-head self-attention mechanism和fully connected feed-forward network。其中每个sub-layer都加了residual connection和normalisation，因此可以将sub-layer的输出表示为：<br>
\[
sub_layer_output=LayerNorm(x+(\text {SubLayer}(x)))
\]</p>
<h5 id="multi-head-self-attention-1">Multi-head self-attention</h5>
<p>由于attention：<br>
\[
attention_output=Attention(Q, K, V)
\]<br>
multi-head attention则是通过h个不同的<strong>线性变换</strong>对Q，K，V进行投影，最后将不同的attention结果拼接起来：<br>
\[
\begin{array}{l}\text { MultiHead(Q,K,V)}= Concat(head_1,\ldots,head_h)W^{O}  \\ head_i = Attention(QW_i\left.^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \\ \text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V\end{array}
\]</p>
<p>多个head带来的优势是：不同的head，所attention的内容不同，比如：\(head_1\) attention 全局的内容，\(head_2\) attention local的内容。</p>
<h5 id="position-wise-feed-forward-networks">Position-wise feed-forward networks</h5>
<p>这层主要是提供非线性变换。Attention输出的维度是[bsz<em>seq_len, num_heads</em>head_size]，第二个sub-layer是个全连接层，之所以是position-wise是因为过线性层时每个位置i的变换参数是一样的.</p>
<h4 id="strong-decoder-strong"><strong>Decoder</strong></h4>
<p>Decoder和Encoder的结构差不多，但是多了一个attention的sub-layer,decoder的输入输出和解码过程:</p>
<ul>
<li>输出：对应i位置的输出词的概率分布</li>
<li>输入：encoder的输出 &amp; 对应i-1位置decoder的输出。所以中间的attention不是self-attention，它的K，V来自encoder，Q来自上一位置decoder的输出</li>
<li>解码：<strong>编码可以并行计算，一次性全部encoding出来，但解码不是一次把所有序列解出来的，而是像rnn一样一个一个解出来的</strong>，因为要用上一个位置的输入当作attention的query</li>
</ul>
<h4 id="que-dian-1">缺点</h4>
<ol>
<li>
<p>不能一次对很长文本进行建模（假如文本长度为\(10^5\),在一个forward的模块中需要\(10^5 * 10^5 = 10^{10}\)大小的矩阵来保存score，很容易导致 out of memory 问题）。解决方法：divide long sequence into smaller sequence. 但是这样导致smaller sequence 之间是没有联系的。（使用rnn的方式来建立smaller sequence 之间的联系–&gt; transformer XL）</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/transformer-xl.png" alt="avatar"></p>
<p>在引入rnn的时候，遇到了absolute position的问题，用relative position进行了解决。</p>
</li>
</ol>
<h5 id="zong-jie-2">总结</h5>
<p>引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</p>
<p>但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。</p>
<h3 id="can-kao">参考</h3>
<ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247484682&amp;idx=1&amp;sn=51520138eed826ec891ac8154ee550f9&amp;chksm=970c2ddca07ba4ca33ee14542cff0457601bb16236f8edc1ff0e617051d1f063354dda0f8893&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">从前馈到反馈：解析循环神经网络（RNN）及其tricks</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2018-07-06-7" target="_blank" rel="noopener">Step-by-step to LSTM: 解析LSTM神经网络设计原理</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/43493999" target="_blank" rel="noopener">Attention原理和源码解析</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2019-12-28?from=synced&amp;keyword=bert" target="_blank" rel="noopener">BERT模型超酷炫，上手又太难？请查收这份BERT快速入门指南！</a></li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习大总结</title>
    <url>/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="ji-qi-xue-xi-da-zong-jie">机器学习大总结</h2>
<h3 id="zhi-shi-dian">知识点</h3>
<h4 id="strong-jin-cheng-he-xian-cheng-strong"><strong>进程和线程</strong></h4>
<p>进程和线程都是一个时间段的描述，是CPU工作时间段的描述，不过是颗粒大小不同.进程就是包换上下文切换的程序执行时间总和 = CPU加载上下文+CPU执行+CPU保存上下文.线程是共享了进程的上下文环境的更为细小的CPU时间段。</p>
<h4 id="strong-pan-bie-shi-mo-xing-he-sheng-cheng-shi-mo-xing-strong"><strong>判别式模型和生成式模型</strong>:</h4>
<ol>
<li>判别式模型直接学习决策函数\(f(X)\)或条件概率分布\(P(Y|X)\)作为预测的模型.往往准确率更高,并且可以简化学习问题.
<ol>
<li>k近邻法、感知机、决策树、最大熵模型、Logistic回归、线性判别分析(LDA)、支持向量机(SVM)、Boosting、CRF、线性回归、神经网络</li>
</ol>
</li>
<li>生成式模型由数据学习<strong>联合概率分布P(X,Y)</strong>,然后由P(Y|X)=P(X,Y)/P(X)求出条件概率分布作为预测的模型,即生成模型.当存在隐变量时只能用生成方法学习.
<ol>
<li>混合高斯模型和其他混合模型、隐马尔可夫模型(HMM)、朴素贝叶斯、依赖贝叶斯(AODE)、LDA文档主题生成模型</li>
</ol>
</li>
</ol>
<a id="more"></a>
<h4 id="strong-gai-lu-zhi-liang-han-shu-gai-lu-mi-du-han-shu-lei-ji-fen-bu-han-shu-strong"><strong>概率质量函数,概率密度函数,累积分布函数</strong>:</h4>
<ol>
<li>概率质量函数 (probability mass function，PMF)是离散随机变量在各特定取值上的概率。</li>
<li>概率密度函数（probability density function，PDF)是对 连续随机变量 定义的，本身不是概率，只有对连续随机变量的取值进行积分后才是概率。</li>
<li>累积分布函数（cumulative distribution function，CDF)能完整描述一个实数随机变量X的概率分布，是概率密度函数的积分。对於所有实数x ，与pdf相对。</li>
</ol>
<h4 id="strong-ji-da-si-ran-gu-ji-strong"><strong>极大似然估计</strong></h4>
<p>已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。</p>
<h4 id="zui-xiao-er-cheng-fa">最小二乘法</h4>
<p>二乘的英文是least square,找一个（组）估计值,使得实际值与估计值之差的平方加总之后的值最小\(Q=\min \sum_{i}^{n}\left(y_{i e}-y_{i}\right)^{2}\).求解方式是对参数求偏导,令偏导为0即可.样本量小时速度快.</p>
<h4 id="strong-ti-du-xia-jiang-fa-strong"><strong>梯度下降法</strong></h4>
<p>负梯度方向是函数值下降最快的方向,每次更新值都等于原值加学习率(<strong>步长</strong>)乘损失函数的<strong>梯度</strong>.每次都试一个步长看会不会下降一定的程度,如果没有的话就按比例减小步长.不断应用参数更新公式直到收敛,可以得到局部最小值.初始值的不同组合可以得到不同局部最小值.在最优点时会有震荡.</p>
<ol>
<li>
<p><strong>批量梯度下降(BGD)</strong>:每次都使用所有的m个样本来更新,容易找到全局最优解,但是m较大时速度较慢。\(\theta_{j}^{\prime}=\theta_{j}+\frac{1}{m} \sum_{i=1}^{m}\left(y^{i}-h_{\theta}\left(x^{i}\right)\right) x_{j}^{i}\)</p>
</li>
<li>
<p><strong>随机梯度下降(SGD)</strong>：每次只使用一个样本来更新,训练速度快,但是噪音较多,不容易找到全局最优解,以损失很小的一部分精确度和增加一定数量的迭代次数为代价,换取了总体的优化效率的提升.注意控制步长缩小,减少震荡.</p>
</li>
</ol>
<p>\[
\theta_{j}=\theta_{j}+\left(y^{i}-h_{\theta}\left(x^{i}\right)\right) x_{j}^{i}
\]</p>
<ol start="3">
<li><strong>小批量梯度下降(MBGD)</strong>:每次使用一部分样本来更新.</li>
</ol>
<h4 id="strong-niu-dun-fa-strong"><strong>牛顿法</strong></h4>
<p>牛顿法是<strong>二次收敛</strong>,因此收敛速度快.从几何上看是每次用一个二次曲面来拟合当前所处位置的局部曲面,而梯度下降法是用一个平面来拟合.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/newton.png" alt="avatar"></p>
<ol>
<li><strong>黑塞矩阵</strong>是由目标函数f(x)在点X处的二阶偏导数组成的n*n阶对称矩阵。</li>
<li><strong>牛顿法</strong>:将f(x)在x(k)附近进行<strong>二阶泰勒展开</strong>:\(f(x)=f\left(x^{(k)}\right)+g_{k}^{\top}\left(x-x^{(k)}\right)+\frac{1}{2}\left(x-x^{(k)}\right)^{\mathrm{T}} H\left(x^{(k)}\right)\left(x-x^{(k)}\right)\)。其中\(g_k\)是\(f(x)\)的梯度向量在\(x^{(k)}\)的值,\(H(x^{(k)})\)是\(f(x)\)的黑塞矩阵在点\(x^k\)的值.牛顿法利用极小点的必要条件\(f(x)\)处的梯度为0,每次迭代中从点\(x^{(k)}\)开始,假设\(\nabla f\left(x^{(k+1)}\right)=0\)，对二阶泰勒展开求偏导有 \(\nabla f(x)=g_{k}+H_{k}\left(x-x^{(k)}\right)\)，代入得到\(g_{k}+H_{k}\left(x^{(k+1)}-x^{(k)}\right)=0\),即\(\quad x^{(k+1)}=x^{(k)}-H_{k}^{-1} g_{k}\),以此为迭代公式就是牛顿法.</li>
</ol>
<h4 id="ni-niu-dun-fa">拟牛顿法</h4>
<p>用一个<strong>n阶正定矩阵Gk</strong>=G(x(k))来<strong>近似代替</strong>黑塞矩阵的<strong>逆矩阵</strong>就是拟牛顿法的基本思想.在牛顿法中黑塞矩阵满足的条件如下:\(g_{k+1}-g_{k}=H_{k}\left(x^{(k+1)}-x^{(k)}\right)\),令$ {y_{k}}=g_{k+1}-g_{k}, \quad \delta_{k}=x<sup>{(k+1)}-x</sup>{(k)}$，则有  $ H_{k}^{-1} y_{k}=\delta_{k}$,称为拟牛顿条件.</p>
<ol>
<li><strong>DFP算法</strong>:假设每一步\(G_{k+1}=G_{k}+P_{k}+Q_{k}\)，为使\(G_{k+1}\)满足拟牛顿条件,可使\(P_k\)和\(Q_k\)满足\(P_{k} y_{k}=\delta_{k}, Q_{k} y_{k}=-G_{k} y_{k}\),例如取\(P_{k}=\frac{\delta_{k} \delta_{k}^{\tau}}{\delta_{k}^{\top} y_{k}} \quad Q_{k}=-\frac{G_{k} y_{k} y_{k}^{\top} G_{k}}{y_{k}^{\top} G_{k} y_{k}}\)，就得到迭代公式\(G_{k+1}=G_{k}+\frac{\delta_{k} \delta_{k}^{\top}}{\delta_{k}^{\tau} y_{k}}-\frac{G_{k} y_{k} y_{k}^{\top} G_{k}}{y_{k}^{T} G_{k} y_{k}}\)</li>
<li><strong>BFGS算法</strong>: 最流行的拟牛顿算法.它用\(B_k\)逼近黑塞矩阵,此时相应的拟牛顿条件是\(B_{k+1} \delta_{k}=y_{k}\),假设每一步\(B_{k+1}=B_{k}+P_{k}+Q_{k}\)，则 \(P_k\) 和 \(Q_k\) 满足\(P_{k} \delta_{k}=y_{k}, \quad Q_{k} \delta_{k}=-B_{k} \delta_{k}\)，类似得到迭代公式\(B_{k+1}=B_{k}+\frac{y_{k} y_{k}^{\top}}{y_{k}^{\top} \delta_{k}}-\frac{B_{k} \delta_{k} \delta_{k}^{\mathrm{T}} B_{k}}{\delta_{k}^{\top} B_{k} \delta_{k}}\)</li>
</ol>
<h4 id="strong-xian-yan-gai-lu-he-hou-yan-gai-lu-strong"><strong>先验概率和后验概率</strong></h4>
<ol>
<li>先验概率就是事情发生前的预测概率.</li>
<li>后验概率是一种条件概率，它限定了事件为隐变量取值，而条件为观测结果。一般的条件概率，条件和事件可以是任意的.</li>
<li>贝叶斯公式\(P(y|x) = ( P(x|y) * P(y) ) / P(x)\)中,\(P(y|x)\)是后验概率,\(P(x|y)\)是条件概率,\(P(y)\)是先验概率.</li>
</ol>
<h4 id="strong-pian-chai-fang-chai-zao-sheng-strong"><strong>偏差,方差,噪声</strong></h4>
<ol>
<li>偏差:度量了学习算法的期望预测和真实结果偏离程度</li>
<li>方差:度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响</li>
<li>噪声:可以认为是数据自身的波动性，表达了目前任何学习算法所能达到泛化误差的下限</li>
<li><strong>泛化误差</strong>可以分解为偏差、方差与噪声之和</li>
</ol>
<h4 id="strong-dui-ou-yuan-li-strong"><strong>对偶原理</strong></h4>
<p>一个优化问题可以从主问题和对偶问题两个方面考虑.在推导对偶问题时,通过将拉格朗日函数对\(x\)求导并使导数为0来获得对偶函数.对偶函数给出了主问题最优解的下界,因此对偶问题一般是凸问题,那么只需求解对偶函数的最优解就可以了.</p>
<h4 id="strong-kkt-tiao-jian-strong"><strong>KKT条件</strong></h4>
<p>通常我们要求解的最优化条件有如下三种:</p>
<ol>
<li>无约束优化问题:通常使用求导,使导数为零,求解候选最优值</li>
<li>有等式约束的优化问题:通常使用拉格朗日乘子法,即把等式约束用拉格朗日乘子和优化问题合并为一个式子,通过对各个变量求导使其为零,求解候选最优值.拉格朗日乘数法其实是KKT条件在等式约束优化问题的简化版.</li>
<li>有不等式约束的优化问题:通常使用KKT条件.即把不等式约束,等式约束和优化问题合并为一个式子.假设有多个等式约束\(h(x)\)和不等式约束\(g(x)\),\(L(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\mu})=f(\boldsymbol{x})+\sum_{i=1}^{m} \lambda_{i} h_{i}(\boldsymbol{x})+\sum_{i=1}^{n} \mu_{j} g_{j}(\boldsymbol{x})\),则不等式约束引入的KKT条件如下:<br>
\(\left\{\begin{array}{l}g_{j}(\boldsymbol{x}) \leqslant 0 \\ mu_{j} \geqslant 0  \\mu_{j} g_{j}(\boldsymbol{x})=0\end{array}\right.\) ,实质是最优解在\(g(x)&lt;0\)区域内时,约束条件不起作用,等价于对\(μ\)置零然后对原函数的偏导数置零;当\(g(x)=0\)时与情况2相近.结合两种情况,那么只需要使\(L\)对\(x\)求导为零,使\(h(x)\)为零,使\(μg(x)\)为零三式即可求解候选最优值.</li>
</ol>
<h4 id="strong-jiang-wei-fang-fa-strong"><strong>降维方法</strong></h4>
<ol>
<li>主成分分析(PCA):降维,不断选择与已有坐标轴正交且方差最大的坐标轴.</li>
<li>奇异值分解(SVD):矩阵分解,降维,推荐系统.</li>
<li>线性判别分析(LDA)</li>
</ol>
<h4 id="ji-cheng-fang-fa">集成方法</h4>
<h5 id="bagging">bagging</h5>
<p>装袋法的核心思想是构建多个相互独立的评估器，然后对其预测进行平均或多数表决原则来决定集成评估器的结果。</p>
<ol>
<li>评估器：相互独立，同时运行</li>
<li>抽样：有放回抽样</li>
<li>如何决定集成的结果：平均或者少数服从多数</li>
<li>目标：降低方差</li>
<li>基学习器过拟合：能够一定程度上解决基学习器过拟合的问题</li>
<li>基学习器学习能力弱：不是很有帮助</li>
<li>代表算法：随机森林</li>
</ol>
<h5 id="boosting">boosting</h5>
<p>提升方法中，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本进行预测，从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树。</p>
<ol>
<li>评估器：相互关联，按顺序依次构建，后建的模型在先建模型预测失败的样本上有更多的权重</li>
<li>抽样：有放回的采样，但会确认数据的权重，每次抽样都会给预测失败的样本更多的权重</li>
<li>如何决定集成的结果：加权平均，在训练集上表现好的模型会有更大的权重</li>
<li>目标：降低偏差，提高模型整体的精确度</li>
<li>基学习器过拟合：加剧过拟合问题</li>
<li>基学习器学习能力弱：提升模型表现</li>
<li>代表算法：GBDT,Adaboost</li>
</ol>
<h5 id="stacking">stacking</h5>
<p>Stacking模型是指将多种分类器组合在一起来取得更好表现的一种集成学习模型。一般情况下，Stacking模型分为两层。第一层中我们训练多个不同的模型，然后再以第一层训练的各个模型的输出作为输入来训练第二层的模型，以得到一个最终的输出。</p>
<h4 id="strong-xing-neng-du-liang-strong"><strong>性能度量</strong></h4>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center">预测值</th>
<th style="text-align:center">预测值</th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">真实值</td>
<td style="text-align:center">1</td>
<td style="text-align:center">11</td>
<td style="text-align:center">10</td>
<td style="text-align:center">Recall = \(\frac{11}{11+10}\)</td>
</tr>
<tr>
<td style="text-align:center">真实值</td>
<td style="text-align:center">0</td>
<td style="text-align:center">01</td>
<td style="text-align:center">00</td>
<td style="text-align:center">FPR = \(\frac{01}{01+00}\)</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">Precision=\(\frac{11}{11+01}\)</td>
<td style="text-align:center"></td>
<td style="text-align:center">Acc = \(\frac{11+00}{11+10+01+00}\)</td>
</tr>
</tbody>
</table>
<ol>
<li><strong>准确度</strong>,最常用,但在数据集不平衡的情况下不好</li>
<li><strong>Precision(精确度/查准率)</strong>:\(P=TP/(TP+FP)\)</li>
<li><strong>Recall(召回率/查全率)</strong>:\(R=TP/(TP+FN)\)</li>
<li><strong>Fβ度量</strong>:\(F_{\beta}=\frac{\left(1+\beta^{2}\right) r p}{\beta^{2} * p+r}\),当β=1时退化为F1度量,是精确率和召回率的调和均值.</li>
<li><strong>TPR(真正例率)</strong>:\(TPR=TP/(TP+FN)\)</li>
<li><strong>FPR(假正例率)</strong>:\(FPR=FP/(TN+FP)\)</li>
<li><strong>PR曲线</strong>:纵轴为Precision,横轴为Recall,一般使用平衡点(BEP,即Precsion=Recall的点)作为衡量标准.</li>
<li><strong>ROC(接受者操作特征)曲线</strong>:（<strong>每判断正确一个少数类，就有多少个多数类会被判断错误。</strong>）纵轴为TRP,横轴为FPR,在绘图时将分类阈值依次设为每个样例的预测值,再连接各点.ROC曲线围住的面积称为AOC,AOC越大则学习器性能越好.</li>
</ol>
<h4 id="strong-sun-shi-han-shu-he-feng-xian-han-shu-strong"><strong>损失函数和风险函数</strong></h4>
<ol>
<li>损失函数度量模型一次预测的好坏.常用的损失函数有:0-1损失函数,平方损失函数,绝对损失函数,对数似然损失函数.</li>
<li>损失函数的期望是理论上模型关于联合分布P(X,Y)的平均意义下的损失,称为风险函数,也叫<strong>期望风险</strong>.但是联合分布是未知的,期望风险不能直接计算.</li>
<li>当样本容量N趋于无穷时经验风险趋于期望风险,但现实中训练样本数目有限.</li>
</ol>
<h4 id="strong-jing-yan-feng-xian-zui-xiao-hua-he-jie-gou-feng-xian-zui-xiao-hua-strong"><strong>经验风险最小化和结构风险最小化</strong></h4>
<ol>
<li>模型关于训练数据集的平均损失称为经验风险.经验风险最小化的策略就是最小化经验风险.当样本数量足够大时学习效果较好.比如当模型是条件概率分布,损失函数是对数损失函数时,经验风险最小化就等价于极大似然估计.但是当样本容量很小时会出现过拟合.</li>
<li>结构风险最小化等于正则化.结构风险在经验风险上加上表示模型复杂度的正则化项.比如当模型是条件概率分布,损失函数是对数损失函数,模型复杂度由模型的先验概率表示时,结构风险最小化就等价于最大后验概率估计.</li>
</ol>
<h4 id="strong-guo-ni-he-strong"><strong>过拟合</strong></h4>
<p>指学习时选择的模型所包含的参数过多,以致于对已知数据预测得很好,但对未知数据预测很差的现象.模型选择旨在避免过拟合并提高模型的预测能力.</p>
<h4 id="strong-zheng-ze-hua-strong"><strong>正则化</strong></h4>
<p>模型选择的典型方法.正则化项一般是模型复杂度的单调递增函数,比如模型参数向量的范数.</p>
<h4 id="strong-jiao-cha-yan-zheng-strong"><strong>交叉验证</strong></h4>
<p>是另一常用的模型选择方法,可分为简单交叉验证,K折交叉验证,留一交叉验证等.</p>
<h4 id="sklearn">sklearn</h4>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/sklearn.png" alt="avatar"></p>
<h3 id="gan-zhi-ji">感知机</h3>
<p>感知机是<strong>二类分类</strong>的线性模型,属于判别模型.感知机学习旨在求出将训练数据进行线性划分的分离超平面.是神经网络和支持向量机的基础.</p>
<h4 id="mo-xing">模型</h4>
<p>\(f(x)=sign(wx + b)\),\(w\)叫作权值向量,\(b\)叫做偏置,\(sign\)是符号函数.</p>
<h4 id="strong-gan-zhi-ji-de-ji-he-jie-shi-strong"><strong>感知机的几何解释</strong></h4>
<p>\(wx+b\)对应于特征空间中的一个分离超平面S,其中\(w\)是\(S\)的法向量,\(b\)是\(S\)的截距.\(S\)将特征空间划分为两个部分,位于两个部分的点分别被分为正负两类.</p>
<h4 id="ce-lue">策略</h4>
<p>假设训练数据集是线性可分的,感知机的损失函数是误分类点到超平面\(S\)的总距离.因为误分类点到超平面\(S\)的距离是\(\frac{1}{\|w\|}\left|w \cdot x_{0}+b\right|\),且对于误分类的数据来说,总有\(-y_i(wx_i+b)>0\)成立,因此不考虑\(\frac{1}{\|w\|}\),就得到感知机的损失函数:</p>
<p>\(L(w, b)=-\sum_{x \in M} y_{i}\left(w \cdot x_{i}+b\right)\),其中M是误分类点的集合.感知机学习的策略就是选取使损失函数最小的模型参数.</p>
<h4 id="suan-fa">算法</h4>
<p>感知机的最优化方法采用<strong>随机梯度下降法</strong>.首先任意选取一个超平面\(w_0,b_0\),然后不断地极小化目标函数.在极小化过程中一次随机选取一个误分类点更新\(w,b\),直到损失函数为0.\(w \leftarrow w+\eta y_{i} x_{i}\) ， \(b \leftarrow b+\eta y_{i}\)，其中\(η\)表示步长.该算法的直观解释是:当一个点被误分类,就调整\(w,b\)使分离超平面向该误分类点接近.感知机的解可以不同.</p>
<h4 id="strong-dui-ou-xing-shi-strong"><strong>对偶形式</strong></h4>
<p>假设原始形式中的\(w_0\)和\(b_0\)均为0,设逐步修改\(w\)和\(b\)共n次,令\(a=nη\),最后学习到的\(w,b\)可以表示为\(w =\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}\),\(b =\sum_{i=1}^{N}{\alpha_{i} y_{i}}\) ,那么对偶算法就变为设初始\(a\)和\(b\)均为0,每次选取数据更新\(a\)和\(b\)直至没有误分类点为止.对偶形式的意义在于可以将训练集中实例间的内积计算出来,存在Gram矩阵中,可以大大加快训练速度.</p>
<h3 id="k-jin-lin-fa">k近邻法</h3>
<p>k近邻法根据其<strong>k个最近邻</strong>的训练实例的类别,通过多数表决等方式进行预测.当k=1时称为最近邻算法.</p>
<h4 id="san-ge-ji-ben-yao-su">三个基本要素:</h4>
<ol>
<li>k值的选择</li>
<li>距离度量</li>
<li>分类决策规则</li>
</ol>
<h4 id="mo-xing-1">模型</h4>
<p>当训练集,距离度量,k值以及分类决策规则确定后,特征空间已经根据这些要素被划分为一些子空间,且子空间里每个点所属的类也已被确定.</p>
<h4 id="strong-ce-lue-strong"><strong>策略</strong></h4>
<ol>
<li><strong>距离</strong>:特征空间中两个实例点的距离是相似程度的反映,k近邻算法一般使用欧氏距离,也可以使用更一般的Lp距离或Minkowski距离.</li>
<li><strong>k值</strong>:k值较小时,整体模型变得复杂,容易发生过拟合.k值较大时,整体模型变得简单.在应用中k一般取较小的值,通过交叉验证法选取最优的k.</li>
<li><strong>分类决策规则</strong>:k近邻中的分类决策规则往往是多数表决,多数表决规则等价于经验风险最小化.</li>
</ol>
<h4 id="suan-fa-1">算法</h4>
<p>根据给定的距离度量,在训练集中找出与x最邻近的k个点,根据分类规则决定x的类别y.</p>
<p><strong>kd树</strong></p>
<ol>
<li>
<p>kd树就是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构.kd树更适用于<strong>训练实例数远大于空间维数</strong>时的k近邻搜索.</p>
</li>
<li>
<p><strong>构造</strong>:可以通过如下<strong>递归</strong>实现:在超矩形区域上选择一个<strong>坐标轴</strong>和此坐标轴上的一个<strong>切分点</strong>,确定一个超平面,该超平面将当前超矩形区域切分为两个子区域.在子区域上重复切分直到子区域内没有实例时终止.通常依次选择坐标轴和选定坐标轴上的<strong>中位数点</strong>为切分点,这样可以得到平衡kd树.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/kd_tree.png" alt="avatar"></p>
</li>
</ol>
<h4 id="strong-sou-suo-strong"><strong>搜索</strong></h4>
<p>从根节点出发,若目标点x当前维的坐标小于切分点的坐标则移动到左子结点,否则移动到右子结点,直到子结点为叶结点为止.以此叶结点为&quot;当前最近点&quot;,<strong>递归</strong>地向上回退,在每个结点:(a)如果该结点比当前最近点距离目标点更近,则以该结点为&quot;当前最近点&quot;(b)&quot;当前最近点&quot;一定存在于该结点一个子结点对应的区域,检查该结点的另一子结点对应的区域是否与以目标点为球心,以目标点与&quot;当前最近点&quot;间的距离为半径的超球体相交.如果相交,移动到另一个子结点,如果不相交,向上回退.持续这个过程直到回退到根结点,最后的&quot;当前最近点&quot;即为最近邻点.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/search_kd.png" alt="avatar"></p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/search_kd_2.png" alt="avatar"></p>
<h3 id="po-su-bei-xie-si">朴素贝叶斯</h3>
<p>朴素贝叶斯是基于<strong>贝叶斯定理</strong>和<strong>特征条件独立假设</strong>的分类方法.首先学习输入/输出的联合概率分布,然后基于此模型,对给定的输入x,利用贝叶斯定理求出后验概率最大的输出y.属于生成模型.</p>
<h4 id="strong-mo-xing-strong"><strong>模型</strong></h4>
<ol>
<li>首先学习先验概率分布\(p(c_k),k=1,2,3,4,...,k\)</li>
<li>然后学习条件概率分布,\(P\left(X=x | Y=c_{k}\right)=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right)\).如果估计实际,需要指数级的计算,所以朴素贝叶斯法对条件概率分布作了条件独立性的假设,上式变成\(\prod_{j=1}^{n} P\left(X^{(n)}=x^{(n)} | Y=c_{k}\right)\)</li>
<li>在分类时,通过学习到的模型计算后验概率分布,由贝叶斯定理得到\(P\left(Y=c_{k} | X=x\right)=\frac{P\left(X=x | Y=c_{k}\right) P\left(Y=c_{k}\right)}{\sum_{k} P\left(X=x | Y=c_{k}\right) P\left(Y=c_{k}\right)}\)</li>
<li>将条件独立性假设得到的等式代入,并且注意到分母都是相同的,所以得到朴素贝叶斯分类器:\(y=\arg \max _{a_{k}} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(l)}=x^{(l)} | Y=c_{k}\right)\)</li>
</ol>
<p>朴素贝叶斯将实例分到后验概率最大的类中,这等价于期望风险最小化.</p>
<h4 id="strong-suan-fa-strong"><strong>算法</strong></h4>
<ol>
<li>使用<strong>极大似然估计法</strong>估计相应的先验概率:\(P\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}{N}, \quad k=1,2, \cdots, K\)和条件概率\(P\left(X^{(j)}=a_{n} | Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(n)}=a_{j i} y_{i}=c_{k}\right)}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}\)</li>
<li>计算条件独立性假设下的实例各个取值的可能性,\(P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right), \quad k=1,2, \cdots, K\)</li>
<li>选取其中的最大值作为输出.</li>
</ol>
<h4 id="te-shu-qing-kuang">特殊情况</h4>
<ol>
<li>用极大似然估计可能会出现所要估计的概率值为0的情况,在累乘后会影响后验概率的计算结果,使分类产生偏差.可以采用<strong>贝叶斯估计</strong>,在随机变量各个取值的频数上赋予一个正数.\(P_{\lambda}\left(X^{(1)}=a_{j q} | Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(l)}=a_{\beta}, y_{i}=c_{k}\right)+\lambda}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)+S_{j} \lambda}\)，\(S_j\)为j属性可能取值数量,当\(λ=0\)时就是极大似然估计.常取\(λ=1\),称为<strong>拉普拉斯平滑</strong>.</li>
<li>如果是连续值的情况,可以假设连续变量服从高斯分布,然后用训练数据估计参数.\(P\left(X_{i}=x_{i} | Y=y_{i}\right)=\frac{1}{\sqrt{2 \pi} \sigma_{i j}^{2}} e^{\frac{\left(x_{i}-\mu_{j}\right)^{2}}{2 \sigma_{i}^{2}}}\)</li>
</ol>
<h3 id="jue-ce-shu">决策树</h3>
<p>决策树是一种基本的分类与回归方法.它可以认为是<strong>if-then规则</strong>的集合,也可以认为是定义在特征空间与类空间上的<strong>条件概率分布</strong>.主要优点是模型具有可读性,分类速度快.其主要围绕着两个问题：</p>
<ol>
<li>如何从数据表中找出最佳节点和最佳分枝？</li>
<li>如何让决策树停止生长，防止过拟合？</li>
</ol>
<h4 id="mo-xing-2">模型</h4>
<p>分类决策树由<strong>结点</strong>和<strong>有向边</strong>组成.结点分为<strong>内部结点</strong>(表示一个特征或属性)和<strong>叶结点</strong>(表示一个类).决策树的路径具有<strong>互斥且完备</strong>的性质.</p>
<h4 id="ce-lue-1">策略</h4>
<p>决策树学习本质上是从训练数据集中归纳出一组分类规则.我们需要的是一个与训练数据<strong>矛盾较小</strong>,同时具有很好的<strong>泛化能力</strong>的决策树.从所有可能的决策树中选取最优决策树是NP完全问题,所以现实中常采用<strong>启发式方法</strong>近似求解.</p>
<h4 id="suan-fa-2">算法</h4>
<p>决策树学习算法包含<strong>特征选择</strong>,<strong>决策树的生成</strong>与<strong>决策树的剪枝过程</strong>.生成只考虑局部最优,剪枝则考虑全局最优。</p>
<h4 id="te-zheng-xuan-ze">特征选择</h4>
<p>如果利用一个特征进行分类的结果与随机分类的结果没有很大差别,则称这个特征是<strong>没有分类能力</strong>的.扔掉这样的特征对决策树学习的精度影响不大.</p>
<ol>
<li><strong>信息熵</strong>：熵是衡量<strong>随机变量不确定性</strong>的度量.熵越大,随机变量的不确定性就越大.信息熵是信息量的期望，\(\left.H(X)=-\sum_{x \in X} P(x) \log P(x)\right)\)</li>
<li><strong>条件熵</strong>：条件熵表示在已知随机变量X的条件下随机变量Y的不确定性.\(H(Y | X)=\sum_{i=1}^{n} p_{i} H\left(Y | X=x_{i}\right)\)</li>
<li><strong>信息增益</strong>：表示得知特征X的信息而使得类Y的信息的<strong>不确定性减少</strong>的程度.定义为集合D的经验熵与特征A在给定条件下D的经验条件熵之差\(g(D,A)=H(D)-H(D|A)\),也就是训练数据集中类与特征的<strong>互信息</strong>.</li>
<li><strong>信息增益算法</strong>:计算数据集D的经验熵\(H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|_{\mathrm{log}_{2}}\left|C_{k}\right|}{|D|}\),计算特征A对数据集D的经验条件熵\(H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{\mu}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{k}\right|}{\left|D_{i}\right|}\),计算信息增益,选取信息增益最大的特征.</li>
<li><strong>信息增益比</strong>:信息增益值的大小是相对于训练数据集而言的,并无绝对意义.使用信息增益比,\(g_{R}(D, A)=\frac{g(D, A)}{H(D)}\)可以对这一问题进行校正.</li>
</ol>
<h4 id="strong-jue-ce-shu-de-sheng-cheng-strong"><strong>决策树的生成</strong></h4>
<ol>
<li><strong>ID3算法</strong>:核心是在决策树各个结点上应用<strong>信息增益准则</strong>选择信息增益最大且大于阈值的特征,递归地构建决策树.ID3相当于用极大似然法进行概率模型的选择.由于算法只有树的生成,所以容易产生过拟合.</li>
<li><strong>C4.5算法</strong>:C4.5算法与ID3算法相似,改用<strong>信息增益比</strong>来选择特征.</li>
</ol>
<h4 id="strong-jue-ce-shu-de-jian-zhi-strong"><strong>决策树的剪枝</strong></h4>
<ol>
<li>在学习时过多考虑如何提高对训练数据的正确分类,从而构建出过于复杂的决策树,产生<strong>过拟合</strong>现象.解决方法是对已生成的决策树进行简化,称为剪枝.</li>
<li>设树的叶结点个数为\(|T|\),每个叶结点有\(N_t\)个样本点,其中\(k\)类样本点有\(N_{tk}\)个,剪枝往往通过极小化决策树整体的损失函数\(C_{\alpha}(T)=\sum_{i=1}^{\pi} N_{i} H_{i}(T)+\alpha|T|\)来实现,其中经验熵\(H_{t}(T)=-\sum_{k} \frac{N_{a}}{N_{t}} \log \frac{N_{u}}{N_{t}}\).剪枝通过加入\(a|T|\)项来考虑模型复杂度,实际上就是用正则化的极大似然估计进行模型选择.</li>
<li><strong>剪枝算法</strong>:剪去某一子结点,如果生成的新的整体树的<strong>损失函数值</strong>小于原树,则进行剪枝,直到不能继续为止.具体可以由动态规划实现.</li>
</ol>
<h4 id="strong-cart-suan-fa-strong"><strong>CART算法</strong></h4>
<ol>
<li>
<p>CART既可以用于<strong>分类也</strong>可以用于<strong>回归</strong>.它假设决策树是<strong>二叉树</strong>,内部结点特征的取值为&quot;是&quot;和&quot;否&quot;.递归地构建二叉树,对回归树用<strong>平方误差</strong>最小化准则,对分类数用<strong>基尼指数</strong>最小化准则.</p>
</li>
<li>
<p><strong>回归树的生成</strong>:在训练数据集所在的输入空间中,递归地将每个区域划分为两个子区域.选择第j个变量和它取的值s作为切分变量和切分点,并定义两个区域\(R_{1}(j, s)=\left\{x | x^{(j)} \leqslant s\right\} \quad\)和\(\quad R_{2}(j, s)=\left\{x | x^{(l)}>s\right\}\),遍历变量j,对固定的j扫描切分点s,求解\(\min _{j, z}\left[\min _{c_1} \sum_{x \in R_{1}(j, x)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x \in R_{2}(j, x)}\left(y_{i}-c_{2}\right)^{2}\right]\).用选定的对(j,s)划分区域并决定相应的输出值\(\hat{c}_{m}=\frac{1}{N_{m}} \sum_{x_{i} \in R_{m}(j, x)} y_{i}, x \in R_{m}, \quad m=1,2\),直到满足停止条件.</p>
</li>
<li>
<p><strong>基尼指数</strong>:假设有K个类,样本属于第k类的概率为\(p_k\),则概率分布的基尼指数为:\(\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}\),表示不确定性.在特征A的条件下集合D的基尼指数定义为\(\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)\),表示分割后集合D的不确定性.基尼指数越大,样本集合的<strong>不确定性</strong>也就越大.</p>
</li>
<li>
<p><strong>分类树的生成</strong></p>
<ol>
<li>从根结点开始,设结点的训练数据集为D,对每个特征A和其可能取的每个值a,计算A=a时的基尼指数,</li>
<li>选择<strong>基尼指数最小</strong>的特征及其对应的切分点作为<strong>最优特征</strong>与<strong>最优切分点</strong>,生成两个子结点</li>
<li>递归进行以上操作,直至满足<strong>停止条件</strong>.停止条件一般是结点中的样本个数小于阈值,或样本集的基尼指数小于阈值,或没有更多特征.</li>
</ol>
</li>
<li>
<p><strong>CART剪枝</strong></p>
<p>\(T_t\)表示以t为根结点的子树,\(|T_t|\)是\(T_t\)的叶结点个数.可以证明当\(\alpha=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}\)时,\(T_t\)与\(t\)有相同的损失函数值,且\(t\)的结点少,因此\(t\)比\(T_t\)更可取,对\(T_t\)进行剪枝.<strong>自下而上</strong>地对各内部结点t计算\(g(t)=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}\),并令\(a=min(g(t))\),<strong>自上而下</strong>地访问内部节点t,如果有\(g(t)=a\),进行剪枝,并对t以<strong>多数表决法</strong>决定其类,得到子树T,如此循环地生成一串<strong>子树序列</strong>,直到新生成的T是由根结点单独构成的树为止.利用<strong>交叉验证法</strong>在子树序列中选取最优子树.</p>
<p>如果是<strong>连续值</strong>的情况,一般用<strong>二分法</strong>作为结点来划分.</p>
</li>
</ol>
<h3 id="logistic-hui-gui-he-zui-da-shang-mo-xing">logistic回归和最大熵模型</h3>
<h4 id="strong-luo-ji-si-di-fen-bu-strong"><strong>逻辑斯谛分布</strong></h4>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/logistic.png" alt="avatar"></p>
<p>分布函数\(f(x)\)以点\((μ,1/2)\)为中心对称,\(γ\)的值越小,曲线在中心附近增长得越快.</p>
<h4 id="strong-luo-ji-si-di-hui-gui-mo-xing-strong"><strong>逻辑斯谛回归模型</strong></h4>
<p>对于给定的输入x,根据\(P(Y=1 | x)=\frac{\exp (w \cdot x+b)}{1+\exp (w \cdot x+b)}\) 和 \(P(Y=0 | x)=\frac{1}{1+\exp (w \cdot x+b)}\)计算出两个条件概率值的大小,将x分到概率值较大的那一类.将偏置b加入到权值向量w中,并在x的最后添加常数项1,得到\(P(Y=1 | x)=\frac{\exp (w \cdot x)}{1+\exp (w \cdot x)}\) 和 \(P(Y=0 | x)=\frac{1}{1+\exp (w \cdot x)}\)。</p>
<h4 id="dui-shu-ji-lu">对数几率</h4>
<p>如果某事件发生的概率是p,则该事件发生的<strong>几率</strong>(此处几率指该事件发生概率与不发生概率之比)是\(\frac{p}{1-p}\),<strong>对数几率</strong>是\(log(\frac{p}{1-p})\),那么\(\log \frac{P(Y=1 | x)}{1-P(Y=1 | x)}=w \cdot x\)，也就是说在逻辑斯谛回归模型中,输出Y=1的对数几率是输入x的<strong>线性函数</strong>,线性函数值越接近正无穷,概率值就越接近1,反之则越接近0.</p>
<h4 id="strong-si-ran-gu-ji-strong"><strong>似然估计</strong></h4>
<p>给定x的情况下参数θ是真实参数的可能性.</p>
<h4 id="strong-mo-xing-can-shu-gu-ji-strong"><strong>模型参数估计</strong></h4>
<p>对于给定的二分类训练数据集,对数似然函数为</p>
<p>\[
\begin{aligned} L(w) &amp;=\sum_{i=1}^{N}\left[y_{i} \log \pi\left(x_{i}\right)+\left(1-y_{i}\right)\log\left(1-\pi\left(x_{i}\right)\right)\right]\\ &amp;=\sum_{i=1}^{N}\left[y_{i} \log \frac{\pi\left(x_{i}\right)}{1-\pi\left(x_{i}\right)}+\log \left(1-\pi\left(x_{i}\right)\right)\right]\\&amp;=\sum_{i=1}^{N}\left[y_{i}\left(w \cdot x_{i}\right)-\log \left(1+\exp \left(w \cdot x_{i}\right)\right]\right.\end{aligned}
\]<br>
也就是<strong>损失函数</strong>.其中\(P(Y=1|x)=π(x)\),对\(L(w)\)求极大值,就可以得到\(w\)的估计值.问题变成了以对数似然函数为目标函数的最优化问题.</p>
<h4 id="strong-duo-xiang-luo-ji-si-di-hui-gui-strong"><strong>多项逻辑斯谛回归</strong></h4>
<p>当问题是多分类问题时,可以作如下推广:设Y有K类可能取值,\(P(Y=k | x)=\frac{\exp \left(w_{k} \cdot x\right)}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}, \quad k=1,2, \cdots, K-1 \quad P(Y=K | x)=\frac{1}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}\),实际上就是<strong>one-vs-all</strong>的思想,将其他所有类当作一个类,问题转换为二分类问题.</p>
<p>使用最大似然法衡量模型输出的概率与真实概率的差别，假设样本一共有N个，那么这组样本发生的总概率可以表示为：<br>
\[
P(\boldsymbol{W})=\prod_{n=1}^{N}\left(\frac{e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}}{\sum_{k^{\prime}}^{C} e^{\boldsymbol{w}_{k^{\prime}}^{T} \boldsymbol{x}_{n}}}\right)
\]<br>
对函数取对数再乘以-1，推导得到：<br>
\[
\begin{aligned}
F(\boldsymbol{W})=-\ln (P(\boldsymbol{W})) &amp;=\sum_{n=1}^{N} \ln \left(\frac{\sum_{k^{\prime}}^{C} e^{\boldsymbol{w}_{k^{\prime}}^{T} \boldsymbol{x}_{n}}}{e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}}\right) \\
&amp;=\sum_{n=1}^{N} \ln \left(\frac{e^{\boldsymbol{w}_{1}^{T} \boldsymbol{x}}+e^{\boldsymbol{w}_{2}^{T} \boldsymbol{x}}+\ldots+e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}+\ldots+e^{\boldsymbol{w}_{c}^{T} \boldsymbol{x}}}{e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}}\right) \\
&amp;=\sum_{n=1}^{N} \ln \left(1+\sum_{k \neq y_{n}} e^{\boldsymbol{w}_{k} \boldsymbol{x}_{n}-\boldsymbol{w}_{y_{n}} \boldsymbol{x}_{n}}\right)
\end{aligned}
\]</p>
<h4 id="strong-zui-da-shang-yuan-li-strong"><strong>最大熵原理</strong></h4>
<p>学习概率模型时,在所有可能的概率模型中,<strong>熵最大</strong>的模型是最好的模型.直观地,最大熵原理认为模型首先要满足已有的事实,即<strong>约束条件</strong>.在没有更多信息的情况下,那些不确定的部分都是&quot;<strong>等可能的</strong>&quot;.</p>
<h4 id="strong-zui-da-shang-mo-xing-strong"><strong>最大熵模型</strong></h4>
<p>给定训练数据集,可以确定联合分布P(X,Y)的经验分布\(\tilde{P}(X=x, Y=y)=\frac{v(X=x, Y=y)}{N}\)和边缘分布P(X)的经验分布\(\tilde{P}(X=x)=\frac{v(X=x)}{N}\),其中v表示频数,N表示样本容量.用<strong>特征函数\(f(x,y)\)</strong>=1描述x与y满足某一事实,可以得到特征函数关于P(X,Y)的经验分布的期望值和关于模型P(Y|X)与P(X)的经验分布的期望值,假设两者相等,就得到了<strong>约束条件</strong>\(\sum_{x, y} \tilde{P}(x) P(y | x) f(x, y)=\sum_{x, y} \tilde{P}(x, y) f(x, y)\).定义在条件概率分布P(Y|X)上的条件熵为\(H(P)=-\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)\),则<strong>条件熵最大</strong>的模型称为最大熵模型.</p>
<h4 id="strong-zui-da-shang-mo-xing-de-xue-xi-strong"><strong>最大熵模型的学习</strong></h4>
<p>就是求解最大熵模型的过程.等价于<strong>约束最优化问题</strong><br>
\[
\begin{aligned}
&amp;\max _{P_{e c}} H(P)=-\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)\\
&amp;\text { s.t. } \quad E_{p}\left(f_{i}\right)=E_{p}\left(f_{i}\right), \quad i=1,2, \cdots, n\\
     &amp;\sum_{y} P(y | x)=1
\end{aligned}
\]<br>
,将求最大值问题改为等价的求最小值问题<br>
\[
\begin{aligned}
&amp;\min _{R \in C}-H(P)=\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)\\
&amp;\text { s.t. } \quad E_{P}\left(f_{i}\right)-E_{\beta}\left(f_{i}\right)=0, \quad i=1,2, \cdots, n\\
&amp;\sum_{y} P(y | x)=1
\end{aligned}
\]</p>
<p>引入<strong>拉格朗日乘子</strong><br>
\[
\begin{aligned}
L(P, w) &amp; \equiv-H(P)+w_{0}\left(1-\sum_{y} P(y | x)\right)+\sum_{i=1}^{n} w_{i}\left(E_{p}\left(f_{i}\right)-E_{P}\left(f_{i}\right)\right) \\
&amp;=\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)+w_{0}\left(1-\sum_{y} P(y | x)\right) \\
&amp;+\sum_{i=1}^{n} w_{i}\left(\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P(y | x) f_{i}(x, y)\right)
\end{aligned}
\]<br>
将原始问题\(\min _{p \in C} \max _{w} L(P, w)\)转换为无约束最优化的<strong>对偶问题</strong>\(\max _{w} \min _{P \in \mathbf{C}} L(P, w)\).首先求解内部的<strong>极小化问题</strong>,即求\(L(P,W)\)对\(P(y|x)\)的偏导数.<br>
\[
\begin{aligned}
\frac{\partial L(P, w)}{\partial P(y | x)} &amp;=\sum_{x, y} \tilde{P}(x)(\log P(y | x)+1)-\sum_{y} w_{0}-\sum_{x, y}\left(\tilde{P}(x) \sum_{i=1}^{n} w_{i} f_{i}(x, y)\right) \\
&amp;=\sum_{x, y} \tilde{P}(x)\left(\log P(y | x)+1-w_{0}-\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
\end{aligned}
\]<br>
,并令偏导数等于0,解得\(Z_{w}(x)=\sum_{y} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)\).可以证明对偶函数等价于对数似然函数,那么对偶函数极大化等价于最大熵模型的<strong>极大似然估计</strong>\(L(w)=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)\).之后可以用最优化算法求解得到w.</p>
<h4 id="luo-ji-hui-gui-yu-zui-da-shang-mo-xing-de-gong-tong-dian">逻辑回归与最大熵模型的共同点</h4>
<p>最大熵模型与逻辑斯谛回归模型有类似的形式,它们又称为<strong>对数线性模型</strong>.模型学习就是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计.</p>
<h4 id="you-hua-suan-fa">优化算法</h4>
<p>似然函数是<strong>光滑的凸函数</strong>,因此多种最优化方法都适用.</p>
<ol>
<li><strong>改进的迭代尺度法(IIS)</strong>:假设当前的参数向量是w,如果能找到一种方法<strong>w-&gt;w+δ</strong>使对数似然函数值变大,就可以<strong>重复</strong>使用这一方法,直到找到最大值.</li>
<li>逻辑斯谛回归常应用梯度下降法,牛顿法或拟牛顿法.</li>
</ol>
<h3 id="zhi-chi-xiang-liang-ji">支持向量机</h3>
<h4 id="mo-xing-3">模型</h4>
<p>支持向量机(SVM)是一种<strong>二类分类模型</strong>.它的基本模型是定义在特征空间上的<strong>间隔最大</strong>的线性分类器.支持向量机还包括<strong>核技巧</strong>,使它成为实质上的非线性分类器.<strong>分离超平面</strong>\(w^\star x+b^\star =0\),<strong>分类决策函数</strong>\(f(s)=sign(w^\star x + b^\star)\).</p>
<h4 id="ce-lue-2">策略</h4>
<p><strong>间隔最大化</strong>,可形式化为一个求解<strong>凸二次规划</strong>的问题,也等价于正则化的<strong>合页损失函数</strong>的最小化问题.</p>
<h4 id="shu-ju-ke-fen-jin-si-ke-fen-bu-ke-fen">数据可分、近似可分、不可分</h4>
<p>当训练数据<strong>线性可分</strong>时,通过硬间隔最大化,学习出<strong>线性可分支持向量机</strong>.当训练数据<strong>近似线性可分</strong>时,通过软间隔最大化,学习出<strong>线性支持向量机</strong>.当训练数据<strong>线性不可分</strong>时,通过使用核技巧及软间隔最大化,学习<strong>非线性支持向量机</strong>.</p>
<h4 id="he-ji-qiao">核技巧</h4>
<p>当输入空间为欧式空间或离散集合,特征空间为希尔伯特空间时,核函数表示将输入从输入空间<strong>映射</strong>到特征空间得到的特征向量之间的<strong>内积</strong>.通过核函数学习非线性支持向量机等价于在高维的特征空间中学习线性支持向量机.这样的方法称为核技巧.</p>
<h4 id="shu-ru-kong-jian-he-te-zheng-kong-jian">输入空间和特征空间</h4>
<p>考虑一个二类分类问题,假设输入空间与特征空间为两个不同的空间,输入空间为<strong>欧氏空间或离散集合</strong>,特征空间为<strong>欧氏空间或希尔伯特空间</strong>.支持向量机都将输入映射为特征向量,所以支持向量机的学习是在<strong>特征空间</strong>进行的.</p>
<h4 id="you-hua">优化</h4>
<p>支持向量机的最优化问题一般通过对偶问题化为<strong>凸二次规划问题</strong>求解,具体步骤是将等式约束条件代入优化目标,通过求偏导求得优化目标在不等式约束条件下的极值.</p>
<h4 id="strong-xian-xing-ke-fen-zhi-chi-xiang-liang-ji-strong"><strong>线性可分支持向量机</strong></h4>
<p>当训练数据集线性可分时,存在无穷个分离超平面可将两类数据正确分开.利用<strong>间隔最大化</strong>得到<strong>唯一</strong>最优分离超平面\(w^\star x +b = 0\)和相应的分类决策函数\(f(s)=sign(w^\star x + b^\star)\).称为线性可分支持向量机.</p>
<h5 id="han-shu-jian-ge">函数间隔</h5>
<p>一般来说,一个点距离分离超平面的<strong>远近</strong>可以表示分类预测的<strong>确信程度</strong>.在超平面\(w^\star x +b = 0\)确定的情况下,\(|wx+b|\)能够相对地表示点x距离超平面的远近,而\(wx+b\)与\(y\)的符号是否一致能够表示分类是否正确.所以可用\(\hat{\gamma}_{i}=y_{i}\left(w \cdot x_{i}+b\right)\)来表示分类的正确性及确信度,这就是<strong>函数间隔</strong>.注意到即使超平面不变,函数间隔仍会受w和b的绝对大小影响.</p>
<h5 id="strong-ji-he-jian-ge-strong"><strong>几何间隔</strong></h5>
<p>一般地,当样本点被超平面正确分类时,点x与超平面的距离是\(\gamma_{i}=y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right)\)其中\(||w||\)是\(w\)的\(l2\)范数.这就是<strong>几何间隔</strong>的定义.定义超平面关于训练数据集T的几何间隔为超平面关于T中所有样本点的几何间隔之<strong>最小值</strong>\(\gamma=\min _{i,…,N} \gamma_{1}\).可知\(\gamma=\frac{\hat{\gamma}}{\|\boldsymbol{w}\|}\)当\(||w||=1\)时几何间隔和函数间隔<strong>相等</strong>.</p>
<h5 id="strong-ying-jian-ge-zui-da-hua-strong"><strong>硬间隔最大化</strong></h5>
<p>对线性可分的训练集而言,这里的间隔最大化又称为<strong>硬间隔最大化</strong>.直观解释是对训练集找到几何间隔最大的超平面意味着以<strong>充分大的确信度</strong>对训练数据进行分类.求最大间隔分离超平面即约束最优化问题:<br>
\[
\begin{aligned}
&amp;\max _{w, b} \quad \gamma\\
&amp;\text { s.t. } \quad y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right) \geqslant \gamma, \quad i=1,2, \cdots, N
\end{aligned}
\]<br>
,将几何间隔用函数间隔表示:<br>
\[
\begin{aligned}
&amp;\max _{w, b} \frac{\hat{\gamma}}{\|w\|}\\
&amp;\text { s.t. } \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant \hat{p}, \quad i=1,2, \cdots, N
\end{aligned}
\]<br>
,并且注意到函数间隔的取值并不影响最优化问题的解,不妨令函数间隔=1,并让最大化\(\frac{1}{||w||}\)等价为最小化\(\frac{||w||^2}{2}\),问题变为<strong>凸二次规划问题</strong><br>
\[
\min _{w, b} \frac{1}{2}\|w\|^{2} \\
s.t. \quad y_{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N
\]</p>
<h5 id="strong-zhi-chi-xiang-liang-he-jian-ge-bian-jie-strong"><strong>支持向量和间隔边界</strong></h5>
<p>与分离超平面距离<strong>最近的样本点</strong>的实例称为<strong>支持向量</strong>.支持向量是使最优化问题中的约束条件等号成立的点.因此对\(y=+1\)的正例点和\(y=-1\)的负例点,支持向量分别在超平面H1:\(wx+b=+1\)和H2:\(wx+b=-1\).H1和H2平行,两者之间形成一条长带,长带的宽度!称\(\frac{2}{||w||}\)为<strong>间隔</strong>,H1和H2称为<strong>间隔边界</strong>.在决定分离超平面时只有支持向量起作用,所以支持向量机是由很少的&quot;重要的&quot;训练样本确定的.由对偶问题同样可以得到支持向量一定在间隔边界上.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/support_vector.png" alt="avatar"></p>
<h5 id="strong-dui-ou-suan-fa-strong"><strong>对偶算法</strong></h5>
<p>引进拉格朗日乘子,定义拉格朗日函数\(L(w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}\),根据拉格朗日对偶性,原始问题的对偶问题是极大极小问题:\(\max _{\alpha} \min _{w, b} L(w, b, \alpha)\).先求对w,b的<strong>极小值</strong>.将\(L(w,b,a)\)分别对w,b求偏导数并令其等于0,得<br>
\[
\begin{array}{l}
w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} \\
\sum_{i=1}^{N} \alpha_{i} y_{i}=0
\end{array}
\]<br>
,代入拉格朗日函数得<br>
\[
\begin{aligned}
L(w, b, \alpha) &amp;=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j}\right) \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i} \\
&amp;=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}
\end{aligned}
\]<br>
这就是极小值.接下来对极小值求对a的极大,即是<strong>对偶问题</strong><br>
\[
\begin{array}{l}
\max _{a}-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
\quad \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
\]<br>
.将求极大转换为求极小<br>
\[
\begin{array}{cl}
\min _{\alpha} &amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&amp; \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
\]<br>
.由<strong>KKT条件</strong>成立得到<br>
\[
\begin{aligned}
&amp;w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}\\
&amp;b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i} \cdot x_{j}\right)
\end{aligned}
\]<br>
,其中\(j\)为使\(a_j^*>0\)的下标之一.所以问题就变为求对偶问题的解\(a^*\),再求得原始问题的解\(w^*,b^*\),从而得分离超平面及分类决策函数可以看出\(w^*\)和\(b^*\)都只依赖训练数据中\(a_i^*>0\)的样本点Z\((x_i,y_i)\),这些实例点\(x_i\)被称为<strong>支持向量</strong>.</p>
<h4 id="strong-xian-xing-zhi-chi-xiang-liang-ji-strong"><strong>线性支持向量机</strong></h4>
<p>如果训练数据是<strong>线性不可分</strong>的(近似线性可分),那么上述方法中的不等式约束并不能都成立,需要修改硬间隔最大化,使其成为<strong>软间隔最大化</strong>.</p>
<h5 id="song-chi-bian-liang">松弛变量</h5>
<p>线性不可分意味着某些<strong>特异点</strong>不能满足函数间隔大于等于1的约束条件,可以对每个样本点引进一个<strong>松弛变量</strong>,使函数间隔加上松弛变量大于等于1,约束条件变为\(y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}\),同时对每个松弛变量,支付一个代价,目标函数变为\(\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}\),其中\(C>0\)称为<strong>惩罚参数</strong>,C值越大对误分类的惩罚也越大.新目标函数包含了两层含义:使<strong>间隔尽量大</strong>,同时使误分类点的<strong>个数尽量小</strong>.</p>
<h5 id="ruan-jian-ge-zui-da-hua">软间隔最大化</h5>
<p>学习问题变成如下<strong>凸二次规划</strong>问题:</p>
<p>\[
\min _{w, b, k} \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i} \\
s.t. \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N \\
\xi_{i} \geqslant 0, \quad i=1,2, \cdots, N
\]</p>
<p>,可以证明w的解是唯一的,但b的解存在一个<strong>区间</strong>.线性支持向量机包含线性可分支持向量机,因此<strong>适用性更广</strong>.</p>
<h5 id="dui-ou-suan-fa">对偶算法</h5>
<p>原始问题的对偶问题是,构造<strong>拉格朗日函数</strong> \(L(w, b, \xi, \alpha, \mu) \equiv \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}-\sum_{i=1}^{N} \alpha_{i}\left(y_{i}\left(w \cdot x_{i}+b\right)-1+\xi_{i}\right)-\sum_{i=1}^{N} \mu_{i} \xi_{i}\) ，先求对w,b,ξ的<strong>极小值</strong>,分别求偏导并令导数为0,得<br>
\[
\begin{aligned}
&amp;w=\sum_{i=1} \alpha_{i} y_{i} x_{i}\\
&amp;\sum_{i=1}^{N} \alpha_{i} y_{i}=0\\
&amp;C-\alpha_{i}-\mu_{i}=0
\end{aligned}
\]<br>
,代入原函数,再对极小值求a的<strong>极大值</strong>,得到<br>
\[
\begin{aligned}
&amp;\max _{a}-\frac{1}{2} \sum_{i=1}^{N} \sum_{i=1}^{N} \alpha_{i} \alpha, y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}\\
&amp;\text { s.t. } \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0\\
&amp;\begin{array}{l}
C-\alpha_{i}-\mu_{i}=0 \\
\alpha_{i} \geqslant 0 \\
\mu_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
\end{aligned}
\]<br>
,利用后三条约束<strong>消去μ</strong>,再将求极大转换为<strong>求极小</strong>,得到<strong>对偶问题</strong><br>
\[
\begin{aligned}
&amp;\min _{\alpha} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}\\
&amp;\text { s.t. } \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0\\
&amp;0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{aligned}
\]<br>
由<strong>KKT条件</strong>成立可以得到<br>
\[
\begin{array}{c}
w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \\
b^{*}=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{*}\left(x_{i} \cdot x_{j}\right)
\end{array}
\]<br>
\(j\)是满足\(0&lt;\alpha_j^*&lt;C\)的下标之一.问题就变为选择惩罚参数\(C>0\),求得对偶问题(<strong>凸二次规划问题</strong>)的<strong>最优解\(\alpha^*\)</strong>,代入计算\(w^*\)和\(b^*\),求得分离超平面和分类决策函数.因为\(b\)的解并不唯一,所以实际计算\(b^*\)时可以取所有样本点上的<strong>平均值</strong>.</p>
<h5 id="zhi-chi-xiang-liang">支持向量</h5>
<p>在<strong>线性不可分</strong>的情况下,将对应与\(\alpha_i^*>0\)的样本点\((x_i,y_i)\)的实例点\(x_i\)称为<strong>支持向量</strong>.软间隔的支持向量或者在间隔边界上,或者在间隔边界与分类超平面之间,或者再分离超平面误分一侧.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/support_soft.png" alt="avatar"></p>
<h5 id="he-ye-sun-shi">合页损失</h5>
<p>可以认为是0-1损失函数的上界,而线性支持向量机可以认为是优化合页损失函数构成的目标函数.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/0_1loss.png" alt="avatar"></p>
<h4 id="strong-fei-xian-xing-zhi-chi-xiang-liang-ji-strong"><strong>非线性支持向量机</strong></h4>
<p>如果分类问题是<strong>非线性</strong>的,就要使用<strong>非线性支持向量机</strong>.主要特点是使用<strong>核技巧</strong>.</p>
<h5 id="strong-fei-xian-xing-fen-lei-wen-ti-strong"><strong>非线性分类问题</strong></h5>
<p>用线性分类方法求解非线性分类问题分为两步:首先使用一个变换将原空间的数据映射到新空间,然后在新空间里用线性分类学习方法从训练数据中学习分类模型.</p>
<h5 id="strong-he-han-shu-strong"><strong>核函数</strong></h5>
<p>设X是<strong>输入空间</strong>(欧式空间的子集或离散集合),H为<strong>特征空间</strong>(希尔伯特空间),一般是<strong>高维</strong>甚至无穷维的.如果存在一个从X到H的映射\(\phi(x): \mathcal{X} \rightarrow \mathcal{H}\)使得对所有x,z属于X,函数K(x,z)满足条件\(K(x, z)=\phi(x) \cdot \phi(z)\),点乘代表<strong>内积</strong>,则称K(x,z)为<strong>核函数</strong>.</p>
<h5 id="strong-he-ji-qiao-strong"><strong>核技巧</strong></h5>
<p>基本思想是通过一个<strong>非线性变换</strong>将输入空间对应于一个<strong>特征空间</strong>,使得在输入空间中的<strong>超曲面模型</strong>对应于特征空间中的<strong>超平面模型</strong>(支持向量机).在学习和预测中只定义核函数\(K(x,z)\),而<strong>不显式</strong>地定义映射函数.对于给定的核\(K(x,z)\),特征空间和映射函数的取法并<strong>不唯一</strong>.注意到在线性支持向量机的对偶问题中,目标函数和决策函数都只涉及输入实例与实例之间的<strong>内积</strong>,\(x_i,x_j\)可以用核函数\(K(x_i,x_j)=\phi (x_i)\phi (x_j)\)来<strong>代替</strong>.当映射函数是非线性函数时,学习到的含有核函数的支持向量机是非线性分类模型.在实际应用中,往往依赖领域知识<strong>直接选择</strong>核函数.</p>
<h5 id="strong-zheng-ding-he-strong"><strong>正定核</strong></h5>
<p>通常所说的核函数是指<strong>正定核函数</strong>.只要满足正定核的充要条件,那么给定的函数K(x,z)就是正定核函数.设K是定义在X*X上的<strong>对称函数</strong>,如果任意xi属于X,K(x,z)对应的<strong>Gram矩阵</strong>\(K=\left[K\left(x_{i}, x_{j}\right)\right]_{mxm}\)是<strong>半正定矩阵</strong>,则称\(K(x,z)\)是正定核.这一定义在构造核函数时很有用,但要验证一个具体函数是否为正定核函数并不容易,所以在实际问题中往往应用已有的核函数.</p>
<h5 id="strong-suan-fa-strong-1"><strong>算法</strong></h5>
<p>选取适当的核函数K(x,z)和适当的参数C,将线性支持向量机对偶形式中的内积换成核函数,构造并求解最优化问题</p>
<p>\[
\min _{a} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
s.t. \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\]<br>
,选择最优解\(a^*\)的一个正分量\(0&lt;a_j^*&lt;C\)计算\(b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K\left(x_{i} \cdot x_{j}\right)\),构造决策函数\(f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K\left(x \cdot x_{i}\right)+b^{*}\right)\)</p>
<h5 id="strong-chang-yong-he-han-shu-strong"><strong>常用核函数</strong></h5>
<ol>
<li><strong>多项式核函数(polynomial kernel function)</strong> :\(K(x, z)=(x \cdot z+1)^{p}\),对应的支持向量机是一个p次多项式分类器,分类决策函数为:\(f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{i}} a_{i}^{*} y_{i}\left(x_{i} \cdot x+1\right)^{p}+b^{*}\right)\)</li>
<li><strong>高斯核函数(Gaussian krenel function)</strong> :\(K(x, z)=\exp \left(-\frac{\|x-z\|^{2}}{2 \sigma^{2}}\right)\) ,对应的支持向量机是高斯径向基函数(RBF)分类器.分类决策函数为\(f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{t}} a_{i}^{*} y_{i} \exp \left(-\frac{\|x-z\|^{2}}{2 \sigma^{2}}\right)+b^{*}\right)\)</li>
<li><strong>字符串核函数(string kernel function)</strong>: 核函数不仅可以定义在欧氏空间上,还可以定义在<strong>离散数据的集合</strong>上.字符串核函数给出了字符串中长度等于n的所有子串组成的特征向量的余弦相似度.</li>
</ol>
<h5 id="strong-xu-lie-zui-xiao-zui-you-hua-smo-suan-fa-strong"><strong>序列最小最优化(SMO)算法</strong></h5>
<p>SMO是一种<em>快速求解凸二次规划问题</em><br>
\[
\begin{aligned}
&amp;\min _{\alpha} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}\\
&amp;\text { s.t. } \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0\\
&amp;0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{aligned}
\]<br>
的算法.基本思路是:如果所有变量都满足此优化问题的KKT条件,那么解就得到了.否则,选择<strong>两个变量</strong>,固定其他变量,针对这两个变量构建一个二次规划问题.不断地将原问题分解为<strong>子问题</strong>并对子问题求解,就可以求解原问题.注意子问题两个变量中只有一个是<strong>自由变量</strong>,另一个由<strong>等式约束</strong>确定.</p>
<h5 id="strong-liang-ge-bian-liang-er-ci-gui-hua-de-qiu-jie-fang-fa-strong"><strong>两个变量二次规划的求解方法</strong></h5>
<p>假设选择的两个变量是a1,a2,其他变量是固定的,于是得到子问题<br>
\[
\begin{aligned}
&amp;\begin{aligned}
\min _{\alpha, \alpha_{i}} W\left(\alpha_{1}, \alpha_{2}\right)=&amp; \frac{1}{2} K_{11} \alpha_{1}^{2}+\frac{1}{2} K_{22} \alpha_{2}^{2}+y_{1} y_{2} K_{12} \alpha_{1} \alpha_{2} \\
&amp;-\left(\alpha_{1}+\alpha_{2}\right)+y_{1} \alpha_{1} \sum_{i=1}^{N} y_{i} \alpha_{i} K_{n}+y_{2} \alpha_{2} \sum_{i=1}^{N} y_{i} \alpha_{i} K_{i}
\end{aligned}\\
&amp;\text { s.t. } \quad \alpha_{1} y_{1}+\alpha_{2} y_{2}=-\sum_{i=3}^{N} y_{i} \alpha_{i}=\zeta\\
&amp;0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2
\end{aligned}
\]<br>
,\(\epsilon\)是常数,目标函数式省略了不含\(a_1,a_2\)的常数项.考虑不等式约束和等式约束,要求的是目标函数在一条平行于对角线的线段上的最优值</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/1.png" alt="avatar"></p>
<p>问题变为<strong>单变量</strong>的最优化问题.假设初始可行解为aold,最优解为anew,考虑沿着约束方向未经剪辑的最优解anew,unc(即未考虑不等式约束).对该问题求偏导数,并令导数为0,代入原式,令\(E_{i}=g\left(x_{i}\right)-y_{i}=\left(\sum_{j=1}^{N} \alpha, y_{j} K\left(x_{j}, x_{i}\right)+b\right)-y_{i}, \quad i=1,2\),得到\(\alpha_{2}^{\text {new, } \text { unc }}=\alpha_{2}^{\text {old }}+\frac{y_{2}\left(E_{1}-E_{2}\right)}{\eta}\),经剪辑后a2的解是<br>
\[
\alpha_{2}^{\text {new }}=\left\{\begin{array}{ll}H, &amp; \alpha_{2}^{\text {new }, \text { unc }}>H \\ \alpha_{2}^{\text {new }, \text { unc }}, &amp; L \leqslant \alpha_{2}^{\text {new}, \text { unc}} \leqslant H \\ L, &amp; \alpha_{2}^{\text {new }, \text { unc }}&lt;L\end{array}\right.
\]<br>
L与H是\(a_2^{new}\)所在的对角线段端点的界.并解得\(\alpha_{1}^{\mathrm{new}}=\alpha_{1}^{\mathrm{old}}+y_{1} y_{2}\left(\alpha_{2}^{\mathrm{old}}-\alpha_{2}^{\mathrm{new}}\right)\)</p>
<h5 id="strong-bian-liang-de-xuan-ze-fang-fa-strong"><strong>变量的选择方法</strong></h5>
<p>在每个子问题中选择两个变量优化,其中至少一个变量是违反KKT条件的.第一个变量的选取标准是<strong>违反KKT条件最严重</strong>的样本点,第二个变量的选取标准是希望能使该变量有<strong>足够大的变化</strong>,一般可以选取使对应的\(|E1-E2|\)最大的点.在每次选取完点后,<strong>更新</strong>阈值\(b\)和差值\(Ei\).</p>
<h3 id="ti-sheng-fang-fa">提升方法</h3>
<h4 id="ti-sheng-fang-fa-1">提升方法</h4>
<p>boosting是<strong>一种常用的统计学习方法,是集成学习的一种.它通过改变训练样本的权重(概率分布),学习</strong>多个<strong>弱分类器(基本分类器),并将这些分类器</strong>线性组合**来构成一个强分类器提高分类的性能.</p>
<h4 id="jia-fa-mo-xing-he-qian-xiang-fen-bu-suan-fa">加法模型和前向分步算法</h4>
<p>加法模型\(f(x)= \sum_{m=1}^M{\beta_mb(x;\gamma_m)}\),其中，\(b(x;\gamma_m)\)为基函数，\(\gamma_m\)为基函数参数，\(\beta_m\)为基函数的系数。在给定训练数据及损失函数\(L(y,f(x))\)的条件下，学习加法模型\(f(x)\)成为经验风险极小化即损失函数极小化问题：<br>
\[
min_{\beta_m,\gamma_m}\sum_{m=1}^{M}{\beta_m b(x_i;\gamma_m)}
\]<br>
通常这是一个复杂的优化问题。</p>
<p>前向分步算法求解这一优化问题的想法是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数：\(\left(\beta_{m}, \gamma_{m}\right)=\arg \min _{\beta, y} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\beta b\left(x_{i} ; \gamma\right)\right)\)，得到参数βm和γm,更新\(f_{m}(x)=f_{m-1}(x)+\beta_{m} b\left(x ; \gamma_{m}\right)\),逐步逼近优化目标函数式，那么就可以简化优化的复杂度，最终得到加法模型。</p>
<h4 id="strong-ada-boost-strong"><strong>AdaBoost</strong></h4>
<h5 id="si-xiang">思想：</h5>
<p>AdaBoost提高那些被前一轮弱分类器错误分类样本的权值,而降低那些被正确分类样本的权值.然后采取<strong>加权多数表决</strong>的方法组合弱分类器.</p>
<h5 id="strong-suan-fa-strong-2"><strong>算法</strong></h5>
<p>首先假设训练数据集具有均匀的权值分布D1,使用具有<strong>权值分布</strong>Dm的训练数据集学习得到<strong>基本分类器</strong>Gm(x),计算<strong>分类误差率</strong>\(e_{m}=P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)\)和Gm(x)的<strong>系数</strong>\(\alpha_{m}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}\),更新训练数据集的权值分布\(D_{m+1}=\left(w_{m+1,1}, \cdots, w_{m+1, i}, \cdots, w_{m+1, N}\right)\)，其中<br>
\[
w_{m+1, i}=\left\{\begin{array}{ll}
\frac{w_{mi}}{Z_{m}} \mathrm{e}^{-\alpha_{m}}, &amp; G_{m}\left(x_{i}\right)=y_{i} \\
\frac{w_{m i}}{Z_{m}} \mathrm{e}^{\alpha_{m}}, &amp; G_{m}\left(x_{i}\right) \neq y_{i}
\end{array}\right.
\]<br>
\(Z_m\)是使Dm+1成为概率分布的<strong>规范化因子</strong>\(Z_{m}=\sum_{i=1}^{N} w_{m i} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right)\).重复上述操作M次后得到M个弱分类器,构建线性组合得到<strong>最终分类器</strong>\(G(x)=\operatorname{sign}(f(x))=\operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)\)</p>
<h4 id="strong-ti-sheng-shu-strong"><strong>提升树</strong></h4>
<p>提升树是模型为加法模型,算法为前向分布算法,基函数为<strong>决策树</strong>的提升方法.第m步的模型是\(f_{m}(x)=f_{m-1}(x)+T\left(x ; \Theta_{m}\right)\),通过经验风险极小化确定下一棵决策树的参数\(\hat{\Theta}_{m}=\arg \min _{\theta_{m}} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \Theta_{m}\right)\right)\).不同问题的提升树学习算法主要区别在于使用的<strong>损失函数</strong>不同.</p>
<h5 id="strong-er-lei-fen-lei-wen-ti-strong"><strong>二类分类问题</strong></h5>
<p>只需将AdaBoost算法中的基本分类器限制为二类分类数即可.</p>
<h5 id="strong-hui-gui-wen-ti-strong"><strong>回归问题</strong></h5>
<p>如果将输入空间划分为J个互不相交的区域,并且在每个区域上确定输出的常量\(C_j\),那么树可表示为\(T(x ; \Theta)=\sum_{j=1}^{J} c_{j} I\left(x \in R_{j}\right)\),其中\(\Theta=\left\{\left(R_{1}, c_{1}\right),\left(R_{2}, c_{2}\right), \cdots,\left(R_{J}, c_{J}\right)\right\}\)提升树采用<strong>前向分步算法</strong>:<br>
\[
\begin{aligned}
&amp;f_{0}(x)=0\\
&amp;\begin{array}{l}
f_{m}(x)=f_{m-1}(x)+T\left(x ; \Theta_{m}\right), m=1,2, \cdots, M \\
f_{M}(x)=\sum_{m=1}^{M} T\left(x ; \Theta_{m}\right)
\end{array}
\end{aligned}
\]<br>
.当采用平方误差损失函数时,损失变为<br>
\[
\begin{array}{l}
L\left(y, f_{m-1}(x)+T\left(x ; \Theta_{m}\right)\right) \\
\quad=\left[y-f_{m-1}(x)-T\left(x ; \Theta_{m}\right)\right]^{2} \\
\quad=\left[r-T\left(x ; \Theta_{m}\right)\right]^{2}
\end{array}
\]<br>
,其中r是当前模型拟合数据的<strong>残差</strong>.每一步都只需<strong>拟合残差</strong>学习一个回归树即可.</p>
<h5 id="cun-zai-de-que-dian">存在的缺点</h5>
<p>当损失函数式平方误差损失函数或者交叉熵损失函数的时候，残差即为对应的梯度，这个时候损失函数沿着梯度的方向下降最快。当损失函数式其他损失函数的时候，残差和损失函数的导数并不相等，函数收敛的速度就会没有沿着梯度的方向收敛的快。</p>
<h4 id="ti-du-ti-sheng-shu-gbdt">梯度提升树(GBDT)</h4>
<p>利用最速下降法的近似方法来实现每一步的优化,关键在于用损失函数的<strong>负梯度</strong>在当前模型的值\(-\left[\frac{\partial L\left(y, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right]_{f(x)=f_{-1}(x)}\)作为回归问题中提升树算法中的残差的<strong>近似值</strong>,每一步以此来估计回归树叶结点区域以拟合残差的近似值,并利用线性搜索估计叶结点区域的值使损失函数最小化,然后更新回归树即可.</p>
<h4 id="xgboost">Xgboost</h4>
<p>相比传统GBDT有以下优点:</p>
<ol>
<li>在优化时用到了二阶导数信息.</li>
<li>在代价函数里加入了正则项.</li>
<li>每次迭代后都将叶子结点的权重乘上一个系数,削弱每棵树的影响.</li>
<li>列抽样.</li>
<li>在训练前对数据进行排序,保存为block结构,并行地对各个特征进行增益计算.</li>
</ol>
<h3 id="em-suan-fa">EM算法</h3>
<p>EM算法是一种<strong>迭代</strong>算法,用于含有<strong>隐变量</strong>的概率模型参数的极大似然估计.每次迭代由两步组成:E步,求<strong>期望</strong>(expectation),M步,求<strong>极大值</strong>(maximization),直至收敛为止.</p>
<h4 id="strong-yin-bian-liang-strong"><strong>隐变量</strong></h4>
<p>不能被直接观察到，但是对系统的状态和能观察到的输出存在影响的一种东西.</p>
<h4 id="strong-suan-fa-strong-3"><strong>算法</strong></h4>
<ol>
<li>选择参数的初始值θ(0),开始迭代.注意EM算法对初值是<strong>敏感</strong>的.</li>
<li><strong>E步</strong>:θ(i)为第i次迭代参数θ的估计值,在第i+1次迭代的E步,计算\(\begin{aligned} Q\left(\theta, \theta^{(i)}\right) &amp;=E_{z}\left[\log P(Y, Z | \theta) | Y, \theta^{(i)}\right] =\sum_{z} \log P(Y, Z | \theta) P\left(Z | Y, \theta^{(i)}\right) \end{aligned}\) ,\(P(Z|Y,θ(i))\)是在给定<strong>观测数据</strong>Y和当前参数估计θ(i)下<strong>隐变量数据</strong>Z的条件概率分布.</li>
<li><strong>M步</strong>:求使Q(θ,θ(i))极大化的θ,确定第i+1次迭代的参数的估计值\(\theta^{(i+1)}=\arg \max _{\theta} Q\left(\theta, \theta^{(i)}\right)\)</li>
<li>重复2和3直到<strong>收敛</strong>,一般是对较小的正数\(\varepsilon1\)和\(\varepsilon2\)满足\(\left\|\theta^{(i+1)}-\theta^{(i)}\right\|&lt;\varepsilon_{1} \quad\) 或\(\quad\left\|Q\left(\theta^{(i+1)}, \theta^{(i)}\right)-Q\left(\theta^{(i)}, \theta^{(i)}\right)\right\|&lt;\varepsilon_{2}\)则停止迭代.</li>
</ol>
<h4 id="ying-yong">应用</h4>
<p>EM算法是通过不断求解<strong>下界</strong>的极大化逼近求解对数似然函数极大化的算法.可以用于生成模型的<strong>非监督学习</strong>.生成模型由联合概率分布P(X,Y)表示.X为观测数据,Y为未观测数据.</p>
<h3 id="yin-ma-er-ke-fu-mo-xing-hmm">隐马尔科夫模型(HMM)</h3>
<p>隐马尔可夫模型是关于<strong>时序</strong>的概率模型,描述由一个隐藏的马尔可夫链随机生成不可观测的<strong>状态序列</strong>,再由各个状态生成一个观测而产生<strong>观测随机序列</strong>的过程.HMM是一种生成式模型，它的理论基础是朴素贝叶斯，本质上就类似于我们将朴素贝叶斯在单样本分类问题上的应用推广到序列样本分类问题上。</p>
<h4 id="mo-xing-biao-shi">模型表示</h4>
<p>设Q是所有可能的状态的集合\(Q=\left\{q_{1}, q_{2}, \cdots, q_{N}\right\}\),V是所有可能的观测的集合\(V=\left\{v_{1}, v_{2}, \cdots, v_{M}\right\}\),I是长度为T的状态序列\(I=\left(i_{1}, i_{2}, \cdots, i_{T}\right)\), O是对应的观测序列\(O=\left(o_{1}, o_{2}, \cdots, o_{T}\right)\),</p>
<ol>
<li>A是<strong>状态转移概率矩阵</strong>\(A=\left[a_{i j}\right]_{N \times N}\),\(a_{ij}\)表示在时刻t处于状态\(q_i\)的条件下在时刻t+1转移到状态\(q_j\)的概率.</li>
<li>.B是<strong>观测概率矩阵</strong> \(B=\left[b_{j}(k)\right]_{N \times M}\),\(b_{ij}\)是在时刻t处于状态\(q_j\)的条件下生成观测\(v_k\)的概率.</li>
<li>\(\pi\)是<strong>初始状态概率向量</strong>\(\pi=\pi(x)\),\(\pi_i\)表示时刻t=1处于状态qi的概率.</li>
</ol>
<p>隐马尔可夫模型由初始状态概率向量\(pi\),状态转移概率矩阵A以及观测概率矩阵B确定.\(\pi\)和A决定即隐藏的<strong>马尔可夫链</strong>,生成不可观测的<strong>状态序列</strong>.B决定如何从状态生成观测,与状态序列综合确定了<strong>观测序列</strong>.因此,隐马尔可夫模型可以用<strong>三元符号</strong>表示\(\lambda = (A,B,\pi)\)</p>
<h4 id="liang-ge-ji-ben-jia-she">两个基本假设</h4>
<ol>
<li><strong>齐次马尔可夫性假设</strong>:假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态.</li>
<li><strong>观测独立性假设</strong>:假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态.</li>
</ol>
<h4 id="san-ge-ji-ben-wen-ti">三个基本问题</h4>
<h5 id="strong-1-gai-lu-ji-suan-wen-ti-strong"><strong>1. 概率计算问题</strong></h5>
<p>给定模型\(\lambda = (A,B,\pi)\)和观测序列,\(O=\left(o_{1}, o_{2}, \cdots, o_{T}\right)\)计算在模型\(\lambda\)下观测序列O出现的概率\(P(O|λ)\).</p>
<ol>
<li>直接计算：列举所有可能长度为T的状态序列,求各个状态序列I与观测序列O的联合概率,但计算量太大,实际操作不可行.</li>
<li><strong>前向算法</strong>：定义到时刻t部分观测序列为\(o_1\)~\(o_t\)且状态为\(q_i\)的概率为<strong>前向概率</strong>,记作\(\alpha_{t}(i)=P\left(o_{1}, o_{2}, \cdots, o_{t}, i_{t}=q_{i} | \lambda\right)\).初始化前向概率\(\alpha_{1}(i)=\pi_{i} b_{i}\left(o_{1}\right), \quad i=1,2, \cdots, N\)，递推，对\(t=1\) ~ \(T-1\),\(\alpha_{t+1}(i)=\left[\sum_{j=1}^{N} \alpha_{t}(j) a_{j i}\right] b_{i}\left(o_{t+1}\right), \quad i=1,2, \cdots, N\),得到\(P(O | \lambda)=\sum_{i=1}^{N} \alpha_{T}(i)\)减少计算量的原因在于每一次计算直接引用前一个时刻的计算结果,避免重复计算.</li>
<li><strong>后向算法</strong>:定义在时刻t状态为\(q_i\)的条件下,从t+1到T的部分观测序列为\(o_{i+1}\)~\(o_T\)的概率为<strong>后向概率</strong>,记作\(\beta_{t}(i)=P\left(o_{t+1}, o_{t+2}, \cdots, o_{r} | i_{t}=q_{i}, \lambda\right)\).初始化后向概率\(\beta_{r}(i)=1, \quad i=1,2, \cdots, N\),递推,对\(t=T-1\)~\(1\)\(\beta_{t}(i)=\sum_{j=1}^{N} a_{i j} b_{j}\left(o_{i+1}\right) \beta_{i+1}(j), \quad i=1,2, \cdots, N\),得到\(P(O | \lambda)=\sum_{i=1}^{N} \pi_{i} b_{i}\left(o_{1}\right) \beta_{1}(i)\)</li>
</ol>
<h5 id="strong-2-xue-xi-suan-fa-strong"><strong>2. 学习算法</strong></h5>
<p>已知观测序列\(O=(o_1,o_2, \cdots,o_r)\),估计模型\(\lambda = (A,B,\pi)\),的参数,使得在该模型下观测序列概率\(p(O|\lambda)\)最大.根据训练数据是否包括观察序列对应的状态序列分别由监督学习与非监督学习实现.</p>
<ol>
<li>
<p>监督学习：估计转移概率\(\hat{a}_{i j}=\frac{A_{i j}}{\sum_{j=1}^{N} A_{i j}}, \quad i=1,2, \cdots, N ; \quad j=1,2, \cdots, N\) 和观测概率\(\hat{b}_{j}(k)=\frac{B_{j k}}{\sum_{k=1}^{M} B_{j k}}, \quad j=1,2, \cdots, N, k=1,2, \cdots, M\).初始状态概率\(\pi_i\)的估计为S个样本中初始状态为\(q_i\)的频率.</p>
</li>
<li>
<p><strong>非监督学习(Baum-Welch算法)</strong>:将观测序列数据看作观测数据O,状态序列数据看作不可观测的<strong>隐数据</strong>I.首先确定完全数据的对数似然函数\(log p(O,I|\lambda)\),求Q函数<br>
\[
\begin{aligned}
Q(\lambda, \bar{\lambda})=&amp; \sum_{t} \log \pi_{i_1} P(O, I | \bar{\lambda}) \\
&amp;+\sum_{I}\left(\sum_{i=1}^{I-1} \log a_{i_t,i_{t+1}}\right) P(O, I | \bar{\lambda}) \\
&amp;+\sum_{I}\left(\sum_{i=1}^{T} \log b_{i_t}\left(o_{t}\right)\right) P(O, I | \bar{\lambda})
\end{aligned}
\]<br>
,用拉格朗日乘子法极大化Q函数求模型参数\(\pi_{i}=\frac{P\left(O, i_{1}=i | \bar{\lambda}\right)}{P(O | \bar{\lambda})}\),\(a_{i j}=\frac{\sum_{i=1}^{T-1} P\left(O, i_{t}=i, i_{t+1}=j | \bar{\lambda}\right)}{\sum_{t=1}^{T-1} P\left(O, i_{t}=i | \bar{\lambda}\right)}\),\(b_{j}(k)=\frac{\sum_{i=1}^{T} P\left(O, i_{t}=j | \bar{\lambda}\right) I\left(o_{i}=v_{k}\right)}{\sum_{i=1}^{T} P\left(O, i_{t}=j | \bar{\lambda}\right)}\),</p>
</li>
</ol>
<h5 id="strong-3-yu-ce-wen-ti-strong"><strong>3. 预测问题</strong></h5>
<p>也称为解码问题.已知模型\(\lambda = (A,B,\pi)\)和观测序列\(O=(O_1,O_2,\cdots,O_T)\),求对给定观测序列条件概率\(P(I|O)\)最大的状态序列\(I=(i_1,i_2,\cdots,i_T)\)</p>
<ol>
<li>
<p><strong>近似算法</strong>: 在每个时刻t选择在该时刻最有可能出现的状态\(i_t^*\),从而得到一个状态序列作为预测的结果.优点是<strong>计算简单</strong>,缺点是不能保证状态序列整体是最有可能的状态序列</p>
</li>
<li>
<p><strong>维特比算法</strong>:用<strong>动态规划</strong>求概率最大路径,这一条路径对应着一个状态序列.从t=1开始,递推地计算在时刻t状态为i的各条部分路径的最大概率,直至得到时刻t=T状态为i的各条路径的最大概率.时刻t=T的最大概率即为<strong>最优路径</strong>的概率\(P^\star\),最优路径的<strong>终结点</strong>\(i_t^\star\)也同时得到,之后从终结点开始由后向前逐步求得<strong>结点</strong>,得到最优路径.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(obs, states, Pi, A, B)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param obs:观测序列</span></span><br><span class="line"><span class="string">    :param states:隐状态</span></span><br><span class="line"><span class="string">    :param Pi:初始概率（隐状态）</span></span><br><span class="line"><span class="string">    :param A:转移概率（隐状态）</span></span><br><span class="line"><span class="string">    :param B: 发射概率 （隐状态表现为显状态的概率）</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 路径概率表 V[时间][隐状态] = 概率</span></span><br><span class="line">    V = [&#123;&#125;]</span><br><span class="line">    <span class="comment"># 一个中间变量，代表当前状态是哪个隐状态</span></span><br><span class="line">    path = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化初始状态 (t == 0)</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">        V[<span class="number">0</span>][y] = Pi[y] * B[y][obs[<span class="number">0</span>]]</span><br><span class="line">        path[y] = [y]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对 t &gt; 0 跑一遍维特比算法</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, len(obs)):</span><br><span class="line">        V.append(&#123;&#125;)</span><br><span class="line">        newpath = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">            <span class="comment"># 概率 隐状态 =    前状态是y0的概率 * y0转移到y的概率 * y表现为当前状态的概率</span></span><br><span class="line">            (prob, state) = max([(V[t - <span class="number">1</span>][y0] * A[y0][y] * B[y][obs[t]], y0) <span class="keyword">for</span> y0 <span class="keyword">in</span> states])</span><br><span class="line">            <span class="comment"># 记录最大概率</span></span><br><span class="line">            V[t][y] = prob</span><br><span class="line">            <span class="comment"># 记录路径</span></span><br><span class="line">            newpath[y] = path[state] + [y]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 不需要保留旧路径</span></span><br><span class="line">        path = newpath</span><br><span class="line"></span><br><span class="line">    print_dptable(V)</span><br><span class="line">    (prob, state) = max([(V[len(obs) - <span class="number">1</span>][y], y) <span class="keyword">for</span> y <span class="keyword">in</span> states])</span><br><span class="line">    <span class="keyword">return</span> (prob, path[state])</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 打印路径概率表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_dptable</span><span class="params">(V)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"    "</span>,</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(V)): <span class="keyword">print</span> <span class="string">"%7d"</span> % i,</span><br><span class="line">    <span class="keyword">print</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> V[<span class="number">0</span>].keys():</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"%.5s: "</span> % y,</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(len(V)):</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"%.7s"</span> % (<span class="string">"%f"</span> % V[t][y]),</span><br><span class="line">        <span class="keyword">print</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="zui-da-shang-ma-er-ke-fu-mo-xing-memm">最大熵马尔科夫模型(MEMM)</h3>
<p>最大熵马尔科夫模型利用判别式模型的特点，直接对每一个时刻的状态建立一个分类器，然后将所有的分类器的概率值连乘起来\(P\left(y_{1}^{n} | x_{1}^{n}\right)=\prod_{t=1}^{n} P\left(y_{t} | y_{t-1}, x_{t}\right)\)。为了实现是对整个序列进行的分类，在每个时刻t时，它的特征不仅来自当前观测值\(x_t\)，而且还来自前一状态值\(y_{t-1}\),通过最大熵分类器建模\(P\left(y_{t}=y^{*} | y_{t-1}=y^{\prime}, x_{t}\right)=\frac{1}{Z\left(x_{t}, y^{\prime}\right)} \exp \left(\sum_{a} \lambda_{a} f_{a}\left(x_{t}, y^{\prime}, y^{*}\right)\right)\),其中，\(Z\left(x_{t}, y \prime\right)=\sum_{y} \exp \left(\sum_{a} \lambda_{a} f_{a}\left(x_{t}, y \prime, y\right)\right)\)，</p>
<h4 id="biao-zhu-pian-zhi-wen-ti">标注偏置问题</h4>
<p>使用维特比算法进行解码时，\(v_{t}(j)=\max _{i} v_{t-1}(i) * P\left(y_{j} | y_{i}, x_{t}\right) 1 \leq j \leq n, 1&lt;t&lt;T\)。最大熵模型在每一个时刻，针对不同的前一状态y′进行归一化操作，这是一种局部的归一化操作，会存在标签偏置问题。</p>
<h5 id="li-zi">例子</h5>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/label_bias.png" alt="avatar"></p>
<p>状态转换(1→2),(2→3),(4→5),(5→3)的概率值都是1，而无论观测值是什么，换言之有\(P(2|1,i)=P(2|1,o)=1\)</p>
<p>你可能会很惊讶\(P(2∣1,i)=1,P(2∣1,o)=1\)怎么可能会成立呢？你可以套用上面的公式试一试，由于状态&quot;1&quot;的只能转换为&quot;2&quot;，所以计算归一化项时， 其实只有一个枚举值，就是状态&quot;2&quot;，所以无论你分子为多少，分母都和它一样，所以概率值就是1。在这种情况下，其实观测值并没有任何作用，这就是标签偏置。</p>
<h5 id="strong-hou-guo-strong"><strong>后果：</strong></h5>
<p>它会造成什么后果呢？<br>
他会导致模型进行预测时只依赖数据统计出来的概率值，没有利用到样本的特征。<br>
假设训练集现在有3个rib和1个rob，当我们在测试阶段，遇到词rob，它会被解码成什么状态序列呢？答案是(0→1→2→3)！你可以套公式试一试，因为\(P(1∣0,r)>P(4∣0,r)\),\(P(2∣1,o)=P(5∣4,0)=1,P(3∣2,b)>P(3∣5,b)\)。</p>
<h5 id="strong-yuan-yin-strong"><strong>原因</strong></h5>
<p>那么问题出在哪里呢？因为MEMM中在每一时刻t，都在前一时刻某状态y′下做了局部的归一化操作，如何解决这种标签偏置问题呢？<br>
在CRF中并不是对每个时刻都进行一次分类，而是直接对整个序列进行分类，做一个全局的归一化操作。</p>
<h3 id="tiao-jian-sui-ji-chang-crf">条件随机场CRF</h3>
<h4 id="gai-lu-tu-mo-xing">概率图模型</h4>
<p>结点表示随机变量，边表示随机变量之间的概率依赖关系。</p>
<h5 id="ma-er-ke-fu-xing">马尔科夫性</h5>
<ol>
<li>成对马尔可夫性</li>
<li>局部马尔科夫性</li>
<li>全局马尔科夫性</li>
</ol>
<h5 id="gai-lu-wu-xiang-tu-mo-xing-de-yin-shi-fen-jie">概率无向图模型的因式分解</h5>
<h6 id="tuan-yu-zui-da-tuan">团与最大团</h6>
<p>无向图G中任何两个结点均有边连接的结点子集称为<strong>团</strong>。若C是无向图G的一个团，并且不能再加进任何一个G的结点使其成为一个更大的团，称此C为<strong>最大团</strong>。</p>
<p>给定概率无向图模型，设其无向图为G，C为G上的最大团，\(Y_C\)表示C对应的随机变量。那么概率无向图模型的联合概率分布\(P(Y)\)可写作图中所有最大团C上的函数\(\phi_C(Y_C)\)的乘积的形式。</p>
<p><strong>Hammersley-Clifford定理</strong><br>
概率无向图模型的联合概率分布\(P(Y)\)可以表示为如下形式：<br>
\[
P(Y)=\frac{1}{Z}\prod_C{\phi_C(Y_C)}\\
Z=\sum_Y\prod_C{\phi_C(Y_C)}
\]</p>
<h5 id="tiao-jian-sui-ji-chang-crf-1">条件随机场CRF</h5>
<p>条件随机场基于概率无向图模型，利用最大团理论，对随机变量的联合分布\(P(Y)\)进行建模。</p>
<p>在序列标注任务中的条件随机场，往往是指<strong>线性链条件随机场</strong></p>
<h6 id="tiao-jian-sui-ji-chang-de-can-shu-hua-xing-shi">条件随机场的参数化形式</h6>
<p>\[
P(y|x)=\frac{1}{Z(x)} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, j} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right) 
\\
Z(x)=\sum_{y} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, j} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right)
\]</p>
<h6 id="tiao-jian-sui-ji-chang-de-jian-hua-xing-shi">条件随机场的简化形式</h6>
<p>\[
\begin{aligned}
P(y | x) &amp;=\frac{1}{Z(x)} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x) \\
Z(x) &amp;=\sum_{y} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x)
\end{aligned}
\]</p>
<h6 id="tiao-jian-sui-ji-chang-de-ju-zhen-xing-shi">条件随机场的矩阵形式</h6>
<p>\[
P_{w}(y | x)=\frac{1}{Z_{w}(x)} \prod_{i=1}^{n+1} M_{i}\left(y_{i-1}, y_{i} | x\right)\\
Z_{w}(x)=\left(M_{1}(x) M_{2}(x) \cdots M_{n+1}(x)\right)_{\text {start, stop }}
\]</p>
<h3 id="k-means">K-Means</h3>
<p>K-Means是<strong>无监督</strong>的<strong>聚类</strong>算法.思想是对于给定的样本集,按照样本之间的距离大小将样本集划分为K个簇,让簇内的点尽量紧密地连在一起,而让簇间的距离尽量的大.</p>
<h4 id="chuan-tong-suan-fa">传统算法</h4>
<ol>
<li>用先验知识或交叉验证选择一个合适的<strong>k</strong>值.</li>
<li>随机选择k个样本作为初始的<strong>质心</strong>.注意初始化质心的选择对最后的聚类结果和运行时间都有很大的影响.</li>
<li>计算每个样本点和各个质心的距离,将样本点标记为<strong>距离最小</strong>的质心所对应的簇.</li>
<li>重新计算每个<strong>簇</strong>的质心,取该簇中每个点位置的平均值.</li>
<li>重复2,3,4步直到k个质心都没有发生变化为止.</li>
</ol>
<h4 id="k-means-1">K-Means++</h4>
<p>用于优化随机初始化质心的方法</p>
<ol>
<li>从输入样本点中随机选择一个点作为第一个质心.</li>
<li>计算每一个样本点到已选择的质心中<strong>最近质心</strong>的距离D(x).</li>
<li>选择一个新的样本点作为新的质心,选择原则是D(x)越大的点被选中的概率越大.</li>
<li>重复2和3直到选出k个质心.</li>
</ol>
<h4 id="strong-elkan-k-means-strong"><strong>Elkan K-Means</strong></h4>
<p>利用两边之和大于第三边以及两边之差小于第三边来减少距离的计算.不适用于特征稀疏的情况.</p>
<h4 id="strong-mini-batch-k-means-strong"><strong>Mini Batch K-Means</strong></h4>
<p>样本量很大时,只用其中的一部分来做传统的K-Means.一般多用几次该算法,从不同的随即采样中选择最优的聚类簇.</p>
<h3 id="apriori">Apriori</h3>
<p>Apriori是常用的挖掘出<strong>数据关联规则</strong>的算法,用于找出数据值中<strong>频繁</strong>出现的数据集合.一般使用支持度或者支持度与置信度的组合作为<strong>评估标准</strong>.</p>
<ol>
<li>支持度：几个关联的数据在数据集中出现的次数占总数据集的比重Support\((X, Y)=P(X Y)=\frac{\text {number}(X Y)}{\text {num}(\text {AllSamples})}\)</li>
<li><strong>置信度</strong>：一个数据出现后.另一个数据出现的概率Confidence\((X \Leftarrow Y)=P(X | Y)=P(X Y) / P(Y)\)</li>
</ol>
<p>Apriori算法的目标是找到最大的<strong>K项频繁集</strong>.假设使用支持度来作为评估标准,首先搜索出<strong>候选1项集</strong>及对应的支持度,<strong>剪枝</strong>去掉低于支持度的1项集,得到<strong>频繁1项集</strong>.然后对剩下的频繁1项集进行<strong>连接</strong>,得到候选的频繁2项集…以此类推,不断迭代,直到无法找到频繁k+1项集为止,对应的频繁k项集的集合即为输出结果.</p>
<h3 id="can-kao">参考</h3>
<ol>
<li><a href="https://blog.csdn.net/qq_20989105/article/details/81218696" target="_blank" rel="noopener">HMM隐马尔可夫模型与viterbi维特比算法</a></li>
</ol>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title>scrapy</title>
    <url>/2019/08/31/scrapy/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id></h3>
<p><img src="/2019/08/31/scrapy/scrapy_framework.png" alt="avatar"></p>
<a id="more"></a>
<p><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/" target="_blank" rel="noopener">scrapy中文文档</a></p>
<h3 id="1-bian-xie-scrapy-pa-chong-bu-zou">1. 编写scrapy爬虫步骤</h3>
<ol>
<li>
<p>新建项目：（scrapy startproject projectname）:新建爬虫项目</p>
</li>
<li>
<p>创建爬虫：scrapy genspider spidername “<a href="http://www.xxx.cn/" target="_blank" rel="noopener">http://www.xxx.cn/</a>”</p>
</li>
<li>
<p>明确目标：（<a href="http://xn--items-by5h803y.py" target="_blank" rel="noopener">编写items.py</a>）：明确想要抓取的目标</p>
</li>
<li>
<p>制作爬虫：（spiders/xxspider.py）:制作爬虫开始爬取的网页</p>
</li>
<li>
<p><a href="http://xn--pipeline-ts6mn078a.py" target="_blank" rel="noopener">编写pipeline.py</a>，处理spider返回的item数据。写Pipeline函数</p>
</li>
<li>
<p><a href="http://xn--settings-ts6mn078a.py" target="_blank" rel="noopener">编写settings.py</a>,启动管道组件ITEM_PIPELINES={}，以及其他相关设置USER_AGENT,DEFAULT_REQUEST_HEADERS</p>
</li>
<li>
<p>执行爬虫</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> cmdline</span><br><span class="line">cmdline.execute([<span class="string">'scrapy'</span>,<span class="string">'crawl'</span>,<span class="string">'cib'</span>])</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="2-chuang-jian-pa-chong">2. 创建爬虫</h3>
<ol>
<li>scrapy genspider spidername “<a href="http://www.xxx.cn/" target="_blank" rel="noopener">http://www.xxx.cn/</a>”
<ul>
<li>genspider:表示生成一个爬虫（默认是scrapy.Spider类）</li>
<li>spidername：表示爬虫名（对应爬虫代码里的name参数）</li>
<li>“<a href="http://www.xxx.cn/" target="_blank" rel="noopener">http://www.xxx.cn/</a>” ：表示允许爬虫爬取的域范围</li>
</ul>
</li>
<li><a href="http://spidername.py" target="_blank" rel="noopener">spidername.py</a>
<ul>
<li>name= ‘’:爬虫的识别名称，唯一</li>
<li>allow_domains=[] ：搜索的域名范围，爬虫的约束区域，规定爬虫只爬取这个域名下的网页，不存在的url会被忽略</li>
<li>start_urls=():爬取的url列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成</li>
<li>parse(self,response):解析的方法，每个初始URL完成下载后将被调用，调用的时候传入每一个URL传回的Response对象来作为唯一参数，主要作用如下：</li>
<li>负责解析返回的网页数据（response.body），提取结构化数据（生成item）</li>
<li>生成需要下一页的URL请求</li>
<li>start_requests(self):这个方法必须返回一个可迭代对象。该对象包含spider用于爬取（默认实现是使用start_urls的url）的第一个Request。当spider启动爬取并且为指定start_urls时，调用该方法</li>
<li>log（self,message[,level,component]）:使用scrapy.log.msg()方法记录（log）message</li>
</ul>
</li>
</ol>
<h3 id="3-zhi-xing-pa-chong">3. 执行爬虫</h3>
<ol>
<li>scrapy crawl spidername -o save_filename
<ul>
<li>crawl：表示启动一个scrapy爬虫</li>
<li>spidername：表示需要启动的爬虫名（对应爬虫代码里的name参数）</li>
<li>-0 :表示输出到文件</li>
<li>save_filename:表示保存文件的名称,，默认4种输出文件格式：json，jsonl，csv，xml</li>
</ul>
</li>
</ol>
<h3 id="4-cha-kan-dang-qian-xiang-mu-xia-de-pa-chong">4. 查看当前项目下的爬虫</h3>
<ol>
<li>scrapy list</li>
</ol>
<h3 id="5-pipeline-de-yi-xie-dian-xing-ying-yong">5. pipeline 的一些典型应用</h3>
<ol>
<li>验证爬取的数据（检查item包含某些字段，比如说name）</li>
<li>数据查重（并丢弃）</li>
<li>将爬取结果保存到文件或者数据库中</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SomethingPiple</span><span class="params">(object)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="comment"># 可选实现，做参数初始化，比如打开文件操作f.open('xxx','w',edcoding='utf-8')</span></span><br><span class="line">            <span class="comment"># doing something</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">            <span class="comment"># spider(Spider 对象) - 被关闭的spider</span></span><br><span class="line">            <span class="comment"># 可选实现，当spider被开启时，这个方法被调用。</span></span><br><span class="line">            <span class="comment"># 该方法和__init__方法功能基本相同。</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self,item,spider)</span>:</span></span><br><span class="line">            <span class="comment"># item(Item对象) - 被爬取的item</span></span><br><span class="line">            <span class="comment"># spider （Spider对象） - 爬取该item的spider</span></span><br><span class="line">            <span class="comment"># 这个方法必须实现，每个item pipeline 组件都需要调用该方法</span></span><br><span class="line">            <span class="comment"># 这个方法必须返回一个Item对象，被丢弃的item将不会被之后的pipeline组件处理</span></span><br><span class="line">            <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">            <span class="comment"># spider(Spider 对象) - 被关闭的spider</span></span><br><span class="line">            <span class="comment"># 可选实现，当spider被关闭时，这个方法被调用。</span></span><br><span class="line">            <span class="comment"># 比如关闭初始化打开的文件f.close()</span></span><br></pre></td></tr></table></figure>
<h3 id="6-qi-dong-scrapy-shell">6. 启动Scrapy Shell</h3>
<p>命令：scrapy shell “<a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a>”</p>
<h3 id="7-selector-xuan-ze-qi">7. selector 选择器</h3>
<ol>
<li>
<p>Selector有四个基本方法，最常用的是xpath：</p>
<ol>
<li>
<p>xpath（）：传入xpath表达式，返回该表达式所对应的所有节点的selector list列表</p>
<p>XPath表达式的例子及对应含义：</p>
<ul>
<li>/html/head/title:选择<html>文档中<head>标签内的<title>元素</title></head></html></li>
<li>/html/head/title/text():选择<html>文档中<head>标签内的<title>元素的文字</title></head></html></li>
<li>//td：选择所有的<td>元素</td></li>
<li>//div[@class=“mine”]:选择所有具有class=&quot;mine&quot;属性的div元素</li>
</ul>
</li>
<li>
<p>extract（）：序列化该结点为Unicode字符串，并返回list</p>
</li>
<li>
<p>css()：传入css表达式，返回该表达式所对应的所有节点的selector list列表，语法同BeautifulSoup4</p>
</li>
<li>
<p>re()：根据传入的正则表达式对数据进行提取，返回Unicode字符串list列表</p>
</li>
</ol>
</li>
<li>
<p>注意</p>
<ol>
<li>
<p>xpath 返回的是一个列表</p>
</li>
<li>
<p>xpath.extract():将xpath对象转换成Unicode字符串</p>
</li>
<li>
<p>settings设置</p>
<ul>
<li>
<p>HTTPERROR_ALLOWED_CODES = [403, 500, 404]</p>
</li>
<li>
<p>ROBOTSTXT_OBEY = False</p>
</li>
<li>
<p>下载中间件：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DOWNLOAD_DELAY = <span class="number">2</span></span><br><span class="line">        RANDOMIZE_DOWNLOAD_DELAY = <span class="literal">True</span></span><br><span class="line">        COOKIES_ENABLED = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">        DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">        <span class="string">'bank_info.middlewares.MyUserAgentMiddleware'</span>: <span class="number">300</span>,</span><br><span class="line">        <span class="string">'bank_info.middlewares.BankInfoDownloaderMiddleware'</span>: <span class="number">543</span>, <span class="comment"># 值越小优先级越高</span></span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        ITEM_PIPELINES = &#123;</span><br><span class="line">        <span class="string">'bank_info.pipelines.BankInfoPipeline'</span>: <span class="number">300</span>, <span class="comment"># 值越小优先级越高</span></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="8-scrapy-gao-ji">8. scrapy高级</h3>
<ol>
<li>
<p>翻页功能：scrapy.follow(next_page,callback=self.parse)   会自动拼接url和next_page</p>
</li>
<li>
<p>抽取response中满足xpath规则的链接：LinkExtractor(restrict_xpath=‘xxxx’), links = link.extract_links(response)</p>
</li>
<li>
<p>要防止scrapy被ban，主要有以下几个策略：</p>
<ul>
<li>
<p>动态设置user agent（ 在middleware.py中随机选取user-agent,并把它赋值给request）</p>
<ol>
<li>
<p>在settings开启UAMiddleware这个中间件：DOWNLOADER_MIDDLEWARES</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UAMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 定义一个User-Agent的List</span></span><br><span class="line">    ua_list = [</span><br><span class="line">    <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 '</span>,</span><br><span class="line">    <span class="string">'(KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)'</span>,</span><br><span class="line">    <span class="string">'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)'</span>,</span><br><span class="line">    <span class="string">'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'</span>,</span><br><span class="line">    <span class="string">'Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)'</span>,</span><br><span class="line">    ]</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span> <span class="comment"># 对request进行拦截</span></span><br><span class="line">        ua = random.choices(self.ua_list) <span class="comment"># 使用random模块，随机在ua_list中选取User-Agent</span></span><br><span class="line">        request.headers[<span class="string">'User-Agent'</span>] = ua <span class="comment"># 把选取出来的User-Agent赋给request</span></span><br><span class="line">        print(request.url) <span class="comment"># 打印出request的url</span></span><br><span class="line">        print(request.headers[<span class="string">'User-Agent'</span>]) <span class="comment"># 打印出request的headers</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span><span class="params">(self, request, response, spider)</span>:</span> <span class="comment"># 对response进行拦截</span></span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_exception</span><span class="params">(self, request, exception, spider)</span>:</span> <span class="comment"># 对process_request方法传出来的异常进行处理</span></span><br><span class="line">       <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li>
<p>禁用cookies ： COOKIES_ENABLED=False</p>
</li>
<li>
<p>设置延迟下载 ： DOWNLOAD_DELAY=2</p>
</li>
<li>
<p>使用Google cache</p>
</li>
<li>
<p>使用IP地址池（Tor project、VPN和代理IP）</p>
</li>
<li>
<p>使用Crawlera</p>
</li>
</ul>
</li>
<li>
<p>scrapy中间件的分类：</p>
<ul>
<li>scrapy的中间件理论上有三种(Schduler Middleware,Spider Middleware,Downloader Middleware),在应用上一般有以下两种：
<ol>
<li>爬虫中间件Spider Middleware：可以添加代码来处理发送给 Spiders 的response及spider产生的item和request.
<ul>
<li>当蜘蛛传递请求和items给引擎的过程中，蜘蛛中间件可以对其进行处理（过滤出 URL 长度比 URLLENGTH_LIMIT 的 request。）</li>
<li>当引擎传递响应给蜘蛛的过程中，蜘蛛中间件可以对响应进行过滤（例如过滤出所有失败(错误)的 HTTP response）</li>
</ul>
</li>
<li>下载器中间件Downloader Middleware：主要功能在请求到网页后,页面被下载时进行一些处理.（反爬策略都是部署在下载中间件的）
<ul>
<li>当引擎传递请求给下载器的过程中，下载中间件可以对请求进行处理 （例如增加http header信息，增加proxy信息等）</li>
<li>在下载器完成http请求，传递响应给引擎的过程中， 下载中间件可以对响应进行处理（例如进行gzip的解压等）</li>
<li>下载中间件三大函数：
<ol>
<li>process_request(request, spider)——主要函数
<ul>
<li>process_request() 必须返回其中之一: 返回 None 、返回一个 Response 对象、返回一个 Request 对象或raise IgnoreRequest</li>
<li>如果其返回 None： Scrapy将继续处理该request，执行其他的中间件的相应方法，直到合适的下载器处理函数(download handler)被调用， 该request被执行(其response被下载)</li>
<li>如果其返回Response 对象： Scrapy将不会调用任何其他的process_request()或 process_exception()方法，或相应的下载函数。其将返回该response，已安装的中间件的 process_response() 方法则会在每个response返回时被调用</li>
<li>如果其返回 Request对象 ： Scrapy则会停止调用 process_request方法并重新调度返回的request，也就是把request重新返回，进入调度器重新入队列</li>
<li>如果其返回raise IgnoreRequest异常 ： 则安装的下载中间件的 process_exception()方法 会被调用。如果没有任何一个方法处理该异常， 则request的errback(Request.errback)方法会被调用。如果没有代码处理抛出的异常， 则该异常被忽略且不记录(不同于其他异常那样)</li>
</ul>
</li>
<li>process_response(request, response, spider)——主要函数
<ul>
<li>process_response() 必须返回以下之一：返回一个Response 对象、 返回一个Request 对象或raise IgnoreRequest 异常</li>
<li>如果其返回一个 Response对象： (可以与传入的response相同，也可以是全新的对象)， 该response会被在链中的其他中间件的 process_response() 方法处理</li>
<li>如果其返回一个 Request对象： 则中间件链停止， 返回的request会被重新调度下载。处理类似于 process_request() 返回request所做的那样</li>
<li>如果其抛出一个IgnoreRequest异常 ：则调用request的errback(Request.errback)。</li>
<li>如果没有代码处理抛出的异常，则该异常被忽略且不记录(不同于其他异常那样)</li>
</ul>
</li>
<li>process_exception(request, exception, spider)
<ul>
<li>如果其返回 None ： Scrapy将会继续处理该异常，接着调用已安装的其他中间件的 process_exception()方法，直到所有中间件都被调用完毕，则调用默认的异常处理</li>
<li>如果其返回一个 Response 对象： 相当于异常被纠正了，则已安装的中间件链的 process_response()方法被调用。Scrapy将不会调用任何其他中间件的 process_exception()方法</li>
<li>如果其返回一个 Request 对象： 则返回的request将会被重新调用下载。这将停止中间件的 process_exception() 方法执行，就如返回一个response的那样</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li>
<p>其他内置downloader middleware</p>
<table>
<thead>
<tr>
<th>item</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td>DefaultHeadersMiddleware</td>
<td>将所有request的头设置为默认模式</td>
</tr>
<tr>
<td>DownloadTimeoutMiddleware</td>
<td>设置request的timeout</td>
</tr>
<tr>
<td>HttpAuthMiddleware</td>
<td>对来自特定spider的request授权</td>
</tr>
<tr>
<td>HttpCacheMiddleware</td>
<td>给request&amp;response设置缓存策略</td>
</tr>
<tr>
<td>HttpProxyMiddleware</td>
<td>给所有request设置http代理</td>
</tr>
<tr>
<td>RedirectMiddleware</td>
<td>处理request的重定向</td>
</tr>
<tr>
<td>MetaRefreshMiddleware</td>
<td>根据meta-refresh html tag处理重定向</td>
</tr>
<tr>
<td>RetryMiddleware</td>
<td>失败重试策略</td>
</tr>
<tr>
<td>RobotsTxtMiddleware</td>
<td>robots封禁处理</td>
</tr>
<tr>
<td>UserAgentMiddleware</td>
<td>支持user agent重写</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>把数据保存到json文件</p>
<p>下面这个例子将会把所有爬虫所爬取到的数据保存到 items.jl 文件中，.jl既是表示 JSON Lines 格式，既是每一行存储一个 item；</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWriterPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file = open(<span class="string">'items.jl'</span>, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        line = json.dumps(dict(item)) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(line)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>把数据写到MongoDB</p>
<p>MongoDB address 以及 database name 是通过 Scrapy settings 配置的；下面这个用例主要用来展示如何使用 from_crawler 的用法以及如何正确的清理掉这些相关的 resources</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    collection_name = <span class="string">'scrapy_items'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DATABASE'</span>, <span class="string">'items'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.db[self.collection_name].insert_one(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="9-pa-qu-dong-tai-ye-mian">9. 爬取动态页面</h3>
<p>在scrapy中使用splash</p>
<ol>
<li>
<p>安装docker ：</p>
</li>
<li>
<p>安装splash：docker pull scrapinghub/splash</p>
</li>
<li>
<p>开启端口:docker run -p 5023:5023 -p 8050:8050 -p 8051:8051 scrapinghub/splash</p>
</li>
<li>
<p>安装 scrapy-splash: pip install scrapy-splash</p>
</li>
<li>
<p>修改setting.py文件对scrapy-splash进行配置</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Splash服务器地址</span></span><br><span class="line"><span class="string">SPLASH_URL</span> <span class="string">=</span> <span class="string">'http://localhost:8050'</span></span><br><span class="line"><span class="comment">#开启Splash的两个下载中间件并调整HttpCompressionMiddleware的次序</span></span><br><span class="line"> <span class="string">DOWNLOADER_MIDDLEWARES</span> <span class="string">=</span> <span class="string">&#123;</span></span><br><span class="line">    <span class="attr">'scrapy_splash.SplashCookiesMiddleware':</span> <span class="number">723</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy_splash.SplashMiddleware':</span> <span class="number">725</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware':</span> <span class="number">810</span><span class="string">,</span></span><br><span class="line"> <span class="string">&#125;</span></span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line"> <span class="comment">#设置去重过滤器</span></span><br><span class="line"> <span class="string">DUPEFILTER_CLASS</span> <span class="string">=</span> <span class="string">'scrapy_splash.SplashAwareDupeFilter'</span></span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line"> <span class="comment">#用来支持cache_args（可选）</span></span><br><span class="line"> <span class="string">SPIDER_MIDDLEWARES</span> <span class="string">=</span> <span class="string">&#123;</span></span><br><span class="line">    <span class="attr">'scrapy_splash.SplashDeduplicateArgsMiddleware':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>专业技能</tag>
      </tags>
  </entry>
  <entry>
    <title>selenium</title>
    <url>/2019/08/09/selenium/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2019/08/09/selenium/1_hdbXQfve5Yfuo0qEnS8K9Q.jpg" alt></p>
<a id="more"></a>
<h1 id="an-zhuang-pip-install-selenium">安装：pip install selenium</h1>
<p>因为selenium是配合浏览器一起使用，所以需要下载浏览器的驱动(webdriver)，以chrome为例：chrome的webdriver： <a href="http://chromedriver.storage.googleapis.com/index.html" target="_blank" rel="noopener">chromedriver</a> 不同的Chrome的版本对应的chromedriver.exe 版本也不一样。如果是最新的Chrome, 下载最新的chromedriver.exe 就可以。把chromedriver的路径也加到<strong>环境变量</strong>里</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver      <span class="comment"># 引入webdriver api</span></span><br><span class="line"></span><br><span class="line">driver = webdriver.Chrome()     <span class="comment"># 使用chrome浏览器声明一个webdriver对象</span></span><br><span class="line"><span class="comment"># driver = webdriver.Chrome('/your path /webdriver')</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  driver.get(<span class="string">'http://www.baidu.com/'</span>) <span class="comment"># 表示使用chrome以get的方式请求百度的url</span></span><br><span class="line">  driver.find_element_by_id(<span class="string">"kw"</span>).send_keys(<span class="string">"selenium"</span>)   <span class="comment"># 检索到百度的输入框，输入selenium</span></span><br><span class="line">  driver.find_element_by_id(<span class="string">"su"</span>).click() <span class="comment"># 检索到百度的搜索按钮并点击</span></span><br><span class="line">  wait = WebDriverWait(driever,<span class="number">10</span>) <span class="comment"># 等待加载</span></span><br><span class="line">  wait.until(EC.presence_of_element_located((By.ID,<span class="string">'content_left'</span>)))</span><br><span class="line">  print(driver.current_url)  <span class="comment"># 输出当前页面url</span></span><br><span class="line">  print(driver.get_cookies) <span class="comment"># 输出cookies</span></span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">  driver.close()</span><br></pre></td></tr></table></figure>
<h1 id="yuan-su-xuan-qu">元素选取</h1>
<h2 id="dan-yuan-su-xuan-qu">单元素选取</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_element_by_id      <span class="comment"># 通过元素id</span></span><br><span class="line">find_element_by_name    <span class="comment"># 通过name属性</span></span><br><span class="line">find_element_by_xpath   <span class="comment"># 通过xpath</span></span><br><span class="line">find_element_by_link_text   <span class="comment"># 通过链接文本</span></span><br><span class="line">find_element_by_partial_link_text</span><br><span class="line">find_element_by_tag_name    <span class="comment"># 通过标签名</span></span><br><span class="line">find_element_by_class_name      <span class="comment"># 通过class名称定位</span></span><br><span class="line">find_element_by_css_selector    <span class="comment"># 通过css选择器定位</span></span><br></pre></td></tr></table></figure>
<h2 id="duo-yuan-su-xuan-qu">多元素选取</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_elements_by_name</span><br><span class="line">find_elements_by_xpath</span><br><span class="line">find_elements_by_link_text</span><br><span class="line">find_elements_by_partial_link_text</span><br><span class="line">find_elements_by_tag_name</span><br><span class="line">find_elements_by_class_name</span><br><span class="line">find_elements_by_css_selector</span><br></pre></td></tr></table></figure>
<h2 id="yuan-su-cao-zuo">元素操作</h2>
<ul>
<li>clear 清除元素的内容：clear(self)</li>
<li>send_keys 模拟按键输入：send_keys(self, *value)</li>
<li>click 点击元素：click(self)</li>
<li>submit 提交表单：submit(self)</li>
<li>获取元素属性：get_attribute(self, name)</li>
<li>获取元素文本：text</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver.find_element_by_id(<span class="string">"kw"</span>).clear()</span><br></pre></td></tr></table></figure>
<h1 id="ye-mian-cao-zuo-fang-fa">页面操作方法</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打开浏览器</span></span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line"><span class="comment"># 请求一个url</span></span><br><span class="line">driver.get(<span class="string">"www.baidu.com"</span>)</span><br><span class="line"><span class="comment"># 返回当前页面的title</span></span><br><span class="line">title = driver.title</span><br><span class="line"><span class="comment"># 返回当前页面的url</span></span><br><span class="line">url = driver.current_url</span><br><span class="line"><span class="comment"># 返回当前页面的源码</span></span><br><span class="line">source = driver.page_source</span><br><span class="line"><span class="comment"># 关闭当前页面</span></span><br><span class="line">driver.close()</span><br><span class="line"><span class="comment"># 注销并关闭浏览器</span></span><br><span class="line">driver.quit()</span><br><span class="line"><span class="comment"># 浏览器前进</span></span><br><span class="line">driver.forward()</span><br><span class="line"><span class="comment"># 浏览器后退</span></span><br><span class="line">driver.back()</span><br><span class="line"><span class="comment"># 刷新当前页面</span></span><br><span class="line">driver.refresh()</span><br><span class="line"><span class="comment"># 获取当前session中的全部cookie</span></span><br><span class="line">get_cookies(self)</span><br><span class="line"><span class="comment"># 获取当前会中中的指定cookie</span></span><br><span class="line">get_cookie(self, name)</span><br><span class="line"><span class="comment"># 在当前会话中添加cookie</span></span><br><span class="line">add_cookie(self, cookie_dict)</span><br><span class="line"><span class="comment"># 添加浏览器User-Agent：</span></span><br><span class="line">options.add_argument(<span class="string">'User-Agent=Mozilla/5.0 (Linux; U; Android 4.0.2; en-us; Galaxy Nexus Build/ICL53F) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30'</span>)</span><br><span class="line"><span class="comment"># 添加设置项Chrome Options：</span></span><br><span class="line">options = webdriver.ChromeOptions()</span><br><span class="line">options.add_argument(<span class="string">'xxxx'</span>)</span><br><span class="line">driver = webdriver.Chrome(chrome_options=options)</span><br></pre></td></tr></table></figure>
<h2 id="ye-mian-deng-dai">页面等待</h2>
<h3 id="yin-shi-deng-dai-jian-dan-de-she-zhi-deng-dai-shi-jian-dan-wei-wei-miao">隐式等待：简单的设置等待时间，单位为：秒</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">driver.implicitly_wait(<span class="number">10</span>) <span class="comment"># seconds</span></span><br><span class="line">driver.get(<span class="string">"http://somedomain/url_that_delays_loading"</span>)</span><br><span class="line">myDynamicElement = driver.find_element_by_id(<span class="string">"myDynamicElement"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="xian-shi-deng-dai-zhi-ding-mou-ge-tiao-jian-ran-hou-she-zhi-zui-chang-deng-dai-shi-jian-ru-guo-zai-zhe-ge-shi-jian-huan-mei-you-zhao-dao-yuan-su-bian-hui-pao-chu-yi-chang">显式等待：指定某个条件，然后设置最长等待时间。如果在这个时间还没有找到元素，便会抛出异常。</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">driver.get(<span class="string">"http://somedomain/url_that_delays_loading"</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line"> 	<span class="comment"># 这里需要特别注意的是until或until_not中的可执行方法method参数，很多人传入了WebElement对象</span></span><br><span class="line">  <span class="comment"># WebDriverWait(driver, 10).until(driver.find_element_by_id('kw'))  错误</span></span><br><span class="line">  element = WebDriverWait(driver, <span class="number">10</span>).until(</span><br><span class="line">            EC.presence_of_element_located((By.ID, <span class="string">"myDynamicElement"</span>))</span><br><span class="line">            )</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">driver.quit()</span><br></pre></td></tr></table></figure>
<h3 id="expected-conditions-17-ge-tiao-jian-expected-conditions-shi-selenium-de-yi-ge-mo-kuai-qi-zhong-bao-han-yi-xi-lie-ke-yong-yu-pan-duan-de-tiao-jian">expected_conditions(17个条件)：expected_conditions是selenium的一个模块，其中包含一系列可用于判断的条件</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">selenium.webdriver.support.expected_conditions</span><br><span class="line"></span><br><span class="line">这两个条件类验证title，验证传入的参数title是否等于或包含于driver.title</span><br><span class="line">title_is</span><br><span class="line">title_contains</span><br><span class="line"></span><br><span class="line">一个只要一个符合条件的元素加载出来就通过；另一个必须所有符合条件的元素都加载出来才行</span><br><span class="line">presence_of_element_located</span><br><span class="line">presence_of_all_elements_located</span><br><span class="line"></span><br><span class="line">这三个条件验证元素是否可见，前两个传入参数是元组类型的locator，第三个传入WebElement</span><br><span class="line">第一个和第三个其实质是一样的</span><br><span class="line">visibility_of_element_located</span><br><span class="line">invisibility_of_element_located</span><br><span class="line">visibility_of</span><br><span class="line"></span><br><span class="line">这两个人条件判断某段文本是否出现在某元素中，一个判断元素的text，一个判断元素的value</span><br><span class="line">text_to_be_present_in_element</span><br><span class="line">text_to_be_present_in_element_value</span><br><span class="line"></span><br><span class="line">这个条件判断frame是否可切入，可传入locator元组或者直接传入定位方式：id、name、index或WebElement</span><br><span class="line">frame_to_be_available_and_switch_to_it</span><br><span class="line"></span><br><span class="line">这个条件判断是否有alert出现</span><br><span class="line">alert_is_present</span><br><span class="line"></span><br><span class="line">这个条件判断元素是否可点击，传入locator</span><br><span class="line">element_to_be_clickable</span><br><span class="line"></span><br><span class="line">这四个条件判断元素是否被选中，第一个条件传入WebElement对象，第二个传入locator元组</span><br><span class="line">第三个传入WebElement对象以及状态，相等返回<span class="literal">True</span>，否则返回<span class="literal">False</span></span><br><span class="line">第四个传入locator以及状态，相等返回<span class="literal">True</span>，否则返回<span class="literal">False</span></span><br><span class="line">element_to_be_selected</span><br><span class="line">element_located_to_be_selected</span><br><span class="line">element_selection_state_to_be</span><br><span class="line">element_located_selection_state_to_be</span><br><span class="line"></span><br><span class="line">最后一个条件判断一个元素是否仍在DOM中，传入WebElement对象，可以判断页面是否刷新了</span><br><span class="line">staleness_of</span><br></pre></td></tr></table></figure>
<h2 id="shu-biao-cao-zuo">鼠标操作</h2>
<ul>
<li>context_click(elem) 右击鼠标点击元素elem，另存为等行为</li>
<li>double_click(elem) 双击鼠标点击元素elem，地图web可实现放大功能</li>
<li>drag_and_drop(source,target) 拖动鼠标，源元素按下左键移动至目标元素释放</li>
<li>move_to_element(elem) 鼠标移动到一个元素上</li>
<li>click_and_hold(elem) 按下鼠标左键在一个元素上</li>
<li>perform() 在通过调用该函数执行ActionChains中存储行为</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取通过鼠标右键另存为百度图片logo的例子</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.action_chains <span class="keyword">import</span> ActionChains</span><br><span class="line"> </span><br><span class="line">driver = webdriver.Firefox()</span><br><span class="line">driver.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#鼠标移动至图片上 右键保存图片</span></span><br><span class="line">elem_pic = driver.find_element_by_xpath(<span class="string">"//div[@id='lg']/img"</span>)</span><br><span class="line"><span class="keyword">print</span> elem_pic.get_attribute(<span class="string">"src"</span>)</span><br><span class="line">action = ActionChains(driver).move_to_element(elem_pic)</span><br><span class="line">action.context_click(elem_pic)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#重点:当右键鼠标点击键盘光标向下则移动至右键菜单第一个选项</span></span><br><span class="line">action.send_keys(Keys.ARROW_DOWN)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line">action.send_keys(<span class="string">'v'</span>) <span class="comment">#另存为</span></span><br><span class="line">action.perform()</span><br><span class="line"> </span><br><span class="line"><span class="comment">#获取另存为对话框(失败)</span></span><br><span class="line">alert.switch_to_alert()</span><br><span class="line">alert.accept()</span><br><span class="line"><span class="comment"># driver.switch_to_alert().accept()  # 点击弹出里面的确定按钮</span></span><br><span class="line"><span class="comment"># driver.switch_to_alert().dismiss() # 点击弹出上面的X按钮</span></span><br></pre></td></tr></table></figure>
<h2 id="jian-pan-cao-zuo">键盘操作</h2>
<ul>
<li>send_keys(Keys.ENTER) 按下回车键</li>
<li>send_keys(Keys.TAB) 按下Tab制表键</li>
<li>send_keys(Keys.SPACE) 按下空格键space</li>
<li>send_keys(Kyes.ESCAPE) 按下回退键Esc</li>
<li>send_keys(Keys.BACK_SPACE) 按下删除键BackSpace</li>
<li>send_keys(Keys.SHIFT) 按下shift键</li>
<li>send_keys(Keys.CONTROL) 按下Ctrl键</li>
<li>send_keys(Keys.ARROW_DOWN) 按下鼠标光标向下按键</li>
<li>send_keys(Keys.CONTROL,‘a’) 组合键全选Ctrl+A</li>
<li>send_keys(Keys.CONTROL,‘c’) 组合键复制Ctrl+C</li>
<li>send_keys(Keys.CONTROL,‘x’) 组合键剪切Ctrl+X</li>
<li>send_keys(Keys.CONTROL,‘v’) 组合键粘贴Ctrl+V</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"> </span><br><span class="line">driver = webdriver.Firefox()</span><br><span class="line">driver.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#输入框输入内容</span></span><br><span class="line">elem = driver.find_element_by_id(<span class="string">"kw"</span>)</span><br><span class="line">elem.send_keys(<span class="string">"Eastmount CSDN"</span>)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#删除一个字符CSDN 回退键</span></span><br><span class="line">elem.send_keys(Keys.BACK_SPACE)</span><br><span class="line">elem.send_keys(Keys.BACK_SPACE)</span><br><span class="line">elem.send_keys(Keys.BACK_SPACE)</span><br><span class="line">elem.send_keys(Keys.BACK_SPACE)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#输入空格+"博客"</span></span><br><span class="line">elem.send_keys(Keys.SPACE)</span><br><span class="line">elem.send_keys(<span class="string">u"博客"</span>)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#ctrl+a 全选输入框内容</span></span><br><span class="line">elem.send_keys(Keys.CONTROL,<span class="string">'a'</span>)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#ctrl+x 剪切输入框内容</span></span><br><span class="line">elem.send_keys(Keys.CONTROL,<span class="string">'x'</span>)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#输入框重新输入搜索</span></span><br><span class="line">elem.send_keys(Keys.CONTROL,<span class="string">'v'</span>)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#通过回车键替代点击操作</span></span><br><span class="line">driver.find_element_by_id(<span class="string">"su"</span>).send_keys(Keys.ENTER)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">driver.quit()</span><br></pre></td></tr></table></figure>
<h2 id="jie-tu-bao-cun">截图保存</h2>
<p>当爬虫出错时可以截图保存当时页面，以便复现bug，进行分析。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver.maximize_window()</span><br><span class="line">driver.get_screenshot_as_file(<span class="string">"screen shot saved path/pic_&#123;&#125;.png"</span>.format(xx))</span><br></pre></td></tr></table></figure>
<h3 id="dui-selenium-er-ci-feng-zhuang">对selenium二次封装</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasePage</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    主要是把常用的几个Selenium方法封装到BasePage这个类，我们这里演示以下几个方法</span></span><br><span class="line"><span class="string">    back()</span></span><br><span class="line"><span class="string">    forward()</span></span><br><span class="line"><span class="string">    get()</span></span><br><span class="line"><span class="string">    quit()</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, driver)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        写一个构造函数，有一个参数driver</span></span><br><span class="line"><span class="string">        :param driver:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.driver = driver</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">back</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        浏览器后退按钮</span></span><br><span class="line"><span class="string">        :param none:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.driver.back()</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        浏览器前进按钮</span></span><br><span class="line"><span class="string">        :param none:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.driver.forward()</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_url</span><span class="params">(self, url)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        打开url站点</span></span><br><span class="line"><span class="string">        :param url:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.driver.get(url)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">quit_browser</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        关闭并停止浏览器服务</span></span><br><span class="line"><span class="string">        :param none:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.driver.quit()</span><br><span class="line">       </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BrowserEngine</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    定义一个浏览器引擎类，根据browser_type的值去，控制启动不同的浏览器，这里主要是IE，Firefox, Chrome</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, driver)</span>:</span></span><br><span class="line">        self.driver = driver</span><br><span class="line"> </span><br><span class="line">    browser_type = <span class="string">"IE"</span>   <span class="comment"># maybe Firefox, Chrome, IE</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_browser</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        通过if语句，来控制初始化不同浏览器的启动，默认是启动Chrome</span></span><br><span class="line"><span class="string">        :return: driver</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> self.browser_type == <span class="string">'Firefox'</span>:</span><br><span class="line">            driver = webdriver.Firefox()</span><br><span class="line">        <span class="keyword">elif</span> self.browser_type == <span class="string">'Chrome'</span>:</span><br><span class="line">            driver = webdriver.Chrome()</span><br><span class="line">        <span class="keyword">elif</span> self.browser_type == <span class="string">'IE'</span>:</span><br><span class="line">            driver = webdriver.Ie()</span><br><span class="line">        <span class="keyword">else</span>: driver = webdriver.Chrome()</span><br><span class="line"> </span><br><span class="line">        driver.maximize_window()</span><br><span class="line">        driver.implicitly_wait(<span class="number">10</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> driver</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>专业技能</tag>
      </tags>
  </entry>
  <entry>
    <title>requests</title>
    <url>/2019/07/19/requests/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><ol>
<li>爬虫的基本流程</li>
</ol>
<ul>
<li><strong>发起请求</strong><br>
通过HTTP库向目标站点发起请求，也就是发送一个Request，请求可以包含额外的header等信息，等待服务器响应</li>
<li><strong>获取响应内容</strong><br>
如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能是HTML,Json字符串，二进制数据（图片或者视频）等类型</li>
<li><strong>解析内容</strong><br>
得到的内容可能是HTML,可以用正则表达式，页面解析库进行解析，可能是Json,可以直接转换为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理</li>
<li><strong>保存数据</strong><br>
保存形式多样，可以存为文本，也可以保存到数据库，或者保存特定格式的文件</li>
</ul>
<a id="more"></a>
<ol start="2">
<li>
<p><strong>Request中包含什么？</strong></p>
<ul>
<li>
<p><strong>请求方式</strong></p>
<p>主要有：GET/POST两种类型常用，另外还有HEAD/PUT/DELETE/OPTIONS</p>
<ol>
<li>
<p>GET和POST的区别就是：请求的数据GET是在url中，POST则是存放在头部</p>
<p>GET:向指定的资源发出“显示”请求。使用GET方法应该只用在读取数据，而不应当被用于产生“副作用”的操作中，例如在Web Application中。其中一个原因是GET可能会被网络蜘蛛等随意访问</p>
<p>POST:向指定资源提交数据，请求服务器进行处理（例如提交表单或者上传文件）。数据被包含在请求本文中。这个请求可能会创建新的资源或修改现有资源，或二者皆有。</p>
</li>
<li>
<p>HEAD：与GET方法一样，都是向服务器发出指定资源的请求。只不过服务器将不传回资源的本文部分。它的好处在于，使用这个方法可以在不必传输全部内容的情况下，就可以获取其中“关于该资源的信息”（元信息或称元数据）。</p>
</li>
<li>
<p>PUT：向指定资源位置上传其最新内容。</p>
</li>
<li>
<p>OPTIONS：这个方法可使服务器传回该资源所支持的所有HTTP请求方法。用’*'来代替资源名称，向Web服务器发送OPTIONS请求，可以测试服务器功能是否正常运作。</p>
</li>
<li>
<p>DELETE：请求服务器删除Request-URI所标识的资源。</p>
</li>
</ol>
</li>
</ul>
</li>
<li>
<p><strong>请求URL</strong></p>
<p>URL，即统一资源定位符，也就是我们说的网址，统一资源定位符是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址。互联网上的每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。</p>
<p>URL的格式由三个部分组成：</p>
<ul>
<li>第一部分是协议(或称为服务方式)。</li>
<li>第二部分是存有该资源的主机IP地址(有时也包括端口号)。</li>
<li>第三部分是主机资源的具体地址，如目录和文件名等。</li>
</ul>
<p>爬虫爬取数据时必须要有一个目标的URL才可以获取数据，因此，它是爬虫获取数据的基本依据。</p>
</li>
<li>
<p><strong>请求头</strong></p>
<p>包含请求时的头部信息，如User-Agent,Host,Cookies等信息.</p>
</li>
<li>
<p><strong>请求体</strong></p>
<p>请求是携带的数据，如提交表单数据时候的表单数据（POST）</p>
</li>
<li>
<p>能爬取什么样的数据？</p>
<ul>
<li>网页文本：如HTML文档，Json格式化文本等</li>
<li>图片：获取到的是二进制文件，保存为图片格式</li>
<li>视频：同样是二进制文件</li>
<li>其他：只要请求到的，都可以获取</li>
</ul>
</li>
<li>
<p><strong>如何解析数据？</strong></p>
<ol>
<li>直接处理</li>
<li>Json解析</li>
<li>正则表达式处理</li>
<li>BeautifulSoup解析处理</li>
<li>PyQuery解析处理</li>
<li>XPath解析处理</li>
</ol>
</li>
<li>
<p><strong>关于抓取的页面数据和浏览器里看到的不一样的问题？</strong></p>
<p>出现这种情况是因为，很多网站中的数据都是通过js，ajax动态加载的，所以直接通过get请求获取的页面和浏览器显示的不同。</p>
</li>
<li>
<p><strong>如何解决js渲染的问题？</strong></p>
<ul>
<li>分析ajax</li>
<li>Selenium/webdriver</li>
<li>Splash</li>
</ul>
</li>
<li>
<p><strong>怎样保存数据？</strong></p>
</li>
</ol>
<ul>
<li>文本：纯文本，Json,Xml等</li>
<li>关系型数据库：如mysql,oracle,sql server等结构化数据库</li>
<li>非关系型数据库：MongoDB,Redis等key-value形式存储</li>
</ul>
<ol start="11">
<li>
<p><strong>request的应用</strong></p>
<ol>
<li>
<p>Requests是用python语言基于urllib编写的，采用的是Apache2 Licensed开源协议的HTTP库。一句话，requests是python实现的最简单易用的HTTP库，建议爬虫使用requests库。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response  = requests.get(<span class="string">"https://www.baidu.com"</span>)</span><br><span class="line">print(type(response))</span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.text) </span><br><span class="line">print(response.cookies)</span><br><span class="line">print(response.content) <span class="comment">#这样获取的数据是二进制数据</span></span><br><span class="line">print(response.content.decode(<span class="string">"utf-8"</span>))</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>很多情况下的网站如果直接response.text会出现乱码的问题，所以这个使用response.content这样返回的数据格式其实是二进制格式，然后通过decode()转换为utf-8，这样就解决了通过response.text直接返回显示乱码的问题.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response =requests.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line">response.encoding=<span class="string">"utf-8"</span></span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>
<p>不管是通过response.content.decode(&quot;utf-8)的方式还是通过response.encoding=&quot;utf-8&quot;的方式都可以避免乱码的问题发生</p>
</li>
<li>
<p><strong>requests包中的请求方式</strong></p>
<p>requests里提供个各种请求方式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">requests.post(<span class="string">"http://httpbin.org/post"</span>)</span><br><span class="line">requests.put(<span class="string">"http://httpbin.org/put"</span>)</span><br><span class="line">requests.delete(<span class="string">"http://httpbin.org/delete"</span>)</span><br><span class="line">requests.head(<span class="string">"http://httpbin.org/get"</span>)</span><br><span class="line">requests.options(<span class="string">"http://httpbin.org/get"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基本GET请求</span></span><br><span class="line">requests.get(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line"><span class="comment"># 带参数的GET请求</span></span><br><span class="line">requests.get(<span class="string">"http://httpbin.org/get?name=zhaofan&amp;age=23"</span>)</span><br><span class="line"><span class="comment"># 或者 使用params关键字传递参数，如果字典中的参数为None则不会添加到url上</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">"name"</span>:<span class="string">"zhaofan"</span>,</span><br><span class="line">    <span class="string">"age"</span>:<span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">"http://httpbin.org/get"</span>,params=data)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>response的主要属性</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response = requests.get(<span class="string">'http://www.jianshu.com'</span>)</span><br><span class="line">print(type(response.status_code), response.status_code)</span><br><span class="line">print(type(response.headers), response.headers)</span><br><span class="line">print(type(response.cookies), response.cookies)</span><br><span class="line">print(type(response.url), response.url)</span><br><span class="line">print(type(response.history), response.history)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>解析json</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response = requests.get(<span class="string">"http://httpbin.org/get"</span>)</span><br><span class="line"><span class="comment"># requests里面集成的json其实就是执行了json.loads()方法，两者的结果是一样的</span></span><br><span class="line">print(response.json())</span><br><span class="line">print(json.loads(response.text))</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>添加headers</strong></p>
<p>可以定制headers的信息，如当我们直接通过requests请求知乎网站的时候，默认是无法访问的,因为访问知乎需要头部信息，这个时候我们在谷歌浏览器里输入chrome://version,就可以看到用户代理，将用户代理添加到头部信息.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="string">"User-Agent"</span>:<span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"</span></span><br><span class="line">&#125;</span><br><span class="line">response =requests.get(<span class="string">"https://www.zhihu.com"</span>,headers=headers)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>基本POST请求</strong></p>
<p>通过在发送post请求时添加一个data参数，这个data参数可以通过字典构造成，这样对于发送post请求就非常方便.同样的在发送post请求的时候也可以和发送get请求一样通过headers参数传递一个字典类型的数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = &#123;</span><br><span class="line">    <span class="string">"name"</span>:<span class="string">"zhaofan"</span>,</span><br><span class="line">    <span class="string">"age"</span>:<span class="number">23</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.post(<span class="string">"http://httpbin.org/post"</span>,data=data)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>关于请求状态</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response= requests.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"><span class="keyword">if</span> response.status_code == requests.codes.ok:</span><br><span class="line">    print(<span class="string">"访问成功"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>文件上传</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">files= &#123;<span class="string">"files"</span>:open(<span class="string">"git.jpeg"</span>,<span class="string">"rb"</span>)&#125;</span><br><span class="line">response = requests.post(<span class="string">"http://httpbin.org/post"</span>,files=files)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>获取cookie</strong></p>
</li>
</ol>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response = requests.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line">print(response.cookies)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key,value <span class="keyword">in</span> response.cookies.items():</span><br><span class="line">    print(key+<span class="string">"="</span>+value)</span><br></pre></td></tr></table></figure>
<ol start="11">
<li>
<p><strong>会话维持</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = requests.Session()</span><br><span class="line">s.get(<span class="string">"http://httpbin.org/cookies/set/number/123456"</span>)</span><br><span class="line">response = s.get(<span class="string">"http://httpbin.org/cookies"</span>)</span><br><span class="line">print(response.text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面是错误示范</span></span><br><span class="line">requests.get(<span class="string">"http://httpbin.org/cookies/set/number/123456"</span>)</span><br><span class="line">response = requests.get(<span class="string">"http://httpbin.org/cookies"</span>)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>证书验证</strong></p>
<p>现在很多网站都需要证书的验证，没有证书会出现“你访问的不是一个私密链接”之类的错误。对于Https协议，直首先会检查证书是否合法，如果证书不合法，则会抛出：SSLError。针对这一点有两种措施：<br>
下面这种方法：在访问的时候，设置不进行证书的验证，此时返回状态码200，但是依旧会有警告。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.packages <span class="keyword">import</span> urllib3</span><br><span class="line">urllib3.disable_warnings()</span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, verify=<span class="literal">False</span>)</span><br><span class="line">print(response.status_code)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动指定证书</span></span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, cert=(<span class="string">'/path/server.crt'</span>, <span class="string">'/path/key'</span>))</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>代理的设置</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">proxies = &#123;</span><br><span class="line">  <span class="string">"http"</span>: <span class="string">"http://127.0.0.1:9743"</span>,</span><br><span class="line">  <span class="string">"https"</span>: <span class="string">"https://127.0.0.1:9743"</span>,</span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">"https://www.taobao.com"</span>, proxies=proxies)</span><br><span class="line">print(response.status_code)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于需要用户名和密码的代理</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">"http"</span>: <span class="string">"http://user:password@127.0.0.1:9743/"</span>,<span class="comment">#指定好用户名和密码</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">"https://www.taobao.com"</span>, proxies=proxies)</span><br><span class="line">print(response.status_code)</span><br><span class="line"><span class="comment"># socks代理：先安装该模块   pip3 install 'requests[socks]'</span></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'socks5://127.0.0.1:9742'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'socks5://127.0.0.1:9742'</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">"https://www.taobao.com"</span>, proxies=proxies)</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>超时设置</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> ReadTimeout</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = requests.get(<span class="string">"http://httpbin.org/get"</span>, timeout = <span class="number">0.5</span>)</span><br><span class="line">    print(response.status_code)</span><br><span class="line"><span class="keyword">except</span> ReadTimeout:</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>需要登录认证才能访问的网站</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.auth <span class="keyword">import</span> HTTPBasicAuth</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'http://120.27.34.24:9001'</span>, auth=HTTPBasicAuth(<span class="string">'user'</span>, <span class="string">'123'</span>))</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">r = requests.get(<span class="string">'http://120.27.34.24:9001'</span>, auth=(<span class="string">'user'</span>, <span class="string">'123'</span>))</span><br><span class="line">print(r.status_code)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>异常处理</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> ReadTimeout, ConnectionError, RequestException</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = requests.get(<span class="string">"http://httpbin.org/get"</span>, timeout = <span class="number">0.5</span>)</span><br><span class="line">    print(response.status_code)</span><br><span class="line"><span class="keyword">except</span> ReadTimeout:</span><br><span class="line">    print(<span class="string">'Timeout'</span>)</span><br><span class="line"><span class="keyword">except</span> ConnectionError:</span><br><span class="line">    print(<span class="string">'Connection error'</span>)</span><br><span class="line"><span class="keyword">except</span> RequestException:</span><br><span class="line">    print(<span class="string">'Error'</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>专业技能</tag>
      </tags>
  </entry>
  <entry>
    <title>激活函数</title>
    <url>/2018/07/31/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="sigmoid">sigmoid</h3>
<h4 id="jian-jie">简介</h4>
<p>sigmoid函数也叫Logistic函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。</p>
<p><img src="/2018/07/31/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/sigmoid.png" alt="avatat"></p>
<a id="more"></a>
<p>Sigmoid函数公式：<br>
\[
S(x)=\frac{1}{1+e^{-x}}
\]<br>
其对x的导数可以用自身表示:<br>
\[
S^{\prime}(x)=\frac{e^{-x}}{\left(1+e^{-x}\right)^{2}}=S(x)(1-S(x))
\]</p>
<h4 id="ti-du-xiao-shi">梯度消失</h4>
<p>优化神经网络的方法是Back Propagation，即导数的后向传递：先计算输出层对应的loss，然后将loss以导数的形式不断向上一层网络传递，修正相应的参数，达到降低loss的目的。 Sigmoid函数在深度网络中常常会导致导数逐渐变为0，使得参数无法被更新，神经网络无法被优化。原因在于两点：</p>
<ol>
<li>在上图中容易看出，当\(\sigma(x)\)中\(x\)较大或较小时，导数接近0，而后向传递的数学依据是微积分求导的链式法则，当前层的导数需要之前各层导数的乘积，几个小数的相乘，结果会很接近0 .</li>
<li>Sigmoid导数的最大值是0.25，这意味着导数在每一层至少会被压缩为原来的1/4，通过两层后被变为1/16，…，通过10层后为\(\frac{1}{4^10}\)。请注意这里是“至少”，导数达到最大值这种情况还是很少见的。</li>
</ol>
<p>优点：平滑、易于求导。<br>
缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。<strong>幂运算相对耗时</strong></p>
<h4 id="dai-ma">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1</span>+np.exp(-x))</span><br></pre></td></tr></table></figure>
<h3 id="tanh">tanh</h3>
<p><img src="/2018/07/31/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/tanh.png" alt="avatar"></p>
<p>tanh公式<br>
\[
\tanh x=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
\]</p>
<h3 id="re-lu">ReLU</h3>
<p><img src="/2018/07/31/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/relu.png" alt="avatar"><br>
\[
ReLU = max(0,x)
\]</p>
<p>优点：</p>
<ul>
<li>解决了gradient vanishing问题 (在正区间)</li>
<li>计算速度非常快，只需要判断输入是否大于0</li>
<li>收敛速度远快于sigmoid和tanh</li>
</ul>
<p>存在的问题：</p>
<ol>
<li>ReLU的输出不是zero-centered</li>
<li>Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生:
<ol>
<li>非常不幸的参数初始化，这种情况比较少见</li>
<li>learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。</li>
</ol>
</li>
</ol>
<h3 id="leaky-re-lu">Leaky ReLU</h3>
<p><img src="/2018/07/31/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/leakrelu.png" alt="avatar"></p>
<p>\[
f(x)=max(\alpha x,x)
\]<br>
人们为了解决Dead ReLU Problem，提出了将ReLU的前半段设为\(0.01x\)而非0。另外一种直观的想法是基于参数的方法，即Parametric ReLU:\(f(x)=max(\alpha x,x)\)，其中\(\alpha\)可由back propagation学出来。理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。</p>
<h3 id="strong-elu-exponential-linear-units-strong"><strong>ELU (Exponential Linear Units)</strong></h3>
<p><img src="/2018/07/31/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/ELU.png" alt="avata"><br>
\[
f(x)=\left\{\begin{array}{ll}
x, &amp; \text { if } x>0 \\
\alpha\left(e^{x}-1\right), &amp; \text { otherwise }
\end{array}\right.
\]<br>
ELU也是为解决ReLU存在的问题而提出，显然，ELU有ReLU的基本所有优点，以及：</p>
<ul>
<li>不会有Dead ReLU问题</li>
<li>输出的均值接近0，zero-centered</li>
</ul>
<p>它的一个小问题在于计算量稍大。类似于Leaky ReLU，理论上虽然好于ReLU，但在实际使用中目前并没有好的证据ELU总是优于ReLU。</p>
<h3 id="can-kao">参考</h3>
<ol>
<li><a href="https://baike.baidu.com/item/Sigmoid%E5%87%BD%E6%95%B0/7981407?fr=aladdin" target="_blank" rel="noopener">Sigmoid函数</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247484448&amp;idx=1&amp;sn=80479bf682d7ade4184fde809ea0cc3e&amp;chksm=970c2cf6a07ba5e0e046e4d5b8169e8fb58c9e858878840b571d43060e30986152a534830bc6&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">神经网络激活函数=生物转换器？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/25110450" target="_blank" rel="noopener">聊一聊深度学习的activation function</a></li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title>优化算法</title>
    <url>/2018/07/11/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="ti-du-xia-jiang-suan-fa">梯度下降算法</h2>
<p>梯度下降法(Gradient descent)或最速下降法(steepest descent)是求解无约束最优化问题的一种常用的、实现简单的方法。<br>
假设\(f(x)\)是\(R^n\)上具有一阶连续偏导数的函数。求解<br>
\[
   min_{x \in R^n}f(x)
\]<br>
无约束最优化问题。 \(f^\ast\)表示目标函数\(f(x)\)的极小点。<br>
梯度下降法是一种迭代算法，选取适当的初值\(x^0\),不断迭代，更新x的值，进行目标函数的极小化，直到收敛。由于负梯度的方向是使函数下降最快的方向，在迭代的每一步，以负梯度方向更新x的值，从而达到减少函数值的目的。<br>
第k+1次迭代值：<br>
\[
x^{(k+1)} \leftarrow x^{(k)}+\lambda_{k} p_{k}
\]<br>
其中，\(p_k\)是搜索方向，取负梯度方向\(p_k = - \nabla f(x^k)\),\(\lambda_k\)使得<br>
\[
f\left(x^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geq 0} f\left(x^{(k)}+\lambda p_{k}\right)
\]</p>
<a id="more"></a>
<h3 id="suan-fa-miao-shu">算法描述：</h3>
<p>输入:目标函数\(f(x)\)，梯度函数\(g(x^k)=\nabla f(x^k)\),计算\(\epsilon\).<br>
输出：\(f(x)\)的极小值点\(x^\star\)</p>
<ol>
<li>取初值\(x^0 \in R^n\),置k=0</li>
<li>计算\(f(x^k)\)</li>
<li>计算梯度\(g_k = g(x^k)\),当\(\left\|g_{k}\right\|&lt;\varepsilon\)时，停止迭代，令\(p_k = -g(x^k)\)求\(\lambda_k\),使<br>
\[
f\left(x^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geq 0} f\left(x^{(k)}+\lambda p_{k}\right)
\]</li>
<li>置\(x^{k+1}=x^k+\lambda_k p_k\),计算\(f(x^{k+1})\)当$<br>
\left|f\left(x<sup>{(k+1)}\right)-f\left(x</sup>{(k)}\right)\right|&lt;\varepsilon<br>
\(或\)\left|x<sup>{(k+1)}-x</sup>{(k)}\right|&lt;\varepsilon\(停止迭代。令\)x^\star = x^{k+1}$</li>
<li>否则，置\(k=k+1\),转3.</li>
</ol>
<h3 id="ti-du-xia-jiang-diao-you">梯度下降调优</h3>
<ol>
<li><strong>算法的步长选择</strong>。步长取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。</li>
<li><strong>算法参数的初始值选择</strong>。初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</li>
<li><strong>归一化</strong>。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化。</li>
</ol>
<h3 id="ti-du-xia-jiang-fa-da-jia-zu-bgd-sgd-mbgd">梯度下降法大家族（BGD，SGD，MBGD）</h3>
<ol>
<li><strong>批量梯度下降法（Batch Gradient Descent）</strong> 批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新。</li>
<li><strong>随机梯度下降法（Stochastic Gradient Descent）</strong> 其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的样本的数据，而是仅仅选取一个样本来求梯度。随机梯度下降法，和批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。</li>
<li><strong>小批量梯度下降法（Mini-batch Gradient Descent）</strong> 小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于\(m\)个样本，我们采用\(x\)个样本来迭代，\(1&lt;x&lt;m\)。一般可以取\(x=16,32,64...\)，当然根据样本的数据，可以调整这个\(x\) 的值。</li>
</ol>
<p>上述三种方法得到局部最优解的过程：<br>
<img src="/2018/07/11/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/C80A2811-1678-4371-BCCD-8AA51A777820.jpg" alt="avatar"></p>
<h3 id="dai-ma-shi-xian">代码实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(X, y, W, B, alpha, max_iters)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    使用了所有的样本进行梯度下降</span></span><br><span class="line"><span class="string">    X: 训练集,</span></span><br><span class="line"><span class="string">    y: 标签,</span></span><br><span class="line"><span class="string">    W: 权重向量,</span></span><br><span class="line"><span class="string">    B: bias,</span></span><br><span class="line"><span class="string">    alpha: 学习率,</span></span><br><span class="line"><span class="string">    max_iters: 最大迭代次数.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    dW = <span class="number">0</span> <span class="comment"># 权重梯度收集器</span></span><br><span class="line">    dB = <span class="number">0</span> <span class="comment"># Bias梯度的收集器</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>] <span class="comment"># 样本数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_iters):</span><br><span class="line">        dW = <span class="number">0</span> <span class="comment"># 每次迭代重置</span></span><br><span class="line">        dB = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment"># 1. 迭代所有的样本</span></span><br><span class="line">            <span class="comment"># 2. 计算权重和bias的梯度保存在w_grad和b_grad,</span></span><br><span class="line">            <span class="comment"># 3. 通过增加w_grad和b_grad来更新dW和dB</span></span><br><span class="line">            W = W - alpha * (dW / m) <span class="comment"># 更新权重</span></span><br><span class="line">            B = B - alpha * (dB / m) <span class="comment"># 更新bias</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> W, B</span><br></pre></td></tr></table></figure>
<h3 id="ti-du-xia-jiang-he-zui-xiao-er-cheng-fa">梯度下降和最小二乘法</h3>
<p>梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。</p>
<h2 id="niu-dun-fa">牛顿法</h2>
<p>一般来说，牛顿法主要应用在两个方面，1：求方程的根；2：最优化。</p>
<h3 id="qiu-jie-guo-cheng">求解过程</h3>
<p>并不是所有的方程都有求根公式，或者求根公式很复杂，导致求解困难。利用牛顿法，可以迭代求解。</p>
<p>原理是利用泰勒公式，在 \(x_0\)处展开，且展开到一阶, 即 \(f(x)=f(x_0)+f^\prime{x_0}(x-x_0)\) 。求解方程\(f(x)=0\)，等价于\(f(x_0)+f^\prime(x_0)(x-x_0)=0\)，求解 \(x=x_1=x_0-\frac{f(x_0)}{f^\prime(x_0)}\)。因为这是利用泰勒公式的一阶展开,\(f(x)=f(x_0)+f^\prime(x_0)(x-x_0)\) 处并不是完全相等，而是近似相等，这里求得的的\(x_1\)并不能让 \(f(x)=0\)，只能说 \(f(x_1)\) 的值比\(f(x_9)\)的值更接近于0，于是迭代的想法就很自然了，可以进而推出 \(x_{n+1}=x_n-\frac{f(x_n)}{f\prime(x_n)}\)，通过迭代，这个式子必然在 \(f(x^\star)=0\) 的时候收敛。整个过程如下：<br>
<img src="/2018/07/11/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/B03C2A8B-AF9F-4871-94F6-0E7F6D6EC890.png" alt="avatar"></p>
<h3 id="zui-you-hua">最优化</h3>
<p>无约束最优化问题<br>
\[
min_{x \in R^N}f(x)
\]<br>
其中\(s^\star\)为目标函数的极小点</p>
<p>设\(f(x)\)具有二阶连续偏导，若第\(k\)次迭代值为\(x^k\),则可将\(f(x)\)在\(x^k\)附近进行二阶泰勒展开：<br>
\[
f(x)=f\left(x^{(k)}\right)+g_{k}^{T}\left(x-x^{(k)}\right)+\frac{1}{2}\left(x-x^{(k)}\right)^{T} H\left(x^{(k)}\right)\left(x-x^{(x)}\right)
\]<br>
其中，\(g_k=g(x^k)=\nabla f(x^k)\)是\(f(x)\)的梯度向量在点\(x^k\)的值，\(H(x^k)\)是\(f(x)\)的海塞矩阵<br>
\[
H(x)=\left[\frac{\partial^{2} f}{\partial x_{i} \partial x_{j}}\right]_{n \times n}
\]<br>
在点\(x^k\)的值。</p>
<p>函数\(f(x)\)有极值的必要条件是在极值点处一阶导数为0，即梯度向量为0。特别的当\(H(x^k)\)是正定矩阵时，函数\(f(x)\)的极值为最小值。</p>
<p>为了的得到一阶导数为0的点，可以用到<strong>求解方程</strong>部分的方法。根据二阶泰勒展开，对\(\nabla f(x)\)在\(x^k\)进行展开得（也可以对泰勒公式再进行求导）：<br>
\[
\nabla f(x)=g_{k}+H_{k}\left(x-x^{(k)}\right)
\]<br>
其中，\(H_k=H(x^k)\)则：<br>
\[
\begin{aligned}&amp;g_{k}+H_{k}\left(x^{(k+1)}-x^{(k)}\right)=0\\&amp;x^{(k+1)}=x^{(k)}-H_{k}^{-1} g_{k}\end{aligned}
\]<br>
令<br>
\[
H_kp_k = -g_k
\]<br>
得到迭代公式：<br>
\[
x^{k+1} = x^k+p_k
\]<br>
最终在\(\nabla f(x^\star)=0\)收敛</p>
<h3 id="suan-fa-miao-shu-1">算法描述</h3>
<p>输入：目标函数\(f(x)\)，梯度\(g(x) = \nabla f(x)\),海塞矩阵\(H(x)\),精度(阈值)\(\epsilon\)</p>
<p>输出：\(f(x)\)的极小值点\(x^\star\)</p>
<ol>
<li>
<p>取初始点\(x^0\),置k=0</p>
</li>
<li>
<p>计算\(g_k = g(x^k)\)</p>
</li>
<li>
<p>若\(\left\|g_k \right\| &lt; \epsilon\)，则停止计算，求得近似解:\(x^\star = x^k\)</p>
</li>
<li>
<p>计算\(H_k = H(x^k)\),并求\(p_k\)。<br>
\[
H_kp_k = -g_k
\]</p>
</li>
<li>
<p>置 \(x(k+1) = x^k + p_k\)</p>
</li>
<li>
<p>置 k=k+1, 转2.</p>
</li>
</ol>
<h3 id="niu-dun-fa-he-ti-du-xia-jiang">牛顿法和梯度下降</h3>
<p>梯度下降法和牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法是用二阶的海森矩阵的逆矩阵求解。相对而言，使用牛顿法收敛更快（迭代更少次数）。但是每次迭代的时间比梯度下降法长。</p>
<p>梯度下降法：<br>
\[
x^{k+1} = x^k - \lambda \nabla f(x^k)
\]<br>
牛顿法：<br>
\[
x^{k+1} = x^k-\lambda(H^k)^{-1}\nabla f(x^k)
\]<br>
至于为什么牛顿法收敛更快，通俗来说梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。<br>
<img src="/2018/07/11/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/84835FEB-0EA4-41E5-B07D-41BBEB49AE9B.jpg" alt="9c6e73f0200cbe504e0370a18ca3a95a"><br>
红色的牛顿法的迭代路径，绿色的是梯度下降法的迭代路径。<br>
牛顿法是梯度下降法的进一步发展，梯度法利用了目标函数的一阶偏导信息、一负梯度方向作为搜索方向，只考虑目标函数在迭代点的局部性质；而牛顿法不仅使用目标函数的一阶偏导数，还进一步利用了目标函数的二阶偏导数，这样就考虑了梯度变化的趋势，因而能更全面地确定合适的搜索方向以加快收敛，它具有二阶收敛速度，但是牛顿法也存在两个<strong>缺点</strong> ：</p>
<ul>
<li>对目标函数有较严格的要求，函数必须具有连续的一、二阶偏导，海塞矩阵必须正定。</li>
<li>计算相当复杂，除了需要计算梯度外，还需计算二阶偏导矩阵和它的逆矩阵。计算量、存储量都很大。且均以维数N的平方比增加，当N很大时，这个问题就更加突出。</li>
</ul>
<h2 id="ni-niu-dun-fa">拟牛顿法</h2>
<p>在牛顿法的迭代计算中，需要计算海塞矩阵的逆矩阵\(H^-1\)，这一计算比较复杂，考虑用一个n阶矩阵\(G_k=G(x^k)\)来近似替代\(H^{-1}_{k}=H^{-1}(x^k)\)。这就是拟牛顿的基本想法。</p>
<p>要找到近似的替代矩阵，必定要和\(H_k\)有类似的性质。先看下牛顿法迭代中海塞矩阵\(H_k\)满足的条件。首先，\(H_k\)满足以下关系：</p>
<p>取\(x=x^{k-1}\)，由<br>
\[
\nabla f(x)=g_k+H_k(x-x^k)
\]<br>
得：<br>
\[
g_{k-1}-g_k = H_k(x^{k-1}-x^k)
\]<br>
记\(y_k=g_k-g_{k-1},\delta_k = x^k-x^{k-1}\),则：<br>
\[
y_{k-1}=H_k\delta_{k-1}H^{-1}_ky_{k-1}=\delta_{k-1}
\]<br>
称为拟牛顿条件。</p>
<p>其次，如果\(H_k\)是正定的（\(H_k^{-1}\)也是正定的），那么保证牛顿法的搜索方向\(p_k\)是下降方向。这是因为搜索方向是\(p_k = -H_k^{-1}g_k\),</p>
<p>由<br>
\[
x^{k+1} = x^k -H^{-1}_kg_k
\]<br>
有<br>
\[
x = x^k - \lambda H_k^{-1}g_k = x^k+\lambda p_k
\]<br>
则\(f(x)\)在\(x^k\)的泰勒展开可近似为：<br>
\[
f(x) = f(x^k) - \lambda g^T_kH_k^{-1}g_k
\]<br>
由于\(H_k^{-1}\)正定，故\(g_k^TH_k^{-1}g_k>0\)。当\(\lambda\)为一个充分小的正数时，有\(f(x)&lt;f(x^k)\),即搜索方向\(p_k\)是下降方向。</p>
<p>因此拟牛顿法将\(G_k\)作为\(H^{-1}_k\)近似。要求\(G_k\)满足同样的条件。首先，每次迭代矩阵\(G_k\)是正定的。同时，$G_k满足下面的拟牛顿条件<br>
\[
G_{k+1}y_k = \delta_k
\]<br>
按照拟牛顿条件，在每次迭代中可以选择更新矩阵\(G_{k+1}\):<br>
\[
G_{k+1} = G_k + \nabla G_k
\]</p>
<h3 id="dfp">DFP</h3>
<h3 id="bfgs">BFGS</h3>
<h3 id="broyden">Broyden</h3>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title>linux命令速查</title>
    <url>/2018/06/25/linux_quickcheck/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2018/06/25/linux_quickcheck/terminal_command.jpg" alt></p>
<a id="more"></a>
<h1 id="a-href-https-linux-gaomeluo-com-linux-ming-ling-a"><a href="https://linux.gaomeluo.com/" target="_blank" rel="noopener">Linux 命令</a></h1>
<p>linux命令是对Linux系统进行管理的命令。对于Linux系统来说，无论是中央处理器、内存、磁盘驱动器、键盘、鼠标，还是用户等都是文件，Linux系统管理的命令是它正常运行的核心，与之前的DOS命令类似。linux命令在系统中有两种类型：内置Shell命令和Linux命令。</p>
<h2 id="linux-de-wen-jian-xi-tong">linux的文件系统</h2>
<ol>
<li>/boot:系统启动相关的文件，如内核、initrd、以及grub（bootloader）</li>
<li>/dev:设备文件：
<ol>
<li>块设备：随机访问，数据块</li>
<li>字符设备：线性访问，按字符为单位</li>
<li>设备号：主设备号（major）和次设备号（minor）</li>
</ol>
</li>
<li>/etc：配置文件</li>
<li>/home：用户的家目录，每一个用户的家目录通常默认为/home/username</li>
<li>/root：管理员的家目录；</li>
<li>/lib:库文件
<ol>
<li>静态库：.a</li>
<li>动态库：.dll, .so(shared object)</li>
<li>/lib/moudules:内核模块文件</li>
</ol>
</li>
<li>/lib64</li>
<li>/media:挂载点目录，移动设备</li>
<li>/mnt:挂载点目录，额外的临时文件系统</li>
<li>/opt:可选目录，第三方程序的安装目录</li>
<li>/proc：伪文件系统，内核映射文件</li>
<li>/sys：伪文件系统，跟硬件设备相关的属性映射文件</li>
<li>/tmp:临时文件，/var/tmp</li>
<li>/var:可变化的文件</li>
<li>/bin:可执行文件，用户命令</li>
<li>/sbin：管理命令</li>
</ol>
<h2 id="yong-hu-guan-li">用户管理</h2>
<h3 id="tian-jia-yong-hu">添加用户</h3>
<figure class="highlight armasm"><table><tr><td class="code"><pre><span class="line"><span class="symbol">sudo</span> <span class="keyword">adduser </span>xxx</span><br></pre></td></tr></table></figure>
<h3 id="tian-jia-guan-li-yuan-quan-xian">添加管理员权限</h3>
<figure class="highlight ada"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/sudoers</span><br><span class="line"># 添加 xxx <span class="keyword">ALL</span>=(<span class="keyword">ALL</span>:<span class="keyword">ALL</span>) <span class="keyword">ALL</span></span><br></pre></td></tr></table></figure>
<h3 id="fu-yu-yong-hu-dui-mou-ge-mu-lu-de-du-xie-quan-xian">赋予用户对某个目录的读写权限：</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sudo setfacl -R -m u:username:rwx your_file_or_fir</span><br><span class="line"><span class="comment"># 如sudo setfacl -R -m u:ssr:rwx /data/ssr</span></span><br></pre></td></tr></table></figure>
<h2 id="chang-yong-ming-ling">常用命令</h2>
<h3 id="ya-suo">压缩</h3>
<ol>
<li>
<p>tar</p>
<ol>
<li>解包：tar zxvf filename.tar</li>
<li>打包：tar czvf filename.tar dirname</li>
</ol>
</li>
<li>
<p>gz命令</p>
<ol>
<li>解压1：gunzip filename.gz</li>
<li>解压2：gzip -d filename.gz</li>
<li>压缩：gzip filename
<ol>
<li>.tar.gz 和  .tgz</li>
<li>解压：tar zxvf filename.tar.gz</li>
<li>压缩：tar zcvf filename.tar.gz dirname</li>
<li>压缩多个文件：tar zcvf filename.tar.gz dirname1 dirname2 dirname3…</li>
</ol>
</li>
</ol>
</li>
<li>
<p>bz2命令</p>
<ol>
<li>解压1：bzip2 -d filename.bz2</li>
<li>解压2：bunzip2 filename.bz2</li>
<li>压缩：bzip2 -z filename</li>
<li>.tar.bz2
<ol>
<li>解压：tar jxvf filename.tar.bz2</li>
<li>压缩：tar jcvf filename.tar.bz2 dirname</li>
</ol>
</li>
</ol>
</li>
<li>
<p>bz命令</p>
<ol>
<li>解压1：bzip2 -d <a href="http://filename.bz" target="_blank" rel="noopener">filename.bz</a></li>
<li>解压2：bunzip2 <a href="http://filename.bz" target="_blank" rel="noopener">filename.bz</a></li>
<li>.tar.bz
<ol>
<li>解压：tar jxvf <a href="http://filename.tar.bz" target="_blank" rel="noopener">filename.tar.bz</a></li>
</ol>
</li>
<li>z命令
<ol>
<li>解压：uncompress filename.z</li>
<li>压缩：compress filename</li>
<li>.tar.z<br>
1.   解压：tar zxvf filename.tar.z<br>
2.   压缩：tar zcvf filename.tar.z dirname</li>
</ol>
</li>
<li>zip命令<br>
1.   解压：unzip filename.zip<br>
2.   压缩：zip filename.zip dirname</li>
</ol>
</li>
<li>
<p>总结：z 以gzip格式压缩，c表示create，v显示压缩的详细情况 f file</p>
<ol>
<li>压缩</li>
<li>tar –cvf jpg.tar *.jpg <a href="//xn--jpgtar-k18i60zu6u9rh3xhkqa1h032blufb97e1p9e.jpg" target="_blank" rel="noopener">//将目录里所有jpg文件打包成tar.jpg</a></li>
<li>tar –czf jpg.tar.gz *.jpg <a href="//xn--jpgjpg-k18i60zu6u9rh3xhkqa1h032blufb97e1p9e.xn--tar-x33e" target="_blank" rel="noopener">//将目录里所有jpg文件打包成jpg.tar后</a>，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gz</li>
<li>tar –cjf jpg.tar.bz2 *.jpg <a href="//xn--jpgjpg-k18i60zu6u9rh3xhkqa1h032blufb97e1p9e.xn--tar-x33e" target="_blank" rel="noopener">//将目录里所有jpg文件打包成jpg.tar后</a>，并且将其用bzip2压缩，生成一个bzip2压缩过的包，命名为jpg.tar.bz2</li>
<li>tar –cZf jpg.tar.Z *.jpg <a href="//xn--jpgjpg-k18i60zu6u9rh3xhkqa1h032blufb97e1p9e.xn--tar-x33e" target="_blank" rel="noopener">//将目录里所有jpg文件打包成jpg.tar后</a>，并且将其用compress压缩，生成一个umcompress压缩过的包，命名为jpg.tar.Z</li>
<li>rar a jpg.rar *.jpg //rar格式的压缩，需要先下载rar for linux</li>
<li>zip jpg.zip *.jpg //zip格式的压缩，需要先下载zip for linux</li>
</ol>
</li>
</ol>
<pre><code>2. 解压
    1. tar –xvf file.tar //解压 tar包 
    2. tar -xzvf file.tar.gz //解压tar.gz 
    3. tar -xjvf file.tar.bz2 //解压 
    4. tar.bz2 tar –xZvf file.tar.Z //解压tar.Z 
    5. unrar e file.rar //解压rar 
    6. unzip file.zip //解压zip 
</code></pre>
<h3 id="screen">screen</h3>
<ol>
<li>新建一个叫session_name的session：screen -S session_name</li>
<li>列出当前所有的session：screen -ls</li>
<li>回到session_name这个session：screen -r session_name</li>
<li>远程detach某个session：screen -d session_name</li>
<li>结束当前session并回到session_name这个session：screen -d -r session_name</li>
<li>利用exit退出并kill掉session</li>
</ol>
<h3 id="cha-kan-xian-qia-he-nei-cun-shi-yong-qing-kuang">查看显卡和内存使用情况</h3>
<ol>
<li>查看显卡使用情况：watch -n 5 nvidia-smi</li>
<li>查看磁盘使用：df -h</li>
</ol>
<h3 id="yi-dong-zhong-ming-ming">移动重命名</h3>
<ol>
<li>将/usr/udt中的所有文件移到当前目录(用”.”表示)中：$ mv /usr/udt/* .</li>
<li>将文件test.txt重命名为wbk.txt：$ mv test.txt wbk.txt</li>
</ol>
<h3 id="mu-lu-guan-li">目录管理</h3>
<ol>
<li>ls</li>
<li>cd</li>
<li>pwd</li>
<li>mkdir</li>
<li>rmdir</li>
<li>tree</li>
</ol>
<h3 id="wen-jian-guan-li">文件管理</h3>
<ol>
<li>touch</li>
<li>stat</li>
<li>file</li>
<li>rm
<ol>
<li>-i: 若指定目录已有同名文件，则先询问是否覆盖旧文件;</li>
<li>-f: 在mv操作要覆盖某已有的目标文件时不给任何指示;</li>
</ol>
</li>
<li>cp [options] source dest
<ol>
<li>-a:此选项通常在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容。其作用等于dpr参数组合。</li>
<li>-d：复制时保留链接。这里所说的链接相当于Windows系统中的快捷方式。</li>
<li>-f：覆盖已经存在的目标文件而不给出提示。</li>
<li>-i：与-f选项相反，在覆盖目标文件之前给出提示，要求用户确认是否覆盖，回答&quot;y&quot;时目标文件将被覆盖。</li>
<li>-p：除复制文件的内容外，还把修改时间和访问权限也复制到新文件中。</li>
<li>-r：若给出的源文件是一个目录文件，此时将复制该目录下所有的子目录和文件。</li>
<li>-l：不复制文件，只是生成链接文件。</li>
</ol>
</li>
<li>mv</li>
<li>nano</li>
<li>vi</li>
<li>vim</li>
</ol>
<h3 id="ri-qi-shi-jian">日期时间</h3>
<ol>
<li>data</li>
<li>clock</li>
<li>hwclock</li>
<li>cal</li>
<li>ntpdate</li>
</ol>
<h3 id="cha-kan-wen-ben">查看文本</h3>
<ol>
<li>cat</li>
<li>tac</li>
<li>more</li>
<li>less</li>
<li>head</li>
<li>tail</li>
</ol>
<h3 id="wen-jian-cha-zhao-find">文件查找:find</h3>
<h4 id="shuo-ming">说明</h4>
<p><strong>find命令</strong> 用来在指定目录下查找文件。任何位于参数之前的字符串都将被视为欲查找的目录名。如果使用该命令时，不设置任何参数，则find命令将在当前目录下查找子目录与文件。并且将查找到的子目录和文件全部进行显示。</p>
<h4 id="yu-fa">语法</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find(选项)(参数)</span><br></pre></td></tr></table></figure>
<h4 id="xuan-xiang">选项</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-amin&lt;分钟&gt;：查找在指定时间曾被存取过的文件或目录，单位以分钟计算；</span><br><span class="line">-anewer&lt;参考文件或目录&gt;：查找其存取时间较指定文件或目录的存取时间更接近现在的文件或目录；</span><br><span class="line">-atime&lt;24小时数&gt;：查找在指定时间曾被存取过的文件或目录，单位以24小时计算；</span><br><span class="line">-cmin&lt;分钟&gt;：查找在指定时间之时被更改过的文件或目录；</span><br><span class="line">-cnewer&lt;参考文件或目录&gt;查找其更改时间较指定文件或目录的更改时间更接近现在的文件或目录；</span><br><span class="line">-ctime&lt;24小时数&gt;：查找在指定时间之时被更改的文件或目录，单位以24小时计算；</span><br><span class="line">-daystart：从本日开始计算时间；</span><br><span class="line">-depth：从指定目录下最深层的子目录开始查找；</span><br><span class="line">-expty：寻找文件大小为0 Byte的文件，或目录下没有任何子目录或文件的空目录；</span><br><span class="line">-exec&lt;执行指令&gt;：假设find指令的回传值为True，就执行该指令；</span><br><span class="line">-false：将find指令的回传值皆设为False；</span><br><span class="line">-fls&lt;列表文件&gt;：此参数的效果和指定“-ls”参数类似，但会把结果保存为指定的列表文件；</span><br><span class="line">-follow：排除符号连接；</span><br><span class="line">-fprint&lt;列表文件&gt;：此参数的效果和指定“-print”参数类似，但会把结果保存成指定的列表文件；</span><br><span class="line">-fprint0&lt;列表文件&gt;：此参数的效果和指定“-print0”参数类似，但会把结果保存成指定的列表文件；</span><br><span class="line">-fprintf&lt;列表文件&gt;&lt;输出格式&gt;：此参数的效果和指定“-printf”参数类似，但会把结果保存成指定的列表文件；</span><br><span class="line">-fstype&lt;文件系统类型&gt;：只寻找该文件系统类型下的文件或目录；</span><br><span class="line">-gid&lt;群组识别码&gt;：查找符合指定之群组识别码的文件或目录；</span><br><span class="line">-group&lt;群组名称&gt;：查找符合指定之群组名称的文件或目录；</span><br><span class="line">-help或——help：在线帮助；</span><br><span class="line">-ilname&lt;范本样式&gt;：此参数的效果和指定“-lname”参数类似，但忽略字符大小写的差别；</span><br><span class="line">-iname&lt;范本样式&gt;：此参数的效果和指定“-name”参数类似，但忽略字符大小写的差别；</span><br><span class="line">-inum&lt;inode编号&gt;：查找符合指定的inode编号的文件或目录；</span><br><span class="line">-ipath&lt;范本样式&gt;：此参数的效果和指定“-path”参数类似，但忽略字符大小写的差别；</span><br><span class="line">-iregex&lt;范本样式&gt;：此参数的效果和指定“-regexe”参数类似，但忽略字符大小写的差别；</span><br><span class="line">-links&lt;连接数目&gt;：查找符合指定的硬连接数目的文件或目录；</span><br><span class="line">-iname&lt;范本样式&gt;：指定字符串作为寻找符号连接的范本样式；</span><br><span class="line">-ls：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出；</span><br><span class="line">-maxdepth&lt;目录层级&gt;：设置最大目录层级；</span><br><span class="line">-mindepth&lt;目录层级&gt;：设置最小目录层级；</span><br><span class="line">-mmin&lt;分钟&gt;：查找在指定时间曾被更改过的文件或目录，单位以分钟计算；</span><br><span class="line">-mount：此参数的效果和指定“-xdev”相同；</span><br><span class="line">-mtime&lt;24小时数&gt;：查找在指定时间曾被更改过的文件或目录，单位以24小时计算；</span><br><span class="line">-name&lt;范本样式&gt;：指定字符串作为寻找文件或目录的范本样式；</span><br><span class="line">-newer&lt;参考文件或目录&gt;：查找其更改时间较指定文件或目录的更改时间更接近现在的文件或目录；</span><br><span class="line">-nogroup：找出不属于本地主机群组识别码的文件或目录；</span><br><span class="line">-noleaf：不去考虑目录至少需拥有两个硬连接存在；</span><br><span class="line">-nouser：找出不属于本地主机用户识别码的文件或目录；</span><br><span class="line">-ok&lt;执行指令&gt;：此参数的效果和指定“-exec”类似，但在执行指令之前会先询问用户，若回答“y”或“Y”，则放弃执行命令；</span><br><span class="line">-path&lt;范本样式&gt;：指定字符串作为寻找目录的范本样式；</span><br><span class="line">-perm&lt;权限数值&gt;：查找符合指定的权限数值的文件或目录；</span><br><span class="line">-print：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出。格式为每列一个名称，每个名称前皆有“.&#x2F;”字符串；</span><br><span class="line">-print0：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出。格式为全部的名称皆在同一行；</span><br><span class="line">-printf&lt;输出格式&gt;：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出。格式可以自行指定；</span><br><span class="line">-prune：不寻找字符串作为寻找文件或目录的范本样式;</span><br><span class="line">-regex&lt;范本样式&gt;：指定字符串作为寻找文件或目录的范本样式；</span><br><span class="line">-size&lt;文件大小&gt;：查找符合指定的文件大小的文件；</span><br><span class="line">-true：将find指令的回传值皆设为True；</span><br><span class="line">-type&lt;文件类型&gt;：只寻找符合指定的文件类型的文件；</span><br><span class="line">-uid&lt;用户识别码&gt;：查找符合指定的用户识别码的文件或目录；</span><br><span class="line">-used&lt;日数&gt;：查找文件或目录被更改之后在指定时间曾被存取过的文件或目录，单位以日计算；</span><br><span class="line">-user&lt;拥有者名称&gt;：查找符和指定的拥有者名称的文件或目录；</span><br><span class="line">-version或——version：显示版本信息；</span><br><span class="line">-xdev：将范围局限在先行的文件系统中；</span><br><span class="line">-xtype&lt;文件类型&gt;：此参数的效果和指定“-type”参数类似，差别在于它针对符号连接检查。</span><br></pre></td></tr></table></figure>
<h4 id="can-shu">参数</h4>
<p>起始目录：查找文件的起始目录。</p>
<h4 id="shi-li">实例</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 当前目录搜索所有文件，文件内容 包含 “140.206.111.111” 的内容</span><br><span class="line">find . -type f -name &quot;*&quot; | xargs grep &quot;140.206.111.111&quot;</span><br><span class="line"></span><br><span class="line"># 列出当前目录及子目录下所有文件和文件夹</span><br><span class="line">find .</span><br><span class="line"></span><br><span class="line"># 在&#x2F;home目录下查找以.txt结尾的文件名</span><br><span class="line">find &#x2F;home -name &quot;*.txt&quot;</span><br><span class="line"></span><br><span class="line"># 在&#x2F;home目录下查找以.txt结尾的文件名 忽略大小写</span><br><span class="line">find &#x2F;home -iname &quot;*.txt&quot;</span><br><span class="line"></span><br><span class="line"># 当前目录及子目录下查找所有以.txt和.pdf结尾的文件</span><br><span class="line">find . \( -nam &quot;*.txt&quot; -o -name &quot;*.pdf&quot;\)  或 find . -name &quot;*.txt&quot; -o -name &quot;*.pdf&quot;</span><br><span class="line"></span><br><span class="line"># 匹配文件路径或者文件</span><br><span class="line">find &#x2F;usr&#x2F; -path &quot;*local*&quot;</span><br><span class="line"></span><br><span class="line"># 基于正则表达式匹配文件路径</span><br><span class="line">find . -regex &quot;.*\(\.txt\|\.pdf\)$&quot;</span><br><span class="line"></span><br><span class="line"># 基于正则表达式匹配文件路径(同上，但是忽略大小写)</span><br><span class="line">find . -iregex &quot;.*\(\.txt\|\.pdf\)$&quot;</span><br><span class="line"></span><br><span class="line"># 要列出所有长度为零的文件</span><br><span class="line">find . -empty</span><br><span class="line"></span><br><span class="line"># 如果你有一些以 jpg 结尾的目录呢？ 所以要给搜索加上文件类型</span><br><span class="line">find ~ \( -iname &#39;*jpeg&#39; -o -iname &#39;*jpg&#39; \) -type f </span><br><span class="line"></span><br><span class="line"># 在 log 目录下找到所有巨大的（定义为“大于 1GB”）文件：</span><br><span class="line">find &#x2F;var&#x2F;log -size +1G</span><br><span class="line"></span><br><span class="line"># 在主目录中找到对所有人可读的文件</span><br><span class="line">find ~ -perm -o&#x3D;r</span><br><span class="line"></span><br><span class="line"># 删除 mac 下自动生成的文件</span><br><span class="line">find .&#x2F; -name &#39;__MACOSX&#39; -depth -exec rm -rf &#123;&#125; \;</span><br><span class="line"></span><br><span class="line"># 统计代码行数</span><br><span class="line">find . -name &quot;*.java&quot;|xargs cat|grep -v ^$|wc -l # 代码行数统计, 排除空行</span><br></pre></td></tr></table></figure>
<h5 id="fou-ding-can-shu">否定参数</h5>
<figure class="highlight arduino"><table><tr><td class="code"><pre><span class="line"># 找出/<span class="built_in">home</span>下不是以.txt结尾的文件</span><br><span class="line"><span class="built_in">find</span> /<span class="built_in">home</span> ! -name <span class="string">"*.txt"</span></span><br></pre></td></tr></table></figure>
<h5 id="gen-ju-wen-jian-lei-xing-jin-xing-sou-suo">根据文件类型进行搜索</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find . -type 类型参数</span><br><span class="line"></span><br><span class="line">类型参数列表：</span><br><span class="line">    f 普通文件</span><br><span class="line">    l 符号连接</span><br><span class="line">    d 目录</span><br><span class="line">    c 字符设备</span><br><span class="line">    b 块设备</span><br><span class="line">    s 套接字</span><br><span class="line">    p Fifo</span><br></pre></td></tr></table></figure>
<h5 id="ji-yu-mu-lu-shen-du-sou-suo">基于目录深度搜索</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 向下最大深度限制为3</span><br><span class="line">find . -maxdepth 3 -type f</span><br><span class="line"></span><br><span class="line"># 搜索出深度距离当前目录至少2个子目录的所有文件</span><br><span class="line">find . -mindepth 2 -type f</span><br></pre></td></tr></table></figure>
<h5 id="gen-ju-wen-jian-shi-jian-chuo-jin-xing-sou-suo">根据文件时间戳进行搜索</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find . -type f 时间戳</span><br><span class="line"></span><br><span class="line">UNIX&#x2F;Linux文件系统每个文件都有三种时间戳：</span><br><span class="line">    访问时间 （-atime&#x2F;天，-amin&#x2F;分钟）：用户最近一次访问时间。</span><br><span class="line">    修改时间 （-mtime&#x2F;天，-mmin&#x2F;分钟）：文件最后一次修改时间。</span><br><span class="line">    变化时间 （-ctime&#x2F;天，-cmin&#x2F;分钟）：文件数据元（例如权限等）最后一次修改时间。</span><br><span class="line">    </span><br><span class="line"># 搜索最近七天内被访问过的所有文件</span><br><span class="line">find . -type f -atime -7</span><br><span class="line"></span><br><span class="line"># 搜索恰好在七天前被访问过的所有文件</span><br><span class="line">find . -type f -atime 7</span><br><span class="line"></span><br><span class="line"># 搜索超过七天内被访问过的所有文件</span><br><span class="line">find . -type f -atime +7</span><br><span class="line"></span><br><span class="line"># 搜索访问时间超过10分钟的所有文件</span><br><span class="line">find . -type f -amin +10</span><br><span class="line"></span><br><span class="line"># 找出比file.log修改时间更长的所有文件</span><br><span class="line">find . -type f -newer file.log</span><br></pre></td></tr></table></figure>
<h5 id="gen-ju-wen-jian-da-xiao-jin-xing-pi-pei">根据文件大小进行匹配</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find . -type f -size 文件大小单元</span><br><span class="line"></span><br><span class="line">文件大小单元：</span><br><span class="line">    b —— 块（512字节）</span><br><span class="line">    c —— 字节</span><br><span class="line">    w —— 字（2字节）</span><br><span class="line">    k —— 千字节</span><br><span class="line">    M —— 兆字节</span><br><span class="line">    G —— 吉字节</span><br><span class="line"></span><br><span class="line"># 搜索大于10KB的文件</span><br><span class="line">find . -type f -size +10k</span><br><span class="line"></span><br><span class="line"># 搜索小于10KB的文件</span><br><span class="line">find . -type f -size -10k</span><br><span class="line"></span><br><span class="line"># 搜索等于10KB的文件</span><br><span class="line">find . -type f -size 10k</span><br></pre></td></tr></table></figure>
<h5 id="shan-chu-pi-pei-wen-jian">删除匹配文件</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 删除当前目录下所有.txt文件</span><br><span class="line">find . -type f -name &quot;*.txt&quot; -delete</span><br></pre></td></tr></table></figure>
<h5 id="gen-ju-wen-jian-quan-xian-suo-you-quan-jin-xing-pi-pei">根据文件权限/所有权进行匹配</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 当前目录下搜索出权限为777的文件</span><br><span class="line">find . -type f -perm 777</span><br><span class="line"></span><br><span class="line"># 找出当前目录下权限不是644的php文件</span><br><span class="line">find . -type f -name &quot;*.php&quot; ! -perm 644</span><br><span class="line"></span><br><span class="line"># 找出当前目录用户tom拥有的所有文件</span><br><span class="line">find . -type f -user tom</span><br><span class="line"></span><br><span class="line"># 找出当前目录用户组sunk拥有的所有文件</span><br><span class="line">find . -type f -group sunk</span><br></pre></td></tr></table></figure>
<h5 id="jie-zhu-code-exec-code-xuan-xiang-yu-qi-ta-ming-ling-jie-he-shi-yong">借助<code>-exec</code>选项与其他命令结合使用</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 找出当前目录下所有root的文件，并把所有权更改为用户tom，&#123;&#125; 用于与 -exec 选项结合使用来匹配所有文件，然后会被替换为相应的文件名。</span><br><span class="line">find .-type f -user root -exec chown tom &#123;&#125; \;</span><br><span class="line"></span><br><span class="line"># 找出自己home目录下所有的.txt文件并删除， -ok 和 -exec 行为一样，不过它会给出提示，是否执行相应的操作。</span><br><span class="line">find $HOME&#x2F;. -name &quot;*.txt&quot; -ok rm &#123;&#125; \;</span><br><span class="line"></span><br><span class="line"># 查找当前目录下所有.txt文件并把他们拼接起来写入到all.txt文件中</span><br><span class="line">find . -type f -name &quot;*.txt&quot; -exec cat &#123;&#125; \;&gt; &#x2F;all.txt</span><br><span class="line"></span><br><span class="line"># 将30天前的.log文件移动到old目录中</span><br><span class="line">find . -type f -mtime +30 -name &quot;*.log&quot; -exec cp &#123;&#125; old \;</span><br><span class="line"></span><br><span class="line"># 找出当前目录下所有.txt文件并以“File:文件名”的形式打印出来</span><br><span class="line">find . -type f -name &quot;*.txt&quot; -exec printf &quot;File: %s\n&quot; &#123;&#125; \;</span><br><span class="line"></span><br><span class="line"># 因为单行命令中-exec参数中无法使用多个命令，以下方法可以实现在-exec之后接受多条命令</span><br><span class="line">-exec .&#x2F;text.sh &#123;&#125; \;</span><br></pre></td></tr></table></figure>
<h5 id="sou-suo-dan-shi-tiao-guo-zhi-ding-mu-lu">搜索但是跳过指定目录</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查找当前目录或者子目录下所有.txt文件，但是跳过子目录sk</span><br><span class="line">find . -path &quot;.&#x2F;sk&quot; -prune -o -name &quot;*.txt&quot; -print</span><br></pre></td></tr></table></figure>
<h3 id="wen-ben-nei-rong-sou-suo-grep">文本内容搜索:grep</h3>
<h4 id="shuo-ming-1">说明</h4>
<p><strong>grep</strong> （global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。用于过滤/搜索的特定字符。可使用正则表达式能多种命令配合使用。</p>
<h4 id="yu-fa-1">语法</h4>
<p>在文件中搜索一个单词，命令会返回一个包含 <strong>“match_pattern”</strong> 的文本行：</p>
<figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line"><span class="keyword">grep</span> match_pattern file_name</span><br><span class="line"><span class="keyword">grep</span> <span class="string">"match_pattern"</span> file_name</span><br></pre></td></tr></table></figure>
<h4 id="xuan-xiang-1">选项</h4>
<figure class="highlight haml"><table><tr><td class="code"><pre><span class="line">-<span class="ruby">a --text  <span class="comment"># 不要忽略二进制数据。</span></span></span><br><span class="line"><span class="ruby">-A &lt;显示行数&gt;   --after-context=&lt;显示行数&gt;   <span class="comment"># 除了显示符合范本样式的那一行之外，并显示该行之后的内容。</span></span></span><br><span class="line"><span class="ruby">-b --byte-offset                           <span class="comment"># 在显示符合范本样式的那一行之外，并显示该行之前的内容。</span></span></span><br><span class="line"><span class="ruby">-B&lt;显示行数&gt;   --before-context=&lt;显示行数&gt;   <span class="comment"># 除了显示符合样式的那一行之外，并显示该行之前的内容。</span></span></span><br><span class="line"><span class="ruby">-c --count    <span class="comment"># 计算符合范本样式的列数。</span></span></span><br><span class="line"><span class="ruby">-C&lt;显示行数&gt; --context=&lt;显示行数&gt;或-&lt;显示行数&gt; <span class="comment"># 除了显示符合范本样式的那一列之外，并显示该列之前后的内容。</span></span></span><br><span class="line"><span class="ruby">-d&lt;进行动作&gt; --directories=&lt;动作&gt;  <span class="comment"># 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep命令将回报信息并停止动作。</span></span></span><br><span class="line"><span class="ruby">-e&lt;范本样式&gt; --regexp=&lt;范本样式&gt;   <span class="comment"># 指定字符串作为查找文件内容的范本样式。</span></span></span><br><span class="line"><span class="ruby">-E --extended-regexp             <span class="comment"># 将范本样式为延伸的普通表示法来使用，意味着使用能使用扩展正则表达式。</span></span></span><br><span class="line"><span class="ruby">-f&lt;范本文件&gt; --file=&lt;规则文件&gt;     <span class="comment"># 指定范本文件，其内容有一个或多个范本样式，让grep查找符合范本条件的文件内容，格式为每一列的范本样式。</span></span></span><br><span class="line"><span class="ruby">-F --fixed-regexp   <span class="comment"># 将范本样式视为固定字符串的列表。</span></span></span><br><span class="line"><span class="ruby">-G --basic-regexp   <span class="comment"># 将范本样式视为普通的表示法来使用。</span></span></span><br><span class="line"><span class="ruby">-h --no-filename    <span class="comment"># 在显示符合范本样式的那一列之前，不标示该列所属的文件名称。</span></span></span><br><span class="line"><span class="ruby">-H --with-filename  <span class="comment"># 在显示符合范本样式的那一列之前，标示该列的文件名称。</span></span></span><br><span class="line"><span class="ruby">-i --ignore-<span class="keyword">case</span>    <span class="comment"># 忽略字符大小写的差别。</span></span></span><br><span class="line"><span class="ruby">-l --file-with-matches   <span class="comment"># 列出文件内容符合指定的范本样式的文件名称。</span></span></span><br><span class="line"><span class="ruby">-L --files-without-match <span class="comment"># 列出文件内容不符合指定的范本样式的文件名称。</span></span></span><br><span class="line"><span class="ruby">-n --line-number         <span class="comment"># 在显示符合范本样式的那一列之前，标示出该列的编号。</span></span></span><br><span class="line"><span class="ruby">-P --perl-regexp         <span class="comment"># PATTERN 是一个 Perl 正则表达式</span></span></span><br><span class="line"><span class="ruby">-q --quiet或--silent     <span class="comment"># 不显示任何信息。</span></span></span><br><span class="line"><span class="ruby">-R/-r  --recursive       <span class="comment"># 此参数的效果和指定“-d recurse”参数相同。</span></span></span><br><span class="line"><span class="ruby">-s --no-messages  <span class="comment"># 不显示错误信息。</span></span></span><br><span class="line"><span class="ruby">-v --revert-match <span class="comment"># 反转查找。</span></span></span><br><span class="line"><span class="ruby">-V --version      <span class="comment"># 显示版本信息。   </span></span></span><br><span class="line"><span class="ruby">-w --word-regexp  <span class="comment"># 只显示全字符合的列。</span></span></span><br><span class="line"><span class="ruby">-x --line-regexp  <span class="comment"># 只显示全列符合的列。</span></span></span><br><span class="line"><span class="ruby">-y <span class="comment"># 此参数效果跟“-i”相同。</span></span></span><br><span class="line"><span class="ruby">-o <span class="comment"># 只输出文件中匹配到的部分。</span></span></span><br><span class="line"><span class="ruby">-m &lt;num&gt; --max-count=&lt;num&gt; <span class="comment"># 找到num行结果后停止查找，用来限制匹配行数</span></span></span><br></pre></td></tr></table></figure>
<h4 id="zheng-ze-biao-da-shi">正则表达式</h4>
<figure class="highlight autoit"><table><tr><td class="code"><pre><span class="line">^    <span class="meta"># 锚定行的开始 如：<span class="string">'^grep'</span>匹配所有以grep开头的行。    </span></span><br><span class="line">$    <span class="meta"># 锚定行的结束 如：<span class="string">'grep$'</span> 匹配所有以grep结尾的行。</span></span><br><span class="line">.    <span class="meta"># 匹配一个非换行符的字符 如：<span class="string">'gr.p'</span>匹配gr后接一个任意字符，然后是p。    </span></span><br><span class="line">*    <span class="meta"># 匹配零个或多个先前字符 如：<span class="string">'*grep'</span>匹配所有一个或多个空格后紧跟grep的行。    </span></span><br><span class="line">.*   <span class="meta"># 一起用代表任意字符。   </span></span><br><span class="line">[]   <span class="meta"># 匹配一个指定范围内的字符，如<span class="string">'[Gg]rep'</span>匹配Grep和grep。    </span></span><br><span class="line">[^]  <span class="meta"># 匹配一个不在指定范围内的字符，如：<span class="string">'[^A-FH-Z]rep'</span>匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。    </span></span><br><span class="line">\(..\)  <span class="meta"># 标记匹配字符，如<span class="string">'\(love\)'</span>，love被标记为1。    </span></span><br><span class="line">\&lt;      <span class="meta"># 锚定单词的开始，如:<span class="string">'\&lt;grep'</span>匹配包含以grep开头的单词的行。    </span></span><br><span class="line">\&gt;      <span class="meta"># 锚定单词的结束，如<span class="string">'grep\&gt;'</span>匹配包含以grep结尾的单词的行。    </span></span><br><span class="line">x\&#123;m\&#125;  <span class="meta"># 重复字符x，m次，如：<span class="string">'0\&#123;5\&#125;'</span>匹配包含5个o的行。    </span></span><br><span class="line">x\&#123;m,\&#125;   <span class="meta"># 重复字符x,至少m次，如：<span class="string">'o\&#123;5,\&#125;'</span>匹配至少有5个o的行。    </span></span><br><span class="line">x\&#123;m,n\&#125;  <span class="meta"># 重复字符x，至少m次，不多于n次，如：<span class="string">'o\&#123;5,10\&#125;'</span>匹配5--10个o的行。   </span></span><br><span class="line">\w    <span class="meta"># 匹配文字和数字字符，也就是[A-Za-z0-9]，如：<span class="string">'G\w*p'</span>匹配以G后跟零个或多个文字或数字字符，然后是p。   </span></span><br><span class="line">\W    <span class="meta"># \w的反置形式，匹配一个或多个非单词字符，如点号句号等。   </span></span><br><span class="line">\b    <span class="meta"># 单词锁定符，如: <span class="string">'\bgrep\b'</span>只匹配grep。</span></span><br></pre></td></tr></table></figure>
<h4 id="shi-li-1">实例</h4>
<figure class="highlight vala"><table><tr><td class="code"><pre><span class="line"><span class="meta"># 在多个文件中查找：</span></span><br><span class="line">grep <span class="string">"match_pattern"</span> file_1 file_2 file_3 ...</span><br><span class="line"></span><br><span class="line"><span class="meta">#  -v 选项：输出除match_pattern之外的所有行</span></span><br><span class="line">grep -v <span class="string">"match_pattern"</span> file_name</span><br><span class="line"></span><br><span class="line"><span class="meta"># 标记匹配颜色 --color=auto 选项：</span></span><br><span class="line">grep <span class="string">"match_pattern"</span> file_name --color=auto</span><br><span class="line"></span><br><span class="line"><span class="meta"># 使用正则表达式 -E 选项：</span></span><br><span class="line">grep -E <span class="string">"[1-9]+"</span>  or  egrep <span class="string">"[1-9]+"</span></span><br><span class="line">echo <span class="keyword">this</span> is a test line. | grep -o -E <span class="string">"[a-z]+\."</span> 输出：line.</span><br><span class="line">echo <span class="keyword">this</span> is a test line. | egrep -o <span class="string">"[a-z]+\."</span> 输出：line.</span><br><span class="line"></span><br><span class="line"><span class="meta"># 使用正则表达式 -P 选项：</span></span><br><span class="line">grep -P <span class="string">"(\d&#123;3&#125;\-)&#123;2&#125;\d&#123;4&#125;"</span> file_name</span><br><span class="line"></span><br><span class="line"><span class="meta"># -c 选项：统计文件或者文本中包含匹配字符串的行数 </span></span><br><span class="line">grep -c <span class="string">"text"</span> file_name</span><br><span class="line"></span><br><span class="line"><span class="meta"># -n 选项：输出包含匹配字符串的行数 </span></span><br><span class="line">grep <span class="string">"text"</span> -n file_name   or  cat file_name | grep <span class="string">"text"</span> -n</span><br><span class="line"><span class="meta"># 多个文件</span></span><br><span class="line">grep <span class="string">"text"</span> -n file_1 file_2</span><br><span class="line"></span><br><span class="line"><span class="meta"># 打印样式匹配所位于的字符或字节偏移：</span></span><br><span class="line">echo gun is not unix | grep -b -o <span class="string">"not"</span>  输出：<span class="number">7</span>:not</span><br><span class="line"><span class="meta"># 一行中字符串的字符偏移是从该行的第一个字符开始计算，起始值为0。选项  -b -o  一般总是配合使用。</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 搜索多个文件并查找匹配文本在哪些文件中：</span></span><br><span class="line">grep -l <span class="string">"text"</span> file1 file2 file3...</span><br></pre></td></tr></table></figure>
<h4 id="di-gui-sou-suo">递归搜索</h4>
<figure class="highlight perl"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在多级目录中对文本进行递归搜索, .表示当前目录。</span></span><br><span class="line"><span class="keyword">grep</span> <span class="string">"text"</span> . -r -n</span><br></pre></td></tr></table></figure>
<h4 id="hu-lue-da-xiao-xie">忽略大小写</h4>
<figure class="highlight 1c"><table><tr><td class="code"><pre><span class="line"><span class="meta"># 忽略匹配样式中的字符大小写：</span></span><br><span class="line">echo <span class="string">"hello world"</span> <span class="string">| grep -i "</span>HELLO<span class="string">"   输出:hello</span></span><br></pre></td></tr></table></figure>
<h4 id="duo-ge-pi-pei-yang-shi">多个匹配样式</h4>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -e:自动多个匹配样式</span></span><br><span class="line">echo this is a text line | grep -e <span class="string">"is"</span> -e <span class="string">"line"</span> -o  输出:is line</span><br><span class="line"></span><br><span class="line"><span class="comment"># -f:自动匹配多个样式</span></span><br><span class="line">cat patfile</span><br><span class="line">aaa</span><br><span class="line">bbb</span><br><span class="line"></span><br><span class="line">echo<span class="built_in"> aaa </span>bbb ccc ddd eee | grep -f patfile -o</span><br></pre></td></tr></table></figure>
<h4 id="zai-grep-sou-suo-jie-guo-zhong-bao-gua-huo-zhe-pai-chu-zhi-ding-wen-jian">在grep搜索结果中包括或者排除指定文件</h4>
<figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line"># 只在目录中所有的.php和.html文件中递归搜索字符<span class="string">"main()"</span></span><br><span class="line"><span class="keyword">grep</span> <span class="string">"main()"</span> . -r --<span class="keyword">include</span> *.&#123;php,html&#125;</span><br><span class="line"></span><br><span class="line"># 在搜索结果中排除所有README文件</span><br><span class="line"><span class="keyword">grep</span> <span class="string">"main()"</span> . -r --<span class="keyword">exclude</span> <span class="string">"README"</span></span><br><span class="line"></span><br><span class="line"># 在搜索结果中排除filelist文件列表里的文件</span><br><span class="line"><span class="keyword">grep</span> <span class="string">"main()"</span> . -r --<span class="keyword">exclude</span>-<span class="keyword">from</span> filelist</span><br></pre></td></tr></table></figure>
<h4 id="shi-yong-0-zhi-zi-jie-hou-zhui">使用0值字节后缀</h4>
<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line"># 测试文件：</span><br><span class="line"><span class="keyword">echo</span> <span class="string">"aaa"</span> &gt; file1</span><br><span class="line"><span class="keyword">echo</span> <span class="string">"bbb"</span> &gt; file2</span><br><span class="line"><span class="keyword">echo</span> <span class="string">"aaa"</span> &gt; file3</span><br><span class="line"></span><br><span class="line"><span class="keyword">grep</span> <span class="string">"aaa"</span> <span class="keyword">file</span>* -lZ | xargs -<span class="number">0</span> rm</span><br><span class="line"></span><br><span class="line"># 执行后会删除file1和file3，<span class="keyword">grep</span>输出用-Z选项来指定以<span class="number">0</span>值字节作为终结符文件名（\<span class="number">0</span>），xargs -<span class="number">0</span> 读取输入并用<span class="number">0</span>值字节终结符分隔文件名，然后删除匹配文件，-Z通常和-<span class="keyword">l</span>结合使用。</span><br></pre></td></tr></table></figure>
<h4 id="jing-mo-shu-chu">静默输出</h4>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line"># 不会输出任何信息，如果命令运行成功返回<span class="number">0</span>，失败则返回非<span class="number">0</span>值。一般用于条件测试。</span><br><span class="line">grep -q <span class="string">"test"</span> filename</span><br></pre></td></tr></table></figure>
<h4 id="da-yin-chu-pi-pei-wen-ben-zhi-qian-huo-zhe-zhi-hou-de-xing">打印出匹配文本之前或者之后的行</h4>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line"># 显示匹配某个结果之后的<span class="number">3</span>行，使用 -A 选项：</span><br><span class="line">seq <span class="number">10</span> | grep <span class="string">"5"</span> -A <span class="number">3</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">8</span></span><br><span class="line"></span><br><span class="line"># 显示匹配某个结果之前的<span class="number">3</span>行，使用 -B 选项：</span><br><span class="line">seq <span class="number">10</span> | grep <span class="string">"5"</span> -B <span class="number">3</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"></span><br><span class="line"># 显示匹配某个结果的前三行和后三行，使用 -C 选项：</span><br><span class="line">seq <span class="number">10</span> | grep <span class="string">"5"</span> -C <span class="number">3</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">8</span></span><br><span class="line"></span><br><span class="line"># 如果匹配结果有多个，会用“--”作为各匹配结果之间的分隔符：</span><br><span class="line">echo -e <span class="string">"a<span class="subst">\n</span>b<span class="subst">\n</span>c<span class="subst">\n</span>a<span class="subst">\n</span>b<span class="subst">\n</span>c"</span> | grep a -A <span class="number">1</span></span><br><span class="line">a</span><br><span class="line">b</span><br><span class="line">--</span><br><span class="line">a</span><br><span class="line">b</span><br></pre></td></tr></table></figure>
<h3 id="wen-ben-bian-ji-sed">文本编辑:sed</h3>
<h4 id="shuo-ming-2">说明</h4>
<p><strong>sed</strong> 是一种流编辑器，它是文本处理中非常重要的工具，能够完美的配合正则表达式使用，功能不同凡响。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有 改变，除非你使用重定向存储输出。Sed主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等。</p>
<h4 id="yu-fa-2">语法</h4>
<figure class="highlight gams"><table><tr><td class="code"><pre><span class="line">sed [<span class="keyword">options</span>] <span class="string">'command'</span> <span class="keyword">file</span>(s)</span><br><span class="line">sed [<span class="keyword">options</span>] -f scriptfile <span class="keyword">file</span>(s)</span><br></pre></td></tr></table></figure>
<h4 id="xuan-xiang-2">选项</h4>
<figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line">-e&lt;<span class="keyword">script</span>&gt;或<span class="comment">--expression=&lt;script&gt;：以选项中的指定的script来处理输入的文本文件；</span></span><br><span class="line">-f&lt;<span class="keyword">script</span>文件&gt;或<span class="comment">--file=&lt;script文件&gt;：以选项中指定的script文件来处理输入的文本文件；</span></span><br><span class="line">-h或<span class="comment">--help：显示帮助；</span></span><br><span class="line">-n或<span class="comment">--quiet或——silent：仅显示script处理后的结果；</span></span><br><span class="line">-V或<span class="comment">--version：显示版本信息。</span></span><br></pre></td></tr></table></figure>
<h4 id="sed-ming-ling">sed 命令</h4>
<figure class="highlight clean"><table><tr><td class="code"><pre><span class="line">a\ # 在当前行下面插入文本。</span><br><span class="line">i\ # 在当前行上面插入文本。</span><br><span class="line">c\ # 把选定的行改为新的文本。</span><br><span class="line">d # 删除，删除选择的行。</span><br><span class="line">D # 删除模板块的第一行。</span><br><span class="line">s # 替换指定字符</span><br><span class="line">h # 拷贝模板块的内容到内存中的缓冲区。</span><br><span class="line">H # 追加模板块的内容到内存中的缓冲区。</span><br><span class="line">g # 获得内存缓冲区的内容，并替代当前模板块中的文本。</span><br><span class="line">G # 获得内存缓冲区的内容，并追加到当前模板块文本的后面。</span><br><span class="line">l # 列表不能打印字符的清单。</span><br><span class="line">n # 读取下一个输入行，用下一个命令处理新的行而不是用第一个命令。</span><br><span class="line">N # 追加下一个输入行到模板块后面并在二者间嵌入一个新行，改变当前行号码。</span><br><span class="line">p # 打印模板块的行。</span><br><span class="line">P # (大写) 打印模板块的第一行。</span><br><span class="line">q # 退出Sed。</span><br><span class="line">b lable # 分支到脚本中带有标记的地方，如果分支不存在则分支到脚本的末尾。</span><br><span class="line">r file # 从file中读行。</span><br><span class="line">t label # <span class="keyword">if</span>分支，从最后一行开始，条件一旦满足或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。</span><br><span class="line">T label # 错误分支，从最后一行开始，一旦发生错误或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。</span><br><span class="line">w file # 写并追加模板块到file末尾。  </span><br><span class="line">W file # 写并追加模板块的第一行到file末尾。  </span><br><span class="line">! # 表示后面的命令对所有没有被选定的行发生作用。  </span><br><span class="line">= # 打印当前行号码。  </span><br><span class="line"># # 把注释扩展到下一个换行符以前。  </span><br><span class="line"></span><br><span class="line"># ##############################替换标记###################################</span><br><span class="line">g # 表示行内全面替换。  </span><br><span class="line">p # 表示打印行。  </span><br><span class="line">w # 表示把行写入一个文件。  </span><br><span class="line">x # 表示互换模板块中的文本和缓冲区中的文本。  </span><br><span class="line">y # 表示把一个字符翻译为另外的字符（但是不用于正则表达式）</span><br><span class="line">\<span class="number">1</span> # 子串匹配标记</span><br><span class="line">&amp; # 已匹配字符串标记</span><br><span class="line"></span><br><span class="line"># ##############################sed 正则###################################</span><br><span class="line">^ # 匹配行开始，如：/^sed/匹配所有以sed开头的行。</span><br><span class="line">$ # 匹配行结束，如：/sed$/匹配所有以sed结尾的行。</span><br><span class="line">. # 匹配一个非换行符的任意字符，如：/s.d/匹配s后接一个任意字符，最后是d。</span><br><span class="line">* # 匹配<span class="number">0</span>个或多个字符，如：<span class="comment">/*sed/匹配所有模板是一个或多个空格后紧跟sed的行。</span></span><br><span class="line"><span class="comment">[] # 匹配一个指定范围内的字符，如/[ss]ed/匹配sed和Sed。  </span></span><br><span class="line"><span class="comment">[^] # 匹配一个不在指定范围内的字符，如：/[^A-RT-Z]ed/匹配不包含A-R和T-Z的一个字母开头，紧跟ed的行。</span></span><br><span class="line"><span class="comment">\(..\) # 匹配子串，保存匹配的字符，如s/\(love\)able/\1rs，loveable被替换成lovers。</span></span><br><span class="line"><span class="comment">&amp; # 保存搜索字符用来替换其他字符，如s/love/ **&amp;** /，love这成 **love** 。</span></span><br><span class="line"><span class="comment">\&lt; # 匹配单词的开始，如:/\&lt;love/匹配包含以love开头的单词的行。</span></span><br><span class="line"><span class="comment">\&gt; # 匹配单词的结束，如/love\&gt;/匹配包含以love结尾的单词的行。</span></span><br><span class="line"><span class="comment">x\&#123;m\&#125; # 重复字符x，m次，如：/0\&#123;5\&#125;/匹配包含5个0的行。</span></span><br><span class="line"><span class="comment">x\&#123;m,\&#125; # 重复字符x，至少m次，如：/0\&#123;5,\&#125;/匹配至少有5个0的行。</span></span><br><span class="line"><span class="comment">x\&#123;m,n\&#125; # 重复字符x，至少m次，不多于n次，如：/0\&#123;5,10\&#125;/匹配5~10个0的行。</span></span><br></pre></td></tr></table></figure>
<h4 id="yong-fa-shi-li">用法实例</h4>
<h5 id="ti-huan-cao-zuo-s-ming-ling">替换操作：s命令</h5>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 替换文本中的字符串：</span></span><br><span class="line">sed <span class="string">'s/book/books/'</span> <span class="built_in">file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -n选项 和 p命令 一起使用表示只打印那些发生替换的行：</span></span><br><span class="line">sed -n <span class="string">'s/test/TEST/p'</span> <span class="built_in">file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接编辑文件 选项-i ，会匹配file文件中每一行的所有book替换为books：</span></span><br><span class="line">sed -i <span class="string">'s/book/books/g'</span> <span class="built_in">file</span></span><br></pre></td></tr></table></figure>
<h5 id="quan-mian-ti-huan-biao-ji-g">全面替换标记g</h5>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用后缀 /g 标记会替换每一行中的所有匹配：</span></span><br><span class="line">sed <span class="string">'s/book/books/g'</span> file</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当需要从第N处匹配开始替换时，可以使用 /Ng：</span></span><br><span class="line"><span class="built_in">echo</span> sksksksksksk | sed <span class="string">'s/sk/SK/2g'</span></span><br><span class="line">skSKSKSKSKSK</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> sksksksksksk | sed <span class="string">'s/sk/SK/3g'</span></span><br><span class="line">skskSKSKSKSK</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> sksksksksksk | sed <span class="string">'s/sk/SK/4g'</span></span><br><span class="line">skskskSKSKSK</span><br></pre></td></tr></table></figure>
<h5 id="ding-jie-fu">定界符</h5>
<figure class="highlight sml"><table><tr><td class="code"><pre><span class="line"> #  / 在sed中作为定界符使用，也可以使用任意的定界符</span><br><span class="line"> sed <span class="symbol">'s</span>:test:<span class="type">TEXT</span>:g'</span><br><span class="line">sed <span class="symbol">'s</span>|test|<span class="type">TEXT</span>|g'</span><br><span class="line"></span><br><span class="line"># 定界符出现在样式内部时，需要进行转义：</span><br><span class="line">sed <span class="symbol">'s</span>/\/bin/\/usr\/<span class="keyword">local</span>\/bin/g'</span><br></pre></td></tr></table></figure>
<h5 id="shan-chu-cao-zuo-d-ming-ling">删除操作：d命令</h5>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 删除空白行：</span></span><br><span class="line">sed <span class="string">'/^$/d'</span> <span class="built_in">file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件的第2行：</span></span><br><span class="line">sed <span class="string">'2d'</span> <span class="built_in">file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件的第2行到末尾所有行：</span></span><br><span class="line">sed <span class="string">'2,$d'</span> <span class="built_in">file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件最后一行：</span></span><br><span class="line">sed <span class="string">'$d'</span> <span class="built_in">file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件中所有开头是test的行：</span></span><br><span class="line">sed <span class="string">'/^test/'</span>d <span class="built_in">file</span></span><br></pre></td></tr></table></figure>
<h5 id="yi-pi-pei-zi-fu-chuan-biao-ji-amp">已匹配字符串标记&amp;</h5>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"># 正则表达式 \w+ 匹配每一个单词，使用 [&amp;] 替换它，&amp; 对应于之前所匹配到的单词</span><br><span class="line">echo <span class="keyword">this</span> <span class="keyword">is</span> a test line | sed <span class="string">'s/\w\+/[&amp;]/g'</span></span><br><span class="line">[<span class="keyword">this</span>] [<span class="keyword">is</span>] [a] [test] [line]</span><br><span class="line"></span><br><span class="line"># 所有以<span class="number">192.168</span><span class="number">.0</span><span class="number">.1</span>开头的行都会被替换成它自已加localhost：</span><br><span class="line">sed <span class="string">'s/^192.168.0.1/&amp;localhost/'</span> file</span><br><span class="line"><span class="number">192.168</span><span class="number">.0</span><span class="number">.1</span>localhost</span><br></pre></td></tr></table></figure>
<h5 id="zi-chuan-pi-pei-biao-ji-1">子串匹配标记\1</h5>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 匹配给定样式的其中一部分：命令中 digit 7，被替换成了 7。样式匹配到的子串是 7，(..) 用于匹配子串，对于匹配到的第一个子串就标记为 \1 ，依此类推匹配到的第二个结果就是 \2</span></span><br><span class="line">echo this is digit 7 <span class="keyword">in</span> a number | sed <span class="string">'s/digit \([0-9]\)/\1/'</span></span><br><span class="line">this is 7 <span class="keyword">in</span> a number</span><br><span class="line"></span><br><span class="line">echo<span class="built_in"> aaa </span>BBB | sed <span class="string">'s/\([a-z]\+\) \([A-Z]\+\)/\2 \1/'</span></span><br><span class="line">BBB aaa</span><br><span class="line"></span><br><span class="line"><span class="comment"># love被标记为1，所有loveable会被替换成lovers，并打印出来：</span></span><br><span class="line">sed -n <span class="string">'s/\(love\)able/\1rs/p'</span> file</span><br></pre></td></tr></table></figure>
<h5 id="zu-he-duo-ge-biao-da-shi">组合多个表达式</h5>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="attribute">sed</span> <span class="string">'表达式'</span> | sed <span class="string">'表达式'</span></span><br><span class="line"></span><br><span class="line">等价于：</span><br><span class="line"></span><br><span class="line">sed <span class="string">'表达式; 表达式'</span></span><br></pre></td></tr></table></figure>
<h5 id="yin-yong">引用</h5>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sed表达式可以使用单引号来引用，但是如果表达式内部包含变量字符串，就需要使用双引号。</span></span><br><span class="line"><span class="built_in">test</span>=hello</span><br><span class="line"><span class="built_in">echo</span> hello WORLD | sed <span class="string">"s/<span class="variable">$test</span>/HELLO"</span></span><br><span class="line">HELLO WORLD</span><br></pre></td></tr></table></figure>
<h5 id="xuan-ding-xing-de-fan-wei-dou-hao">选定行的范围：,（逗号）</h5>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 所有在模板test和check所确定的范围内的行都被打印</span></span><br><span class="line">sed -n '/<span class="keyword">test</span>/,/check/p' <span class="keyword">file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印从第5行开始到第一个包含以test开始的行之间的所有行：</span></span><br><span class="line">sed -n '<span class="number">5</span>,/^<span class="keyword">test</span>/p' <span class="keyword">file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于模板test和west之间的行，每行的末尾用字符串aaa bbb替换：</span></span><br><span class="line">sed '/<span class="keyword">test</span>/,/west/s/$/aaa bbb/' <span class="keyword">file</span></span><br></pre></td></tr></table></figure>
<h5 id="duo-dian-bian-ji-e-ming-ling">多点编辑：e命令</h5>
<figure class="highlight mel"><table><tr><td class="code"><pre><span class="line"># -e选项允许在同一行里执行多条命令：sed表达式的第一条命令删除<span class="number">1</span>至<span class="number">5</span>行，第二条命令用check替换test。命令的执行顺序对结果有影响。如果两个命令都是替换命令，那么第一个替换命令将影响第二个替换命令的结果。</span><br><span class="line">sed -e <span class="string">'1,5d'</span> -e <span class="string">'s/test/check/'</span> <span class="keyword">file</span></span><br><span class="line"></span><br><span class="line"># 和 -e 等价的命令是 --<span class="keyword">expression</span>：</span><br><span class="line">sed --<span class="keyword">expression</span>=<span class="string">'s/test/check/'</span> --<span class="keyword">expression</span>=<span class="string">'/love/d'</span> <span class="keyword">file</span></span><br><span class="line">从文件读入：r命令</span><br></pre></td></tr></table></figure>
<h5 id="cong-wen-jian-du-ru-r-ming-ling">从文件读入：r命令</h5>
<figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"># <span class="keyword">file</span>里的内容被读进来，显示在与<span class="keyword">test</span>匹配的行后面，如果匹配多行，则<span class="keyword">file</span>的内容将显示在所有匹配行的下面：</span><br><span class="line">sed '/<span class="keyword">test</span>/r <span class="keyword">file</span>' filename</span><br></pre></td></tr></table></figure>
<h5 id="xie-ru-wen-jian-w-ming-ling">写入文件：w命令</h5>
<figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"># 在example中所有包含<span class="keyword">test</span>的行都被写入<span class="keyword">file</span>里：</span><br><span class="line">sed -<span class="keyword">n</span> '/<span class="keyword">test</span>/w <span class="keyword">file</span>' example</span><br></pre></td></tr></table></figure>
<h5 id="zhui-jia-xing-xia-a-ming-ling">追加（行下）：a\命令</h5>
<figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"># 将 this is a <span class="keyword">test</span> <span class="keyword">line</span> 追加到 以<span class="keyword">test</span> 开头的行后面：</span><br><span class="line">sed '/^<span class="keyword">test</span>/a\this is a <span class="keyword">test</span> <span class="keyword">line</span>' <span class="keyword">file</span></span><br><span class="line"></span><br><span class="line"># 在 <span class="keyword">test</span>.<span class="keyword">conf</span> 文件第2行之后插入 this is a <span class="keyword">test</span> <span class="keyword">line</span>：</span><br><span class="line">sed -i '2a\this is a <span class="keyword">test</span> <span class="keyword">line</span>' <span class="keyword">test</span>.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure>
<h5 id="cha-ru-xing-shang-i-ming-ling">插入（行上）：i\命令</h5>
<figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"># 将 this is a <span class="keyword">test</span> <span class="keyword">line</span> 追加到以<span class="keyword">test</span>开头的行前面</span><br><span class="line">sed '/^<span class="keyword">test</span>/i\this is a <span class="keyword">test</span> <span class="keyword">line</span>' <span class="keyword">file</span></span><br><span class="line"></span><br><span class="line"># 在<span class="keyword">test</span>.<span class="keyword">conf</span>文件第5行之前插入this is a <span class="keyword">test</span> <span class="keyword">line</span>：</span><br><span class="line">sed -i '5i\this is a <span class="keyword">test</span> <span class="keyword">line</span>' <span class="keyword">test</span>.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure>
<h5 id="xia-yi-ge-n-ming-ling">下一个：n命令</h5>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果test被匹配，则移动到匹配行的下一行，替换这一行的aa，变为bb，并打印该行，然后继续：</span></span><br><span class="line">sed '/<span class="keyword">test</span>/&#123; n; s/aa/bb/; &#125;' <span class="keyword">file</span></span><br></pre></td></tr></table></figure>
<h5 id="bian-xing-y-ming-ling">变形：y命令</h5>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line"># 把<span class="number">1</span>~<span class="number">10</span>行内所有abcde转变为大写，注意，正则表达式元字符不能使用这个命令：</span><br><span class="line">sed '<span class="number">1</span>,<span class="number">10</span>y/abcde/ABCDE/' file</span><br></pre></td></tr></table></figure>
<h5 id="tui-chu-q-ming-ling">退出：q命令</h5>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打印完第10行后，退出sed</span></span><br><span class="line">sed <span class="string">'10q'</span> <span class="built_in">file</span></span><br></pre></td></tr></table></figure>
<h5 id="bao-chi-he-huo-qu-h-ming-ling-he-g-ming-ling">保持和获取：h命令和G命令</h5>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在sed处理文件的时候，每一行都被保存在一个叫模式空间的临时缓冲区中，除非行被删除或者输出被取消，否则所有被处理的行都将 打印在屏幕上。接着模式空间被清空，并存入新的一行等待处理。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 匹配test的行被找到后，将存入模式空间，h命令将其复制并存入一个称为保持缓存区的特殊缓冲区内。第二条语句的意思是，当到达最后一行后，G命令取出保持缓冲区的行，然后把它放回模式空间中，且追加到现在已经存在于模式空间中的行的末尾。在这个例子中就是追加到最后一行。简单来说，任何包含test的行都被复制并追加到该文件的末尾。</span></span><br><span class="line"><span class="attribute">sed</span> -e <span class="string">'/test/h'</span> -e <span class="string">'<span class="variable">$G</span>'</span> file</span><br></pre></td></tr></table></figure>
<h5 id="bao-chi-he-hu-huan-h-ming-ling-he-x-ming-ling">保持和互换：h命令和x命令</h5>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 互换模式空间和保持缓冲区的内容。也就是把包含test与check的行互换：</span></span><br><span class="line">sed -e <span class="string">'/test/h'</span> -e <span class="string">'/check/x'</span> <span class="built_in">file</span></span><br></pre></td></tr></table></figure>
<h5 id="da-yin-qi-shu-xing-huo-ou-shu-xing">打印奇数行或偶数行</h5>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="attribute">sed</span> -n <span class="string">'p;n'</span> test.txt  <span class="comment">#奇数行</span></span><br><span class="line">sed -n <span class="string">'n;p'</span> test.txt  <span class="comment">#偶数行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">sed -n <span class="string">'1~2p'</span> test.txt  <span class="comment">#奇数行</span></span><br><span class="line">sed -n <span class="string">'2~2p'</span> test.txt  <span class="comment">#偶数行</span></span><br></pre></td></tr></table></figure>
<h5 id="da-yin-pi-pei-zi-fu-chuan-de-xia-yi-xing">打印匹配字符串的下一行</h5>
<figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line"><span class="keyword">grep</span> -A <span class="number">1</span> SCC URFILE</span><br><span class="line">sed -n <span class="string">'/SCC/&#123;n;p&#125;'</span> URFILE</span><br><span class="line">awk <span class="string">'/SCC/&#123;getline; print&#125;'</span> URFILE</span><br></pre></td></tr></table></figure>
<h3 id="guan-dao">管道</h3>
<ol>
<li>| 表示管道，表示管道左边命令的结果传给管道右边。eg: ls -l | more、 ps aux | grep xxx</li>
<li>命令1 | 命令2 | 命令3</li>
</ol>
<h3 id="wen-ben-chu-li">文本处理</h3>
<ol>
<li>cut
<ol>
<li>-d：指定字段分割符，默认是空格
<ol>
<li>-d&quot;,&quot;</li>
</ol>
</li>
<li>-f： 指定要显示的字段
<ol>
<li>-f 1,3</li>
<li>-f 1-3</li>
</ol>
</li>
</ol>
</li>
<li>sort：文本排序
<ol>
<li>-n：数值排序</li>
<li>-r：降序</li>
<li>-t：字段分隔符</li>
<li>-k：以哪个字段为关键字进行排序</li>
<li>-u：排序后相同的行只显示一次</li>
<li>-f：排序时忽略字符大小写</li>
</ol>
</li>
<li>join</li>
<li>sed</li>
<li>awk</li>
</ol>
<h2 id="xi-tong-guan-li-ming-ling">系统管理命令</h2>
<ol>
<li>ps 命令可以查看进程的详细状况
<ol>
<li>-a 显示终端上所有进程，包括其他用户的进程</li>
<li>-u 显示进程的详细状态</li>
<li>-x 显示没有控制终端的进程</li>
<li>-w 显示加宽，以便显示更多的信息</li>
<li>-r 只显示正在运行的进程</li>
</ol>
</li>
<li>top 命令用来动态显示运行中的进程，可以在使用top命令时加上-d来指定显示信息更新的时间间隔。在top命令下，通过按下下列字母，来对显示结果进行排序
<ol>
<li>m 根据内存使用量来排序</li>
<li>p  根据cpu占有率来排序</li>
<li>t   根据进程运行时间的长短来排序</li>
<li>u  可以根据后面输入的用户名来筛选进程</li>
<li>k  可以根据后面输入的pid来杀死进程</li>
<li>q  退出</li>
<li>h  获得帮助</li>
</ol>
</li>
<li>kill ，killall
<ol>
<li>kill -9 pid</li>
<li>killall 进程名</li>
</ol>
</li>
</ol>
<h2 id="ci-pan-kong-jian-ming-ling">磁盘空间命令</h2>
<ol>
<li>df 命令用于检测文件系统的磁盘空间占用和剩余情况，可以显示所有文件系统对节点和磁盘块的使用情况
<ol>
<li>a 显示所有文件系统的磁盘使用情况</li>
<li>m 以1024字节为单位显示</li>
<li>t 显示各指定文件系统的磁盘空间使用情况</li>
<li>T 显示文件系统</li>
</ol>
</li>
<li>du 命令用于统计目录或文件所占磁盘空间的大小，该命令的执行结果与df类似，du更侧重于磁盘的使用情况。 格式： du [选项] 目录或文件名
<ol>
<li>a 递归显示指定目录中各文件和子目录中文件占用的数据块</li>
<li>s 显示指定文件或目录占用的数据块</li>
<li>b 以字节为单位显示磁盘占用情况</li>
<li>l 计算所有文件大小，对硬链接文件计算多次</li>
</ol>
</li>
</ol>
<h2 id="wang-luo-ming-ling">网络命令</h2>
<h3 id="cha-kan-huo-pei-zhi-wang-qia-xin-xi-ifconfig">查看或配置网卡信息：ifconfig</h3>
<h3 id="ce-shi-yuan-cheng-zhu-ji-lian-tong-xing-ping">测试远程主机连通性：ping</h3>
<ol>
<li>ping -c4 <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a></li>
</ol>
<h3 id="cha-kan-wang-luo-qing-kuang-netstat-ntpl">查看网络情况： netstat -ntpl</h3>
<h1 id="ruan-jian-an-zhuang-he-guan-li">软件安装和管理</h1>
<h2 id="ruan-jian-bao">软件包</h2>
<ol>
<li>bin文件（适合所有Linux发行版），是可执行的文件</li>
<li>rpm包，yum（redhat系列）
<ol>
<li>rpm命令：安装过程中不需要指定安装路径，rpm文件在制作的时候已经确定了安装路径</li>
<li>查询软件安装路径：rpm -ql xxx</li>
</ol>
</li>
<li>源码压缩包（适合所有的Linux发行版）</li>
<li>官方已经编译好的，下载软件包直接可以使用</li>
<li>安装步骤：
<ol>
<li>检查是否已经安装：rpm -qa | grep jdk</li>
<li>下载软件包</li>
<li>安装依赖</li>
</ol>
</li>
</ol>
<h2 id="yum">yum</h2>
<ol>
<li>解决rpm下载问题</li>
<li>解决rpm文件的查询问题</li>
<li>解决rpm安装问题</li>
<li>解决rpm的依赖问题</li>
</ol>
]]></content>
      <categories>
        <category>技术/linux</category>
      </categories>
      <tags>
        <tag>速查</tag>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title>Vim速查</title>
    <url>/2018/05/29/Vim%E9%80%9F%E6%9F%A5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="vim-visual-interface">vim（Visual Interface）</h2>
<p><img src="/2018/05/29/Vim%E9%80%9F%E6%9F%A5/NSFileHandle.png" alt="avatar"></p>
<a id="more"></a>
<h3 id="ming-ling-mo-shi">命令模式</h3>
<ol>
<li>h：左移</li>
<li>j：下移</li>
<li>k：上移</li>
<li>l(L)：右移</li>
<li><code>^</code> 或 <code>0</code>：光标移动到当前行的行首。</li>
<li><code>1$</code>：光标移动到当前行的行末，<code>2$</code>:光标移动到下一行的行末,…。</li>
<li>M：光标移动到中间行</li>
<li>L：光标移动到屏幕最后一行行首</li>
<li>G：移动到指定行，行号 -G，只有G的话，到文件最末尾</li>
<li>gg：文件第一个字符</li>
<li>w:向后一次移动一个字</li>
<li>b：向前一次移动一个字</li>
<li>{：按段移动，上移</li>
<li>}：按段下移，下移</li>
<li>ctrl+d：向下翻半屏</li>
<li>ctrl+u：向上翻半屏</li>
<li>ctrl+f：向下翻一屏</li>
<li>ctrl+b：向上翻一屏</li>
</ol>
<h4 id="ke-shi-mo-shi">可视模式</h4>
<ol>
<li>v:按字符移动，结合h,j,k,l选中文本内容。</li>
<li>V：按行移动，选中文本可视模式，可以配合d，y，&gt;&gt;,&lt;&lt;实现对文本块的删除，复制，左右移动。</li>
</ol>
<h4 id="shan-chu-ming-ling">删除命令</h4>
<ol>
<li>x：删除光标后一个字符，n x 删除光标后的n个字符</li>
<li>X：删除光标前一个字符，相当于Backspace</li>
<li>dd：删除光标所在行，n dd删除指定的n行</li>
<li>D：删除光标后本行所有内容，包含光标所在字符</li>
<li>d0：删除光标前本行所有内容，不包含光标所在字符</li>
<li>dw：删除光标开始位置的字，包含光标所在字符</li>
</ol>
<h4 id="che-xiao-ming-ling">撤销命令</h4>
<ol>
<li>u：一步一步撤销</li>
<li>ctrl+r：反撤销</li>
</ol>
<h4 id="zhong-fu-ming-ling">重复命令</h4>
<ol>
<li>. ：重复上一次操作的命令</li>
</ol>
<h4 id="wen-ben-yi-dong">文本移动</h4>
<ol>
<li>&gt;&gt;:文本行右移</li>
<li>&lt;&lt;:文本行左移</li>
</ol>
<h4 id="fu-zhi-nian-tie">复制粘贴</h4>
<ol>
<li>yy：复制当前行，n yy复制n行</li>
<li>在末行模式，输入：a,by 复制从第a行开始，到第b行结束的内容</li>
<li>p：在光标坐在位置向下新开辟一行，粘贴</li>
<li>d：剪切选中内容</li>
</ol>
<h4 id="cha-zhao-ming-ling">查找命令</h4>
<ol>
<li>/str : 查找str，从光标所在行往下查找</li>
<li>？str：查找str，从光标所在行往上查找</li>
<li>n：查找下一个</li>
<li>N：查找上一个</li>
</ol>
<h4 id="ti-huan-cao-zuo">替换操作</h4>
<ol>
<li>r：替换当前字符</li>
<li>R替换当前行光标后的字符</li>
</ol>
<h4 id="ti-huan-ming-ling">替换命令</h4>
<ol>
<li>末行模式下，将光标所在行的abc替换成123，：s/abc/123/g</li>
<li>末行模式下，将第一行到第10行之间的abc替换成123，:1,10s/abc/123/g</li>
<li>末行模式下，把文件中的abc全部替换成123，:%s/abc/123</li>
</ol>
<h3 id="shu-ru-mo-shi">输入模式</h3>
<ol>
<li>i：插入光标前一个字符</li>
<li>I：插入行首</li>
<li>a：插入光标后一个字符</li>
<li>A: 插入行末</li>
<li>o：向下新开一行，插入行首</li>
<li>O：向上新开一行，插入行首</li>
</ol>
<h3 id="mo-xing-mo-shi">末行模式</h3>
<ol>
<li>：set nu 显示行号</li>
<li>! shell命令</li>
<li>wq：保存退出</li>
<li>ZZ： 保存退出</li>
<li>q！：不保存退出</li>
</ol>
<h3 id="bu-chong">补充</h3>
<p>修改用户目录下的vimrc（~/.vimrc）文件，修改vim配置</p>
<ol>
<li>set nu :设置每次vim 打开文件显示行号。</li>
<li>set ts=4:设置tab键每次4个空格</li>
</ol>
]]></content>
      <categories>
        <category>soft skill</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Vim</tag>
        <tag>速查</tag>
      </tags>
  </entry>
  <entry>
    <title>latex公式速查</title>
    <url>/2018/05/10/latex%E5%85%AC%E5%BC%8F%E9%80%9F%E6%9F%A5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="shang-xia-biao">上下标</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>x_{2}</td>
<td>\(x_{2}\)</td>
</tr>
<tr>
<td>x^2</td>
<td>\(x^{2}\)</td>
</tr>
</tbody>
</table>
<a id="more"></a>
<h3 id="gong-shi-xu-hao">公式序号</h3>
<p>\[
y = wx+b \tag{1.1}
\]</p>
<h3 id="gua-hao">括号</h3>
<ol>
<li>
<p>() [] 直接写就行，而 {} 则需要转义</p>
<ol>
<li>y \in {3, 4, 5} : \(y \in \{3, 4, 5\}\)</li>
</ol>
</li>
<li>
<p>有时候括号需要大号的，此时需要使用\left和\right加大括号的大小。</p>
<ol>
<li>\left(\frac {x} {y} \right)^2 : \(\left(\frac {x} {y} \right)^2\)</li>
</ol>
</li>
<li>
<p>\left 和 \right必须成对出现，对于不显示的一边可以使用 . 代替</p>
</li>
<li>
<p>\left. \frac{du}{dx} \right| _{x=0} : \(\left. \frac{du}{dx} \right| _{x=0}\)</p>
</li>
<li>
<p>双括号\(\left\|g_{k}\right\|\)</p>
</li>
<li>
<p>左大括号：</p>
<p><code>\left\{\begin{array}{ll}  x=\frac{3 \pi}{2}(1+2 t) \cos \left(\frac{3 \pi}{2}(1+2 t)\right) &amp; \\  y=s, &amp; 0 \leq s \leq L,|t| \leq 1 \\  z=\frac{3 \pi}{2}(1+2 t) \sin \left(\frac{3 \pi}{2}(1+2 t)\right)  \end{array}\right.</code><br>
\[
 \left\{\begin{array}{ll}
 x=\frac{3 \pi}{2}(1+2 t) \cos \left(\frac{3 \pi}{2}(1+2 t)\right) &amp; \\
 y=s, &amp; 0 \leq s \leq L,|t| \leq 1 \\
 z=\frac{3 \pi}{2}(1+2 t) \sin \left(\frac{3 \pi}{2}(1+2 t)\right)
 \end{array}\right.
 \]</p>
</li>
</ol>
<h3 id="fen-shu">分数</h3>
<p>两种写法：</p>
<table>
<thead>
<tr>
<th style="text-align:center">命令</th>
<th style="text-align:center">显示</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">\frac{1}{2x+1}</td>
<td style="text-align:center">\(\frac{1}{2x+1}\)</td>
</tr>
</tbody>
</table>
<h3 id="kai-fang">开方</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\sqrt[n]{a}</td>
<td>\(\sqrt[n]{a}\)</td>
</tr>
</tbody>
</table>
<h3 id="dui-shu">对数</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\log</td>
<td>\(\log\)</td>
</tr>
<tr>
<td>\lg</td>
<td>\(\lg\)</td>
</tr>
<tr>
<td>\ln</td>
<td>\(\ln\)</td>
</tr>
</tbody>
</table>
<h3 id="xiang-liang">向量</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\vec{a}</td>
<td>\(\vec{a}\)</td>
</tr>
</tbody>
</table>
<h3 id="san-jiao-yun-suan">三角运算</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\bot</td>
<td>\(\bot\)</td>
<td></td>
<td>\angle</td>
<td>\(\angle\)</td>
</tr>
<tr>
<td>\sin</td>
<td>\(\sin\)</td>
<td></td>
<td>\cos</td>
<td>\(\cos\)</td>
</tr>
<tr>
<td>\tan</td>
<td>\(\tan\)</td>
<td></td>
<td>\cot</td>
<td>\(\cot\)</td>
</tr>
<tr>
<td>\sec</td>
<td>\(\sec\)</td>
<td></td>
<td>\csc</td>
<td>\(\csc\)</td>
</tr>
</tbody>
</table>
<h3 id="sheng-lue-hao">省略号</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\ldots</td>
<td>\(\ldots\)</td>
</tr>
<tr>
<td>\cdots</td>
<td>\(\cdots\)</td>
</tr>
<tr>
<td>\cdot</td>
<td>\(\cdot\)</td>
</tr>
</tbody>
</table>
<h3 id="xu-yao-zhuan-yi-de-zi-fu">需要转义的字符</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\#</td>
<td>\(\#\)</td>
<td></td>
<td>\_</td>
<td>\(\_\)</td>
</tr>
<tr>
<td>\$</td>
<td>$$$</td>
<td></td>
<td>\%</td>
<td>\(\%\)</td>
</tr>
<tr>
<td>\&amp;</td>
<td>\(\&amp;\)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\{</td>
<td>\(\{\)</td>
<td></td>
<td>\}</td>
<td>\(\}\)</td>
</tr>
</tbody>
</table>
<h3 id="pu-tong-fu-hao">普通符号</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\pm</td>
<td>\(\pm\)</td>
<td></td>
<td>\times</td>
<td>\(\times\)</td>
</tr>
<tr>
<td>\div</td>
<td>\(\div\)</td>
<td></td>
<td>\mid</td>
<td>\(\mid\)</td>
</tr>
<tr>
<td>\cdot</td>
<td>\(\cdot\)</td>
<td></td>
<td>\circ</td>
<td>\(\circ\)</td>
</tr>
<tr>
<td>\ast</td>
<td>\(\ast\)</td>
<td></td>
<td>\bigodot</td>
<td>\(\bigodot\)</td>
</tr>
<tr>
<td>\bigotimes</td>
<td>\(\bigotimes\)</td>
<td></td>
<td>\equiv</td>
<td>\(\equiv\)</td>
</tr>
<tr>
<td>\leq or \le</td>
<td>\(\leq\)</td>
<td></td>
<td>\geq or \ge</td>
<td>\(\geq\)</td>
</tr>
<tr>
<td>\ll</td>
<td>\(\ll\)</td>
<td></td>
<td>\gg</td>
<td>\(\gg\)</td>
</tr>
<tr>
<td>\neq or  \ne</td>
<td>\(\neq\)</td>
<td></td>
<td>\approx</td>
<td>\(\approx\)</td>
</tr>
<tr>
<td>\succeq</td>
<td>\(\succeq\)</td>
<td></td>
<td>\preceq</td>
<td>\(\preceq\)</td>
</tr>
<tr>
<td>\doteq</td>
<td>\(\doteq\)</td>
<td></td>
<td>\cong</td>
<td>\(\cong\)</td>
</tr>
<tr>
<td>\sim</td>
<td>\(\sim\)</td>
<td></td>
<td>\simeq</td>
<td>\(\simeq\)</td>
</tr>
<tr>
<td>\vdash</td>
<td>\(\vdash\)</td>
<td></td>
<td>\dashv</td>
<td>\(\dashv\)</td>
</tr>
<tr>
<td>\models</td>
<td>\(\models\)</td>
<td></td>
<td>\perp</td>
<td>\(\perp\)</td>
</tr>
<tr>
<td>\smile</td>
<td>\(\smile\)</td>
<td></td>
<td>\frown</td>
<td>\(\frown\)</td>
</tr>
<tr>
<td>\saymp</td>
<td>\(\asymp\)</td>
<td></td>
<td>:</td>
<td>\(\:\)</td>
</tr>
</tbody>
</table>
<h3 id="xi-la-zi-mu">希腊字母</h3>
<p>大写：开头字母大写<br>
斜体：命令前面加var</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th>大写</th>
<th>斜体</th>
<th></th>
<th>命令</th>
<th>显示</th>
<th>大写</th>
<th>斜体</th>
</tr>
</thead>
<tbody>
<tr>
<td>\alpha</td>
<td>\(\alpha\)</td>
<td>A</td>
<td></td>
<td></td>
<td>\beta</td>
<td>\(\beta\)</td>
<td>B</td>
<td></td>
</tr>
<tr>
<td>\gamma</td>
<td>\(\gamma\)</td>
<td>\(\Gamma\)</td>
<td>\(\varGamma\)</td>
<td></td>
<td>\delta</td>
<td>\(\delta\)</td>
<td>\(\Delta\)</td>
<td>\(\varDelta\)</td>
</tr>
<tr>
<td>\epsilon</td>
<td>\(\epsilon\)</td>
<td>E</td>
<td>\(\varepsilon\)</td>
<td></td>
<td>\zeta</td>
<td>\(\zeta\)</td>
<td>Z</td>
<td></td>
</tr>
<tr>
<td>\eta</td>
<td>\(\eta\)</td>
<td>H</td>
<td></td>
<td></td>
<td>\theta</td>
<td>\(\theta\)</td>
<td>\(\Theta\)</td>
<td>\(\varTheta\)</td>
</tr>
<tr>
<td>\iota</td>
<td>\(\iota\)</td>
<td>I</td>
<td></td>
<td></td>
<td>\kappa</td>
<td>\(\kappa\)</td>
<td>K</td>
<td></td>
</tr>
<tr>
<td>\lambda</td>
<td>\(\lambda\)</td>
<td>\(\Lambda\)</td>
<td>\(\varLambda\)</td>
<td></td>
<td>\mu</td>
<td>\(\mu\)</td>
<td>M</td>
<td></td>
</tr>
<tr>
<td>\xi</td>
<td>\(\xi\)</td>
<td>\(\Xi\)</td>
<td>\(\varXi\)</td>
<td></td>
<td>\nu</td>
<td>\(\nu\)</td>
<td>N</td>
<td></td>
</tr>
<tr>
<td>\pi</td>
<td>\(\pi\)</td>
<td>\(\Pi\)</td>
<td>\(\varPi\)</td>
<td></td>
<td>\rho</td>
<td>\(\rho\)</td>
<td>P</td>
<td>\(\varrho\)</td>
</tr>
<tr>
<td>\sigma</td>
<td>\(\sigma\)</td>
<td>\(\Sigma\)</td>
<td>\(\varSigma\)</td>
<td></td>
<td>\tau</td>
<td>\(\tau\)</td>
<td>T</td>
<td></td>
</tr>
<tr>
<td>\upsilon</td>
<td>\(\upsilon\)</td>
<td>\(\Upsilon\)</td>
<td>\(\varUpsilon\)</td>
<td></td>
<td>\phi</td>
<td>\(\phi\)</td>
<td>\(\Phi\)</td>
<td>\(\varPhi\)</td>
</tr>
<tr>
<td>\chi</td>
<td>\(\chi\)</td>
<td>X</td>
<td></td>
<td></td>
<td>\psi</td>
<td>\(\psi\)</td>
<td>\(\Psi\)</td>
<td>\(\varPsi\)</td>
</tr>
<tr>
<td>\omega</td>
<td>\(\omega\)</td>
<td>\(\Omega\)</td>
<td>\(\varOmega\)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="/2018/05/10/latex%E5%85%AC%E5%BC%8F%E9%80%9F%E6%9F%A5/NSFileHandle.png" alt="5f979c551b937e468a1ed99ff62134d4"></p>
<h3 id="ji-he-yun-suan">集合运算</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\emptyset</td>
<td>\(\emptyset\)</td>
<td></td>
<td>\propto</td>
<td>\(\propto\)</td>
</tr>
<tr>
<td>\notin</td>
<td>\(\notin\)</td>
<td></td>
<td>\Join or \bowtie</td>
<td>\(\Join\)</td>
</tr>
<tr>
<td>\supset</td>
<td>\(\supset\)</td>
<td></td>
<td>\subseteq</td>
<td>\(\subseteq\)</td>
</tr>
<tr>
<td>\subset</td>
<td>\(\subset\)</td>
<td></td>
<td>\supseteq</td>
<td>\(\supseteq\)</td>
</tr>
<tr>
<td>\sqsubset</td>
<td>\(\sqsubset\)</td>
<td></td>
<td>\sqsupseteq</td>
<td>\(\sqsupseteq\)</td>
</tr>
<tr>
<td>\bigcap</td>
<td>\(\bigcap\)</td>
<td></td>
<td>\bigcup</td>
<td>\(\bigcup\)</td>
</tr>
<tr>
<td>\bigvee</td>
<td>\(\bigvee\)</td>
<td></td>
<td>\bigwedge</td>
<td>\(\bigwedge\)</td>
</tr>
<tr>
<td>\biguplus</td>
<td>\(\biguplus\)</td>
<td></td>
<td>\bigsqcup</td>
<td>\(\bigsqcup\)</td>
</tr>
<tr>
<td>\in</td>
<td>\(\in\)</td>
<td></td>
<td>\ni or \owns</td>
<td>\(\owns\)</td>
</tr>
</tbody>
</table>
<h3 id="luo-ji-yun-suan">逻辑运算</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\because</td>
<td>\(\because\)</td>
<td></td>
<td>\therefore</td>
<td>\(\therefore\)</td>
</tr>
<tr>
<td>\forall</td>
<td>\(\forall\)</td>
<td></td>
<td>\exists</td>
<td>\(\exists\)</td>
</tr>
</tbody>
</table>
<h3 id="wei-fen">微分</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>y{\prime}x</td>
<td>\(y{\prime}x\)</td>
<td></td>
<td>\int</td>
<td>\(\int\)</td>
</tr>
<tr>
<td>\iint</td>
<td>\(\iint\)</td>
<td></td>
<td>\iiint</td>
<td>\(\iiint\)</td>
</tr>
<tr>
<td>\oint</td>
<td>\(oint\)</td>
<td></td>
<td>\lim</td>
<td>\(lim\)</td>
</tr>
<tr>
<td>\infty</td>
<td>\(\infty\)</td>
<td></td>
<td>\nabla</td>
<td>\(\nabla\)</td>
</tr>
<tr>
<td>\partial</td>
<td>\(\partial\)</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="jian-tou">箭头</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\uparrow</td>
<td>\(\uparrow\)</td>
<td></td>
<td>\downarrow</td>
<td>\(\downarrow\)</td>
</tr>
<tr>
<td>\Uparrow</td>
<td>\(\Uparrow\)</td>
<td></td>
<td>\Downarrow</td>
<td>\(\Downarrow\)</td>
</tr>
<tr>
<td>\leftarrow</td>
<td>\(\leftarrow\)</td>
<td></td>
<td>\rightarrow</td>
<td>\(\rightarrow\)</td>
</tr>
<tr>
<td>\Leftarrow</td>
<td>\(\Leftarrow\)</td>
<td></td>
<td>\Rightarrow</td>
<td>\(\Rightarrow\)</td>
</tr>
<tr>
<td>\longleftarrow</td>
<td>\(\longleftarrow\)</td>
<td></td>
<td>\longrightarrow</td>
<td>\(\longrightarrow\)</td>
</tr>
<tr>
<td>\Longrightarrow</td>
<td>\(\Longrightarrow\)</td>
<td></td>
<td>\Longrightarrow</td>
<td>\(\Longrightarrow\)</td>
</tr>
</tbody>
</table>
<h3 id="qiu-he-lian-cheng-he-ji-fen-hao">求和、连乘和积分号</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th>使用</th>
<th>例子</th>
</tr>
</thead>
<tbody>
<tr>
<td>\sum</td>
<td>\(\sum\)</td>
<td>\sum_{i=1}^{N}</td>
<td>\(\sum_{i=1}^{N}\)</td>
</tr>
<tr>
<td>\prod</td>
<td>\(\prod\)</td>
<td>\prod_{i=1}^{N}</td>
<td>\(\prod_{i=1}^{N}\)</td>
</tr>
<tr>
<td>\coprod</td>
<td>\(\coprod\)</td>
<td>\coprod_{i=1}^{N}</td>
<td>\(\coprod_{i=1}^{N}\)</td>
</tr>
<tr>
<td>\int</td>
<td>\(\int\)</td>
<td>\int_{a}^{b}</td>
<td>\(\int_{a}^{b}\)</td>
</tr>
<tr>
<td>\iint</td>
<td>\(\iint\)</td>
<td>\iint_{a}^{b}</td>
<td>\(\iint_{a}^{b}\)</td>
</tr>
<tr>
<td>\bigcup</td>
<td>\(\bigcup\)</td>
<td>\bigcup_{i=1}^{N}</td>
<td>\(\bigcup_{i=1}^{N}\)</td>
</tr>
<tr>
<td>\bigcap</td>
<td>\(\bigcap\)</td>
<td>\bigcap_{i=1}^{N}</td>
<td>\(\bigcap_{i=1}^{N}\)</td>
</tr>
<tr>
<td>\lim_{n\rightarrow+\infty}</td>
<td>\(lim_{n\rightarrow+\infty}\)</td>
<td>\lim_{n\rightarrow+\infty}\frac{1}{n(n+1)}</td>
<td>\(\lim_{n\rightarrow+\infty}\frac{1}{n(n+1)}\)</td>
</tr>
</tbody>
</table>
<h3 id="shu-zi-mo-shi-zhong-yin-fu">数字模式重音符</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\hat{a}</td>
<td>\(\hat{a}\)</td>
<td></td>
<td>\dot{a}</td>
<td>\(\dot{a}\)</td>
</tr>
<tr>
<td>\grave{a}</td>
<td>\(\grave{a}\)</td>
<td></td>
<td>\acute{a}</td>
<td>\(\acute{a}\)</td>
</tr>
<tr>
<td>\bar{a}</td>
<td>\(\bar{a}\)</td>
<td></td>
<td>\tilde{a}</td>
<td>\(\tilde{a}\)</td>
</tr>
<tr>
<td>\check{a}</td>
<td>\(\check{a}\)</td>
<td></td>
<td>\ddot{a}</td>
<td>\(\ddot{a}\)</td>
</tr>
<tr>
<td>\widehat{A}</td>
<td>\(\widehat{A}\)</td>
<td></td>
<td>\widetilde{A}</td>
<td>\(\widetilde{A}\)</td>
</tr>
<tr>
<td>\vec{a}</td>
<td>\(\vec{a}\)</td>
<td></td>
<td>\breve{a}</td>
<td>\(\breve{a}\)</td>
</tr>
</tbody>
</table>
<h3 id="lian-xian-fu-hao">连线符号</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\overline{a+b+c+d}</td>
<td>\(\overline{a+b+c+d}\)</td>
</tr>
<tr>
<td>\underline{a+b+c+d}</td>
<td>\(\underline{a+b+c+d}\)</td>
</tr>
<tr>
<td>\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}</td>
<td>\(\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}\)</td>
</tr>
</tbody>
</table>
<h3 id="ju-zhen">矩阵</h3>
<p>命令</p>
<p>\left|\begin{array}{cccc}
     1 &amp; 6 &amp; 9 \\\ 
     7 &amp; 9 &amp; 0 \\\
     9 &amp; 5 &amp; 0
\end{array}\right|</p>
<p>显示：<br>
\[
\left|\begin{array}{cccc}
     1 &amp; 6 &amp; 9 \\ 
     7 &amp; 9 &amp; 0 \\
     9 &amp; 5 &amp; 0
\end{array}\right|
\]</p>
<h3 id="gong-ju">工具</h3>
<p><a href="https://mathpix.com/" target="_blank" rel="noopener">https://mathpix.com/</a></p>
<p><a href="https://houmin.cc/posts/fedfc052/" target="_blank" rel="noopener">https://houmin.cc/posts/fedfc052/</a></p>
]]></content>
      <categories>
        <category>soft skill</category>
      </categories>
      <tags>
        <tag>速查</tag>
        <tag>latex</tag>
      </tags>
  </entry>
  <entry>
    <title>图论</title>
    <url>/2018/03/21/%E5%9B%BE%E8%AE%BA/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="wu-xiang-tu">无向图</h3>
<h4 id="ding-yi-wu-xiang-tu-shu-ju-jie-gou">定义无向图数据结构</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Graph</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        self.n = n</span><br><span class="line">        self.e = <span class="number">0</span></span><br><span class="line">        self.adj = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_edge</span><span class="params">(self, v, w)</span>:</span></span><br><span class="line">        self.adj[v].append(w)</span><br><span class="line">        self.adj[w].append(v)</span><br><span class="line">        self.e += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h4 id="wu-xiang-tu-pan-huan">无向图判环</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cycle</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, G)</span>:</span></span><br><span class="line">        self.vis = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(G.n)]</span><br><span class="line">        self.has_cycle = <span class="literal">False</span></span><br><span class="line">        self.G = G</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> range(G.n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.vis[s]:</span><br><span class="line">                self.dfs(s, s)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self, v, parent)</span>:</span></span><br><span class="line">        self.vis[v] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> self.G.adj[v]:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.vis[w]:</span><br><span class="line">                self.dfs(w, v)</span><br><span class="line">            <span class="comment"># 相邻节点已被访问过，同时还不是parent节点，则存在环</span></span><br><span class="line">            <span class="keyword">elif</span> w != parent:</span><br><span class="line">                self.has_cycle = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h4 id="wu-xiang-tu-shi-fou-wei-er-fen-tu">无向图是否为二分图</h4>
<p>二分图又叫二部图，是图论中的一种特殊模型。设G=(V,E)是一个无向图，如果顶点V可分割为两个互不相交的子集<code>(A,B)</code>，并且图中的每条边<code>（i，j）</code>所关联的两个顶点i和j分别属于这两个不同的顶点集<code>(i in A,j in B)</code>，则称图G为一个二分图。</p>
<p><img src="/2018/03/21/%E5%9B%BE%E8%AE%BA/%E4%BA%8C%E5%88%86%E5%9B%BE.png" alt="avatar"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoColor</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, G)</span>:</span></span><br><span class="line">        self.vis = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(G.n)]</span><br><span class="line">        self.color = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(G.n)]</span><br><span class="line">        self.G = G</span><br><span class="line">        self.is_two_colorable = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> range(G.n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.vis[s]:</span><br><span class="line">                self.dfs(s)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self, v)</span>:</span></span><br><span class="line">        self.vis[v] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> self.G.adj[v]:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.vis[w]:</span><br><span class="line">                self.color[w] = <span class="keyword">not</span> self.color[v]</span><br><span class="line">                self.dfs(w)</span><br><span class="line">            <span class="keyword">elif</span> self.color[w] == self.color[v]:</span><br><span class="line">                self.is_two_colorable = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="zui-xiao-sheng-cheng-shu">最小生成树</h3>
<h4 id="prime-suan-fa">prime 算法</h4>
<p><img src="/2018/03/21/%E5%9B%BE%E8%AE%BA/prime.gif" alt="avatar"></p>
<p>每次向最小生成树中加入权重最小的横切边，横切边是连接树与非树顶点的边。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prim</span><span class="params">(adj_matrix)</span>:</span></span><br><span class="line">    <span class="string">"""给定邻接矩阵，返回MST权值，返回-1表示图不连通"""</span></span><br><span class="line">    n = len(adj_matrix)  <span class="comment"># 顶点0~n-1</span></span><br><span class="line">    vis = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">    low_cut = [float(<span class="string">'inf'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]  <span class="comment"># 横切边的权重</span></span><br><span class="line"></span><br><span class="line">    ans = <span class="number">0</span></span><br><span class="line">    vis[<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        low_cut[i] = adj_matrix[<span class="number">0</span>][i]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        min_cut = float(<span class="string">'inf'</span>)</span><br><span class="line">        p = <span class="number">-1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[j] <span class="keyword">and</span> min_cut &gt; low_cut[j]:</span><br><span class="line">                min_cut = low_cut[j]</span><br><span class="line">                p = j</span><br><span class="line">        <span class="keyword">if</span> min_cut == float(<span class="string">'inf'</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>  <span class="comment"># 原图不连通</span></span><br><span class="line">        ans += min_cut</span><br><span class="line">        vis[p] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment"># 横切边更新为更小的权重</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[j] <span class="keyword">and</span> low_cut[j] &gt; adj_matrix[p][j]:</span><br><span class="line">                low_cut[j] = adj_matrix[p][j]</span><br><span class="line">    <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4 id="kuskal-suan-fa">Kuskal 算法</h4>
<p><img src="/2018/03/21/%E5%9B%BE%E8%AE%BA/Kuskal.png" alt="avatar"></p>
<p>将图中所有边按照从小到大的顺序加入最小生成树，加入的边不会与已加入的边构成换。(利用并查集UF判断连通性)，直到树中含有n-1条边为止。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kruskal</span><span class="params">(edges, n)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    求最小生成树权值</span></span><br><span class="line"><span class="string">    :param edges: [(u, v, w), ...]，三元组含义(顶点u，顶点v，边权重w)</span></span><br><span class="line"><span class="string">    :param n: 顶点数，顶点范围0~n-1</span></span><br><span class="line"><span class="string">    :return: 最小生成树权值（图不连通，返回-1）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    uf = UnionFind(n)  <span class="comment"># 并查集</span></span><br><span class="line">    edges.sort(key=<span class="keyword">lambda</span> item: item[<span class="number">2</span>])</span><br><span class="line">    edge_cnt = <span class="number">0</span></span><br><span class="line">    ans = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(edges)):</span><br><span class="line">        u, v, w = edges[i]</span><br><span class="line">        root_u, root_v = uf.find(u), uf.find(v)</span><br><span class="line">        <span class="keyword">if</span> root_u != root_v:</span><br><span class="line">            ans += w</span><br><span class="line">            uf.union(u, v)</span><br><span class="line">            edge_cnt += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> edge_cnt == n<span class="number">-1</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> edge_cnt &lt; n<span class="number">-1</span>:  <span class="comment"># 不连通</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">return</span> ans</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UnionFind</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""并查集类"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="string">"""长度为n的并查集"""</span></span><br><span class="line">        self.uf = [<span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n + <span class="number">1</span>)]    <span class="comment"># 列表0位置空出</span></span><br><span class="line">        self.sets_count = n                     <span class="comment"># 判断并查集里共有几个集合, 初始化默认互相独立</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># def find(self, p):</span></span><br><span class="line">    <span class="comment">#     """查找p的根结点(祖先)"""</span></span><br><span class="line">    <span class="comment">#     r = p                                   # 初始p</span></span><br><span class="line">    <span class="comment">#     while self.uf[p] &gt; 0:</span></span><br><span class="line">    <span class="comment">#         p = self.uf[p]</span></span><br><span class="line">    <span class="comment">#     while r != p:                           # 路径压缩, 把搜索下来的结点祖先全指向根结点</span></span><br><span class="line">    <span class="comment">#         self.uf[r], r = p, self.uf[r]</span></span><br><span class="line">    <span class="comment">#     return p</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># def find(self, p):</span></span><br><span class="line">    <span class="comment">#     while self.uf[p] &gt;= 0:</span></span><br><span class="line">    <span class="comment">#         p = self.uf[p]</span></span><br><span class="line">    <span class="comment">#     return p</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">(self, p)</span>:</span></span><br><span class="line">        <span class="string">"""尾递归"""</span></span><br><span class="line">        <span class="keyword">if</span> self.uf[p] &lt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> p</span><br><span class="line">        self.uf[p] = self.find(self.uf[p])</span><br><span class="line">        <span class="keyword">return</span> self.uf[p]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">union</span><span class="params">(self, p, q)</span>:</span></span><br><span class="line">        <span class="string">"""连通p,q 让q指向p"""</span></span><br><span class="line">        proot = self.find(p)</span><br><span class="line">        qroot = self.find(q)</span><br><span class="line">        <span class="keyword">if</span> proot == qroot:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">elif</span> self.uf[proot] &gt; self.uf[qroot]:   <span class="comment"># 负数比较, 左边规模更小</span></span><br><span class="line">            self.uf[qroot] += self.uf[proot]</span><br><span class="line">            self.uf[proot] = qroot</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.uf[proot] += self.uf[qroot]  <span class="comment"># 规模相加</span></span><br><span class="line">            self.uf[qroot] = proot</span><br><span class="line">        self.sets_count -= <span class="number">1</span>                    <span class="comment"># 连通后集合总数减一</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_connected</span><span class="params">(self, p, q)</span>:</span></span><br><span class="line">        <span class="string">"""判断pq是否已经连通"""</span></span><br><span class="line">        <span class="keyword">return</span> self.find(p) == self.find(q)     <span class="comment"># 即判断两个结点是否是属于同一个祖先</span></span><br></pre></td></tr></table></figure>
<h3 id="you-xiang-tu">有向图</h3>
<h4 id="ding-yi-wu-xiang-tu-de-shu-ju-jie-gou">定义无向图的数据结构</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiGraph</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        self.n = n</span><br><span class="line">        self.e = <span class="number">0</span></span><br><span class="line">        self.adj = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_edge</span><span class="params">(self, v, w)</span>:</span></span><br><span class="line">        self.adj[v].append(w)</span><br><span class="line">        self.e += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h4 id="you-xiang-tu-pan-huan">有向图判环</h4>
<p>对有向图使用dfs搜索，系统调用栈表示了当前遍历了的有向路径</p>
<p>若递归过程访问到出现在栈中的节点，则图中存在了环。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DirectedCycle</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, G)</span>:</span></span><br><span class="line">        self.on_stack = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(G.n)]</span><br><span class="line">        self.vis = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(G.n)]</span><br><span class="line">        self.has_cycle = <span class="literal">False</span></span><br><span class="line">        self.G = G</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> range(G.n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.vis[s]:</span><br><span class="line">                self.dfs(s)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self, v)</span>:</span></span><br><span class="line">        self.on_stack[v] = <span class="literal">True</span></span><br><span class="line">        self.vis[v] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> self.G.adj[v]:</span><br><span class="line">            <span class="keyword">if</span> self.has_cycle:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="keyword">not</span> self.vis[w]:</span><br><span class="line">                self.dfs(w)</span><br><span class="line">            <span class="keyword">elif</span> self.on_stack[w]:</span><br><span class="line">                self.has_cycle = <span class="literal">True</span></span><br><span class="line">        self.on_stack[v] = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h4 id="zong-jie">总结</h4>
<p>一个有向五环图的拓扑排序即为所有顶点dfs的逆后序排列</p>
<p>强连通性：有向图任意两点互相可达，则为强连通图。</p>
<p><strong>Kosaraju 算法</strong>：</p>
<ol>
<li>有向图G翻转得到\(G^R\)</li>
<li>dfs得到\(G^R\)中顶点的逆后序排列</li>
<li>按照这个你后序排列在G中执行标准dfs，每次递归调用所标记的顶点在同一强连通分量重</li>
</ol>
<p><strong>最短路径比较、总结</strong></p>
<table>
<thead>
<tr>
<th>算法</th>
<th>条件</th>
<th>平均</th>
<th>最坏</th>
<th>空间复杂度</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Dijkstra</td>
<td>边权为正</td>
<td>\(ElogV\)</td>
<td>\(ElogV\)</td>
<td>\(V\)</td>
<td></td>
</tr>
<tr>
<td>拓扑排序</td>
<td>无环</td>
<td>\(E\)</td>
<td>\(E+V\)</td>
<td>\(V\)</td>
<td>无环图最优算法</td>
</tr>
<tr>
<td>Bellman-Ford（基于队列）</td>
<td>无负权重</td>
<td>\(E+V\)</td>
<td>\(VE\)</td>
<td>\(V\)</td>
<td></td>
</tr>
</tbody>
</table>
<p>Dijkstra 算法：每次添加离起点最近的非树节点</p>
<p>prim：每次添加离树最近的非树节点</p>
<h4 id="dan-yuan-zui-duan-lu-jing-lin-jie-ju-zhen-xing-shi">单源最短路径，邻接矩阵形式</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dijkstra</span><span class="params">(adj_matrix, s)</span>:</span></span><br><span class="line">    <span class="string">"""给定邻接矩阵和起点s，返回s到所有点的最短路径"""</span></span><br><span class="line">    n = len(adj_matrix)  <span class="comment"># 顶点0~n-1</span></span><br><span class="line">    vis = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">    edge_to = [<span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]  <span class="comment"># 路径父节点</span></span><br><span class="line">    dist = [float(<span class="string">'inf'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]  <span class="comment"># 最短路径</span></span><br><span class="line"></span><br><span class="line">    dist[s] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        min_dist = float(<span class="string">'inf'</span>)</span><br><span class="line">        p = <span class="number">-1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[j] <span class="keyword">and</span> dist[j] &lt; min_dist:</span><br><span class="line">                min_dist = dist[j]</span><br><span class="line">                p = j</span><br><span class="line">        <span class="keyword">if</span> p == <span class="number">-1</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        vis[p] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[j] <span class="keyword">and</span> dist[p] + adj_matrix[p][j] &lt; dist[j]:</span><br><span class="line">                dist[j] = dist[p] + adj_matrix[p][j]</span><br><span class="line">                edge_to[j] = p</span><br><span class="line">    <span class="keyword">return</span> dist, edge_to</span><br></pre></td></tr></table></figure>
<p>单元最短路径，邻接表形式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dijkstra</span><span class="params">(adj, s)</span>:</span></span><br><span class="line">    n = len(adj)</span><br><span class="line">    dist = [float(<span class="string">'inf'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line"></span><br><span class="line">    dist[s] = <span class="number">0</span></span><br><span class="line">    pq = [(<span class="number">0</span>, s)]</span><br><span class="line">    <span class="keyword">while</span> pq:</span><br><span class="line">        <span class="comment"># u, v是顶点，d是距离，w是边权重</span></span><br><span class="line">        d, u = heappop(pq)</span><br><span class="line">        <span class="keyword">if</span> d &gt; dist[u]:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> v, w <span class="keyword">in</span> adj[u]:</span><br><span class="line">            <span class="keyword">if</span> d + w &lt; dist[v]:</span><br><span class="line">                dist[v] = d + w</span><br><span class="line">                heappush(pq, (dist[v], v))</span><br><span class="line">    <span class="keyword">return</span> dist</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>图论</tag>
      </tags>
  </entry>
  <entry>
    <title>字符串匹配问题</title>
    <url>/2018/03/14/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="zi-fu-chuan-pi-pei-wen-ti">字符串匹配问题</h2>
<p>给定两个字符串<code>S,P</code>,其中<code>S</code>串长度为<code>n</code>，<code>P</code>串长度为<code>m</code>, <code>(m&lt;=n)</code>，判断字符串<code>P</code>是否是<code>S</code>的子串</p>
<h4 id="fu-za-du">复杂度</h4>
<p>容易计算复杂度：<code>O(m*n)</code></p>
<h3 id="brute-force-jian-dan-pi-pei">Brute-Force(简单匹配)</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_match</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    m, n = len(s), len(p)</span><br><span class="line">    <span class="keyword">if</span> m-n &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m-n+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> s[i:i+n] == p:</span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="rabin-karp-suan-fa">Rabin-Karp算法</h3>
<h4 id="si-xiang">思想</h4>
<p>基本思想和暴力破解算法是一样的。也需要一个大小为<strong>m</strong>的窗口，但是不一样的是，不是直接比较两个长度为<strong>m</strong>的字符串，而是比较他们的哈希值。</p>
<h4 id="fu-za-du-fen-xi">复杂度分析</h4>
<p>一共会有(<strong>n-m+1</strong>)个窗口滑动，这一步的复杂度是<strong>O(n)</strong>。</p>
<p>计算哈希值:</p>
<p>假设现在窗口的起点在<strong>j</strong>这个位置，此时窗口内的字符串哈希值为，<br>
\[
H(S, j)=\sum_{i=0}^{m-1} \alpha^{m-(i+1)} \times \operatorname{char}\left(s_{i}\right)
\]<br>
那么，当计算下一个窗口的哈希值时，也就是当窗口的起点为<strong>j+1</strong>时，哈希函数值可由如下方法计算：<br>
\[
H(S, j+1)=\alpha\left(H(S, j)-\alpha^{m-1} \operatorname{char}\left(s_{j}\right)\right)+\operatorname{char}\left(s_{j+m}\right)
\]<br>
这样看来，在计算出第一个窗口的函数值之后，后面的每一个窗口哈希值都可以根据上述公式计算，只需要做一次减法，一次乘法，一次加法。之后的每一次哈希值计算都是<strong>O(1)<strong>的复杂度。所以计算第一个窗口的复杂度，<strong>O(m)</strong>，此后计算每一个窗口的复杂度</strong>O(1)</strong>，总的时间复杂度：<code>O(n+m)</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rabin_karp_match</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    n = len(s)</span><br><span class="line">    m = len(p)</span><br><span class="line">    h1 = hash(p)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, n-m+<span class="number">1</span>):</span><br><span class="line">        h2 = hash(s[i:i+m])</span><br><span class="line">        <span class="keyword">if</span> h1 != h2:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> s[i:i+m] == p:</span><br><span class="line">                <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<h3 id="kmp-suan-fa">KMP算法</h3>
<p>有点复杂，有时间再写上。</p>
<h4 id="si-xiang-1">思想</h4>
<h4 id="fu-za-du-fen-xi-1">复杂度分析</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># KMP算法：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmp_match</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    m, n = len(s), len(p)</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span> <span class="keyword">or</span> m - n &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    table = partial_table(p)</span><br><span class="line">    i, cur = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> i &lt; m-n+<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> p[cur] == s[i+cur]:</span><br><span class="line">            cur += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> cur == <span class="number">0</span>:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i += cur - table[cur<span class="number">-1</span>]  <span class="comment"># 移动位数=已匹配的字符数-对应的部分匹配值</span></span><br><span class="line">                cur = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> cur == n:</span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">partial_table</span><span class="params">(p)</span>:</span></span><br><span class="line">    m = len(p)</span><br><span class="line">    table = [<span class="number">0</span>] * m</span><br><span class="line">    k = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> q <span class="keyword">in</span> range(<span class="number">1</span>, m):</span><br><span class="line">        <span class="keyword">if</span> p[k] == p[q]:</span><br><span class="line">            k = k + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            k = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> p[k] == p[q]:</span><br><span class="line">                k = k + <span class="number">1</span></span><br><span class="line">        table[q] = k</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> table</span><br></pre></td></tr></table></figure>
<h3 id="horspool-suan-fa">Horspool算法</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shift_table</span><span class="params">(p)</span>:</span></span><br><span class="line">    <span class="comment"># 生成 Horspool 算法的移动表</span></span><br><span class="line">    <span class="comment"># 当前检测字符为c，模式长度为m</span></span><br><span class="line">    <span class="comment"># 如果当前c不包含在模式的前m-1个字符中，移动模式的长度m</span></span><br><span class="line">    <span class="comment"># 其他情况下移动最右边的的c到模式最后一个字符的距离</span></span><br><span class="line">    <span class="comment"># from collections import defaultdict</span></span><br><span class="line">    <span class="comment"># table = defaultdict(lambda: len(p))</span></span><br><span class="line">    table = dict()</span><br><span class="line">    <span class="comment"># print table</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, len(p) - <span class="number">1</span>):</span><br><span class="line">        table[p[index]] = len(p) - <span class="number">1</span> - index</span><br><span class="line">    <span class="comment"># print table</span></span><br><span class="line">    <span class="keyword">return</span> table</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">horspool_match</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    m, n = len(s), len(p)</span><br><span class="line">    table = shift_table(p)</span><br><span class="line">    index = len(p) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> index &lt;= m - <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># print("start matching at", index)</span></span><br><span class="line">        match_count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> match_count &lt; n <span class="keyword">and</span> p[n - <span class="number">1</span> - match_count] == s[index - match_count]:</span><br><span class="line">            match_count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> match_count == len(p):</span><br><span class="line">            <span class="keyword">return</span> index - match_count + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># print s[index], table.get(s[index])</span></span><br><span class="line">            <span class="keyword">if</span> table.get(s[index]) <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                table[s[index]] = n</span><br><span class="line">            index += table[s[index]]</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<h3 id="sunday-suan-fa">Sunday 算法</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sunday_match</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    char_pos = dict()</span><br><span class="line">    <span class="keyword">for</span> i, ch <span class="keyword">in</span> enumerate(p):</span><br><span class="line">        char_pos[ch] = i</span><br><span class="line">    <span class="comment"># print "char_pos:", char_pos</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    m, n = len(s), len(p)</span><br><span class="line">    <span class="keyword">while</span> i &lt;= m - n:</span><br><span class="line">        found = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> j, ch <span class="keyword">in</span> enumerate(p):</span><br><span class="line">            <span class="keyword">if</span> s[i + j] != ch:</span><br><span class="line">                found = <span class="literal">False</span></span><br><span class="line">                <span class="keyword">if</span> (i + n) &lt; m:</span><br><span class="line">                    <span class="keyword">if</span> s[i + n] <span class="keyword">not</span> <span class="keyword">in</span> char_pos:</span><br><span class="line">                        i += (n + <span class="number">1</span>)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        i += (n - char_pos[s[i + n]])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> found:</span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<h3 id="boyer-moore-suan-fa">Boyer_Moore算法</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">boyer_moore_search</span><span class="params">(string, des)</span>:</span></span><br><span class="line">    l = len(des) - <span class="number">1</span></span><br><span class="line">    strlen = len(string) - <span class="number">1</span></span><br><span class="line">    start, end = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> strlen &gt;= <span class="number">0</span>:</span><br><span class="line">        end = start + len(des)</span><br><span class="line">        <span class="comment"># print string[start:end]</span></span><br><span class="line">        cr = compare(string[start:end], des)</span><br><span class="line">        <span class="keyword">if</span> cr[<span class="number">0</span>] == <span class="number">-2</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'not found'</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> cr[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># return end - l</span></span><br><span class="line">            <span class="keyword">return</span> start</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pos = is_character_in(des, cr[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> cr[<span class="number">0</span>] == (len(des) - <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> pos != <span class="number">-1</span>:</span><br><span class="line">                    start += len(des) - <span class="number">1</span> - pos</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    start += len(des)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> pos == <span class="number">-1</span>:</span><br><span class="line">                    <span class="comment">#  have good  string</span></span><br><span class="line">                    goodPos = is_character_in(des, des[l])</span><br><span class="line">                    <span class="keyword">if</span> goodPos == l:</span><br><span class="line">                        start += l + <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        start += l - goodPos</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_character_in</span><span class="params">(s, c)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> pos <span class="keyword">in</span> range(len(s)):</span><br><span class="line">        <span class="keyword">if</span> s[pos] == c:</span><br><span class="line">            <span class="keyword">return</span> pos</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compare</span><span class="params">(str1, str2)</span>:</span></span><br><span class="line">    l1 = len(str1) - <span class="number">1</span></span><br><span class="line">    l2 = len(str2) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> l1 != l2:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-2</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> l1 &gt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> str1[l1] != str2[l1]:</span><br><span class="line">            <span class="keyword">return</span> l1, str1[l1]</span><br><span class="line">        l1 -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>, <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h3 id="bmhbnfs-suan-fa-kuai-su-pi-pei-suan-fa">BMHBNFS算法（快速匹配算法）</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_skip</span><span class="params">(p)</span>:</span></span><br><span class="line">    m = len(p)</span><br><span class="line">    a = p[m<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m<span class="number">-2</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">if</span> p[i] == a:</span><br><span class="line">            <span class="keyword">return</span> m<span class="number">-2</span>-i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bmhbnfs_find</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    <span class="comment"># find first occurrence of p in s</span></span><br><span class="line">    m = len(s)</span><br><span class="line">    n = len(p)</span><br><span class="line">    <span class="comment"># skip是把离p[m-1]最近且字符相同的字符移到m-1位需要跳过的步数-1</span></span><br><span class="line">    <span class="comment"># skip = delta1(p)[p[m - 1]]</span></span><br><span class="line">    skip = compute_skip(p)</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt;= m - n:</span><br><span class="line">        <span class="keyword">if</span> s[i + n - <span class="number">1</span>] == p[n - <span class="number">1</span>]:  <span class="comment"># (boyer-moore)</span></span><br><span class="line">            <span class="comment"># potential match</span></span><br><span class="line">            <span class="keyword">if</span> s[i:i + n - <span class="number">1</span>] == p[:n - <span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">return</span> i</span><br><span class="line">            <span class="keyword">if</span> s[i + n] <span class="keyword">not</span> <span class="keyword">in</span> p:</span><br><span class="line">                i = i + n + <span class="number">1</span>  <span class="comment"># (sunday)</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i = i + skip  <span class="comment"># (horspool)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># skip</span></span><br><span class="line">            <span class="keyword">if</span> s[i + n] <span class="keyword">not</span> <span class="keyword">in</span> p:</span><br><span class="line">                i = i + n + <span class="number">1</span>  <span class="comment"># (sunday)</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i = i + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>  <span class="comment"># not found</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>字符串匹配</tag>
      </tags>
  </entry>
  <entry>
    <title>动态规划</title>
    <url>/2018/03/09/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2018/03/09/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/dynamic_programming.png" alt="avatar"></p>
<a id="more"></a>
<h2 id="ji-ben-si-lu">基本思路</h2>
<h3 id="si-kao-zhuang-tai">思考状态</h3>
<p>状态先尝试“题目问什么，就把什么设置为状态”。然后考虑“状态如何转移”，如果“状态转移方程”不容易得到，尝试修改定义，目的仍然是为了方便得到“状态转移方程”。</p>
<h3 id="si-kao-zhuang-tai-zhuan-yi-fang-cheng-he-xin-nan-dian">思考状态转移方程（核心、难点）</h3>
<p>状态转移方程是非常重要的，是动态规划的核心，也是难点，起到承上启下的作用。</p>
<blockquote>
<p>技巧是分类讨论。对状态空间进行分类，思考最优子结构到底是什么。即大问题的最优解如何由小问题的最优解得到。</p>
</blockquote>
<p>归纳“状态转移方程”是一个很灵活的事情，得具体问题具体分析，除了掌握经典的动态规划问题以外，还需要多做题。如果是针对面试，请自行把握难度，我个人觉得掌握常见问题的动态规划解法，明白动态规划的本质就是打表格，从一个小规模问题出发，逐步得到大问题的解，并记录过程。动态规划依然是“空间换时间”思想的体现。</p>
<h3 id="si-kao-chu-shi-hua">思考初始化</h3>
<p>初始化是非常重要的，一步错，步步错，初始化状态一定要设置对，才可能得到正确的结果。</p>
<p>角度 1：直接从状态的语义出发；</p>
<p>角度 2：如果状态的语义不好思考，就考虑“状态转移方程”的边界需要什么样初始化的条件；</p>
<p>角度 3：从“状态转移方程”方程的下标看是否需要多设置一行、一列表示“哨兵”，这样可以避免一些边界的讨论，使得代码变得比较短。</p>
<h3 id="si-kao-shu-chu">思考输出</h3>
<p>有些时候是最后一个状态，有些时候可能会综合所有计算过的状态。</p>
<h3 id="si-kao-zhuang-tai-ya-suo">思考状态压缩</h3>
<p>“状态压缩”会使得代码难于理解，初学的时候可以不一步到位。先把代码写正确，然后再思考状态压缩。</p>
<p>状态压缩在有一种情况下是很有必要的，那就是状态空间非常庞大的时候（处理海量数据），此时空间不够用，就必须状态压缩。</p>
<hr>
<h2 id="ti-mu-leet-code">题目（LeetCode）</h2>
<h3 id="5-a-href-https-leetcode-cn-com-problems-longest-palindromic-substring-zui-chang-hui-wen-zi-chuan-a">5. <a href="https://leetcode-cn.com/problems/longest-palindromic-substring/" target="_blank" rel="noopener">最长回文子串</a></h3>
<p>描述：给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。</p>
<p>示例 1：</p>
<p>输入: “babad”</p>
<p>输出: “bab”</p>
<p>注意: “aba” 也是一个有效答案。</p>
<p>示例 2：</p>
<p>输入: “cbbd”</p>
<p>输出: “bb”</p>
<p>“动态规划”最关键的步骤是想清楚“状态如何转移”，事实上，“回文”是天然具有“状态转移”性质的：</p>
<blockquote>
<p>一个回文去掉两头以后，剩下的部分依然是回文（这里暂不讨论边界）。</p>
</blockquote>
<p>依然从回文串的定义展开讨论：</p>
<ol>
<li>
<p>如果一个字符串的头尾两个字符都不相等，那么这个字符串一定不是回文串；</p>
</li>
<li>
<p>如果一个字符串的头尾两个字符相等，才有必要继续判断下去。</p>
</li>
<li>
<p>如果里面的子串是回文，整体就是回文串；</p>
</li>
<li>
<p>如果里面的子串不是回文串，整体就不是回文串。</p>
</li>
</ol>
<p>即在头尾字符相等的情况下，里面子串的回文性质据定了整个子串的回文性质，这就是状态转移。因此可以把“状态”定义为原字符串的一个子串是否为回文子串。</p>
<h4 id="di-1-bu-ding-yi-zhuang-tai">第 1 步：定义状态</h4>
<p><code>dp[i][j]</code> 表示子串 <code>s[i,j]</code> 是否为回文子串。</p>
<h4 id="di-2-bu-si-kao-zhuang-tai-zhuan-yi-fang-cheng">第 2 步：思考状态转移方程</h4>
<p>这一步在做分类讨论（根据头尾字符是否相等），根据上面的分析得到：</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">dp[<span class="string">i</span>][<span class="symbol">j</span>] = (s[<span class="string">i</span>] == s[<span class="string">j</span>]) and dp[<span class="string">i + 1</span>][<span class="symbol">j - 1</span>]</span><br></pre></td></tr></table></figure>
<p>分析：</p>
<ol>
<li>
<p><code>i</code> 和 <code>j</code> 的关系是 <code>i &lt;= j</code>，因此，只需要填这张表的上半部分；</p>
</li>
<li>
<p>看到 <code>dp[i + 1][j - 1]</code> 就得考虑边界情况。边界条件是：表达式 <code>[i + 1, j - 1]</code>不构成区间，即长度严格小于 2，即<code>j - 1 - (i + 1) + 1 &lt; 2</code> ，整理得 <code>j - i &lt; 3</code>。</p>
</li>
</ol>
<p>这个结论很显然：当子串 <code>s[i, j]</code> 的长度等于 2 或者等于 3 的时候，我其实只需要判断一下头尾两个字符是否相等就可以直接下结论了。</p>
<ul>
<li>
<p>如果子串 <code>s[i + 1, j - 1]</code> 只有 1 个字符，即去掉两头，剩下中间部分只有 1 个字符，当然是回文；</p>
</li>
<li>
<p>如果子串 <code>s[i + 1, j - 1]</code> 为空串，那么子串 <code>s[i, j]</code> 一定是回文子串。</p>
</li>
</ul>
<p>因此，在 <code>s[i] == s[j]</code> 成立和 <code>j - i &lt; 3</code> 的前提下，直接可以下结论，<code>dp[i][j] = true</code>，否则才执行状态转移。</p>
<h4 id="di-3-bu-kao-lu-chu-shi-hua">第 3 步:考虑初始化</h4>
<p>初始化的时候，单个字符一定是回文串，因此把对角线先初始化为 1，即 <code>dp[i][i] = 1 </code>。</p>
<h4 id="di-4-bu-kao-lu-shu-chu">第 4 步:考虑输出</h4>
<p>只要一得到 <code>dp[i][j] = true</code>，就记录子串的长度和起始位置，没有必要截取，因为截取字符串也要消耗性能，记录此时的回文子串的“起始位置”和“回文长度”即可。</p>
<h4 id="di-5-bu-kao-lu-zhuang-tai-shi-fou-ke-yi-ya-suo">第 5 步:考虑状态是否可以压缩</h4>
<p>在填表的过程中，只参考了左下方的数值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">longestPalindrome</span><span class="params">(self,s)</span>:</span></span><br><span class="line"></span><br><span class="line">size = len(s)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> size &lt; <span class="number">2</span>:</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line"></span><br><span class="line">dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(size)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(size)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(size):</span><br><span class="line"></span><br><span class="line">dp[_][_] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">start_idx = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">max_len = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,size):</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(j):</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> s[i] == s[j]:</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> j - i &lt; <span class="number">3</span>: <span class="comment"># i,j 之间只有一个字符或者没有字符</span></span><br><span class="line"></span><br><span class="line">dp[i][j] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">dp[i][j] = dp[i+<span class="number">1</span>][j<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> dp[i][j] == <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">cur_len = j - i + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> cur_len &gt; max_len:</span><br><span class="line"></span><br><span class="line">max_len = cur_len</span><br><span class="line"></span><br><span class="line">start_idx = i</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> s[start_idx:start_idx+max_len]</span><br></pre></td></tr></table></figure>
<h3 id="1143-a-href-https-leetcode-cn-com-problems-longest-common-subsequence-zui-chang-gong-gong-zi-xu-lie-a">1143. <a href="https://leetcode-cn.com/problems/longest-common-subsequence/" target="_blank" rel="noopener">最长公共子序列</a></h3>
<p>描述：给定两个字符串 text1 和 text2，返回这两个字符串的最长公共子序列。一个字符串的 子序列是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。</p>
<p>例如，“ace” 是 “abcde” 的子序列，但 “aec” 不是 “abcde” 的子序列。两个字符串的「公共子序列」是这两个字符串所共同拥有的子序列。若这两个字符串没有公共子序列，则返回 0。</p>
<p>示例 1:</p>
<p>输入：text1 = “abcde”, text2 = “ace”</p>
<p>输出：3</p>
<p>解释：最长公共子序列是 “ace”，它的长度为 3。</p>
<p>示例 2:</p>
<p>输入：text1 = “abc”, text2 = “abc”</p>
<p>输出：3</p>
<p>解释：最长公共子序列是 “abc”，它的长度为 3。</p>
<p>示例 3:</p>
<p>输入：text1 = “abc”, text2 = “def”</p>
<p>输出：0</p>
<p>解释：两个字符串没有公共子序列，返回 0。</p>
<p>提示:</p>
<p>1 &lt;= text1.length &lt;= 1000</p>
<p>1 &lt;= text2.length &lt;= 1000</p>
<p>输入的字符串只含有小写英文字符。</p>
<h4 id="si-lu">思路：</h4>
<ol>
<li>
<p>状态：问题要求最长公共子序列，那就让dp表存储长度。</p>
</li>
<li>
<p>状态转移：</p>
</li>
<li>
<p>当 <code>S1i==S2j</code> 时，那么就能在 <code>S1</code> 的前 <code>i-1</code> 个字符与 <code>S2</code> 的前 <code>j-1</code> 个字符最长公共子序列的基础上再加上 <code>S1i</code> 这个值，最长公共子序列长度加 <code>1</code>，即 <code>dp[i][j] = dp[i-1][j-1] + 1</code>。</p>
</li>
<li>
<p>当 <code>S1i != S2j</code> 时，此时最长公共子序列为 <code>S1</code> 的前 <code>i-1</code> 个字符和 <code>S2</code> 的前 <code>j</code> 个字符最长公共子序列，或者 <code>S1</code> 的前 i 个字符和 <code>S2</code> 的前 <code>j-1</code> 个字符最长公共子序列，取它们的最大者，即 <code>dp[i][j] = max{ dp[i-1][j], dp[i][j-1] }</code>。</p>
</li>
</ol>
<p>综上，状态转移方程：</p>
<p><img src="/2018/03/09/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/dp_1.png" alt="avatar"></p>
<ol start="3">
<li>解的形式：解在dp表的右下角</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">longestCommonSubsequence</span><span class="params">(self, text1, text2)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:type text1: str</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:type text2: str</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:rtype: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> text1 <span class="keyword">or</span> <span class="keyword">not</span> text2:</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">size_1 = len(text1)</span><br><span class="line"></span><br><span class="line">size_2 = len(text2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化表格</span></span><br><span class="line"></span><br><span class="line">dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(size_2+<span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(size_1+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(size_1):</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(size_2):</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> text1[i] == text2[j]:</span><br><span class="line"></span><br><span class="line">dp[i+<span class="number">1</span>][j+<span class="number">1</span>] = dp[i][j] + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">dp[i+<span class="number">1</span>][j+<span class="number">1</span>] = max(dp[i][j+<span class="number">1</span>],dp[i+<span class="number">1</span>][j])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> dp[size_1][size_2]</span><br></pre></td></tr></table></figure>
<h3 id="53-a-href-https-leetcode-cn-com-problems-maximum-subarray-zui-da-xu-lie-he-a">53. <a href="https://leetcode-cn.com/problems/maximum-subarray/" target="_blank" rel="noopener">最大序列和</a></h3>
<p>给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。</p>
<p>示例:</p>
<p>输入: [-2,1,-3,4,-1,2,1,-5,4],</p>
<p>输出: 6</p>
<p>解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。</p>
<p>进阶:</p>
<p>如果你已经实现复杂度为 O(n) 的解法，尝试使用更为精妙的分治法求解。</p>
<h4 id="si-lu-1">思路</h4>
<p>法一：暴力动态规划：超时</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxSubArray</span><span class="params">(self, nums)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:type nums: List[int]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:rtype: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">size = len(nums)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> size == <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> nums[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(size)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(size)]</span><br><span class="line"></span><br><span class="line">start = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">end = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">max_val = float(<span class="string">'-inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(size):</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(j):</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> j - i == <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">dp[i][j] = nums[i] + nums[j]</span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">dp[i][j] = dp[i][j<span class="number">-1</span>] + nums[j]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> max_val &lt; dp[i][j]:</span><br><span class="line"></span><br><span class="line">start = i</span><br><span class="line"></span><br><span class="line">end = j</span><br><span class="line"></span><br><span class="line">max_val = dp[i][j]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> max(max_val, max(nums))</span><br></pre></td></tr></table></figure>
<p>法二：DP法，用了些灵巧的思路</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxSubArray</span><span class="params">(self, nums)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:type nums: List[int]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:rtype: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">max_, sum_ = nums[<span class="number">0</span>],nums[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> nums[<span class="number">1</span>:]:</span><br><span class="line"></span><br><span class="line">sum_ = sum_ + num <span class="keyword">if</span> sum_ &gt; <span class="number">0</span> <span class="keyword">else</span> num</span><br><span class="line"></span><br><span class="line">max_ = max_ <span class="keyword">if</span> max_ &gt; sum_ <span class="keyword">else</span> sum_</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> max_</span><br></pre></td></tr></table></figure>
<h3 id="300-a-href-https-leetcode-cn-com-problems-longest-increasing-subsequence-zui-chang-shang-sheng-zi-xu-lie-a">300. <a href="https://leetcode-cn.com/problems/longest-increasing-subsequence/" target="_blank" rel="noopener">最长上升子序列</a></h3>
<p>给定一个无序的整数数组，找到其中最长上升子序列的长度。</p>
<p>示例:</p>
<p>输入: [10,9,2,5,3,7,101,18]</p>
<p>输出: 4</p>
<p>解释: 最长的上升子序列是 [2,3,7,101]，它的长度是 4。</p>
<p>说明:可能会有多种最长上升子序列的组合，你只需要输出对应的长度即可。你算法的时间复杂度应该为 \(O(n^2)\) 。</p>
<p>进阶: 你能将算法的时间复杂度降低到 <code>O(n log n)</code> 吗?</p>
<h4 id="si-lu-2">思路</h4>
<p>状态：定义 dp[i]为考虑前 i个元素，以第 i 个数字结尾的最长上升子序列的长度，注意 nums[i]必须被选取。</p>
<p>状态转移：从小到大计算 dp[] 数组的值，在计算 dp[i] 之前，我们已经计算出 dp[0…i−1] 的值，则状态转移方程为：</p>
<p><img src="/2018/03/09/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/dp_2.png" alt="avatar"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lengthOfLIS</span><span class="params">(self, nums)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:type nums: List[int]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:rtype: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">dp = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line"></span><br><span class="line">dp.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> nums[i] &gt; nums[j]:</span><br><span class="line"></span><br><span class="line">dp[i] = max(dp[i], dp[j] + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> max(dp)</span><br></pre></td></tr></table></figure>
<h3 id="a-href-https-leetcode-cn-com-problems-regular-expression-matching-10-zheng-ze-biao-da-shi-pi-pei-a"><a href="https://leetcode-cn.com/problems/regular-expression-matching/" target="_blank" rel="noopener">10. 正则表达式匹配</a></h3>
<p><strong>状态</strong>：问题要求解的是正则串是否可以匹配，则把状态设为是否可匹配。<code>dp[i][j]</code> 表示 <code>s</code> 的前 <code>i</code>个是否能被 <code>p</code> 的前 <code>j</code> 个匹配</p>
<p><strong>转移方程</strong> ：</p>
<ol>
<li>
<p><code>p[j] == s[i] or p[j] == &quot;.&quot; : dp[i][j] = dp[i-1][j-1] </code></p>
</li>
<li>
<p><code>p[j] ==&quot; * &quot;</code>:     # 比较难想</p>
<ol>
<li><code>p[j-1] != s[i]: dp[i][j] = dp[i][j-2] </code> # <code>*</code> 前的字符和原字符串不匹配,相当于该字符匹配0次。</li>
<li><code>p[j-1] == s[i] or p[j-1] == &quot;.&quot;：</code> # 匹配<br>
3.  <code>dp[i][j] = dp[i-1][j]</code> # * 匹配多次<br>
2.  <code>dp[i][j] = dp[i][j-1]</code> # * 匹配一次<br>
3.  <code>dp[i][j] = dp[i][j-2]</code> # * 匹配0次</li>
</ol>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span><span class="params">(self, s, p)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> p:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">not</span> s</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> s <span class="keyword">and</span> len(p) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        nrow = len(s) + <span class="number">1</span></span><br><span class="line">        ncol = len(p) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        dp = [[<span class="literal">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(ncol)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(nrow)]</span><br><span class="line"></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">1</span>] = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">2</span>, ncol):</span><br><span class="line">            j = c - <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> p[j] == <span class="string">'*'</span>:</span><br><span class="line">                dp[<span class="number">0</span>][c] = dp[<span class="number">0</span>][c - <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">1</span>, nrow): <span class="comment"># r 原始串</span></span><br><span class="line">            i = r - <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">1</span>, ncol): <span class="comment"># c 正则串</span></span><br><span class="line">                j = c - <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> s[i] == p[j] <span class="keyword">or</span> p[j] == <span class="string">'.'</span>:</span><br><span class="line">                    dp[r][c] = dp[r - <span class="number">1</span>][c - <span class="number">1</span>]</span><br><span class="line">                <span class="keyword">elif</span> p[j] == <span class="string">'*'</span>:</span><br><span class="line">                    <span class="keyword">if</span> p[j - <span class="number">1</span>] == s[i] <span class="keyword">or</span> p[j - <span class="number">1</span>] == <span class="string">'.'</span>:</span><br><span class="line">                        dp[r][c] = dp[r - <span class="number">1</span>][c] <span class="keyword">or</span> dp[r][c - <span class="number">2</span>]</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        dp[r][c] = dp[r][c - <span class="number">2</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[r][c] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[nrow - <span class="number">1</span>][ncol - <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>专业技能</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>背包问题</title>
    <url>/2018/03/01/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="ti-mu">题目</h3>
<p>有N件物品和一个容量为<code>v</code>的背包，放入第i件物品耗费空间<code>c[i]</code>得到价值为<code>W[i]</code>,求哪些物品装入背包可使得总价值最大？</p>
<h4 id="dong-tai-gui-hua-qiu-jie">动态规划求解：</h4>
<p>状态：问题所求最大价值，设状态即为背包中物品的价值</p>
<p>状态转移方程：</p>
<p><code>dp[i][j]</code>表示前i件物品放入一个容量为V的背包获得的最大价值状态转移方程为<br>
\[
\mathrm{dp}[\mathrm{i}][\mathrm{v}]=\max \left\{\begin{array}{l}
dp[i-1][v](\text {not select}) \\
dp[i-1][v-C[i]]+W[i](\text {select})
\end{array}\right.
\]<br>
其中<code>dp[i-1][v-C[i]]</code>表示前<code>i-1</code>件物品放入容量为<code>v-C[i]</code>的背包中。</p>
<h4 id="dai-ma">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># F[0, 0...V] = 0</span></span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>):</span><br><span class="line">    dp[<span class="number">0</span>][v] = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># 容量装不下当前物品时，保留原值</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(<span class="number">0</span>, C[i]):</span><br><span class="line">        dp[i][v] = dp[i<span class="number">-1</span>][v]</span><br><span class="line">    <span class="comment"># 状态转移</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(C[i], V+<span class="number">1</span>):</span><br><span class="line">        dp[i][v] = max(dp[i<span class="number">-1</span>][v], dp[i<span class="number">-1</span>][v-C[i]] + W[i])</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h4 id="you-hua">优化</h4>
<ol>
<li>
<p>时间复杂度：<code>O(VN)</code></p>
</li>
<li>
<p>空间复杂度：<code>O(VN)</code>可优化成<code>O(N)</code></p>
<p><code>dp[i][v]</code>由<code>dp[i-1][v-C[i]]</code>和<code>dp[i-1][v]</code>两个子问题递推得到，所以第<code>i</code>次迭代时，<code>v</code>按照<code>V-&gt;0</code>的逆序计算<code>dp[v]</code>可以保证<code>dp[v]=max(dp[v],dp[v-C[i]]+W[i])</code>，这样在赋值之前，访问到的均为<code>i-1</code>次迭代保留的值。</p>
<p>空间为<code>c[i]</code>的物品不会受到状态<code>dp[0...C[i]]</code>的影响，即不影响装不下该物品的背包能装入的最大价值，所以<code>v</code>中<code>V</code>递减到<code>C[i]</code>就可以了。</p>
</li>
<li>
<p>代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dp = [<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(V, C[i]<span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        dp[v] = max(dp[v], dp[v-C[i]] + W[i])</span><br></pre></td></tr></table></figure>
<p>对处理0-1背包的物品进行抽象：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_one_pack</span><span class="params">(F, c, w)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(V, c<span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        dp[v] = max(dp[v], dp[v-c] + w)</span><br><span class="line"></span><br><span class="line">dp = [<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    zero_one_pack(dp, C[i], W[i])</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="chu-shi-hua-xi-jie">初始化细节</h4>
<p>初始化dp的过程，实际上就是在没有任何物品可以装入背包时的合法状态（必要但不充分条件）</p>
<ol>
<li>
<p>恰好装满背包时，求最优解：<code>dp[0]=0,dp[1...V]=</code>\(-\infty\)</p>
<ul>
<li>
<p>容量为0时，什么也装不下，价值为0，所以<code>dp[0]=0</code></p>
</li>
<li>
<p>容量不为0的时候，状态未定义，状态设为\(-\infty\)</p>
</li>
</ul>
<p>按此进行初始化，迭代过程中，只有能够恰好装入的物品才会更新出合理值（正值），基于<code>dp[0]=0</code></p>
</li>
<li>
<p>没要求把背包装满：<code>dp[0...V]=0</code></p>
<p>任何容量都有一个合法的“什么都不装”，所以设<code>dp[0...V]=0</code></p>
</li>
</ol>
<h3 id="wan-quan-bei-bao">完全背包</h3>
<p>![image-20200321181733468](/Users/lizhen/Library/Application Support/typora-user-images/image-20200321181733468.png)</p>
<h4 id="ti-mu-1">题目</h4>
<p>有N种物品和1个容量为V的背包，每种物品可以有任意多个，放入第i种物品的耗费空间<code>C[i]</code>，价值为<code>W[i]</code>,求背包能够装入的最大价值？</p>
<h4 id="si-lu">思路</h4>
<p>每种物品可以放入0件，1件，…，<code>int(V/C[i])</code>件，令<code>dp[i][v]</code>表示前i种物品放入容量v的背包获取的最大价值，则<code>dp[i][v]=max(dp[i-1][v-k*C[i]]+k*W[i]),0&lt;=k*C[i]&lt;=v</code></p>
<h4 id="you-hua-1">优化</h4>
<p>时间复杂度：<code>O(nv</code>\(\sum \frac{V}{C[i]}\))</p>
<p>简单优化：一个简单有效的优化思路是：单位价值更便宜的应该直接跳过</p>
<p>若两件物品<code>i,j</code>有<code>C[i]&lt;=C[j] and W[i]&gt;=W[j]</code>;则<code>j</code>可以直接跳过，不必考虑，对于随机数据，这会大大减少物品数量，加快速度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sieve = &#123;C[i]: <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>)&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># 跳过装不下的</span></span><br><span class="line">    <span class="keyword">if</span> C[i] &gt; V:</span><br><span class="line">        sieve[C[i]] = <span class="number">-1</span></span><br><span class="line">    <span class="comment"># 跳过 C[i] == C[j] and W[i] &gt; W[j]的j物品</span></span><br><span class="line">    <span class="keyword">elif</span> sieve[C[i]] &lt; W[i]:</span><br><span class="line">        sieve[C[i]] = W[i]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">if</span> sieve[C[i]] &lt; <span class="number">0</span> <span class="keyword">or</span> W[i] &lt; sieve[C[i]]:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(C[i], V+<span class="number">1</span>):</span><br><span class="line">        max_k = V // C[i]</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, max_k+<span class="number">1</span>):</span><br><span class="line">            dp[i][v] = max(dp[i<span class="number">-1</span>][v], dp[i<span class="number">-1</span>][v-k*C[i]] + k*W[i])</span><br></pre></td></tr></table></figure>
<p>最终优化方案：</p>
<p>按照正序容量更新当前总价值，<code>dp[v-C[i]]</code>中可能是<code>i-1</code>次迭代中保留的值，也可能是本次迭代中产生了变化的值。（如果加选第i件物品）</p>
<p>时间复杂度：<code>O(NV)</code></p>
<p>空间复杂度:<code>O(V)</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dp = [<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(C[i], V+<span class="number">1</span>):</span><br><span class="line">        dp[v] = max(dp[v], dp[v-C[i]] + W[i])</span><br></pre></td></tr></table></figure>
<p>对处理一种完全背包的物品过程进行抽象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">complete_pack</span><span class="params">(F, c, w)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(c, V+<span class="number">1</span>):</span><br><span class="line">        F[v] = max(F[v], F[v-c] + w)</span><br><span class="line"></span><br><span class="line">dp = [<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    complete_pack(dp, C[i], W[i])</span><br></pre></td></tr></table></figure>
<h4 id="bi-jiao">比较</h4>
<p>0-1 背包：倒序容量，当前新物品之多影响一次当前总价值</p>
<p>完全背包：正序容量，随着容量增加，当前新物品可以多次影响当前总价值</p>
<h3 id="duo-zhong-bei-bao">多重背包</h3>
<p>有<code>N</code>种物品和1个容量为<code>V</code>的背包，第i种物品有<code>M[i]</code>件，每件耗费空间<code>C[i]</code>,价值<code>W[i]</code>,求能装入背包的最大价值。</p>
<h4 id="si-lu-1">思路1</h4>
<p>和完全背包类似，对i种物品有<code>M[i]+1</code>种策略，取<code>0</code>件，取<code>1</code>件，…，取<code>M[i]</code>,令<code>dp[i][v]</code>表示前i中物品放入容量v中获得的最大价值，则<code>dp[i][v]=max(dp[i-1][v-k*C[i]]+k*W[i]),0&lt;=k&lt;=M[i]</code></p>
<p>时间复杂度：<code>O(V</code>\(\sum M_i\)`</p>
<h4 id="si-lu-2">思路2</h4>
<p>转化为0-1背包问题，把第i种物品换成<code>M[i]</code>件<code>0-1</code>背包中的物品，得到物品数为\(\sum M_i\)的<code>0-1</code>背包问题，时间复杂度仍为\(O(V\sum M_i)\)。使用二进制的思想，可以把时间复杂度下降为\(O(V\sum{log{M_i}})\),即按照如下的方式进行拆分：</p>
<p>\(2^0,2^1,2^2,...,2^{k-1},M_i-2^k+1\)</p>
<p>这样拼成的新物品总量等于原来物品的数量，第i件物品拆分成了\(O(log{m_i})\)</p>
<h5 id="dai-ma-1">代码</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiple_pack</span><span class="params">(F, c, w, m)</span>:</span></span><br><span class="line">    <span class="comment"># 如果物品足够多，就变成了完全背包</span></span><br><span class="line">    <span class="keyword">if</span> c * m &gt;= V:</span><br><span class="line">        complete_pack(F, c, w)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 多重背包按照2次幂进行划分成0-1背包</span></span><br><span class="line">    k = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> k &lt; m:</span><br><span class="line">        zero_one_pack(F, k*c, k*w)</span><br><span class="line">        m = m - k</span><br><span class="line">        k = <span class="number">2</span> * k</span><br><span class="line">    <span class="comment"># 把剩余的物品当作1件物品</span></span><br><span class="line">    zero_one_pack(F, c * m, w * m)</span><br><span class="line"></span><br><span class="line">dp = [<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    multiple_pack(dp, C[i], W[i], M[i])</span><br></pre></td></tr></table></figure>
<h3 id="er-wei-fei-yong-bei-bao">二维费用背包</h3>
<p>对于每件物品有两种不同的cost，每种cost有一个最大容纳值（背包容量），怎样选择物品使得背包容纳最大价值？第i种物品两种费用分别为<code>C[i]</code>和<code>D[i]</code>，两种cost最大容纳值为V,U，物品的价值为<code>W[i]</code>(题目场景样例：打矩形中放小矩形，求最大容量)</p>
<h4 id="si-lu-3">思路</h4>
<p>费用加1维，状态加1维；令<code>dp[i][v][u]</code>表示前i件物品付出两种费用分别为v，u时可获得的最大价值。则<code>dp[i][v][u]=max(dp[i-1][v][u],dp[i-1][v-C[i]][u-D[i]]+W[i])</code></p>
<h4 id="you-hua-2">优化</h4>
<p>类似于一维背包空间优化方法，二维背包与之类似：</p>
<ol>
<li>二维0-1背包：v和u采用逆序循环</li>
<li>二维完全背包：v和u采用顺序循环</li>
<li>二维多重背包：拆分物品</li>
</ol>
<h4 id="dai-ma-2">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_dim_zero_one_pack</span><span class="params">(F, c, d, w)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(V, c<span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> range(U, d<span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            F[v][u] = max(F[v][u], F[v-c][u-d] + w)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_dim_complete_pack</span><span class="params">(F, c, d, w)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(c, V+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> range(d, U+<span class="number">1</span>):</span><br><span class="line">            F[v][u] = max(F[v][u], F[v-c][u-d] + w)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_dim_multiple_pack</span><span class="params">(F, c, d, w, m)</span>:</span></span><br><span class="line">    <span class="comment"># 两种cost数量都充足，可直接按照完全背包处理</span></span><br><span class="line">    <span class="keyword">if</span> c * m &gt;= V <span class="keyword">and</span> d * m &gt;= U:</span><br><span class="line">        two_dim_complete_pack(F, c, d, w)</span><br><span class="line">    <span class="comment"># 多重背包按照2次幂进行划分成0-1背包</span></span><br><span class="line">    k = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> k &lt; m:</span><br><span class="line">        two_dim_complete_pack(F, k * c, k * d, k * w)</span><br><span class="line">        m = m - k</span><br><span class="line">        k = <span class="number">2</span> * k</span><br><span class="line">    <span class="comment"># 把剩余的物品当作1件物品</span></span><br><span class="line">    two_dim_zero_one_pack(F, m * c, m * d, m * w)</span><br><span class="line"></span><br><span class="line">F = [[<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)] <span class="keyword">for</span> u <span class="keyword">in</span> range(U+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    two_dim_zero_one_pack(F, C[i], D[i], W[i])</span><br><span class="line">    <span class="comment"># two_dim_complete_pack(F, C[i], D[i], W[i])</span></span><br><span class="line">    <span class="comment"># two_dim_multiple_pack(F, C[i], D[i], W[i], M[i])</span></span><br></pre></td></tr></table></figure>
<h4 id="ying-yong">应用</h4>
<p>二维费用题目的一种隐含表述方式：最多取U件</p>
<p>对于有这种限制的题目，可以将’件数’看作一种费用，每件物品件数费用为1，可以达到的最大件数费用非U，<code>dp[v][u]</code>表示cost为v，最多选择u件物品时所达到的最大价值（再根据物品的特性（0-1,完全，多重）采用不同的方式进行循环，得出答案）</p>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>背包问题</tag>
      </tags>
  </entry>
  <entry>
    <title>排序算法</title>
    <url>/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="pai-xu-suan-fa">排序算法</h2>
<p><img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/%E6%8E%92%E5%BA%8F.png" alt="avatar"></p>
<a id="more"></a>
<h3 id="mou-pao-pai-xu">冒泡排序</h3>
<h4 id="si-xiang">思想</h4>
<p>是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。</p>
<h4 id="bu-zou">步骤</h4>
<ol>
<li>比较相邻的元素。如果第一个比第二个大，就交换它们两个；</li>
<li>对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数；</li>
<li>针对所有的元素重复以上的步骤，除了最后一个；</li>
<li>重复步骤1~3，直到排序完成。</li>
</ol>
<h4 id="fu-za-du">复杂度</h4>
<ol>
<li>时间复杂度：<code>O(1)</code></li>
<li>空间复杂度：\(O(n^2)\)</li>
</ol>
<h4 id="dai-ma">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubble_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="comment"># 外层循环: 走访数据的次数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 设置是否交换标志位</span></span><br><span class="line">        flag = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 内层循环: 每次走访数据时, 相邻对比次数</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(nums) - i - <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 要求从低到高</span></span><br><span class="line">            <span class="comment"># 如次序有误就交换</span></span><br><span class="line">            <span class="keyword">if</span> nums[j] &gt; nums[j + <span class="number">1</span>]:</span><br><span class="line">                nums[j], nums[j + <span class="number">1</span>] = nums[j + <span class="number">1</span>], nums[j]</span><br><span class="line">                <span class="comment"># 发生了数据交换</span></span><br><span class="line">                flag = <span class="literal">True</span></span><br><span class="line">        <span class="comment"># 如果未发生交换数据, 则说明后续数据均有序</span></span><br><span class="line">        <span class="keyword">if</span> flag == <span class="literal">False</span>:</span><br><span class="line">            <span class="keyword">break</span>  <span class="comment"># 跳出数据走访</span></span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<h3 id="xuan-ze-pai-xu">选择排序</h3>
<h4 id="si-xiang-1">思想</h4>
<p>它的工作原理：首先在未排序序列中找到最小元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。</p>
<h4 id="bu-zou-1">步骤</h4>
<p>n个记录的直接选择排序可经过n-1趟直接选择排序得到有序结果。</p>
<ol>
<li>初始状态：无序区为<code>R[1…n]</code>，有序区为空；</li>
<li>第i趟排序<code>(i=1,2,3…n-1)</code>开始时，当前有序区和无序区分别为<code>R[1…i-1]和R(i…n）</code>。该趟排序从当前无序区中-选出关键字最小的记录 <code>R[k]</code>，将它与无序区的第1个记录R交换，使<code>R[1…i]</code>和<code>R[i+1…n)</code>分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区；</li>
<li>n-1趟结束，数组有序化了。</li>
</ol>
<h4 id="fu-za-du-1">复杂度</h4>
<ol>
<li>时间复杂度：\(O(n^2)\)</li>
<li>空间复杂度：O(1)</li>
</ol>
<h4 id="dai-ma-1">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selection_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line">        <span class="keyword">return</span> nums</span><br><span class="line">    size = len(nums)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(size):</span><br><span class="line">        min_idx = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>,size):</span><br><span class="line">            <span class="keyword">if</span> nums[j] &lt; nums[min_idx]:</span><br><span class="line">                min_idx = j</span><br><span class="line">        temp = nums[min_idx]</span><br><span class="line">        nums[min_idx] = nums[i]</span><br><span class="line">        nums[i] = temp</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<h3 id="gui-bing-pai-xu">归并排序</h3>
<h4 id="si-xiang-2">思想</h4>
<p>和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是O(n log n）的时间复杂度。代价是需要额外的内存空间。</p>
<h4 id="bu-zou-2">步骤</h4>
<p><strong>归并排序</strong>是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。归并排序是一种稳定的排序方法。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。</p>
<ol>
<li>把长度为n的输入序列分成两个长度为n/2的子序列；</li>
<li>对这两个子序列分别采用归并排序；</li>
<li>将两个排序好的子序列合并成一个最终的排序序列。</li>
</ol>
<h4 id="fu-za-du-2">复杂度</h4>
<ol>
<li>时间复杂度\(O(nlog_2 n)\)</li>
</ol>
<h4 id="dai-ma-2">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(nums) &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> nums</span><br><span class="line"></span><br><span class="line">    mid_idx = len(nums) / <span class="number">2</span></span><br><span class="line">    left_array = nums[<span class="number">0</span>:mid_idx]</span><br><span class="line">    right_array = nums[mid_idx:len(nums)]</span><br><span class="line">    <span class="keyword">return</span> merge(merge_sort(left_array),merge_sort(right_array))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span><span class="params">(left_array,right_array)</span>:</span></span><br><span class="line">    <span class="comment"># 合并两个有序数组</span></span><br><span class="line">    res = []</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    j = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; len(left_array) <span class="keyword">and</span> j &lt; len(right_array):</span><br><span class="line">        <span class="keyword">if</span> left_array[i] &lt; right_array[j]:</span><br><span class="line">            res.append(left_array[i])</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            res.append(right_array[j])</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i &lt; len(left_array):</span><br><span class="line">        res.extend(left_array[i:])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        res.extend(right_array[j:])</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3 id="cha-ru-pai-xu">插入排序</h3>
<h4 id="si-xiang-3">思想</h4>
<p>通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。</p>
<h4 id="bu-zou-3">步骤</h4>
<p>一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下：</p>
<ol>
<li>从第一个元素开始，该元素可以认为已经被排序；</li>
<li>取出下一个元素，在已经排序的元素序列中从后向前扫描；</li>
<li>如果该元素（已排序）大于新元素，将该元素移到下一位置；</li>
<li>重复步骤3，直到找到已排序的元素小于或者等于新元素的位置；</li>
<li>将新元素插入到该位置后；</li>
<li>重复步骤2~5。</li>
</ol>
<h4 id="fu-za-du-3">复杂度</h4>
<ol>
<li>时间复杂度：<code>O(1)</code></li>
<li>空间复杂度：\(O(n^2)\)</li>
</ol>
<h4 id="dai-ma-3">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(nums)):</span><br><span class="line">        temp = nums[i]</span><br><span class="line">        pos = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> temp &lt; nums[j]:</span><br><span class="line">                nums[j + <span class="number">1</span>] = nums[j]</span><br><span class="line">                pos = j</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pos = j + <span class="number">1</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        nums[pos] = temp</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<h3 id="xi-er-pai-xu">希尔排序</h3>
<p>希尔排序是希尔（Donald Shell） 于1959年提出的一种排序算法。希尔排序也是一种插入排序，它是简单插入排序经过改进之后的一个更高效的版本，也称为缩小增量排序，同时该算法是冲破O(n2）的第一批算法之一。它与插入排序的不同之处在于，它会优先比较距离较远的元素。希尔排序又叫缩小增量排序。</p>
<h4 id="si-xiang-4">思想</h4>
<p>希尔排序是把记录按下表的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。</p>
<h4 id="bu-zou-4">步骤:</h4>
<p>先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序,具体步骤如下：</p>
<ol>
<li>选择一个增量序列<code>t1，t2，…，tk</code>，其中<code>ti&gt;tj，tk=1</code>；</li>
<li>按增量序列个数k，对序列进行 k 趟排序；</li>
<li>每趟排序，根据对应的增量ti，将待排序列分割成若干长度为 m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。<br>
<img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/aHR0cHM6Ly9pbWFnZXMyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvMTE5MjY5OS8yMDE4MDMvMTE5MjY5OS0yMDE4MDMxOTA5NDExNjA0MC0xNjM4NzY2MjcxLnBuZw.png" alt="avat"></li>
</ol>
<h4 id="fu-za-du-4">复杂度</h4>
<ol>
<li>时间复杂度\(O(nlog_2 n)\)</li>
</ol>
<h4 id="dai-ma-4">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shell_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    size = len(nums)</span><br><span class="line">    gap = size / <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> gap:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(gap, size):</span><br><span class="line">            temp = nums[i]</span><br><span class="line">            pre_index = i - gap</span><br><span class="line">            <span class="comment"># 对于划分到一组内的数进行排序</span></span><br><span class="line">            <span class="keyword">while</span> pre_index &gt;= <span class="number">0</span> <span class="keyword">and</span> nums[pre_index] &gt; temp:</span><br><span class="line">                nums[pre_index + gap] = nums[pre_index]</span><br><span class="line">                pre_index -= gap</span><br><span class="line">            <span class="comment"># 插入</span></span><br><span class="line">            nums[pre_index+gap] = temp</span><br><span class="line">        </span><br><span class="line">        gap /= <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<h3 id="kuai-pai">快排</h3>
<p><img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/1450319-20190525163106857-545290968.png" alt="avatar"></p>
<h4 id="bu-zou-5">步骤</h4>
<p>快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。</p>
<ol>
<li>
<p>从数列中挑出一个元素，称为 “基准”（pivot ）</p>
</li>
<li>
<p>重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作</p>
</li>
<li>
<p>递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。<br>
复杂度:</p>
</li>
<li>
<p>时间复杂度：\(O(nlogn)\)</p>
</li>
<li>
<p>空间复杂度：\(O(logn)\)</p>
</li>
<li>
<p>稳定性：不稳定</p>
</li>
</ol>
<h4 id="dai-ma-5">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="comment"># 递归退出条件</span></span><br><span class="line">    <span class="comment"># 仅剩一个元素无需继续分组</span></span><br><span class="line">    <span class="keyword">if</span> len(nums) &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> nums</span><br><span class="line">        <span class="comment"># 设置关键数据</span></span><br><span class="line">    a = nums[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 找出所有比 a 大的数据</span></span><br><span class="line">    big = [x <span class="keyword">for</span> x <span class="keyword">in</span> nums <span class="keyword">if</span> x &gt; a]</span><br><span class="line">    <span class="comment"># 找出所有比 a 小的数据</span></span><br><span class="line">    small = [x <span class="keyword">for</span> x <span class="keyword">in</span> nums <span class="keyword">if</span> x &lt; a]</span><br><span class="line">    <span class="comment"># 找出所有与 a 相等的数据</span></span><br><span class="line">    same = [x <span class="keyword">for</span> x <span class="keyword">in</span> nums <span class="keyword">if</span> x == a]</span><br><span class="line">    <span class="comment"># 拼接数据排序的结果</span></span><br><span class="line">    <span class="keyword">return</span> quick_sort(small) + same + quick_sort(big)</span><br></pre></td></tr></table></figure>
<h3 id="dui-pai-xu">堆排序</h3>
<p>堆排序（Heapsort） 是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。</p>
<h4 id="bu-zou-6">步骤</h4>
<ol>
<li>将初始待排序关键字序列(R1,R2….Rn)构建成大顶堆，此堆为初始的无序区；</li>
<li>将堆顶元素R[1]与最后一个元素R[n]交换，此时得到新的无序区<code>(R1,R2,……Rn-1)</code>和新的有序区(Rn),且满足<code>R[1,2…n-1]&lt;=R[n]</code>；</li>
<li>由于交换后新的堆顶R[1]可能违反堆的性质，因此需要对当前无序区<code>(R1,R2,……Rn-1)</code>调整为新堆，然后再次将R[1]与无序区最后一个元素交换，得到新的无序区<code>(R1,R2….Rn-2)</code>和新的有序区<code>(Rn-1,Rn)</code>。不断重复此过程直到有序区的元素个数为n-1，则整个排序过程完成。</li>
</ol>
<p><img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/CA6B626B-98BA-4AA5-BC01-FDAFE554BB87.gif" alt="avatar"></p>
<h4 id="fu-za-du-5">复杂度</h4>
<ol>
<li>时间复杂度：O(nlogn)</li>
</ol>
<h4 id="dai-ma-6">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">heap_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    size = len(nums)</span><br><span class="line">    <span class="keyword">if</span> size &lt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> nums</span><br><span class="line">    <span class="comment"># 构建一个最大堆</span></span><br><span class="line">    build_max_heap(nums)</span><br><span class="line">    <span class="comment"># 循环将根和最后一个元素交换，然后重新调整最大堆。</span></span><br><span class="line">    <span class="keyword">while</span> size &gt; <span class="number">0</span>:</span><br><span class="line">        temp = nums[<span class="number">0</span>]</span><br><span class="line">        nums[<span class="number">0</span>] = nums[size - <span class="number">1</span>]</span><br><span class="line">        nums[size - <span class="number">1</span>] = temp</span><br><span class="line"></span><br><span class="line">        size -= <span class="number">1</span></span><br><span class="line">        adjust_heap(nums, <span class="number">0</span>,size)</span><br><span class="line">    <span class="keyword">return</span> nums</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_max_heap</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="comment"># 从最后一个非叶子节点开始向上构造最大堆</span></span><br><span class="line">    <span class="comment"># i 的左子树和右子树分别为2i+1 和 2(i+1)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums) / <span class="number">2</span><span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        adjust_heap(nums, i,len(nums))</span><br><span class="line">    <span class="comment"># return nums</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adjust_heap</span><span class="params">(nums, i,size)</span>:</span></span><br><span class="line">    max_index = i</span><br><span class="line">    <span class="comment"># 如果有左子树，且左子树大于父节点，则将最大指针指向左子树</span></span><br><span class="line">    <span class="keyword">if</span> <span class="number">2</span> * i + <span class="number">1</span> &lt; size <span class="keyword">and</span> nums[<span class="number">2</span> * i + <span class="number">1</span>] &gt; nums[max_index]:</span><br><span class="line">        max_index = <span class="number">2</span> * i + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> <span class="number">2</span> * (i + <span class="number">1</span>) &lt; size <span class="keyword">and</span> nums[<span class="number">2</span> * (i + <span class="number">1</span>)] &gt; nums[max_index]:</span><br><span class="line">        max_index = <span class="number">2</span> * (i + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> max_index == i:</span><br><span class="line">        temp = nums[max_index]</span><br><span class="line">        nums[max_index] = nums[i]</span><br><span class="line">        nums[i] = temp</span><br><span class="line">        adjust_heap(nums, max_index,size)</span><br></pre></td></tr></table></figure>
<h3 id="ji-shu-pai-xu">计数排序</h3>
<p><strong>计数排序</strong>的核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。</p>
<h4 id="ji-ben-si-xiang">基本思想</h4>
<p>计数排序(Counting sort) 是一种稳定的排序算法。计数排序使用一个额外的数组C，其中第i个元素是待排序数组A中值等于i的元素的个数。然后根据数组C来将A中的元素排到正确的位置。它<strong>只能对整数进行排序</strong>。<br>
<img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/E4559E2C-6389-4EAB-976A-95289A079EB8.gif" alt="avatar"></p>
<h4 id="suan-fa-bu-zou">算法步骤</h4>
<ol>
<li>找出待排序的数组中最大和最小的元素</li>
<li>统计数组中每个值为i的元素出现的次数，存入数组C的第i项</li>
<li>对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）</li>
<li>反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。</li>
</ol>
<h4 id="fu-za-du-6">复杂度</h4>
<ol>
<li>时间复杂度：O(n+k)</li>
</ol>
<h4 id="dai-ma-7">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_sort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="comment"># 获取arr中的最大值和最小值</span></span><br><span class="line">    max_num = max(arr)</span><br><span class="line">    min_num = min(arr)</span><br><span class="line">    <span class="comment"># 以最大值和最小值的差作为中间数组的长度,并构建中间数组，初始化为0</span></span><br><span class="line">    length = max_num - min_num + <span class="number">1</span></span><br><span class="line">    temp_arr = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(length)]</span><br><span class="line">    <span class="comment"># 创建结果List，存放排序完成的结果</span></span><br><span class="line">    res_arr = list(range(len(arr)))</span><br><span class="line">    <span class="comment"># 第一次循环遍历</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> arr:</span><br><span class="line">        temp_arr[num - min_num] += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 第二次循环遍历</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, length):</span><br><span class="line">        temp_arr[i] = temp_arr[i] + temp_arr[i - <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 第三次循环遍历</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr) - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        res_arr[temp_arr[arr[i] - min_num] - <span class="number">1</span>] = arr[i]</span><br><span class="line">        temp_arr[arr[i] - min_num] -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res_arr</span><br></pre></td></tr></table></figure>
<h3 id="tong-pai-xu">桶排序</h3>
<p>桶排序 是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。</p>
<h4 id="ji-ben-si-xiang-1">基本思想</h4>
<p>假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排</p>
<p><img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/D7FA465B-93A5-493B-BE2B-15375A28FAE2.gif" alt="avatar"></p>
<h4 id="suan-fa-bu-zou-1">算法步骤</h4>
<ol>
<li>根据数据分桶</li>
<li>遍历数组，把数据一个一个放到对应的桶中。</li>
<li>对每个不是空的桶进行排序，可以使用其他排序方法，也可以递归使用桶排序。</li>
<li>从不是空的桶里把排好序的数据拼接起来。</li>
</ol>
<h4 id="fu-za-du-7">复杂度</h4>
<ol>
<li>时间复杂度：最好情况下使用线性时间O(n)，桶排序的时间复杂度，取决与对各个桶之间数据进行排序的时间复杂度，因为其它部分的时间复杂度都为O(n)。很显然，桶划分的越小，各个桶之间的数据越少，排序所用的时间也会越少。</li>
</ol>
<h4 id="dai-ma-8">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k)</span>:</span></span><br><span class="line">        self.key = k;</span><br><span class="line">        self.next = <span class="literal">None</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bucket_sort</span><span class="params">(arr,bucket_num)</span>:</span></span><br><span class="line">    h = [];</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, bucket_num):</span><br><span class="line">        h.append(node(<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(arr)):</span><br><span class="line">        tmp = node(arr[i])</span><br><span class="line">        map = arr[i] / bucket_num</span><br><span class="line">        p = h[map]</span><br><span class="line">        <span class="keyword">if</span> p.key <span class="keyword">is</span> <span class="number">0</span>:</span><br><span class="line">            h[map].next = tmp</span><br><span class="line">            h[map].key = h[map].key + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">while</span> (p.next != <span class="literal">None</span> <span class="keyword">and</span> p.next.key &lt;= tmp.key):</span><br><span class="line">                p = p.next</span><br><span class="line">            tmp.next = p.next</span><br><span class="line">            p.next = tmp</span><br><span class="line">            h[map].key = h[map].key + <span class="number">1</span></span><br><span class="line">    k = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">10</span>):</span><br><span class="line">        q = h[i].next</span><br><span class="line">        <span class="keyword">while</span> (q != <span class="literal">None</span>):</span><br><span class="line">            arr[k] = q.key</span><br><span class="line">            k = k + <span class="number">1</span></span><br><span class="line">            q = q.next</span><br><span class="line">    <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure>
<h3 id="ji-shu-pai-xu-1">基数排序</h3>
<p>基数排序也是非比较的排序算法，对每一位进行排序，从最低位开始排序，复杂度为O(kn),为数组长度，k为数组中的数的最大的位数；</p>
<h4 id="ji-ben-si-xiang-2">基本思想</h4>
<p>基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。基数排序基于分别排序，分别收集，所以是稳定的。<br>
<img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/2BA80F27-FCDC-4BEA-BD0B-2AAB59242752.gif" alt="avatar"></p>
<h4 id="suan-fa-bu-zou-2">算法步骤</h4>
<ol>
<li>取得数组中的最大数，并取得位数；</li>
<li>arr为原始数组，从最低位开始取每个位组成radix数组；</li>
<li>对radix进行计数排序（利用计数排序适用于小范围数的特点）；</li>
</ol>
<h4 id="fu-za-du-8">复杂度</h4>
<ol>
<li>时间复杂度：O（kn）</li>
</ol>
<h4 id="dai-ma-9">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">radix_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    k = len(str(max(nums)))  <span class="comment"># 最大的数的位数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        tong = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            tong[int(num / (<span class="number">10</span> ** i)) % <span class="number">10</span>].append(num)  <span class="comment"># 获取当前排序位数上的数字</span></span><br><span class="line">        <span class="comment">#print(tong)</span></span><br><span class="line">        nums = []</span><br><span class="line">        <span class="keyword">for</span> zitong <span class="keyword">in</span> tong:</span><br><span class="line">            nums = nums + zitong</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>专业技能</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>查找算法</title>
    <url>/2018/02/10/%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="jian-dan-sou-suo-cha-zhao">简单搜索查找</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">brute_force_search</span><span class="params">(nums,key)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">        <span class="keyword">if</span> num == key:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="er-fen-cha-zhao">二分查找</h3>
<p>前提：数组有序</p>
<h4 id="die-dai-ban-ben">迭代版本</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span><span class="params">(nums, key)</span>:</span></span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = len(nums) - <span class="number">1</span></span><br><span class="line">    time = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        time += <span class="number">1</span></span><br><span class="line">        mid = int((low + high) / <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> key &lt; nums[mid]:</span><br><span class="line">            high = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> key &gt; nums[mid]:</span><br><span class="line">            low = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 打印折半的次数</span></span><br><span class="line">            <span class="keyword">print</span> <span class="string">"success! times: %s"</span> % time</span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"fail! times: %s"</span> % time</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h4 id="di-gui-ban-ben">递归版本</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search_recursive</span><span class="params">(nums, item)</span>:</span></span><br><span class="line">    mid = len(nums) // <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> nums[mid] == item:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> mid == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># mid等于0就是找到最后一个元素了。</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> item &gt; nums[mid]:  <span class="comment"># 找后半部分</span></span><br><span class="line">            <span class="comment"># print(lst[mid:])</span></span><br><span class="line">            <span class="keyword">return</span> binary_search_recursive(nums[mid:], item)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> binary_search_recursive(nums[:mid], item)  <span class="comment"># 找前半部分</span></span><br></pre></td></tr></table></figure>
<h3 id="hash-cha-zhao">Hash 查找</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashTable</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size)</span>:</span></span><br><span class="line">        self.elem = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(size)]  <span class="comment"># 使用list数据结构作为哈希表元素保存方法</span></span><br><span class="line">        self.count = size  <span class="comment"># 最大表长</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hash</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> key % self.count  <span class="comment"># 散列函数采用除留余数法</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert_hash</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="string">"""插入关键字到哈希表内"""</span></span><br><span class="line">        address = self.hash(key)  <span class="comment"># 求散列地址</span></span><br><span class="line">        <span class="keyword">while</span> self.elem[address]:  <span class="comment"># 当前位置已经有数据了，发生冲突。</span></span><br><span class="line">            address = (address+<span class="number">1</span>) % self.count  <span class="comment"># 线性探测下一地址是否可用</span></span><br><span class="line">        self.elem[address] = key  <span class="comment"># 没有冲突则直接保存。</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search_hash</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="string">"""查找关键字，返回布尔值"""</span></span><br><span class="line">        star = address = self.hash(key)</span><br><span class="line">        <span class="keyword">while</span> self.elem[address] != key:</span><br><span class="line">            address = (address + <span class="number">1</span>) % self.count</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.elem[address] <span class="keyword">or</span> address == star:  <span class="comment"># 说明没找到或者循环到了开始的位置</span></span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">      </span><br><span class="line">     </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># hash查找</span></span><br><span class="line">    list_a = [<span class="number">12</span>, <span class="number">67</span>, <span class="number">56</span>, <span class="number">16</span>, <span class="number">25</span>, <span class="number">37</span>, <span class="number">22</span>, <span class="number">29</span>, <span class="number">15</span>, <span class="number">47</span>, <span class="number">48</span>, <span class="number">34</span>]</span><br><span class="line">    hash_table = HashTable(len(list_a))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> list_a:</span><br><span class="line">        hash_table.insert_hash(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> hash_table.elem:</span><br><span class="line">        <span class="keyword">if</span> i:</span><br><span class="line">            print(i, hash_table.elem.index(i))</span><br><span class="line"></span><br><span class="line">    print(hash_table.search_hash(<span class="number">15</span>))</span><br><span class="line">    print(hash_table.search_hash(<span class="number">33</span>))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>查找算法</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo-advanced-settings</title>
    <url>/2018/02/10/hexo/hexo-advanced-settings/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2018/02/10/hexo/hexo-advanced-settings/Moments.jpg" alt="These Magic Moments"></p>
<a id="more"></a>
<h1 id="shu-ju-tong-ji">数据统计</h1>
<h2 id="zhan-dian-fang-wen-liang-tong-ji">站点访问量统计</h2>
<p>该功能由 <a href="http://ibruce.info/2015/04/04/busuanzi/" target="_blank" rel="noopener">不蒜子</a> 提供，效果如下图：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511200426274.png" alt></p>
<p>左侧数据表示独立访客数 UV，右侧数据表示网站浏览量 PV，访客数和浏览量的区别在于一个用户连续点击 n 篇文章，会记录 n 次浏览量，但只记录一次访客数。</p>
<p>由于不蒜子是基于域名来进行统计计算的，所以通过 localhost:4000 端口访问的时候统计数据 PV 和 UV 都会异常的大，属于正常现象。</p>
<p>在页脚布局模板文件首行添加如下代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout_partial\footer.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&lt;script <span class="keyword">async</span>=<span class="string">""</span> src=<span class="string">"//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"</span>&gt;</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br></pre></td></tr></table></figure>
<p>在主题配置文件中做出如下修改：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">busuanzi_count:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">total_visitors:</span> <span class="literal">true</span> <span class="comment"># 访客数</span></span><br><span class="line">  <span class="attr">total_visitors_icon:</span> <span class="string">user</span></span><br><span class="line">  <span class="attr">total_views:</span> <span class="literal">true</span> <span class="comment"># 访问量</span></span><br><span class="line">  <span class="attr">total_views_icon:</span> <span class="string">eye</span></span><br><span class="line">  <span class="attr">post_views:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">post_views_icon:</span> <span class="string">eye</span></span><br></pre></td></tr></table></figure>
<p>刷新浏览器即可生效。</p>
<p>高阶用法：通过修改代码来自定义统计文案，如果你想使用本站统计文案，需要对不蒜子的代码做出如下修改：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout_third-party\analytics\busuanzi-counter.swig</span></figcaption><table><tr><td class="code"><pre><span class="line"> &#123;% if theme.busuanzi_count.total_visitors %&#125;</span><br><span class="line"><span class="deletion">-   &lt;span class="site-uv" title="&#123;&#123; __('footer.total_visitors') &#125;&#125;"&gt;</span></span><br><span class="line"><span class="addition">+   &lt;span class="site-uv"&gt;</span></span><br><span class="line"><span class="addition">+     &#123;&#123; __('footer.total_visitors', '&lt;span class="busuanzi-value" id="busuanzi_value_site_uv"&gt;&lt;/span&gt;') &#125;&#125;</span></span><br><span class="line"><span class="deletion">-     &lt;i class="fa fa-&#123;&#123; theme.busuanzi_count.total_visitors_icon &#125;&#125;"&gt;&lt;/i&gt;</span></span><br><span class="line"><span class="deletion">-     &lt;span class="busuanzi-value" id="busuanzi_value_site_uv"&gt;&lt;/span&gt;</span></span><br><span class="line">    &lt;/span&gt;</span><br><span class="line">  &#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">  &#123;% if theme.busuanzi_count.total_views %&#125;</span><br><span class="line"><span class="deletion">-   &lt;span class="site-pv" title="&#123;&#123; __('footer.total_views') &#125;&#125;"&gt;</span></span><br><span class="line"><span class="addition">+   &lt;span class="site-pv"&gt;</span></span><br><span class="line"><span class="addition">+     &#123;&#123; __('footer.total_views', '&lt;span class="busuanzi-value" id="busuanzi_value_site_pv"&gt;&lt;/span&gt;') &#125;&#125;</span></span><br><span class="line"><span class="deletion">-     &lt;i class="fa fa-&#123;&#123; theme.busuanzi_count.total_views_icon &#125;&#125;"&gt;&lt;/i&gt;</span></span><br><span class="line"><span class="deletion">-     &lt;span class="busuanzi-value" id="busuanzi_value_site_pv"&gt;&lt;/span&gt;</span></span><br><span class="line">    &lt;/span&gt;</span><br><span class="line">  &#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<p>在自定义样式文件中添加如下样式：</p>
<figure class="highlight css"><figcaption><span>themes/next/source/css/_custom/custom.styl</span></figcaption><table><tr><td class="code"><pre><span class="line">//修改不蒜子数据颜色</span><br><span class="line"><span class="selector-class">.busuanzi-value</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#1890ff</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后修改统计表述文案：</p>
<figure class="highlight yml"><figcaption><span>themes\next\languages\zh-CN.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">footer:</span></span><br><span class="line">  <span class="attr">total_views:</span> <span class="string">"历经 %s 次回眸才与你相遇"</span></span><br><span class="line">  <span class="attr">total_visitors:</span> <span class="string">"我的第 %s 位朋友，"</span></span><br></pre></td></tr></table></figure>
<h2 id="zhan-dian-yun-xing-shi-jian-tong-ji">站点运行时间统计</h2>
<p>在站点底部显示站点已运行时间，效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511201038905.png" alt></p>
<p>在主题自定义布局文件中添加以下代码：</p>
<figure class="highlight js"><figcaption><span>thems\next\layout_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;# 页脚站点运行时间统计 #&#125; &#123;% if theme.footer.ages.enable %&#125;</span><br><span class="line">&lt;script src=<span class="string">"https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">&lt;script src="https:/</span><span class="regexp">/cdn.jsdelivr.net/</span>npm/moment-precise-range-plugin@<span class="number">1.3</span><span class="number">.0</span>/moment-precise-range.min.js<span class="string">"&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="string">&lt;script&gt;</span></span><br><span class="line"><span class="string">  function timer() &#123;</span></span><br><span class="line"><span class="string">    var ages = moment.preciseDiff(moment(),moment(&#123;&#123; theme.footer.ages.birthday &#125;&#125;,"</span>YYYYMMDD<span class="string">"));</span></span><br><span class="line"><span class="string">    ages = ages.replace(/years?/, "</span>年<span class="string">");</span></span><br><span class="line"><span class="string">    ages = ages.replace(/months?/, "</span>月<span class="string">");</span></span><br><span class="line"><span class="string">    ages = ages.replace(/days?/, "</span>天<span class="string">");</span></span><br><span class="line"><span class="string">    ages = ages.replace(/hours?/, "</span>小时<span class="string">");</span></span><br><span class="line"><span class="string">    ages = ages.replace(/minutes?/, "</span>分<span class="string">");</span></span><br><span class="line"><span class="string">    ages = ages.replace(/seconds?/, "</span>秒<span class="string">");</span></span><br><span class="line"><span class="string">    ages = ages.replace(/\d+/g, '&lt;span style="</span>color:&#123;&#123; theme.footer.ages.color &#125;&#125;<span class="string">"&gt;$&amp;&lt;/span&gt;');</span></span><br><span class="line"><span class="string">    div.innerHTML = `&#123;&#123; __('footer.age')&#125;&#125; $&#123;ages&#125;`;</span></span><br><span class="line"><span class="string">  &#125;</span></span><br><span class="line"><span class="string">  var div = document.createElement("</span>div<span class="string">");</span></span><br><span class="line"><span class="string">  //插入到copyright之后</span></span><br><span class="line"><span class="string">  var copyright = document.querySelector("</span>.copyright<span class="string">");</span></span><br><span class="line"><span class="string">  document.querySelector("</span>.footer-inner<span class="string">").insertBefore(div, copyright.nextSibling);</span></span><br><span class="line"><span class="string">  timer();</span></span><br><span class="line"><span class="string">  setInterval("</span>timer()<span class="string">",1000)</span></span><br><span class="line"><span class="string">&lt;/script&gt;</span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure>
<p>如果 custom.swig 文件不存在，需要手动新建并在布局页面中 body 末尾引入：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout_layout.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">      ...</span><br><span class="line">      &#123;% include '_third-party/exturl.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/bookmark.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/copy-code.swig' %&#125;</span><br><span class="line"></span><br><span class="line"><span class="addition">+     &#123;% include '_custom/custom.swig' %&#125;</span></span><br><span class="line">    &lt;/body&gt;</span><br><span class="line">  &lt;/html&gt;</span><br></pre></td></tr></table></figure>
<p>修改主题配置文件：</p>
<figure class="highlight diff"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">  footer:</span><br><span class="line">    ...</span><br><span class="line"><span class="addition">+   ages:</span></span><br><span class="line"><span class="addition">+     # site running time</span></span><br><span class="line"><span class="addition">+     enable: true</span></span><br><span class="line"><span class="addition">+     # birthday of your site</span></span><br><span class="line"><span class="addition">+     birthday: 20181001</span></span><br><span class="line"><span class="addition">+     # color of number</span></span><br><span class="line"><span class="addition">+     color: "#1890ff"</span></span><br></pre></td></tr></table></figure>
<p>然后补全对应文案：</p>
<figure class="highlight diff"><figcaption><span>themes\next\languages\zh-CN.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">  footer:</span><br><span class="line">    powered: "由 %s 强力驱动"</span><br><span class="line">    theme: 主题</span><br><span class="line"><span class="addition">+   age: 我已在此等候你</span></span><br><span class="line">    total_views: "历经 %s 次回眸才与你相遇"</span><br><span class="line">    total_visitors: "我的第 %s 位朋友，"</span><br></pre></td></tr></table></figure>
<p>刷新浏览器即可生效。</p>
<h2 id="wen-zhang-fang-wen-liang-tong-ji">文章访问量统计</h2>
<p>该功能基于 <a href="https://leancloud.cn/" target="_blank" rel="noopener">LeanCloud</a> 提供后端数据服务，效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511201401441.png" alt></p>
<p>在 LeanCloud 上注册账号并创建应用，新建一个名为 Counter 的 Class，ACL 权限设置为 <strong>无限制</strong>：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511201501222.png" alt></p>
<p>在 LeanCloud 中的 Class 可以理解为数据库中的数据表。Counter 用于存储记录文章访问量，记录是以 url 作为唯一依据的，所以根据默认的 permalink 组成结构，如果你更改了文章的发布日期和标题中的任意一个，都会造成文章阅读数值的清零重计。</p>
<p>在控制台的 <strong>设置</strong> -&gt; <strong>应用 Key</strong> 中获取 App ID 和 App Key 填入到主题配置文件中：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">leancloud_visitors:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">app_id:</span> <span class="string">***&lt;app_id***</span></span><br><span class="line">  <span class="attr">app_key:</span> <span class="string">***&lt;app_key&gt;***</span></span><br><span class="line">  <span class="attr">security:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">betterPerformance:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p>站点上线后可以在 <strong>设置</strong> -&gt; <strong>安全中心</strong> 中添加博客域名到 Web 安全域名中，设置后仅可在该域名下通过 JavaScript SDK 调用服务器资源，借以保护 LeanCloud 应用的数据安全。</p>
<p>如果想要自定义 PV 表述文案，可以修改文章布局模板中的相关代码：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout_macro\post.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">  &#123;# LeanCould PageView #&#125;</span><br><span class="line">    ...</span><br><span class="line">    &#123;% if theme.post_meta.item_text %&#125;</span><br><span class="line"><span class="deletion">-     &lt;span class="post-meta-item-text"&gt;&#123;&#123;__('post.views') + __('symbol.colon') &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="addition">+     &lt;span class="post-meta-item-text"&gt;&#123;&#123;__('post.views')&#125;&#125; &lt;/span&gt;</span></span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line">    &lt;span class="leancloud-visitors-count"&gt;&lt;/span&gt;</span><br><span class="line"><span class="addition">+   &lt;span&gt;℃&lt;/span&gt;</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>修改统计表述文案：</p>
<figure class="highlight yml"><figcaption><span>themes/next/languages/zh-CN.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">post:</span></span><br><span class="line">  <span class="attr">views:</span> <span class="string">阅读次数</span></span><br></pre></td></tr></table></figure>
<p>刷新浏览器即可生效。</p>
<p>如果遇到如下报错，可能是你配置了 <code>security: true</code> 但又没有做好安全策略配置。</p>
<blockquote>
<p>阅读次数： Counter not initialized! See more at console err msg.</p>
</blockquote>
<p>有以下两种解决方案：</p>
<ul>
<li>下载安装 <a href="https://github.com/theme-next/hexo-leancloud-counter-security" target="_blank" rel="noopener">hexo-leancloud-counter-security</a> 插件</li>
<li>在主题配置中设置 <code>security: false</code></li>
</ul>
<p>个人推荐第二种，简单粗暴。</p>
<p>除了 LeanCloud，不蒜子也能提供文章阅读次数统计，但是不蒜子的统计结果只会在文章页显示，而不会显示在首页列表中，相关讨论可以参见 <a href="https://github.com/iissnan/hexo-theme-next/issues/801" target="_blank" rel="noopener">阅读计数。对比 LeanCloud 和不蒜子</a></p>
<h2 id="zhan-dian-ji-wen-zhang-zi-shu-tong-ji">站点及文章字数统计</h2>
<p>该功能由 <a href="https://github.com/theme-next/hexo-symbols-count-time" target="_blank" rel="noopener">hexo-symbols-count-time</a> 提供，效果如下图：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511201945103.png" alt></p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511202019299.png" alt></p>
<p>在根目录下执行如下命令安装相关依赖</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ npm <span class="keyword">install</span> hexo-symbols-<span class="keyword">count</span>-<span class="built_in">time</span> <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
<p>启用该功能需要同时修改站点配置文件和主题配置文件。</p>
<p>将如下配置项添加到<strong>站点配置文件</strong>中，这些配置项主要用于控制每项统计信息是否显示。</p>
<figure class="highlight yml"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">symbols_count_time:</span></span><br><span class="line">  <span class="attr">symbols:</span> <span class="literal">true</span> <span class="comment"># 统计单篇文章字数</span></span><br><span class="line">  <span class="attr">time:</span> <span class="literal">false</span> <span class="comment"># 取消估算单篇文章阅读时间</span></span><br><span class="line">  <span class="attr">total_symbols:</span> <span class="literal">true</span> <span class="comment"># 统计站点总字数</span></span><br><span class="line">  <span class="attr">total_time:</span> <span class="literal">false</span> <span class="comment"># 取消估算站点总阅读时间</span></span><br></pre></td></tr></table></figure>
<p>在<strong>主题配置文件</strong>中做如下修改，这些配置项主要用于控制统计信息的显示样式。</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">symbols_count_time:</span></span><br><span class="line">  <span class="attr">separated_meta:</span> <span class="literal">false</span> <span class="comment"># 统计信息不换行显示</span></span><br><span class="line">  <span class="attr">item_text_post:</span> <span class="literal">true</span> <span class="comment"># 文章统计信息中是否显示“本文字数/阅读时长”等描述文字</span></span><br><span class="line">  <span class="attr">item_text_total:</span> <span class="literal">true</span> <span class="comment"># 站点统计信息中是否显示“本文字数/阅读时长”等描述文字</span></span><br><span class="line">  <span class="attr">awl:</span> <span class="number">4</span> <span class="comment"># Average Word Length：平均字符长度</span></span><br><span class="line">  <span class="attr">wpm:</span> <span class="number">275</span> <span class="comment"># Words Per Minute：阅读速度</span></span><br></pre></td></tr></table></figure>
<p>汉字的平均字符长度为 1.5，如果在文章中使用纯中文进行写作（没有混杂英文），那么推荐设置 <code>awl: 2</code> 及 <code>wpm: 300</code>，但是如果文章中存在英文，建议设置 <code>awl: 4</code> 及 <code>wpm: 275</code>。</p>
<p>因为修改了站点配置文件，所以需要重新启动服务器才能生效。</p>
<h1 id="zhan-dian-ge-xing-hua-she-zhi">站点个性化设置</h1>
<h2 id="gao-guai-wang-ye-biao-ti">搞怪网页标题</h2>
<p>离开和进入页面时动态修改 Tab 标签中的标题。</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/title-trick.png" alt></p>
<p>在主题自定义布局文件中添加以下代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;# 搞怪网页标题 #&#125; &#123;% if theme.title_trick.enable %&#125;</span><br><span class="line">&lt;script&gt;</span><br><span class="line">  <span class="keyword">var</span> OriginTitile = <span class="built_in">document</span>.title;</span><br><span class="line">  <span class="keyword">var</span> titleTime;</span><br><span class="line">  <span class="built_in">document</span>.addEventListener(<span class="string">"visibilitychange"</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">document</span>.hidden) &#123;</span><br><span class="line">      <span class="built_in">document</span>.title = <span class="string">"&#123;&#123; theme.title_trick.leave &#125;&#125;"</span> + OriginTitile;</span><br><span class="line">      clearTimeout(titleTime);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="built_in">document</span>.title = <span class="string">"&#123;&#123; theme.title_trick.enter &#125;&#125;"</span> + OriginTitile;</span><br><span class="line">      titleTime = setTimeout(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">        <span class="built_in">document</span>.title = OriginTitile;</span><br><span class="line">      &#125;, <span class="number">2000</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">&#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure>
<p>如果 custom.swig 文件不存在，需要手动新建并在布局页面中 body 末尾引入：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout_layout.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">      ...</span><br><span class="line">      &#123;% include '_third-party/exturl.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/bookmark.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/copy-code.swig' %&#125;</span><br><span class="line"></span><br><span class="line"><span class="addition">+     &#123;% include '_custom/custom.swig' %&#125;</span></span><br><span class="line">    &lt;/body&gt;</span><br><span class="line">  &lt;/html&gt;</span><br></pre></td></tr></table></figure>
<p>在主题配置文件中添加以下代码：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># a trick on website title</span></span><br><span class="line"><span class="attr">title_trick:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">leave:</span> <span class="string">"(つェ⊂)我藏好了哦~"</span></span><br><span class="line">  <span class="attr">enter:</span> <span class="string">"(*´∇｀*) 被你发现啦~"</span></span><br></pre></td></tr></table></figure>
<h2 id="zhan-nei-sou-suo">站内搜索</h2>
<p>该功能由 <a href="https://github.com/theme-next/hexo-generator-searchdb" target="_blank" rel="noopener">hexo-generator-searchdb</a> 提供，效果如下图：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511202645563.png" alt></p>
<p>在根目录下执行以下命令安装相关依赖：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ npm <span class="keyword">install</span> hexo-generator-searchdb <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
<p>在<strong>主题配置</strong>文件中修改相关字段：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">local_search:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">trigger:</span> <span class="string">auto</span> <span class="comment"># 每次输入改变都执行搜索</span></span><br><span class="line">  <span class="attr">top_n_per_article:</span> <span class="number">3</span> <span class="comment"># 每篇文章显示的搜索结果数量</span></span><br><span class="line">  <span class="attr">unescape:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p>在站点配置文件中添加以下字段：</p>
<figure class="highlight yml"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">search:</span></span><br><span class="line">  <span class="attr">path:</span> <span class="string">search.xml</span></span><br><span class="line">  <span class="attr">field:</span> <span class="string">post</span> <span class="comment"># 指定搜索范围，可选 post | page | all</span></span><br><span class="line">  <span class="attr">format:</span> <span class="string">html</span> <span class="comment"># 指定页面内容形式，可选 html | raw (Markdown) | excerpt | more</span></span><br><span class="line">  <span class="attr">limit:</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure>
<p>在自定义样式文件中添加如下样式规则来增加搜索弹窗的页边距：</p>
<figure class="highlight css"><figcaption><span>themes\next\source\css_custom\custom.styl</span></figcaption><table><tr><td class="code"><pre><span class="line">//增加搜索弹窗的页边距</span><br><span class="line"><span class="selector-class">.local-search-popup</span> <span class="selector-id">#local-search-result</span> &#123;</span><br><span class="line">  <span class="attribute">padding</span>: <span class="number">25px</span> <span class="number">40px</span></span><br><span class="line">  height: <span class="built_in">calc</span>(<span class="number">100%</span> - <span class="number">95px</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果你同时在站点内启用了 wobblewindow 边缘摆动效果，则有可能会出现背景蒙版叠加在弹窗之前的问题，这种层级叠加异常的问题主要是因为 wobblewindow 中修改了弹窗父元素的 <code>position</code> 定位和 <code>z-index</code> 优先级，目前只能通过修改 localsearch 源码来修复该 Bug：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout_third-party\search\localsearch.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">  $.ajax(&#123;</span><br><span class="line">    url: path,</span><br><span class="line">    dataType: isXml ? <span class="string">"xml"</span> : <span class="string">"json"</span>,</span><br><span class="line">    <span class="keyword">async</span>: <span class="literal">true</span>,</span><br><span class="line">    success: <span class="function"><span class="keyword">function</span>(<span class="params">res</span>) </span>&#123;</span><br><span class="line">      <span class="comment">// get the contents from search data</span></span><br><span class="line">      isfetched = <span class="literal">true</span>;</span><br><span class="line">-     $(<span class="string">'.popup'</span>).detach().appendTo(<span class="string">'.header-inner'</span>);</span><br><span class="line">+     $(<span class="string">'.popup'</span>).detach().appendTo(<span class="string">'body'</span>);</span><br><span class="line">      <span class="keyword">var</span> datas = isXml ? $(<span class="string">"entry"</span>, res).map(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">          title: $(<span class="string">"title"</span>, <span class="keyword">this</span>).text(),</span><br><span class="line">          content: $(<span class="string">"content"</span>,<span class="keyword">this</span>).text(),</span><br><span class="line">          url: $(<span class="string">"url"</span> , <span class="keyword">this</span>).text()</span><br><span class="line">        &#125;;</span><br><span class="line">      &#125;).get() : res;</span><br><span class="line">      ...</span><br></pre></td></tr></table></figure>
<h2 id="re-men-wen-zhang-pai-xing-bang">热门文章排行榜</h2>
<p>本章节部分思路参考 <a href="https://thief.one/2017/03/03/Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">nMask | Hexo 搭建博客教程 #7.16</a>，自行进行了代码重构。</p>
<p>该功能同样是基于 LeanCloud 提供的后端服务支持。具体实现方案如下：</p>
<p>在站点目录下执行以下命令新建页面</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">$ hexo new<span class="built_in"> page </span>top</span><br></pre></td></tr></table></figure>
<p>在主题配置文件中新增一项菜单入口：</p>
<figure class="highlight diff"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">  menu:</span><br><span class="line">    home: / || home</span><br><span class="line"><span class="addition">+   top: /top/ || signal</span></span><br><span class="line">    tags: /tags/ || tags</span><br><span class="line">    categories: /categories/ || th</span><br><span class="line">    archives: /archives/ || archive</span><br><span class="line">    about: /about/ || user</span><br></pre></td></tr></table></figure>
<p>在语言包中新增菜单中文：</p>
<figure class="highlight diff"><figcaption><span>themes\next\languages\zh-CN.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">  menu:</span><br><span class="line">    home: 首页</span><br><span class="line">    archives: 归档</span><br><span class="line">    categories: 分类</span><br><span class="line">    tags: 标签</span><br><span class="line">    about: 关于</span><br><span class="line"><span class="addition">+   top: 排行榜</span></span><br></pre></td></tr></table></figure>
<p>然后在新增的排行榜页面内添加以下内容：</p>
<figure class="highlight js"><figcaption><span>source\top\index.md</span></figcaption><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: 热门文章Top <span class="number">10</span></span><br><span class="line">comments: <span class="literal">false</span></span><br><span class="line">date: <span class="number">2019</span><span class="number">-10</span><span class="number">-30</span> <span class="number">00</span>:<span class="number">54</span>:<span class="number">50</span></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">&lt;div id=<span class="string">"post-rank"</span>&gt;&lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">&lt;script src="/</span><span class="regexp">/cdn.jsdelivr.net/</span>npm/leancloud-storage@<span class="number">3.10</span><span class="number">.0</span>/dist/av-min.js<span class="string">"&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="string">&lt;script&gt;</span></span><br><span class="line"><span class="string">  var APP_ID = ******;  //输入个人LeanCloud账号AppID</span></span><br><span class="line"><span class="string">  var APP_KEY = ******;  //输入个人LeanCloud账号AppKey</span></span><br><span class="line"><span class="string">  AV.init(&#123;</span></span><br><span class="line"><span class="string">    appId: APP_ID,</span></span><br><span class="line"><span class="string">    appKey: APP_KEY</span></span><br><span class="line"><span class="string">  &#125;);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  var query = new AV.Query('Counter');//表名</span></span><br><span class="line"><span class="string">  query.descending('time'); //结果按阅读次数降序排序</span></span><br><span class="line"><span class="string">  query.limit(10);  //最终只返回10条结果</span></span><br><span class="line"><span class="string">  query.find().then( response =&gt; &#123;</span></span><br><span class="line"><span class="string">    var content = response.reduce( (accum, &#123;attributes&#125;) =&gt; &#123;</span></span><br><span class="line"><span class="string">      accum += `&lt;p&gt;&lt;div class="</span>prefix<span class="string">"&gt;热度 $&#123;attributes.time&#125; ℃&lt;/div&gt;&lt;div&gt;&lt;a href="</span>$&#123;attributes.url&#125;<span class="string">"&gt;$&#123;attributes.title&#125;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;`</span></span><br><span class="line"><span class="string">      return accum;</span></span><br><span class="line"><span class="string">    &#125;,"</span><span class="string">")</span></span><br><span class="line">    document.querySelector("#post-rank").innerHTML = content;</span><br><span class="line">  &#125;)</span><br><span class="line">  .catch( <span class="function"><span class="params">error</span> =&gt;</span> &#123;</span><br><span class="line">    <span class="built_in">console</span>.log(error);</span><br><span class="line">  &#125;);</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">&lt;style type="text/</span>css<span class="string">"&gt;</span></span><br><span class="line"><span class="string">  #post-rank &#123;</span></span><br><span class="line"><span class="string">    text-align: center;</span></span><br><span class="line"><span class="string">  &#125;</span></span><br><span class="line"><span class="string">  #post-rank .prefix &#123;</span></span><br><span class="line"><span class="string">    color: #ff4d4f;</span></span><br><span class="line"><span class="string">  &#125;</span></span><br><span class="line"><span class="string">&lt;/style&gt;</span></span><br></pre></td></tr></table></figure>
<div class="note danger">
            <p>该功能控制台有bug，但是不影响使用</p>
          </div>
<h2 id="dou-ban-yue-du-dian-ying-you-xi">豆瓣阅读 / 电影 / 游戏</h2>
<p>本章节参考 <a href="https://github.com/mythsman/hexo-douban" target="_blank" rel="noopener">mythsman/hexo-douban README.md</a>。为站点添加豆瓣阅读 / 电影 / 游戏页面，效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511203614743.png" alt></p>
<p>在根目录下执行以下命令安装相关依赖：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ npm <span class="keyword">install</span> hexo-douban <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
<p>在站点配置文件中添加以下内容：</p>
<figure class="highlight yml"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">douban:</span></span><br><span class="line">  <span class="attr">user:</span> <span class="comment"># 个人豆瓣ID</span></span><br><span class="line">  <span class="attr">builtin:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">book:</span></span><br><span class="line">    <span class="attr">title:</span> <span class="string">"This is my book title"</span></span><br><span class="line">    <span class="attr">quote:</span> <span class="string">"This is my book quote"</span></span><br><span class="line">  <span class="attr">movie:</span></span><br><span class="line">    <span class="attr">title:</span> <span class="string">"This is my movie title"</span></span><br><span class="line">    <span class="attr">quote:</span> <span class="string">"This is my movie quote"</span></span><br><span class="line">  <span class="attr">game:</span></span><br><span class="line">    <span class="attr">title:</span> <span class="string">"This is my game title"</span></span><br><span class="line">    <span class="attr">quote:</span> <span class="string">"This is my game quote"</span></span><br><span class="line">  <span class="attr">timeout:</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure>
<ul>
<li>user: 填写豆瓣 ID。登陆豆瓣后点击<strong>个人主页</strong>，此时 url 中最后一段即是用户 ID，一般情况下会是一段数字，如果设置了个人域名的话，则个人域名即为 ID。</li>
<li>builtin: 是否将生成页面的功能嵌入 <code>hexo s</code> 和 <code>hexo g</code> 中。</li>
<li>timeout: 爬取数据的超时时间。</li>
</ul>
<p>如果只想生成某一个页面（比如只生成读书页面），把其他的配置项注释掉即可。</p>
<p>在主题配置文件中新增菜单入口：</p>
<figure class="highlight diff"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">  menu:</span><br><span class="line">    home: / || home</span><br><span class="line">    tags: /tags/ || tags</span><br><span class="line">    categories: /categories/ || th</span><br><span class="line">    archives: /archives/ || tasks</span><br><span class="line"><span class="addition">+   books: /books/ || book</span></span><br><span class="line"><span class="addition">+   movies: /movies/ || video-camera</span></span><br><span class="line"><span class="addition">+   games: /games/ || gamepad</span></span><br></pre></td></tr></table></figure>
<p>在语言包中新增菜单中文：</p>
<figure class="highlight diff"><figcaption><span>themes\next\language\zh_CN.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">  menu:</span><br><span class="line">    home: 首页</span><br><span class="line">    archives: 归档</span><br><span class="line">    categories: 分类</span><br><span class="line">    tags: 标签</span><br><span class="line"><span class="addition">+   movies: 电影</span></span><br><span class="line"><span class="addition">+   books: 读书</span></span><br><span class="line"><span class="addition">+   games: 游戏</span></span><br></pre></td></tr></table></figure>
<p>然后在根目录下执行以下命令生成豆瓣阅读 / 电影 / 游戏页面：</p>
<p>可选参数:</p>
<ul>
<li>-b | --books: 只生成豆瓣读书页面</li>
<li>-m | --movies: 只生成豆瓣电影页面</li>
<li>-g | --games: 只生成豆瓣游戏页面</li>
</ul>
<p>执行命令后，插件会根据用户提供的 ID 爬取豆瓣中的数据信息并在 <code>public</code> 目录下生成对应的页面，当服务器启动或部署后会将页面显示在对应的菜单路由下。</p>
<p>如果在站点配置中设置了 <code>douban.builtin: false</code>，则每次豆瓣数据变动后需要手动执行一次 <code>hexo douban</code> 来刷新页面数据。如果设置了 <code>douban.builtin: true</code>，则每次执行 <code>hexo s</code> 和 <code>hexo g</code> 的时候将会自动同时执行 <code>hexo douban</code> 命令，但这样可能会增加打包编译的时间。建议如果豆瓣数据变动不频繁的情况下该项设为 <code>false</code> 即可。</p>
<p>通常大家都喜欢用 <code>hexo d</code> 来作为 <code>hexo deploy</code> 命令的简化，但是当安装了 <code>hexo douban</code> 之后， <code>hexo d</code> 就会有歧义而无法执行，因为 <code>hexo douban</code> 跟 <code>hexo deploy</code> 的 Alias 都是 <code>hexo d</code>。</p>
<h2 id="zai-xian-liao-tian">在线聊天</h2>
<p>在线聊天算是一个比较成熟的 SaaS 商业应用了，业内产品如 <a href="https://www.tidiochat.com/" target="_blank" rel="noopener">Tidio</a>、 <a href="https://talkjs.com/" target="_blank" rel="noopener">TalkJS</a>、<a href="https://www.intercom.com/" target="_blank" rel="noopener">Intercom</a>、<a href="https://www.tawk.to/" target="_blank" rel="noopener">tawk.to</a> 等，使用体验都很好，交互界面也很干净别致。经过比较，本站最终选择了 Tidio：</p>
<ul>
<li>
<p>在个人博客这种业务场景中，几乎用不到它的收费功能，可以算是终身免费了。</p>
</li>
<li>
<p>Tidio 提供了多种消息回复渠道，包括网页、桌面应用、iOS/Android APP（需要 Google play 服务支持）。</p>
</li>
<li>
<p>除了在线聊天，Tidio 还可以在线发送邮件，以及关联接收 Fackbook 消息。</p>
</li>
<li>
<p>在几款产品的界面风格中，还是 Tidio 看起来更加优雅一些，深得我爱。</p>
</li>
</ul>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511204929049.png" alt></p>
<p>首先需要<a href="https://www.tidiochat.com/panel/login" target="_blank" rel="noopener">注册 Tidio 账号</a>，根据引导填写应用信息。进入控制台后，在 <strong>SETTINGS</strong> -&gt; <strong>Developer</strong> -&gt; <strong>Project data</strong> 中获取到 Public Key：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511205128090.png" alt></p>
<p>在主题配置文件下添加以下代码并补全 Public Key：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Tidio online chat</span></span><br><span class="line"><span class="comment"># see: https://www.tidiochat.com</span></span><br><span class="line"><span class="attr">tidio:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">key:</span> <span class="comment"># Public_Key</span></span><br></pre></td></tr></table></figure>
<p>在主题自定义布局文件中添加以下代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;# Tidio 在线联系功能 #&#125; &#123;% if theme.tidio.enable %&#125;</span><br><span class="line">&lt;script <span class="keyword">async</span> src=<span class="string">"//code.tidio.co/&#123;&#123; theme.tidio.key &#125;&#125;.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">&#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure>
<p>为避免代码加载阻塞页面渲染，需要为脚本添加 <code>async</code> 属性使其异步加载。</p>
<p>刷新页面即可在右下角看到 Tidio 的会话标志了。接下来可以在 Tidio 控制台的 <strong>Channel</strong> -&gt; <strong>Live chat</strong> -&gt; <strong>Appearance</strong> 中根据提示定制聊天对话框的主题外观和语言包：</p>
<h2 id="xing-wei-jian-ce-yu-fan-kui">行为监测与反馈</h2>
<p><a href="https://www.hotjar.com/" target="_blank" rel="noopener">Hotjar</a> 是一款轻量级的监测分析工具，能够提供用户行为监测和用户反馈分析，相比 Google Analysis 而言，它没有复杂的监测指标与分析报表，更加的简单实用，并且为免费用户提供 2000pv/day 的数据采集服务，适用于小型网站或个人博客的监测分析。</p>
<p>Hotjar 主要提供 <strong>ANALYTICS</strong> 和 <strong>FEEDBACK</strong> 两大类服务。</p>
<p>ANALYTICS 主要用于用户交互行为的监测分析，属于客观分析，包括以下四项具体功能：</p>
<ul>
<li>Heatmaps: 通过热力图可视化用户的鼠标交互行为，帮助理解用户动机和需求。</li>
<li>Recording: 记录用户在站点的行为轨迹，了解应用的可用性以及用户遭遇的问题。</li>
<li>Funnels: 记录每个页面或者步骤的用户流失率。</li>
<li>Forms: 记录表单中每一项输入的完成率，完成时间以及用户流失率。</li>
</ul>
<p>FEEDBACK 主要为用户提供反馈渠道，收集用户观点与数据，属于主观分析，包括以下四项具体功能：</p>
<ul>
<li>Incoming: 即时反馈，了解用户对页面的评价。</li>
<li>Polls: 投票反馈，获取某个问题的用户答案。</li>
<li>Surveys: 问卷调查，以问卷形式获取用户反馈。</li>
<li>Recruiters: 获取用户信息，招募用户用于用户调查或测试反馈。</li>
</ul>
<p>Hotjar 通过以上八项具体而实用的功能为用户提供主客观相结合的监测分析服务，可以说它是所有轻量级分析工具中唯一做到了主客观相结合的，同时也是所有主客观分析工具中，做的最轻量的。</p>
<p>读者可以通过该渠道评价页面或者提交勘误，点击悬挂在屏幕右侧的 Feedback 按钮弹出对话框，点击人物头像评价后将会跳转到如下界面</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/hotjar-feedback.png" alt></p>
<p>你可以在此页面输入反馈内容，并通过点击左下角的按钮在当前页面上标识目标元素，之后 hotjar 会将反馈内容连同带有高亮标识的页面截图一起提交到后台。</p>
<p>在站点中集成 Hotjar 的各项功能，需要先 <a href="https://insights.hotjar.com/register" target="_blank" rel="noopener">注册 Hotjar 账号</a>，根据指引一步步填写站点信息，然后在控制面板首页中获取 site ID，在主题配置文件下添加以下代码并补全 site ID：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Hotjar</span></span><br><span class="line"><span class="comment"># see: https://www.hotjar.com/</span></span><br><span class="line"><span class="attr">hotjar:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">siteID:</span> <span class="comment"># site ID</span></span><br></pre></td></tr></table></figure>
<p>在主题自定义布局文件中添加以下代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;# hotjar 页面反馈 #&#125; &#123;% if theme.hotjar.enable %&#125;</span><br><span class="line">&lt;script&gt;</span><br><span class="line">  (<span class="function"><span class="keyword">function</span>(<span class="params">h,o,t,j,a,r</span>)</span>&#123;</span><br><span class="line">    h.hj=h.hj||<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;(h.hj.q=h.hj.q||[]).push(<span class="built_in">arguments</span>)&#125;;</span><br><span class="line">    h._hjSettings=&#123;<span class="attr">hjid</span>:&#123;&#123; theme.hotjar.siteID &#125;&#125;,<span class="attr">hjsv</span>:<span class="number">6</span>&#125;;</span><br><span class="line">    a=o.getElementsByTagName(<span class="string">'head'</span>)[<span class="number">0</span>];</span><br><span class="line">    r=o.createElement(<span class="string">'script'</span>);r.async=<span class="number">1</span>;</span><br><span class="line">    r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;</span><br><span class="line">    a.appendChild(r);</span><br><span class="line">  &#125;)(<span class="built_in">window</span>,<span class="built_in">document</span>,<span class="string">'https://static.hotjar.com/c/hotjar-'</span>,<span class="string">'.js?sv='</span>);</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">&#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure>
<p>如此即可将 Hotjar 嵌入到站内，接下来在 Hotjar 控制台菜单中点击 Incoming，然后根据引导一步步配置即时反馈服务即可。</p>
<h1 id="wen-zhang-ye-mian-ge-xing-hua-she-zhi">文章页面个性化设置</h1>
<h2 id="tian-jia-ping-lun-gong-neng">添加评论功能</h2>
<p>Next 支持多款评论系统：</p>
<ul>
<li><a href="https://disqus.com/" target="_blank" rel="noopener">Disqus</a>：欧美 UI 风格，支持 Tweet、Facebook 等国外社交软件的三方登陆和一键分享。 <a href="https://blog.disqus.com/disqus-welcomes-the-spruce" target="_blank" rel="noopener">Demo</a></li>
<li><a href="https://github.com/imsun/gitment" target="_blank" rel="noopener">gitment</a>：必须用 github 账号登陆才能评论，支持 Markdown 语法，与 github issues 页面风格一致 <a href="https://imsun.github.io/gitment/" target="_blank" rel="noopener">Demo</a></li>
<li><a href="https://valine.js.org/" target="_blank" rel="noopener">Valine</a>：支持匿名评论，支持 Markdown 语法，界面简洁美观</li>
<li><a href="http://changyan.kuaizhan.com/" target="_blank" rel="noopener">畅言</a>：国产评论系统，可区分热评和最新评论，论坛贴吧风</li>
<li><a href="https://www.livere.com/" target="_blank" rel="noopener">来必力</a>：支持插入图片和 GIF，支持国内外多种社交媒体的三方登陆 <a href="https://www.livere.com/city-demo" target="_blank" rel="noopener">Demo</a></li>
</ul>
<p>博客的评论系统不需要太过复杂的功能，我的要求是一定要轻量级，足够简洁美观，并且支持 Markdown 语法，因此我首选 Valine 和 gitment，这两个评论系统都是由国内个人开发的，在此向开发者致敬。</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/valine-comments.png" alt></p>
<p>Next 已经内置了 Valine 组件，在主题配置文件中开启评论功能即可，同时，由于 Valine 是基于 Leancloud 提供后端服务的，所以需要填写 LeanCloud 的 App ID 和 App Key。</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">valine:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">appid:</span>  <span class="string">***&lt;app_id***</span></span><br><span class="line">  <span class="attr">appkey:</span> <span class="string">***&lt;app_key&gt;***</span></span><br><span class="line">  <span class="attr">notify:</span> <span class="literal">false</span>  <span class="comment"># 收到新评论是否邮件通知</span></span><br><span class="line">  <span class="attr">verify:</span> <span class="literal">false</span>  <span class="comment"># 是否开启验证码</span></span><br><span class="line">  <span class="attr">placeholder:</span>  <span class="comment"># 默认填充文字</span></span><br><span class="line">  <span class="attr">avatar:</span> <span class="string">mm</span>  <span class="comment"># 设置默认评论列表</span></span><br><span class="line">  <span class="attr">guest_info:</span> <span class="string">nick,mail</span>  <span class="comment"># 评论区头部表单</span></span><br><span class="line">  <span class="attr">pageSize:</span> <span class="number">10</span>  <span class="comment"># 每页评论数</span></span><br><span class="line">  <span class="attr">visitor:</span> <span class="literal">true</span>  <span class="comment"># 同时开启文章阅读次数统计</span></span><br></pre></td></tr></table></figure>
<p>Valine 也附带了阅读统计功能，可以在 Valine 配置项中设置 <code>visitor: true</code> 开启该功能。为避免后端服务冲突，建议不要同时启用 Valine 的阅读统计功能和 <code>leancloud_visitors</code>。Next 暂时不支持通过配置的方式隐藏文章标题下的评论数量，如要隐藏，可在自定义样式文件中添加如下代码：</p>
<figure class="highlight yml"><figcaption><span>themes/next/source/css/_custom/custom.styl</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="string">//屏蔽标题下的评论数量</span></span><br><span class="line"><span class="string">.post-comments-count</span> <span class="string">&#123;</span></span><br><span class="line">  <span class="attr">display:</span> <span class="string">none;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<p>如果你是轻度洁癖患者，想要隐藏评论区的浏览器和操作系统版本号以拥有更加干净的评论界面，可在自定义样式文件中添加如下代码：</p>
<figure class="highlight yml"><figcaption><span>themes/next/source/css/_custom/custom.styl</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="string">//屏蔽评论组件的多余信息</span></span><br><span class="line"><span class="comment">#comments .vsys &#123;</span></span><br><span class="line">  <span class="attr">display:</span> <span class="string">none;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="wen-mo-ban-quan-sheng-ming">文末版权声明</h2>
<p>在主题配置文件中开启文章底部的版权声明，版权声明默认使用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议，用户可以根据自身需要修改 <code>licence</code> 字段变更协议。</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">post_copyright:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">license:</span> <span class="string">&lt;a</span> <span class="string">href="https://creativecommons.org/licenses/by-nc-sa/4.0/"</span> <span class="string">rel="external</span> <span class="string">nofollow"</span> <span class="string">target="_blank"&gt;CC</span> <span class="string">BY-NC-SA</span> <span class="number">4.0</span><span class="string">&lt;/a&gt;</span></span><br></pre></td></tr></table></figure>
<p>默认版权声明中只有 <strong>本文作者</strong>、<strong>本文链接</strong>、<strong>版权声明</strong> 三项，如果你想添加更多内容，如 <strong>创建时间</strong>、<strong>修改时间</strong>、<strong>引用链接</strong> 等，需要修改版权声明的相关代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout_macro\post-copyright.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&lt;!-- JS库 clipboard 拷贝内容到粘贴板--&gt;</span><br><span class="line">&lt;script src=<span class="string">"https://cdn.bootcss.com/clipboard.js/2.0.1/clipboard.min.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">&lt;!-- JS库 sweetalert 显示提示信息--&gt;</span></span><br><span class="line"><span class="regexp">&lt;script src="https:/</span><span class="regexp">/unpkg.com/</span>sweetalert/dist/sweetalert.min.js<span class="string">"&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&lt;ul class="</span>post-copyright<span class="string">"&gt;</span></span><br><span class="line"><span class="string">  &lt;!-- 本文标题 --&gt;</span></span><br><span class="line"><span class="string">  &lt;li&gt;</span></span><br><span class="line"><span class="string">    &lt;strong&gt;&#123;&#123; __('post.copyright.title') + __('symbol.colon') &#125;&#125; &lt;/strong&gt;</span></span><br><span class="line"><span class="string">    &#123;&#123; post.title &#125;&#125;</span></span><br><span class="line"><span class="string">  &lt;/li&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &lt;!-- 本文作者 --&gt;</span></span><br><span class="line"><span class="string">  &lt;li class="</span>post-copyright-author<span class="string">"&gt;</span></span><br><span class="line"><span class="string">    &lt;strong&gt;&#123;&#123; __('post.copyright.author') + __('symbol.colon') &#125;&#125; &lt;/strong&gt;</span></span><br><span class="line"><span class="string">    &#123;&#123; post.author | default(author) &#125;&#125;</span></span><br><span class="line"><span class="string">  &lt;/li&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &lt;!-- 创建时间 --&gt;</span></span><br><span class="line"><span class="string">  &lt;li&gt;</span></span><br><span class="line"><span class="string">    &lt;strong&gt;&#123;&#123; __('post.created') + __('symbol.colon') &#125;&#125; &lt;/strong&gt;</span></span><br><span class="line"><span class="string">    &#123;&#123; post.date.format("</span>YYYY年MM月DD日 - HH时MM分<span class="string">") &#125;&#125;</span></span><br><span class="line"><span class="string">  &lt;/li&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &lt;!-- 修改时间 --&gt;</span></span><br><span class="line"><span class="string">  &lt;li&gt;</span></span><br><span class="line"><span class="string">    &lt;strong&gt;&#123;&#123; __('post.modified') + __('symbol.colon') &#125;&#125; &lt;/strong&gt;</span></span><br><span class="line"><span class="string">    &#123;&#123; post.updated.format("</span>YYYY年MM月DD日 - HH时MM分<span class="string">") &#125;&#125;</span></span><br><span class="line"><span class="string">  &lt;/li&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &lt;!-- 引用链接 --&gt;</span></span><br><span class="line"><span class="string">  &lt;li class="</span>post-copyright-link<span class="string">"&gt;</span></span><br><span class="line"><span class="string">    &lt;strong&gt;&#123;&#123; __('post.copyright.link') + __('symbol.colon') &#125;&#125;&lt;/strong&gt;</span></span><br><span class="line"><span class="string">    &lt;a href="</span>&#123;&#123; post.url | <span class="keyword">default</span>(post.permalink) &#125;&#125;<span class="string">" title="</span>&#123;&#123; post.title &#125;&#125;<span class="string">"</span></span><br><span class="line"><span class="string">      &gt;&#123;&#123; post.url | default(post.permalink) &#125;&#125;&lt;/a</span></span><br><span class="line"><span class="string">    &gt;</span></span><br><span class="line"><span class="string">    &lt;span class="</span>copy-path<span class="string">" title="</span>点击复制引用链接<span class="string">"</span></span><br><span class="line"><span class="string">      &gt;&lt;i</span></span><br><span class="line"><span class="string">        style="</span>cursor: pointer<span class="string">"</span></span><br><span class="line"><span class="string">        class="</span>fa fa-clipboard<span class="string">"</span></span><br><span class="line"><span class="string">        data-clipboard-text="</span>[&#123;&#123; post.author | <span class="keyword">default</span>(author) &#125;&#125;<span class="string">'s Blog | &#123;&#123; post.title &#125;&#125;](&#123;&#123; post.permalink &#125;&#125;)"</span></span><br><span class="line"><span class="string">        aria-label="&#123;&#123; __('</span>post.copy_success<span class="string">') &#125;&#125;"</span></span><br><span class="line"><span class="string">      &gt;&lt;/i</span></span><br><span class="line"><span class="string">    &gt;&lt;/span&gt;</span></span><br><span class="line"><span class="string">  &lt;/li&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &lt;!-- 版权声明 --&gt;</span></span><br><span class="line"><span class="string">  &lt;li class="post-copyright-license"&gt;</span></span><br><span class="line"><span class="string">    &lt;strong</span></span><br><span class="line"><span class="string">      &gt;&#123;&#123; __('</span>post.copyright.license_title<span class="string">') + __('</span>symbol.colon<span class="string">') &#125;&#125;</span></span><br><span class="line"><span class="string">    &lt;/strong&gt;</span></span><br><span class="line"><span class="string">    &#123;&#123; __('</span>post.copyright.license_content<span class="string">', theme.post_copyright.license) &#125;&#125;</span></span><br><span class="line"><span class="string">  &lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;/ul&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&lt;script&gt;</span></span><br><span class="line"><span class="string">  var clipboard = new ClipboardJS(".fa-clipboard");</span></span><br><span class="line"><span class="string">  clipboard.on("success", function(target) &#123;</span></span><br><span class="line"><span class="string">    var message = document.createElement("div");</span></span><br><span class="line"><span class="string">    message.innerHTML =</span></span><br><span class="line"><span class="string">      '</span>&lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-check-circle message-icon"</span>&gt;&lt;<span class="regexp">/i&gt;&lt;span class="message-content"&gt;' +</span></span><br><span class="line"><span class="regexp">      target.trigger.getAttribute("aria-label") +</span></span><br><span class="line"><span class="regexp">      "&lt;/</span>span&gt;<span class="string">";</span></span><br><span class="line"><span class="string">    swal(&#123;</span></span><br><span class="line"><span class="string">      content: message,</span></span><br><span class="line"><span class="string">      className: "</span>copy-success-message<span class="string">",</span></span><br><span class="line"><span class="string">      timer: 1000,</span></span><br><span class="line"><span class="string">      button: false</span></span><br><span class="line"><span class="string">    &#125;);</span></span><br><span class="line"><span class="string">  &#125;);</span></span><br><span class="line"><span class="string">&lt;/script&gt;</span></span><br></pre></td></tr></table></figure>
<p>在版权样式文件中添加如下样式：</p>
<figure class="highlight yml"><figcaption><span>themes\next\source\css_common\components\post\post-copyright.styl</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="string">.swal-overlay</span> <span class="string">&#123;</span></span><br><span class="line">  <span class="attr">background-color:</span> <span class="string">transparent;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">.copy-success-message</span> <span class="string">&#123;</span></span><br><span class="line">  <span class="attr">box-shadow:</span> <span class="string">0px</span> <span class="string">4px</span> <span class="string">12px</span> <span class="string">rgba(0,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0.15</span><span class="string">);</span></span><br><span class="line">  <span class="attr">border-radius:</span> <span class="string">4px;</span></span><br><span class="line">  <span class="attr">width:</span> <span class="string">auto;</span></span><br><span class="line">  <span class="attr">margin:</span> <span class="string">16x</span> <span class="string">0px;</span></span><br><span class="line">  <span class="attr">vertical-align:</span> <span class="string">top;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">.copy-success-message</span> <span class="string">.swal-content</span> <span class="string">&#123;</span></span><br><span class="line">  <span class="attr">margin:</span> <span class="string">0px</span> <span class="string">0px</span> <span class="type">!important</span><span class="string">;</span></span><br><span class="line">  <span class="attr">padding:</span> <span class="string">10px</span> <span class="string">16px;</span></span><br><span class="line">  <span class="attr">line-height:</span> <span class="string">1em;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">.copy-success-message</span> <span class="string">.message-icon</span> <span class="string">&#123;</span></span><br><span class="line">  <span class="attr">color:</span> <span class="comment">#52c41a;</span></span><br><span class="line">  <span class="attr">margin-right:</span> <span class="string">8px;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">.copy-success-message</span> <span class="string">.message-content</span> <span class="string">&#123;</span></span><br><span class="line">  <span class="attr">font-size:</span> <span class="string">14px;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<p>然后补全版权信息文案字段：</p>
<figure class="highlight yml"><figcaption><span>themes/next/languages/zh-CN.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">post:</span></span><br><span class="line">  <span class="attr">created:</span> <span class="string">创建时间</span></span><br><span class="line">  <span class="attr">modified:</span> <span class="string">修改时间</span></span><br><span class="line">  <span class="attr">copy_success:</span> <span class="string">复制成功</span></span><br><span class="line">  <span class="attr">copyright:</span></span><br><span class="line">    <span class="attr">title:</span> <span class="string">本文标题</span></span><br><span class="line">    <span class="attr">author:</span> <span class="string">本文作者</span></span><br><span class="line">    <span class="attr">link:</span> <span class="string">引用链接</span></span><br><span class="line">    <span class="attr">license_title:</span> <span class="string">版权声明</span></span><br><span class="line">    <span class="attr">license_content:</span> <span class="string">"本博客所有文章除特别声明外，均采用 %s 许可协议。转载请注明出处！"</span></span><br></pre></td></tr></table></figure>
<p>在实际使用过程中，并非每篇文章都需要版权声明，如果转载了别人的文章，文末再出现个人版权声明就不太合适。此时可在 Front-Matter 中设定变量 <code>copyright</code> 用于控制是否显示版权信息。修改文章布局模板中相关代码，使得只有当主题配置文件中 <code>post_copyright.enable</code> 字段和 <code>page.copyright</code> 字段同时为 <code>true</code> 时才会插入版权声明：</p>
<figure class="highlight diff"><figcaption><span>themes/next/layout/_macro/post.swig</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="deletion">- &#123;% if theme.post_copyright.enable and not is_index %&#125;</span></span><br><span class="line"><span class="addition">+ &#123;% if theme.post_copyright.enable and page.copyright and not is_index %&#125;</span></span><br><span class="line">    &lt;div&gt;</span><br><span class="line">      &#123;% include 'post-copyright.swig' with &#123; post: post &#125; %&#125;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">  &#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<p>为了批量为每篇新文章设定该变量并赋默认值，可以修改草稿模板内容，这样以来每篇草稿发布为正文后都会默认显示底部版权信息：</p>
<figure class="highlight diff"><figcaption><span>scaffolds\draft.md</span></figcaption><table><tr><td class="code"><pre><span class="line"> title: &#123;&#123; title &#125;&#125;</span><br><span class="line">  tags:</span><br><span class="line">  categories:</span><br><span class="line"><span class="addition">+ copyright: true</span></span><br></pre></td></tr></table></figure>
<h2 id="tian-jia-da-shang-gong-neng">添加打赏功能</h2>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511212128532.png" alt></p>
<p>启用主题配置文件中的打赏相关字段，并将个人收款码图片置于 themes\next\source\images\ 目录下，注意保持图片命名与配置文件中一致：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">reward_comment:</span></span><br><span class="line"><span class="attr">wechatpay:</span> <span class="string">/images/wechatpay.png</span></span><br><span class="line"><span class="attr">alipay:</span> <span class="string">/images/alipay.jpg</span></span><br></pre></td></tr></table></figure>
<p>如果要关闭悬停收款码上的文字抖动效果，可以在自定义样式文件中添加以下代码：</p>
<figure class="highlight yml"><figcaption><span>themes/next/source/css/_custom/custom.styl</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="string">//关闭打赏收款码的文字抖动效果</span></span><br><span class="line"><span class="comment">#QR &gt; div:hover p &#123;</span></span><br><span class="line">  <span class="attr">animation:</span> <span class="string">none;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<p>并非每个页面都需要开启打赏功能，可以在 Front-Matter 中添加 <code>reward</code> 字段来控制是否在本文章中添加打赏信息，然后修改文章布局模板中相关的判定条件：</p>
<figure class="highlight diff"><figcaption><span>themes/next/layout/_macro/post.swig</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="deletion">- &#123;% if (theme.alipay or theme.wechatpay or theme.bitcoin) and not is_index %&#125;</span></span><br><span class="line"><span class="addition">+ &#123;% if ( post.reward and (theme.alipay or theme.wechatpay or theme.bitcoin) and not is_index %&#125;</span></span><br><span class="line">    &lt;div&gt;</span><br><span class="line">      &#123;% include 'reward.swig' %&#125;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">  &#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<p>为了方便可在草稿模板 scaffolds\<a href="http://draft.md" target="_blank" rel="noopener">draft.md</a> 中统一添加 <code>reward</code> 字段默认值：</p>
<figure class="highlight diff"><figcaption><span>scaffolds\draft.md</span></figcaption><table><tr><td class="code"><pre><span class="line">  title: &#123;&#123; title &#125;&#125;</span><br><span class="line">  tags:</span><br><span class="line">  categories:</span><br><span class="line"><span class="addition">+ reward: true</span></span><br></pre></td></tr></table></figure>
<h2 id="tian-jia-tu-pian-deng-xiang">添加图片灯箱</h2>
<p>添加灯箱功能，实现点击图片后放大聚焦图片，并支持幻灯片播放、全屏播放、缩略图、快速分享到社交媒体等，该功能由 <a href="https://github.com/fancyapps/fancybox" target="_blank" rel="noopener">fancyBox</a> 提供，效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511212409792.png" alt></p>
<p>在根目录下执行以下命令安装相关依赖：</p>
<figure class="highlight crystal"><table><tr><td class="code"><pre><span class="line">$ git clone <span class="symbol">https:</span>/<span class="regexp">/github.com/theme</span>-<span class="keyword">next</span>/theme-<span class="keyword">next</span>-fancybox3 themes/<span class="keyword">next</span>/source/<span class="class"><span class="keyword">lib</span>/<span class="title">fancybox</span></span></span><br></pre></td></tr></table></figure>
<p>在主题配置文件中设置 <code>fancybox: true</code>：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">fancybox:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h2 id="xiang-guan-wen-zhang-tui-jian">相关文章推荐</h2>
<p>该功能由 <a href="https://github.com/tea3/hexo-related-popular-posts" target="_blank" rel="noopener">hexo-related-popular-posts</a> 插件提供，效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/related-post.png" alt></p>
<p>在站点根目录中执行以下命令安装依赖：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ npm <span class="keyword">install</span> hexo-related-popular-posts <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
<p>在主题配置文件中开启相关文章推荐功能：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">related_posts:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">title:</span> <span class="comment"># custom header, leave empty to use the default one</span></span><br><span class="line">  <span class="attr">display_in_home:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="attr">maxCount:</span> <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>此时会在每篇文章结尾根据标签相关性和内容相关性来推荐相关文章。</p>
<p>事实上并非每篇文章都需要开启该功能，可在文章 Front-Matter 中设置 <code>related_posts</code> 字段来控制是否在文末显示相关文章，然后修改文章布局模板中相关的判定条件：</p>
<figure class="highlight diff"><figcaption><span>themes/next/layout/_macro/post.swig</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="deletion">- &#123;% if theme.related_posts.enable and (theme.related_posts.display_in_home or not is_index) %&#125;</span></span><br><span class="line"><span class="addition">+ &#123;% if theme.related_posts.enable and (theme.related_posts.display_in_home or not is_index) and post.related_posts %&#125;</span></span><br><span class="line">    &#123;% include 'post-related.swig' with &#123; post: post &#125; %&#125;</span><br><span class="line">  &#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<p>为了方便可在草稿模板 scaffolds\<a href="http://draft.md" target="_blank" rel="noopener">draft.md</a> 中统一添加 <code>related_posts</code> 字段默认值：</p>
<figure class="highlight yml"><figcaption><span>scaffolds\draft.md</span></figcaption><table><tr><td class="code"><pre><span class="line">  <span class="attr">title:</span> <span class="string">&#123;&#123;</span> <span class="string">title</span> <span class="string">&#125;&#125;</span></span><br><span class="line">  <span class="attr">tags:</span></span><br><span class="line">  <span class="attr">categories:</span></span><br><span class="line"><span class="string">+</span> <span class="attr">related_posts:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h2 id="wen-zhang-ping-fen">文章评分</h2>
<p><a href="https://widgetpack.com/" target="_blank" rel="noopener">widgetpack</a> 是一款轻量级的插件，提供四项具体的功能：</p>
<ul>
<li>Comments: 评论系统，类似于留言板</li>
<li>Reviews: 评价系统，类似于商品评价</li>
<li>Rating: 星级评分系统</li>
<li>Google Reviews: 关联展示 Google Rating</li>
</ul>
<p>Next 主题中已经集成了 widgetpack 的星级评分系统，用户无须再安装或引入插件脚本，只需在 widgetpack 中注册账号并修改主题配置即可，应用效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/rating.png" alt></p>
<p>在 <a href="https://widgetpack.com/" target="_blank" rel="noopener">widgetpack</a> 中注册账号，根据引导填写应用名称和域名创建应用，创建后可在页面左上角看到应用 id。</p>
<p>在主题配置文件中开启评分功能，填写应用 id，并设置评分颜色：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Star rating support to each article.</span></span><br><span class="line"><span class="comment"># To get your ID visit https://widgetpack.com</span></span><br><span class="line"><span class="attr">rating:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">id:</span> <span class="comment">#&lt;app_id&gt;</span></span><br><span class="line">  <span class="attr">color:</span> <span class="string">fadb14</span></span><br></pre></td></tr></table></figure>
<p>此时刷新浏览器即可在文章末尾看到空的评分栏了。点击评分发现需要以社交账号登陆，而这些社交账号基本都是 facebook、twitter 等墙外的社交软件，限制了评分系统可用性，我们可以在 widgetpack 控制台中修改评分认证机制。</p>
<p>在控制台中点击左上角展开菜单，在 <strong>Rating</strong> -&gt; <strong>Setting</strong> 中将 Vote via 选项改为 Device(cookie) 以开启匿名评分，该选项将基于设备认证访问者身份：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/rate-vote-via.png" alt></p>
<p>用户还可以在该页面设定 star 数量和大小。修改后记得勾选右下角的 SAVE SETTING 才会生效。</p>
<p>在实际使用过程中，并非每篇文章都需要开启评分。此时可在 Front-Matter 中设定变量 rating 用于控制是否开启评分。修改文章布局模板中相关代码，使得只有当主题配置文件中 <code>rating.enable</code> 字段和 <code>page.rating</code> 字段同时为 <code>true</code> 才会插入评分组件：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout_macro\post.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">  &#123;% if not is_index %&#125;</span><br><span class="line"><span class="deletion">-  &#123;% if theme.rating.enable or (theme.vkontakte_api.enable and theme.vkontakte_api.like) or (theme.facebook_sdk.enable and theme.facebook_sdk.like_button) or (theme.needmoreshare2.enable and theme.needmoreshare2.postbottom.enable) or (theme.baidushare and theme.baidushare.type === "button" )%&#125;</span></span><br><span class="line"><span class="addition">+  &#123;% if (theme.rating.enable and post.rating) or (theme.vkontakte_api.enable and theme.vkontakte_api.like) or (theme.facebook_sdk.enable and theme.facebook_sdk.like_button) or (theme.needmoreshare2.enable and theme.needmoreshare2.postbottom.enable) or (theme.baidushare and theme.baidushare.type === "button" )%&#125;</span></span><br><span class="line">    &lt;div class="post-widgets"&gt;</span><br><span class="line">    &#123;% if theme.rating.enable %&#125;</span><br><span class="line">      &lt;div class="wp_rating"&gt;</span><br><span class="line">        &lt;div id="wpac-rating"&gt;&lt;/div&gt;</span><br><span class="line">      &lt;/div&gt;</span><br><span class="line">    &#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<p>为了批量为每篇新文章设定该变量并赋默认值，可以修改草稿模板内容，这样以来每篇草稿发布后都会默认开启评分：</p>
<figure class="highlight diff"><figcaption><span>scaffolds\draft.md</span></figcaption><table><tr><td class="code"><pre><span class="line">  title: &#123;&#123; title &#125;&#125;</span><br><span class="line">  tags:</span><br><span class="line">  categories:</span><br><span class="line"><span class="addition">+ rating: true</span></span><br></pre></td></tr></table></figure>
<p>站点上线后，可以在控制台菜单的 <strong>Site</strong> -&gt; <strong>Setting</strong> 中勾选 Private，使得组件只对应用内指定的域名上生效，这样以来即时别人错填了你的 id 也不会将评分数据误提交到你的应用中了。</p>
<p>widgetpack 与前文提到的 hotjar 在评价反馈功能上的侧重点不一样，widgetpack 更侧重于对文章的评分，而 hotjar 侧重于对整个页面的评分，并提供了文字和截图反馈的渠道。</p>
<h2 id="wen-zhang-jia-mi-fang-wen">文章加密访问</h2>
<p>该功能由 <a href="https://github.com/MikeCoder/hexo-blog-encrypt" target="_blank" rel="noopener">hexo-blog-encrypt</a> 插件提供，效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511213059680.png" alt></p>
<p>在站点根目录中执行以下命令安装依赖：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">npm <span class="keyword">install</span> hexo-blog-<span class="keyword">encrypt</span> <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
<p>在站点配置文件中添加如下字段：</p>
<figure class="highlight yml"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">encrypt:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">default_abstract:</span> <span class="string">此文章已被加密，需要输入密码访问。</span>  <span class="string">//首页文章列表中加密文章的默认描述文案</span></span><br><span class="line">  <span class="attr">default_message:</span> <span class="string">请输入密码以阅读这篇私密文章。</span>  <span class="string">//文章详情页的密码输入框上的默认描述文案</span></span><br></pre></td></tr></table></figure>
<p>然后在文章 Front-Matter 中添加 <code>password</code> 字段用于设置文章访问密码。重启服务器，这个时候可能需要经历较长一段时间的加密过程，请耐心等待，加密完成后刷新页面将会显示密码输入框，输入密码后才能继续访问文章内容。</p>
<p>该功能只会加密文章正文，其他内容如打赏、版权信息、标签等则不会被加密隐藏，这样看起来有点奇怪，所以建议加密文章隐藏掉打赏和版权信息内容。</p>
<figure class="highlight js"><figcaption><span>themes\next\layout_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&lt;script src=<span class="string">"https://unpkg.com/sweetalert/dist/sweetalert.min.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br></pre></td></tr></table></figure>
<p>在 node_modules 依赖库中修改 hexo-blog-encrypt 源码：</p>
<figure class="highlight js"><figcaption><span>node_modules\hexo-blog-encrypt\lib\blog.encrypt.js</span></figcaption><table><tr><td class="code"><pre><span class="line">  ...</span><br><span class="line">  &#125; <span class="keyword">catch</span> (e) &#123;</span><br><span class="line">-   alert(decryptionError);</span><br><span class="line">+   swal(&#123;</span><br><span class="line">+     text: <span class="string">"密码错误!"</span>,</span><br><span class="line">+     icon: <span class="string">"error"</span>,</span><br><span class="line">+     className: <span class="string">"password-error"</span>,</span><br><span class="line">+     timer: <span class="number">1000</span>,</span><br><span class="line">+     button: <span class="literal">false</span></span><br><span class="line">+   &#125;);</span><br><span class="line">    <span class="built_in">console</span>.log(e);</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<p>在自定义样式文件中添加如下代码：</p>
<figure class="highlight css"><figcaption><span>themes/next/source/css/_custom/custom.styl</span></figcaption><table><tr><td class="code"><pre><span class="line">//密码错误sweetalert弹框样式修改</span><br><span class="line"><span class="selector-class">.swal-overlay</span> &#123;</span><br><span class="line">  <span class="attribute">background-color</span>: transparent;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.password-error</span> &#123;</span><br><span class="line">  <span class="attribute">box-shadow</span>: <span class="number">0px</span> <span class="number">4px</span> <span class="number">12px</span> <span class="built_in">rgba</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.15</span>);</span><br><span class="line">  <span class="attribute">border-radius</span>: <span class="number">4px</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由于是在 node_module 中修改的依赖文件，一旦更新或者重装依赖都会覆盖修改，需要重新修改一遍。</p>
]]></content>
      <categories>
        <category>技术/博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo主题美化</title>
    <url>/2018/02/10/hexo/hexo-theme-beautify/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2018/02/10/hexo/hexo-theme-beautify/coast.jpg" alt></p>
<a id="more"></a>
<h1 id="xiu-gai-bo-ke-zi-ti">修改博客字体</h1>
<p>在 <a href="https://www.google.com/fonts" target="_blank" rel="noopener">Google Fonts</a> 上找到心仪的字体，然后在主题配置文件中为不同的应用场景配置字体：</p>
<figure class="highlight python"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">font:</span><br><span class="line">  enable: true</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 外链字体库地址，例如 //fonts.googleapis.com (默认值)</span></span><br><span class="line">  host:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 全局字体，应用在 body 元素上</span></span><br><span class="line">  <span class="keyword">global</span>:</span><br><span class="line">    external: true</span><br><span class="line">    family: Monda</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 标题字体 (h1, h2, h3, h4, h5, h6)</span></span><br><span class="line">  headings:</span><br><span class="line">    external: true</span><br><span class="line">    family: Roboto Slab</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 文章字体</span></span><br><span class="line">  posts:</span><br><span class="line">    external: true</span><br><span class="line">    family:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Logo 字体</span></span><br><span class="line">  logo:</span><br><span class="line">    external: true</span><br><span class="line">    family:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 代码字体，应用于 code 以及代码块</span></span><br><span class="line">  codes:</span><br><span class="line">    external: true</span><br><span class="line">    family:</span><br></pre></td></tr></table></figure>
<h1 id="wen-zhang-ye-mo-mei-hua">文章页末美化</h1>
<h2 id="wei-biao-qian-tian-jia-tu-biao">为标签添加图标</h2>
<p>默认情况下标签前缀是 <code>#</code> 字符，用户可以通过修改主题源码将标签的字符前缀改为图标前缀，更改后效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-theme-beautify/screenshot.png" alt></p>
<p>在文章布局模板中找到文末标签相关代码段，将 <code>#</code> 换成 <code>&lt;i class=&quot;fa fa-tags&quot;&gt;&lt;/i&gt;</code> 即可：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout\_macro\post.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&lt;footer class="post-footer"&gt;</span><br><span class="line">    &#123;% if post.tags and post.tags.length and not is_index %&#125;</span><br><span class="line">      &lt;div class="post-tags"&gt;</span><br><span class="line">        &#123;% for tag in post.tags %&#125;</span><br><span class="line"><span class="deletion">-          &lt;a href="&#123;&#123; url_for(tag.path) &#125;&#125;" rel="tag"&gt;# &#123;&#123; tag.name &#125;&#125;&lt;/a&gt;</span></span><br><span class="line"><span class="addition">+          &lt;a href="&#123;&#123; url_for(tag.path) &#125;&#125;" rel="tag"&gt;&lt;i class="fa fa-tags"&gt;&lt;/i&gt; &#123;&#123; tag.name &#125;&#125;&lt;/a&gt;</span></span><br><span class="line">        &#123;% endfor %&#125;</span><br><span class="line">      &lt;/div&gt;</span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line">    ...</span><br><span class="line">  &lt;/footer&gt;</span><br></pre></td></tr></table></figure>
<p>Next 中使用 <a href="https://fontawesome.com/v4.7.0/icons/" target="_blank" rel="noopener">FontAwesome</a> 作为图标库，用户可以在 FontAwesome 上找到心仪的图标来替换标签的字符前缀。</p>
<h2 id="tian-jia-jie-shu-biao-ji">添加结束标记</h2>
<p>在文末添加结束标记，效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-theme-beautify/20181113045252399.png" alt></p>
<p>修改主题配置文件：取消对 <code>postBodyEnd: source/_data/post-body-end.swig</code>的注释</p>
<figure class="highlight diff"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">custom_file_path:</span><br><span class="line">      #head: source/_data/head.swig</span><br><span class="line">      #header: source/_data/header.swig</span><br><span class="line">      #sidebar: source/_data/sidebar.swig</span><br><span class="line">      #postMeta: source/_data/post-meta.swig     </span><br><span class="line"><span class="addition">+     postBodyEnd: source/_data/post-body-end.swig</span></span><br><span class="line"><span class="deletion">-     #postBodyEnd: source/_data/post-body-end.swig</span></span><br><span class="line">      footer: source/_data/footer.swig</span><br><span class="line">      #bodyEnd: source/_data/body-end.swig</span><br><span class="line">      #variable: source/_data/variables.styl</span><br><span class="line">      #mixin: source/_data/mixins.styl</span><br><span class="line">      style: source/_data/styles.styl</span><br></pre></td></tr></table></figure>
<p>在<code>source/_data/post-body-end.swig</code>中添加如下代码：</p>
<figure class="highlight html"><figcaption><span>source/_data/post-body-end.swig</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">  &#123;% if not is_index %&#125;</span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">"text-align:center;color:#bfbfbf;font-size:16px;"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">span</span>&gt;</span>-------- 本文结束 <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"fa fa-coffee"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">span</span>&gt;</span> 感谢阅读 --------<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  &#123;% endif %&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h1 id="ye-mian-jia-zai-jin-du-tiao">页面加载进度条</h1>
<p>当网络不好的时候可能会在打开站点或跳转文章时出现短暂的白屏，此时如果能有加载进度提示将会提高用户操作体验。</p>
<p>在根目录下执行以下命令安装相关依赖：</p>
<figure class="highlight crystal"><table><tr><td class="code"><pre><span class="line">$ git clone <span class="symbol">https:</span>/<span class="regexp">/github.com/theme</span>-<span class="keyword">next</span>/theme-<span class="keyword">next</span>-pace themes/<span class="keyword">next</span>/source/<span class="class"><span class="keyword">lib</span>/<span class="title">pace</span></span></span><br></pre></td></tr></table></figure>
<p>在主题配置文件中设置 <code>pace: true</code>。</p>
<p>默认提供了多种主题的进度条加载样式，有顶部提示的，有中间提示的，还有全页面遮挡提示的，个人认为默认的进度条效果就恰如其当，既能够在页面空白的时候起到加载作用，也不会因为太过花里胡哨而喧宾夺主，尤其是当你如果使用了不蒜子的站点访问统计的功能的时候，常常会遇到所有资源都加载完毕而不蒜子还在等待响应，如果这个时候在页面较显眼的位置出现一个停滞不前的进度条，很让人抓狂。</p>
<h1 id="ce-bian-lan-fang-zuo-bian">侧边栏放左边</h1>
<p>Next 主题各系列中只有 Pisces 和 Gemini 支持通过主题配置文件来将侧边栏置于左侧或右侧，而 Muse 和 Mist 则需要深度修改源码才能实现改变侧边栏位置。</p>
<p>在自定义样式文件中添加如下规则：</p>
<figure class="highlight css"><figcaption><span>source/_data/styles.styl</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="selector-class">.sidebar-toggle</span> &#123;</span><br><span class="line">  <span class="attribute">left</span>: <span class="number">30px</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.sidebar</span> &#123;</span><br><span class="line">  <span class="attribute">left</span>: <span class="number">0px</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>若<code>source/_data/styles.styl</code>没有开启，需要在主题配置文件中开启</p>
<figure class="highlight diff"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">custom_file_path:</span><br><span class="line">      #head: source/_data/head.swig</span><br><span class="line">      #header: source/_data/header.swig</span><br><span class="line">      #sidebar: source/_data/sidebar.swig</span><br><span class="line">      #postMeta: source/_data/post-meta.swig     </span><br><span class="line">      postBodyEnd: source/_data/post-body-end.swig</span><br><span class="line">      #postBodyEnd: source/_data/post-body-end.swig</span><br><span class="line">      footer: source/_data/footer.swig</span><br><span class="line">      #bodyEnd: source/_data/body-end.swig</span><br><span class="line">      #variable: source/_data/variables.styl</span><br><span class="line">      #mixin: source/_data/mixins.styl</span><br><span class="line"><span class="addition">+      style: source/_data/styles.styl</span></span><br></pre></td></tr></table></figure>
<figure class="highlight js"><figcaption><span>themes\next\source\js\src\motion.js</span></figcaption><table><tr><td class="code"><pre><span class="line">$(<span class="built_in">document</span>)</span><br><span class="line">  .on(<span class="string">'sidebar.isShowing'</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    NexT.utils.isDesktop() &amp;&amp; $(<span class="string">'body'</span>).velocity(<span class="string">'stop'</span>).velocity(</span><br><span class="line">-     &#123;<span class="attr">paddingRight</span>: SIDEBAR_WIDTH&#125;,</span><br><span class="line">+     &#123;<span class="attr">paddingLeft</span>: SIDEBAR_WIDTH&#125;,</span><br><span class="line">      SIDEBAR_DISPLAY_DURATION</span><br><span class="line">    );</span><br><span class="line">  &#125;)</span><br><span class="line">  .on(<span class="string">'sidebar.isHiding'</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  &#125;);</span><br><span class="line">  ...</span><br><span class="line">  hideSidebar: <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">-   NexT.utils.isDesktop() &amp;&amp; $(<span class="string">'body'</span>).velocity(<span class="string">'stop'</span>).velocity(&#123;<span class="attr">paddingRight</span>: <span class="number">0</span>&#125;);</span><br><span class="line">+   NexT.utils.isDesktop() &amp;&amp; $(<span class="string">'body'</span>).velocity(<span class="string">'stop'</span>).velocity(&#123;<span class="attr">paddingLeft</span>: <span class="number">0</span>&#125;);</span><br><span class="line">    <span class="keyword">this</span>.sidebarEl.find(<span class="string">'.motion-element'</span>).velocity(<span class="string">'stop'</span>).css(<span class="string">'display'</span>, <span class="string">'none'</span>);</span><br><span class="line">    <span class="keyword">this</span>.sidebarEl.velocity(<span class="string">'stop'</span>).velocity(&#123;<span class="attr">width</span>: <span class="number">0</span>&#125;, &#123;<span class="attr">display</span>: <span class="string">'none'</span>&#125;);</span><br><span class="line"></span><br><span class="line">    sidebarToggleLines.init();</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如此以来就可以将侧边栏放置在左边了，但当窗口宽度缩小到 991px 之后会出现样式错误：侧边栏收缩消失但是页面左侧仍留有空白间距，此时修改如下代码即可：</p>
<figure class="highlight diff"><figcaption><span>themes\next\source\css\_common\scaffolding\base.styl</span></figcaption><table><tr><td class="code"><pre><span class="line">body &#123;</span><br><span class="line">  position: relative; // Required by scrollspy</span><br><span class="line">  font-family: $font-family-base;</span><br><span class="line">  font-size: $font-size-base;</span><br><span class="line">  line-height: $line-height-base;</span><br><span class="line">  color: $text-color;</span><br><span class="line">  background: $body-bg-color;</span><br><span class="line"></span><br><span class="line"><span class="deletion">- +mobile() &#123; padding-left: 0 !important; &#125;</span></span><br><span class="line"><span class="deletion">- +tablet() &#123; padding-left: 0 !important; &#125;  </span></span><br><span class="line"><span class="addition">+ +mobile() &#123; padding-right: 0 !important; &#125;</span></span><br><span class="line"><span class="addition">+ +tablet() &#123; padding-right: 0 !important; &#125;</span></span><br><span class="line">  +desktop-large() &#123; font-size: $font-size-large; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="tian-jia-dong-tai-bei-jing">添加动态背景</h1>
<p>Next 主题可以通过安装插件快速为站点添加不同效果的动态背景。</p>
<h2 id="li-zi-piao-fu-ju-he">粒子漂浮聚合</h2>
<p>应用效果如下图：</p>
<p><img src="/2018/02/10/hexo/hexo-theme-beautify/image-20200511162449080.png" alt></p>
<p>该功能由 <a href="https://github.com/theme-next/theme-next-canvas-nest" target="_blank" rel="noopener">theme-next-canvas-nest</a> 插件提供，在根目录下执行如下命令：</p>
<figure class="highlight crystal"><table><tr><td class="code"><pre><span class="line">$ git clone <span class="symbol">https:</span>/<span class="regexp">/github.com/theme</span>-<span class="keyword">next</span>/theme-<span class="keyword">next</span>-canvas-nest themes/<span class="keyword">next</span>/source/<span class="class"><span class="keyword">lib</span>/<span class="title">canvas</span>-<span class="title">nest</span></span></span><br></pre></td></tr></table></figure>
<p>然后在主题配置文件中设置 <code>canvas_nest: true</code> 即可。</p>
<p>Next v6.5.0 及以上版本支持更多的自定义选项：</p>
<figure class="highlight yml"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">canvas_nest:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">onmobile:</span> <span class="literal">true</span> <span class="comment"># 是否在移动端显示</span></span><br><span class="line">  <span class="attr">color:</span> <span class="string">'0,0,255'</span> <span class="comment"># 动态背景中线条的 RGB 颜色</span></span><br><span class="line">  <span class="attr">opacity:</span> <span class="number">0.5</span> <span class="comment"># 动态背景中线条透明度</span></span><br><span class="line">  <span class="attr">zIndex:</span> <span class="number">-1</span> <span class="comment"># 动态背景的 z-index 属性值</span></span><br><span class="line">  <span class="attr">count:</span> <span class="number">99</span> <span class="comment"># 动态背景中线条数量</span></span><br></pre></td></tr></table></figure>
<h2 id="three-san-wei-dong-xiao">Three 三维动效</h2>
<p><a href="https://github.com/theme-next/theme-next-three" target="_blank" rel="noopener">theme-next-three</a> 插件提供了三个类型的背景动效，应用效果如下：</p>
<p><code><div class="tabs" id="三维动效"><ul class="nav-tabs"><li class="tab active"><a href="#三维动效-1">three-waves</a></li><li class="tab"><a href="#三维动效-2">canvas-lines</a></li><li class="tab"><a href="#三维动效-3">canvas-sphere</a></li></ul><div class="tab-content"><div class="tab-pane active" id="三维动效-1"><p><img src="/2018/02/10/hexo/hexo-theme-beautify/screenshot-9186344.png" alt></p></div><div class="tab-pane" id="三维动效-2"><p><img src="/2018/02/10/hexo/hexo-theme-beautify/image-20200511164017378.png" alt></p></div><div class="tab-pane" id="三维动效-3"><p><img src="/2018/02/10/hexo/hexo-theme-beautify/image-20200511164038461.png" alt></p></div></div></div></code></p>
<p>在根目录下执行如下命令安装相关依赖：</p>
<figure class="highlight crystal"><table><tr><td class="code"><pre><span class="line">$ git clone <span class="symbol">https:</span>/<span class="regexp">/github.com/theme</span>-<span class="keyword">next</span>/theme-<span class="keyword">next</span>-three themes/<span class="keyword">next</span>/source/<span class="class"><span class="keyword">lib</span>/<span class="title">three</span></span></span><br></pre></td></tr></table></figure>
<p>然后在主题配置文件中设置开启对应的动效选项即可。</p>
<figure class="highlight yml"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># JavaScript 3D library.</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/theme-next-three</span></span><br><span class="line"><span class="comment"># three_waves</span></span><br><span class="line"><span class="attr">three_waves:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># canvas_lines</span></span><br><span class="line"><span class="attr">canvas_lines:</span> <span class="literal">false</span></span><br><span class="line"><span class="comment"># canvas_sphere</span></span><br><span class="line"><span class="attr">canvas_sphere:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p>个人认为在站点中添加动态背景并没有实际的意义，只会凭空增加页面内存占用及 CPU 消耗，所以本站没有添加任何动态背景。</p>
<h2 id="sui-ji-san-jiao-si-dai">随机三角丝带</h2>
<p><img src="/2018/02/10/hexo/hexo-theme-beautify/evan-you.png" alt></p>
<p>点击下方按钮下载相应的脚本，并置<code>evan-you.js</code>于<code> themes\next\source\js\</code> 目录下：</p>
<p><code><a class="btn" href="js.zip">
            <i class="fa fa-download"></i>下载
          </a></code></p>
<p>在主题自定义布局文件中添加以下代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout\_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;# 随机三角丝带背景 #&#125;</span><br><span class="line">&#123;% <span class="keyword">if</span> theme.evanyou %&#125;</span><br><span class="line">  &lt;canvas id=<span class="string">"evanyou"</span>&gt;&lt;<span class="regexp">/canvas&gt;</span></span><br><span class="line"><span class="regexp">  &lt;style&gt;</span></span><br><span class="line"><span class="regexp">    #evanyou &#123;</span></span><br><span class="line"><span class="regexp">      position: fixed;</span></span><br><span class="line"><span class="regexp">      width: 100%;</span></span><br><span class="line"><span class="regexp">      height: 100%;</span></span><br><span class="line"><span class="regexp">      top: 0;</span></span><br><span class="line"><span class="regexp">      left: 0;</span></span><br><span class="line"><span class="regexp">      z-index: -1;</span></span><br><span class="line"><span class="regexp">    &#125;</span></span><br><span class="line"><span class="regexp">  &lt;/</span>style&gt;</span><br><span class="line">  &lt;script src=<span class="string">"/js/evan-you.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">&#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure>
<p>如果 custom.swig 文件不存在，需要手动新建并在布局页面中 body 末尾引入：</p>
<figure class="highlight html"><figcaption><span>themes\next\layout\_layout.swig</span></figcaption><table><tr><td class="code"><pre><span class="line"> ...</span><br><span class="line">      &#123;% include '_third-party/exturl.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/bookmark.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/copy-code.swig' %&#125;</span><br><span class="line"></span><br><span class="line">+     &#123;% include '_custom/custom.swig' %&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>在主题配置文件中添加以下代码：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># colorful trilateral riband background</span></span><br><span class="line"><span class="attr">evanyou:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h1 id="tian-jia-kan-ban-niang">添加看板娘</h1>
<p>该功能由 <a href="https://github.com/EYHN/hexo-helper-live2d" target="_blank" rel="noopener">hexo-helper-live2d</a> 插件支持，效果如下图：</p>
<p><img src="/2018/02/10/hexo/hexo-theme-beautify/image-20200511170001505.png" alt></p>
<p>在站点根目录下执行以下命令安装依赖：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ npm <span class="keyword">install</span> <span class="comment">--save hexo-helper-live2d</span></span><br></pre></td></tr></table></figure>
<p>在站点配置文件中添加以下下配置项</p>
<figure class="highlight yml"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Live2D</span></span><br><span class="line"><span class="comment"># https://github.com/EYHN/hexo-helper-live2d</span></span><br><span class="line"><span class="attr">live2d:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">pluginRootPath:</span> <span class="string">live2dw/</span></span><br><span class="line">  <span class="attr">pluginJsPath:</span> <span class="string">lib/</span></span><br><span class="line">  <span class="attr">pluginModelPath:</span> <span class="string">assets/</span> <span class="string">Relative)</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 脚本加载源</span></span><br><span class="line">  <span class="attr">scriptFrom:</span> <span class="string">local</span> <span class="comment"># 默认从本地加载脚本</span></span><br><span class="line">  <span class="comment"># scriptFrom: jsdelivr # 从 jsdelivr CDN 加载脚本</span></span><br><span class="line">  <span class="comment"># scriptFrom: unpkg # 从 unpkg CDN 加载脚本</span></span><br><span class="line">  <span class="comment"># scriptFrom: https://cdn.jsdelivr.net/npm/live2d-widget@3.x/lib/L2Dwidget.min.js # 从自定义地址加载脚本</span></span><br><span class="line">  <span class="attr">tagMode:</span> <span class="literal">false</span> <span class="comment"># 只在有 &#123;&#123; live2d() &#125;&#125; 标签的页面上加载 / 在所有页面上加载</span></span><br><span class="line">  <span class="attr">log:</span> <span class="literal">false</span> <span class="comment"># 是否在控制台打印日志</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 选择看板娘模型</span></span><br><span class="line">  <span class="attr">model:</span></span><br><span class="line">    <span class="attr">use:</span> <span class="string">live2d-widget-model-shizuku</span>  <span class="comment"># npm package的名字</span></span><br><span class="line">    <span class="comment"># use: wanko # /live2d_models/ 目录下的模型文件夹名称</span></span><br><span class="line">    <span class="comment"># use: ./wives/wanko # 站点根目录下的模型文件夹名称</span></span><br><span class="line">    <span class="comment"># use: https://cdn.jsdelivr.net/npm/live2d-widget-model-wanko@1.0.5/assets/wanko.model.json # 自定义网络数据源</span></span><br><span class="line">  <span class="attr">display:</span></span><br><span class="line">    <span class="attr">position:</span> <span class="string">left</span> <span class="comment"># 显示在左边还是右边</span></span><br><span class="line">    <span class="attr">width:</span> <span class="number">100</span> <span class="comment"># 宽度</span></span><br><span class="line">    <span class="attr">height:</span> <span class="number">180</span> <span class="comment"># 高度</span></span><br><span class="line">  <span class="attr">mobile:</span></span><br><span class="line">    <span class="attr">show:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">react:</span></span><br><span class="line">    <span class="attr">opacityDefault:</span> <span class="number">0.7</span> <span class="comment"># 默认透明度</span></span><br></pre></td></tr></table></figure>
<p>更多配置参数请查看 <a href="https://l2dwidget.js.org/docs/class/src/index.js~L2Dwidget.html#instance-method-init" target="_blank" rel="noopener">L2Dwidget | live2d-widget.js</a></p>
<p>此时重启服务器暂时还看不到看板娘，需要手动下载或安装模型资源。可以从 <a href="https://huaji8.top/post/live2d-plugin-2.0/" target="_blank" rel="noopener">hexo live2d 模型预览</a> 里找到你喜欢的角色，然后根据 <a href="https://github.com/xiazeyu/live2d-widget-models" target="_blank" rel="noopener">live2d-widget-models</a> 中提供的方法来下载模型数据.</p>
<p>例如通过以下命令下载模型 shizuku：</p>
<figure class="highlight gams"><table><tr><td class="code"><pre><span class="line"><span class="symbol">$</span> npm install live2d-widget-<span class="keyword">model</span>-shizuku</span><br></pre></td></tr></table></figure>
<p>因为修改了站点配置文件，所以需要重启服务器才能预览模型效果。</p>
<p>如果设置了 <code>live2d.tagMode: true</code>，则可以在指定页面中插入以下标签：</p>
<figure class="highlight clojure"><table><tr><td class="code"><pre><span class="line">&#123;&#123; live2d() &#125;&#125;</span><br></pre></td></tr></table></figure>
<p>只有拥有该标签的页面才会渲染 live2d 模型，这样以来就可以精确控制在哪些页面上显示看板娘了。</p>
<p>如果只想在一级菜单页面上显示看板娘，可以在 Header 模板中添加以下代码：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout\_partials\header\index.swig</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="addition">+ &#123;% if is_index %&#125;</span></span><br><span class="line"><span class="addition">+   &#123;&#123; live2d() &#125;&#125;</span></span><br><span class="line"><span class="addition">+ &#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure>
<p>个人认为在文章内出现看板娘将会影响读者注意力的集中，毕竟一篇博客里最重要的是内容，而不是这些花里胡哨转移注意力的东西。所以本站只在一级菜单页面添加了看板娘，文章页面则保持极致精简的阅读体验。</p>
<h1 id="bian-yuan-bai-dong-xiao-guo">边缘摆动效果</h1>
<p>点击下方按钮下载脚本，并置<code> wobblewindow.js</code>于 themes\next\source\js\ 目录下：</p>
<p><code><a class="btn" href="js.zip">
            <i class="fa fa-download"></i>wobblewindow.js
          </a></code></p>
<p>在主题自定义布局文件中添加以下代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout\_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;# wobble窗口摆动特效 #&#125;</span><br><span class="line">&#123;% <span class="keyword">if</span> theme.wobble %&#125;</span><br><span class="line">  &lt;script src=<span class="string">"/js/wobblewindow.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">  &lt;script&gt;</span></span><br><span class="line"><span class="regexp">    /</span><span class="regexp">/只在桌面版网页启用特效</span></span><br><span class="line"><span class="regexp">    if( window.innerWidth &gt; 768  )&#123;</span></span><br><span class="line"><span class="regexp">      $(document).ready(function () &#123;</span></span><br><span class="line"><span class="regexp">        &#123;% if theme.wobble.header %&#125;</span></span><br><span class="line"><span class="regexp">          $('#header').wobbleWindow(&#123;</span></span><br><span class="line"><span class="regexp">            radius: &#123;&#123; theme.wobble.radius &#125;&#125;,</span></span><br><span class="line"><span class="regexp">            movementTop: false,</span></span><br><span class="line"><span class="regexp">            movementLeft: false,</span></span><br><span class="line"><span class="regexp">            movementRight: false,</span></span><br><span class="line"><span class="regexp">            debug: false,</span></span><br><span class="line"><span class="regexp">          &#125;);</span></span><br><span class="line"><span class="regexp">        &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">        &#123;% if theme.wobble.sidebar %&#125;</span></span><br><span class="line"><span class="regexp">          $('#sidebar').wobbleWindow(&#123;</span></span><br><span class="line"><span class="regexp">            radius: &#123;&#123; theme.wobble.radius &#125;&#125;,</span></span><br><span class="line"><span class="regexp">            movementLeft: false,</span></span><br><span class="line"><span class="regexp">            movementTop: false,</span></span><br><span class="line"><span class="regexp">            movementBottom: false,</span></span><br><span class="line"><span class="regexp">            position: 'fixed',</span></span><br><span class="line"><span class="regexp">            debug: false,</span></span><br><span class="line"><span class="regexp">          &#125;);</span></span><br><span class="line"><span class="regexp">        &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">        &#123;% if theme.wobble.footer %&#125;</span></span><br><span class="line"><span class="regexp">          $('#footer').wobbleWindow(&#123;</span></span><br><span class="line"><span class="regexp">            radius: &#123;&#123; theme.wobble.radius &#125;&#125;,</span></span><br><span class="line"><span class="regexp">            movementBottom: false,</span></span><br><span class="line"><span class="regexp">            movementLeft: false,</span></span><br><span class="line"><span class="regexp">            movementRight: false,</span></span><br><span class="line"><span class="regexp">            offsetX: &#123;&#123; theme.wobble.offset &#125;&#125;,</span></span><br><span class="line"><span class="regexp">            position: 'absolute',</span></span><br><span class="line"><span class="regexp">            debug: false,</span></span><br><span class="line"><span class="regexp">          &#125;);</span></span><br><span class="line"><span class="regexp">        &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp">      &#125;);</span></span><br><span class="line"><span class="regexp">    &#125;</span></span><br><span class="line"><span class="regexp">  &lt;/</span>script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<p>在自定义样式文件中添加以下样式：</p>
<figure class="highlight js"><figcaption><span>source/_data/style.styl</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment">//窗口波动效果相关样式</span></span><br><span class="line"><span class="keyword">if</span> hexo-config(<span class="string">'wobble'</span>)  &#123;</span><br><span class="line">  .sidebar &#123;</span><br><span class="line">    box-shadow: none;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  .wobbleTransparentBK&#123;</span><br><span class="line">    background-color: rgba(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) !important;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  .wobbleTransparentLine&#123;</span><br><span class="line">    border-color: rgba(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) !important;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//Next.Muse中为Header和Footer添加背景色</span></span><br><span class="line">  #header, #footer &#123;</span><br><span class="line">    background-color: rgb(<span class="number">245</span>, <span class="number">245</span>, <span class="number">245</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//防止sidebar和footer同时开启动效时堆叠异常</span></span><br><span class="line">  #sidebar, header &#123;</span><br><span class="line">    z-index: <span class="number">1</span> !important;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//防止挡住页末文章的阅读全文按钮</span></span><br><span class="line">  .main &#123;</span><br><span class="line">    padding-bottom: <span class="number">200</span>px;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Next.Muse 主题方案中 Header 和 Footer 是没有背景色的，所以需要添加背景色后才能看出边缘摆动效果。另外，实现边缘摆动效果所需的 <code>z-index</code> 属性可能会导致元素堆叠异常，需要添加以上样式来矫正。</p>
<p>在主题配置文件中添加以下代码：</p>
<figure class="highlight yml"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># window woblle</span></span><br><span class="line"><span class="attr">wobble:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span>  <span class="comment"># 是否开启边缘波动效果</span></span><br><span class="line">  <span class="attr">radius:</span> <span class="number">50</span>  <span class="comment"># 波动半径</span></span><br><span class="line">  <span class="attr">sidebar:</span> <span class="literal">true</span>  <span class="comment"># 开启侧边栏边缘摆动</span></span><br><span class="line">  <span class="attr">header:</span> <span class="literal">true</span>  <span class="comment"># 开启头部边缘摆动</span></span><br><span class="line">  <span class="attr">footer:</span> <span class="literal">true</span>  <span class="comment"># 开启脚部边缘摆动</span></span><br></pre></td></tr></table></figure>
<p>用户可以根据需要在配置文件中为选择开启边缘摆动效果的布局元素。刷新浏览器，然后将鼠标移动到布局边缘上尽情的挑逗它吧。如果从本地加载 JS 脚本速度较慢，可以考虑将脚本放到 CDN 上再引入。</p>
<h1 id="ge-xing-hua-hui-dao-ding-bu">个性化回到顶部</h1>
<p>效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-theme-beautify/image-20200511170913805.png" alt></p>
<p>原理很简单，将 back-to-top 按钮添加图片背景，并添加 CSS3 动效即可。首先，找到自己喜欢的图片素材放到 source\images\ 目录下。你可以点击下方按钮下载本站所使用的小猫上吊素材（ 小猫咪这么可爱，当然要多放点孜然啦…）</p>
<p><code><a class="btn" href="scroll.png">
            <i class="fa fa-download"></i>下载图片
          </a></code></p>
<p>然后在自定义样式文件中添加如下代码：</p>
<figure class="highlight css"><figcaption><span>source/_data/style.styl</span></figcaption><table><tr><td class="code"><pre><span class="line">//自定义回到顶部样式</span><br><span class="line"><span class="selector-class">.back-to-top</span> &#123;</span><br><span class="line">  <span class="attribute">right</span>: <span class="number">60px</span>;</span><br><span class="line">  width: 70px;  //图片素材宽度</span><br><span class="line">  height: 900px;  //图片素材高度</span><br><span class="line">  <span class="selector-tag">top</span>: <span class="selector-tag">-900px</span>;</span><br><span class="line">  <span class="selector-tag">bottom</span>: <span class="selector-tag">unset</span>;</span><br><span class="line">  <span class="selector-tag">transition</span>: <span class="selector-tag">all</span> <span class="selector-class">.5s</span> <span class="selector-tag">ease-in-out</span>;</span><br><span class="line">  background: url("/images/scroll.png");</span><br><span class="line"></span><br><span class="line">  //隐藏箭头图标</span><br><span class="line">  &gt; <span class="selector-tag">i</span> &#123;</span><br><span class="line">    <span class="attribute">display</span>: none;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &amp;<span class="selector-class">.back-to-top-on</span> &#123;</span><br><span class="line">    <span class="attribute">bottom</span>: unset;</span><br><span class="line">    <span class="attribute">top</span>: <span class="number">100vh</span> &lt; (<span class="number">900px</span> + <span class="number">200px</span>) ? <span class="built_in">calc</span>( <span class="number">100vh</span> - <span class="number">900px</span> - <span class="number">200px</span> ) : <span class="number">0px</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>刷新浏览器即可预览效果。</p>
<h1 id="shu-biao-dian-ji-te-xiao">鼠标点击特效</h1>
<p>从各个站点里搜罗了以下四个比较常用的鼠标点击特效：</p>
<p><code><div class="tabs" id="点击特效"><ul class="nav-tabs"><li class="tab active"><a href="#点击特效-1">礼花特效</a></li><li class="tab"><a href="#点击特效-2">爆炸特效</a></li><li class="tab"><a href="#点击特效-3">浮出爱心</a></li><li class="tab"><a href="#点击特效-4">浮出文字</a></li></ul><div class="tab-content"><div class="tab-pane active" id="点击特效-1"><p><img src="/2018/02/10/hexo/hexo-theme-beautify/cursor-fireworks.gif" alt></p></div><div class="tab-pane" id="点击特效-2"><p><img src="/2018/02/10/hexo/hexo-theme-beautify/cursor-explosion.gif" alt></p></div><div class="tab-pane" id="点击特效-3"><p><img src="/2018/02/10/hexo/hexo-theme-beautify/cursor-love.gif" alt></p></div><div class="tab-pane" id="点击特效-4"><p><img src="/2018/02/10/hexo/hexo-theme-beautify/cursor-text.gif" alt></p></div></div></div></code></p>
<p>点击下方按钮下载相应的脚本，并置<code>fireworks.js.css</code>,<code>explosion.min.js</code>,<code>love.min.js</code>,<code>text.js</code>于 themes\next\source\js\cursor\ 目录下：</p>
<p><code><a class="btn" href="js.zip">
            <i class="fa fa-download"></i>礼花特效
          </a></code></p>
<p><code><a class="btn" href="js.zip">
            <i class="fa fa-download"></i>爆炸特效
          </a></code></p>
<p><code><a class="btn" href="js.zip">
            <i class="fa fa-download"></i>浮出爱心
          </a></code></p>
<p><code><a class="btn" href="js.zip">
            <i class="fa fa-download"></i>浮出文字
          </a></code></p>
<p>在主题自定义布局文件中添加以下代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout\_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;# 鼠标点击特效 #&#125;</span><br><span class="line">&#123;% <span class="keyword">if</span> theme.cursor_effect == <span class="string">"fireworks"</span> %&#125;</span><br><span class="line">  &lt;script <span class="keyword">async</span> src=<span class="string">"/js/cursor/fireworks.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">&#123;% elseif theme.cursor_effect == "explosion" %&#125;</span></span><br><span class="line"><span class="regexp">  &lt;canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" &gt;&lt;/</span>canvas&gt;</span><br><span class="line">  &lt;script src=<span class="string">"//cdn.bootcss.com/animejs/2.2.0/anime.min.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">  &lt;script async src="/</span>js/cursor/explosion.min.js<span class="string">"&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="string">&#123;% elseif theme.cursor_effect == "</span>love<span class="string">" %&#125;</span></span><br><span class="line"><span class="string">  &lt;script async src="</span>/js/cursor/love.min.js<span class="string">"&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="string">&#123;% elseif theme.cursor_effect == "</span>text<span class="string">" %&#125;</span></span><br><span class="line"><span class="string">  &lt;script async src="</span>/js/cursor/text.js<span class="string">"&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure>
<p>如果 custom.swig 文件不存在，需要手动新建并在布局页面中 body 末尾引入：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout\_layout.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">      ...</span><br><span class="line">      &#123;% include '_third-party/exturl.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/bookmark.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/copy-code.swig' %&#125;</span><br><span class="line"></span><br><span class="line"><span class="addition">+     &#123;% include '_custom/custom.swig' %&#125;</span></span><br><span class="line">    &lt;/body&gt;</span><br><span class="line">  &lt;/html&gt;</span><br></pre></td></tr></table></figure>
<p>在主题配置文件中添加以下代码：</p>
<figure class="highlight yml"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># mouse click effect: fireworks | explosion | love | text</span></span><br><span class="line"><span class="attr">cursor_effect:</span> <span class="string">fireworks</span></span><br></pre></td></tr></table></figure>
<p>这样即可在配置文件中一键快速切换鼠标点击特效。</p>
<h1 id="da-zi-te-xiao">打字特效</h1>
<p><img src="/2018/02/10/hexo/hexo-theme-beautify/typing-effect.gif" alt></p>
<p>点击下方按钮下载相应的脚本，并置<code>activate-power-mode.min.js.css</code>于 themes\next\source\js\ 目录下：</p>
<p><code><a class="btn" href="js.zip">
            <i class="fa fa-download"></i>打字特效
          </a></code></p>
<p>在主题自定义布局文件中添加以下代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout\_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;# 打字特效 #&#125;</span><br><span class="line">&#123;% <span class="keyword">if</span> theme.typing_effect %&#125;</span><br><span class="line">  &lt;script src=<span class="string">"/js/activate-power-mode.min.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">  &lt;script&gt;</span></span><br><span class="line"><span class="regexp">    POWERMODE.colorful = &#123;&#123; theme.typing_effect.colorful &#125;&#125;;</span></span><br><span class="line"><span class="regexp">    POWERMODE.shake = &#123;&#123; theme.typing_effect.shake &#125;&#125;;</span></span><br><span class="line"><span class="regexp">    document.body.addEventListener('input', POWERMODE);</span></span><br><span class="line"><span class="regexp">  &lt;/</span>script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<p>如果 custom.swig 文件不存在，需要手动新建并在布局页面中 body 末尾引入：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout\_layout.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">      ...</span><br><span class="line">      &#123;% include '_third-party/exturl.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/bookmark.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/copy-code.swig' %&#125;</span><br><span class="line"></span><br><span class="line"><span class="addition">+     &#123;% include '_custom/custom.swig' %&#125;</span></span><br><span class="line">    &lt;/body&gt;</span><br><span class="line">  &lt;/html&gt;</span><br></pre></td></tr></table></figure>
<p>在主题配置文件中添加以下代码：</p>
<figure class="highlight yml"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># typing effect</span></span><br><span class="line"><span class="attr">typing_effect:</span></span><br><span class="line">  <span class="attr">colorful:</span> <span class="literal">true</span>  <span class="comment"># 礼花特效</span></span><br><span class="line">  <span class="attr">shake:</span> <span class="literal">false</span>  <span class="comment"># 震动特效</span></span><br></pre></td></tr></table></figure>
<p>如果从本地加载 JS 脚本速度较慢，可以考虑将脚本放到 CDN 上再引入。</p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://asdfv1929.github.io/2018/01/28/add-the-end/" target="_blank" rel="noopener">asdfv1929 | Hexo NexT 主题内给每篇文章后添加结束标语</a></li>
<li><a href="https://fjkang.github.io/2017/12/08/%E6%B7%BB%E5%8A%A0%E4%B8%80%E4%B8%AA%E8%90%8C%E7%89%A9/" target="_blank" rel="noopener">FJKang | 添加一个萌物</a></li>
<li><a href="http://evanyou.me/" target="_blank" rel="noopener">尤雨溪的个人主页</a></li>
<li><a href="https://diygod.me/" target="_blank" rel="noopener">DIYgod 的博客</a></li>
<li><a href="https://www.ofind.cn/" target="_blank" rel="noopener">猪猪侠的博客</a></li>
<li><a href="https://qianling.pw/hexo-optimization/" target="_blank" rel="noopener">千灵夙赋 | Hexo 优化汇总</a></li>
<li><a href="https://www.ilxtx.com/comment-input-effects.html" target="_blank" rel="noopener">龙笑天下 | 给 WordPress 博客网站添加评论输入打字礼花及震动特效</a></li>
<li><a href="https://sunhwee.com/posts/6e8839eb.html#toc-heading-26" target="_blank" rel="noopener">Hexo+Github博客搭建完全教程</a></li>
<li><a href="https://www.aomanhao.top/2019/11/04/hexo_clock/" target="_blank" rel="noopener">粒子时钟特效</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo写作技巧</title>
    <url>/2018/02/09/hexo/hexo-writing-skills/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2018/02/09/hexo/hexo-writing-skills/writing.jpg" alt></p>
<p>本文介绍 Hexo 博客的写作技巧。</p>
<a id="more"></a>
<h1 id="kai-shi-xie-zuo">开始写作</h1>
<p>在博客目录下执行如下命令新建一篇文章</p>
<figure class="highlight gauss"><table><tr><td class="code"><pre><span class="line">$ hexo <span class="keyword">new</span> [layout] &lt;<span class="built_in">title</span>&gt;</span><br></pre></td></tr></table></figure>
<p>如果未指定文章的布局（layout），则默认使用 <code>post</code> 布局，生成的文档存放于 <code>source\_posts\</code> 目录下，打开后使用 Markdown 语法进行写作，保存后刷新浏览器即可看到文章。</p>
<h2 id="bu-ju">布局</h2>
<p>布局是什么概念呢，你可以理解为新建文档时的一个模板，基于布局生成的文档将会继承布局的样式。</p>
<p>Hexo 默认有三种布局：<code>post</code>、 <code>page</code> 和 <code>draft</code>，用户可以在 <code>scaffolds</code> 目录下新建文档来自定义布局格式，还可以修改站点配置文件中的 <code>default_layout</code>参数来指定生成文档时的默认布局。</p>
<h3 id="wen-zhang-post">文章(post)</h3>
<p>基于 <code>post</code> 布局生成的文档存在于 <code>source\_posts\</code> 目录下，该目录下的文档会作为博客正文显示在网站中。</p>
<h3 id="ye-mian-page">页面（page）</h3>
<p><code>page</code> 布局用于生成类似 <strong>首页</strong> 和 <strong>归档</strong> 这样的页面。默认的 Next 主题样式中只包含首页和归档这两个链接，可以通过修改主题配置文件中的 <code>menu</code> 字段来新增更多页面菜单。</p>
<figure class="highlight diff"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: / || home</span><br><span class="line">  about: /about/ || user</span><br><span class="line"><span class="addition">+ tags: /tags/ || tags</span></span><br><span class="line"><span class="addition">+ categories: /categories/ || th</span></span><br><span class="line"><span class="addition">+ archives: /archives/ || archive</span></span><br></pre></td></tr></table></figure>
<p>其中，<code>||</code> 之前的值表示菜单链接，之后的值表示所用的 <code>FontAwesome</code> 图标名称。</p>
<p>刷新页面后即可看到页面内多了几项菜单。</p>
<p><img src="/2018/02/09/hexo/hexo-writing-skills/add.png" alt></p>
<p>此时点击 <strong>关于</strong>、<strong>标签</strong> 和 <strong>分类</strong> 都会跳转到 404 页面，原因是我们只开放了页面入口，却没有创造对应于链接的页面视图，此时就需要通过 <code>hexo new page </code> 命令来新建页面。</p>
<p>基于 <code>page</code> 布局的新建命令将会在 <code>source</code> 目录下新建一个 <code>title</code> 文件夹，并在该文件夹下创建一个 <code>index.md</code> 文件，编辑该文件即可修改页面内容。</p>
<p>例如，通过 <code>hexo new page tags</code> 命令将会生成如下目录。</p>
<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">└──  <span class="keyword">source</span>             </span><br><span class="line">  ├── _posts          </span><br><span class="line">  └── <span class="keyword">tags</span></span><br><span class="line">    └── <span class="built_in">index</span>.md</span><br></pre></td></tr></table></figure>
<p>编辑 <code>index.md</code> 文件，在 Front-Matter 中添加 <code>type: tags</code> ，Next 主题将自动在该页面内显示标签云。</p>
<figure class="highlight diff"><figcaption><span>source\tags\index.md</span></figcaption><table><tr><td class="code"><pre><span class="line">  title: 标签</span><br><span class="line">  date: 2018-10-19 22:57:00</span><br><span class="line"><span class="addition">+ type: tags</span></span><br></pre></td></tr></table></figure>
<div class="note info">
            <p>Front-Matter 是文件最上方以 <code>---</code> 分隔的区域，用于指定本文件的各种参数值</p>
          </div>
<p>在菜单中点击 <strong>标签</strong> 跳转到刚创建的标签页面。</p>
<p><img src="/2018/02/09/hexo/hexo-writing-skills/tags.png" alt></p>
<p>同理可通过 <code>page</code> 布局生成 <strong>关于</strong> 和 <strong>分类</strong> 两个页面。</p>
<h3 id="cao-gao-draft">草稿（draft）</h3>
<p><code>draft</code> 布局用于创建草稿，生成的文档存在于 source_drafts\ 目录中，默认配置下将不会把该目录下的文档渲染到网站中。</p>
<p>通过以下命令将草稿发布为正式文章：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">$ hexo publish <span class="tag">&lt;<span class="name">title</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>该命令会将 source_drafts\ 目录下,考虑到一些文章可能需要数天才能完成，建议将新建文档时的默认布局设置为 <code>draft</code>：</p>
<figure class="highlight diff"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="deletion">- default_layout: post</span></span><br><span class="line"><span class="addition">+ default_layout: draft</span></span><br></pre></td></tr></table></figure>
<h2 id="biao-qian-yu-fen-lei">标签与分类</h2>
<p>我们刚提到了标签，也提到了分类，那么标签和分类是什么，其区别是什么。</p>
<p>标签和分类都是用于对文章进行归档的一种方式，标签是一种列表结构，而分类是一种树结构。我们以人作为例子，从标签的角度考虑，我可以拥有程序员、高颜值、幽默等标签，这些标签之间没有层级关系；从分类的角度考虑，我是亚洲人、中国人、河南人，这些分类之间是有明确的包含关系的。</p>
<p>可以在 Front-Matter 中添加 <code>catergories</code> 和 <code>tags</code> 字段为文章添加标签和分类，如我为本文添加了 <strong>Hexo</strong> 和 <strong>Markdown</strong> 两个标签，并将其归类到了 <strong>技术 / 博客</strong> 类别，对应的 Front-Matter 结构如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">title:</span> <span class="string">写作技巧</span></span><br><span class="line"><span class="attr">mathjax:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">tags:</span> <span class="string">[hexo,</span> <span class="string">markdown]</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2020</span><span class="number">-05</span><span class="number">-10</span> <span class="number">10</span><span class="string">:35:32</span></span><br><span class="line"><span class="attr">categories:</span> <span class="string">技术/博客</span></span><br><span class="line"><span class="attr">password:</span> </span><br><span class="line"><span class="attr">top:</span></span><br></pre></td></tr></table></figure>
<h2 id="markdown-ji-ben-yu-fa">Markdown 基本语法</h2>
<p>Markdown 是一种标记语言，语法简单，易阅读易编写，可以让用户完全脱离鼠标写出样式丰富的文档，广受程序员喜爱，目前许多网站都已经支持通过 Markdown 语法来写文章或者发表评论。</p>
<table>
<thead>
<tr>
<th style="text-align:left">元素</th>
<th style="text-align:left">Markdown 语法</th>
<th style="text-align:left">效果预览</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">标题</td>
<td style="text-align:left"><code># 标题1</code> <code>## h2</code> <code>### h3</code></td>
<td style="text-align:left">标题一标题二标题三</td>
</tr>
<tr>
<td style="text-align:left">加粗</td>
<td style="text-align:left"><code>**文字加粗**</code></td>
<td style="text-align:left"><strong>文字加粗</strong></td>
</tr>
<tr>
<td style="text-align:left">引用</td>
<td style="text-align:left"><code>&gt; 引用文字</code></td>
<td style="text-align:left">引用文字</td>
</tr>
<tr>
<td style="text-align:left">有序列表</td>
<td style="text-align:left"><code>1. 第一项</code> <code>2. 第二项</code> <code>3. 第三项</code></td>
<td style="text-align:left">第一项第二项第三项</td>
</tr>
<tr>
<td style="text-align:left">无序列表</td>
<td style="text-align:left"><code>- 第一项</code> <code>- 第二项</code> <code>- 第三项</code></td>
<td style="text-align:left">第一项第二项第三项</td>
</tr>
<tr>
<td style="text-align:left">链接</td>
<td style="text-align:left"><code>[链接](url)</code></td>
<td style="text-align:left"><a href="http://yearito.cn/posts/url" target="_blank" rel="noopener">链接</a></td>
</tr>
<tr>
<td style="text-align:left">图片</td>
<td style="text-align:left"><code>![图片](image.jpg)</code></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">水平线</td>
<td style="text-align:left"><code>---</code></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">代码</td>
<td style="text-align:left"><code>code</code></td>
<td style="text-align:left"><code>code</code></td>
</tr>
<tr>
<td style="text-align:left">代码块</td>
<td style="text-align:left"><code>code snippet</code></td>
<td style="text-align:left"><code>code snippet</code></td>
</tr>
</tbody>
</table>
<div class="note info">
            <p>更多语法请参考 <a href="https://www.markdownguide.org/basic-syntax" target="_blank" rel="noopener">基础语法 | Markdown Guide</a> 和 <a href="https://www.markdownguide.org/extended-syntax" target="_blank" rel="noopener">扩展语法 | Markdown Guide</a></p>
          </div>
<h1 id="hexo-nei-zhi-biao-qian">Hexo 内置标签</h1>
<h2 id="wen-ben-ju-zhong-biao-qian">文本居中标签</h2>
<p>居中标签效果如下：</p>
<blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p>我不去想是否能够成功，既然选择了远方，便只顾风雨兼程。</p>

            <i class="fa fa-quote-right"></i>
          </blockquote>
<p>一般在引用单行文本时使用，如作为文章开篇题词。可以通过以下几种方式使用该标签：</p>
<figure class="highlight django"><table><tr><td class="code"><pre><span class="line"><span class="xml"><span class="comment">&lt;!-- HTML方式: 直接在 Markdown 文件中编写 HTML 来调用 --&gt;</span></span></span><br><span class="line"><span class="xml"><span class="comment">&lt;!-- 其中 class="blockquote-center" 是必须的 --&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">blockquote</span> <span class="attr">class</span>=<span class="string">"blockquote-center"</span>&gt;</span>blah blah blah<span class="tag">&lt;/<span class="name">blockquote</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="comment">&lt;!-- 标签方式 --&gt;</span></span></span><br><span class="line"><span class="template-tag">&#123;% <span class="name">centerquote</span> %&#125;</span><span class="xml">blah blah blah</span><span class="template-tag">&#123;% <span class="name">endcenterquote</span> %&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="comment">&lt;!-- 标签别名 --&gt;</span></span></span><br><span class="line"><span class="template-tag">&#123;% <span class="name">cq</span> %&#125;</span><span class="xml"> blah blah blah </span><span class="template-tag">&#123;% <span class="name">endcq</span> %&#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="dai-ma-kuai-jin-jie-yong-fa">代码块进阶用法</h2>
<p>可以通过为代码块附加参数的形式为其添加更丰富的信息提示，效果如下：</p>
<figure class="highlight python"><figcaption><span>hello world</span><a href="https://jeffery0628.github.io/" target="_blank" rel="noopener">我的主页</a></figcaption><table><tr><td class="code"><pre><span class="line">print(<span class="string">"Hello world!"</span>);</span><br></pre></td></tr></table></figure>
<p>代码块进阶语法规则：</p>
<div class="note ">
            <p>``` [language] [title] [url] [link text]<br>code snippet<br>```</p>
          </div>
<p>其中，各参数意义如下：</p>
<ul>
<li>langugae：语言名称，引导渲染引擎正确解析并高亮显示关键字</li>
<li>title：代码块标题，将会显示在左上角</li>
<li>url：链接地址，如果没有指定 link text 则会在右上角显示 link</li>
<li>link text：链接名称，指定 url 后有效，将会显示在右上角</li>
</ul>
<p>url 必须为有效链接地址才会以链接的形式显示在右上角，否则将作为标题显示在左上角。以 url 为分界，左侧除了第一个单词会被解析为 language，其他所有单词都会被解析为 title，而右侧的所有单词都会被解析为 link text。如果不想填写 title，可以在 language 和 url 之间添加至少三个空格。</p>
<p>可以在站点配置文件中设置 <code>highlight.auto_detect: true</code> 来开启自动语言检测高亮。</p>
<figure class="highlight diff"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">highlight:</span><br><span class="line">   enable: true</span><br><span class="line">   line_number: false</span><br><span class="line"><span class="deletion">-  auto_detect: false</span></span><br><span class="line"><span class="addition">+  auto_detect: true</span></span><br><span class="line">   tab_replace:</span><br></pre></td></tr></table></figure>
<p>如果设置语言为 diff，可以在代码前添加 <code>+</code> 和 <code>-</code> 来使用如上所示的高亮增删行提示效果，在展示代码改动痕迹时比较实用。</p>
<h2 id="note-biao-qian">note 标签</h2>
<p>通过 note 标签可以为段落添加背景色，语法如下：</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">&#123;%<span class="built_in"> note </span>[class] %&#125;</span><br><span class="line">文本内容 (支持行内标签)</span><br><span class="line">&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure>
<p>支持的 class 种类包括 <code>default</code> <code>primary</code> <code>success</code> <code>info</code> <code>warning</code> <code>danger</code>，也可以不指定 class。</p>
<p>各种 class 种类的效果如下：</p>
<div class="note primary">
            <p><strong>primary</strong> note tag</p>
          </div>
<div class="note success">
            <p><strong>success</strong> note tag</p>
          </div>
<div class="note info">
            <p><strong>info</strong> note tag</p>
          </div>
<div class="note warning">
            <p><strong>warning</strong> note tag</p>
          </div>
<div class="note danger">
            <p><strong>danger</strong> note tag</p>
          </div>
<div class="note ">
            <p>undefined class note tag</p>
          </div>
<p>更多配置可在主题配置文件中设置</p>
<figure class="highlight css"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">note</span>:</span><br><span class="line">  # <span class="selector-tag">Note</span> 标签样式预设</span><br><span class="line">  style: modern  # simple | modern | flat | disabled</span><br><span class="line">  <span class="selector-tag">icons</span>: <span class="selector-tag">false</span>  # 是否显示图标</span><br><span class="line">  <span class="selector-tag">border_radius</span>: 3  # 圆角半径</span><br><span class="line">  <span class="selector-tag">light_bg_offset</span>: 0  # 默认背景减淡效果，以百分比计算</span><br></pre></td></tr></table></figure>
<h2 id="label-biao-qian">label 标签</h2>
<p>通过 label 标签可以为文字添加背景色，语法如下：</p>
<figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line">&#123;% label [<span class="class"><span class="keyword">class</span>]<span class="meta">@text</span>  %&#125;</span></span><br></pre></td></tr></table></figure>
<p>支持的 class 种类包括 <code>default</code> <code>primary</code> <code>success</code> <code>info</code> <code>warning</code> <code>danger</code>，默认使用 <code>default</code> 作为缺省。</p>
<figure class="highlight puppet"><table><tr><td class="code"><pre><span class="line">I heard the echo, &#123;% label default@from the valleys and the heart %&#125;</span><br><span class="line">Open to the lonely soul <span class="keyword">of</span> &#123;% label info@sickle harvesting %&#125;</span><br><span class="line"><span class="keyword">Repeat</span> <span class="keyword">outrightly</span>, but also repeat the well-being of</span><br><span class="line"><span class="keyword">Eventually</span> &#123;% label warning@swaying <span class="keyword">in</span> the desert oasis %&#125;</span><br><span class="line">&#123;% label success@I believe %&#125; <span class="keyword">I</span> <span class="keyword">am</span></span><br><span class="line">&#123;% label primary@Born as the bright summer flowers %&#125;</span><br><span class="line"><span class="keyword">Do</span> <span class="keyword">not</span> <span class="keyword">withered</span> <span class="keyword">undefeated</span> <span class="keyword">fiery</span> <span class="keyword">demon</span> <span class="keyword">rule</span></span><br><span class="line"><span class="keyword">Heart</span> <span class="keyword">rate</span> <span class="keyword">and</span> <span class="keyword">breathing</span> <span class="keyword">to</span> <span class="keyword">bear</span> &#123;% label danger@the load of the cumbersome %&#125;</span><br><span class="line"><span class="keyword">Bored</span></span><br></pre></td></tr></table></figure>
<p>I heard the echo, <span class="label default">from the valleys and the heart</span><br>
Open to the lonely soul of <span class="label info">sickle harvesting</span><br>
Repeat outrightly, but also repeat the well-being of<br>
Eventually <span class="label warning">swaying in the desert oasis</span></p>
<span class="label success">I believe</span> I am
<span class="label primary">Born as the bright summer flowers</span>
<p>Do not withered undefeated fiery demon rule<br>
Heart rate and breathing to bear <span class="label danger">the load of the cumbersome</span><br>
Bored</p>
<p>可在主题配置文件中设置 <code>label: false</code> 来取消 label 标签默认 CSS 样式。</p>
<h2 id="button-an-niu">button 按钮</h2>
<p>通过 button 标签可以快速添加带有主题样式的按钮，语法如下：</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line">&#123;% button /<span class="type">path</span>/<span class="keyword">to</span>/url/, <span class="type">text</span>, icon [<span class="keyword">class</span>], title %&#125;</span><br></pre></td></tr></table></figure>
<p>也可简写为：</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line">&#123;% btn /<span class="type">path</span>/<span class="keyword">to</span>/url/, <span class="type">text</span>, icon [<span class="keyword">class</span>], title %&#125;</span><br></pre></td></tr></table></figure>
<p>其中， 图标 ID 来源于 <a href="https://fontawesome.com/v4.7.0/icons/" target="_blank" rel="noopener">FontAwesome</a> 。</p>
<p>使用示例如下：</p>
<figure class="highlight django"><table><tr><td class="code"><pre><span class="line"><span class="template-tag">&#123;% <span class="name">btn</span> #, 文本 %&#125;</span></span><br><span class="line"><span class="template-tag">&#123;% <span class="name">btn</span> #, 文本 &amp; 标题,, 标题 %&#125;</span></span><br><span class="line"><span class="template-tag">&#123;% <span class="name">btn</span> #, 文本 &amp; 图标, home %&#125;</span></span><br><span class="line"><span class="template-tag">&#123;% <span class="name">btn</span> #, 文本 &amp; 大图标 (固定宽度), home fa-fw fa-lg %&#125;</span></span><br></pre></td></tr></table></figure>
<a class="btn" href="#">
            <i class="fa fa-"></i>文本
          </a>
<a class="btn" href="#" title="标题">
            <i class="fa fa-"></i>文本 & 标题
          </a>
<a class="btn" href="#">
            <i class="fa fa-home"></i>文本 & 图标
          </a>
<a class="btn" href="#">
            <i class="fa fa-home fa-fw fa-lg"></i>文本 & 大图标 (固定宽度)
          </a>
<h2 id="tab-biao-qian">tab 标签</h2>
<p>tab 标签用于快速创建 tab 选项卡，语法如下</p>
<figure class="highlight django"><table><tr><td class="code"><pre><span class="line"><span class="template-tag">&#123;% <span class="name">tabs</span> [Unique name], [index] %&#125;</span></span><br><span class="line"><span class="xml">  <span class="comment">&lt;!-- tab [Tab caption]@[icon] --&gt;</span></span></span><br><span class="line"><span class="xml">  标签页内容（支持行内标签）</span></span><br><span class="line"><span class="xml">  <span class="comment">&lt;!-- endtab --&gt;</span></span></span><br><span class="line"><span class="template-tag">&#123;% <span class="name">endtabs</span> %&#125;</span></span><br></pre></td></tr></table></figure>
<p>其中，各参数意义如下：</p>
<ul>
<li>Unique name: 全局唯一的 Tab 名称，将作为各个标签页的 id 属性前缀</li>
<li>index: 当前激活的标签页索引，如果未定义则默认选中显示第一个标签页，如果设为 - 1 则默认隐藏所有标签页</li>
<li>Tab caption: 当前标签页的标题，如果不指定则会以 Unique name 加上索引作为标题</li>
<li>icon: 在标签页标题中添加 Font awesome 图标</li>
</ul>
<p>使用示例如下：</p>
<figure class="highlight django"><table><tr><td class="code"><pre><span class="line"><span class="template-tag">&#123;% <span class="name">tabs</span> Tab标签列表 %&#125;</span></span><br><span class="line"><span class="xml">  <span class="comment">&lt;!-- tab 标签页1 --&gt;</span></span></span><br><span class="line"><span class="xml">    标签页1文本内容</span></span><br><span class="line"><span class="xml">  <span class="comment">&lt;!-- endtab --&gt;</span></span></span><br><span class="line"><span class="xml">  <span class="comment">&lt;!-- tab 标签页2 --&gt;</span></span></span><br><span class="line"><span class="xml">    标签页2文本内容</span></span><br><span class="line"><span class="xml">  <span class="comment">&lt;!-- endtab --&gt;</span></span></span><br><span class="line"><span class="xml">  <span class="comment">&lt;!-- tab 标签页3 --&gt;</span></span></span><br><span class="line"><span class="xml">    标签页3文本内容</span></span><br><span class="line"><span class="xml">  <span class="comment">&lt;!-- endtab --&gt;</span></span></span><br><span class="line"><span class="template-tag">&#123;% <span class="name">endtabs</span> %&#125;</span></span><br></pre></td></tr></table></figure>
<div class="tabs" id="tab标签列表"><ul class="nav-tabs"><li class="tab active"><a href="#tab标签列表-1">标签页1</a></li><li class="tab"><a href="#tab标签列表-2">标签页2</a></li><li class="tab"><a href="#tab标签列表-3">标签页3</a></li></ul><div class="tab-content"><div class="tab-pane active" id="tab标签列表-1"><p>标签页1文本内容</p></div><div class="tab-pane" id="tab标签列表-2"><p>标签页2文本内容</p></div><div class="tab-pane" id="tab标签列表-3"><p>标签页3文本内容</p></div></div></div>
<h2 id="yin-yong-zhan-nei-lian-jie">引用站内链接</h2>
<p>可以通过如下语法引入站内文章的地址或链接：</p>
<figure class="highlight django"><table><tr><td class="code"><pre><span class="line"><span class="template-tag">&#123;% <span class="name">post_path</span> slug %&#125;</span></span><br><span class="line"><span class="template-tag">&#123;% <span class="name">post_link</span> slug [title] %&#125;</span></span><br></pre></td></tr></table></figure>
<p>其中，<code>slug</code> 表示 <code>_post</code> 目录下的 Markdown 文件名。</p>
<p><code>post_path</code> 标签将会渲染为文章的地址，即 <code>permalink</code>；而 <code>post_link</code> 标签将会渲染为链接，可以通过 <code>title</code> 指定链接标题。</p>
<p>如以下标签将会生成 </p>
<figure class="highlight django"><table><tr><td class="code"><pre><span class="line"><span class="template-tag">&#123;% <span class="name">post_path</span> hexo-writing-skills %&#125;</span></span><br></pre></td></tr></table></figure>
<p>而以下标签则会生成 </p>
<figure class="highlight django"><table><tr><td class="code"><pre><span class="line"><span class="template-tag">&#123;% <span class="name">post_link</span> hexo-writing-skills 链接标题 %&#125;</span></span><br></pre></td></tr></table></figure>
<p>这种站内引用方式比直接使用 url 引用的形式更为可靠，因为即使修改了 <code>permalink</code> 格式，或者修改了文章的路由地址，只要 Markdown 文件名没有发生改变，引用链接都不会失效。</p>
<h2 id="cha-ru-swig-dai-ma">插入 Swig 代码</h2>
<p>如果需要在页面内插入 Swig 代码，包括原生 HTML 代码，JavaScript 脚本等，可以通过 raw 标签来禁止 Markdown 引擎渲染标签内的内容。语法如下：</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">&#123;%<span class="built_in"> raw </span>%&#125;</span><br><span class="line">content</span><br><span class="line">&#123;% endraw %&#125;</span><br></pre></td></tr></table></figure>
<p>该标签通常用于在页面内引入三方脚本实现特殊功能，尤其是当该三方脚本尚无相关 hexo 插件支持的时候，可以通过写原生 Web 页面的形式引入脚本并编写实现逻辑。</p>
<h1 id="cha-ru-duo-mei-ti">插入多媒体</h1>
<h2 id="cha-ru-tu-pian">插入图片</h2>
<p>Markdown 并不会保存插入的图片资源本身，只是记录了获取资源的链接。因此我们需要选择一款合适的图床来支持博客写作，目前各大云服务商都提供了对象存储服务，如七牛云 KODO、又拍云 USS、腾讯云 COS、阿里云 OSS 等。</p>
<p>所以在 Markdown 中插入一张图片要分为以下几步来进行：</p>
<ol>
<li>将图片资源上传到图床中</li>
<li>获取图片外链</li>
<li>插入到 Markdown 文档中</li>
</ol>
<p>对于博客这种低频访问的应用场景，各大服务商的服务其实并没有显著的差异，并且前期的使用都提供了免费的流量，所以我认为图床的选择主要参考以下几个方面：</p>
<ul>
<li>
<p>图床是否提供了便捷的图形化管理工具用于图片的上传下载？</p>
<p>如阿里云有 ossbrowser，腾讯云有 cosbrowser，七牛云有 QsunSync 等，但就本人使用体验来说，七牛云 QsunSync 的 UI 界面确实很拙劣，功能较为单一，而腾讯云 cosbrowser 的界面就相对美观优雅的多，并以 Windows 资源管理器的交互方式为用户提供资源的上传、下载和管理服务。</p>
<p><img src="/2018/02/09/hexo/hexo-writing-skills/cos_client.png" alt></p>
</li>
<li>
<p>是否能够方便的插入到 Markdown 文档中？</p>
</li>
</ul>
<h2 id="wang-yi-yun-yin-le">网易云音乐</h2>
<p>在网页版云音乐中找到歌曲，点击生成外链播放器：</p>
<p><img src="/2018/02/09/hexo/hexo-writing-skills/music.png" alt></p>
<p>根据个人喜好选择播放器尺寸和播放模式：</p>
<p><img src="/2018/02/09/hexo/hexo-writing-skills/music_size.png" alt></p>
<p>将获取到的 <code>iframe</code> 代码添加到页面中，默认样式如下：</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=31165363&auto=1&height=66"></iframe>
<p>播放器宽度将会被拉长占满整个页宽，看起来有点别扭。查看控制台之后发现 <code>iframe</code> 在渲染的时候被处理过，外层包了一个类名为 <code>fluid-vids</code> 的 <code>div</code> 元素。顺藤摸瓜，找到了相关代码，原来是为了让嵌入的视频支持自适应布局，恰好也将 <code>music.163.com</code> 域名包含在了处理逻辑内，只需要将该行删除即可。</p>
<figure class="highlight js"><figcaption><span>themes\next\source\js\src\utils.js</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> SUPPORTED_PLAYERS = [</span><br><span class="line">  <span class="string">'www.youtube.com'</span>,</span><br><span class="line">  <span class="string">'player.vimeo.com'</span>,</span><br><span class="line">  <span class="string">'player.youku.com'</span>,</span><br><span class="line">  <span class="comment">//'music.163.com',</span></span><br><span class="line">  <span class="string">'www.tudou.com'</span></span><br><span class="line">];</span><br></pre></td></tr></table></figure>
<p>这样播放器样式就变成左对齐固定宽度了，如果你还想让播放器居中，可以将 <code>iframe</code> 包在 <code>&lt;center&gt;</code> 标签内。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">center</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">iframe</span> <span class="attr">frameborder</span>=<span class="string">"no"</span> <span class="attr">border</span>=<span class="string">"0"</span> <span class="attr">marginwidth</span>=<span class="string">"0"</span> <span class="attr">marginheight</span>=<span class="string">"0"</span> <span class="attr">width</span>=<span class="string">329</span> <span class="attr">height</span>=<span class="string">86</span> <span class="attr">src</span>=<span class="string">"//music.163.com/outchain/player?type=2&amp;id=34613621&amp;auto=0&amp;height=66"</span>&gt;</span><span class="tag">&lt;/<span class="name">iframe</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">center</span>&gt;</span></span><br></pre></td></tr></table></figure>
<div class="note warning">
            <p>网易云音乐中部分歌曲因版权保护已经无法生成外链了，即使是通过控制台强行拿到外链地址，嵌入网页后也无法播放。</p>
          </div>
<h2 id="aplayer-yin-pin-bo-fang-qi">Aplayer 音频播放器</h2>
<p><a href="https://aplayer.js.org/#/" target="_blank" rel="noopener">APlayer</a> 是 HTML5 音频播放器，提供了另一种音频播放方案。借助 <a href="https://github.com/MoePlayer/hexo-tag-aplayer" target="_blank" rel="noopener">hexo-tag-aplayer</a> 插件，可以通过标签的形式方便快捷的插入音频组件。在站点根目录下执行以下命令：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ npm <span class="keyword">install</span> hexo-tag-aplayer <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
<p>然后在页面中按照以下标签格式插入歌曲链接和相关信息：</p>
<figure class="highlight clojure"><table><tr><td class="code"><pre><span class="line">&#123;% aplayer title author url [picture_url, narrow, autoplay, width:xxx, lrc:xxx] %&#125;</span><br></pre></td></tr></table></figure>
<p>其中，各参数意义如下：</p>
<ul>
<li>title: 曲目标题</li>
<li>author: 曲目作者</li>
<li>url: 音乐文件 URL 地址</li>
<li>picture_url: (可选) 音乐对应的图片地址</li>
<li>narrow: （可选）播放器袖珍风格</li>
<li>autoplay: (可选) 自动播放，移动端浏览器暂时不支持此功能</li>
<li>width:xxx: (可选) 播放器宽度 (默认: 100%)</li>
<li>lrc:xxx: （可选）歌词文件 URL 地址</li>
</ul>
<p>当开启 Hexo 的 <a href="https://hexo.io/zh-cn/docs/asset-folders.html#%E6%96%87%E7%AB%A0%E8%B5%84%E6%BA%90%E6%96%87%E4%BB%B6%E5%A4%B9" target="_blank" rel="noopener">文章资源文件夹</a> 功能时，可以将图片、音乐文件、歌词文件放入与文章对应的资源文件夹中，然后直接引用，示例如下：</p>
<figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">&#123;<span class="meta">%</span> aplayer <span class="string">"Caffeine"</span> <span class="string">"Jeff Williams"</span> <span class="string">"caffeine.mp3"</span> <span class="string">"picture.jpg"</span> <span class="string">"lrc:caffeine.txt"</span> <span class="meta">%</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight js"><figcaption><span>themes\next\source\css\_custom\custom.styl</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment">//Aplayer 播放器居中</span></span><br><span class="line">div.aplayer &#123;</span><br><span class="line">  margin: <span class="number">5</span>px auto;</span><br><span class="line">  max-width: <span class="number">500</span>px;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<div class="note info">
            <p>插入播放列表功能请参考： <a href="https://github.com/MoePlayer/hexo-tag-aplayer#with-playlist" target="_blank" rel="noopener">hexo-tag-aplayer | With playlist</a></p>
          </div>
<h2 id="dpalyer-shi-pin-bo-fang-qi">Dpalyer 视频播放器</h2>
<p><a href="http://dplayer.js.org/#/" target="_blank" rel="noopener">DPlayer</a> 是一款简洁美观的 HTML5 视频播放器，支持弹幕互动。借助 <a href="https://github.com/MoePlayer/hexo-tag-dplayer" target="_blank" rel="noopener">hexo-tag-dplayer</a> 插件，可以通过标签的形式方便快捷的插入视频组件。在站点根目录下执行以下命令：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ npm <span class="keyword">install</span> hexo-tag-dplayer <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
<p>然后在页面中按照以下标签格式插入歌曲链接和相关信息：</p>
<figure class="highlight clojure"><table><tr><td class="code"><pre><span class="line">&#123;% dplayer <span class="string">"url=video-url"</span> <span class="string">"pic=image-url"</span> ... [<span class="string">"key=value"</span>] %&#125;</span><br></pre></td></tr></table></figure>
<p>此处列举部分重要 <code>key</code> 的参数意义:</p>
<div class="tabs" id="tab标签列表"><ul class="nav-tabs"><li class="tab active"><a href="#tab标签列表-1">播放器</a></li><li class="tab"><a href="#tab标签列表-2">视频</a></li><li class="tab"><a href="#tab标签列表-3">字幕</a></li><li class="tab"><a href="#tab标签列表-4">弹幕</a></li></ul><div class="tab-content"><div class="tab-pane active" id="tab标签列表-1"><ul>
<li>
<p>autoplay：是否开启视频自动播放，默认为 fasle</p>
</li>
<li>
<p>loop：是否开启视频循环播放，默认为 false</p>
</li>
<li>
<p>screenshot：是否开启截图，默认为 false</p>
</li>
<li>
<p>mutex：是否禁止多个播放器同时播放，默认为 true</p>
</li>
<li>
<p>dmunlimited：是否开启海量弹幕模式，默认为 false</p>
</li>
<li>
<p>preload：预加载模式，可选 note metadata auto</p>
</li>
<li>
<p>theme：主题色</p>
</li>
<li>
<p>lang：语言，可选 en zh-cn zh-tw</p>
</li>
<li>
<p>logo：左上角的 Logo</p>
</li>
<li>
<p>volume：默认音量，默认为 0.7</p>
</li>
<li>
<p>width：播放器宽度</p>
</li>
<li>
<p>height：播放器长度</p>
</li>
</ul></div><div class="tab-pane" id="tab标签列表-2"><ul>
<li>
<p>url：视频链接</p>
</li>
<li>
<p>pic：视频封面</p>
</li>
<li>
<p>thumbnails：视频缩略图，可以使用 DPlayer-thumbnails 生成</p>
</li>
<li>
<p>vidtype：视频类型，可选 auto hls flv dash 或其他自定义类型</p>
</li>
</ul></div><div class="tab-pane" id="tab标签列表-3"><ul>
<li>
<p>suburl：字幕链接</p>
</li>
<li>
<p>subtype：字幕类型，可选 webvtt ass，目前只支持 webvtt</p>
</li>
<li>
<p>subbottom：字幕距离播放器底部的距离，如 10px 10%</p>
</li>
<li>
<p>subcolor：字幕颜色</p>
</li>
</ul></div><div class="tab-pane" id="tab标签列表-4"><ul>
<li>
<p>id：弹幕 id</p>
</li>
<li>
<p>api：弹幕 api</p>
</li>
<li>
<p>token：弹幕后端验证 token</p>
</li>
<li>
<p>addition：额外外挂弹幕</p>
</li>
<li>
<p>dmuser：弹幕用户名</p>
</li>
<li>
<p>maximum：弹幕最大数量</p>
</li>
</ul></div></div></div>
<h1 id="jie-shu-yu">结束语</h1>
<p>本文介绍了 Hexo 博客的几项关键写作技巧，包括 Markdown 的基本语法，Hexo 主题的内置标签等，本文还介绍了如何在文章中利用图床外链插入图片，如何利用 Aplayer / Dplayer 等音视频播放器插件在页面内插入多媒体元素等。</p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://qcmoke.netlify.app/blog/hexo_code.html" target="_blank" rel="noopener">hexo代码块进阶写法</a></li>
<li><a href="https://www.ofind.cn/blog/HEXO/HEXO%E4%B8%8B%E7%9A%84%E8%AF%AD%E6%B3%95%E9%AB%98%E4%BA%AE%E6%8B%93%E5%B1%95%E4%BF%AE%E6%94%B9.html#%E6%98%AF%E5%90%A6%E6%98%BE%E7%A4%BA%E8%A1%8C%E5%8F%B7" target="_blank" rel="noopener">HEXO下的语法高亮拓展修改</a></li>
<li><a href="http://dplayer.js.org/#/zh-Hans/" target="_blank" rel="noopener">Dplayer 官方中文文档</a></li>
<li><a href="https://github.com/MoePlayer/hexo-tag-aplayer/blob/master/docs/README-zh_cn.md" target="_blank" rel="noopener">hexo-tag-aplayer | 中文文档</a></li>
<li><a href="https://theme-next.iissnan.com/tag-plugins.html" target="_blank" rel="noopener">NexT 使用文档 | 内置标签</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>writing skills</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo基础建站</title>
    <url>/2018/02/09/hexo/hexo-get-start/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2018/02/09/hexo/hexo-get-start/hexo.png" alt></p>
<p>Hexo 是一个高效简洁的静态博客框架，支持 Markdown 写作语法，插件丰富，主题优雅，部署方便。目前已成为多数人博客建站的选择。</p>
<p>本文为 Hexo 搭建个人博客系列中的第一篇。第一章中介绍了如何在本地搭建 Hexo 博客，第二章中介绍了如何安装使用 Next 主题，第三章和第四章分别介绍了针对于站点和文章详情页的一些基础优化方案。</p>
<a id="more"></a>
<h1 id="kai-shi-shi-yong">开始使用</h1>
<p>在命令行中通过 npm 来安装 Hexo：</p>
<figure class="highlight avrasm"><table><tr><td class="code"><pre><span class="line">$ npm install -g hexo-<span class="keyword">cli</span></span><br></pre></td></tr></table></figure>
<p><code>-g</code> 表示全局安装，会将 Hexo 命令加入环境变量中，以使其在 cmd 下有效。</p>
<p>Hexo 依赖于 <a href="https://nodejs.org/zh-cn/" target="_blank" rel="noopener">Node.js</a> 和 <a href="https://git-scm.com/download/" target="_blank" rel="noopener">git</a>，所以在安装 Hexo 之前先确保已安装了这两项应用。Hexo 依赖于 <a href="https://nodejs.org/zh-cn/" target="_blank" rel="noopener">Node.js</a> 和 <a href="https://git-scm.com/download/" target="_blank" rel="noopener">git</a>，所以在安装 Hexo 之前先确保已安装了这两项应用。</p>
<p>新建博客目录，然后在该路径下执行初始化命令：</p>
<figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line"><span class="variable">$ </span>hexo init</span><br></pre></td></tr></table></figure>
<p>官方教程中提到要在项目目录下执行 <code>npm install</code> 命令，事实上不必如此，在执行 <code>hexo init</code> 的过程中就已经自动安装好了项目依赖。</p>
<p>执行完毕后，将会生成以下文件结构：</p>
<figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── node_modules       <span class="comment">//依赖安装目录</span></span><br><span class="line">├── scaffolds          <span class="comment">//模板文件夹，新建的文章将会从此目录下的文件中继承格式</span></span><br><span class="line">|   ├── draft.md         <span class="comment">//草稿模板</span></span><br><span class="line">|   ├── page.md          <span class="comment">//页面模板</span></span><br><span class="line">|   └── post.md          <span class="comment">//文章模板</span></span><br><span class="line">├── <span class="keyword">source</span>             <span class="comment">//资源文件夹，用于放置图片、数据、文章等资源</span></span><br><span class="line">|   └── _posts           <span class="comment">//文章目录</span></span><br><span class="line">├── themes             <span class="comment">//主题文件夹</span></span><br><span class="line">|   └── landscape        <span class="comment">//默认主题</span></span><br><span class="line">├── .gitignore         <span class="comment">//指定不纳入git版本控制的文件</span></span><br><span class="line">├── _config.yml        <span class="comment">//站点配置文件</span></span><br><span class="line">├── db.json            </span><br><span class="line">├── <span class="keyword">package</span>.json</span><br><span class="line">└── <span class="keyword">package</span>-lock.json</span><br></pre></td></tr></table></figure>
<p>在根目录下执行如下命令启动 hexo 的内置 Web 服务器</p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line">$ hexo <span class="keyword">server</span></span><br></pre></td></tr></table></figure>
<p>该命令将会调用 Markdown 引擎解析项目中的博客内容生成网页资源，资源将会存于内存中，所以用户执行完命令之后在项目文件夹中是找不到相关的 Web 资源目录的。该命令还会启动一个简易的 Web 服务器用于提供对内存中网页资源的访问（工作机制类似于 webpack-dev-server），Web 服务器默认监听 4000 端口，用户可在浏览器中通过地址 <code>localhost:4000</code> 访问博客。</p>
<p><img src="/2018/02/09/hexo/hexo-get-start/hexo_default.png" alt></p>
<p>此外，可以通过添加命令行参数来支持高级用法：</p>
<ul>
<li>当 4000 端口已被其他应用占用时，可以添加 <code>-p</code> / <code>--port</code> 参数来设置 Web 服务监听的端口号，如<code>hexo s -p 8000</code></li>
<li>默认情况下，hexo 监听项目目录的文件变化，用户对于项目文件的任何改动都会触发实时解析编译并更新内存中的网页资源，也就是说，用户在本地修改后刷新浏览器就可以看到改动效果。如果不希望 hexo 监听项目目录的文件变化，可以添加 <code>-s</code> / <code>--static</code> 参数，这样本地改动就不会触发 hexo 实时解析更新。</li>
</ul>
<h1 id="geng-huan-next-zhu-ti">更换 Next 主题</h1>
<p>Next 作为一款符合广大程序员审美的主题，还是有着较高的出场率的。Hexo 中切换主题的方式非常简单，只需要将主题文件拷贝至根目录下的 <code>themes</code> 文件夹中， 然后修改 <code>_config.yml</code> 文件中的 <code>theme</code> 字段即可。</p>
<p>在根目录下执行以下命令下载主题文件：</p>
<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line">$ git clone <span class="symbol">https:</span>/<span class="regexp">/github.com/theme</span>-<span class="keyword">next</span>/hexo-theme-<span class="keyword">next</span>.git themes/<span class="keyword">next</span></span><br></pre></td></tr></table></figure>
<p>也可以在 <a href="https://github.com/theme-next/hexo-theme-next/releases" target="_blank" rel="noopener">NexT 版本发布页面</a> 手动下载然后解压到根目录下的 <code>theme</code> 文件夹下，并将文件夹命名为 <code>next</code> 。这里可以看到 <code>theme</code> 文件夹下已经有一个名为 <code>landscape</code> 的文件夹了，这就是默认主题了。</p>
<p><img src="/2018/02/09/hexo/hexo-get-start/next_download.png" alt></p>
<p>打开站点配置文件，将 <code>theme</code> 字段的值修改为 <code>next</code>。</p>
<figure class="highlight xml"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">theme: next</span><br></pre></td></tr></table></figure>
<p>这个时候刷新浏览器页面并不会发生变化，需要重启服务器并刷新才能使主题生效。如果重启服务器仍无效，尝试使用 <code>hexo clean</code> 清除缓存.</p>
<p>Next 默认主题风格为 Muse，用户可以在主题配置文件中修改 <code>scheme</code> 字段以选择自己喜欢的主题风格：</p>
<figure class="highlight xml"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"># Schemes</span><br><span class="line">#scheme: Muse</span><br><span class="line">#scheme: Mist</span><br><span class="line">#scheme: Pisces</span><br><span class="line">scheme: Gemini</span><br></pre></td></tr></table></figure>
<h1 id="zhan-dian-you-hua">站点优化</h1>
<p>根目录下的 _config.yml 文件负责站点的相关配置，用户可以通过修改该文件来自定义站点内容或功能，修改后需要重启服务器才能看到效果。</p>
<p>本节通过修改站点配置文件完善了网站标题、网站描述、社交链接、站点版权信息、友情链接等，效果如下图：</p>
<p><img src="/2018/02/09/hexo/hexo-get-start/info.png" alt></p>
<h2 id="wan-shan-zhan-dian-ji-chu-xin-xi">完善站点基础信息</h2>
<p>在站点配置文件中完善网站基本信息：</p>
<figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line"><span class="string">title:</span> 火种<span class="number">2</span>号</span><br><span class="line"><span class="string">subtitle:</span> <span class="string">'火种计划'</span></span><br><span class="line"><span class="string">description:</span> <span class="string">'但行好事，莫问前程！'</span></span><br><span class="line"><span class="string">keywords:</span></span><br><span class="line"><span class="string">author:</span> Li Zhen</span><br><span class="line"><span class="string">language:</span> zh-CN</span><br><span class="line"><span class="string">timezone:</span> Asia/Shanghai</span><br></pre></td></tr></table></figure>
<p>每个字段的冒号与值之间需要<strong>间隔一个空格</strong>。</p>
<h2 id="shou-ye-xian-shi-wen-zhang-zhai-yao">首页显示文章摘要</h2>
<p>根据默认的主题配置，首页将会显示每一篇文章的全文，如果想要只显示文章摘要，可以在主题配置文件中做出如下更改：</p>
<figure class="highlight python"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">auto_excerpt:</span><br><span class="line">  enable: true  <span class="comment"># 开启自动摘要提取</span></span><br><span class="line">  length: <span class="number">150</span></span><br></pre></td></tr></table></figure>
<p>此时将会从文章中提取 150 个字符作为摘要。</p>
<p>用户可以在文章中通过 <code>&lt;!--more--&gt;</code> 标记来精确划分摘要信息，标记之前的段落将作为摘要显示在首页。</p>
<p>如果在文章的 Front-Matter 中有非空的 <code>description</code> 字段，则该字段的内容会被作为摘要显示在首页。</p>
<h2 id="xiu-gai-zhan-dian-ye-jiao">修改站点页脚</h2>
<p>在主题配置文件中修改网站页脚信息：</p>
<figure class="highlight python"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">footer:  <span class="comment"># 底部信息区</span></span><br><span class="line">  <span class="comment"># Specify the date when the site was setup. If not defined, current year will be used.</span></span><br><span class="line">  since: <span class="number">2018</span> <span class="comment"># 建站时间</span></span><br><span class="line"></span><br><span class="line">  ages:</span><br><span class="line">    <span class="comment"># site running time</span></span><br><span class="line">    enable: true</span><br><span class="line">    <span class="comment"># birthday of your site</span></span><br><span class="line">    birthday: <span class="number">201810628</span></span><br><span class="line">    <span class="comment"># color of number</span></span><br><span class="line">    color: <span class="string">"#1890ff"</span></span><br><span class="line">  <span class="comment"># Icon between year and copyright info.</span></span><br><span class="line">  icon:</span><br><span class="line">    <span class="comment"># Icon name in Font Awesome. See: https://fontawesome.com/icons</span></span><br><span class="line">    name: fa fa-heart <span class="comment"># 图标名称</span></span><br><span class="line">    <span class="comment"># If you want to animate the icon, set it to true.</span></span><br><span class="line">    animated: true <span class="comment"># 开启动画</span></span><br><span class="line">    <span class="comment"># Change the color of icon, using Hex Code.</span></span><br><span class="line">    color: <span class="string">"#ff0000"</span>  <span class="comment"># 图标颜色</span></span><br></pre></td></tr></table></figure>
<p>更改后效果如下：</p>
<p><img src="/2018/02/09/hexo/hexo-get-start/footer.png" alt></p>
<h2 id="xiu-gai-wang-zhan-favicon">修改网站 Favicon</h2>
<p>Favicon 即浏览器标签左侧的图标。下载自己喜欢的图标置于 <code>themes\next\source\images\</code> 目录下，命名方式参考主题配置文件中的 <code>favicon</code> 字段。</p>
<figure class="highlight python"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">favicon:</span><br><span class="line">  small: /images/snoppy.jpeg</span><br><span class="line">  medium: /images/snoppy.jpeg</span><br><span class="line">  apple_touch_icon: /images/snoppy.jpeg</span><br><span class="line">  safari_pinned_tab: /images/logo.svg</span><br><span class="line">  <span class="comment">#android_manifest: /images/manifest.json</span></span><br><span class="line">  <span class="comment">#ms_browserconfig: /images/browserconfig.xml</span></span><br></pre></td></tr></table></figure>
<h2 id="tian-jia-you-lian">添加友链</h2>
<p>在主题配置文件中修改相应字段：</p>
<figure class="highlight less"><table><tr><td class="code"><pre><span class="line"><span class="attribute">links</span>:</span><br><span class="line">  # <span class="attribute">Title</span>: <span class="attribute">https</span>:<span class="comment">//github.com/jeffery0628</span></span><br></pre></td></tr></table></figure>
<h2 id="tian-jia-she-jiao-lian-jie">添加社交链接</h2>
<p>用户可以在主题配置文件中根据样例提示添加个人社交软件链接：</p>
<figure class="highlight python"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">social:</span><br><span class="line">  GitHub: hhttps://github.com/jeffery0628 || fab fa-github</span><br><span class="line">  邮箱: mailto:jeffery.lee<span class="number">.0628</span>@gmail.com || fa fa-envelope</span><br><span class="line">  <span class="comment">#Weibo: https://weibo.com/yourname || fab fa-weibo</span></span><br><span class="line">  <span class="comment">#Google: https://plus.google.com/yourname || fab fa-google</span></span><br><span class="line">  <span class="comment">#Twitter: https://twitter.com/yourname || fab fa-twitter</span></span><br><span class="line">  <span class="comment">#FB Page: https://www.facebook.com/yourname || fab fa-facebook</span></span><br><span class="line">  <span class="comment">#StackOverflow: https://stackoverflow.com/yourname || fab fa-stack-overflow</span></span><br><span class="line">  <span class="comment">#YouTube: https://youtube.com/yourname || fab fa-youtube</span></span><br><span class="line">  <span class="comment">#Instagram: https://instagram.com/yourname || fab fa-instagram</span></span><br><span class="line">  <span class="comment">#Skype: skype:yourname?call|chat || fab fa-skype</span></span><br><span class="line"></span><br><span class="line">social_icons:</span><br><span class="line">  enable: true <span class="comment"># 显示社交软件图标</span></span><br><span class="line">  icons_only: false <span class="comment"># 显示图标的同时显示文字</span></span><br><span class="line">  transition: false</span><br></pre></td></tr></table></figure>
<h2 id="tian-jia-ban-quan-xie-yi">添加版权协议</h2>
<p>在主题配置文件中开启相关字段并选择知识共享协议：</p>
<figure class="highlight python"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">creative_commons: by-nc-sa</span><br></pre></td></tr></table></figure>
<h2 id="dian-ji-tou-xiang-hui-dao-shou-ye">点击头像回到首页</h2>
<p>修改侧边栏模板代码:</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout\_partials\sidebar\site-overview.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;% if theme.avatar.url %&#125;</span><br><span class="line"><span class="addition">+   &lt;a href="/"&gt;</span></span><br><span class="line">      &lt;img class="site-author-image" itemprop="image"</span><br><span class="line">        src="&#123;&#123; url_for( theme.avatar.url | default(theme.images + '/avatar.gif') ) &#125;&#125;"</span><br><span class="line">        alt="&#123;&#123; author &#125;&#125;" /&gt;</span><br><span class="line"><span class="addition">+   &lt;/a&gt;</span></span><br><span class="line">  &#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<h1 id="wen-zhang-ye-you-hua">文章页优化</h1>
<p>主题目录下的 themes\next_config.yml 文件负责与主题相关的配置，用户可以通过修改该文件来自定义与主题相关的内容或功能，修改后刷新浏览器即可即时生效。</p>
<h2 id="xiu-gai-wen-zhang-mu-lu-dao-hang">修改文章目录导航</h2>
<p>默认情况下文章的多级目录是折叠的，点击才会触发下级菜单的展开，并且并且同时只能展开一个目录分支，这会造成在点击不同目录标题的时候目录跳来跳去。如果你想实现默认展开全部目录的功能，可以在自定义样式文件中添加以下代码：</p>
<figure class="highlight python"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">toc:</span><br><span class="line">  enable: true</span><br><span class="line">  <span class="comment"># Automatically add list number to toc.</span></span><br><span class="line">  number: true</span><br><span class="line">  <span class="comment"># If true, all words will placed on next lines if header width longer then sidebar width.</span></span><br><span class="line">  wrap: false</span><br><span class="line">  <span class="comment"># If true, all level of TOC in a post will be displayed, rather than the activated part of it.</span></span><br><span class="line">  expand_all: true <span class="comment"># 自动展开所有</span></span><br><span class="line">  <span class="comment"># Maximum heading depth of generated toc.</span></span><br><span class="line">  max_depth: <span class="number">4</span> <span class="comment"># 展开深度</span></span><br></pre></td></tr></table></figure>
<h2 id="xiu-gai-wen-zhang-meta-xin-xi">修改文章 meta 信息</h2>
<p>默认主题配置中，标题下方会显示文章的创建时间、文章的修改时间、文章分类信息等元数据，用户可以在主题配置文件中自定义设置需要显示的 meta 元信息：</p>
<figure class="highlight python"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># 显示在 home 页的文章创建于、更新于、阅读次数之类的数据</span></span><br><span class="line">post_meta:</span><br><span class="line">  item_text: true <span class="comment"># 显示文字说明</span></span><br><span class="line">  created_at: false <span class="comment"># 显示文章创建时间</span></span><br><span class="line">  updated_at:</span><br><span class="line">    enable: true <span class="comment"># 文章修改时间</span></span><br><span class="line">    another_day: true <span class="comment"># 更新日期显示规则，只有更新日期与创建日期不同时，才会显示</span></span><br><span class="line">  categories: true <span class="comment"># post分类信息</span></span><br></pre></td></tr></table></figure>
<h2 id="zhong-ying-wen-zhi-jian-zi-dong-tian-jia-kong-ge">中英文之间自动添加空格</h2>
<p>该功能由 <a href="https://github.com/vinta/pangu.js" target="_blank" rel="noopener">pangu</a> 提供，在根目录下执行如下命令克隆插件到项目中：</p>
<figure class="highlight crystal"><table><tr><td class="code"><pre><span class="line">$ git clone <span class="symbol">https:</span>/<span class="regexp">/github.com/theme</span>-<span class="keyword">next</span>/theme-<span class="keyword">next</span>-pangu.git themes/<span class="keyword">next</span>/source/<span class="class"><span class="keyword">lib</span>/<span class="title">pangu</span></span></span><br></pre></td></tr></table></figure>
<p>在主题配置文件中设置 <code>pangu: true</code> 即可启用该动能。</p>
<h1 id="jie-shu-yu">结束语</h1>
<div class="note warning">
            <p>不同版本的 Hexo 和 Next 主题之间配置项可能存在差异，本系列文章中的配置有效性以 Hexo v3.7.1 和 Next v7.8.0 为准。</p>
          </div>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">Hexo 官方文档</a></li>
<li><a href="https://theme-next.iissnan.com/getting-started.html" target="_blank" rel="noopener">NexT 使用文档</a></li>
<li><a href="https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html" target="_blank" rel="noopener">reuixiy | 打造个性超赞博客 Hexo+NexT+GitHubPages 的超深度优化</a></li>
<li><a href="http://yearito.cn/posts/hexo-get-started.html" target="_blank" rel="noopener">Hexo 搭建个人博客系列：基础建站篇</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
</search>
