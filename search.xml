<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>过去、现在、未来</title>
    <url>/2020/03/20/life/past_present_future/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">此文章已被加密，需要输入密码访问.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="f8e9a8fdedf7169e9c3500063fd0a96c556a52d1de41f1a544e84ed4b52bc8d3">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>生活/成长</category>
      </categories>
      <tags>
        <tag>成长</tag>
      </tags>
  </entry>
  <entry>
    <title>时光集</title>
    <url>/2020/06/01/life/timeline/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">此文章已被加密，需要输入密码访问.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="e40982cc01569fefeb50f890a043268da3dc608f28b4b10793afc07c65374891">396fda26e6b6c5475f01e2dc06ea0484accac1a6c42fd231dfa48bc7b477fdc63eaf994d330dcb29736018a76bfc7b2705bf66d0d83e939f096b44e4f275c0ec30fc8ae4193982e03b75048aaa481cdcef9d04829a3ff03788d194e5186a38d1ee96ce604dd6a58aa4a37969d6e786ecfbddbe92cae9400e3b78caa0232ee6400b50f2f69e599dd5103e2dada3029829ae96c70a06f1772bbaaf20511e19de0309c6414bc6c9ce58ab7c2471acd303a775f784495f3939bf3359d5d55d0a6c4c93cceb2fb46f862380fafd67d18a75d60ad520b6cd3308030e251a83556d095f5c31834475956b7cae5b44034c328d477e947ae262eee1021132fc0b2720c087f0930eb67b908d4b375fec9b3722ca060847b72857c7e9f368b4b14e8cb0d253f1a1777e868fedd43d320379d739fce4ed87f49ba750fb40df3dc823a7f82b709aa2a34edb98dbe02d6cca2d1e1abaca4c41dc391d0f472e3e52af78a1cbaaece1de6f7a3cfb490b3945ae32676f32c0375e25d549c1c7a3df7a4e2607b7d8ce44ace513674c8bd4861d0cc4caa380e84456850ae5cf82a147d45ddb163159272be91d28c132d28393f7dbddf76ef0bbc606e39ec1b722ea68fbb35d829f6011f184acf955a045999d39fa4c5b498197314f43e9c0045c01c8155a20c5d56c191900fe201fb1b6661bbca47cba520ba925b2652c0cc4647ae9756f0685184823eb781dd88050806edba53d5919e4040c7915eadcabc4f5f4f8572db488206bbc9c2db3d8dd99267da30b9786b9559077d390f250b14fa1b54b4bba7cec44b6a9c35b7bd56850f6925f4ca4e9b3f7d2718001186e6ef40b84779dd5b68fac6d7de8426e9c6c2ecf3b94b2adb4edb256a526402a1870dcc9060ebfa2c8a4dd373410afb845957dcda1dc4dc8708d0b6466405cd93d42a7068ddcc610730f9451b54aafe3740ebcede0cf7e35b27b20e9f08a61a18f9264badd84ac8c1b32ce075a854e5f3b98201a2d3d2791d33ee582d4f0ba69bb3280b72335f3b2793d1a782bccc769653f578ce9f304d85a0ea1ada68dc7f7914c7608971fdd1c7f05be4df672966956be6ff45ed18837b12959fa785a1ae17a578e4b67ff3b2286e3752f6063516c9c01cf3b98d7339c8416826915a4c58debf2d1f1170850104f201bac196b9fec92ce8b297ccb7de2cf5e48c9e83d81e3bdd6e14823a74a11182477818637e0c64305f6dfeb795e12d5b5d731d95c2dd14a04d1f3757bbaf643f7b4ba1ef650457e7f6def3fb46effe21a4f90095d98ee8004c4216dac04965fb5d1f2ba832c71a993f6e3314ca3417ffbb9ded1762950fa0173efe5fb4e65b8289b46b32b32865ea6e2ccc5bb477fb9cae726d974bd770447343a0c71a1dfd2ed11bfa3b19131041243bc78097a310354b62d1335c3b10c455fcaf5618757fdf39f72f1950cb049ffaf6d2cd21a3a30b6b3360883b8d05e8a5656df5b98f194f4e77b219093880c6bc9bab48a84f1b7765577e8dc082288e73f82a9aa63d51c66eeef4109fc02adfc96f928b3abc6f332a1b38c77035983fe6ff1b416930f8c761dea394fd3424468d4971667fbd169d3cfe78b9273a83fd6da054f8d603618f30938f5a5177b3eff456db0cf6e59da73a0ef60c90623828a670e3eb0641b651d3f4ef67354caf7337f19b3419010c56edb2e56767accfc2a5e16360145e63a7955f72e555b66451b5bbb10b3578d41ee7c06f15b3715c07525e6a71883fdacc11540c37de89762d4be0bbb671ed36701d9c2b6f7f8a554c12a9cd2636b4654a1e4656e0cb837b807b871c6f0b29942ddd9037eb39c9a086f6ad7d61341ba29989a6c49203ae9ce8fce160064814abd278b1741619612a781cf9d3a02cf7b8949b5d0a11ee9549d464f6402ff450d41ecfabd19de982f0f524f27cd439b1b018925e019b7917b29a67c3d47a0e19be2247d1301c3ceebf1e0625da7b2f7daa4021e8e79e17f2020dea1e36bc7041b6cbe9e6469fcf4cc1f5ab512199d56a22fc6011d70237a49e869daea2bbf3da0a5aa66dbd9d6d0a00719a69239de875726528e9813543e76ea401511b6850b039ca360488ea3cea8eba59a307b13c1242ac324e1391e4c7333ec213fa565c15220d4b1329851a0c3b535afdab331fd7e511aff88eeb62ea3e16d62c09918511afb24625935a3a5a96ab31997162607169fb7a0e20488cfc05028d924dd084fb4f7b2cd1e3427c3e314dafd8bf07a861ccbe6c1667cdfe00711d793458f0e970927bca5a6d93288a77287df359917884b1b21e96103c07f02acac7134b7a828b1e86b6c51a6c23ebbe3e0b14b7a64dae088df4991bc2720b7c5c4e138f31e7b09180d393204324710dd1987c48450353bb73fc0f7637067bfc323681b4bbadf9be2ba6828cd4b1d4864a600ef00b186819189ff5476e6509efe514526a96fde0a96647628f5d4446c303ec19e1a46473dc60b5a113e6fa84ef47ded29ebe7d6dfba44e7d57a2ee7800efc7d9c731fd0e19989136f9f5e3364a444a275c8070cb69a200e9d08326ca2a44f7e13020bb58cdc0da566a7617551918eff8e4d551f2ab319f1cd3f0a3873a3e2a9f30abd0bc6ba6154c11431821c0d1cd4e8679d6da66f2a7c65132edcd1284021c2856c28d6448f00b0900a4577bdabe5b0f2b42b9cb518619708312ae9be8a89ce765493c85f261aa24a9a68cb23776c861d69b700a16c66811084efd6ffee664df3d85af16004093a02a9e5f821ecb9368879491e828f0f7c7175e73f489f35ff690713f8b867a60fda5cf1cec8d3904e64141971e79bb6fa16475669b5df62dadd0d38c4c21ed13c59195ad5bd36c57d2cfb5f5d648a82d22fb5b0634ba536562461b5d73cb0b346db19c22491cba8d32a8ce026d70b846f9a653c494f295944c64e9b005360fb1fa77d06abf298a414941bb2b29d0ebbcdf1e043332c1aa3ec7a32d7d6ee50e3704189bfbc8d04b01a77ca2429c2b3a7a1551833b5ee37417c0fd0d95bce468c8cd920b27d60e607ea86e0f4295e718ff939ef60ee89aee7210e5c8afb48fcc18262bb541ab7b13463e42f312b3cd4bbabc12d66d5a8219939f0532371bd0cb12927102059406ef639f5a47c06ee9392ba1faff89561b3426a9b8757fcdc384d10e980f60be5e737cd9aa9b63fa120ec9678f235f21dcd302c98495a99f3e76652a290f721aa331fb2373e23c50e8202eb752e834f86beb2b8d5ea2ad5a2052de2fdb228d0a7a21d481f41959ef7dd19d77899a4118f988cc37e438bd4f7a5bf10acfcc3db18028e519d4210d510b9ee4b6be9ce861caf052b4963621537f66eaec641aa9062d66076bdc70946935763d88cd16bab7833285ae4d5ab760b813c74a17ad81865fe4365d91af4ce7cc88eb5c8a468b3bd2fffc75a07ae83ec2e671edffe84c41f6033edc500ae023977aad622ea331c220040fe4549107ee0ed983d9d75099b83b3862a10f292b1b7f6f241397a7e4cad0001d21ae943029004bd433048168d4af31f514f7129b7ddab253a97e214b2aa57fee5c17c9a8da1418b1b15641df8243db9e75c98668d3aa00475a4876dc34ef8e065cf8bbfe900897cef10973954bd4478c6fc2c6bad184700da51930071fcb78f9fd6db89eccd9651f50e94f5a3518d4689dbcc9b2aa5a3a6ab2d0ce7f6920704a7e47aa7ad95cc4806b866e512e4dd13d0ba1af31224fc463a8f4b050ab90a5ae1fd13be40d091d9449ce37c337322ec87c63139f59f127473626221581f33218733fec86c644734fc806de5c6125a1333bde079a578f7e4551b946cb5c969ad98fb4fea910bfdab4e735ae3aba2f11f2816ba3c39b00168b93ee0edded805570fd2c336ed921e0f9f7bf071e6a116cfa83374198307a5cf63e4af374f4d867ce5970b9df1cfafdfdfc71e796573c1aee76571803630e25864c518accf5b5efb67d13a320364f3bd60f13fe8b0ca9366667745cf0370ebf7892240b1b0fa0370e6fe57a9372bf6cfbb0112da955622307919cd08959f76d111e059c974f4c3b60fe4b8604a5e766196225d9283e98bbb1183520d437e32a70360b1451f7b537df7a16c42b0ba24c9870ea11ee8ffec6136e7d76ed19246f6232ce4a736fc6f85a712f51a2a2f797d1cfa14adf672af718667ea830f4a5b6fc23a63ca33e433b1e07b44a3f01d0e6fc9cfa06b52c9ee9dc5704bfdaa3f5002e314bc3e59d47c5c4ee8e45bf7431950a62fc330157067e0876d83faf44e104951ffd9e746e096ef866612d6bc6108d425e77b3672e04af4afd0e46d837a20ed923cbe57e71f015314688c2e076694d0b0a41d3493298729f921c7b3666358dbc7f1559d1683a9c25fde7874b9fe4bb1a515e36686caf6b7e7b3e0f51a5e55cd13cd798633e5ba95281d88f89f038dd8edad37edc7aefe0fd050e976f418de95a769b2b50824aab3535d2caed8889978ad8224ff4e85fb45a03b542dbbb73882d22414cc963caa69488ff316f810ee2a69ba40472d35f31abc428d20165c677f6a36dec177449867e35cd44e607312bd2aa8d61e5c32a07c00d7b1b1b0636860f6e09652a20e631f16059b659dd925a71a9b1cc45637bfa641240d3446c3881e37708d303bd6bd46cec9e8d7c8805a457907de76d2c91e483382927ce3fc410ead4b4fda7bc5be7c6916e1615faae049bc224fe5eb56118a3cdc772ec8c29c219f717ff6b1c917c3b6d1a46af4cc689f31d1747d70ceec843522dbacccd3ac18f56e10672d341bae63bab7cc23b2c20655becf249a7d54e06c52ce210f54cb2a9fd0e8f19974cdd68f528c53909f3322aa2b726729bc5fa5e7e77a6aef5973752950adf304cb006310bf68696ebbaff2019093896035f23f62ce9b0cfb39676c41cf1d25fc0c3a16d5152803dd4d4c19dc0fbc9844d79508bc25cc549d0f1789c7a763063d85f7e4c38d9314585cb0f70e8b05d4466bd4099a90d8122b2aa4c1f18719fc2640655219bc53590e9a2b38899f75e5922729752dc7ee8dd77e4aaa872a9f069018509ff920161e4c05bba69340f7c1f2511fddae9e722223a39b17004f394f9c92cf5a5dfa40acda988b99eec1052277c5a080895e0932f2a348ba8c05f8f23701f4d0c7d923459d4a7a953efbd5d02f71138c04cd93e5d3822d4db25fb34b64fba93877ee708e78ef932d828bc045dd31491efb628ff750a93c9305082dfac25a55e341941e22925ce877d7de20471fd98d7f83da892b96724e6be9641194a5763f7d62b50816e04b26f2d038ce9f9189482ec8c43dec656aa5aa2d7f203fe9c95059a8d3f97a5063739f557d2864be89b56cfe9e767ca611290e6c656100d0b12a8f41672fe954daf22c31f87c753455d7c3e570140ea6c735912e0309af30ffb3043004cb8e62471757b9087ee939593eae374afde78d808774348ee85daef559dc9bd64206e4a7fac883840e9d3767538a626d013a7319e9dadcdd364ffcda9051aad4d83f6c32c4853923233f1f14ec442cadb42251d8bfdf4ac886f841a939409bcaea0e73c9ad17dc3eda3683b61b5b1af455863d9261ed911e32c0975e9f8a6931357f009cc1718dad1dbb025525280ddd2d80b67444bacbbe19df3d7569732369864fdf0b0a6ab6d7e8db478dcf18399880f6868131d9915aa5891e44e1d5550f3a0b2ad186459dece72c5b86ad5e80560ba4d7e91229b204541b300a26a25b0bb52eeb9830eb2fa28cebae56a85f5eb21083016921492b1e72af197e34456f80e8464e6941b94bf6b97cf2ac71999529977225ae2fc43d9a202da1cc0303d9d4fa30b6e8d24cd7b977b6006ef819594d0d318aefe9e93b27846111894e9295ba3f54b6e0169c317a0837f2634c377adc2f4bb0c20d95aeb39e54e40e1d308fd548229d84bf100807cd44c5b0f3ad140aa66913f8c4e109081bfde33cde65d1f82e1623420b39013e4b629bcc465edaad1f6e44b9392a11123157c674005e0043d9984d456c3e89dd9e9e94b24fe4c42499622ef5966330f07533d5c2943cb2d93905a347b05b6173aff0ff7392307e2c18d15507d9eab5c065950c707db2ad5d6a97b60ad2376c6e88870c40a610189e38f3c27d4ab236db78de6b1d20284e58279a1a47115d43cbe9d68f7953cb38e64467d9c078e06e8b220b22716a1096962e659f65d2f8903e66175e44827395e78585c11dd6ddc6e561a1f9ca0b3fd08efea0171309bf7221f5098e7d06357b877c1221f3036d050b1d7883daf7fb7cee895a9510d1f4d5a7f154c1d9ce98865a589d5f0490e03eee651a4ce2f94f663b80530d19f0b1825e68e1576996d6096a91943fda0cd7a278f8cba09b2be728fded40f687cba26ab471147d2c49dbb3a81053b9f0afb120188d211bf00e6f0c9bf7a144e2ee41237cf59617f2e524fc3dad00207258c5bc8fb79d34fa44d71a4a2d5f71b80747650da0450588b48be83eeedee8c6fd326e90adc012fae80c325931e20087b487edb12b9f8c02e492d466c531c9c06d006e503767114e6e97f39698ae6ff6d044f7923cf45aa99d6770612a795321ac67572dfe25c714bd7eb4bc59cb9db9b74c4a8866928565edb7f48784611ebced98eb84b00bb13eb344608273127b93394fc77b0caa5b65a6d60b417509f14c6b93e0d2ff3592b25f695b257eef7e7b7db32346116353b99595ec5c3397b31871936812644133100137b3546e177576ba1b3deaa40c444ce8cd3867abbd948f858b762c0771a49ae258c638476b2960fd43e5ec943194a54dd2c637dd0db369acdb280c17f38dd2d5d83d9b6e2b38c6ea93ada2d81e4e742c8ad03b29636bec0056d1368afc41eaeb6722c0ff0d5f293b83809823c49cca6e39d72e86f286b18291c59c9c3278a9b742f1ab89cde93fe9aa0ab26c6c4237a69477edd530cc8c9aae95e5cb5f54a059571b19180414a1387e491fb6c8bde6032571fdabdb9cf7e2a9764129f4314ec31cb9a129acc49a0ca3b496262031809381bf6cca3b523e5fe10e74325666c453f069e6e5e776ec815225e4b97268985fc54bdc16caaffa6f354571ea113c9c004d0b2316cc7e4038684d73e5475e2bbd0c099344ef439fc3a4af76ba7b658b1fc26dbfac56ba4504e0a32633323d125ae41a6fcd79a9a1a656716b813ff4b814552c19bf6d35f576e9bb7b7ba61043a981ededa7557def7ec0a87fc89de6d5343737f6c33817bd2d632dd5542d52396c793a48b8cd7ccf05dfedb969035a77d19a1c3059c514c5d8d048b5ff10366a66b4f0d8ff41da725beea1fb00ddb778da29f33795d036640f7b4e0b6477cab505fab3c7b3d9d975ac5562fd392461077a61b1cc9015a1d77c57d760725da18754c657608ac85e6137b68f48d1b3475b7b0658c9de0cc15632c96c5725833c80e0b45591e1effb4bff2d58a51f15820fb1914ea4d15121cbe1d3ce535283d6a633e4b92a895579ef99d9da7f174ab72cd176d8191c2e1856311f28e01421069511c9123f3a5ca3129ae448af4383c8c6c84ebc0e55157e44a0b04d3a58cd79e790b449168b93ffa03921c23984fd28551824443233c7e7eb159a298a329fb6e5d0cab5f86ccd32089b690fc78226b95dcacc0d8dc7577c8070c21040ab2ea5d5b898afe410bc0566ba42a0dfaf6d918859eaa0f0f7ba2aeac773d1d56e0be4f57979d031c1796834982ab93233609e57dcccde2c93252b592fd7b38a431088cb6256e2c649ab07122ca1805b19bfd2d80166158708d5794d087ed128f58e09791cb8e9cffabca7dfa02b15d18528102ff91bc298c8e97d68d95643cef5b4e7f3703dc74f6f17a7d7aed331f5613f0c1b630acb292a2edf2df2be3f13f5608b9cc059f1899f1e14703d7c58951ccb51ec2c107596f65db2497da57b81013cdd17303f3d50820b232ccdab5d94baae3f69833e66e525d82802738d8d4b1919a9ccaa5cdae896cb4bed6e1bfc12f1693664a31fcbb65cc582b5049df6f549e5c52c53329c55e2af19f4f37528e53213e547bb703302c3c147fe709475ea0b510fabc85b9befef2acdaf1571252409631477ace70460f08ddf31401f6e7f25bbe77283aeccf40c6008e43f43631f69c883a22e96543480c19de64d47c866a3d76302a298a8a23f1b9530e0ead8591e2d420aa1d252cfb81546299bf4781f90930a4f60ba7c7bee6c7a511e57838572790a62945d7cabcb66846ec1f4c7d46526ad17c0b75eee725b4e5b89910755b1cdefdaf661d57ae57c6457e2502a8316634f955065f5b04d259a2dcdea7df3d38e9b8f9aea9413290e69cfc104d7b3aa566eb30c3baaa570d50019c18dc6e25df5ad9562fc6709bca8bcb1364a61b93d4d43c7c634ad2b14a53f11d6180033c0ffbfc1415c8f8ea7fae7cf8db65e5fbd840394861c95f851f26b62ac6cfe562c93306e0e39b33b1d3f7ccdc53adaf0e901223fad61f3afd348f7e65b8a2f322fad77ce6b95865c61b0ae6597516c25c9f02106151e8a185629492859c7b001bfbf3606e3651e8d3e60be4d54e784bf2a0218e36dc62fe42edcf1ddd172151ca4f034950090b07b2e106898b31f150d42c37912ac9d9bd60ba8c47a46655877c3babb636ff72f93ea6c70b2f7c4a4adf1a827315cae81361f16a4aacaff63bede8d1a11bd7a2943aac07cc030d5d58e61ca1314c17ef4a946e49a596cde0b02b4937fd919d155d511ea1011ad02298169c1b9de00206f1fadf36feedb85f0c98cf55c31376eaabc2bd0866bcf878e41201db71af72c1692b2ca3b9bdf1992805db7542a141084d932531694baeea4403cebec9d6f4914b93520040a740a9cfe866f2bd186737cc63b059b3ed9a1e9a8f3d069de3c7615579c3ad2be659a4c142fb40aea1a77be6b7aa1a2e607aa88f190375d4f1a1c4fed1375127c2a7ab14cf4c59bee64c8a012a7ab0ef71706dc817c74fba87758203d177a097b252b49d47dc57f7ecbc020ffa121fd828848f5d2ae7bcbde8e8bcd72021e3119884408ba0f14f7e983ac072630f71141628af08fac665031a38119dcd091e476318e21bf83398b61dee7816082545b625d3ef421bc877b9106611a02defb2b841079c626b2ab2ebc6fb110a0d3bf3a354d089a5d4b3dad78b2842c1d67d44443fb0b3b6ed638d29bde72f0ff324dfcd637221124593f77d5c0e0272d67a34d6d08d0cc752f68388676bcef8875102d711c9e5e3288c2eb89d071f04afaf6c26133f5e553ae1618b21ab3487761e21bc83e042f45ff9ece486a562be4bdafeca2977a0426a16a63c513a9af7fbfcbdb7249acd2e8045fb24ba0ae0f10ac912d9d9cbc334c92cd32fd2ade6d48ad9d1d2af627a0ed9cec1a288bfa0e4b006293062768cf0162821270aae8010862876dcf38c23d818f7753c3704bcd0a54649c97c80c7e15025830bb36a78184e4340ad127fc3c5376b5c1be3308d95a1045cdade6ae531dcb6af6b1251091fdb141dd5e9be93357ebca62c634d1db688a6c8db7fd1b4980af509c4518845d3335c0f4bcc52a93ecf91b292f77b402c48537a5e84ca3bdcce4ef86fc4788fc610e19a9121559291a2fcc0b17ab8184ad56a8a08570097c9b229be03ed33fd6c68b2ee8d78b891eea7abd3f6cbef3d24f0ec7ae53275505fb348b38488fa90cb8f5d93ad05616f4d1eac29a4434f661325cd11844c10389391d0f39c70a9e5d392be1481d642bb983f1b0bc10b6dee4a58bc2f0f17c2d452c032fad3b5bb1f29ccea0a36b13497adf2f12cfd9d78e1fcb071898aeadcf183da2ea7c4ee838b46f92d78bc8324d49f00701c622a7d481ea66d531a90b7d4bf9b8528589342b5088aa7e7431e0095ece05b40724ecdb24276ff9661f53d4dfbe53f4d2a1426cb5bb01dedcdbe5e077d7af31a93c5beb8627928a02a74cbc8c73721a72010988da9c571513bfbbdddd497f2a3de6d8e7fd83fc2ce0180559c105ef5fd3c27754ee1530ee407afa4487358850008e1ab3bde06557847366584cd5b22026e4867e190861dbd594f87758adb93459f0e830d15e071e36c3ef74ac1cdb14433baacc8fa6be8788aa1e14328dc27769a49a1da1a6bbcd8a7f122fb3e2a7e8e217d9a3a703e628c6a34b6cfb829c2c9159ae1aa945357310cc4da29e01e07562e27bbca160561914548f337926613c805c062e037984bfd85c5ed3190004bfc13fc3c3a67aff731b5df0da43d6d390dffdb66f50ffb29ea98937868d7175c581542628b5cca95c3d44963110643905769f4570278167ea65cb9c7a0ab1d8a79ba3670520af48d70c3abf39417c81faf8870f2a43596b721082bcc51215c048b7005ad5ffe52d45fab23cd7c6de01c7a28d83314cea1bec0c914a5183d3573ebfcf6841cc3819cebd93033af357c58f92b860f03b365376c1f95ef60843e87dff0fbd56b750c10d1a24ae3b14270f0dcaf15aa4249208a004b5eeac9c644e31020dc21a836600ed464cb68969ab8e66e47e984a9494f632a1ec774ec7f96769a70e337c158a823a67fc0032bd98d675f7b4da39f6fbc46bda4cc53f46c4595a09955bfc15bcd0899421580a3b564f4eb426a1f206eff749d8c2af6f7cfc46f88237bd3d6adfe99a79e752e886399544b6458c150be49a8280e5c5972497d236780c73d71b178dcd6a083c9233c298076e42c4e2c2f048951108b8bd45516933462bb8ef715cf2b834189819c20bcf12cee11885dc0304d0f5643ea542a15ae21c6445ab549881cdb58da65b69eca5639501729c729ff391544dae00719088f570194a2e8527862dc2bf488de2612da7ed6e3e1c8c8eac90c6d49edbf1e62d3014517a0f90ae91d251a58ff4d94a962e55d4cd7c6679ea07ac1203cd38406dfc0e37e8310eaede7d880213c8a2ed3c6232f3aa0cac5e3af506290f9d5108cc5524ce58d22f48b70b43d1d918dfd9f6d65c155114378af3001160e7bc3d93183514daca5cd9225d2bf0bce46514976668416628d5ce46ad04c7c166f54565ed73247744f934bdaf8115def0a6889a378fdbe73ecc357bc13788e7e375374d60239609c8c4f163284a6b630dd725c54118c22f5efdbbc39adaf78c92cd24c63bcb50cf0a12ff846d3af6cb07337cf2dd7e6b8adca6bd6afac4ffffae810eae8835fb2c037c5a9c902c64e9079933c6860282e78bb48f3f38f15ff63bd18528205404e8c991e22fc1fed024cdd6ff16a558dd0d193e43b3093a50189fb0879bfba8579e75c39905827cf9ba69486f1ca9979ce9c781b45442141391ca2e463279fcd2fe33c1709e131e98bbaaa5e5f280aca3685072c2ce9085f280f3187c38dab34a07ce6fc14cf1e09913f2b91c0e6f4b6d57c59ff26f7bdcdafb73b9f99fc4fa93c9023246c979ddef128a619fb347d00840d2ba66da4686adc7f3a05058b7b3213a242a3eaaf46db295720c1fd284f6e51a90c07e6d2501ba16c8cface0165c764707d9608da17b9657c731fcd33069c2599fd58856ec288b2adf35e39106e9bfe516e4a77e7e6ee8834640c09eb9715ffe78f380155057490f5de0db19b0fb78ea294ab41482f01fdacb47ff8326b8d0bf6d6d86d6daab851b59f48c122b4639ec42b012dc03beb1b7ae979cb8d9857787d8f69241a136ab7f2951c5d566b62a1649e8cf4a21a93777bf4d41079e81d274f6977ad3d1c92fb4e9647ee6ed9dbe27fbb9c73022cf9b86d4c2e97aca1cbb23745000ec5fa1031747ec126bc8bcf5280ac6463653716369dc34207089b9a2c286dbba27021e216d4af3e40d018a3188bc65c1004e47c65664985e131819a528b699998b23bdb7ef6f42a236a36525407f831733e59bb572507965245e31cd7162c10b71f28d3a41fa4741b6907eeceac5de51c3fda19ae22c3107b2aa36bd9f53a558c064d563f6afd3420407a78143c2eeb12ff9aae4014ebc6608dba2524ef317b1068d62390a61a515d0b001af4b881284ddfc50b88c950ae7529ab8b8ce9e5e871e4217ac2d72db2d564a00daaa7b142d8a31d4bc638c2b94c22e654e8ab373ecdd46191857e4d042f3d4e521fff29871b053e7343ea6f1ad34df9582014ad2b28ffde0e10f537e3bb6a9e35d0a674eb5030fa1ae2d7b810fbd5198d32f29ac3b05ff9437c329972b110d431cb218a9b348e1f41d14c3e6a94a27dd84dc38046c47cc56a26932a8166208b173b55083b50bc9244e0fa4a80e52f3d3803f62cea9c560fc4a462d3ee56759292292e1884fc0da90e6766300f8dbe20e45954cfed68478c7e9446dc1b7ebbeb2662e48ef0791979d15ae810a37d9dd95e46b18e11e7a743e9b6c3e6491914b6a6146c9f23414518e9c26ad3388a512bd2115c2cf2eae377267f383597c2a4f9d7a1aa6efe4789221a45ecfc60e762239ca213bcf13b0f81ffc7a14cb6455eb24805d95d054db64e7b9202d1f9b4a0bccc47a163e963a724588cf9664de6d13925f4409925572f02d28674d58881d4ec86a4c747f6c0be30ea2f30e836a2cffed785ebb5118123fbeb13a1de34ec1a094e79cf47870cf1ef49f2aca6d4156e3838030388a9b4f168fa8a0a95eab6048b4483bbf01f96e7dedf74b2a9d7732338a78d27edce0b40d66711f1dbdbaa37a1f97cc77e69ca765da2cfe250d3f638d862df12da24291e63128b1540d92952efadb2ed7fef9dd90591e79709dc31e45a41a6aa565c3d66d5f34d5a26669343608125b350ed6a33f9ed4335b8428050bfeef6a82837e0ce1f34660476267e0046eb7149a242addb04f8bacacb2817b31c33687036534f0d4d12242f10809804cc38214072b066f414c86bab512744637ab5670e435c79bac51cd98b91f6f4af327b8efc29460835053716a4dff16c0e5bea60976d13b6dfcf43013e24c8636bf6bba10be8c4fd353d765b70df574f9df71351dc36dde1eaefc01113dc3833afa88c34c7d547ca11aa2c6b929ccf67816e964765fc770e0bfecc06d01240fc637806b90398ce94013a01946b7ff7cde758831aaf35620c35bb7496d584c9528355ca9c8dd0da2c10be82fb848e4e12d0f662332a747a0fd607d11db8fedb1535893368ac62cb01bbb5daf3ad864895c3980963dca6505cdc672d3ed7fb63fcd5c6671994ca68b8b41c5bcfcc156e45cf42bff3b36722e77c667e28eac71341a7f6b74168916847cf7d4ebc3231eabae669fde312055a80322043813ebc2d5135f4ebcadbc4a46b1107eb91619d2812330e9fdb00dcac37fa0c07f0df9703b1f2c46c20954226dd8a847bcb878af50c19f85f2f67d2b4796aa4c410d633d0023e990b91b45b7a28dc15f9a6efb819cdd0be4d8bd58f59d9d9de308ac414a16373ed025498978e94291796926714e0de988b3d48b09ac2341bc94511c75cf7719c8cd1afba3f95982ca828f42ccfa06b46bbe661f27076e72f788e52fee2f3f89ace22213159550e1730be703da9e83350ed05bf3953b3cb86622fa54b857a06e0e28271794211b818e30c890bf800fb90438e66052c09ecd0c51857be24bd578ac0ed65bec8b4ea08fed5232356b0ff6b3ba76591e0cfd9fa27a3e841892bbd01fc0795a15ca1f9a0a10e9cfe255a5f66194972a1bdf09f353a5aae95dc2bbcf0a7b341d318af90545bbd51ed47d6c100bde1f2d80df91f1fdf799c862f8d9401c83e6d70e59834197cf57a301ec7fc70d5453f82c285715dfec6a1b0fd855d417af380e79b5b54fc5c2b140a644984c00beceda99981e6aa6cd68a1dcd51da5566dff3a031aa6cb828e8cb60980202622b6eaa2fb80e34e0f55774e2a7f15fa5c0cabd1e09af144808ac7f8584420af4737571ddda55732e917994b041965a133db5d911288e41db0ff1f8f0c08ba877bb54ac9e4cbba641a98f5df05a6d7c239eaabb6cb2e19307b850e54d6352fbd469e30ee7186f200a1c86099dc48c000b4c0497289806e19bef50d8e78528df1eb77ec3ffb73224819784f752e34e0cfd60d9c030c2393397b3c1ac7a7916bbd09e2e67d0637982f010c8e7971c28a9eca4651b65dcaa526346e02b319714f7e2018c685a183f902834de14c4e7ebbfc36eff370e591059873bd9af3cc149a2dbc64df3283db1d8e43674071ac50190362aab837ccb5584fb99b71465530870687bb170dcedbbb8cf625c37250b78e16b708aeaf2fa1aaa57d0e7f5da94b6ca8ef7e7850dd6e80456e8dc9fc75249bfda414392f152135336245545927e49dd99155f6a5cf527817be052be16b730c78d759cce1e988135c4ec8f66ce0f9b5b6c79c1bf424dccd763d72028c93a56d87248759200921eb4449678a346e8917e267efaa95a88af95c27251da591199ba932619b73d5e0c727f0a2213449ace04472aee5cd0ddc3d87a509ea5e12e708021c0f60561fe3f2cc9353cbdcbf9924a76d2168a878a4f6524c2b9aee1e888b23a98632b10f236a63b535971180e649d6c1812485d350faf230e5557333596bc5a54dd9d9091a4573791ad5d0f2a6678056b9c5804dacbda6dbb35a3ac9fde4aa1821987902b009730c8b4f44236269fb7a83611722bb4b6f672eaae6ced1968ac321055d9210b6fe1e9ffafae4adae4bb922e55314f3dabecbdd601df73c42c2b2757fe85219a660fd9ee8e6c9902c546d369c175c6a9815a85f306e2286a12180ab1dcba4b74556b2fc7e8b0c2faa8fc2ec7bfca10f307710dd9457e45658299c4ee25f40de88d1a620e0f87f1ea19bf8672e3a98a6fcecd69ea0ecf9ed05ffff38db3c13a6b4d8066bc075654954860666d1d470b69b724d391125c826e7b13e886d1378ba4292f94e00daa3eada5577259bbadc1ae9cf96c4df687d2b2e31500219dee0dc2fa6c85eca659e6b9b0812f7d3d35cc1c7771b9ca060c038134db1967eb5f9bb3783ec078341a1c28f14d29b163a001e0ae7a135aeadb50583b8672fa6196e57f2759cef26a0f255f82344acdd6feedd28e1f3f6cd2c1fa6556d06013df44ed7f69977a9313c6ab29e991d43a8c689398b43eacce5ec751ce3fefee54e57cd5956e0ad0a585fb33df6b6435519daa10fa6da23921a6243c5c251233eaa8594c40f8c10cbea6a042f856593aa142e74747442cbc8702f3884047a6899bad5e7be4c2f55ecad7ded3e615f67e7b7940cadb63d64a7d165ab78466b98e55a02a34437c3bbd78c7395a74ea9a76bbd67a1e2544644584dc9ff58107d97e46140df81aa075b134472ebf1a1b70e8e969f2a5f89c55c3115b268634c4d6a364e90b7c9eaf2d151113194dcb6b1c37c4cafc5cf7e2b23ecc653ec1435c8b04807bdf236ac40b76fc3820447cc8d164757ff2f21e6efe85309ad8463b09b5790354647510254e760fb77921d9b29d775ad111e6d7a965283dab34be8ba42c46c4797d98449ad3a0704080063d3e06ace383c5bd2254fe6f4bc012ae7a61d3bf7d84f02392683772e016f4713279fa955482b0141fdd938381b04d47f83bc9b3d2fa1bfc2381292e5937994264bf3d3a8e43744554b7aad6e6c500bfd70dc84a25e61d0d1bd20caddb3c6372f21e73e87d6a271dcc9b0d608228c2b1454e438e12593f7535cdc2433d06119fc620381361b311086e4a7cbddd75f18678ed22fa5af12b16f91625349d698980747663b5f882e8e5b5f2dd0c19e1ea53d7204e995acf190a99687f4ccf9c1a23efafa4cf4bb748915eb159254ab6da5be438836ef7335bb80ab969bce6c9d5f85a4f878a298c3622e171ac16890d7e4a5447ee15b2887f42ea61dec2b9f6107dcf902b8708b68298dd82fa18ae4430b743f9ce5a0479db41a725031cfd61540110ed129bed43772df8fc8e698d888b281d05271d862f26e8e33d0fc803c2c47ef26c456fa58916445ea4faa21d790d051199ce4759ccba780a77a2760b9daef8fc02093b1b2dfe3f7449a9459c6177c5b4911853c27d2abeda2b32c059c08fad31d599ed3a507e444b2d351315560c60ddee866e6858bd29503d41636c0a64a121ca813ab7c8e92ee522dfee20fd39af19fd92a2821afceb8a0d14e5f23625a16a1a0dac32988a03d0eec3e6ffa37e9a797584447a79835e8efe2ffc5c26705d11ec5547320a780f239641ac1778067a63f56b76678835e206a28a716b7f6f8a6a0abf31f6f5bf8c3682922c0c69a0ab838d92211dda24eacfd8c17e84d1d2f6f1daacf76d0967893c8097f66d563d46b6556d84f3e17f1c942b01ff0deca219024f0c444a15b9ad81fc2b6c993420d8b299e69c5fc0c9948f54960cd2b3983383b879c0dac1317e65a77acac6e620d0100ba22cc315961693322f41d16dabd833d02f37e04be1c3d14e0bb6ba3ccd44a23c4a4e68a353266167424fab371ec5e2ec53126a979a05ac1cd603a976a9fe37a91253ff4b0fdd39dc543701b20d4b02f16665e6005f7dddefe5938d31ce747daeb02f09a93694a4a8b69ba9c3d4b8a8963c891c004c467f7570b8eb0cae75822f4ff5577563a5fb2b81cd447cf1a0cb6c84d7e53d705788a5c85f1b0e0c350b037ac5d53dbca427e46ecfeaf7012e37b2d7a8bcd44270adbd7a567bbd0fb9fb2e749e84c7810caa36a2dabe0ee84b814705f4d962919bc45561b43ba41f5ac6666d1140cb1a3fe74b880a4356f7ebf9e836bec03a1c8b8e42598186092b274ee43786265d1242f13d8d8700eeb90e8f856dfaaf13dd0088ea2cc63352b2c11a26ddde3e9334f0508905d8e20a3f9a54e29af5bf692169db2e309f96aa93f45b9768ee969eb3bf8e74804098d7568bd84b9fe5b293417d6654946d124f770a462b3a6037721a20effd023121572d1f9f38933dbea85d72af0b8951c6b96e4296f68ddc21b43061562388e225ae191adc1ddd120586aa12e46a246849b4de65d89bf8816f2be004066d7da84609874f0bec4c4995b121f5f3eaa1c378d3ee2d9c976734147cf37974fd55c24394b9e77ad85c63efcd8048b27a2888940e8c2c42335a09ec1e85df6a39bae34e6fa1cf0b4aa875afc331c051f3e91e40ef1a8aa528457fa74bce53007602edc9ba1034503afda8893bda0022385bd201896c23e6291b9a3eff3dd160e14c3f3b7422de37c14af2d450de36ec34f0022efeed1b74522c00f11bbefbeb64d584784a676de130b7074518907fa102f909933fc9f1984dc12ab24dc75982836f228a97c8f69955053a33217a0a893948f66eb9dba562e478b77cdac923e182ecb039db82eafaaa7176090f2fc519cb61daf62d0ca5a54625736e1597c008df92a22ca36cc830dd3c977f724d7cfdaf3272480ee0b916213320eb744084964828584488807de5a5bc72b3fe6e85c8674b482a7304b9e9f3de68b7a9468283a09b2c5a521d83cb4402da9d316cd7167f6d15ef94fa3bb6e93efaf4212fb42285daf96ddd56d00e1dc82d190bfa13900e8e3d02398e63ecf741d72a0a060f99f4dad8af6d57c726f97385f8a7d8e08a9b04ba3d3a9b291bd4fa389feb17454097a7d07005eef6c82b7135ba20d544c52aab80d2136b9d178524a3af4257ceddbe2fb1432a5be8caae2d7bec6b8aa8388bdfbbe42afa7e092d110afcefed8c7e46c53737f9114946300852b2a1adc7df9b4850c8b95832e7740cca4cbeff94159c19dd5d72a2a861309752ca6c01b581f13606df2cbb7980ce0a2260a5000436a890c93abb017f39128255b22f7de5eae396008c5c1e7ae6154e02ebc01b310a5a918c32b6d9348e946cc73d3d09573498428c6ec11fa63acfc7e53d7c9ddd42659df59684809e1d01648137d807a378f78a7dff7ee961e7fe37fe81adbb887b20b1846346d0b93e460b4c0447de234dca76cf09c0f64aa693e8dab44a1c9e05e464fea5880b1bba27a834e033e8536298fe14de7c480b464117aca57b065286c14c8c8d8161655d5af22e1ec9500b00c9a4265c577f16241741f7d9b9fc0ac599a4e9ac3d7115a2a0185922607d2b3cafc47d400aaa1f78e3031a635dfbdcef24680cbd066c49ebb23333c9446c95aa6041d63e4ae89814388184a5a33de7243c3f818c5fca95ad054529984b51a674b0d0172a9445b52bb908f6367fad1a0c0bab5abe18657ff729a6decef8be31f0ccd1e248fb7b195790299a7ee5da39b80a7e26a9bf6c76810e6fecdc9df1eaaa5e61550629e9cea470ae25d7a35dd4594720f37b0aaa0e538aae6c702341cc370d82a6fa15912f6f86198f99d1f5e220cac5d3de2f1545f3b1fd412ed0b1e2c2e9bf794b7b083d8dcb2791163a1df2ea531cb8f5d08d62ff137a22e7ab943e95b3d3fb5b461fd22c4ed84b4cf8102ffa41cdd01fe66431dbe0b0c3387ff8ae8d59618f6f4942a4334a2c7ed1b6d4e2e5534227c099f3680a6a9a77c5a29e06411e7e8dcd5649fbc471f8077f7e1182029b322fbf5c0347bf32640cdcf6c6a5e071b171502498f832c2cc88e879bca0e95a5b3ab310b849090505094698f727e864c1adea9c8a616f8263c18f7fd8b164636bb06aecf6247d7900ca7554c90672d1083174bc88c6af3683c3677cb5cd909b0ee36745c712114fbe30c0b1a9f166d7878de04ef3495870f1bb3030b20930b9cbecf9b8fbf516d121bf7c1908e7f2ba7f141a16a7d56aac81000d3580cdcc8897f841bb13bbe9daa3a0a67002ed3a80d2260f3c6031394ca46765baa5d427bde72e95e756318247210f8bda326d45d80d91f32e076f5ded6a68050ef1d827adc2d23a952a0d8e05b3bd85edfec328e031256cd2ce71a1af2054e6a5fec1af0780da21921178819338d67927a45ee03ab340249e2ed4ed05296c1745b9af4b30fd480da18fc0d33696abfcbcacc36580c79d2ffb7ecf8130477ca049eb6a512bd888a43e68f923db0ce534c341be71ecf89d57283d0fc2dc5b33fec9218234ab597e16fcc7eee6b817bad8e50db02af766c599b6c15428885e37fccb0ea77b744af021bffdfc994842c39160bb8391bd081ffdeb43f4855a796650459cbbf12bca0063bc55914c04564cc124df3b87f8fe81783636e4b30bf47e7abebea988d42cb111b56ad81b6c14248d3b02bf18a55708c1d4d40b524e86e27f97bb24af8f7bd645d084362d027fb1adfdd979242fca606e65a1bbf478c617c1537377f28e529c0b9300d2f70f17dd4fb24b41dcbee3fca7a2e0385236075c601ff15f7166ba0522d09bd871ebd865558f81053724fdab4bd18599ba07295ffe0e8133d764c243f51a49316f77bfd3519469169be1603eacd9ecf05290c0d8961bf6853ef219a531abf608396d3643004cf81f29e23b7956b8884d6c9c87fb575df60fe519cfe68eac4a7601baafc1c37c3c86965ebae5c051acd1c3371b7d5bcab8b5f54bd3402a5e9a91389096ecee5e486c77edb0f5767c04049cf88375d6b6024b9432fec6125c08aea474501906a454fe357f16130d91755fb732afcc143c9632f4fee3037500952a2597fbdb27645375a31fd096ae815f95057e8c1046f75216ffd19923700fa43d9291240ad7e63aca5a2f7f3c5ba625a3b87c191a7d64ef73dbd93392bc122ac38c5e1616e5f9bcf4e4ea0cbea74dcc6194a28a4ac5259f595f29d435d24674ef38d869f3fc94e8bb02cb1cd0967affa272cd24659615b4e38b7862118094d16d620212b3281a750234e096ff12462dd6b4d41a17704b8d2fbb87811e12f9979d8fa363af85b838ddd55d2ea95b4747fa0751fa750d7981806e521fa8776b0cffafae06ba600a33463ba2f294b8de4725a84ce45c2e8071886fdbedcd28b6640c7b6f8c1673d7a49c56692be6787c6ef451849dfb3bbbef09eb33097174eac30f523f38885cf48f27305bc67e51ff836515eb3e5a74675534b994b78f6c7df48ebb2404b75d4e38a8ebaf40550eb3011dad70eb7d855652c5fc14b6aacc15ffa46a38b04e62de65c49445608b66e8552332fc4d6fd3e613143609ed47d3b25b4da328bf4734a4bb344179729da4a84388b0dd29679600b5cc586c45fb2453985908da5983a17f58fe22ec578be8302663015abfd0091376906a4361164c5b3959b38dea1ffec94fafd25471cb1a3789d13a1737a7a335871f90b6401dc2b01d3b9a97d0148a11d0f9cf6d045e5b6fd297022ac205de69ac0082ba07b8da7e436c257e299972cb2c449641529984f4499d13c8609388ba53cec5e3009f3aae12503ce0f5a56adce0664ea2f0eba8d5de459b3ffeab98866a9071bdf8d0a69d4c061b6674ee27d3545f8cccc01eb3c2f825dc3240cb1352aa31a74bcb7361550cf39b7408efd89b92db78f3707cd7cc8507bd03b229333eeb7ed7b99a3af956dd578a4534e5ddd51eeac7e8093d85d0da5deebbd10c8a02d915796c39569e2c6b6dca50dd44250241450689f68afd90860a248f16bbe2d940a29dd075b57499d4b7882c849f1cc5ea9d5b5eec82373d7eb5df7cdec052059ebadc5675997bce0e75289692fa4387c89057f09c3aa9aa13ec9faf75686c13a3d6b9ba32b74af23e472b650b0a02d88fc0a2642c769dd3b21e40860fff0ecd2836b50234c9eb59367e94c4f7d67a15233a8cdb89a00cde39abfa37cd39c05545f76dd0f10080ff063e5b570e4380ec1bf3749c888d365f5d3af40ae8b882464dddfe49ae94b4a854ba5e8a12ca3c8e0ef84e54e5f18462765084b05cff952da91f697a06152f1fe08e466eabc0db68bb71aa60995de9aecf59eb1e046b5c66bfd546a226aafc06eb189ce8ebf861616d4893de4e69771b838f14e172f5128fa5d64cf044e078c3790e4f726d58060b68db7d35075734f9dd15da312aaae259953b317d886aa147be29ff67c8c7d3357e541b6c7b52532ebca563f1f762fcee41a15010122d881751ea780adac8f5f3635a3c5e3e63800ca562d51793920a9b16e64820d175ca51d1201f89f70fdd0fc507e092983b56480e65c9d54e1e739967e2bce7492cf6e204af79635c180d898ffd25aad756e2c022a17f575ad27d766ecccc5df7ca619ed6b3535d5f41d92879457920d303f11dc8e4319b1c5648a3217d8884a392bf35f78ab1afcb32d48018580e6cf672f5f282dfbeadb7663247d642402b817e0eaaf6262dadb464ae109172ebad7c7b61cd967792d252c3a2fc86cc2e32b6f292210988a1e91da624b73177cced6908860777f60344a854b4790de8ce08923b3ce6f312b7e0fa3b64d1497f3c5f0419c453f021d54a4c8220099fef8baff641a8222c597944e2b2c8a42c0792b94b0fd68728134ee065d9abd034e1898284fa2bbcd40b50a28890b8cfa2fb8879fe7fa134655de855a5f368013e2093e09e2885440fd65b2680c10716190a3a3bce7924e0bada9be6869450ff33c126a61d5a47222c98f05d62e2d3445cea71c74461408b4af5f4ebf611233b07abdae7b83e0a2b23ae726bc33d6295a4c7cc095ccee846db6420a99d2fd10671f29965fc651333a2e9a81aa1e1c5a1f1594c4301e3ac29bbbc617956d25b6a3bf3027fc78cbb5231b252a594c356b5f9424f5b93f8e6873787cda67306001d45f06061ef5deae1e6bc6653efaa14963ec1c32bf3d8da880668bda732247427640447dc305ce88c131d87b09915c281c607df4f61df5c4e9958edc14896554d2b07e82f084b20298ee79e266a232fcfc425ab8feb6557c19ccad2d0e79f08a747c75a858d830714a6a0d0b6637f232c4fb7f9b61f441a61e220a4bf7f647f88166574e29a964d5e7bf7779725fb5bee2c82fb6fed9e84a146f80fdea27f529f90aa7888f9c27b772392516c627ee9703956246570bd5336dc3d6d31f51774ac6785649b7bfebc0b897f2b2ff6d359cc96f952b3ae90c9e28fb3280656c76e63d6cb9201a0706418bdd22b6cf8453566aed70e06084a04ca1a13be193d8dd700861ab76b560dbc894a35126702d74c2fb8f0a1120d4bace6766d4ff416bf085a309d03989021872025608fc7406af3bc62e34ab0a07d6a436750b6e8ef2e0943d9e6d2ba0288c895d40690d9bb8b9418148f567ced9eafdec35393a27f6f2693ecc3603b016a72ca1315f7f5942fadfe388817e9dd0c5da1bd4f3204f79857e6d20d77767d4f628e1024e80d95a781e79a4d993355f7dff28a645a86f33efd3bbc0f6c762432cd78422c02734395da433eee586e7cb16e7b4598e7295cc954a2e1ae30be37c7c02bf1673850e969da95006722daaa568e93ff3ade823cc5f3e51f47650106e78d7db0bb653695e1038b270a3ab44c87b6260d9621aafd43420b9d4be85884e32816b6f5d7fcc9c54edc6cfcd81f7ca83b788c93ba8f8a8264b6c77eae8ba8686b5d9db75b559af9d56e9c6efa9db15d5db21a9875f6bcb2108b3e142acb2b382211ef0ee9d5dbc1631b7af1409ffdb26ee50a9481a9b3ba9a50777ff4c56927eaa1ff14b5af2fbadf2eeca2851c9bf2a10dbe8158675d3ffc08ea340bdf3f9d7221f55334cc4dbe75d28deec542e90e0dbf073e19416279ec1bbd13f3ad0a1f4db20f5a5ccbca4b2adde7667e6b23289916737e611c0e3fe0b387766e590d80720983d41b864dd861d9438deb90b82e0a71b938a4f8a5ce218ff6ca3b005fea41546b3de751809e900a7da605ddf4e0a7e8f3e009a5db607140ac4ff92de65b8b3848852e58ed183e09fffb82f56a864bdc1ed5e60f1a83d8aa4040f1bf8389b4625312197c6d9c616b86b3ebe6802235a0952aa1944b970c5e50141baa027002efb870713af8d4c02359cfb7bbda74f59f5a08eb790528122cd00f578879df2e643c90434af48a9c48175ca62a6167de63c7c5a43dbd5be7368e52254f7ab54d9f4f6e8a8289808f2a3a894faa1e25fba49bf2f2cd9433997b1329cce9204e8d2ea26c9bc0e151ba2a900a74cf96fec71372ab681e479dd26a5f8176bd3a3c4451c088fe201bc3de284899705d08d861b41f58133a4e6445ad8f83739e71349d76f22c28ac65893ac167972b7dca167f5859ee83f87545a55702c83eaa6b54d669c398e5ed91589e0a3f10cd666ba3e2a2fd6a306e9eefd033c5040851b0173736d7b223e5d68458cfe7562c73051388050329dbf48516fd8b1a1b1be0a0958dc17826ab76ae2cd0b1560b28807279ef51f4265c92216e2da68fceea6166bf74498d92d5f3142514fc8ebde801a32d1c8b6ca7ae61bb476fe5e7ffdfba2ffd9d705648241ad26f278d2448b25ee498a8e437742bece827dbeb59959449bfa72fb429846d51c12c2b2c9bfa794d86f6bee29cfb07640d03e8e0906d2293eeb08fa9feb4b1690385fc9325d1ceadfa7ad329e97a5199d525c10221acff28c102e0c1f8d1df1d567ee0fa57a7828446aaf0b96f473500e7dac125c9ba6c7dec185b30943b9124d34a30f933515ff514eea539575a20141c6dd23a213fb9e01bfd5ace7a9e32ca3a2cae774f7c5602c22584f33d8de4e90c593b8419647e77ae59f0b188431ac3d06ad814a3f9f49b2da4d6ce45563b469bb748a72c9ca0c96a5080707fc5048a05f96a89a023eaf77108368a20179113f0df1aad01befb32685b06acde84becb22156d2abcdc4a65d98bc52791644e526ef928b301f096ba6891bad65dfc5ffbcb1b9d5222be72fc00a7fb1396962d46e82f173de80d6af187beaf4928bececa505a96eb90e1ed5a9132e54d6d95d21653717a917fdeb5bcef69f09b4155fdd7e3b478d98cf7c9bdfa3a2646b386abfe634554821b26f4e33bbb10dff4d2b5be964ef2db057f454ab0c60e8f6626d2c3e44c47f24a12577ed4cf302bcbbf27350eab1dda661186d1c3577094058f7108adc4cf0397f8072ef95854dbc35d28b15d265c72cfa0c94563b651353c14f25c2216542349d7231b675f130bdc69e0b5fa1d874bf09b8b5b218aa969d2b2996992739aa248c40a0b28e0a214ca107425bf5753bd1e193f2b6ed0e0a6fac2531b34e6ac53ecb752fb2257063b6c259503962464d4c243d71dd5b8616a7fea3be14067fc1c897599158d8560bb79c01f433182e56e93b71e2be0e894eac3c7c3e381f50e963127949fb93a758b2a30d3bcb2e0c3b6ecddab17dffe261589340d9d6639d37ae426698440e431e587fb3b0ff697cb04805d8b1ec8ad3003e1392b64f79d61f3ba30cc8dc8f36804e3f2cd994d99f2b33cb494019b35083a0a823aa62c22d9e26fb2e366ac91e65a21e16cb80e723d1354e1115c167e975edcd6d41990c29a1894362ec226bee7a76fbca65383b6f4eb8fc60572a687668a2ed1a858839d1ac14462749bd7bcc53d072af2196120e845934a4e84648b79db7838fe012f0f35dd2df948aae9d0107047d84555bea035d3a1a1d4a7a5a4019559a5abe00b67a06451bb6d3e51b4a92412321d4a59a5ca1bfebe8ea3500343d3143cf04866807f99cccd19da339f855ef74871c2e831728e3fce147d510d23eae0ac1c6666fb09dfa5fc3c6ddf147872faa7afc2cdc2ddfc3dcd681f4dddf30db2c0966f9ddb4e515a2382002f963aa5443d11aed18b336bb6b8ea544515e914e92d70d732eeedce00fd5a218006295a5015e97cc60db3ead326083c021da3e6e022dc10efd2a5f1426956460d5e35db3e791b4ffedc57a5a1cb87e524107a00969a284b48ec9f430f3213dc460522e7002ee97597bd0f479097ce091fc1cf6a68acb1b61701acf10f5d01c29793290ff32f8c19fe92abd038ae5d84c872ab28cdfdb7f93754f6258230585bfac505ba68315e11ea5d7b9f59b93d8342a6c8fbcb6f3b8d7c5a385bab9705250a0f2c992179b3fe1af0c1ce788a13378ba35eb4e2c456ba2274d2be5e8954cabf3744fc5ef123e437e48ce689722e74e1d1d1c6809d4dca2a98373f96d89ecf08cf9a9bf8da5638f24dcf667efb659aa73fde25ad225032881f0bc5f2ef4b0bfaf11a74212178d50b0ce1c9ef1c030dd875b21279b36f699653e5ec9ae54461261e7e0ac440e180f2b8018570e376fca26e9d1075e83dca5a40433e9a55b09e2df997a33b142e58c4416d4c7a31182321a7f24af6860b357d82e25d984b45b217207a8a03b7215f15470b277e47af0124a1527a0ba721e1329c2adbd03845b9c3700f113f4db82933aeb97a164b29246162235586aa7943a2da94b1af0b553856921b47fac329781f5d1b50e00e3b88f3497c9e4336dd840d362fe6b827c64c2d8f65d2fd0247711a89b90507088345eecae8591ee9a35c8da7f2e507b18ee1aac5b0fcca23f1e3de8693d4fba0b462e18035346816a10f887827c42d3c1bfcdb7b8826f973080b74934b1862117ed512ca39cbe3a698b5a2b945532eb3c7a25eb3374952b843b60022d2aba97aa93c1b48b7b8444d5fb5461dc47ca9df38a44ab59904630460222677f759269e61f3266d908c021e3baeb4619279658ce9c635da69aa6d4058a9f5f0b58555a35c0c8b2b409c43ba4cb55989a71128df78c04ac91dfe1e6bcdf90ca631326d8f4278eaab3c26758dee51372da4b7f18e03b2ab68bb9d2d3f81e8a82d40f4f7b5a4b5c6548c27ab132bc3ce2856377f8af2a8af287f78efb243c47b2e0a13899a87d05cacd3202b39d582738a9221fc8d0167e413de4ccbe17564af7c415266638417b5d35108a03b0e14ca526a9bef566046a89d6db5ac3aad7207fff984d55d76bac178a3f2b8b7f109ee1fec63dd8eea152b7cc61788f5228b12fd581c9fc0f4159c61f93d50b3dfe053a80bffc5bbe93d2c65c18520dee1b46bb0b826b2fb3e8a16921b3017b1bb19fc07b158d332cd0a70c359a1e3e72e7d01d04db02b75afd3f150912bcc05658b52fa93067c237da180ae81819dcb047d8f471f453572b3cc3f109cff00da2ab9b2e5a095aa3b0563f92161e1580f707c5e490e72ee7e0514e7d0441e44bebb2d6a94a1fb35c6674344a6219f9d638a7d59662f4bcde24a89c2c731f69a29966f7a28a70a1a721297f91da79a15ff42f59c075b5e20098ca267a7a563b19ba54eca502277f82497300c3b92ebb796caa2afcfe1374581802dded4d355e65d1d30c74b766993ad77dad2c81796eadea182024ee201c09015d49214d3df7a2d32cc4d4969880bb8590f5e3002551a5e4b8688b7162a5d7c2bc413f1a02078c374106cc6d0ffbd4f48eb429ef8e94b441189bc2f1e0877f1f306026ed1f2ff0a034bdefcef3b79ddae37b79abd029693995f9958d9a63f787df7842d1060a5f4ef8017d29f1c7b2b04e9784f8242872c8c5e05bfbdcbb87270580daef668b581225ff8a0c8f213acc7965d1f5213f5a3c8597d62082127d8819439c124c6b84e449c4e712fd13507998f8e3b3b3db4d690c87db4f592894ee1c64a80c90b4959c29b96373a6784872be5b8bcf1fd2c062df46e2ef54eaa1705e08ee9a096596314ceb2f55b279908977aae194dbdcd34335ee63a10d7465c62ae830e91df35ff36bb3929e1818e3d1235bee64b1d8e66a436b09066c2881dca0ef0ef758a9052d4e13075db6cd36f4365c49b1ccf3ef03077b0531c7bbfebed491fca4c06e7ca562232a7a93e60f41c0bb9f02628bf0c95b016a0308c2c52b43fcd804809a8a854df51e1edd1aae0a75b497118521d51dd427ffba09f923102f7ba8915f393ba4cdf5fb8f748908cd79c12c76ab36c6b9ee39a95db93264a29e33a7891070c92b565e5f386aaa16b3cb4bfe0c03e62a82e2db3d425e499f37598202f8ee097640fcdd9fb601dfb4f6c27c7db8b8ade029c11cb72c2b0881f89bb57122829bee2defac86bb7a437a1a6c7a79450697e0f86f919c91f14af8eeb9118507e3846a8f1472ce206bdda24c22f2711455e1241f3f643d04226d29fb98b201ff64356f5c7772f591c45afe0a7fed0e95e74010ec682f1c97f3402bf640f0ce3c9b97f0482f541f57bd09692ce8d860b6a5f8d3e51af156f1dbebbbfefe983ebff0638f44e1b5b0033397793746c7fe549ef8c80abb1186664312a91fd0f1cf9828a247c9d1ab05997d737fdb66745bf47def54950c7751e864fec1bdcefaac86c7f5db041d5d4ca429ea2f9254a74cf7133afb5cd8feaa04454bb360642a8d954676729678fc349220f718a06e68e45bc0758f10cba02d82f4dd01633a27692bec219aeabffb0eee964612e45d623ddef73d369e8bfe1abb2619880b85b66f12b761fbd179bfc97dc26b066181c0a7b177d01be54eb6159b09d3ad95a1e6368d5af17a147443a471ca81f2c9f87bff557fcb77a4529418d548c6dd5c16dcb5b9812dde89c8a517297a646cf2db32dfa9c909409cca197f8e9125ad10afe56c12cd9b8ec15a3f774782b735dbbcf0c6ba96b0905f825fa9dec1fcb0db6e57bfb9b72912edf116af524f44b775034dcbf0b1c865c7419ed6a5ef97271aeebeb98a85362d3a20434edb6b3218d81f02e7415f9316a537b7a10a28da2345230f7dec5415c1c8c561cc14ebd3d453a3b4f56488124512d4bcb88f48d082169756e05f5bb90804511e2e5d588f7cbaeb5a5cfd350edb377765cefa1c76f566d6e0312409230d79a3c934d04e62ee5f6bc44f5bbd6c63e99ff6664941310bc5299c43dfe9e96d3d1794887b32d132336650d9f9606bbae40b8b63b318f092ad1717c2beba0e8a2ee674bf0699f3bd0da8e2cad4ad46282606a5eb2ac181f1f89e2c210fbbf85b2912e9ead70766f3a559976bebd334db4f9e84f66945f9d4f3f7918d8167050644f23da485041ded7871624e9b364f1333410ebce688374aeb3a0929edc5c45d219783313a130339a94440c79724889bf976ab981eb7be801ec1e85d9399f6b931b939241b5fd712eff9c0148a13b499fc21366f9be9274bc35aef3142a971ed416778a3df66b91030afaf6e3b24a78f34e8ea3a5635b6ae92a14bd248808090652574d57109adc362aedc2aee2f4a73ba8560bc20d1cb3e316be6acbf3d6d13d9a416135faf4eb67c10413530413d57d49fc15f3c9777e80fa44f57f7d7d0f2a1f14277f3b0242c8555567d35f7c900aac52f697d5e607f8f9d0f4f936e96df9e63bc328f74fa362d420cab92c3805aa38456875693aeb4a1316501e118bd0ef601e9d13414a7d048a12cd2d9ba0f53392cbf3864dd7935a6cf60022adc58f68b6fb280725fec99a0e2a1afb38160150ac03e27e38b3e7de0739ca40c591bc1d18969034d3ecf0f186a3f573765438aabc9c1d0b96413e6b4a631fd6920bf72d695000df5184e659cbb8ec9c1027d33856d33e678889a671cf0f71e841ca2b757b831ffe17737e919ea5bd9e458f7276a9505f5b7daac567f30afedebcc3d3146b47f3940d597cb45d831c63b769421d9210870abf7a23eede19e9227b1a643fc7afcccae763a0e22427d864774715c745b8717d527c4ff48d174bb6a88dae2f42633cc1abc29f9b3a8b27a195084802847e017d926d3d559cd27b3264a2aeab130d0f723001b193dbf993a896de4ca1c27f3a15726abe9fb36ab6ddd6703d18853a7013e227049378c9d0c25f2d6d984cf6603277ce52da22483ccf1ce8a8f55d7a9ff8a6807c0b6ccad96cd6f88381141e37a0c404ee9bcfd3876b327d1f18e0f09b8f50f9a2fb3324cd4e86666201cb83865968dbf59093746db5bb71d89888538cb3c2194a9458b68c31ab5403ac0c31e90fb05ff92d6f4ed00aa18e30e58cf1b3677b38f5f7548d0f78fd8210a09aa7dc9124ba35f078d9475bc9e47a20bc3cdfbae884bcecdaf4a7012cfc6a69f54c31d19edc397b65e6ea8fe3005834de2e0e9cce9cd9c45993f9e05e52eeb8dc1231514bf126774e91ea342d1ea0c9dd49ce76138063e5c809ab5e37823d03823934ab7255dfaa967ef9c14da21e39d9791c4b41d6b407a2ef0ad1641df868c1b01ec3be94925e31a1df3a06e33aa21bcb25fdae5b05ec23a6136048e6d64376002bd668389501e5d86d1bf0fad8fd298a3df9b9921288937f00d6f7132518e76d7a8746362b77ad0c8d1077aef6f50ac47ee9225f86ea1a1603c1f719d21c5422ab59831a8cef95a0d0c4c910e5897d79c559d3ead86fc610b7ad2cfc7b3f8e9574c43beab297afd108eab08f2b19b7314bb97b8833780f9d8a58b1066ca1ea122784cf2b04d04f190f971896982d744afc9e4d91ecb212181252799c555114a59d937a178fc4cb43e6bb4141ec8d05bfb9f8a576a3fab84e045b52f3c3e6dedc07c67a017b56d8211b718362218ecbb336274046f4443e27cf01e2da473d608e6faf7c7e259966cccead0e635166d8c3c7524d6cd9513ad948392b2b109dda124fb7a53e4b1c268803e5667e7f4cce50a2253d7d4a99be9753ffd41953d84a0973fdc7e08408c6c17b3c3f827dfd4e6b850eef98e368b77ab889ae42b0a8541020c049e279f0877b64a4e8385b501a9122f9edd0ba52b90bda60bfdf95894bd22657c77ff091392d6b09004cf282de315c58b60b56a038504f300d31feea8d76e27f9b546ad7541d093e903aabc452b0cf9badf6658a531dd768f5fe807d8546c21ce28d42e2202440e958dab82533e59a1b4e670d3ece6862c084391bf9acef0dd87a9de45012311de08843ad9a4f4808bfa61979b0b3f75eb40deb08a0dd8c9db0949262d1bbd9b6a80ddec0ec6322be78d4a98e689dadb2aa05a585989ba5ae698c72ab8f7fedfee6db1c5d439924c94e8e8c6d83e996f2df67a7738c140405c7ebe24ff8c4c59e44d1d99cfcb3085efc3f938680784f09f03aedd9ab398f5e4f3eea9c8e3615c07814c4d54da95cfb4f6bd85b3b2a79dafb20f866650b0aea1d54c33cd979a13e0885f9c896a874429e14cf7980fc8c120ee16c2253417dfcca2cc80bfc521c9bb468edb9746341a09c5f16f1b93e17c3e6e91bf2a26e80dc56cfb103166e76c1bce340dd0d04310e16341e3043eebbbb878da0b76fd4e9bd662f13424d7bdb18b1b53fd9855b8c8adb52edf6271b0ff3548085f0b0c869d59206d08b20b5e1195d5c48ef02c213fc0027908b714ab95a9bb63215bb6fe37c80acccb350c33695b5e60db4a8b147e01b57c70b68cbbc9e417a83048fdfcd7b0e8d4207ec18dbff61bdc5b0d97dce4e3516995b7b6c2e023afddb9b8a6749fbb8222bca7a496ff6998a582a740c71430e393a2b3b7eb4a22a2420e4285c91a95298c2f88b2e1bab8c7ed69c20fb81a78a1cfded8c9a6913862103410891d0b6b9827e1a6a87c13aac510b89924f3c8fd763297e3fe0dc6e57319c3ba8a7eb3a087f3eda54d39192edb8bb205998030462e4edde3039dfc2f618ff4b10b556f05976c782b646636f58fe8802f0cc8c7f5cee60bf9c409a4f357d8011f5511e4cc18b5f954a2238e02cd613df00d2e3e5ae55578f3e7cdeb6450a9b4d4bd6ac2078b959128168329c4edaa79fa62e3cb781349b92bb54494127b68c0c277899a2617a95b41f7fd537ad95cb36137d860a6e5b339dc8944248d3cce19de8693d6339684945ef2a9d17a11f6ca5e404ab31acb4e11a2f7364c56522cd96009c777f90181f1cf67f1c0860493efc407412d1a5cd75b545bb7ece55074976f31b486b2f524f71fabc9d694bc4c05e7e328cae7a157d222916d9931bf245de66ba8dc2c8740742d225d2f016f4218e8901308d05dbad6ab94c677749cf98e1d81978fcb53b32fc411309d2e189323854dabea8626da4dde5a21d80ab55c2a16cb4eca35ceed56dc5f26809e22603416c6f08022e88a0ca4d62e4df409c41af74c5e55b85116f5f9311f24afabd0256524c26a9b1ee1ee58bc1dd11e0cd0f793852fda2a2a1bbd3884bd9cce2b00d59aef6cdbbabb4e04fe7d96a3165d1070ac3c0732ec15a9ca0580e1759b27642e351e4a95c9a4b157a3a46a5deda3ac1742ba7b8b2387fd012dc2f9157f7c5f1c6578a704a3b1f2624cc7021804cec3093cacfdf7f2930611d0062ed2eba7c4041ad95158d88fcea16e9ac1997977d7578cad3e464c9b56d4b030c9c272dd84496f65fc94c0a8541fa75d8993430ac2564e1374437fbe5d4ae70d0e3a64fbe63d1d14cb32a67500110a32fb3a4a72ff1eff845c5f2ad70095b9e9cde4c6dfdc23760ac37fdaef6755154d18078530e40b3d603c08788ca434e1e1db0ba8a19c02fd1cc3bf37d3fc7cc453c0b93abcf856e01c76fef1b805cb31a9881d18e97310bf08090bca2fa89837657bebc9f2f2153aa93ba414a5fe8b0667697f5ef3f3b4f4d10d8db54873d019ca525d53c891c01fb8a4f99e60cbf78e2edfe86e982d9c674c31c7173b2dee775f8c5e94f2f8aaaf4b741a0c9a89b54a24ee19fff55150a52948286ac614e69c29dcca54efbe4a2f53c8ced1f6f68f0a2e7627a17e186a574a5453c0a2f3f873522d90510968ef2ff9db83fae4feb2b461dd91f48e9840df3297023b50c1282ba53c2ba60b98ed10ed57fd08a3410ac86d29c5206a4c279988ad873648b980d319bf9b7e29b7899df5b26c2b96ec6c44dee424bdbfdfb924f654586ad8a3332c9fba40095550b0b75856937502b366885e6df3028125d48fca067bc09ff8125b45a7b24e9dc59c795355b5dab9bbbe782156e4c38965d748ef8c2e3c9de64ea60853c5bb882ec8a22b57c7180d1d0d9de7cda8f5f705f5b3fe95e1fae2593ec5985f6608cad4c298fc62eb4be7ae1852f968b9b2774a609795f4f96e7fa0a9a8f4afebc57162fb02e08db117a8e33ce2982f6e249d8c9087c331750791268c3a00d13cc981a078f85357c396fbd0d60fb5d43d42422c4cb23da0a412e43a69b81dfe01e9f38bb8ab2f599841cfa5a1a8b1d87167163c35b2070879c0b3bc24e5b1798c421db7d6f827b25233cd1f4517e982424955cf5a6e07c03d94e91edbb7c81f40921758996629a0c5a69b3ef9f9acce8cd8fb5822c9e6bb5f4d636a177dfd56fac618b661e20906b28165c35bad2988e463816e158b32af160aed2192aca8dd2f39afd693f6122e10453c93e0ee116c75d0cca15a63a3d898bdf9b4e1756fdeded55489f3ce4a50ddce203b76074cfabede0dfc25f6626b12356c3a8c6622f8a05e120bac150cc7874386db5b0d0cf664602631de1385e6236687f75f0781e45dad7a28d259a3ce7f439c48e44fad7caf98e92376995317c7ac6dcd5589a4dcd842196c852fa2b591a2557b867c9479533b7b1a28a3d7bdc3a4da4861894b11969c7f36ca59deb7942049b810bddbfb24c8a49c27a77b6e542c015dc866fbb3ef006975e18e438d69442515c10b432b8c70d1ca8f46a771d92df5ffa6a78313c65eb2c56aed2bde518e276f5595ae6132713e392015cd491125f787dbcf6ae6e90353fb7bdd3e7aa2363269ea7b470bde6ed0a1b4aa67eb648cd9d3df309c90021ac416d1555cc933055db54dd569597994e7615e320778a8738e0d86a87c04da605148c637c791edb063cc5c64f45f78324bbf90bf03935a251e150ae0a10854c1e9effa7d086a8d44759fa6cf2e5f6f3dd754863ae05b2825f2497275715a39159c4a9be61f606df86956c9aa5a651be36a3f5a520929de69d79954af032ba6763f89be58f63c6c1a282ffb626b3135995f98c46f9ba7b13bd936146ea99e784af266c1f7f1f3a33ea3991b1ce7c3aee2db0d68108de533ea423a1cc1de2b44e98f040feda784596e19cb6936a368a3aa8cebc85a74b4f500375f2c5ebaa2c1b1b677ec1e6e7ebe9fea466270098b49be291119582d55a0843e6ea06dd0fec819225031d305fcc82dd2a39baf6a312baff530ad082b1b44ecafbdf70f449d2c542804e19f6be8a742b29e3b8e665e4272142f056756d9bd0d733836691234f98d323030a343395176d524b5e51f56cf33788286298cce2c978e61c5c1cfc1aaed1262d107ab9d19d8d7145b76b925b6875cb1ce1b156f6790431a45c75b8b73cf7dfe80a6c6e693eeade486f18a4663929d3a1f8b12f3305feed29a47dba5eae530ede3bc609a16e2e179a08497eecb2aa21de64a02d69557fc72f001b1f5a566ae789ab58a961f0728547df0f2bb55c8e2627e5f7f318392d3398c19acea1b0b0b70cb7bb5ddf7cc778eb53b764cee1f0696b07264b04b88c02b687cfb907c746982a30b71f586870cb8176fcec9bde7b793bdb122a77e1e704edcb63b3deb857cc943e1a886f867cf013a734f48f686bd3a66cb7fb770aca7b3d4e0483f3c7ebfeafce472a69672fc407bb1c3aa8d1b6e913ec02db4875172f220d25248629c81fad92a83bf3acc0271050f7908a4f643576b0993ba49a69ec0ebb9037cb844447c77110d380138f19a04c9e363baece871c4d0aaf5ad69dd1f0f41eab4294ab641974457c8a9830a131d3e5ba98641688fec9a057b455cd70eaba287fd8f747e39460d018baabd2ed33532d275eb13d06c6212037563c59d46ff550afc1bd908342109a0f96bbf45df8fb2d4af5677aea71ae4f837250889ea5c4ba28c268aa079a55e9bffbe499ac62ab340840a8e0d8bff79414b5d247c7732b484a7786e50d178c1e3eb6081a1b9b64da60f83a6ae613ba8f156b97d7be0b3f72c138a04cdb03c0d71e789d8318ba259576c8d105620f796c9f4f17b0ddc0ff4fa923e6ebbdbdf71c357ff60d4ecec4ab951e5540fee769a8b48b38d1165b62ee0848e9eb2908f9f05d88eda61a68b65bfb6b77c4b2a975587b2f5514a7466b539ac2e44b8cf280ef82405d36d92fc4cc3d9e9ead6965db534b16f08f8fc556ea92f5eee2dda6f50797b137f4575f3921f6573944d9a052eea0873fa8afd4a69ba4212149173ff31c159423c143bf85cd3d2ad4cdf1d2c3f38b8d9269eea790abc83fb8ae7fb1708f76063d9288de7ee3c6d577e891025b0498b82ec8e863ef49ebe0f87c5f3b5c77b265ddb3392d23cb77c0ee6a81e00057a8d405ed1823f31b1d31c6bc4f7d7d0fde70dbb877c8558601324d53c74cf9a77e4020a406139f05ac1e11c7fb1075230f726d6de8345dcadcb0c015aa88ea6bc35f7fc87cc859a7efeb3cf4bc8dc7335de202550694594b1706423ecf6743478136db1a89690849f945640306572c5d5a9d97acef802951c7bd38f5c44c663b94ccde29654b0e33289d7e0e66e68d782e98f1784f28a1472bcefb023bcd0d1e42e7b49cd1ef4eea6eb21cdbf8ba564ba6d1141974b5c51c1cda49b0b02a875629c701cc543db97de7837660328a372e7ac2e0753cfe0cd7edfc529643f3cdfa48098a077867c630994e9ba3aeed06d5e0e7052cc8d3dbc55484e7ce0f3b177cdf4bfe3dfdc5421e279324fada3174ca1e0b25bc96d937fb78052826f64f29f99fc609c4d2aab90eb15080fc96aa1593d5aac0b5567bba3f7a00c436a939a56b107066cef8e5f71062d7ddc2c5d9075c6d2382c734ef81ebcc885bedcd2b630956fc2393455b2cb0f56bf713259bc3b166ae009ab20e7aff2cda13b93a60c535c70dd317f70aac6c540037ff523f209cde91c9c54bf15d2129bac8c4a2ebb741a1d43c1198af6343af98f6bd411f23ec95f993104b5454ebface582f46109a789726e240073d328596af383972088755b5faeefced0caebed61edf04e7bae2c55a2322f5c174e39fa516abddaf093e70654515f9892a1da5855dc52dc75e7c9a167420ca6b4b4eea04ab634f9d6ccc5efcb18cf888836c4ebeeb9cbd667b3b8fe778e319c77a32fbcbd8616aa6f6b61260c2c8cc7b5fc4efb65ba345f181a9264ade518758e750cf6f07d75d1ca8b6ac6f452f8438d61028811f3d5c678d119edb98540d6f8e2520921037f68117496d785b89790a526896a4a42a06b02f7977fb115d599d643441ce233dd2db9cfe4802894fbdc19bd2070c7064dc7af0941eb0dda6723706da4f7245b5af0d6adc1fd8d23eb42c9c68512d83be00d834eca14a593cf396003309bf90379e4fe792938ed3e4efa5181c33a6f74e6b6b431a3f190236d21c1d7041ce6c59a7c43918d99120596794eb257e92b497ad3662475096ffc80cf6e91222d8d6af1bb238b5ea798a79f4e39a9bad5ce500f61d7139d3eee77bdbecf75e8d337080a2b227351125a54267bc5721d6b187539da55dd53413e524a8c7bdef2ac54c731471f93c08f8de124207317cde8e78c44cdf4e6630ca181ab7930edf5f9b722dde9e473da737db0ab6cc207a94b44c6e16bd2a8ba577dccfcb009a4b2089505351d72d3c057d1140622317bcd9cafa8c1cb64b7913bd8512620f76af88a7c651e58d3962ab87eae307da4c37114bd24455172ee1c657b7201ca3bfe20c7eede2dbdecd472ff4bc4785aa71ec1429e743d38a9a5318ac6978c031d93b078a850272a348a3760d4c63b50078bb66318a0ee38b722772cfed88fb1a22aefd783eaefd36d28e62f6ac95777ec856561adb769b17eefc1acc6447e14c4208efd10d64bada603ca2147689bfa798033f81795d47a6dd311ef9950386a1b26569b385af56042f9215cac96577b9ee436f594fb112f3cc4ad17a45833abefab136741cd555d85afa351a0a8f9e7bf4f72fdc538b8a3d5752b6297b9dfe8affc3e3902cd076a1a7c0abeab53c8f8a81b0dcb6427c776135078b5b2fa82b38f23a821cf3b3ba35d415fe1e1cb770f192702256837ce3cbb54a11ded14dfd50ef8897f6a1609df1d475b516c491f0ababd52391ca48eccaf0a398460135e7665c70633e4f6c6d9a2208e4655d1789f10cecba1fac01df59c15c45ceb93b9222e7fef4a44e694cff7aa72eafd1c35ddc8190b355751f35aa5bcae6b3be7acb3e45d76dc2223b3121c78b75c1dd1446ae01eeed6b8dd6db6fd5a8ae05694471466eb8b3ecfd921c4214248da21305b357115022040ea75385f1054c4cd5d1f91cee66eee9f9c9e35d3999b500bf1d3f70025b8d74997bb114934b09a62b6ef23d9ce8bcd3907cab22e87e2eec19212eea7a1dcc1a6802c8eed43b7ba1ebe5c66fcaac2f456314e4ff0e54095d124f8d08caaa63bdff38c13bf336edb7b231016ea84bfe2da93a9ea10302cb5fa23c57408aaf0f1a16f200022869bf05e43fa61fdde4812689c757a4beb4cef516e1ec6d4cd6353ed7dd9112854e130f818248a90b2c5b431aa365c64632da9d6c9074988701ac54deb3da83bb0397680225ebea1ca00e27b28b19bed307a405338f4c23fc7784620a6415cef347b9311bf0e6084ee2bdc6f165bf1685b821c24dfac480fb7478aa7f73abc02cd9f632701c31b8c4f49411b64dd34d2e8ce42470f2188a4d57d494670d60e307afd8047594b09302c385a1624ef5242d1d8c054fe5ee4ca35b9f90ad32ce2d54ed9879846af3775f6d2486fb9853224b4f09c3010be6063b8fddf5b3fdf09802caa295071517aa0b2699cb402c30eb2c32fdd2bf3b30958af00d4d4c547c93fede0dd1f7fb6e32a49f649da030ffe5aedd68b6b794f2286cc8c43599873768e146789a7f2b89e698bf97e79c7f4e305f6a0bef199c9d124b146f93f0678b62d3bf3550ac8fec9d6fdaea3e79dba8a9d42bc7bd7812194c5832972e1cec7db029e27ab833401df0c70ffa35661183da13ff5b68115898d4f1788ea348f3314805ffb43aba5e1665e115f68fc2a6367bf51d3707fa31d56786b15050b783f9e74b74bc1fef429b1464cafdc18e234a7e9b904fa9aa994ac58ef00f295afe24e9a4adde9bbf483eaa602463921670623737b994abfe95383129a6764ad113c61ea3c55e1b089f3c1d8c2d9816f26210fd5a75aaf52910a36212a6dd2afded5aca22c99a507a5ec257eb31cfbc242fa504e5517b74937c4a309aa000a311d6e9cbf8c109bac63bf97c9ab0f6cb028a9014f149954aa884b9d8284ebdfeb18718cab6d98e5d98081dcc2ff09330710d006f480b4238257ddbf0140af8a9351005f5a0f36c56b9ce82700bd55b32a76ea79e74dff518715ac45830758512926af2a09465e29593c1d0826801cecb2db27398f3d5fa2d5091487855b52254afe0e98c08a08cb1c610b14ffa7164f2fa158c53e15633ea9988ea69ba03bcd24a6deea20cb4023d809d6a83a217cef13ffd4b758b2b4ab86d96f9d5e3c574ad30e8ee220b238134f5effa0a8d31b4555853c65bacd221b76aab756943700628d96d9401631cc3802162385a262d21e80bbe7c51a40488deca03a6c57dfbc8875c974ef664c54c8a7e8227c9fb0f6c4f9924752a700645f10fcff521ee74a6fd19adad46a8f646eb4a295753342c5100a26b059dde60746da1af259453f82f672e5f0c0bbce1d7f843f16b27c36631497326674a674c6a752179252449b56cbdd4319fd65b37de1cf00bdc80be509b0768feac04e8e0bd19a0f8365fc80d61de79b3bd1a65d16798baa1c652e248bc6b5a7013e5891f0e5835885bb555d7f883d0fbecc819133894759052e1df6846ff3c70c25ad1596be9e52181b7733e71e3be14655cc35b72ddb5032c5a8eb40eabaa90ff830a447f04903c9f78e6695d166330b971cd84944895d6766b28b321906901bd93311d143c622ac31e6107fba0242a1fb49617bd50e7950fc2e4854b98b21fc92176c6cd41bc6ab8204c539ef7aa7bffc97632e9a82a5242735e44b1b8ee88484d31ca96ee581cfd90ef87dfc15ebf7a95f44e2ebed025322cbcd82ba65b6536dc314b1ed851ac2acf110a2b320b92419f75c4a630dad50c391ea9dfb83db8d0e079547380f27ba7e1b59a7665623ec384d608a68051e9cf336e670ec5f552d7a9fb9ec6d50a9f16efe592593e37e67cdf336123e311009368bb7f839018584f4999092500724cfefdd9a2f033fd6f9442e272246d85123ca48251d44726839201d6ffaa4f04cad71f914d4062a5bdfda4d0b17742b913405c7a9caffe943cdcc03a4b98be22df774930d04da2c9960bd40bad1dc1041ea9c232a6d26dbdca13486d4ab57a4f9f43e18a1c053462d4f55fac921ed91ad742a44b5014c43c6d36d6ac31e68cf085ea672a5fbe069d283ecf7ba532c358b712244a7082fd18b5cf4beaaf685db8606274eaf9bd3829d60ce134cd4ac751e2d800ad6aaef62ed299acdc4a7070c51f7078b6fe13ba2f3ea303e35dc00a7b5b34a63a776480200ca33df0f220fba4b39f7353c8e7869959d487e94ba7d090585bea9d916c3f8a64c27efb0c2403f5067e0e0f00b103edb274c64f1d4364045b0ff836d099a58c6c5867181bcbccb995e0533b3e5d99eb76b8e8cc7ef1d0b4ef0aaf8c9d74b575905615be194038a87a3307232cb153a34aad5875d240be74963d635e9c05762cb81028bb7f6216f5116e5a6ce7faf934f77e7902215fee00fb7e8d2457380f30e8f4598c00d0bccda1f0e4d2bea27e72d5de2d5e374ecde5b8c1d27c19c14a695293056c60107bf57ac2a79d5ada2b25dc49861fd385bf79234341ba1b9b18c29bbe372976e5e00faa9e0c625bcf10fe2106a54729ef63c9760b8121b1fa6371a04503ecb885820409936ac6c3fcd17f4adc231be010c19f3cc7520e60a8f1f71b6e721def47901f4ef891b40b1bd6f11bf50411cae43cb086378acadcbda143d6bc705c444ac37645b1ccf5ad7601c855f83eb6ac210c361b6d5adffd02d6a9806e37b131cecf3032038c8628142abbfe8ee58136c7bbbcc2d67d8947287d8e4b024844203f0d78dd752f91a7e6c19fb5ac09d0f1bc8c466f3daf902cfcce8080c7d57de820635429d96091590d0600151e654604e6f150a2ef9601fdef8a51cac8ee2ad9ad18fcf720304bbd8d87a76c40a6960fb7d32caaadb586d8c42d083eee5427337ffcc35edff85c318454c10023e1c2cc129f503d613c9c7ac93b05fc2b39ba703d4a7a172798368cc089aa874fb36c520f4cca8d3d05f936484ecebb73aa5c7671585948e13037452c489717accb981123568ce02b88788292eff3d6e83045e8c8f6428d55e6b5c7c65d687d4b54b97c9e8e76fd0fea56e632792e0b5997b08fb81d4d2c309b07f4b9977af6c78a673e376fb93599664ba33e989ebdba4635eb1d8ee1fee1dcc6e966f02b53bb3ba40a861636dbb04460c39f0b824a974ea36f87902a774c3473a2eba5a1ff6b88b933c8e292d8dc22106c9bb05c74dee28815d070a0b5b0986034b08068f6e2152981a41041906bcddadbccc0c591016865f5f2d6086476eff833acd92938b1e8f15d3fdf4609b6cf916736bed22e906591f1e92fbe6e482fd166da44082e18ab7d6b825b2a62b24b4bf3f8901f588fdc9e47a22026cc4c89272c53d23138bf841598d962c631e915af1a0a15d9f24abf73a190ecd3a1dfebabea3594f4efe54b400129869b5eb4c1c6e27d98e35bf819da4f2c81a769d1586bc9c25b433f514efac19e0ee1abf1781966d174022a9ec0797d43c1fe5f10e779b1dd3ab051eb0f8df571e328e3403999c0223504799f724ed18f6c6df435e25dd435d0cc066e6627d953a16eeefe5a351e2a1c43b4ef92ba937d4eae7573262879fb0489e57967e4269afbb12a087b67320ae4b7a701c237120ddd86ce1eb4d4b243ca9a41b178d48018abe01b5e335cbe13db99727387634fa62ffe20a8acdc952bc9b2c3bb2e2ec9354880859b7897d6f31f39cf11389f05a58ec31febdc2ab4d70ab9e8c460729b9bc9133aa6a75f028f7ed01501fe620e391d90dc6828758e014aee32396b092aa78dc8a42f694dd7402e8ffd66675af0a94fefa8343acb2b0f5f68940f47f98b26f7556a273040ed646018d8297e3b80c6efad7a9b78f9aa4ebb1463dfbaba335a1cb9405027a9e3eddc6a620e3097730bb75c85292d5d30530cd72df4daa00219b39409961387db71c98a40a9bbc16f430f2308428cc79a32dcdc02213dd2f70d32b9338f05b283ff207cef29c13ea9af15d54dd43ac282621a31eb67a40beec094e48455121dfda0869058ca4427d3cc0c39200e3a5bb41e7ef5a2d1485e7af100c81bf8102439bbdfafddf9f1e3b6914e6e50c9f0082df29df6d924bf0b91129950f81b658db03b9013c32f03c058a581997583506625adaeae6428234856d4b08f4d398b1a73adec66704fc8a94e894a646cfea1065bea82ade06f06e26616522972a1050d27b4dc1b564859c2a8f121349dc436f379d6d695ee56fad0a3a3869cf63aa09287a67591ad0595eb638e2ceb4157a33d055c6e5055da26408b86a2d2868d1f28762fd8d2eda019bd5d402dd8f4261148fb86edebbd2bf764c081d0c284d8ff4960cedd2c5e3fa31ccd5df63239f4e7b16b685425d7a16423bd76dec03196711689cf1c8175460df69b0c5a5e6d80227934404f8013c999a747c56a2270e2b9f4d7c92544fa30302f00fb8bfa2d413ae7f870ca5db59ef286f253f6ac86ab6e589ebd7a72f0f33ddfc2410db809a46c980abe130d9f1ca4a467a5b175b59ab531369fda5e6612948755f7a9ab12e2ef33d990da3d1022a3734c61cd887c851a385cfd22175004961e862bbd4db3d79e6167a1302f870b362364c77cd24b6e238b55249e3b0c319d3176c6a59f4b2ec36cf3122ec165e498bf0bf247ac06d11312f3f0845de8092b3f63963836d1aca2b1774cba57effbf6e71c7c03a631744d462040cec431b582e49a405545fdfd23ed7323315e898882ff875562bffc8a8c2dc23b6972834315a96c0fd0def28ece0d9fde6a1ff976751324727a1d980cd0e19d53ce8ba6fa8702b1ac57c2568d534b256d0d5a567404b93c7d490275069b030df9653fc83e1bab38302158c6179ddd5d9578668a6cd55ad8d6ae9bb90f3d047f19a6873fa08691cdbc7b2c9516c14688f7b62e4404dd40fef727da9abbd2ca9dc1e4a128c48d0fa0619cb9eb0333560a8f5092c610f8a68c6b25e0e79488da9b11170eee34c764df1fc0e22d186565412ebe86fcf87b4dd6ac5154e380e256dc3a2d0ae8821c1846162f487b0927e80e3ce7cf56352ee149bdeb214420adf65cb633f11864924325c207b6eeac079e025472e22dc5c5e40adbd2e3a892d5e1793922ba28da54965da01ff083f3e727ecfaa581003737b759c5b885d4735a5a684f07297eb2f7adf102427e512eb583c16005aaab38ca584fdc10014c76073810bd8ce7db1a1510613aa862f51208fa08ba24a6da0d8a53efa43abf94447b64fe227d3ce30e105524fbf4222b8159f8aeb9af694a8c16d2e222cc83ad56bc5b9f7e00a6ee0689249dc84b89fc3b69ed1c87356692520a1ba09bdb7e39c00d49832714d750ee72d309d045238607a820bc8bfe036552dcbd1e08e9d86e15f855332e08e5991379d4456b777e96e904c12d01bc50ca3ef09dfb984717ffc31185b521d9a7a35733a187e67f223b50c8fe5e8e43819f7e7eece611773c7df1a593d72cfebbf4cea78f215bbe13fa6c96ac941b44f3799ee50d81b1bbf122425ab90c8d334814ba3c8c9d9c35adf4dd5b7a8737b6854cb1b0e63fbb245982ebc1135b454693cd1a0570307dfe3ce0327a2ae16b1246aa9d3f63d7f7e9512ce28a01dcabe3e1b71f0f6791df887ce705521688d4693fa2181cc68a2b7f9273cd32d7f67b389c8748daca4c2791b128df136276d0e5a02e79d0711917f88df83e758a1cfb9806ab97ffe9474feaf92059c5a431ec40606b2d163136e6d26e318978d7bab1dc3473af0e64e03110d9d5b589c209bb5b37729ed1c0cac5314e9997a8dc293981fa2187f2ba412c6563a17f523bfd6dccbe24982b5d92e6aed0cbf21b3cde159d5cbd158bc86fc9af86afbc669eaf145810fab1ad5da830092c8a7d4e342c1b157106bca7e0ed859a700300ed25a5e9f40183236ab390403c4aed8229d3ebefa618360b739030b1fc63423da0e8d725da240663d33f440e2a667e01d5babedb6ba26240585c4a868bb219c293cf2cb5561b212c4810218efbd084b047c99750f429f4855853f4b4ac6275619853b476ee3f75696d602b8df4a5367c68e74f9f9d75ca6bf7e3257d46b221b77fa4ef18602e989a468be90afe081e2b392ac7ee480dc18b9102901bc9fdc5c2a204bda5eb8971e85c53661713581b5b5dcad96695bedf21b63bdef751fd963b75e94b59308d4a299fd69114450ba4d051528a90eb08f60b1403f0cd561d0cec2d0efff24c23c34904f9213fd6b9413815c1f56afdd8d8b5c266a87c088eac942f6eaa5da2e5137fda9ebbde3af85308483b5b4807664a9497d36e98cdb3f591c7d822c598270df5c91ad442fa93b29a2d9ab6b912469cacdfb2c2f3fd57f54fda339c47af63f70a63eb76e0aaaf54d6302d12587bc3ac69f8baf474ff4bf1fcbb008d91c908217b3725356690ad2e1c39c4bf29f3b9c715509392458e5cb2850d1c0b84068b0eb7c2bfc4b306ef3a78b99a92c979db063b03e353398b035428ad0b6367b036605e1ef3c86e3fa70c9af54c0647092483faabdf65357beb173381467a4c68836c04a828d50aaad8e5f6bf325211538508039bb7fcdec6d759e2d2162b38307564f4131ad3d544ab951977d118b459f8c56aa46b11ea8144837e80c1534112f5a5308321e4071fa704612699845e3ee33d57e748b7b75aebabf2f335f3b7516b422ec963d09a125d107c866c22904fc0aaaa53e565d26aecff230d73f32ff209b4a218191489bbe9affafb1b8421c583af28f40b25b273c626cd5bacec8eeb1d11605a02f98a48f4f7e48970f62b81091c2480e71d2e8818b629f359a243e1bd1102ccaccbb7a5639a02cae4db9650164453a2ecc91a3bc94a42c1c2c1f7cecc764ea6524f2111d726d16028cea2e5cad9b6bb65f985ccc9096549c24a98c6908354b61e92b8caecf44137b5089cc1f3d6b4acd05528c7a8f031a56ed5accc3db73bfc91d700ba31d50214985445e28cf6df00a79926506da631922fb531158e199d83e2f7425afa7083ed9e516ec1f78f4dcbb32a3ff2e960bd8a79463e147850c972a4d98132ba8d0088859e15246c58e0680bb429964abfd5fcaa73b2a82a415981bf7ab9613b2bffaed68001a3ae8bb6ab2af134349a9947cdf225a12b48222fdd0c1720de1cc1b77c9b507d8392c8e7c30f6bf585a3561cfc054cd7111fc7bcfd3e71a6528d2bf87ff223b0ef44d852bc5c7896b05d62b3b7b121a5790a1225b24c17adfe9c2dfdabcdb7660a3614e08974665adeb43b7f617f97885cf37332f7421af10dbf89d8d35d1309306a0360be27a40d01d747e02ae62d307d82a72d05b9011b148d76c1e63d94bc59967a1df8e2776565bfbddfa77638204390f728cf8a72bdbc0bb553f56239e27b6ca3a8444bb7567eca57a63239a5becc3f03a4d9b1acbe0b8488f5d30b26f7cd211162ef5b9102e2c121096159a80ddb4d0ba79f9a36fc02720308d9418539f9cc45f532c77b17e22c2abaad6a15193e6fdad3bc5cbdd41246229e76ed760f7bccf031f73ec79c51450810895785dc2753efeb135d893cda44df3abb6dbcb6bbda9e4009411c0883b15983743fbe274401ed63c0527d5ff6ddf35f4fea341f29c0c4399732d8e00644e6a769da01fa89c0305f203a9023c19b17313f44a3757ffc76c2be7b0c85e7704ab6b112e3066fa8e62b98469325642cab7d90e8b896606f03a4f901408417266d9ee09fb73efe4f7993653377d235b863460e5e4dd8b3a2e36cd49c434e4a21c708e02446e41e695d511a9cd31b2c4f46c858c31dec9289e1cbdb3b2462c6bc53a565cc1c48ac64b297fbd6c2ff8a0639530992605f1b14d81d0dbf4d2b1a5095bbfaa7045a69276056d9f22bb23bc9d4d393d219092612e64fa4267a6ecfe3dc7031eb478339cace5439fbced4791b5eb570cc1b90f04252ccb71336b1f19308b5e313a12f4fe931261cfb3a6749ee979ad77219e343d3eef23abd3c5e9eeeebe3ad22e8cbedb798ee5a421ab6784f23167960fda2e381c01e148e0d2f1f587b9d7549f1f73e79b2fb837fc07721e1a6f1f4d9d2b01f7abf2f2313f4e886420eb41eba76c0a9dad0c13aa689891d2a0f2db47e690e89470e3e4c09f91a9f6d754ab81e024cef583a1858b5c371fdda5e42b5bad1f1c39df5c93d77b867ad348cfe11f133eeb195ce82eabff5c8823fc0864a720381d76f5102e7aaf9f9a8d78125e667cef4398854b25a75c9d250c8ef0a0fe0c5a818daaeaf869a645fa519359268ff52d00ef37eea798fc9ae5ec50a641d777fd642e114a2d9ddd06c643faba8eb95cb9bd8b6746839f44638ac5e08f279334aeb745ff2e24e290e6cc46562228f7f69eec2ce12d6143aa57087c0b7cefd33dc39272a99ecba1e1e0c9d42d6f42191fbe5854bb78e41f7f4a3709f8ee6b0a29d8d989ba3910e1d68022a950667c58deb48ac080b8f0e727246ad61bcdf958fef1d4d1c108108a151a07ed0d43e097e28ca7a3a1f9df671e181485156b999b51d8f315d3d336ee31d216185b5c3304ae65e745ede653ebfa974b15a897cdec2194ff55c911fbae1dd8d57de9135c7beda3d03ca412b8a5ccde63ea5bb7c9811adc3f7f3321cc3e3986ca8b7f65c1260cee909c9e1644db5a0b5206ec850dd1eaf9863a90bdd3dc54aee135de8800a59b3222db2d96b4cea807a4e3c38d50764607c9033ddb35072fa510acbdf843f76eb3f6eac0ce77d5d2a048bbfa911ef1b77e6ab33fd7d21450cfed4e655a97d88e108b6a6e4afdafc4d84d1c36081c5c6209e4f9203b1b434276d9e7515fdb89039026a3490e873136ccba4187510d8b31c09b97d601730a120500d3ecd6f18953fe517cd4f4434e17839b95d839f0ee1f293cb06141fdf053e3abe64020f1450adee0bdfe7da8e34c9ef9a0608dbaf7aa74294354f3bb5b286d9ed2015db543d324c91035e2c1d2f0eae774c3497db7907ba9f6c3caca16f4364a0073436fcae5cff5a637291ea7489c303334ed693ccaa9aded0342c588995479aa41bf5fe697826bc9ee037164576d2b7dd8ee6b2631525a9bbd4e4331caa2c44a889a1feee31c0ae89a441eb8c7397cc6f50d04a9ed861e9fb1391b2985ed59fa8a5898424434ef58c471636b53141314b2dba5635d11a5416327869f72cba25f9965048e306fb7c08577a402c5ddde78be5f88b3d5368187988716a31c7b2cd1ea3b2579b7396d3b817db0ae9ce740cb2bb06f48d225e47cee7286ecb6f568ca4aaa2192bcfc48fce169624b6d48226020ce496b8f71f3f70836c51cb6c14e111205c20f3ec34eb109b48871572b0f8841b0382ba300fe67656b49cc9f66c5ef1cb9a2b43aed047dd2533bd733f46891ed39e6c6b6266af265df69a65292bfc05b44b32d08e64d297a847bce636f4e43c9b8e6374509e778e55045c7bde3041a8036ce60be49226c59ca663c9bf466d5cef87d771f87e5161c9e9322a2855260014c08f75a2c2c8baa13ae23dbb8ea14b29f4835680b81a1f7cdcd07625cd31242b783473860c92bab5dc8d58631f30d6cabe25d12617d66ac653cb8b322555ee779bbe8a2030866caa2658f53c01047c3e06fd54414ede512bbaf471d71734ef22f7ae2c1693fad0b0bc1d7369853a87daf6611095bfb6bd1c928ac113e7ae3a12dfe600e904ead60743cfa9449dd522f700b1f84cbf441df42776f27aa2bc9b3cc7356622b97185b61c5c0bb4518b99bec945e7b0c5e6d1ed3f63602d4bbd7f73f962723c889434c34d53261f5d5fdc65f4a07fe876314439d00f3e8b01a6b01a76d94f4d746e74c4326e6aa7c4c2337ab9f53fe051c5c77acdff59c5b32c3f6d740d3db2564419fa24d54e0ba02dc80600dded6e6f473739a818f99ff8692d08d43f339fc51dce5b6ef07f6c3701552660b804bc0d83122d29a714d53bd8ffc75116f71d252503416d019476e0c57c5f6e72421dcff8115d942c0e7a1e627040d6dfd6c0d1c2f6e474640ea8d460e875279aa0ee9a0689f2e9a066c978151c5abe7358c0820e8a3cee87ea964e1434402aef666c8f961ef27f3f9a4b21fb1fede9ab8bef0c7baef34d1507abb43bcfd54018a8c4cc226720e791f4586ecbc5b4f7cb14f88f56df0f0cfaea8865822a9d3670e6c054f69226a596720e2537aca316eebe9a9fb11286c7c076eb6ad3c2499ae0a75aeb76bc6e36cca31bf60f8ea00137d972ea8bae46e5772dbe0ac12e176fcab1161791797349d0fe784f5f36ee4131a093966a9709e98497e5724a59b73fe8f4366372a2958ffb561ff665eee8c23f1d2fffa1ea94d6632cd3cc05d23cdf0e3d12b63a494ad15ca2259ef9302460ca2328525f253116a5bc3e3264de2197d2cb9837fc95f47bd78d3d88fe04b0f382febecf42b97e7e696c387230b4d5a22e37bdab15d49e917b9c85a088b9a5d6803e65a554daf305f5c73e27a8bf05cae391fe9a85d047fc566d68c1156be4c3c4fa684083742f90f7c61d32e50bd840228208c981a70a8aa52736e7f2b9a972d7b694ab3a9a0fcc72c8ed7bd03ca49c69df2bcf591fe7d0cc16f30afac13c3d8969fd93a853fd00bee72adab72d50eb7acd3288ecf30097ecc0827d7f511a597233a840808a9e56caa18a46766e2ced2244d28c6b8a2c772f75d145fc436e18eade678b49fb1c4420e09edb26b202df92e1eec8e0c8ab45910f85e07c3c2484fd8fa9772add41f1de9d56b52c556c1ca4d89b2f9322cd5829eb90971e500a695e8e45c93dbfd5bb5af2aef1a34b37b00faebe7669ecd041785c5bbfdecf27602f49280481c857426ce64305ec58e7dca6d5eaf183db562603ebac604ffaa615866ce0371ccabf6a4c73d86e62dfa2e15cbae3674f1e8a35a59866c60b2ab29059269ae22f584e2b0b40009263af0b59149b71d5af985c627496c5ba844777e89eb0ac8478d9debcbba5735df4e8e7b6da0541d6c090cee55a1c523937d21e61f5273edde91fd6e7e0b97ed2a701637c3e720c7164e8084a88248fc9b7b2bce0bf5134a0665bd5db3020021b452096bbf2d0489205d17b0b59f327c256aa3408d1de04550b44792aabe1ed7460d4b8e251c756524fdfaa87a78cfd848860eeca60061f65da67fd14c1503ea321c89de72e9173e15f8cf9f8d698662d7642cf6dc6d442d014b79aed4c50aad080b3f01361396bbc4f5550a563d5c00bfb6933fe33c414ed5855c34d0e806ebb3038e37f301e0618ef8a347b616da76057f0e2408e75465beccdfe5154be922750783968ac43b44e6df03dd9759c3d29421b93e137bfffbf6b93caddaa62c64f0dad9b246f5ea4404c582da6e03f123ee65768333973409548004ffefbac95dd0b12f8496f95bf0fac323a61d612efbcb1db4029c2ff5ddf3305a685893c024fb83192ea4dbd30ae41b94facd06b935453f797c8188f3e5e745a057be23284269e377df32315e5710130beb9590a6024a8acfa112214fc6c06da745a79d0ea0276a633fae19c963dd0ef173ec05a427f3200e634de1d1a3c451d4f27dd46071d0cdaab6da5618ed6c19d0946c7c2f173b84ce49386f2e39398d65452156e19b55011839eda0f1b8f54cf0210306ad78c34c06e49613e042b0859b8781b80685e806a674b675f6a772585d572f5c466b38947da7380374e03a50e15d1295a1411f02257014918b79c8a445c800c5d309cd5078b476fc177955ba32a6cf397055350a71b02b31ed8e7f23b7c92fb164980a1456785994feae44cd45295f4bb4fa7da0adafae59a117bd5047882323dbd0c0aba508e2acd4841ab2ac6e5dd16b75f9809aee5f4c4a28dbe0598a219e6ae465a557e4aba167d40250c9d334c8e6b0ff8ef757407bb9c43801dce0f742eeaf25a61d18780a8207ba74677671a33e33d8cef54d6b245e6ab80d1d91f60acd22f7992f8f3d40b0970bc512992f0f9fc7eebd9759f73f168c727f4375acab14a226e251d4bb264d0c023cf67d9e3cc3115db56fffefcfb6bba39c703e95d528d48322a8f7373847b2ea416019db1a283678c995e14002e738d1e50efcbd98ab9bb24bbbd786e1b995cf34e9bba6144adeabcdd758a2567e3b1ecc28570690970a3f83563b9438700ab94c08f61851591aa2251871be2df696e73c0115e8e4b12ef08cd8074aa96e18660a5a12192fde1eec07c602aec8d8cf85695bb14e03591c3ec105f5c32f40f88aba2c2852fd10f31778311bda9aa517ea024ef032530fe9fec4b9ec55f35b81a28e581724b465564fb9a680b8220883c5f131c1c81b6723aa7fbc82a9901a8029b8490d7d83ea2594a4ab72708ceb83f97062db29c2252cf9aa808bcf5eaff297298b7b49253e9d6b53adfc70d7abbf60221b177fe21d119eea21767080b89a6fde51b5dda75e956987629b744c625dd57180d15081933e2ec82ee9ef3491922b0966215b73316a7d513ab437212a1eefbead2eef07c4852898d16b817dda58d1c0e0d3dcf5ef7819c61843d78f386dfa712e4fbdc278676fe2d58f38d45cd45819a5d0f36eb8b9944456a0d3599a11373405d8c8c6646ebb8bd02aa98503c1ec0d9a1393568af58b083259604cd0fb4dab1f6488e81ed0db2f8dc0f38cab9bf8a17dfc255e862ebf278828724e2c9e22cfe9706e0adc944c406b35ff7fa65fd2ac265e8041721e2fb7f8ed9b561600e652c9c2c175c93fc3c901a97eb99045aaa25672884beeeff6faa727e25c48830d44cb1f3cec72eac79e351e764f5dfb36762d3a5c39cedeaa340b432ef7c020ed06ee0d1a1889715fa92fa9ef0fa8d1cd5e53a7ea6aef98c112ce299dd588539d1a1ca73c687f36ac8fd8a18be7e8c2e72fd2417b980c45d8f03cb6f6bcb91c69d83d59f0047ff2c5722e4b4f773506c8dd76d879f9597d63f8e09036fd4addf831d621875e6a594d4d62aaed49b56f1b252de129053374eaab474671d29c3323a6c7e72cb2c6768f1f22f083b045574a5bfe837f30433eba19a8440d6f6e0a70b2ef02ae59a9c86d09123b4e85230d8251485e32c754c96d6a2526678dfd0b5a298425d1e86355fcdd9a037fef8fe83e44b8211ed4ae46acafd93f200977dca71e8a86667674972c25a01ba891ade3c961b9ab4b9d6d0ef60627aa698f74d5cca4ec423325610ba27baa835ea1c686974fd7f34bdd621d9c2011387fc10aed0343415c51d0c3cdac13e62cf17789ef55f9751ce447b0cba176b2d757cbfa09c1cc56f1608931101af016443aee77d6bf1cb77b55eebac2828cea08876df33894fc6c76ab40a8b63514f196fc4823fc84d50001101882396fb6f993ff4ffbe1100e0b9659c967381ba21d98df204419796374e0a17711254fe79e738ca5a984885c6a2bc4695f6fe0938d74b701b9678c5b798ccb4d6bdf105d4ebb5fd543da7a932570111f950c89a76dc84c5493f94dfd0d11399fbe6cfe51103dd6b84022fdb1acd50bda4c6808ec6dd7351108c4b7f21aeee460506c12d12e2194caf989686f0790a3836428cfe0d3aba98e3ef4b1ff9d6068ef8e4f9366093f59ad5db456f31ae19a6a0b2aaa800110705655776aa45c9be0151eea20ff0f2f0326cfc423bf1b87235841e1a7c908347f148a27d59323a513140b25bb429f6de4127c04d930311f28d66cefce5350684402b1f936bc5d52de7d51d1b4b33335fc5ae208882ee8a5807239d38f49fd4c7521b83373bfce958e2b421f01465c8738c54cf178a571d09fcd52a97e7cc86af2f15b308a57ab4979b1873db39312134da7948fcc5130bed1aca14290cc9a103b577c21e923143f876a1f142b0f5cc0e912b61cde06804bb32f88b044c987a7f87c345685473e263d3d3fd2533a5cbbdc67db53399bc213bdf8d90962cde34c5bc839d30e7d1b616536faf1fc88df90b2b1a7615090e16ad809f36099f3ea2fb3ca2b8bf5d43fc88fc3bc507010802543ee936212ca52a79e4b3267169cde84d6952510fa4878324c4db2df0348ef052a5103099175cb3c8b5888d94087f085ccdca878545da747d1847aeceabd1b70e11f00f02691ebb826e57b73a5d40b99f5a452742efe94fdb8f36374310fc6a6f7470f78d0837e8167d7bb5e24336af5a4c4a01b89ff664013acd034ae649249049a35feb436c84bed0b5b3b292b686f0dccfb3fae0f343615019d67495d5bafb569e1af084de1db59bc95abe59858f59c4ca7c7aa676fb5bf6889a847f49b90b6c3d52c65a69d9e9774dafeeb4165aa80d2914f3fbe14f00acdff96859960115eaf78c419454510cdbdb8162a0faf55a0eb6e68bc27f7c7c18c5bfc449990f19864d921b7629bb8687ec97b85e3b348ac6141555216682dfde0048b77f6f076b93a65dfaf3ce8f4d738a0720dcbb8a72903e0ae18437799b6fc7af75a8107ff4975cdf831649ffd51977e6893d65a008b9359fa67cb2575c9c370318c3e7ff462a12248dccd5ad09f21c1e63d39d7df8a0edde9e0fee22543a341e4131b4a0c4431acb6a1914f2a13a28585e96a0b3e224cb1c353f37487d8b580064ab9079fb470d4397eff5298e3611e0f467ab5b387b8a5717d8c64fa38cef331eaa3bd98b38f453b87b58f6968be64844438b07f51e5da8710648f2c236dd00e74d10f829dd8f6e774e8a66d3bf2ec642320609b055111d5769da376d784e16aaa4aef488e9496418d97fdfdb2bd06a4ab9cd18d81c9b1e71978778a8b8b8f2e24a38ad5b6b0723d482f08e4f1dc11b8a0c224361b88f48d36b6002267d2706e36ab19279ad0003c36f6e81f6b5c38ef0e1ba027de11e8c71037cd77e74867a1a0465a06a47d40362da44ceecc47839f224055e7e58d59f5d4b2857d1ee6b88f7d06796ebe2bf3832c60b0584faf7f49a82f0431c58af8d0ca6765a0ed900f5dc8e242cff5387d7408a5ef1a1ae79e6a02fdd78237a220bf83745565400fe4d028c7f96f3c8615de306bf0b71c75f39e60c0512c8027c3dc108ed3ec61d1cb228881e6ebc7b438dbfabbb02cb7070670834ae5e80917ef96dd5a3cd7a5704368d9f233aa9f6a45d5ae349847ccc66fbcf377586cb2f9d5b861144dd3363432a3e3eec9b7c6c0a8949e7965cc2873a8b326ac79a4b0f405a4dd715c240aa22cd8e822337250c57d7e4d61b612422a60664f74c52e1be3cfc5b7cf601312c7ae370cb97d1a04b97640cb63e0dac48c58524ba382a6b7a5d6ee44defbe79aa0dcbf9832486b31c6342e52b2985844559423e172a9fe434e2a3a2dca2452c65e5148d624914266855f4b318187ffdd00c127dea0ad2e505b3242e22fc1dcebb0157a62313074d5fd1996464efe32dd7594a8c747dbe376ea34836f310ed60857c0a9639931efb0c0086b2687300859dbce63c02dbb30e2d3d7398b8370865cc3153a1e0298aa25483f94b32083d56c003bcfe3388fd67c6698364b38a2592e4e15a8a6b0144a4cdd89c72fcedcf7caa324de50317bb12e80eebeedc26595fda959240c0e6f28cf7845dcc4dc4bc7ae491ad1ba4df3f30cdf84b1edab42ec9970b2b91a000ffc1293122f48523527ac9cdfa2bb7f31aa941eeb2837d75898d5d723a2486dc2cdf8b5e93f657d251c97d4a01c9fa396ecf516f0897001bcef1bde03ba9b694081466e1c89c4b81e951f29fcd4651d1dde8fe8c282da6910964d8aadfedda9be3b1b276af5c56eb3e205508916b92370701ae304e7badcd49aed6266f24ae7454c3caea7704e023873d047f48cd65f99217134141eeae0166cf4ec5dea08482ce92d1c0c84a1deac2c2b1d6b872d14d2a38d5d1cb98e354d9c5eda6f3c9ed9e50f72fdef04ae6d24cecebab75aa9b3784626a13c7dd0d2d671cebf51601cc23f586f0260ea0386aad153e087a6a4a895840fa57d067aa1867c38336d2d8841e50477971d1bd1b435cbfb9a59c7b0e6fa088b099410a73317099e7eaabdcc75bede61d983d9b0178f548f681a825898c300bfdd5272078c0b7aaaa9fc918fe2911b0345707f1e02870fea81f3fa27deaba4464bafa1cbcac83697d0afb05d14a41d3db9fe3955ac205a5ad34cf62ced6345cd4d076a9644d0b72f06898b7cfd07ef2e6e2ddb842991ccd05902ba71332a98fd51e6a28782195537791026446d81fa8b47828fc7dbcfd92028fc5d152eb0d4e436efc6bafd8f73e0c54b39b8ab8799c3d30d4f6569aa688fd39a617aceaff63c6d28cab752505b5524f33376b688e65fc55d511033d8b2dad94bbe3c7e9a946ce78c11868d5d2bb121946753a34c1ee43fa913862d670ff347eba9b84c96941ce149f608939d1ec959622c128e0587c040212832fb14c2e2913d6d7276f18304fa8163e3fbc7696346be36bfcf8669acafe0874827a33537f644fb8d99e1dc1c6c309c69fe1d31a8f675874a7f09f2f07fb27b13d27460e45250a9988b3425e1dd4ba61f808cc83a2fca570c84191f23ee413d8195f97a8d993d10e7676bf78c67b05526dcbc040f553034d031b1f5875a72e6bb81cb9990b3e9c3050f664632f9344651ea7a1916f903ac8db255e2ef073ddd73787095a4615a4648d6887426ce397046c6af8cf82c3a8314e561550a953ef910f2544dfbf3cbef145242241d038583b9ff1599dfae17d50f6b5f555d413647a189a2bfb0156142c0fd40130887fc488647ccbc06ea6d9850436f79d8f3fe7cd2842d8a72022be1e2544c9fe83fe1616ed12d82be4e60ce14f002811f961722ea577e3cf34934aaae2a3cebb5445e817a4cf0ad919ad132c11be7641ba698f000f7eb40dc3c468f250fff5d8fcb538abca726a3add1422071ea341097cdb0e1ef8f61243bd11acefa9fe4c2d7966a28ab676765ebb2d7a2edd694127bd82d93058bf3368b858da35f4320985c251474904aba126ca56a81a64c07b8e84feae823d9f41094434c7ee959703945cfe3ab38b0505e9cdfd60c499609d01858741500f0847186ab07846e0a78128c5277e94c8838cb60fc2370ac67322a64559dd225ebbfc710c4b6d6a34858ef0591c166347539098a9dbce65820fbc250e94a615600f0e6d686bddd5a9216e07aecb692ee95d3023d7e63d06e0a9e050932e2b95c543a57af1a4112fa7547023b06f380344859d9079216703499a19856a4274d88fdd8c9f147fd59ce741e788452a081170e89a4c714b13ebb7417f6ef26eae0695bf0e9e5b17ae849ff01511a7a411b04c271a29d8381fbb64aafe4a0b228a9aa7210fb087ae51d287334512da08c59edad534ee8707d73fa0575f5d403ed15eff71f9955373b719ec929be8bd1a7b414abc2993b439d2afaffe87f90863188ff8deb6618529aff02c8d85dc8e3150d9915be0affb6d647c3052cdd24d80567cf9cf97ec72e783b76409885c694e38601116e994ee63c07186cdae4f4f21efcbd2620a9c1e2eaa3fdd3fbcf09ad158ab02a8b33ca430e6313aad493bed6dc1e4d7958a7174a73e3145cc16f4cb14d58a1263c61d0daea821b0b3bbc9c6f66e246bdb4c815baf2fee4adade4964958167a06a80bcc30949f4e8f29b0f263345aed7e01011dd5ff8f3729f2b05b71d5bddbf164a14f0bea368bbaf5c08854bf96087d1404fc15a7ecdc94758e6051dc73566b43eff7ed7899d93a7f17f1be0c004ccdf627fb509a7f5cb749a794f8e82d8f4bc0ce382f85883874e253d91a5b6d0519f42a97726093288d8430513e8e52eb38c9a748e552402b3d771afdd0e81d9a49d7cf46e9e4949977a69db637afc7a8e58f17fa7001ddf87a60caa5d10c9e2ecb7d7ff0f73f2611c1933fa7d6cc2cbb16735a5df8b4a74a6ca546d6b1a3c174267baae9ef19b5a6b95c3c7dc6b5865dfec437403a7e562e00ec9dda56efd2c77f4be131f90d1b07c915d0053eae85f78df98e199ed67fa12bc38095da32a99d5bf31d7fc048658c4e45b37f8f402cae7fff6242329f8ebeb4fa51913ab54fa6618f8ebed45fe5cebcd6c4266dc514a6461bbfb067c55679910b2e19f1e13e676cce367a2cb001e8303fa283cba48484cac6244a414db5508fa72fbf4701d96da95ebc1717294d4b4be57ef6881e69167397fd6582d1fa7c34e6d0831123b13d615c1288cb957bb65a338c9d42c8ef37b542bbc4aff35c445dda67d98995b77c94ec5f765ed4ec6ebadb1d01bee1e43406bc45075e81a0107af8462ba4db60813198fc54ccf2e980094acb622bfa1fe423f15108c6292e9c5f83726020d80dea0a7e9123db8fbbd7dc02ba22939e1c12df32111056baa20e822d1aa0a524090335c4a41de2662d9490b8f2a371ca75a4305ec96cb81d4fcd53d3a1526bd3b63783afd4067c48dcb6499b469ea64788f85dd346595fc56261ed9cca310f3827e209f03b07a1ee1fec3063feb5897c943e4cd206e6784d0b26cfcf9f794e28993c5ee0a2835feaea9c45191bf7dcffcb82d7ee99af4efa5157f458cb657250c29aeec1cfed794d39cfe3399f302024fc27969dcaa3702dc199a3d71881607e79ff1f031b5798ab34773693767dd228a45c594013eb8ce9f09ef64ce342c3e6a1d072966ad5892c90214a07079ea90bba7f83a489026e2957cef6dfdc73122de409a35830f80624c2c28afba1e836cfcfc4aca2ded2256b424ecc4fcb045f42ac9e1e3f978cb5eae65a22e082e45941f893d096ab76bad52f6833b46c3d7a88cf67731501315a3e4ab535a36bd779be4a518b69ff610dbc9177c874f4856640a089001490d2584c894b94bb2ab20b5281d5c7fa00dddaecc2fc73fa3e7ac0e5b38bf44729233be4d1668f59b9346db91e0545847bab4274bc78b2f8e833de54e21e7605b935e3f6b729e98ed9f058f0c56c9120a10d1a337c9bd40a49984528277a450f7fe189e93b121beaeb4886b730672182beb7675031ac9298fea5ac5451b8a94ee243a207c6566e28e03226660af7a75179534ca7bf17a656adb19ff1cccd5f661ba911a3c2c3a6c54abae379a290b0ae462d67c87219f93963252f9ef8cdcf17b6a725eaaabf445829193c366dcd8bc0ffc239134ed8c6f704b61e931d9f7df95689f2fcb893bf101f077227f9eeaeb0ec2bc53737edb398613e3718305b1cf011b79be105be4dc122a246e99e38ddb393ef4aefa1b55488ad96c3f46726e13e773c652694443bd9fd5c4327e51f149efcefdfb5d6c6e6c9fd3030a87b08702847a02c230aa156701296ee0cc2581ea05178664a4b7995d8e85d4d7641f969de551a5c65d08fe304caf403947c5a7c94d6784b15dde3d96611f3785e042d6838c1a3aa1a3d813e3a94849d3b5fdefbca0b2ae3ba8ece8994644164d77c0b1f060b57235b9ae039810e80d2265c6352718491b078c53344afb1af0a16fbc3c1f1381fb2d2eb47b7971faa0ca040d887ad283776638c6570d4bd5a7d0690aa2c667cfe8aa916939ae893bdb0d27368069012d7d7b7eb3c4fcdd58b98af6fc9fb779c9b3655486cfa181c01d8262c16d6a84318e26bd0d969a6cc47a8eed41daddccbe8e0ef14cf9b23da103541227275af01384c2587f544eada38e75c0155caffdff58866315ed3d7d967f4e6fc5188bda450239d19aa740a688bfc8887a97fe62d81378be5ba61ac544ef53157570946910ef5c0d03cc4b83a1ec771cc82dc324f52e851690f828600777a5471425eed81c7dcbaf199b95e9089f391857726585d7d3819e08239c5296cad60187589f134bc163e9a45e4b51d14e4f258f5860300bcbafbc9e5ee540a7deb237580cd8f306403c602dfebd00f14eee0e2980a3b906af51b448a5a0f2dc355efe4dd2d842eef8db35196bfd82a456a118413e015d49250be05e413c311805d60b342c39866cee69d266c5e1eae852438966f20733b37ac66835aa33369a6e1712619b75d32d3d5efd6818d0679581d8ee92310c327411799df82fef5e75d93d82017788396483dc835ecdbb20d5a2269b3bdfdc71fb16d39a2f1a72e5e6d22e46d4cd4e08cc218fc32d41c21559027b0f21d0ce2c2be091c8b1f27f0f03d991e292f6eed70351c38c54138a5d3620f966fb5b83ef23a27ce4e16a1930c643b18c710f7b9b3444c9aef4c90abef37e794addf6ae69c3ce691bb1db981759c575a4d788fd8ba985603494ac3ed77be6d02348701765d801865757f02f7eb641120858babba7b06a0d507a58966a69fa0b6062810ca3d260451ef894ab1fe3152f987674b04f8a6c0f4a7fd8d17a7be03977de99a0e304ea6dae54a3528e969ccb6baa0590105fe6483e717c744e343e52c125203c54a2e006d8ac28497ef68d8a64e1c9d1366f3c34492e2928258af0f1d3d58b81647987ee9bc789d13421e13462da08c318f87833eecee96eea5058cddfc45337ab67e05110ce1dfc18d4d4b2ff119d53d3173841cce255c8993735808444862c0323f1aae477bcff585c208fa2c1b8164630cd256e0b17ec66973e38c34982d0582f6100e962cb1ea3dec19e55b3df282e4b8d52cd3dfee4cad5ed1bfda2b04ff6d5709ea8aa09ac97ee4eb421b533e55a8bf6179d0ef40adc1df31eec3a1d0d399afaa12dc9b54730b361526816c374b7cc0149c946e7844f8f9a8610dc8cc0e177a22d533ceafc764dcdd29919a64cb329de4013d089ed18e3ee252443eb0def8afe9eb01bddb12ea23b017fa38d0652c3a618cea3df65e025034d3098798eba7b687f736eefbe666dd4411a2168e58f94f51f4011081ec65c13da84ebd4b7ade4c6756751d14b3f3cd289ae826caea5d194d070ba521294c88754a5975da03b1fa7c512543b40a81a3180468f87ce3d721c29480ad6ebcc5efb388742734edc20c10cb7146a27939afb7d0ccec33a4563c8f2e17338bce6b31ad6fd3ba8fae8c433f10d2ec4c15f97f3dae72fc9970b68a219ed286909e0942c85e6e2ae3045076801491c8168edcff1ed62e67f6dca75829250d950c84bbf5f4194ced15bd91a67b0828926141c7b3fa41faee374d2f9f40fec501f8d5db493af5176a5ba7ce4401c2e6e313269888634defe2688d46075516f11b2ea983a905b2d256efab3ee9ef0462772870591fa711f55e14f3366931927fc935a9b294c6160657501257ea1d64c7f90696f1231106a94b33b4b312d094d46d318ef11077732bf1077417b8bfaf9535e36d39cd88cc56ba62814e6ed0c754e469c573f3eec70f5e863b55a959bd330e1dd75619ffeecc1f9c675606a20e939196d1a09b037169d8d1d793e08782352099c018d51959ef65d0aab41a9d0f3ba921a0cfcdcda2bbbf40ed211f5a820fa8e3a501ace27b7698e7f7cab7ac8cc7e9d0bd1590777591f9388c28465b26d40e24f8119ffcb8d9d6cdea6483712d9e66a7030278a99cf8b2e72423a21d4fc2fca12132153008f54f7c8f04a87b660c45a11a7e72177ef269b8baffa8a9ec911bd985b0f6af2dec8cfaa3618363f84bd5939ec234573adc7ac8c306d427f27b9c4f605fc02127a28b5b434e7485b092484b447b86cc895bf6685cd4f1ed2f0e80578b8866985ac8da089fdb85711bd63bf11f49792d718e437441306d29af2f66160ce711e814b305a3fd540ef009eddcfbd20ad696b2610ee5095f7f7e8045eb900fc1db39e9fb9835a42e76c441ea2147c3af3ea5ae068a97fd38c6ec429488a54504059bdb14af951ecc0d46fd4a9475517784b3e9b3be877ed9617696fe5c2e36aa71b35645e76a15706656a899560bd6bc47bd0ac31877c1e240b56aa4977cb4766c2b12280554ec28ff43630dc74a3cdb185503efcc8baa3f539b21c3176502195644a6829b237adf55daf9fa1425ea62c1e4764c1aa79e5429fd3dd25584bb20b5c1f453b2f1ee9742e343e08d5bbf299b88254187da06968f6021f345ad48b363d41b6444ccb5c363cce9085f2d3dee00ad7d101a371595044cb0e11c141487698763bf3f64553751580c91d74ca72f0867b095d16a7a32fa97c2b6b97de44237eb6c31242b4cab66ee78fe14ba3e4a604ffbbce7643a564de8f8b6ae8e810b3d4695380b85e5fa5ca24b11e4b606fba419fd94d6c7ec29f1500b6c405f174cca68dd5e5833ef2ae276f19b9ed6c58434266e4caaf9dc721e8cd853746ee1dc684cd1b5f6d2c4abdd6adf6009bd3ebb28d26b61370744395c278f26f47273d2b2c7c37b3e101fe32d4f2bbdb4fb31c7e998f5aac995e063f93cbe7e3ce8b5c6cbd80df0d9e175021996323c9c495fde872e71ac8764894a08340b7201b7c972de7e06803c9c4cd04b17d93bdf6825a124fe2da62b1d37a72caf153f988187f8b2692381de1c8216e972df845c0bf2ce4a3cc929767819973b3597e1b0a79eabf7e9f5b008ee217763bb3fd1952fb9d642b1b8877a9c3b187f1721dc932df56097fb4d2e6a963c20ad6a4ed1535ac9b3c8431faf4b2d44ed1bce558a131213a681ac8662301696ef8c114b09fa2e80646dab924a1d6f82f3025fd723151fcb6433bae10f523d0a42c0ca968aa02565ab49416ceb9cee70ae433224a4acdaf861e3fc0146a7fc740b3a71ca8afd83c9802c8f91e81f905bcd4e48c1176556df0351a1ffd437db63e70880cbdb54c47f0b18528696f253ff564dcaac085df94afc1b820b80a69f799127a38374f03047323d4522a28c8a31c4f3366b910be0c3d317ca14e452e95e20aaf2bc267414bf65f402ea379ea1f3189b13167470f6c281a9b5cbec64df90e111a634ce6d2115655d5077150c254d375aebc71df76b8cfdd3f2cf597e8c7e34eda7dbf33cf89a</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>生活/成长</category>
      </categories>
      <tags>
        <tag>时光集</tag>
      </tags>
  </entry>
  <entry>
    <title>激活函数</title>
    <url>/2020/08/02/deeplearning/activate_function/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/08/02/deeplearning/activate_function/image-20200802191015345.png" alt></p>
<a id="more"></a>
<h1 id="step">Step</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802191106564.png" alt></p>
<h1 id="identity">Identity</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802192225991.png" alt></p>
<h1 id="bent-identity">Bent Identity</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802192422411.png" alt></p>
<h1 id="re-lu">ReLU</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802192448922.png" alt></p>
<p>优点：</p>
<ul>
<li>解决了gradient vanishing问题 (在正区间)</li>
<li>计算速度非常快，只需要判断输入是否大于0</li>
<li>收敛速度远快于sigmoid和tanh</li>
</ul>
<p>存在的问题：</p>
<ol>
<li>ReLU的输出不是zero-centered</li>
<li>Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生:
<ol>
<li>非常不幸的参数初始化，这种情况比较少见</li>
<li>learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。</li>
</ol>
</li>
</ol>
<h2 id="dai-ma">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义relu函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> np.where(x&lt;<span class="number">0</span>,<span class="number">0</span>,x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reluDerivative</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义relu的导函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> np.where(x&lt;<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="leaky-re-lu">Leaky ReLU</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802192509160.png" alt></p>
<p>人们为了解决Dead ReLU Problem，提出了将ReLU的前半段设为\(0.01x\)而非0。另外一种直观的想法是基于参数的方法，即Parametric ReLU:\(f(x)=max(\alpha x,x)\)，其中\(\alpha\)可由back propagation学出来。理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。</p>
<h2 id="dai-ma-1">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leakyrelu</span><span class="params">(x,a=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义leakyrelu函数</span></span><br><span class="line"><span class="string">    leakyrelu激活函数是relu的衍变版本，主要就是为了解决relu输出为0的问题</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> np.where(x&lt;<span class="number">0</span>,a*x,x)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leakyreluDerivative</span><span class="params">(x,a=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义leakyrelu导函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> np.where(x&lt;<span class="number">0</span>,a,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="p-re-lu">PReLU</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802192542538.png" alt></p>
<h1 id="r-re-lu">RReLU</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802192616596.png" alt></p>
<h1 id="elu">ELU</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802192635291.png" alt></p>
<p>ELU也是为解决ReLU存在的问题而提出，显然，ELU有ReLU的基本所有优点，以及：</p>
<ul>
<li>不会有Dead ReLU问题</li>
<li>输出的均值接近0，zero-centered</li>
</ul>
<p>它的一个小问题在于计算量稍大。类似于Leaky ReLU，理论上虽然好于ReLU，但在实际使用中目前并没有好的证据ELU总是优于ReLU。</p>
<h2 id="dai-ma-2">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">elu</span><span class="params">(x,a=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义elu函数</span></span><br><span class="line"><span class="string">    elu和relu的区别在负区间，relu输出为0，而elu输出会逐渐接近-α，更具鲁棒性。</span></span><br><span class="line"><span class="string">    elu激活函数另一优点是它将输出值的均值控制为0（这一点确实和BN很像，BN将分布控制到均值为0，标准差为1）</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> np.where(x&lt;<span class="number">0</span>,a*(np.exp(x)<span class="number">-1</span>),x)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eluDerivative</span><span class="params">(x,a=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义elu导函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> np.where(x&lt;<span class="number">0</span>,a*np.exp(x),<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="selu">SELU</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802192849873.png" alt></p>
<h2 id="dai-ma-3">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义selu函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    alpha=<span class="number">1.6732632423543772848170429916717</span></span><br><span class="line">    scale=<span class="number">1.0507009873554804934193349852946</span></span><br><span class="line">    <span class="keyword">return</span> np.where(x&lt;<span class="number">0</span>,scale*alpha*(np.exp(x)<span class="number">-1</span>),scale*x)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">seluDerivative</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义selu导函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    alpha=<span class="number">1.6732632423543772848170429916717</span></span><br><span class="line">    scale=<span class="number">1.0507009873554804934193349852946</span></span><br><span class="line">    <span class="keyword">return</span> np.where(x&lt;<span class="number">0</span>,alpha*np.exp(x),scale)</span><br></pre></td></tr></table></figure>
<h1 id="s-re-lu">SReLU</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802192914396.png" alt></p>
<h1 id="sigmoid">sigmoid</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802191015345.png" alt></p>
<h2 id="ti-du-xiao-shi">梯度消失</h2>
<p>优化神经网络的方法是Back Propagation，即导数的后向传递：先计算输出层对应的loss，然后将loss以导数的形式不断向上一层网络传递，修正相应的参数，达到降低loss的目的。 Sigmoid函数在深度网络中常常会导致导数逐渐变为0，使得参数无法被更新，神经网络无法被优化。原因在于两点：</p>
<ol>
<li>在上图中容易看出，当\(\sigma(x)\)中\(x\)较大或较小时，导数接近0，而后向传递的数学依据是微积分求导的链式法则，当前层的导数需要之前各层导数的乘积，几个小数的相乘，结果会很接近0 .</li>
<li>Sigmoid导数的最大值是0.25，这意味着导数在每一层至少会被压缩为原来的1/4，通过两层后被变为1/16，…，通过10层后为\(\frac{1}{4^10}\)。请注意这里是“至少”，导数达到最大值这种情况还是很少见的。</li>
</ol>
<p>优点：平滑、易于求导。<br>
缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。<strong>幂运算相对耗时</strong></p>
<h2 id="dai-ma-4">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义sigmoid函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoidDerivative</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义sigmoid导函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(x)*(<span class="number">1</span>-sigmoid(x))</span><br></pre></td></tr></table></figure>
<h1 id="hard-sigmoid">Hard Sigmoid</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802192939247.png" alt></p>
<h1 id="symmetrical-sigmoid">Symmetrical Sigmoid</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802193009414.png" alt></p>
<h1 id="tanh">Tanh</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802193035744.png" alt></p>
<h2 id="dai-ma-5">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义tanh函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanhDerivative</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义tanh导函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>-tanh(x)*tanh(x)</span><br></pre></td></tr></table></figure>
<h1 id="hard-tanh">Hard Tanh</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802193102269.png" alt></p>
<h1 id="le-cun-tanh">LeCun Tanh</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802193127341.png" alt></p>
<h1 id="arc-tan">ArcTan</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802193150603.png" alt></p>
<h1 id="soft-sign">SoftSign</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802193210251.png" alt></p>
<h2 id="dai-ma-6">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softsign</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义softsign函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> x/(np.abs(x)+<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softsignDerivative</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义softsign导函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+abs(x)*abs(x))</span><br></pre></td></tr></table></figure>
<h1 id="softplus">Softplus</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802193231689.png" alt></p>
<h2 id="dai-ma-7">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softplus</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义softplus函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> np.log(np.exp(x)+<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softplusDerivative</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义softplus导函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(x)</span><br></pre></td></tr></table></figure>
<h1 id="signum">Signum</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802193251484.png" alt></p>
<h1 id="cos">Cos</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802193308764.png" alt></p>
<h1 id="sinc">Sinc</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802193335606.png" alt></p>
<h1 id="log-log">Log Log</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802193355766.png" alt></p>
<h1 id="gaussian">Gaussian</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802193413489.png" alt></p>
<h1 id="absolute">Absolute</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802193435653.png" alt></p>
<h1 id="sinusoid">Sinusoid</h1>
<p><img src="/2020/08/02/deeplearning/activate_function/image-20200802193457908.png" alt></p>
<h1 id="hua-tu-dai-ma">画图代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">allFuncPloter</span><span class="params">(start=<span class="number">-10</span>,end=<span class="number">10</span>,save_path=<span class="string">'tanh.png'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    所有激活函数曲线绘制</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    x=np.arange(start,end,<span class="number">0.1</span>)</span><br><span class="line">    y1,y11=sigmoid(x),sigmoidDerivative(x)</span><br><span class="line">    y2,y22=tanh(x),tanhDerivative(x)</span><br><span class="line">    y3,y33=relu(x),reluDerivative(x)</span><br><span class="line">    y4,y44=leakyrelu(x,a=<span class="number">0.01</span>),leakyreluDerivative(x)</span><br><span class="line">    y5,y55=elu(x,a=<span class="number">0.01</span>),eluDerivative(x)</span><br><span class="line">    y6,y66=softplus(x),softplusDerivative(x)</span><br><span class="line">    y7,y77=softsign(x),softsignDerivative(x)</span><br><span class="line">    y8,y88=selu(x),seluDerivative(x)</span><br><span class="line">    data=[[y1,y11],[y2,y22],[y3,y33],[y4,y44],[y5,y55],[y6,y66],[y7,y77],[y8,y88]]</span><br><span class="line">    label=[<span class="string">'sigmoid'</span>,<span class="string">'tanh'</span>,<span class="string">'relu'</span>,<span class="string">'leakyrelu'</span>,<span class="string">'elu'</span>,<span class="string">'softplus'</span>,<span class="string">'softsign'</span>,<span class="string">'selu'</span>]</span><br><span class="line">    plt.clf()</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>,<span class="number">8</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</span><br><span class="line">        plt.plot(x,data[i][<span class="number">0</span>],label=label[i],c=color_list[i])</span><br><span class="line">        plt.plot(x,data[i][<span class="number">1</span>],label=label[i]+<span class="string">'Derivative'</span>,c=color_list[i])</span><br><span class="line">    plt.legend(loc=<span class="string">'best'</span>,ncol=<span class="number">2</span>)</span><br><span class="line">    plt.savefig(save_path)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">singleFuncPloter</span><span class="params">(start=<span class="number">-10</span>,end=<span class="number">10</span>,name=<span class="string">'tanh'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    单个指定激活函数曲线绘制</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    x=np.arange(start,end,<span class="number">0.1</span>)</span><br><span class="line">    y1,y11=sigmoid(x),sigmoidDerivative(x)</span><br><span class="line">    y2,y22=tanh(x),tanhDerivative(x)</span><br><span class="line">    y3,y33=relu(x),reluDerivative(x)</span><br><span class="line">    y4,y44=leakyrelu(x,a=<span class="number">0.01</span>),leakyreluDerivative(x)</span><br><span class="line">    y5,y55=elu(x,a=<span class="number">0.01</span>),eluDerivative(x)</span><br><span class="line">    y6,y66=softplus(x),softplusDerivative(x)</span><br><span class="line">    y7,y77=softsign(x),softsignDerivative(x)</span><br><span class="line">    y8,y88=selu(x),seluDerivative(x)</span><br><span class="line">    data=[[y1,y11],[y2,y22],[y3,y33],[y4,y44],[y5,y55],[y6,y66],[y7,y77],[y8,y88]]</span><br><span class="line">    label=[<span class="string">'sigmoid'</span>,<span class="string">'tanh'</span>,<span class="string">'relu'</span>,<span class="string">'leakyrelu'</span>,<span class="string">'elu'</span>,<span class="string">'softplus'</span>,<span class="string">'softsign'</span>,<span class="string">'selu'</span>]</span><br><span class="line">    plt.clf()</span><br><span class="line">    <span class="comment">#plt.figure(figsize=(10,8))</span></span><br><span class="line">    i=label.index(name)</span><br><span class="line">    plt.plot(x,data[i][<span class="number">0</span>],label=label[i],c=color_list[i])</span><br><span class="line">    plt.plot(x,data[i][<span class="number">1</span>],label=label[i]+<span class="string">'Derivative'</span>,c=color_list[i+<span class="number">1</span>])</span><br><span class="line">    plt.legend(loc=<span class="string">'best'</span>,ncol=<span class="number">2</span>)</span><br><span class="line">    plt.savefig(name+<span class="string">'.png'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/" target="_blank" rel="noopener">Visualising Activation Functions in Neural Networks</a></li>
<li><a href="https://blog.csdn.net/together_cz/article/details/96429166" target="_blank" rel="noopener">神经网络中常用激活函数总结【Python实现激活函数与导函数，曲线可视化分析】</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>Xavier</title>
    <url>/2020/08/01/deeplearning/Xavier/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/08/01/deeplearning/Xavier/image-20200801215301970.png" alt></p>
<a id="more"></a>
<h1 id="abstract">Abstract</h1>
<p>论文研究了不同的非线性激活函数的影响，发现 sigmoid 函数它的均值会导致在隐层中很容易到达函数的饱和区域，因此sigmoid 激活函数在随机初始化的深度网络中并不合适。但同时惊喜的发现，处于饱和的神经元能够自己“逃脱出”饱和状态。</p>
<p>最后研究了激活值和梯度值如何在训练过程中的各层次里发生变化，其中，当与每个层相关联的雅可比矩阵的奇异值远远大于1时，训练可能会变得更加困难。基于这些考虑，提出了一种新的初始化方法，可以带来更快的收敛速度。</p>
<blockquote>
<p>这种初始化权值的方法就是这两年在深度网络中经常使用的 <strong>Xavier 初始化</strong>。</p>
</blockquote>
<p><code><a class="btn" href="Understanding the difficulty of training deep feedforward neural networks.pdf">
            <i class="fa fa-download"></i>论文下载
          </a></code></p>
<h1 id="deep-neural-networks">Deep Neural Networks</h1>
<p>主要是对深度神经网络的大致介绍，讲述了这两年的主要发展状况:</p>
<p>最近的深层神经网络结构的实验结果大多数都可以用能够转化为深度有监督神经网络的模型来获得，但是使用了与经典的前馈神经网络不同的初始化或是训练方法。为什么这些新的算法能够获得比经典的随机初始化和梯度优化算法好得多的结果呢？一部分的原因可能是由于采取了<strong>无监督预训练</strong>的策略，表明了它作为一个 “调节器”，在优化过程中使初始化后的参数处于一个更好的 “基准位置”，使优化过程能够避开明显的局部最小值。但是早期的工作已经证明了即使是单纯的采用贪心层次学习的有监督学习神经网络也能够取得更好的效果。</p>
<p>因此本文关注的是<strong>多层神经网络在训练时可能出现的问题</strong>而非关注于无监督的预训练或是半监督标准对深层架构的影响。</p>
<p>实验分析是通过监测在训练迭代过程中不同层之间的激活值 (隐层的饱和状态)和梯度来完成的。同样研究了<strong>不同的激活函数和初始化方法对最终结果的影响</strong>。</p>
<h1 id="experimental-setting-and-datasets">Experimental Setting and Datasets</h1>
<h2 id="online-learning-on-an-infinite-dataset">Online Learning on an Infinite Dataset</h2>
<p>主要介绍了一个无限数据数据集 Shapeser-3×2，在这个数据集上进行在线学习。因为不断地随机生成图片，它的数据的大小是无穷。在线学习具有优点的：它可以把我们的任务focus在最优化问题上而不是小样本中回归问题。 还应该指出：当面对很大很大的训练集时，从非监督学习预训练中来进行初始化网络仍然使网络的性能有巨大的提升（就是当面对在的训练集时，非监督学习预训练的作用依然没有消失）。</p>
<p>使用的数据集中随机生成三个图形（平行四边形，椭圆，三角）中的两个，而且允许相互之间存在遮挡关系(不大于 50% 的遮挡)，最后将图形全都 resize 成为 32×32 大小。</p>
<p>给出示例图像：</p>
<p><img src="/2020/08/01/deeplearning/Xavier/image-20200801222528774.png" alt></p>
<h2 id="finite-datasets">finite datasets</h2>
<p>MNIST digits ，10个数字的识别，50000个用于训练，10000用于 validation，10000个用于测试。</p>
<p>CIFAR-10 ：10种物品的识别，40000个用于训练，10000用于 validation，10000个用于测试。</p>
<p>Small-ImageNet：10个物品的识别，90000个用于训练，10000用于 validation，10000个用于测试。</p>
<p>给出示例图像：</p>
<p><img src="/2020/08/01/deeplearning/Xavier/image-20200801222549174.png" alt></p>
<h2 id="experimental-setting">Experimental Setting</h2>
<p>前馈神经网络是一个具有 1-5 层深度的，每层具有1000个神经元节点，输出层使用一个 <code>softmax</code>，代价函数是 \(−logP(y|x)\) ,其中\(x\)表示输入图片，\(y\)表示图片对应类别。网络优化的方法是使用随机梯度的反向传播算法，<code>mini-batches = 10</code>，权值更新过程中的学习率基于验证集误差来确定。</p>
<p>在隐层选用三种不同的激活函数分别进行试验，分别是 sigmoid 函数，双曲正切函数 (tanh) 与 softsign 函数 ，后面两个函数是相似的，唯一的区别在于，双曲正切函数以指数速率接近渐近线，而 softsign 函数以二次速率接近渐近线。网络权值的初始化使用下面的均匀分布，其中 n 是前一层的神经元：<br>
\[
W_{i j} \sim U\left[-\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{n}}\right]
\]</p>
<blockquote>
<p><img src="/2020/08/01/deeplearning/Xavier/image-20200802000217651.png" alt></p>
<p><img src="/2020/08/01/deeplearning/Xavier/image-20200802000240030.png" alt></p>
<p><img src="/2020/08/01/deeplearning/Xavier/image-20200802000314255.png" alt></p>
</blockquote>
<h1 id="effect-of-activation-functions-and-saturation-during-training">Effect of Activation Functions and Saturation During Training</h1>
<p>在选择激活函数时，要避免两点：</p>
<ol>
<li>避免激活函数处于过饱和状态，在这种状态，梯度就不能很好的得到传递。</li>
<li>激活函数过度线性化，因为只有非线性才能拟合更多的函数。</li>
</ol>
<h2 id="experiments-with-the-sigmoid">Experiments with the Sigmoid</h2>
<p>Sigmoid 函数会降低学习速率的（由于它的mean不是0）。这里选择 Sigmoid 函数是为了通过观察它的激活值来反映它的饱和程度。</p>
<p><img src="/2020/08/01/deeplearning/Xavier/image-20200801215301970.png" alt></p>
<p>竖直方向表示激活值，其中实线表示的是均值，上下的浮动表示上下标准差，水平方向表示迭代的次数。</p>
<p>可以看到，对于第 1~4 层，其均值都在 <code>0.5</code> 左右，且按照输出到输入方向激活值依次降低，但这个位置处于 Sigmoid 函数的线性区附近，而第 5 层在迭代的过程中很快就到达了饱和区，而且这种饱和状态会持续很长的时间，很可能在整个训练过程中这一层都处于饱和状态。但是，随着迭代次数的增加，第 5 层隐层有 “逃离” 饱和状态的趋势，而前面的 4 层有逐渐趋于饱和的趋势，因此会逐渐趋于稳定。</p>
<p>对于上面这种情况，文中假设是由于采用<strong>随机初始化</strong>和对于饱和的 <strong>Sigmoid函数输出接近为 0</strong> 共同造成的。同时也应该注意到，对于经过 pre-training 的深度网络并不会出现这种饱和情况。所以，前面几层的输出对于最后输出的各类的预测是基本没有作用的，或者说直到第三层为止，前几层的输出都是随机的，而非像经过了 <em>pre-training</em> 得到的初始化参数，预训练得到的初始化参数是有意义的。第四层的输出加权后的值直接影响到的最后的代价函数的，因为它后面接的是 softmax的输出层的，因此，输出层的 \(softmax(b+Wh)\) 的最终正确性可能更依赖于偏置 <code>b</code> 而非由前几层共同作用得到的 <code>h</code>，因此为了在一开始让 <code>h</code>的值不至于影响到 <code>b+Wh</code> 值，网络自己就在代价函数的作用下学习了，学习的结果就是：反向梯度算法导致 <code>Wh</code> 的结果趋向于0，而这可以通过让 <code>h</code> 趋向于 0 来完成，偏置 <code>b</code>迅速学习，并支撑起了整个预测结果。但是同时，将<code>h</code>推向 0 会导致处于 <code>Sigmoid</code> 函数的饱和区(根据sigmoid图像，其输出为0值的地方，梯度也几乎为0，而tanh不会有这个问题。)，使得反向梯度学习缓慢。最终 ，前面的隐层会向提取更有意义的特征方向移动，而最后层也会逐渐移出饱和区，但是即便是在这样之后，网络最终也无法得到很好的训练效果 (以及泛化能力)。</p>
<h2 id="experiment-with-the-hyperbolic-tangent">Experiment with the Hyperbolic tangent</h2>
<p>上面提到了 <code>Sigmoid</code> 函数在 0 附近会到达饱和区域，但是 <code>tanh</code> 函数在 0 附近则不会达到饱和，因为 <code>tanh</code>函数在 0 附近呈现对称线性特征，但是，使用标准权重分布来初始化权重，会发现同样会很快达到饱和状态，正如下图中所示:</p>
<p><img src="/2020/08/01/deeplearning/Xavier/image-20200802002132784.png" alt></p>
<p>上面两张图分别是 <code>tanh</code> 作为激活函数的激活值情况和 <code>softsign</code> 函数作为激活函数的激活值情况。</p>
<p>其中有实线点表示上下标准差，而没有实线的点则表示 98% 的数据分布。</p>
<p>首先看上面的图像，可以看出从第一层到第五层，在标准均匀初始化权值的前提下，逐渐每层都慢慢到了饱和区， 对于这个现象，文中也没有给出合理的解释。</p>
<p>再观察下面的图像，可以看出 <code>softsign</code> 函数的激活值比 <code>tanh</code> 函数的激活值区域饱和区的速度慢，因此效果也就更好。</p>
<h2 id="experiments-with-the-softsign">Experiments with the Softsign</h2>
<p>Softsign 函数在形状上与tanh 函数具有一定的相似性，但是在前面也有提到，在趋近渐近线的速度上，tanh 是指数级趋近，而 softsign 则是二次趋近。下图给出的是 <code>tanh</code> 激活函数与 <code>softsign</code> 激活函数在训练完成以后的激活值的分布图。</p>
<p>可以看到对于 <code>tanh</code> 激活函数，它的激活值大多分布在 -1 和 +1 处，而这两个位置都是出于饱和区的位置，第 5 层还有许多出于 0 附近的激活值，但是对于 <code>tanh</code> 激活函数，0 附近具有良好的线性性质，而不具有良好的非线性性质，因此对于拟合函数来说整体并不理想。</p>
<p>对于 <code>softsign</code> 函数，我们可以看到处于饱和取得值并不占数据的大多数，除了第 5 层有相当数量的激活值处于0附近，其余隐层的激活值在 (-0.8, -0.6) 和 (0.6, 0.8) 这两个非线性区也有大量的分布，因此具有良好的非线性拟合能力，对于拟合函数来说比较理想。</p>
<p><img src="/2020/08/01/deeplearning/Xavier/image-20200802122004074.png" alt></p>
<p>上面两图示激活值在学习结束时归一化的直方图，分布在同一层上的所有神经元和300个测试示例。</p>
<p>两幅图中上面的表示的是激活函数为 tanh ，下面表示的是激活函数为 softsign，处在 0 附近的激活值是线性区，而 (-0.8, -0.6) 和 (0.6, 0.8) 区域处于非线性区。</p>
<h1 id="studying-gradients-and-their-propagation">Studying Gradients and their Propagation</h1>
<h2 id="effect-of-the-cost-function">Effect of the Cost Function</h2>
<p>在1986年 Rumelhart 已经发现：logistic function 或者叫 conditional log-likelihood function: \(-log P（y|x)\) 的效果比平方代价函数的效果好很多的，原因在于平方代价函数在训练过程中会出现更多的平坦区域。文章给出了一个两个参数下的图，图中采用的是具有单隐层的神经网络，激活函数使用的是tanh函数，对于输入信号进行随机初始化，可以看到二次代价函数具有更多的平坦区域。</p>
<p><img src="/2020/08/01/deeplearning/Xavier/image-20200802122705664.png" alt></p>
<p>图中上面的曲面表示的是交叉熵代价函数，下面的曲面表示的是二次代价函数，\(w_1\) 和 \(w_2\)分别表示层与层之间的连接权值。</p>
<h2 id="gradient-at-initialization">Gradient at initialization</h2>
<h3 id="theoretical-considerations-and-a-new-normalized-initialization">Theoretical Considerations and a New Normalized Initialization</h3>
<blockquote>
<p>基于 Bradley 在 2009 年的理论分析 the variance of the back-progated gradients，并提出一种新的权值初始化的方法</p>
</blockquote>
<p>研究反向传播的梯度，或是代价函数对于每一层输入偏置的梯度。Bradley 在 2009 年发现在初始化权重后反向<strong>梯度从输出层到输入层逐渐减小</strong>。使用线性的激活函数，发现了<strong>反向传播的梯度的方差随网络的向后而逐渐减小</strong>。</p>
<p>对于使用在0处导数为1 的对称激活函数的全连接神经网络来说，有：<br>
\[
\begin{array}{c}
\frac{\partial \text {Cost}}{\partial s_{k}^{i}}=f^{\prime}\left(s_{k}^{i}\right) W_{k, \bullet}^{i+1} \frac{\partial \text {Cost}}{\partial \mathbf{s}^{i+1}} \\
\frac{\partial \text {Cost}}{\partial w_{l, k}^{i}}=z_{l}^{i} \frac{\partial \text {Cost}}{\partial s_{k}^{i}}
\end{array}
\]<br>
其中，\(z^i\)是第 i 层的激活函数的输出向量，\(s^i\)表示第i层隐藏层的输出向量:</p>
<p>\[
s^i = z^i W^i + b^i \\
z^{i+1} = f(s^i)
\]<br>
方差可以用输入，输出和随机权重初始化来推导，假设初始化时处于线性状态，权重初始化满足独立性假设，输入的方差相同，然后有：<br>
\[
f^\prime(s^i_k) \approx 1\\
Var[z^i] = Var[x]\prod^{i-1}_{i^\prime=0}(n^\prime_iVar[w^{i^\prime}])
\]<br>
设第i层所有权重共享方差：\(Var[w^{i^\prime}]\),因此，对于一个有d层的神经网络：<br>
\[
\begin{aligned}
\operatorname{Var}\left[\frac{\partial \operatorname{Cost}}{\partial s^{i}}\right]=&amp; \operatorname{Var}\left[\frac{\partial \operatorname{Cost}}{\partial s^{d}}\right] \prod_{i^{\prime}=i}^{d} n_{i^{\prime}+1} \operatorname{Var}\left[W^{i^{\prime}}\right] \\
\operatorname{Var}\left[\frac{\partial \operatorname{Cost}}{\partial w^{i}}\right]=\prod_{i^{\prime}=0}^{i-1} n_{i^{\prime}} \operatorname{Var}\left[W^{i^{\prime}}\right] \prod_{i^{\prime}=i}^{d-1} n_{i^{\prime}+1} \operatorname{Var}\left[W^{i^{\prime}}\right]
&amp; \times \operatorname{Var}[x] \operatorname{Var}\left[\frac{\partial \operatorname{Cost}}{\partial s^{d}}\right]
\end{aligned}
\]<br>
对于一个前向传播的网络来说，为了保持信息的流动，我们希望有：<br>
\[
\forall\left(i, i^{\prime}\right), \operatorname{Var}\left[z^{i}\right]=\operatorname{Var}\left[z^{i^\prime}\right]
\]<br>
对于误差反向传播来说，我们希望有：<br>
\[
\forall\left(i, i^{\prime}\right), \operatorname{Var}\left[\frac{\cos t}{\partial s^{i}}\right]=\operatorname{Var}\left[\frac{\partial \cos t}{\partial s^{i^\prime}}\right]
\]<br>
利用上面两个公式可以推出：<br>
\[
\begin{aligned}
\forall i,  &amp; n_{i} \operatorname{Var}\left[W^{i}\right]=1 \\
\forall i, &amp; n_{i+1} \operatorname{Var}\left[W^{i}\right]=1
\end{aligned}
\]</p>
<p>进而得到：<br>
\[
\forall i, \quad \operatorname{Var}\left[W^{i}\right]=\frac{2}{n_{i}+n_{i+1}}
\]<br>
当这两个条件都同时满足时，需要满足所有层的宽度都是相同的，如果使用同样的初始化方法对权重进行初始化，就能够得到下面这两个结论：</p>
<p>\[
\begin{array}{c}
\forall i, V a r\left[\frac{\partial C o s t}{\partial s^{i}}\right]=[n V a r[W]]^{d-i} \operatorname{Var}[x] \\
\forall i, \operatorname{Var}\left[\frac{\partial \operatorname{Cost}}{\partial w^{i}}\right]=[n \operatorname{Var}[W]]^{d} \operatorname{Var}[x] \operatorname{Var}\left[\frac{\partial \operatorname{Cost}}{\partial s^{d}}\right]
\end{array}
\]<br>
可以看出，对于每一层上的权重的方差都是相等的，但是随着加深网络，反向传播梯度的方差可能会消失或爆炸。 在研究递归神经网络时，在时间上可以看成是非常深入的网络。</p>
<p>论文使用的标准化初始化方法：<br>
\[
n \operatorname{Var}[W]=\frac{1}{3}
\]<br>
其中 n 表示的是每一层网络的神经元个数 (假设所有层具有所有相同的神经元数)，而这就会导致 BP 算法的梯度的方差取决于层数。通过层次间的乘法效应来初始化深层网络，建议使用以下初始化过程大致满足维护激活方差和反向传播梯度方差的目标：<br>
\[
W \sim U\left[-\frac{\sqrt{6}}{\sqrt{n_{j}+n_{j+1}}}, \frac{\sqrt{6}}{\sqrt{n_{j}+n_{j+1}}}\right]
\]</p>
<blockquote>
<p>对上面更通俗的解读：</p>
<p>文章先假设的是线性激活函数，而且满足0点处导数为1，即：<br>
\[
f^\prime(0) = 1
\]<br>
对于一层卷积：<br>
\[
y=w_{1} x_{1}+\cdots+w_{n_{i}} x_{n_{i}}+\mathrm{b}
\]<br>
其中\(n_i\)表示输入个数，根据概率统计知识有下面的方差公式：<br>
\[
\operatorname{Var}\left(w_{i} x_{i}\right)=E\left[w_{i}\right]^{2} \operatorname{Var}\left(x_{i}\right)+E\left[x_{i}\right]^{2} \operatorname{Var}\left(w_{i}\right)+\operatorname{Var}\left(w_{i}\right) \operatorname{Var}\left(x_{i}\right)
\]<br>
特别的，当假设输入和权重都是0均值时（目前有了BN之后，这一点也较容易满足），上式可以简化为：<br>
\[
\operatorname{Var}\left(w_{i} x_{i}\right)=\operatorname{Var}\left(w_{i}\right) \operatorname{Var}\left(x_{i}\right)
\]<br>
进一步假设输入x和权重w独立同分布，则有：<br>
\[
\operatorname{Var}(y)=n_{i} \operatorname{Var}\left(w_{i}\right) \operatorname{Var}\left(x_{i}\right)
\]<br>
于是，为了保证输入与输出方差一致，则应该有：<br>
\[
\operatorname{Var}\left(w_{i}\right)=\frac{1}{n_{i}}
\]<br>
对于一个多层的网络，某一层的方差可以用累积的形式表达：<br>
\[
\operatorname{Var}\left[z^{i}\right]=\operatorname{Var}[x] \prod_{i^{\prime}=0}^{i-1} n_{i^{\prime}} \operatorname{Var}\left[W^{i^{\prime}}\right]
\]<br>
反向传播计算梯度时同样具有类似的形式：<br>
\[
\operatorname{Var}\left[\frac{\partial \operatorname{Cost}}{\partial s^{i}}\right]=\operatorname{Var}\left[\frac{\partial \operatorname{Cost}}{\partial s^{d}}\right] \prod_{i^{\prime}=i}^{d} n_{i^{\prime}+1} \operatorname{Var}\left[W^{i^{\prime}}\right]
\]</p>
<p>综上，为了保证前向传播和反向传播时每一层的方差一致，应满足：<br>
\[
\begin{aligned}
\forall i, &amp; n_{i} \operatorname{Var}\left[W^{i}\right]=1 \\
\forall i, &amp; n_{i+1} \operatorname{Var}\left[W^{i}\right]=1
\end{aligned}
\]<br>
但是，实际当中输入与输出的个数往往不相等，于是为了均衡考量，最终权重方差应满足：<br>
\[
\forall i, \quad \operatorname{Var}\left[W^{i}\right]=\frac{2}{n_{i}+n_{i+1}}
\]<br>
根据概率统计知道 [a,b] 间的均匀分布的方差为：<br>
\[
\operatorname{Var}=\frac{(b-a)^{2}}{12}
\]</p>
<p>因此，Xavier初始化的实现就是下面的均匀分布：<br>
\[
W \sim U\left[-\frac{\sqrt{6}}{\sqrt{n_{j}+n_{j+1}}}, \frac{\sqrt{6}}{\sqrt{n_{j}+n_{j+1}}}\right]
\]</p>
</blockquote>
<h3 id="gradient-propagation-study">Gradient Propagation Study</h3>
<p>为了经验验证上述理论思想，作者使用两种不同的初始化方法在初始化时绘制了激活值，权重和反向传播梯度的归一化直方图。通过观测第 i 层网络的雅可比矩阵(\(J=\frac{\partial z^{i+1}}{\partial z^i}\))的奇异值：</p>
<p><img src="/2020/08/01/deeplearning/Xavier/image-20200802182707631.png" alt></p>
<p>上面的图使用的是标准方法初始化，下面的图使用的是作者提出的normalization initialzation 方法。</p>
<p>可以看出，使用标准的初始化方法激活值从外层向内层逐渐衰减，而使用 normalization initialzation 方法进行初始化各层之间的激活值几乎保持不变。</p>
<h2 id="back-propagated-gradients-during-learning">Back-propagated Gradients During Learning</h2>
<p>在训练时，标准初始化\(W_{i j} \sim U\left[-\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{n}}\right]\)之后，反向传播梯度的方差随着向下传播而变小。 然而使用论文提出的归一化初始化，看不到这种降低的反向传播梯度：</p>
<p><img src="/2020/08/01/deeplearning/Xavier/image-20200802183205197.png" alt></p>
<p>即使反向传播的梯度变小（标准初始化），权重梯度的方差大体上是恒定的：</p>
<p><img src="/2020/08/01/deeplearning/Xavier/image-20200802183552802.png" alt></p>
<p>可以看出即使是使用标准初始化方法的反向梯度也没有发生明显的衰减。</p>
<p>随着训练的迭代，特别是在标准初始化方面，梯度起始大体上相同，它们彼此分开（下层具有更大的梯度）。 请注意，在不同层次上具有非常不同的幅度的梯度可能会导致病态和较慢的训练。</p>
<p><img src="/2020/08/01/deeplearning/Xavier/image-20200802184026069.png" alt></p>
<p>最后，观察到softsign网络与正态化初始化的tanh网络有类似特性，可以通过比较两种情况下的激活变化来看出：</p>
<p><img src="/2020/08/01/deeplearning/Xavier/image-20200802184159590.png" alt></p>
<h1 id="error-curves-and-conlusions">Error Curves and Conlusions</h1>
<p>通过误差曲线来展示不同设置的优劣：</p>
<p><img src="/2020/08/01/deeplearning/Xavier/image-20200802184607043.png" alt></p>
<h1 id="conclusion">conclusion</h1>
<ol>
<li>
<p>使用传统的 sigmoid 或 tanh 激活函数和标准初始化方法效果并不好，它们收敛缓慢而且往往会陷入局部最优。</p>
</li>
<li>
<p>使用 softsign 激活函数看上去比 tanh 网络对初始化方法具有更好的鲁棒性。</p>
</li>
<li>
<p>对于 tanh 网络，使用 normalized initialization 初始化方法对提升性能很有用，可能是因为层与层之间的激活值和梯度值得以保持。</p>
</li>
<li>
<p>通过监测迭代过程中层与层之间的梯度值和激活值对于理解深层网络中的训练困难是一个有力的工具。</p>
</li>
<li>
<p>Sigmoid 激活函数在使用小随机值初始化时应该尽量避免使用（在0附近处于饱和状态）</p>
</li>
<li>
<p>保持层间转换，使得激活和渐变都能够很好地发挥作用似乎是有帮助的，并且允许消除纯监督深层网络与预先使用无监督学习训练的深层网络之间的差异的很大一部分。</p>
</li>
</ol>
<h1 id="reference">reference</h1>
<ol>
<li><a href="https://blog.csdn.net/qq_34784753/article/details/78668884" target="_blank" rel="noopener">【Deep Learning】笔记：Understanding the difficulty of training deep feedforward neural networks</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/78857752" target="_blank" rel="noopener">论文阅读-Xavier 初始化</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/深度学习</category>
      </categories>
      <tags>
        <tag>参数初始化</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>代码注释模板</title>
    <url>/2020/08/01/code_template/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/08/01/code_template/163100059ae74c12da8d.gif" alt></p>
<a id="more"></a>
<p>有些规范建义将(创建人、创建时间、修改人、修改时间、版权声明)元素写在文件头部，而对于协同开发同一文件，还是需要把这些元素加在各个方法里面，会更清晰明了。</p>
<h1 id="python">python</h1>
<h2 id="pycharm">pycharm</h2>
<h3 id="pei-zhi-zuo-zhe-bian-ma-ge-shi-deng-xin-xi">配置作者，编码格式等信息</h3>
<ol>
<li>
<blockquote>
<p>Pycharm  --&gt; Preferences  --&gt; Editor --&gt; File and Code Templates --&gt; Python Script</p>
</blockquote>
<p><img src="/2020/08/01/code_template/image-20200801163757876.png" alt></p>
</li>
<li>
<p>配置内容如下：</p>
<figure class="highlight vala"><table><tr><td class="code"><pre><span class="line"><span class="meta"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="meta"># @Time    : $&#123;DATE&#125; $&#123;TIME&#125;</span></span><br><span class="line"><span class="meta"># @Author  : jeffery</span></span><br><span class="line"><span class="meta"># @FileName: $&#123;NAME&#125;.py</span></span><br><span class="line"><span class="meta"># @website : www.jeffery.ink</span></span><br><span class="line"><span class="meta"># @github  : https://github.com/jeffery0628</span></span><br><span class="line"><span class="meta"># @Description:</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="zi-ding-yi-fang-fa-zhu-shi-mo-ban">自定义方法注释模板</h3>
<ol>
<li>
<p>Live Templates中设置路径如下：</p>
<blockquote>
<p>Pycharm  --&gt; Preferences  --&gt;  Editor --&gt;  Live Templates</p>
</blockquote>
<p><img src="/2020/08/01/code_template/image-20200801160622966.png" alt></p>
</li>
<li>
<p>进入 Live Templates 设置页面，点击右方加号，添加 Template Group，再添加 Live Template,在下方添加 Abbreviation（快捷键缩写），Desctiption （快捷键描述）</p>
<p><img src="/2020/08/01/code_template/image-20200801161302231.png" alt></p>
</li>
<li>
<p>在 Template text 中添加下方模板代码</p>
<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line">"""</span><br><span class="line">method description</span><br><span class="line"></span><br><span class="line">Args:</span><br><span class="line"><span class="code">    param (type): parameter description</span></span><br><span class="line"></span><br><span class="line">Returns:</span><br><span class="line"><span class="code">    return_v (rtype): rtype description</span></span><br><span class="line"></span><br><span class="line"><span class="meta">:Author:</span>  jeffery</span><br><span class="line"><span class="meta">:Create:</span>  $DATE$ $TIME$</span><br><span class="line">"""</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>在 Edit variables 中配置 DATE 和 TIME，使创建时间自动生成</p>
<p><img src="/2020/08/01/code_template/image-20200801161555685.png" alt></p>
</li>
<li>
<p>为了使用注释方便，还可添加 更新时间 和 当前时间</p>
<p><img src="/2020/08/01/code_template/image-20200801162011470.png" alt></p>
</li>
<li>
<p>使用：</p>
<p><img src="/2020/08/01/code_template/image-20200801163156950.png" alt></p>
</li>
<li>
<p>效果</p>
<p><img src="/2020/08/01/code_template/image-20200801163219278.png" alt></p>
</li>
</ol>
]]></content>
      <categories>
        <category>技术/工具</category>
      </categories>
  </entry>
  <entry>
    <title>focal loss</title>
    <url>/2020/08/01/deeplearning/loss/focal_loss/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/08/01/deeplearning/loss/focal_loss/image-20200801122615585.png" alt></p>
<a id="more"></a>
<h1 id="yin-ru">引入</h1>
<p>object detection的算法主要可以分为两大类：<strong>two-stage detector和one-stage detector</strong>。前者是指类似Faster RCNN，RFCN这类需要region proposal的检测算法，这类算法可以达到很高的准确率，但是速度较慢。虽然可以通过减少proposal的数量或降低输入图像的分辨率等方式达到提速，但是速度并没有质的提升。后者是指类似YOLO，SSD这样不需要region proposal，直接回归的检测算法，这类算法速度很快，但是准确率不如前者。<strong>作者提出focal loss的出发点也是希望one-stage detector可以达到two-stage detector的准确率，同时不影响原有的速度。</strong></p>
<p>既然有了出发点，<strong>那么就要找one-stage detector的准确率不如two-stage detector的原因，作者认为原因是：样本的类别不均衡导致的</strong>。因为在object detection领域，一张图像可能生成成千上万的candidate locations，但是其中只有很少一部分是包含object的，这就带来了类别不均衡。那么类别不均衡会带来什么后果呢？</p>
<blockquote>
<ol>
<li>
<p>training is inefficient as most locations are easy negatives that contribute no useful learning signal;</p>
</li>
<li>
<p>enmasse, the easy negatives can overwhelm training and lead to degenerate models.</p>
</li>
</ol>
</blockquote>
<p>也就是说：负样本数量太大，占总的loss的大部分，而且多是容易分类的，因此使得模型的优化方向并不是我们所希望的那样。针对类别不均衡问题，提出一种新的损失函数：focal loss，这个损失函数是在标准交叉熵损失基础上修改得到的。<strong>这个函数可以通过减少易分类样本的权重，使得模型在训练时更专注于难分类的样本</strong>。</p>
<p><code><a class="btn" href="focal_loss.pdf">
            <i class="fa fa-download"></i>focal loss论文下载
          </a></code></p>
<h1 id="yuan-li">原理</h1>
<h2 id="jiao-cha-shang">交叉熵</h2>
<p>在介绍focal loss之前，先来看看交叉熵损失，这里以二分类为例，<strong>原来的分类loss是各个训练样本交叉熵的直接求和，也就是各个样本的权重是一样的</strong>。公式如下：<br>
\[
\mathrm{CE}(p, y)=\left\{\begin{array}{ll}
-\log (p) &amp; \text { if } y=1 \\
-\log (1-p) &amp; \text { otherwise }
\end{array}\right. \tag{1}
\]<br>
因为是二分类，p表示预测样本属于1的概率（范围为0-1），y表示label，y的取值为{+1,-1}。这里仅仅以二分类为例，多分类分类以此类推。为了表示简便，用\(p_t\)表示样本属于true class的概率。所以(1)式可以写成:<br>
\[
\mathrm{CE}(p, y)=\operatorname{CE}\left(p_{\mathrm{t}}\right)=-\log \left(p_{\mathrm{t}}\right) \tag{2}
\]<br>
<strong>其中\(p_t\)表示被分为正/负例的概率</strong>，定义如下：<br>
\[
p_{\mathrm{t}}=\left\{\begin{array}{ll}
p &amp; \text { if } y=1 \\
1-p &amp; \text { otherwise }
\end{array}\right. \tag{3}
\]</p>
<h2 id="jun-heng-jiao-cha-shang-blanced-ce">均衡交叉熵 Blanced CE</h2>
<p>既然one-stage detector在训练的时候正负样本的数量差距很大，那么一种常见的做法就是给正负样本加上权重，负样本出现的频次多，那么就降低负样本的权重，正样本数量少，就相对提高正样本的权重。因此可以通过设定\(\alpha\)的值来控制正负样本对总的loss的共享权重。\(\alpha\)取比较小的值来降低负样本（多的那类样本）的权重。<br>
\[
\alpha_{t}=\left\{\begin{array}{ll}
\alpha &amp; \text { if } y=1 \\
1-\alpha &amp; \text { otherwise }
\end{array}\right. \tag{4}
\]</p>
<p>则，\(\alpha\)-balanced loss 为：<br>
\[
C E\left(p_{t}\right)=-\alpha_{t} \log \left(p_{t}\right)
\]</p>
<h2 id="focal-loss">Focal Loss</h2>
<p>虽然\(\alpha\)能够平衡 <code>positive/negative</code> 样本的重要性，但不能区分 <code>easy/had</code> 样本.对此，Focal Loss 提出将损失函数降低 <code>easy</code> 样本的权重，并关注于对 hard negatives 样本的训练.添加调制因子\((1-p_t)^{\gamma}\)到 CE loss，其中 \(\gamma \ge 0\) 为可调的 focusing 参数。Focal Loss 定义为：<br>
\[
F L\left(p_{t}\right)=-\left(1-p_{t}\right)^{\gamma} \log \left(p_{t}\right) \tag{5}
\]<br>
如下图,给出了\(\gamma \in [0,5]\)中几个值的可视化。</p>
<p><img src="/2020/08/01/deeplearning/loss/focal_loss/image-20200801122615585.png" alt></p>
<p>Focal Loss 的两个属性：</p>
<ol>
<li>当样本被误分，且 \(p_t\)值很小时，调制因子接近于 1，loss 不受影响。随着 \(p_t \rightarrow 1\)，则调制因子接近于 0，则容易分类的样本的损失函数被降低权重.</li>
<li>focusing 参数 \(\gamma\) 平滑地调整哪些 <code>easy</code> 样本会被降低权重的比率. 当 \(\gamma=0\)，FL=CE；随着 $\gamma $ 增加，调制因子的影响也会随之增加(实验中发现  效果\(\gamma=2\)最佳.)调制因子能够减少 easy 样本对于损失函数的贡献，并延伸了loss 值比较的样本范围。例如，\(\gamma=2\) 时，被分类为 \(p_t=0.9\) 的样本，与 CE 相比，会减少 100 倍；而当被分类为 $p_t \approx 0.968 $ 时，与 CE 相比，会有小于 1000 倍的 loss 值. 这就自然增加了将难分类样本的重要性(如 \(\gamma=2\) 且 \(p_t \le 0.5\) 时，难分类样本的 loss 值会增加 4 倍。)实际上，</li>
</ol>
<p><strong>focal loss的两个性质算是核心，其实就是用一个合适的函数去度量难分类和易分类样本对总的损失的贡献。</strong></p>
<p>论文采用了 Focal Loss 的  \(\alpha\)-balanced 变形：<br>
\[
F L\left(p_{t}\right)=-\alpha_{t}\left(1-p_{t}\right)^{\gamma} \log \left(p_{t}\right) \tag{6}
\]<br>
在实验中\(\alpha\)的选择范围也很广，一般而言当\(\gamma\)增加的时候，\(\alpha\)需要减小一点（实验中\(\gamma=2,\alpha=0.25\)的效果最好）</p>
<h1 id="dai-ma">代码</h1>
<figure class="highlight python"><figcaption><span>pytorch</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, alpha=<span class="number">0.25</span>, gamma=<span class="number">2</span>, size_average=True)</span>:</span></span><br><span class="line">        super(FocalLoss, self).__init__()</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.gamma = torch.Tensor([gamma])</span><br><span class="line">        self.size_average = size_average</span><br><span class="line">        <span class="keyword">if</span> isinstance(alpha, (float, int, long)):</span><br><span class="line">            <span class="keyword">if</span> self.alpha &gt; <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">'Not supported value, alpha should be small than 1.0'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.alpha = torch.Tensor([alpha, <span class="number">1.0</span> - alpha])</span><br><span class="line">        <span class="keyword">if</span> isinstance(alpha, list): self.alpha = torch.Tensor(alpha)</span><br><span class="line">        self.alpha /= torch.sum(self.alpha)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> input.dim() &gt; <span class="number">2</span>:</span><br><span class="line">            input = input.view(input.size(<span class="number">0</span>), input.size(<span class="number">1</span>), <span class="number">-1</span>)  <span class="comment"># [N,C,H,W]-&gt;[N,C,H*W] ([N,C,D,H,W]-&gt;[N,C,D*H*W])</span></span><br><span class="line">        <span class="comment"># target</span></span><br><span class="line">        <span class="comment"># [N,1,D,H,W] -&gt;[N*D*H*W,1]</span></span><br><span class="line">        <span class="keyword">if</span> self.alpha.device != input.device:</span><br><span class="line">            self.alpha = torch.tensor(self.alpha, device=input.device)</span><br><span class="line">        target = target.view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        logpt = torch.log(input + <span class="number">1e-10</span>)</span><br><span class="line">        logpt = logpt.gather(<span class="number">1</span>, target)</span><br><span class="line">        logpt = logpt.view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        pt = torch.exp(logpt)</span><br><span class="line">        alpha = self.alpha.gather(<span class="number">0</span>, target.view(<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">        gamma = self.gamma</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.gamma.device == input.device:</span><br><span class="line">            gamma = torch.tensor(self.gamma, device=input.device)</span><br><span class="line"></span><br><span class="line">        loss = <span class="number">-1</span> * alpha * torch.pow((<span class="number">1</span> - pt), gamma) * logpt</span><br><span class="line">        <span class="keyword">if</span> self.size_average:</span><br><span class="line">            loss = loss.mean()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss = loss.sum()</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>keras</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categorical_focal_loss</span><span class="params">(gamma=<span class="number">2.0</span>, alpha=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of Focal Loss from the paper in multiclass classification</span></span><br><span class="line"><span class="string">    Formula:</span></span><br><span class="line"><span class="string">        loss = -alpha*((1-p)^gamma)*log(p)</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        alpha -- the same as wighting factor in balanced cross entropy</span></span><br><span class="line"><span class="string">        gamma -- focusing parameter for modulating factor (1-p)</span></span><br><span class="line"><span class="string">    Default value:</span></span><br><span class="line"><span class="string">        gamma -- 2.0 as mentioned in the paper</span></span><br><span class="line"><span class="string">        alpha -- 0.25 as mentioned in the paper</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">focal_loss</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">        <span class="comment"># Define epsilon so that the backpropagation will not result in NaN</span></span><br><span class="line">        <span class="comment"># for 0 divisor case</span></span><br><span class="line">        epsilon = K.epsilon()</span><br><span class="line">        <span class="comment"># Add the epsilon to prediction value</span></span><br><span class="line">        <span class="comment">#y_pred = y_pred + epsilon</span></span><br><span class="line">        <span class="comment"># Clip the prediction value</span></span><br><span class="line">        y_pred = K.clip(y_pred, epsilon, <span class="number">1.0</span>-epsilon)</span><br><span class="line">        <span class="comment"># Calculate cross entropy</span></span><br><span class="line">        cross_entropy = -y_true*K.log(y_pred)</span><br><span class="line">        <span class="comment"># Calculate weight that consists of  modulating factor and weighting factor</span></span><br><span class="line">        weight = alpha * y_true * K.pow((<span class="number">1</span>-y_pred), gamma)</span><br><span class="line">        <span class="comment"># Calculate focal loss</span></span><br><span class="line">        loss = weight * cross_entropy</span><br><span class="line">        <span class="comment"># Sum the losses in mini_batch</span></span><br><span class="line">        loss = K.sum(loss, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> focal_loss</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_focal_loss</span><span class="params">(gamma=<span class="number">2.0</span>, alpha=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of Focal Loss from the paper in multiclass classification</span></span><br><span class="line"><span class="string">    Formula:</span></span><br><span class="line"><span class="string">        loss = -alpha_t*((1-p_t)^gamma)*log(p_t)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        p_t = y_pred, if y_true = 1</span></span><br><span class="line"><span class="string">        p_t = 1-y_pred, otherwise</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        alpha_t = alpha, if y_true=1</span></span><br><span class="line"><span class="string">        alpha_t = 1-alpha, otherwise</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        cross_entropy = -log(p_t)</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        alpha -- the same as wighting factor in balanced cross entropy</span></span><br><span class="line"><span class="string">        gamma -- focusing parameter for modulating factor (1-p)</span></span><br><span class="line"><span class="string">    Default value:</span></span><br><span class="line"><span class="string">        gamma -- 2.0 as mentioned in the paper</span></span><br><span class="line"><span class="string">        alpha -- 0.25 as mentioned in the paper</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">focal_loss</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">        <span class="comment"># Define epsilon so that the backpropagation will not result in NaN</span></span><br><span class="line">        <span class="comment"># for 0 divisor case</span></span><br><span class="line">        epsilon = K.epsilon()</span><br><span class="line">        <span class="comment"># Add the epsilon to prediction value</span></span><br><span class="line">        <span class="comment">#y_pred = y_pred + epsilon</span></span><br><span class="line">        <span class="comment"># Clip the prediciton value</span></span><br><span class="line">        y_pred = K.clip(y_pred, epsilon, <span class="number">1.0</span>-epsilon)</span><br><span class="line">        <span class="comment"># Calculate p_t</span></span><br><span class="line">        p_t = tf.where(K.equal(y_true, <span class="number">1</span>), y_pred, <span class="number">1</span>-y_pred)</span><br><span class="line">        <span class="comment"># Calculate alpha_t</span></span><br><span class="line">        alpha_factor = K.ones_like(y_true)*alpha</span><br><span class="line">        alpha_t = tf.where(K.equal(y_true, <span class="number">1</span>), alpha_factor, <span class="number">1</span>-alpha_factor)</span><br><span class="line">        <span class="comment"># Calculate cross entropy</span></span><br><span class="line">        cross_entropy = -K.log(p_t)</span><br><span class="line">        weight = alpha_t * K.pow((<span class="number">1</span>-p_t), gamma)</span><br><span class="line">        <span class="comment"># Calculate focal loss</span></span><br><span class="line">        loss = weight * cross_entropy</span><br><span class="line">        <span class="comment"># Sum the losses in mini_batch</span></span><br><span class="line">        loss = K.sum(loss, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> focal_loss    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=categorical_focal_loss(gamma=<span class="number">2.0</span>, alpha=<span class="number">0.25</span>), metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<h1 id="zong-jie">总结</h1>
<p>作者将类别不平衡作为阻碍one-stage方法超过top-performing的two-stage方法的主要原因。为了解决这个问题，作者提出了focal loss，在交叉熵里面用一个调整项，为了将学习专注于hard examples上面，并且降低大量的easy negatives的权值。简单高效啊！</p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://blog.csdn.net/u014380165/article/details/77019084" target="_blank" rel="noopener">Focal Loss</a></li>
<li><a href="https://www.aiuai.cn/aifarm636.html" target="_blank" rel="noopener">Focal Loss 论文理解及公式推导</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/深度学习</category>
      </categories>
      <tags>
        <tag>loss</tag>
      </tags>
  </entry>
  <entry>
    <title>switchable normalization</title>
    <url>/2020/08/01/deeplearning/normalization/switchable_normalization/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/08/01/deeplearning/normalization/switchable_normalization/1.jpg" alt></p>
<a id="more"></a>
<h1 id="yin-ru">引入</h1>
<p><code>switchable Normalization（SN）</code>统一了实例归一化<code><a href="/2020/07/31/deeplearning/normalization/instance_normalization_and_group_normalization/" title="instance normalization">instance normalization</a></code>，层归一化<code><a href="/2020/07/30/deeplearning/normalization/layer_normalization/" title="layer normalization">layer normalization</a></code>，和批归一化<code><a href="/2020/07/29/deeplearning/normalization/batch_normalization/" title="batch normalization">batch normalization</a></code>的各种操作。</p>
<h1 id="yuan-li">原理</h1>
<p>假设一个卷积神经网络的一个隐含卷积层的输入数据可表示为具有四个维度的特征图:<code>(N, C, H, W)</code>。这里每个维度分别代表样本数目<code>minibatch size</code>，通道数目<code>number of channels</code>，通道的高<code>height</code>，和通道的宽<code>width</code>。假设每一个像素表示为\(h_{ncij}\)，这里\(n,c,i,j\)为上述四个维度的下标。SN对\(h_{ncij}\)进行归一化操作，并输出归一化后的像素值\(\hat{h}_{ncij}\)。SN的计算公式如下：<br>
\[
\hat{h}_{n c i j}=\gamma \frac{h_{n c i j}-\Sigma_{k \in \Omega} w_{k} \mu_{k}}{\sqrt{\Sigma_{k \in \Omega} w_{k}^{\prime} \sigma_{k}^{2}+\epsilon}}+\beta \tag{1}
\]<br>
上述定义与<code>BN</code>，<code>IN</code>，和<code>LN</code>的定义相似。他们都学习了缩放系数\(\gamma\)和偏移系数\(\beta\)。主要的区别在于<code>SN</code>的统计信息（即均值\(\mu\)和方差\(\sigma^2\)），不像<code>IN</code>只是在一个通道中计算的，也不像<code>LN</code>只是在一个层中计算，而是在一个集合\(\Omega\)当中选择合适的归一化方法来加权平均的。这个集合定义为\(\Omega=\{\mathrm{bn},\mathrm{in},\mathrm{ln}\}\)。\(w_k\)和\(w_k^\prime\)则为相应统计量对应的权重系数。下图直观的展示了SN的基本原理:</p>
<p><img src="/2020/08/01/deeplearning/normalization/switchable_normalization/1.jpg" alt></p>
<p>上图为SN的直观图形解释，<code>SN</code>中每个样本每个通道（\(H\times W\)）的均值和方差，由<code>BN</code>,<code>IN</code>,<code>LN</code>三种不同统计方法计算得到的均值和方差共同决定。在<code>SN</code>中，均值的加权系数\(w_k\)的计算方法如下：<br>
\[
w_{k}=\frac{e^{\lambda_{k}}}{\Sigma_{z \in\{\mathrm{in}, \ln , \mathrm{bn}\}} e^{\lambda_{Z}}}, \quad k \in\{\mathrm{bn}, \mathrm{in}, \ln \} \tag{2}
\]<br>
\(\lambda_k\)为三个维度统计量对应的参数。为了与网络参数区分，这些参数称为控制参数。这些控制参数均初始为1，在反向传播时进行优化学习。该计算公式即利用softmax函数对优化参数\(λ_k\)进行归一化，计算统计量最终的加权系数\(w_k\)。因此，所有加权系数\(w_k\)的和为1，每个加权系数\(w_k\)的值都在0和1之间。类似的，\(w_k^\prime\)可以由另外的三个参数\(\lambda_\mathrm{bn}^\prime\)，\(\lambda_\mathrm{in}^\prime\)，和\(\lambda_\mathrm{ln}^\prime\)计算得出，且\(\Sigma_{k\in\Omega}w_k^\prime=1\)，\(\forall w_k^\prime\in[0,1]\)。因此，相对于BN，SN只额外增加了\(\lambda_\mathrm{bn}\)，\(\lambda_\mathrm{in}\)，\(\lambda_\mathrm{ln}\)和\(\lambda_\mathrm{bn}^\prime\)，\(\lambda_\mathrm{in}^\prime\)，\(\lambda_\mathrm{ln}^\prime\) 6个控制参数。</p>
<p>下面给出<code>bn,ln,in</code>均值和方差的计算公式<br>
\[
\begin{aligned}
\mu_{\mathrm{in}} &amp;=\frac{1}{H W} \sum_{i, j}^{H, W} h_{n c i j}, \sigma_{\mathrm{in}}^{2}=\frac{1}{H W} \sum_{i, j}^{H, W}\left(h_{n c i j}-\mu_{\mathrm{in}}\right)^{2} \\
\mu_{\mathrm{ln}} &amp;=\frac{1}{C} \sum_{c=1}^{C} \mu_{\mathrm{in}}, \sigma_{\ln }^{2}=\frac{1}{C} \sum_{c=1}^{C}\left(\sigma_{\mathrm{in}}^{2}+\mu_{\mathrm{in}}^{2}\right)-\mu_{\mathrm{ln}}^{2} \\
\mu_{\mathrm{bn}} &amp;=\frac{1}{N} \sum_{n=1}^{N} \mu_{\mathrm{in}}, \sigma_{\mathrm{bn}}^{2}=\frac{1}{N} \sum_{n=1}^{N}\left(\sigma_{\mathrm{in}}^{2}+\mu_{\mathrm{in}}^{2}\right)-\mu_{\mathrm{bn}}^{2}
\end{aligned} \tag{5}
\]</p>
<h1 id="sn-yu-minibatch">SN 与minibatch</h1>
<p><code>minibatch size</code>的变化对<code>BN</code>的影响最大，因为<code>BN</code>使用的均值和方差是在<code>minibatch</code>当中统计的。<code>mini batch</code>越小，这些统计量的估计会带有更大的噪声，对模型训练产生过大的规范化作用或称正则化作用，从而影响模型的泛化性能。而<code>IN</code>，<code>LN</code>和<code>GN</code>在计算统计量时虽然与minibatch无关，却由于缺乏正则化能力在大的<code>minibatch</code>时无法达到<code>BN</code>的精度（这些技术单独使用往往导致较明显的过拟合现象）。<code>SN</code>通过学习不同归一化方法的相互作用（权重系数），克服了上述问题。无论<code>minibatch</code>大还是小，<code>SN</code>都能自适配的学习出合适的归一化方式，保持高精度。总的来说：</p>
<blockquote>
<p>minibatch越小，SN中BN的权重系数越小，IN和LN的权重系数则越大；<br>
minibatch越大，SN中BN的权重系数越大，IN和LN的权重系数越小。</p>
</blockquote>
<p>下图为<code>SN</code>在不同<code>minibatch size</code>下自主选择<code>BN</code>，<code>IN</code>，<code>LN</code>的权重系数的展示。括号表示<code>GPU数目，每个GPU样本数即minibatch</code>。在训练中，梯度通过在所有GPU中平均进行估计，而所有归一化方法的统计量只在单独一个GPU当中计算。可以看出，随着<code>minibatch</code>的不断减小，<code>BN</code>的权重越来越低，<code>IN</code>和<code>LN</code>的权重越来越高。当<code>minibatch size</code>等于1时，<code>BN</code>的权重为零，因为此时在训练当中<code>BN</code>等效于<code>IN</code>。</p>
<p><img src="/2020/08/01/deeplearning/normalization/switchable_normalization/2.jpg" alt></p>
<h1 id="sn-diao-jie-you-bn-de-jun-zhi-he-fang-chai-suo-chan-sheng-de-zheng-ze-hua">SN调节由BN的均值和方差所产生的正则化</h1>
<p><code>BN</code>在计算统计量过程会引入随机噪声。这些随机噪声为模型带来正则化作用，该作用的强度与<code>minibatch size</code>成反比。直观的说，由<code>BN</code>的均值和方差分别产生的正则化对模型训练会产生不同的影响。具体来说，计算样本均值引入的噪声要弱于估计样本方差引入的噪声（噪声越大，正则化作用越强）。SN通过分别调节它们的权重，来增加或者减少模型的正则化作用。进一步地，SN的自主选择过程旨在抑制噪声。统计量带来的噪声越大抑制越厉害。可以理解为：</p>
<blockquote>
<p>minibatch较小时，BN中variance的权重会更小于mean的权重</p>
</blockquote>
<p>下图为<code>SN</code>在ResNet50不同的blocks中选择<code>BN</code>，<code>IN</code>和<code>LN</code>权重系数的展示。(a)中<code>minibatch size</code>为32，(b)中<code>minibatch size</code>为2。可以看出对于<code>BN</code>，其均值和方差的加权系数不一样。对比(a)和(b)我们发现，当<code>minibatch size</code>为32时，BN的均值和方差的具有较大的权重；当<code>batch size</code>为2时，BN中variance的权重会被降低，从而降低估计variance时引入的噪声的影响。</p>
<p><img src="/2020/08/01/deeplearning/normalization/switchable_normalization/3.jpg" alt></p>
<p><img src="/2020/08/01/deeplearning/normalization/switchable_normalization/4.jpg" alt></p>
<h1 id="sn-yu-bn-gn-de-fan-hua-xing-neng">SN与BN,GN的泛化性能</h1>
<p>基于<code>SN</code>的设计，可以发现SN不仅可以解决因<code>minibatch</code>太小而导致的<code>BN</code>中噪声太大的问题，并且通过引入其他的归一化方法提升了模型的性能。而且在<code>minibatch</code>较大时发挥出<code>BN</code>的优势，弥补其他归一化方法的缺陷。可以理解为：</p>
<blockquote>
<p>SN是一种覆盖特征图张量各个维度来计算统计信息的归一化方法，不依赖minibatch size的同时对各个维度统计有很好的鲁棒性。</p>
</blockquote>
<p>下图展示出<code>SN</code>，<code>BN</code>，<code>GN</code>在不同<code>minibatch size</code>下的性能，三者都使用ResNet50在ImageNet中训练。纵坐标为ImageNet验证集top-1准确率。可以看出，<code>BN</code>对<code>minibatch size</code>非常敏感，在<code>minibatch</code>较小时性能较差；<code>GN</code>的性能相对稳定，但在<code>minibatch</code>较大时性能不如<code>BN</code>；而<code>SN</code>不仅克服了<code>BN</code>对<code>batchsize</code>较为敏感的问题，在各个<code>minibatch size</code>下都能表现稳定，而且达到较优的模型性能。</p>
<p><img src="/2020/08/01/deeplearning/normalization/switchable_normalization/5.jpg" alt></p>
<h1 id="dai-ma">代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SwitchableNorm</span><span class="params">(x, gamma, beta, w_mean, w_var)</span>:</span></span><br><span class="line">    <span class="comment"># x_shape:[B, C, H, W]</span></span><br><span class="line">    results = <span class="number">0.</span></span><br><span class="line">    eps = <span class="number">1e-5</span></span><br><span class="line"></span><br><span class="line">    mean_in = np.mean(x, axis=(<span class="number">2</span>, <span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    var_in = np.var(x, axis=(<span class="number">2</span>, <span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    mean_ln = np.mean(x, axis=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    var_ln = np.var(x, axis=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    mean_bn = np.mean(x, axis=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    var_bn = np.var(x, axis=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    mean = w_mean[<span class="number">0</span>] * mean_in + w_mean[<span class="number">1</span>] * mean_ln + w_mean[<span class="number">2</span>] * mean_bn</span><br><span class="line">    var = w_var[<span class="number">0</span>] * var_in + w_var[<span class="number">1</span>] * var_ln + w_var[<span class="number">2</span>] * var_bn</span><br><span class="line"></span><br><span class="line">    x_normalized = (x - mean) / np.sqrt(var + eps)</span><br><span class="line">    results = gamma * x_normalized + beta</span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>
<h1 id="zong-jie">总结</h1>
<p>相比于其他归一化方法，SN有以下性质：</p>
<ul>
<li><strong>鲁棒性</strong> ：对mini-batch尺寸的不敏感使其精度在各种batch size设置下都保持稳定。特别是在batch size受限的视觉任务中相对有利，例如物体检测、实例分割、视频识别等。</li>
<li><strong>通用性</strong> ：以往不同的归一化方法依赖不同维度的统计信息，针对不同的视觉任务需要选择不同的网络结构。精细的手动设计和繁琐的实验验证使实际应用中的模型选择变得非常困难。SN适用于各种网络结构包括CNNs和RNNs，并能够解决多种视觉任务。</li>
<li><strong>多样性</strong> ：SN为神经网络不同归一化层选择不同操作，拓展了归一化技术的边界，具有重要意义。直观而言，现有的方法都是对整个网络的所有层保持相同的归一化操作。然而，神经网络学习得到的高层特征具有高层语义，而低层特征学习底层视觉信息。我们应该为处在神经网络不同位置、不同深度的归一化层选择不同的操作。问题在于一方面很难手动选择整个网络的不同归一化操作，另一方面通过实验选择单一的归一化操作不一定能达到最优性能。</li>
</ul>
<p>总的来说，<code>SN</code>是一种任务与数据驱动的归一化方法。它对各种归一化技术对训练所产生的影响进行激励或者抑制，从而取得最优性能。</p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://htmlpreview.github.io/?https://github.com/switchablenorms/Switchable-Normalization/blob/master/blog_cn/blog_cn.html" target="_blank" rel="noopener">深度剖析 | 可微分学习的自适配归一化 (Switchable Normalization)</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/深度学习</category>
      </categories>
      <tags>
        <tag>归一化方法</tag>
      </tags>
  </entry>
  <entry>
    <title>instance normalization 和 group normalization</title>
    <url>/2020/07/31/deeplearning/normalization/instance_normalization_and_group_normalization/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/07/31/deeplearning/normalization/instance_normalization_and_group_normalization/image-20200731215441080.png" alt></p>
<a id="more"></a>
<h1 id="instance-normalization">Instance Normalization</h1>
<p>对于图像风格迁移这类注重每个像素的任务来说，每个样本的像素点的信息都是非常重要的，对于像<code><a href="/2020/07/29/deeplearning/normalization/batch_normalization/" title="batch normalization">batch normalization</a></code>这种每个批量的所有样本都做归一化的算法来说就不太适用了，因为<code>BN</code>计算归一化统计量时考虑了一个批量中所有图片的内容，从而造成了每个样本独特细节的丢失。而对于<code><a href="/2020/07/30/deeplearning/normalization/layer_normalization/" title="layer normalization">layer normalization</a></code>这类需要考虑一个样本所有通道的算法来说可能忽略了不同通道的差异，也不太适用于图像风格迁移这类应用。<code>Instance Normalization(IN)</code>是一种更适合对单个像素有更高要求的场景的归一化算法。<code>IN</code>算法计算归一化统计量时考虑单个样本，单个通道的所有元素。</p>
<h2 id="in-he-bn">IN 和 BN</h2>
<p>关于BN的算法公式：<br>
\[
\mu_{i}=\frac{1}{H W T} \sum_{t=1}^{T} \sum_{l=1}^{W} \sum_{m=1}^{H} x_{t i l m} \quad \sigma_{i}^{2}=\frac{1}{H W T} \sum_{t=1}^{T} \sum_{l=1}^{W} \sum_{m=1}^{H}\left(x_{t i l m}-\mu_{i}\right)^{2} \quad y_{t i j k}=\frac{x_{t i j k}-\mu_{i}}{\sqrt{\sigma_{i}^{2}+\epsilon}} \tag{1}
\]<br>
而在<code>IN</code>中，由于<code>IN</code>在计算归一化统计量时并没有像<code>BN</code>那样跨样本、单通道，也没有像<code>LN</code>那样单样本、跨通道。它取单通道，单样本上的数据进行计算，如下图最右侧所示，所以对比<code>BN</code>的公式，它只需要去掉<code>batch</code>的维度求和即可：<br>
\[
\mu_{t i}=\frac{1}{H W} \sum_{l=1}^{W} \sum_{m=1}^{H} x_{t i l m} \quad \sigma_{t i}^{2}=\frac{1}{H W} \sum_{l=1}^{W} \sum_{m=1}^{H}\left(x_{t i l m}-\mu_{t i}\right)^{2} \quad y_{t i j k}=\frac{x_{t i j k}-\mu_{t i}}{\sqrt{\sigma_{t i}^{2}+\epsilon}} \tag{2}
\]</p>
<p><img src="/2020/07/31/deeplearning/normalization/instance_normalization_and_group_normalization/image-20200731222057465.png" alt></p>
<p>对于<code>BN</code>中的\(\beta\)和\(\gamma\)，<code>IN</code>中也存在这两个参数。</p>
<h2 id="dai-ma">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Instancenorm</span><span class="params">(x, gamma, beta)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># x_shape:[B, C, H, W]</span></span><br><span class="line">    results = <span class="number">0.</span></span><br><span class="line">    eps = <span class="number">1e-5</span></span><br><span class="line"></span><br><span class="line">    x_mean = np.mean(x, axis=(<span class="number">2</span>, <span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    x_var = np.var(x, axis=(<span class="number">2</span>, <span class="number">3</span>), keepdims=True0)</span><br><span class="line">    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)</span><br><span class="line">    results = gamma * x_normalized + beta</span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>
<h2 id="zong-jie">总结</h2>
<p><code>IN</code>是一个非常简单的算法，尤其适用于批量较小且单独考虑每个像素点的场景中，因为其计算归一化统计量时没有混合批量和通道之间的数据，对于这种场景下的应用，可以考虑使用IN。在图像这类应用中，每个通道上的值是比较大的，因此也能够取得比较合适的归一化统计量。但是有两个场景不建议使用<code>IN</code>:</p>
<ol>
<li><code>MLP</code>或者<code>RNN</code>中：因为在<code>MLP</code>或者<code>RNN</code>中，每个通道上只有一个数据，这时会自然不能使用<code>IN</code>；</li>
<li><code>Feature Map</code>比较小时：因为此时<code>IN</code>的采样数据非常少，得到的归一化统计量将不再具有代表性。</li>
</ol>
<h1 id="group-normalization">Group Normalization</h1>
<p><code>Group Normalization(GN)</code>是何恺明提出的一种归一化策略，它是介于<code><a href="/2020/07/29/deeplearning/normalization/batch_normalization/" title="batch normalization">batch normalization</a></code>和<code><a href="/2020/07/30/deeplearning/normalization/layer_normalization/" title="layer normalization">layer normalization</a></code>之间的一种折中方案，如下图最右，它通过将<strong>通道</strong>数据分成几组计算归一化统计量，因此<code>GN</code>也是和批量大小无关的算法，因此可以用在<code>batch size</code>比较小的环境中。作者在论文中指出<code>GN</code>要比<code>LN</code>和<code>IN</code>的效果要好。</p>
<p><img src="/2020/07/31/deeplearning/normalization/instance_normalization_and_group_normalization/image-20200731215441080.png" alt></p>
<h2 id="gn-suan-fa">GN算法</h2>
<p>和之前所有归一化算法相同，<code>GN</code>也是根据该层的输入数据计算均值和方差，然后使用这两个值更新输入数据：<br>
\[
\mu_{i}=\frac{1}{m} \sum_{k \in \mathcal{S} i} x_{k} \quad \sigma_{i}=\sqrt{\frac{1}{m} \sum_{k \in \mathcal{S}_{i}}\left(x_{k}-\mu_{i}\right)^{2}+\epsilon} \quad \hat{x}_{i}=\frac{1}{\sigma_{i}}\left(x_{i}-\mu_{i}\right) \tag{3}
\]<br>
对于归一化方法<code>(BN,LN,IN,GN)</code>均可以使用上面式子进行概括，区别它们的是\(S_i\)的计算方式的不同：</p>
<p>对于<code>BN</code>来说，它是取不同<code>batch</code>的同一个<code>channel</code>上的所有值：<br>
\[
\mathcal{S}_{i}=\left\{k \mid k_{C}=i_{C}\right\} \tag{4}
\]<br>
对于<code>LN</code>来说，则是从同一个<code>batch</code>的不同的<code>channel</code>上取所有的值：<br>
\[
\mathcal{S}_{i}=\left\{k \mid k_{N}=i_{N}\right\} \tag{5}
\]<br>
对于<code>IN</code>来说，它既不跨<code>batch</code>，也不跨<code>channel</code>:<br>
\[
\mathcal{S}_{i}=\left\{k \mid k_{N}=i_{N}, k_{C}=i_{C}\right\} \tag{6}
\]<br>
对于<code>GN</code>来说，<code>GN</code>将<code>Channel</code>分成若干组，只使用组内的数据计算均值和方差。通常组数<code>G</code>是一个超参数：<br>
\[
\mathcal{S}_{i}=\left\{k \mid k_{N}=i_{N},\left\lfloor\frac{k_{C}}{C / G}\right\rfloor=\left\lfloor\frac{i_{C}}{C / G}\right\rfloor\right\} \tag{7}
\]<br>
可以看出，当<code>GN</code>的组数为1时，此时<code>GN</code>和<code>LN</code>等价；当<code>GN</code>的组数为通道数时，<code>GN</code>和<code>IN</code>等价。<code>GN</code>和其它算法一样也可以添加参数\(\gamma\)和\(\beta\)来保证网络的容量。</p>
<h2 id="dai-ma-1">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GroupNorm</span><span class="params">(x, gamma, beta, G=<span class="number">16</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># x_shape:[B, C, H, W]</span></span><br><span class="line">    results = <span class="number">0.</span></span><br><span class="line">    eps = <span class="number">1e-5</span></span><br><span class="line">    x = np.reshape(x, (x.shape[<span class="number">0</span>], G, x.shape[<span class="number">1</span>]/<span class="number">16</span>, x.shape[<span class="number">2</span>], x.shape[<span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line">    x_mean = np.mean(x, axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    x_var = np.var(x, axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), keepdims=True0)</span><br><span class="line">    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)</span><br><span class="line">    results = gamma * x_normalized + beta</span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>
<h2 id="zong-jie-1">总结</h2>
<p><code>GN</code>比<code>LN</code>效果好的原因是<code>GN</code>比<code>LN</code>的限制更少，因为<code>LN</code>假设了一个层的所有通道的数据共享一个均值和方差。而<code>IN</code>则丢失了探索通道之间依赖性的能力。</p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/56613508" target="_blank" rel="noopener">模型优化之Group Normalization</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/56542480" target="_blank" rel="noopener">模型优化之Instance Normalization</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>weight normalization</title>
    <url>/2020/07/30/deeplearning/normalization/weight_normalization/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/07/30/deeplearning/normalization/weight_normalization/v2-67e63301b77923897960fb4b50a84ed9_1440w.jpg" alt></p>
<a id="more"></a>
<h1 id="yin-ru">引入</h1>
<p><code><a href="/2020/07/29/deeplearning/normalization/batch_normalization/" title="batch normalization">batch normalization</a></code>介绍了<code>BN</code>的相关知识，<code><a href="/2020/07/30/deeplearning/normalization/layer_normalization/" title="layer normalization">layer normalization</a></code>介绍了<code>LN</code>的相关知识。而本文介绍的<code>Weight Normalization(WN)</code>是在权值的维度上做归一化，<code>WN</code>的做法是将权值向量<code>w</code>在欧式范数和其方向上解耦成了参数向量\(\vec{v}\)和参数标量\(g\)后使用<code>SGD</code>来分别优化这两个参数。</p>
<p><code>WN</code>也是和样本无关的，所以可以应用在<code>batch size</code>较小以及RNN动态网络中；另外<code>BN</code>使用的基于<code>mini-batch</code>的归一化统计量代替全局统计量，相当于在梯度计算中引入了噪声。而<code>WN</code>则没有这个问题，所以在生成模型，强化学习等噪声敏感的环境中<code>WN</code>的效果要优于<code>BN</code>。此外<code>WN</code>没有额外参数，这样更节约显存。同时<code>WN</code>的计算效率也要优于要计算归一化统计量的<code>BN</code>。</p>
<p><code><a class="btn" href="weight_normalization.pdf">
            <i class="fa fa-download"></i>weight normalization 下载
          </a></code></p>
<h1 id="weight-normalization-de-ji-suan">Weight Normalization的计算</h1>
<p>神经网络的一个节点计算可以表示为：<br>
\[
y=\phi(\mathbf{w} \cdot \mathbf{x}+b) \tag{1}
\]<br>
其中\(w\)是一个\(k\)为的向量，\(y\)是该神经节点的输出，是一个标量。在得到损失后，会根据损失函数的值使用SGD等优化策略更新\(w\)和\(b\)。<code>WN</code>提出的归一化策略是将\(w\)分解为一个参数向量\(v\)和一个参数标量\(g\)，分解方法为：<br>
\[
w = \frac{g}{\|v\|}v \tag{2}
\]<br>
\(\|v\|\)表示\(v\)的欧式范数。当\(v = w\) 且\(g = \|w\|\)时，<code>WN</code>还原为普通的计算方法，所以<code>WN</code>的网络容量要大于普通神经网络。</p>
<p><img src="/2020/07/30/deeplearning/normalization/weight_normalization/qG2TNNu9Rq-compress.jpg" alt></p>
<p>当\(g\)固定为\(\|w\|\)时，只优化\(v\)，这时候相当于只优化\(w\)的方向而保留其范数。当\(v\)固定为\(w\)时，这时相当于只优化\(w\)的范数，而保留其方向，这样为我们优化权值提供了更多可以选择的空间，且解耦方向与范数的策略也能加速其收敛。</p>
<p>在优化\(g\)时，一般通过优化\(g\)的log级参数\(s\)来完成，即\(g = e^s\)。</p>
<p>\(v\)和\(g\)的更新值可以通过SGD计算得到：<br>
\[
\nabla_{g} L=\frac{\nabla_{\mathbf{w}} L \cdot \mathbf{v}}{\|\mathbf{v}\|} \quad \nabla_{\mathbf{v}} L=\frac{g}{\|\mathbf{v}\|} \nabla_{\mathbf{w}} L-\frac{g \nabla_{g} L}{\|\mathbf{v}\|^{2}} \mathbf{v} \tag{3}
\]<br>
其中，\(L\)为损失函数，$\nabla_{w} L $为 \(w\) 在 \(L\) 下的梯度值。从上面的计算公式可以看出<code>WN</code>没有引入新的参数。</p>
<h1 id="wn-yuan-li">WN原理</h1>
<p>向量\(v\)的梯度更新公式可以写作：<br>
\[
\nabla_{\mathbf{v}} L=\frac{g}{\|\mathbf{v}\|} M_{\mathbf{w}} \nabla_{\mathbf{w}} L \quad \text { with } \quad M_{\mathbf{w}}=I-\frac{\mathbf{w} \mathbf{w}^{\prime}}{\|\mathbf{w}\|^{2}} \tag{4}
\]<br>
其推导过程：<br>
\[
\begin{array}{c}
\nabla_{\mathbf{v}} L &amp;=\frac{g}{\|\mathbf{v}\|} \nabla_{\mathbf{w}} L-\frac{g \nabla_{g} L}{\|\mathbf{v}\|^{2}} \mathbf{v} \\
&amp; =\frac{g}{\|\mathbf{v}\|} \nabla_{\mathbf{w}} L-\frac{g}{\|\mathbf{v}\|^{2}} \frac{\nabla_{\mathbf{w}} L \cdot \mathbf{v}}{\|\mathbf{v}\|} \mathbf{v} \\
&amp; =\frac{g}{\|\mathbf{v}\|}\left(I-\frac{\mathbf{v} \mathbf{v}^{\prime}}{\|\mathbf{v}\|^{2}}\right) \nabla_{\mathbf{w}} L \\
&amp; =\frac{g}{\|\mathbf{v}\|}\left(I-\frac{\mathbf{w} \mathbf{w}^{\prime}}{\|\mathbf{w}\|^{2}}\right) \nabla_{\mathbf{w}} L \\
&amp; =\frac{g}{\|\mathbf{v}\|} M_{\mathbf{w}} \nabla_{\mathbf{w}} L
\end{array} \tag{5}
\]<br>
倒数第二步的推导是因为\(v\)是\(w\)的方向向量。上述公式反映了<code>WN</code>两个重要特征：</p>
<ol>
<li>\(\frac{g}{\|v\|}\)表明<code>WN</code>会对权值梯度进行\(\frac{g}{\|v\|}\)的缩放</li>
<li>\(M_{\mathbf{w}} \nabla_{\mathbf{w}} L\)表明<code>WN</code>会将梯度投影到一个远离\(\nabla_w L\)的方向。</li>
</ol>
<p>这两个特征都会加速模型的收敛。主要原因在于：</p>
<ol>
<li>\(w\)垂直于\(M_w\),所以\(\nabla_v L\)非常接近于垂直参数方向\(w\)，这样对于矫正梯度更新方向非常有效。</li>
<li>\(v\)和梯度更新值中的噪声量成正比，而\(v\)是和更新量成反比，所以当更新值中噪声较多时，更新值会变小，这说明<code>WN</code>，这说明<code>WN</code>有自稳定的作用。这个特点使得我们在<code>WN</code>的使用中可以用较大的学习率。</li>
</ol>
<blockquote>
<p>另一个角度理解：从新权值的协方差矩阵出发，假设\(w\)的协方差矩阵时\(C\),那么\(\nabla_v L\)的协方差矩阵\(\mathbf{D}=\left(g^{2} /\|\mathbf{v}\|^{2}\right) M_{\mathbf{w}} \mathbf{C} M_{\mathbf{w}}\),当去掉\(D\)中的特征值后可以发现新的\(D\)非常趋近于一个单位矩阵，这说明\(w\)是\(C\)的主特征向量，说明<code>WN</code>有助于提升收敛速度。</p>
</blockquote>
<h1 id="bn-he-wn-de-guan-xi">BN和WN的关系</h1>
<p>假设\(t=vx\),\(\mu[t]\)和\(\sigma[t]\)分别是\(t\)的均值和方差，<code>BN</code>可以表示为：<br>
\[
t^{\prime}=\frac{t-\mu[t]}{\sigma[t]}=\frac{\mathbf{v}}{\sigma[t]} \mathbf{x}-\frac{\mu[t]}{\sigma[t]} \tag{6}
\]<br>
当网络只有一层且输入样本服从均值为0，方差为1的独立分布时，有\(\mu[t]=0\)且\(\sigma[t]==\|v\|\),此时<code>WN</code>和<code>BN</code>等价。</p>
<h1 id="wn-de-can-shu-chu-shi-hua">WN的参数初始化</h1>
<p>由于WN不像BN有规范化特征尺度的作用，所以WN的初始化需要慎重。作者建议的初始化策略是：</p>
<ol>
<li>\(v\)使用均值为0，标准差为0.05的正态分布进行初始化</li>
<li>\(g\)和偏置\(b\)使用第一批训练样本的统计量进行初始化：\(g \leftarrow \frac{1}{\sigma[t]} \quad b \leftarrow \frac{-\mu[t]}{\sigma[t]}\)，由于使用了样本进行初始化，所以这种初始化不适用于RNN等动态网络。</li>
</ol>
<h1 id="zong-jie">总结</h1>
<p>WN的归一化操作作用在了权值矩阵之上。从其计算方法上来看，WN完全不像是一个归一化方法，因为它并没有对得到的特征范围进行约束的功能，更像是基于矩阵分解的一种优化策略，它带来了四点好处：</p>
<ol>
<li>更快的收敛速度；</li>
<li>更强的学习率鲁棒性；</li>
<li>可以应用在RNN等动态网络中；</li>
<li>对噪声更不敏感，更适用在GAN，RL等场景中。</li>
</ol>
<p><code>WN</code>依旧对参数的初始值非常敏感，这也是<code>WN</code>一个比较严重的问题。</p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/55102378" target="_blank" rel="noopener">模型优化之Weight Normalization</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/深度学习</category>
      </categories>
      <tags>
        <tag>归一化方法</tag>
      </tags>
  </entry>
  <entry>
    <title>layer normalization</title>
    <url>/2020/07/30/deeplearning/normalization/layer_normalization/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/07/30/deeplearning/normalization/layer_normalization/JOsO217B1z-compress.jpg" alt></p>
<a id="more"></a>
<h1 id="yin-ru">引入</h1>
<p><code><a href="/2020/07/29/deeplearning/normalization/batch_normalization/" title="batch normalization">batch normalization</a></code>描述了batch normalization的相关知识,下面介绍layer normalization的相关知识。</p>
<h2 id="bn-cun-zai-de-liang-ge-wen-ti">BN 存在的两个问题</h2>
<p><code>Batch Normalization</code>存在两个问题：<code>batchsize</code>较小的时候效果不好、不适用于RNN等动态网络。</p>
<h3 id="bn-yu-batch-size">BN与Batch Size</h3>
<p>BN是按照样本数计算归一化统计量的，当样本数很少时，比如说只有4个。这四个样本的均值和方差不能反映全局的统计分布息，所以基于少量样本的BN的效果会变得很差。在一些场景中，比如说硬件资源受限，在线学习等场景，BN是非常不适用的。</p>
<h3 id="bn-yu-rnn">BN与RNN</h3>
<p>RNN可以展开成一个隐藏层共享参数的MLP，随着时间片的增多，展开后的MLP的层数也在增多，最终层数由输入数据的时间片的数量决定，所以RNN是一个动态的网络。在一个batch中，通常各个样本的长度都是不同的，当统计到比较靠后的时间片时，例如下图<code>t&gt;4(z轴为时间轴)</code>时，这时只有一个样本还有数据，基于这个样本的统计信息不能反映全局分布，所以这时BN的效果并不好。另外如果在测试时遇到了长度大于任何一个训练样本的测试样本，则无法找到保存的归一化统计量，所以BN无法运行。</p>
<p><img src="/2020/07/30/deeplearning/normalization/layer_normalization/FCWvMkyG1p-compress.jpg" alt></p>
<p><code>Layer Normalization(LN)</code>的提出有效的解决<code>BN</code>的这两个问题。LN和BN不同点是归一化的维度是互相垂直的，如下图所示：</p>
<p><img src="/2020/07/30/deeplearning/normalization/layer_normalization/JOsO217B1z-compress.jpg" alt></p>
<p>在图中 <code>y轴对应数据的样本N，x轴对应数据的通道数C，z轴对应数据的特征维度</code>，BN如右侧所示，它是取不同样本的同一个通道的特征做归一化；LN则是如左侧所示，它取的是同一个样本的不同通道做归一化。</p>
<p><code><a class="btn" href="Layer Normalization.pdf">
            <i class="fa fa-download"></i>layer normalization 下载
          </a></code></p>
<h1 id="layer-normalization">Layer Normalization</h1>
<h2 id="mlp-zhong-de-ln">MLP中的LN</h2>
<p><code>BN</code>的两个缺点的产生原因均是因为计算归一化统计量时计算的样本数太少。<code>LN</code>是一个独立于<code>batch size</code>的算法，所以无论样本数多少都不会影响参与<code>LN</code>计算的数据量，从而解决<code>BN</code>的两个问题。<code>LN</code>的做法上图左侧所示：根据样本的特征数做归一化。</p>
<p>先看<code>MLP</code>中的<code>LN</code>。设\(H\)是一层中隐层节点的数量，\(l\) 是<code>MLP</code>的层数，可以计算<code>LN</code>的归一化统计量\(\mu\) 和 \(\sigma\) ：</p>
<p>\[
\mu^{l}=\frac{1}{H} \sum_{i=1}^{H} a_{i}^{l} \quad \sigma^{l}=\sqrt{\frac{1}{H} \sum_{i=1}^{H}\left(a_{i}^{l}-\mu^{l}\right)^{2}}  \tag{1}
\]<br>
上面统计量的计算是和样本数量没有关系的，它的数量只取决于隐层节点的数量，所以只要隐层节点的数量足够多，就能保证LN的归一化统计量足够具有代表性。通过\(\mu^l\)和\(\sigma^l\)可以得到归一化后的\(\hat{a}^l\):<br>
\[
\hat{\mathbf{a}}^{l}=\frac{\mathbf{a}^{l}-\mu^{l}}{\sqrt{\left(\sigma^{l}\right)^{2}+\epsilon}} \tag{2}
\]<br>
其中\(\epsilon\)是一个很小的数，用于防止除0.</p>
<p>与<code>BN</code>类似，在<code>LN</code>中也需要一组参数来保证归一化操作不会破坏之前的信息，在<code>LN</code>中这组参数叫做增益<code>(gain)</code>\(g\)和偏置<code>(bias)</code>\(b\)（等同于<code>BN</code>中的\(\gamma\)和\(\beta\)）。假设激活函数为\(f\),则最终<code>LN</code>的输出为：<br>
\[
\mathbf{h}^{l}=f\left(\mathbf{g}^{l} \odot \hat{\mathbf{a}}^{l}+\mathbf{b}^{l}\right) \tag{3}
\]<br>
合并公式<code>(2),(3)</code>，忽略参数\(l\)，有：<br>
\[
\mathbf{h}=f\left(\frac{\mathbf{g}}{\sqrt{\sigma^{2}+\epsilon}} \odot(\mathbf{a}-\mu)+\mathbf{b}\right) \tag{4}
\]</p>
<h2 id="rnn-zhong-de-ln">RNN中的LN</h2>
<p>在RNN中，可以非常简单的在每个时间片中使用<code>LN</code>,而且在任何时间片都能保证归一化统计量统计的是\(H\)个节点的信息。对于RNN时刻\(t\)的节点，其输入是\(t-1\)时刻的隐层状态\(h^{t-1}\)和\(t\)时刻的输入数据\(x_t\),可以表示为：<br>
\[
\mathbf{a}^{t}=W_{h h} h^{t-1}+W_{x h} \mathbf{x}^{t} \tag{5}
\]<br>
接着便可以在\(a^t\)上采取和在MLP中完全相同的归一化过程：<br>
\[
\mu^{t}=\frac{1}{H} \sum_{i=1}^{H} a_{i}^{t}  \quad 
\sigma^{t}=\sqrt{\frac{1}{H} \sum_{i=1}^{H}\left(a_{i}^{t}-\mu^{t}\right)^{2}} \\
\mathbf{h}^{t}=f\left(\frac{\mathbf{g}}{\sqrt{\left(\sigma^{t}\right)^{2}+\epsilon}} \odot\left(\mathbf{a}^{t}-\mu^{t}\right)+\mathbf{b}\right) 
\]</p>
<h1 id="shi-yan-dui-bi">实验对比</h1>
<h2 id="zai-mlp-shang-gui-yi-hua">在MLP上归一化</h2>
<p><img src="/2020/07/30/deeplearning/normalization/layer_normalization/image-20200730211835016.png" alt></p>
<p>上图为<code>batch size = 128</code>时的结果，从中可以看出<code>BN</code>和<code>LN</code>均能够取得加速收敛的效果，并且<code>BN</code>小于要优于<code>LN</code>。下图是<code>batch size = 8</code>时的收敛曲线，这时<code>BN</code>反而会减慢收敛速度，验证了一开始的结论，对比之下<code>LN</code>要轻微优于无归一化的网络，说明了<code>LN</code>在小批量上的有效性。</p>
<p><img src="/2020/07/30/deeplearning/normalization/layer_normalization/image-20200730212154075.png" alt></p>
<h2 id="zai-lstm-shang-gui-yi-hua">在lstm上归一化</h2>
<p><img src="/2020/07/30/deeplearning/normalization/layer_normalization/image-20200730212604384.png" alt></p>
<p>从实验结果可以看出<code>LN</code>对于RNN系列动态网络的收敛加速上的效果是略有帮助的。<code>LN</code>的优点主要体现在两个方面：</p>
<ol>
<li><code>LN</code>使得模型更加稳定。</li>
<li><code>LN</code>有正则化的作用，得到的模型不容易过拟合。</li>
</ol>
<h2 id="cnn-shang-de-gui-yi-hua">CNN上的归一化</h2>
<p>将<code>LN</code>添加到CNN之后，实验结果发现<code>LN</code>破坏了卷积学习到的特征，模型无法收敛，所以在CNN之后使用<code>BN</code>是一个更好的选择。</p>
<h1 id="dai-ma">代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ln</span><span class="params">(x, b, s)</span>:</span></span><br><span class="line">  “”“</span><br><span class="line">    用于三维数据</span><br><span class="line">    ”“”</span><br><span class="line">    _eps = <span class="number">1e-5</span></span><br><span class="line">    output = (x - x.mean(<span class="number">1</span>)[:,<span class="literal">None</span>]) / tensor.sqrt((x.var(<span class="number">1</span>)[:,<span class="literal">None</span>] + _eps))</span><br><span class="line">    output = s[<span class="literal">None</span>, :] * output + b[<span class="literal">None</span>,:]</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Layernorm</span><span class="params">(x, gamma, beta)</span>:</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># x_shape:[B, C, H, W]</span></span><br><span class="line">    results = <span class="number">0.</span></span><br><span class="line">    eps = <span class="number">1e-5</span></span><br><span class="line"></span><br><span class="line">    x_mean = np.mean(x, axis=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    x_var = np.var(x, axis=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), keepdims=True0)</span><br><span class="line">    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)</span><br><span class="line">    results = gamma * x_normalized + beta</span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>
<h1 id="zong-jie">总结</h1>
<p>LN是和BN非常近似的一种归一化方法，不同的是<code>BN</code>取的是不同样本的同一个特征，而<code>LN</code>取的是同一个样本的不同特征。在<code>BN</code>和<code>LN</code>都能使用的场景中，<code>BN</code>的效果一般优于<code>LN</code>，原因是基于不同数据，同一特征得到的归一化特征更不容易损失信息。</p>
<p>但是有些场景是不能使用BN的，如<code>batch size</code>较小或者在RNN中，这时候可以选择使用<code>LN</code>，<code>LN</code>得到的模型更稳定且起到正则化的作用。<code>LN</code>能应用到小批量和RNN中是因为<code>LN</code>的归一化统计量的计算是和<code>batch size</code>没有关系的。</p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/54530247" target="_blank" rel="noopener">模型优化之Layer Normalization</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/深度学习</category>
      </categories>
      <tags>
        <tag>归一化方法</tag>
      </tags>
  </entry>
  <entry>
    <title>longformer</title>
    <url>/2020/07/29/transformers/longformer/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/07/29/transformers/longformer/image-20200729201315435.png" alt></p>
<a id="more"></a>
<p><code><a class="btn" href="longformer.pdf">
            <i class="fa fa-download"></i>longformer.pdf
          </a></code></p>
<h1 id="motivation">motivation</h1>
<p>Transforme成功的部分原因在于自注意力机制，自注意力机制使网络能够从整个序列中捕获上下文信息。但是虽然自注意力机制很有效，但它所需的内存和算力会随着序列长度呈平方增长，这使得当前硬件在处理长序列的情况下不可行，或者说非常昂贵、代价很大。</p>
<p>常用来解决长文本依赖的方法多是将上下文缩短或者划分成为较小的序列，以限制这些序列在512的长度以内。但是这种划分可能导致重要的信息丢失。</p>
<h1 id="model">model</h1>
<p>注意力计算方法时间和空间复杂度都是\(O(n^2)\)，这使得难以在长序列上进行训练。如下图所示</p>
<p><img src="/2020/07/29/transformers/longformer/image-20200729203303853.png" alt></p>
<p>self attention 所使用的空间成本会随着序列长度，成平方式的增加。为了解决这个问题，作者根据一个<code>attention patter</code>来稀疏完整的自注意力矩阵</p>
<p><img src="/2020/07/29/transformers/longformer/image-20200729201315435.png" alt></p>
<h2 id="sliding-window">sliding window</h2>
<p>滑动窗口attention，顾名思义，就是围绕每一个token采用固定大小的窗口计算局部注意力。</p>
<blockquote>
<p>对于每一个token，只对其附近的w个token计算attention计算复杂度与文本序列长度成线性关系，为\(O(w * n)\)。</p>
</blockquote>
<p><img src="/2020/07/29/transformers/longformer/image-20200729203937394.png" alt></p>
<p>设<code>window size = w，sequence length = n</code>，那么 <code>computation comlexity = n * w</code>.</p>
<blockquote>
<p>为了提高计算效率，<code>w</code>应该是比<code>n</code>小的值</p>
</blockquote>
<p>transformer无论编码还是解码器都是由l层堆叠形成的。作者认为，根据应用程序的不同，为每一层使用不同的w值对平衡效率和模型的表示能力可能会有所帮助。</p>
<h2 id="dilated-sliding-window">Dilated Sliding Window</h2>
<p><img src="/2020/07/29/transformers/longformer/image-20200729204452610.png" alt></p>
<p>设<code>window size = w, gaps of size dilation = d, transformer layer = n</code>，那么感受野大小为 <code>receptive iled = w * d * n</code>.</p>
<p>在<code>mutilhead attention</code>中，作者设置允许一些没有dilated的head专注于局部上下文，而其他具有膨胀的head专注于较长的上下文，在不增加计算负荷的前提下，拓宽模型“视野”，最终发现这样的做法能提升整体的表现。膨胀滑窗方法参考的CNN里的空洞卷积。</p>
<p><img src="/2020/07/29/transformers/longformer/v2-4959201e816888c6648f2e78cccfd253_b.gif" alt></p>
<h2 id="global-attention">Global Attention</h2>
<p>对于一些任务，比如说QA、文本分类，需要利用全局信息，因此局部注意力在这种情况下就不适用。</p>
<p><img src="/2020/07/29/transformers/longformer/640.png" alt></p>
<p>因此，作者就在一些预先选择的位置上(比如上图中红色点)添加了全局注意力。在这些位置上，会对整个序列做attention。位置的选择要根据具体任务决定。比如在分类任务上，这个global attention就落在[CLS]标签上，在QA任务上，就在整个问句上计算global attention。可以看出，global attention是视具体任务而定的，换个任务可能之前的做法就不适用了，但作者认为，这仍然比现有的trunk或者shorten的做法要简单很多。</p>
<h1 id="attention-pattern">attention pattern</h1>
<p>底层使用较小的滑窗，以建模局部信息；在高层使用较大的滑窗，以扩大感受野。</p>
<h1 id="experiment">Experiment</h1>
<p><strong>数据集</strong>:text8和enwik8</p>
<p><strong>训练</strong>：从理想状况来讲，训练时应该选用gpu所能承受的最大的<code>window size</code>和<code>seq len</code>。但是作者发现，在学习更长的上下文之前，模型需要大量的梯度更新才能首先学好局部的上下文。因此这里采用一种阶段式的训练方式：在学习更长的上下文之前，先学好局部的上下文。在第一阶段，从较短的序列长度和窗口大小开始，然后在每个后续阶段，将窗口大小和序列长度增加一倍，并将学习率减半。这样可以快速训练，同时将最慢的部分放到最后。</p>
<p>作者这里一共训练了5个阶段，第一个阶段<code>seq len</code>是2048，最后一个阶段是23040.</p>
<p><strong>评估</strong>:作者将数据集分为大小为32,256的重叠序列，步长为512，并记录序列上最后512个token的性能。</p>
<h2 id="results">results</h2>
<p>作者使用了两种不同的模型大小，小的有12层，大的有30层。</p>
<blockquote>
<p>small model : 12 layers, 512 hidden units</p>
<p>large model : 30 layers, 512 hidden units</p>
</blockquote>
<img src="/2020/07/29/transformers/longformer/image-20200729210540250.png" style="zoom: 67%;">
<p>在小的模型上，long former比其他模型都要好。这证明了这个模型的有效性。</p>
<p>在大的模型上，比18层的transformerxl要好，跟第二个和第三个齐平，不如第四、五两个。但这两个模型并不适用于pretrain-finetune的模式。</p>
<p><strong>对于不同的窗口大小及设置方式</strong>：</p>
<p><img src="/2020/07/29/transformers/longformer/image-20200729210752242.png" alt></p>
<p>表格的上半部分展示了，递增窗口大小的表现最好，递减窗口大小表现最差，使用固定窗口的表现介于两者之间。</p>
<p>表格的下半部分展示了使不使用空洞窗口的区别，可以看出对两个head增加一些空洞比完全不使用空洞窗口表现要好一些。</p>
<h2 id="pretraining-and-finetuning">Pretraining and Finetuning</h2>
<p>预训练使用的同样是MLM，并且是在RoBERTa的checkpoint后继续训练。在所有layer上使用<code>window size = 512</code>的滑动窗口attention，这么设置是为了与RoBERTa的<code>seq len</code>相匹配，暂不添加global attention。为了支持长文本，作者添加了额外的position embedding到4096的大小，此外，为了利用RoBERTa的权重，作者通过多次复制其512个位置嵌入来对其进行初始化。下表展示了预训练使用的数据集:</p>
<p><img src="/2020/07/29/transformers/longformer/image-20200729211216901.png" alt></p>
<p>上面两个是RoBERTa预训练同样用到的，下面两个是新增的，并且各选了三分之一用作预训练。</p>
<p><img src="/2020/07/29/transformers/longformer/image-20200729211339680.png" alt></p>
<p>第一行在<code>RoBERTa base</code>上的1.846与<code>RoBERTa</code>论文中的1.880相当，这表明训练语料与训练<code>RoBERTa</code>的语料有相似的分布。</p>
<p>第二行展示了<code>longformer</code>在预训练之前使用随机初始化的<code>position embedding</code>的表现。下一行是在预训练之前使用从<code>RoBERTa</code>中拷贝的<code>position embedding</code>的表现。</p>
<p><code>RoBERTa</code>初始化的表现和<code>RoBERTa</code>本身的表现差距较小，这也表明了滑动窗口<code>attention</code>是可以使用<code>RoBERTa</code>的权重的。</p>
<p>最后两行分别是训练2k步和65k之后的结果，可以看出效果是越来越好，这也说明了模型正在学习如何更好地利用滑动窗口<code>attention</code>和更长的上下文。</p>
<p><img src="/2020/07/29/transformers/longformer/image-20200729211559282.png" alt></p>
<p>第一行是数据集里头序列的平均长度，第二行是第95个百分位的数据的长度。</p>
<p>下表为下游任务上的训练结果与RoBERTa的一个对比。</p>
<p><img src="/2020/07/29/transformers/longformer/image-20200729211709262.png" alt></p>
<p>其中QA任务中，在问题和候选答案的位置使用全局attention。共指消解任务上没有使用全局attention。文本分类任务中，在[CLS]标签上用了全局attention。所有结果都是优于RoBERTa的。</p>
<h1 id="codes">codes</h1>
<h1 id="reference">reference</h1>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/134748587" target="_blank" rel="noopener">《Longformer: The Long-Document Transformer》论文笔记</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
  </entry>
  <entry>
    <title>batch normalization</title>
    <url>/2020/07/29/deeplearning/normalization/batch_normalization/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/07/29/deeplearning/normalization/batch_normalization/image-20200718091525347.png" alt></p>
<a id="more"></a>
<h1 id="yin-ru">引入</h1>
<p>机器学习领域有个很重要的假设：<strong>IID独立同分布假设</strong>，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。</p>
<p>那BatchNorm的作用是什么呢？</p>
<p>BatchNorm就是在深度神经网络训练过程中<strong>使得每一层神经网络的输入保持相同分布</strong>。它能让较深的神经网络的训练变得更加容易。</p>
<p>通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常难以训练出有效的深度模型。</p>
<p>批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。</p>
<p><code><a class="btn" href="Batch Normalization.pdf">
            <i class="fa fa-download"></i>Batch Normalization pdf
          </a></code></p>
<h1 id="internal-covariate-shift-wen-ti">Internal Covariate Shift 问题</h1>
<p><strong>如果实例集合中的输入值X的分布老是变，这不符合IID假设</strong>，网络模型很难<strong>稳定的学规律</strong>,对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临<code>covariate shift</code>的问题，也就是<strong>在训练过程中，隐层的输入分布老是变来变去，这就是所谓的<code>Internal Covariate Shift</code>，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是<code>covariate shift</code>问题只发生在输入层</strong>。</p>
<p>然后提出了BatchNorm的基本思想：能不能<strong>让每个隐层节点的激活输入分布固定下来呢</strong>？</p>
<p>这样就避免了<code>Internal Covariate Shift</code>问题。</p>
<p>BN的启发来源：之前的研究表明如果在图像处理中对输入图像进行白化（Whiten）操作的话——所谓<strong>白化</strong>，<strong>就是对输入数据分布变换到0均值，单位方差的正态分布</strong>——那么神经网络会较快收敛，那么BN作者就开始推论了：图像是深度神经网络的输入层，做白化能加快收敛，那么其实对于深度网络来说，其中某个隐层的神经元是下一层的输入，意思是其实深度神经网络的每一个隐层都是输入层，不过是相对下一层来说而已，那么能不能对每个隐层都做白化呢？这就是启发BN产生的原初想法，而BN也确实就是这么做的，<strong>可以理解为对深层神经网络每个隐层神经元的激活值做简化版本的白化操作。</strong></p>
<h1 id="batch-norm-de-ben-zhi-si-xiang">BatchNorm的本质思想</h1>
<p>BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的<strong>激活输入值</strong>（就是那个\(Z=WU+B\)，U是输入）<strong>随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近</strong>（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这<strong>导致反向传播时低层神经网络的梯度消失</strong>，这是训练深层神经网络收敛越来越慢的<strong>本质原因</strong>，<strong>而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布</strong>，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是<strong>这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</strong></p>
<p><strong>对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题</strong>。因为梯度一直都能保持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也就是说收敛地快。BN说到底就是这么个机制，方法很简单，道理很深刻。</p>
<h1 id="xun-lian-jie-duan-ru-he-zuo-batch-norm">训练阶段如何做BatchNorm</h1>
<p>假设对于一个深层神经网络来说，其中两层结构如下：</p>
<p><img src="/2020/07/29/deeplearning/normalization/batch_normalization/bn1.png" alt></p>
<p>要对每个隐层神经元的激活值做BN，可以想象成每个隐层又加上了一层BN操作层，它位于X=WU+B激活值获得之后，非线性函数变换之前，其图示如下：</p>
<p><img src="/2020/07/29/deeplearning/normalization/batch_normalization/bn2.png" alt></p>
<p>对于<code>Mini-Batch SGD</code>来说，一次训练过程里面包含m个训练实例，其具体BN操作就是对于隐层内每个神经元的激活值来说，进行如下变换：<br>
\[
\hat{x}^{(k)}=\frac{x^{(k)}-E\left[x^{(k)}\right]}{\sqrt{\operatorname{Var}\left[x^{(k)}\right]}}
\]<br>
某个神经元对应的原始的激活<code>x</code>通过减去<code>mini-Batch</code>内<code>m</code>个实例获得的<code>m</code>个激活<code>x</code>求得的均值<code>E(x)</code>并除以求得的方差<code>Var(x)</code>来进行转换。</p>
<blockquote>
<p>经过这个变换后某个神经元的激活x形成了均值为0，方差为1的正态分布，目的是把值往后续要进行的非线性变换的线性区拉动，增大导数值，增强反向传播信息流动性，加快训练收敛速度。 但是这样会导致网络表达能力下降，为了防止这一点，每个神经元增加两个调节参数（scale和shift），这两个参数是通过训练来学习到的，用来对变换后的激活反变换，使得网络表达能力增强，即对变换后的激活进行如下的scale和shift操作，这其实是变换的反操作:<br>
\[
y^{(k)}=\gamma^{(k)} \hat{x}^{(k)}+\beta^{(k)}
\]</p>
</blockquote>
<p>算法描述：</p>
<p><img src="/2020/07/29/deeplearning/normalization/batch_normalization/image-20200729213739763.png" alt></p>
<h2 id="dui-quan-lian-jie-ceng-zuo-pi-liang-gui-yi-hua">对全连接层做批量归一化</h2>
<p>先考虑如何对全连接层做批量归一化。通常，将批量归一化层置于全连接层中的<strong>仿射变换和激活函数之间</strong>。设全连接层的输入为\(\boldsymbol{u}\)，权重参数和偏差参数分别为\(\boldsymbol{W}\)和\(\boldsymbol{b}\)，，激活函数为ϕ。设批量归一化的运算符为\(\boldsymbol{BN}\)。那么，使用批量归一化的全连接层的输出为：<br>
\[
\phi(\text{BN}(\boldsymbol{x}))
\]<br>
其中批量归一化输入由\(\boldsymbol{x}\)仿射变换<br>
\[
\boldsymbol{x} = \boldsymbol{W\boldsymbol{u} + \boldsymbol{b}}
\]<br>
得到。</p>
<p>考虑一个由m个样本组成的小批量，仿射变换的输出为一个新的小批量\(\mathcal{B} = \{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(m)} \}\)。它们正是批量归一化层的输入。对于小批量\(\mathcal{B}\)中任意样本\(\boldsymbol{x}^{(i)} \in \mathbb{R}^d, 1 \leq i \leq m\)，批量归一化层的输出同样是\(\boldsymbol{d}\)维向量<br>
\[\boldsymbol{y}^{(i)} = \text{BN}(\boldsymbol{x}^{(i)}),\]<br>
并由以下几步求得。首先，对小批量\(\mathcal{B}\)求均值和方差：<br>
\[
\boldsymbol{\mu}_\mathcal{B} \leftarrow \frac{1}{m}\sum_{i = 1}^{m} \boldsymbol{x}^{(i)}\\
\boldsymbol{\sigma}_\mathcal{B}^2 \leftarrow \frac{1}{m} \sum_{i=1}^{m}(\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B})^2
\]<br>
其中的平方计算是按元素求平方。</p>
<p>接下来，使用按元素开方和按元素除法对\(\boldsymbol{x}^{(i)}\)标准化：<br>
\[
\hat{\boldsymbol{x}}^{(i)} \leftarrow \frac{\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B}}{\sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}}
\]<br>
这里\(\epsilon > 0\)是一个很小的常数，保证分母大于0。</p>
<p>在上面标准化的基础上，批量归一化层引入了两个可以学习的模型参数，拉伸参数\(\boldsymbol{\gamma}\)和偏移参数\(\boldsymbol{\beta}\)。这两个参数和\(\boldsymbol{x}^{(i)}\)形状相同，皆为d维<strong>向量</strong>。它们与\(\hat{\boldsymbol{x}}^{(i)}\)分别做按元素乘法（符号\(\odot\)）和加法计算：<br>
\[
{\boldsymbol{y}}^{(i)} \leftarrow \boldsymbol{\gamma} \odot \hat{\boldsymbol{x}}^{(i)} + \boldsymbol{\beta}
\]<br>
至此，我们得到了\(\boldsymbol{x}^{(i)}\)的批量归一化的输出\(\boldsymbol{y}^{(i)}\)。</p>
<p>值得注意的是，可学习的拉伸和偏移参数保留了不对\(\boldsymbol{x}^{(i)}\)做批量归一化的可能：此时只需学出\(\boldsymbol{\gamma} = \sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}\)和\(\boldsymbol{\beta} = \boldsymbol{\mu}_\mathcal{B}\)。我们可以对此这样理解：如果批量归一化无益，理论上，学出的模型可以不使用批量归一化。</p>
<h2 id="dui-juan-ji-ceng-zuo-pi-liang-gui-yi-hua">对卷积层做批量归一化</h2>
<p>BN除了可以应用在MLP上，其在CNN网络中的表现也非常好，但是在RNN上的表现并不好，这里介绍BN在卷积网络中的使用方法。</p>
<p>卷积网络和MLP的不同点是卷积网络中每个样本的隐层节点的输出是三维（宽度，高度，维度）的，而MLP是一维的</p>
<p>对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。如果卷积计算输出多个通道，需要<strong>对这些通道的输出分别做批量归一化</strong>，且每个通道都拥有独立的拉伸和偏移参数，并均为<strong>标量</strong>。设小批量中有 <code>m</code>个样本。在单个通道上，假设卷积计算输出的高和宽分别为 <code>p</code>和 <code>q </code>。需要对该通道中 <code>m×p×q</code>个元素同时做批量归一化。<strong>对这些元素做标准化计算时，使用相同的均值和方差，即该通道中 <code>m×p×q</code>个元素的均值和方差</strong>。</p>
<h1 id="yu-ce-shi-de-pi-liang-gui-yi-hua">预测时的批量归一化</h1>
<h2 id="tui-li-jie-duan-ru-he-zuo-batch-norm">推理阶段如何做BatchNorm</h2>
<p>BN在训练的时候可以根据<code>Mini-Batch</code>里的若干训练实例进行激活数值调整，但是在推理（inference）的过程中，很明显输入就只有一个实例，看不到<code>Mini-Batch</code>其它实例，那么这时候怎么对输入做BN呢？因为很明显一个实例是没法算实例集合求出的均值和方差的。</p>
<p>既然没有从<code>Mini-Batch</code>数据里可以得到的统计量，那就想其它办法来获得这个统计量，就是均值和方差。可以用从所有训练实例中获得的统计量来代替<code>Mini-Batch</code>里面<code>m</code>个训练实例获得的均值和方差统计量，因为本来就打算用全局的统计量，只是因为计算量等太大所以才会用<code>Mini-Batch</code>这种简化方式的，那么在推理的时候直接用全局统计量即可。</p>
<p>决定了获得统计量的数据范围，那么接下来的问题是如何获得均值和方差的问题?因为每次做<code>Mini-Batch</code>训练时，都会有那个<code>Mini-Batch</code>里<code>m</code>个训练实例获得的均值和方差，现在要全局统计量，只要把每个<code>Mini-Batch</code>的均值和方差统计量记住，然后对这些均值和方差求其对应的数学期望即可得出全局统计量<br>
\[
\begin{aligned}
&amp;E[x] \leftarrow E_{\mathrm{B}}\left[\mu_{\mathrm{B}}\right] \\
&amp;\operatorname{Var}[x] \leftarrow \frac{m}{m-1} E_{\mathrm{B}}\left[\sigma_{\mathrm{B}}^{2}\right]
\end{aligned}
\]<br>
有了均值和方差，每个隐层神经元也已经有对应训练好的<code>Scaling</code>参数和<code>Shift</code>参数，就可以在推导的时候对每个神经元的激活数据计算<code>BN</code>进行变换了，在推理过程中进行<code>BN</code>采取如下方式：<br>
\[
y=\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\varepsilon}} \cdot x+\left(\beta-\frac{\gamma \cdot E[x]}{\sqrt{\operatorname{Var}[x]+\varepsilon})}\right)
\]<br>
这个公式和训练时<br>
\[
y^{(k)}=\gamma^{(k)} \hat{x}^{(k)}+\beta^{(k)}
\]<br>
是等价的，通过简单的合并计算推导就可以得出这个结论。那么为什么要写成这个变换形式呢？作者这么写的意思是：在实际运行的时候，按照这种变体形式可以减少计算量，因为对于每个隐层节点来说：<br>
\[
\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\varepsilon}},\\ \frac{\gamma \cdot E[x]}{\sqrt{\operatorname{Var}[x]+\varepsilon}}
\]<br>
都是固定值，这样这两个值可以事先算好存起来，在推理的时候直接用就行了，这样比原始的公式每一步骤都现算少了除法的运算过程，乍一看也没少多少计算量，但是如果隐层节点个数多的话节省的计算量就比较多了。</p>
<h1 id="dai-ma-shi-xian">代码实现</h1>
<p>下面我们通过numpy中的ndarray来实现批量归一化层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_norm</span><span class="params">(is_training,X, gamma, beta, moving_mean, moving_var, eps, momentum)</span>:</span></span><br><span class="line">    <span class="comment"># 判断是当前模式是训练模式还是预测模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">        <span class="comment"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span></span><br><span class="line">        X_hat = (X - moving_mean) / np.sqrt(moving_var + eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> len(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> len(X.shape) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># 使用全连接层的情况，计算特征维上的均值和方差</span></span><br><span class="line">            mean = X.mean(axis=<span class="number">0</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持</span></span><br><span class="line">            <span class="comment"># X的形状以便后面可以做广播运算</span></span><br><span class="line">            mean = X.mean(axis=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(axis=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 训练模式下用当前的均值和方差做标准化</span></span><br><span class="line">        X_hat = (X - mean) / np.sqrt(var + eps)</span><br><span class="line">        <span class="comment"># 更新移动平均的均值和方差</span></span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line">    Y = gamma * X_hat + beta  <span class="comment"># 拉伸和偏移</span></span><br><span class="line">    <span class="keyword">return</span> Y, moving_mean, moving_var</span><br></pre></td></tr></table></figure>
<p>接下来，自定义一个BatchNorm层。它保存参与求梯度和迭代的拉伸参数gamma和偏移参数beta，同时也维护移动平均得到的均值和方差，以便能够在模型预测时被使用。BatchNorm实例所需指定的num_features参数对于全连接层来说应为输出个数，对于卷积层来说则为输出通道数。该实例所需指定的num_dims参数对于全连接层和卷积层来说分别为2和4。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchNormalization</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, decay=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, **kwargs)</span>:</span></span><br><span class="line">        self.decay = decay</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        super(BatchNormalization, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.gamma = self.add_weight(name=<span class="string">'gamma'</span>,</span><br><span class="line">                                     shape=[input_shape[<span class="number">-1</span>], ],</span><br><span class="line">                                     initializer=tf.initializers.ones,</span><br><span class="line">                                     trainable=<span class="literal">True</span>)</span><br><span class="line">        self.beta = self.add_weight(name=<span class="string">'beta'</span>,</span><br><span class="line">                                    shape=[input_shape[<span class="number">-1</span>], ],</span><br><span class="line">                                    initializer=tf.initializers.zeros,</span><br><span class="line">                                    trainable=<span class="literal">True</span>)</span><br><span class="line">        self.moving_mean = self.add_weight(name=<span class="string">'moving_mean'</span>,</span><br><span class="line">                                           shape=[input_shape[<span class="number">-1</span>], ],</span><br><span class="line">                                           initializer=tf.initializers.zeros,</span><br><span class="line">                                           trainable=<span class="literal">False</span>)</span><br><span class="line">        self.moving_variance = self.add_weight(name=<span class="string">'moving_variance'</span>,</span><br><span class="line">                                               shape=[input_shape[<span class="number">-1</span>], ],</span><br><span class="line">                                               initializer=tf.initializers.ones,</span><br><span class="line">                                               trainable=<span class="literal">False</span>)</span><br><span class="line">        super(BatchNormalization, self).build(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">assign_moving_average</span><span class="params">(self, variable, value)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        variable = variable * decay + value * (1 - decay)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        delta = variable * self.decay + value * (<span class="number">1</span> - self.decay)</span><br><span class="line">        <span class="keyword">return</span> variable.assign(delta)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> training:</span><br><span class="line">            batch_mean, batch_variance = tf.nn.moments(inputs, list(range(len(inputs.shape) - <span class="number">1</span>)))</span><br><span class="line">            mean_update = self.assign_moving_average(self.moving_mean, batch_mean)</span><br><span class="line">            variance_update = self.assign_moving_average(self.moving_variance, batch_variance)</span><br><span class="line">            self.add_update(mean_update)</span><br><span class="line">            self.add_update(variance_update)</span><br><span class="line">            mean, variance = batch_mean, batch_variance</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mean, variance = self.moving_mean, self.moving_variance</span><br><span class="line">        output = tf.nn.batch_normalization(inputs,</span><br><span class="line">                                           mean=mean,</span><br><span class="line">                                           variance=variance,</span><br><span class="line">                                           offset=self.beta,</span><br><span class="line">                                           scale=self.gamma,</span><br><span class="line">                                           variance_epsilon=self.epsilon)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> input_shape</span><br></pre></td></tr></table></figure>
<p>与自己定义的<code>BatchNorm</code>类相比，keras中<code>layers</code>模块定义的<code>BatchNorm</code>类使用起来更加简单。它不需要指定自己定义的<code>BatchNorm</code>类中所需的<code>num_features</code>和<code>num_dims</code>参数值。在keras中，这些参数值都将通过延后初始化而自动获取。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = tf.keras.models.Sequential()</span><br><span class="line">net.add(tf.keras.layers.Conv2D(filters=<span class="number">6</span>,kernel_size=<span class="number">5</span>))</span><br><span class="line">net.add(tf.keras.layers.BatchNormalization())</span><br><span class="line">net.add(tf.keras.layers.Activation(<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure>
<h1 id="zong-jie">总结</h1>
<ol>
<li>当模型发生梯度消失/爆炸或者损失值震荡比较严重的时候，在BN中加入网络往往能取得非常好的效果。</li>
<li>在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络的中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。</li>
<li>对全连接层和卷积层做批量归一化的方法稍有不同。</li>
<li>批量归一化层和丢弃层一样，在训练模式和预测模式的计算结果是不一样的。</li>
<li>不仅仅极大提升了训练速度，收敛过程大大加快。</li>
<li><strong>能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果</strong></li>
<li>另外调参过程也变得简单，对于初始化要求没那么高，而且可以使用大的学习率等。</li>
<li>在类似于RNN的动态网络中谨慎使用BN</li>
</ol>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://cloud.tencent.com/developer/article/1157136" target="_blank" rel="noopener">【深度学习】深入理解Batch Normalization批标准化</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/深度学习</category>
      </categories>
      <tags>
        <tag>归一化方法</tag>
      </tags>
  </entry>
  <entry>
    <title>YAML</title>
    <url>/2020/07/28/yaml/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/07/28/yaml/a.jpg" alt></p>
<a id="more"></a>
<h1 id="yuan-qi">缘起</h1>
<p>最近需要搭建一个搭建一个工具框架以便以后开发使用，而搭建框架最基本的则需要一个配置文件来管理全局参数。目前候选的三种格式：<code>json、ini、yaml</code>。由于<code>json</code>不能添加注释，放弃。比较 <code>ini</code> 和 <code>yaml</code> ，<code>java</code> 的一些开发框架对<code>yaml</code>的使用比较多，而接下来计划对java的开发框架进行学习，为减少不必要的学习成本，所以决定选择<code>yaml</code>来作为开发框架的配置文件格式。</p>
<p>下面记录yaml的语法及使用。</p>
<h1 id="yaml-yu-fa">YAML 语法</h1>
<p>YAML语言的设计参考了JSON，XML和SDL等语言。YAML 强调以<strong>数据为中心</strong>,简洁易读,编写简单。全称是”YAML Ain’t a Markup Language”（YAML不是一种置标语言）的递归缩写。 在开发的这种语言时，YAML 的意思其实是：”Yet Another Markup Language”（仍是一种置标语言）。</p>
<h2 id="yu-fa-te-dian">语法特点</h2>
<ul>
<li>大小写敏感</li>
<li>通过缩进表示层级关系</li>
<li><strong>禁止使用tab缩进，只能使用空格键</strong></li>
<li>缩进的空格数目不重要，只要相同层级左对齐即可</li>
<li><strong>使用#表示注释</strong></li>
</ul>
<h2 id="zhi-chi-de-shu-ju-jie-gou">支持的数据结构</h2>
<ul>
<li>对象：键值对的集合，又称为映射（mapping）/ 哈希（hashes） / 字典（dictionary）</li>
<li>数组：一组按次序排列的值，又称为序列（sequence） / 列表（list）</li>
<li>纯量（scalars）：单个的、不可再分的值</li>
</ul>
<h3 id="shuang-yin-hao-he-dan-yin-hao-de-qu-fen">双引号和单引号的区分</h3>
<p>双引号<code>&quot;&quot;</code>：不会转义字符串里面的特殊字符，特殊字符作为本身想表示的意思。</p>
<h3 id="zhi-de-xie-fa">值的写法</h3>
<h4 id="zi-fu-chuan">字符串</h4>
<p>使用单引号”或双引号”“或不使用引号</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">value0:</span> <span class="string">'hello World!'</span></span><br><span class="line"><span class="attr">value1:</span> <span class="string">"hello World!"</span></span><br><span class="line"><span class="attr">value2:</span> <span class="string">hello</span> <span class="string">World!</span></span><br></pre></td></tr></table></figure>
<h4 id="bu-er-zhi">布尔值</h4>
<p><code>true</code>或<code>false</code>表示。</p>
<h4 id="shu-zi">数字</h4>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="number">12</span> <span class="comment">#整数 </span></span><br><span class="line"><span class="number">014</span> <span class="comment"># 八进制整数 </span></span><br><span class="line"><span class="number">0xC</span> <span class="string">＃十六进制整数</span> </span><br><span class="line"><span class="number">13.4</span> <span class="string">＃浮点数</span> </span><br><span class="line"><span class="number">1.2e+34</span> <span class="string">＃指数</span> </span><br><span class="line"><span class="string">.inf空值</span> <span class="string">＃无穷大</span></span><br></pre></td></tr></table></figure>
<h4 id="kong-zhi">空值</h4>
<p><code>null</code>或<code>~</code>表示</p>
<h4 id="ri-qi">日期</h4>
<p>使用 iso-8601 标准表示日期，<strong>在springboot中yaml文件的时间格式 date: yyyy/MM/dd HH:mm:ss</strong></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">date:</span> <span class="number">2018</span><span class="number">-01</span><span class="string">-01t16:59:43.10-05:00</span></span><br><span class="line"><span class="attr">date:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">2018</span><span class="number">-02</span><span class="number">-17</span>    <span class="comment">#日期必须使用ISO 8601格式，即yyyy-MM-dd</span></span><br><span class="line"><span class="attr">datetime:</span> </span><br><span class="line">    <span class="bullet">-</span>  <span class="number">2018</span><span class="number">-02</span><span class="string">-17T15:02:31+08:00</span>    <span class="comment">#时间使用ISO 8601格式，时间和日期之间使用T连接，最后使用+代表时区</span></span><br></pre></td></tr></table></figure>
<h4 id="qiang-zhi-lei-xing-zhuan-huan">强制类型转换</h4>
<p>YAML 允许使用感叹号<code>!</code>，强制转换数据类型，<code>单叹号</code>通常是自定义类型，<code>双叹号</code>是内置类型。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">money:</span> <span class="type">!!str</span></span><br><span class="line"><span class="number">123</span></span><br><span class="line"><span class="attr">date:</span> <span class="type">!Boolean</span></span><br><span class="line"><span class="literal">true</span></span><br></pre></td></tr></table></figure>
<p><strong>内置类型列表</strong></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="type">!!int</span> <span class="comment"># 整数类型 </span></span><br><span class="line"><span class="type">!!float</span> <span class="comment"># 浮点类型 </span></span><br><span class="line"><span class="type">!!bool</span> <span class="comment"># 布尔类型 </span></span><br><span class="line"><span class="type">!!str</span> <span class="comment"># 字符串类型 </span></span><br><span class="line"><span class="type">!!binary</span> <span class="comment"># 也是字符串类型 </span></span><br><span class="line"><span class="type">!!timestamp</span> <span class="comment"># 日期时间类型 </span></span><br><span class="line"><span class="type">!!null</span> <span class="comment"># 空值 </span></span><br><span class="line"><span class="type">!!set</span> <span class="comment"># 集合 </span></span><br><span class="line"><span class="type">!!omap</span><span class="string">,!!pairs</span> <span class="comment"># 键值列表或对象列表</span></span><br><span class="line"><span class="type">!!seq</span> <span class="comment"># 序列，也是列表 !!map # 键值表</span></span><br></pre></td></tr></table></figure>
<h4 id="dui-xiang-zhong-dian">对象（重点）</h4>
<p>Map（属性和值）（键值对）的形式： key:(空格)v ：表示一对键值对，空格不可省略。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">websites:</span></span><br><span class="line"> <span class="attr">YAML:</span> <span class="string">yaml.org</span> </span><br><span class="line"> <span class="attr">Ruby:</span> <span class="string">ruby-lang.org</span> </span><br><span class="line"> <span class="attr">Python:</span> <span class="string">python.org</span> </span><br><span class="line"> </span><br><span class="line"><span class="string">转为</span> <span class="string">JavaScript</span> </span><br><span class="line"><span class="string">&#123;websites:</span> <span class="string">&#123;</span> <span class="attr">YAML:</span> <span class="string">'yaml.org'</span><span class="string">,</span> <span class="attr">Ruby:</span> <span class="string">'ruby-lang.org'</span><span class="string">,</span> <span class="attr">Python:</span> <span class="string">'python.org'</span> <span class="string">&#125;</span> <span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<p>一行写法</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">websites:&#123;YAML:</span> <span class="string">yaml.org，Ruby:</span> <span class="string">ruby-lang.org</span> <span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<p>相当于JSON格式：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">&#123;"YAML":"yaml.org","Ruby":"ruby-lang.org"&#125;</span></span><br></pre></td></tr></table></figure>
<h4 id="shu-zu">数组</h4>
<p>一组连词线开头的行，构成一个数组。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">brand:</span></span><br><span class="line">   <span class="bullet">-</span> <span class="string">audi</span></span><br><span class="line">   <span class="bullet">-</span> <span class="string">bmw</span></span><br><span class="line">   <span class="bullet">-</span> <span class="string">ferrari</span></span><br></pre></td></tr></table></figure>
<p>一行写法</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">brand:</span> <span class="string">[audi,bmw,ferrari]</span></span><br></pre></td></tr></table></figure>
<p>相当于JSON</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">["auri","bmw","ferrari"]</span></span><br></pre></td></tr></table></figure>
<h4 id="wen-ben-kuai">文本块</h4>
<p>|：使用<code>|</code>标注的文本内容缩进表示的块，可以保留块中已有的回车换行</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">value:</span> <span class="string">|</span></span><br><span class="line">   <span class="string">hello</span></span><br><span class="line">   <span class="string">world!</span></span><br><span class="line"><span class="string">输出结果：hello</span> <span class="string">换行</span> <span class="string">world！</span></span><br></pre></td></tr></table></figure>
<p><code>+</code>表示保留文字块末尾的换行，<code>-</code>表示删除字符串末尾的换行。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">value:</span> <span class="string">|</span></span><br><span class="line"><span class="string">hello</span></span><br><span class="line"></span><br><span class="line"><span class="attr">value:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">hello</span></span><br><span class="line"></span><br><span class="line"><span class="attr">value:</span> <span class="string">|+</span></span><br><span class="line"><span class="string">hello</span></span><br><span class="line"><span class="string">输出：hello\n</span> <span class="string">hello</span> <span class="string">hello\n\n(有多少个回车就有多少个\n)</span></span><br></pre></td></tr></table></figure>
<p><strong>注意 <code>|</code> 与 文本之间须另起一行</strong></p>
<p>&gt;：使用 <code>&gt;</code> 标注的文本内容缩进表示的块，将块中回车替换为空格，最终连接成一行</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">value:</span> <span class="string">&gt;</span> <span class="string">hello</span></span><br><span class="line"><span class="string">world!</span></span><br><span class="line"><span class="string">输出：hello</span> <span class="string">空格</span> <span class="string">world！</span></span><br></pre></td></tr></table></figure>
<p><strong>注意 “&gt;” 与 文本之间的空格</strong></p>
<h4 id="wen-jian-kai-shi-yu-jie-shu">文件开始与结束</h4>
<p>… 和—配合使用，在一个配置文件中代表一个文件的结束：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">time: 20:03:20</span><br><span class="line">player: Sammy Sosa</span><br><span class="line">action: strike (miss)</span><br><span class="line">...</span><br><span class="line">---</span><br><span class="line">time: 20:03:47</span><br><span class="line">player: Sammy Sosa</span><br><span class="line">action: grand slam</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h4 id="mao-dian-yu-yin-yong">锚点与引用</h4>
<p>使用 <code>&amp;</code> 定义数据锚点（即要复制的数据），使用 <code>*</code> 引用锚点数据（即数据的复制目的地）</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">&amp;a</span> <span class="string">yaml</span></span><br><span class="line"><span class="attr">book:</span> <span class="meta">*a</span></span><br><span class="line"><span class="attr">books:</span> </span><br><span class="line">   <span class="bullet">-</span> <span class="string">java</span></span><br><span class="line">   <span class="bullet">-</span> <span class="meta">*a</span></span><br><span class="line">   <span class="bullet">-</span> <span class="string">python</span></span><br><span class="line"><span class="string">输出book：</span> <span class="string">yaml</span> </span><br><span class="line"><span class="string">输出books：[java,yaml,python]</span></span><br></pre></td></tr></table></figure>
<p><strong>注意<code>*</code>引用部分不能追加内容</strong></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">defaults:</span> <span class="meta">&amp;defaults</span></span><br><span class="line">  <span class="attr">adapter:</span>  <span class="string">postgres</span></span><br><span class="line">  <span class="attr">host:</span>     <span class="string">localhost</span></span><br><span class="line"></span><br><span class="line"><span class="attr">development:</span></span><br><span class="line">  <span class="attr">database:</span> <span class="string">myapp_development</span></span><br><span class="line">  <span class="string">&lt;&lt;:</span> <span class="meta">*defaults</span></span><br><span class="line"></span><br><span class="line"><span class="attr">test:</span></span><br><span class="line">  <span class="attr">database:</span> <span class="string">myapp_test</span></span><br><span class="line">  <span class="string">&lt;&lt;:</span> <span class="meta">*defaults</span></span><br><span class="line"></span><br><span class="line"><span class="string">转为</span> <span class="string">JavaScript</span> <span class="string">如下:</span></span><br><span class="line"><span class="string">&#123;</span> <span class="attr">defaults:</span> <span class="string">&#123;</span> <span class="attr">adapter:</span> <span class="string">'postgres'</span><span class="string">,</span> <span class="attr">host:</span> <span class="string">'localhost'</span> <span class="string">&#125;,</span></span><br><span class="line">  <span class="attr">development:</span> </span><br><span class="line">   <span class="string">&#123;</span> <span class="attr">database:</span> <span class="string">'myapp_development'</span><span class="string">,</span></span><br><span class="line">     <span class="attr">adapter:</span> <span class="string">'postgres'</span><span class="string">,</span></span><br><span class="line">     <span class="attr">host:</span> <span class="string">'localhost'</span> <span class="string">&#125;,</span></span><br><span class="line">  <span class="attr">test:</span> </span><br><span class="line">   <span class="string">&#123;</span> <span class="attr">database:</span> <span class="string">'myapp_test'</span><span class="string">,</span></span><br><span class="line">     <span class="attr">adapter:</span> <span class="string">'postgres'</span><span class="string">,</span></span><br><span class="line">     <span class="attr">host:</span> <span class="string">'localhost'</span> <span class="string">&#125;</span> <span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="example">example</h3>
<p>以模型的配置文件为例</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">project_name :</span> <span class="string">'seq labeling'</span></span><br><span class="line"><span class="attr">num_gpu :</span> <span class="number">4</span>                            <span class="comment"># GPU数量</span></span><br><span class="line"><span class="attr">device:</span> <span class="string">[0,1,2,3]</span></span><br><span class="line"><span class="attr">visual_device:</span> <span class="string">[0,1,2,3]</span></span><br><span class="line"><span class="attr">resume_path:</span> <span class="string">''</span>                        <span class="comment"># path to latest checkpoint</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型</span></span><br><span class="line"><span class="attr">model_arch:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">'BertTagger'</span></span><br><span class="line">  <span class="attr">args:</span></span><br><span class="line">    <span class="attr">bert_path:</span> <span class="string">'/home/lizhen/pretrain_models/pytorch/chinese_roberta_wwm_ext_pytorch'</span></span><br><span class="line">    <span class="attr">bert_train:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">dropout:</span> <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="attr">data_set:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">'type'</span></span><br><span class="line">  <span class="attr">args:</span></span><br><span class="line">    <span class="attr">data_dir:</span> <span class="string">''</span></span><br><span class="line">    <span class="attr">embedding_path:</span> <span class="string">'/home/lizhen/pretrain_models/pytorch/chinese_roberta_wwm_ext_pytorch'</span></span><br><span class="line">    <span class="attr">valid_size:</span> <span class="number">0.3</span></span><br><span class="line">    <span class="attr">train:</span> <span class="string">''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="attr">data_loader:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">'DataLoader'</span></span><br><span class="line">  <span class="attr">args:</span></span><br><span class="line">    <span class="attr">shuffle:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">batch_size:</span> <span class="number">16</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="attr">optimizer:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">'AdamW'</span></span><br><span class="line">  <span class="attr">args:</span></span><br><span class="line">    <span class="attr">learning_rate:</span> <span class="number">1e-5</span></span><br><span class="line"></span><br><span class="line"><span class="attr">lr_scheduler:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">'get_linear_schedule_with_warmup'</span></span><br><span class="line">  <span class="attr">args:</span></span><br><span class="line">    <span class="attr">num_warmup_steps:</span> <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="attr">loss:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">'tag_loss'</span></span><br><span class="line"></span><br><span class="line"><span class="attr">metrics:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">'tag_accuracy'</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">'tag_precision'</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">'tag_recall'</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">'tag_f1_score'</span></span><br><span class="line"></span><br><span class="line"><span class="attr">trainer:</span></span><br><span class="line">  <span class="attr">epochs:</span> <span class="number">100</span></span><br><span class="line">  <span class="attr">save_dir:</span> <span class="string">'saved/'</span></span><br><span class="line">  <span class="attr">save_perioid:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">verbosity:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">monitor:</span> <span class="string">"max val_tag_f1_score"</span></span><br><span class="line">  <span class="attr">early_stop:</span> <span class="number">20</span></span><br><span class="line">  <span class="attr">tensorboard:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h2 id="python-shi-yong">python 使用</h2>
<p>yaml是一种很清晰、简洁的格式，而且跟Python非常合拍，非常容易操作。</p>
<h3 id="an-zhuang-mo-kuai">安装模块</h3>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">pip <span class="keyword">install</span> pyyaml</span><br></pre></td></tr></table></figure>
<h3 id="shi-yong">使用</h3>
<figure class="highlight yaml"><figcaption><span>items.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">raincoat:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">coins:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">books:</span> <span class="number">23</span></span><br><span class="line"><span class="attr">spectacles:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">chairs:</span> <span class="number">12</span></span><br><span class="line"><span class="attr">pens:</span> <span class="number">6</span></span><br></pre></td></tr></table></figure>
<figure class="highlight yaml"><figcaption><span>data.yaml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">cities:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Bratislava</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Kosice</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Trnava</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Moldava</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Trencin</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">companies:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Eset</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Slovnaft</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Duslo</span> <span class="string">Sala</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Matador</span> <span class="string">Puchov</span></span><br></pre></td></tr></table></figure>
<h4 id="du-qu-load">读取：load</h4>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> yaml</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'items.yaml'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    </span><br><span class="line">    data = yaml.load(f, Loader=yaml.FullLoader)</span><br><span class="line">    print(data)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&#123;<span class="string">'raincoat'</span>: <span class="number">1</span>, <span class="string">'coins'</span>: <span class="number">5</span>, <span class="string">'books'</span>: <span class="number">23</span>, <span class="string">'spectacles'</span>: <span class="number">2</span>, <span class="string">'chairs'</span>: <span class="number">12</span>, <span class="string">'pens'</span>: <span class="number">6</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.yaml'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    </span><br><span class="line">    docs = yaml.load_all(f, Loader=yaml.FullLoader)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> docs:</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> doc.items():</span><br><span class="line">            print(k, <span class="string">"-&gt;"</span>, v)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">cities -&gt; ['Bratislava', 'Kosice', 'Trnava', 'Moldava', 'Trencin']</span><br><span class="line">companies -&gt; ['Eset', 'Slovnaft', 'Duslo Sala', 'Matador Puchov']</span><br></pre></td></tr></table></figure>
<h4 id="xie-ru-dump">写入：dump</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">users = [&#123;<span class="string">'name'</span>: <span class="string">'John Doe'</span>, <span class="string">'occupation'</span>: <span class="string">'gardener'</span>&#125;,</span><br><span class="line">         &#123;<span class="string">'name'</span>: <span class="string">'Lucy Black'</span>, <span class="string">'occupation'</span>: <span class="string">'teacher'</span>&#125;]</span><br><span class="line"></span><br><span class="line">print(yaml.dump(users))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output：</span></span><br><span class="line">- name: John Doe</span><br><span class="line">  occupation: gardener</span><br><span class="line">- name: Lucy Black</span><br><span class="line">  occupation: teacher</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 写入到文件</span></span><br><span class="line">users = [&#123;<span class="string">'name'</span>: <span class="string">'John Doe'</span>, <span class="string">'occupation'</span>: <span class="string">'gardener'</span>&#125;,</span><br><span class="line">         &#123;<span class="string">'name'</span>: <span class="string">'Lucy Black'</span>, <span class="string">'occupation'</span>: <span class="string">'teacher'</span>&#125;]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'users.yaml'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    </span><br><span class="line">    data = yaml.dump(users, f)</span><br></pre></td></tr></table></figure>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://juejin.im/post/5c1a4a0fe51d45344a1c3d2a" target="_blank" rel="noopener">YAML语法简易入门</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/工程</category>
      </categories>
  </entry>
  <entry>
    <title>docker</title>
    <url>/2020/07/21/docker/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/07/21/docker/ownCloud-docker-1.png" alt></p>
<a id="more"></a>
<h1 id="an-zhuang-docker">安装Docker</h1>
<h2 id="on-ubuntu-18-04">on ubuntu 18.04</h2>
<p>1.更换国内软件源，推荐中国科技大学的源，稳定速度快（可选）</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">sudo cp /etc/apt/sources<span class="selector-class">.list</span> /etc/apt/sources<span class="selector-class">.list</span>.bak</span><br><span class="line">sudo sed -<span class="selector-tag">i</span> <span class="string">'s/archive.ubuntu.com/mirrors.ustc.edu.cn/g'</span> /etc/apt/sources.list</span><br><span class="line">sudo apt update</span><br></pre></td></tr></table></figure>
<p>2.安装需要的包</p>
<figure class="highlight elm"><table><tr><td class="code"><pre><span class="line"><span class="title">sudo</span> apt install apt-trans<span class="keyword">port</span>-https ca-certificates software-properties-common curl</span><br></pre></td></tr></table></figure>
<p>3.添加 GPG 密钥，并添加 Docker-ce 软件源，这里还是以中国科技大学的 Docker-ce 源为例</p>
<figure class="highlight smali"><table><tr><td class="code"><pre><span class="line">curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key<span class="built_in"> add </span>-</span><br><span class="line">sudo<span class="built_in"> add-apt-repository </span><span class="string">"deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu \</span></span><br><span class="line"><span class="string">$(lsb_release -cs) stable"</span></span><br></pre></td></tr></table></figure>
<p>4.添加成功后更新软件包缓存</p>
<figure class="highlight ebnf"><table><tr><td class="code"><pre><span class="line"><span class="attribute">sudo apt update</span></span><br></pre></td></tr></table></figure>
<p>5.安装 Docker-ce</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">sudo apt <span class="keyword">install</span> docker-ce</span><br></pre></td></tr></table></figure>
<p>6.设置开机自启动并启动 Docker-ce（安装成功后默认已设置并启动，可忽略）</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line">sudo systemctl <span class="keyword">enable</span> docker</span><br><span class="line">sudo systemctl <span class="keyword">start</span> docker</span><br></pre></td></tr></table></figure>
<p>7.测试运行</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line">sudo docker <span class="keyword">run</span><span class="bash"> hello-world</span></span><br></pre></td></tr></table></figure>
<p>8.添加当前用户到 docker 用户组，可以不用 sudo 运行 docker（可选）</p>
<figure class="highlight crmsh"><table><tr><td class="code"><pre><span class="line">sudo groupadd docker     <span class="comment">#添加docker用户组</span></span><br><span class="line">sudo gpasswd -a $<span class="keyword">USER</span> <span class="title">docker</span>     <span class="comment">#将登陆用户加入到docker用户组中</span></span><br><span class="line">newgrp docker     <span class="comment">#更新用户组</span></span><br><span class="line">docker ps    <span class="comment">#测试docker命令是否可以使用sudo正常使用</span></span><br></pre></td></tr></table></figure>
<p>9.测试安装docanno</p>
<figure class="highlight livescript"><table><tr><td class="code"><pre><span class="line">docker pull doccano/doccano</span><br><span class="line"></span><br><span class="line">docker container create --name doccano <span class="string">\</span></span><br><span class="line">  -e <span class="string">"ADMIN_USERNAME=admin"</span> <span class="string">\</span></span><br><span class="line">  -e <span class="string">"ADMIN_EMAIL=admin@example.com"</span> <span class="string">\</span></span><br><span class="line">  -e <span class="string">"ADMIN_PASSWORD=password"</span> <span class="string">\</span></span><br><span class="line">  -p <span class="number">8000</span>:<span class="number">8000</span> doccano/doccano</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">docker container start doccano</span><br></pre></td></tr></table></figure>
<h1 id="an-zhuang-docker-compose">安装Docker Compose</h1>
<h2 id="compose-jian-jie">Compose 简介</h2>
<p>Compose 是用于定义和运行多容器 Docker 应用程序的工具。通过 Compose，可以使用 YML 文件来配置应用程序需要的所有服务。然后，使用一个命令，就可以从 YML 文件配置中创建并启动所有服务。</p>
<p>Compose 使用的三个步骤：</p>
<ul>
<li>使用 Dockerfile 定义应用程序的环境。</li>
<li>使用 docker-compose.yml 定义构成应用程序的服务，这样它们可以在隔离环境中一起运行。</li>
<li>最后，执行 docker-compose up 命令来启动并运行整个应用程序。</li>
</ul>
<h2 id="an-zhuang-on-linux">安装 on linux</h2>
<p>下载</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo curl -L <span class="string">"https://github.com/docker/compose/releases/download/1.26.2/docker-compose-<span class="variable">$(uname -s)</span>-<span class="variable">$(uname -m)</span>"</span> -o /usr/<span class="built_in">local</span>/bin/docker-compose</span><br></pre></td></tr></table></figure>
<p>将可执行权限应用于二进制文件：</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">sudo chmod +x <span class="regexp">/usr/</span>local<span class="regexp">/bin/</span>docker-compose</span><br></pre></td></tr></table></figure>
<p>创建软链：</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">sudo ln -s <span class="regexp">/usr/</span>local<span class="regexp">/bin/</span>docker-compose <span class="regexp">/usr/</span>bin<span class="regexp">/docker-compose</span></span><br></pre></td></tr></table></figure>
<p>测试是否安装成功：</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">docker-compose --version</span><br><span class="line">docker-compose version <span class="number">1.26</span><span class="number">.2</span>, build eefe0d31</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>注意</strong>： 对于 alpine，需要以下依赖包： py-pip，python-dev，libffi-dev，openssl-dev，gcc，libc-dev，和 make。</p>
</blockquote>
<p>应用：安装doccano</p>
<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">git clone http<span class="variable">s:</span>//github.<span class="keyword">com</span>/doccano/doccano.git</span><br><span class="line"><span class="keyword">cd</span> doccano</span><br><span class="line">docker-compose -<span class="keyword">f</span> docker-compose.prod.yml <span class="keyword">up</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术/docker</category>
      </categories>
  </entry>
  <entry>
    <title>Beam Search</title>
    <url>/2020/07/20/deeplearning/beam_search/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/07/20/deeplearning/beam_search/10.10_beam_search.svg" alt></p>
<a id="more"></a>
<h1 id="jian-jie">简介</h1>
<p>编码器—解码器来预测不定长的序列时，通常会在样本的输入序列和输出序列后面分别附上一个特殊符号&quot;&lt;eos&gt;“表示序列的终止。假设解码器的输出是一段文本序列。设输出文本词典\(\mathcal{Y}\)（包含特殊符号”&lt;eos&gt;&quot;）的大小为\(\left|\mathcal{Y}\right|\)，输出序列的最大长度为\(T'\)。所有可能的输出序列一共有\(\mathcal{O}(\left|\mathcal{Y}\right|^{T'})\)种。这些输出序列中所有特殊符号&quot;&lt;eos&gt;&quot;后面的子序列将被舍弃。</p>
<h1 id="tan-lan-sou-suo">贪婪搜索</h1>
<p>先来看一个简单的解决方案：贪婪搜索。对于输出序列任一时间步\(t'\)，从\(|\mathcal{Y}|\)个词中搜索出条件概率最大的词：</p>
<p>\[
y _ { t ^ { \prime } } = \underset { y \in \mathcal { Y } } { \operatorname { argmax } } P \left( y | y _ { 1 } , \ldots , y _ { t ^ { \prime } - 1 } , c \right)
\]</p>
<p>作为输出。一旦搜索出&quot;&lt;eos&gt;&quot;符号，或者输出序列长度已经达到了最大长度\(T'\)，便完成输出。</p>
<p>在描述解码器时提到，基于输入序列生成输出序列的条件概率是\(\prod_{t'=1}^{T'} P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c})\)。我们将该条件概率最大的输出序列称为最优输出序列。而贪婪搜索的主要问题是不能保证得到最优输出序列。</p>
<p>下面来看一个例子。假设输出词典里面有“A”“B”“C”和“&lt;eos&gt;”这4个词。下图中每个时间步下的4个数字分别代表了该时间步生成“A”“B”“C”和“&lt;eos&gt;”这4个词的条件概率。在每个时间步，贪婪搜索选取条件概率最大的词。因此，图10.9中将生成输出序列“A”“B”“C”“&lt;eos&gt;”。该输出序列的条件概率是\(0.5\times0.4\times0.4\times0.6 = 0.048\)。</p>
<p><img src="/2020/07/20/deeplearning/beam_search/10.10_s2s_prob1.svg" alt="在每个时间步，贪婪搜索选取条件概率最大的词"></p>
<p>接下来，观察下图演示的例子。与上图中不同，下图在时间步2中选取了条件概率第二大的词“C”。由于时间步3所基于的时间步1和2的输出子序列由图10.9中的“A”“B”变为了图10.10中的“A”“C”，图10.10中时间步3生成各个词的条件概率发生了变化。我们选取条件概率最大的词“B”。此时时间步4所基于的前3个时间步的输出子序列为“A”“C”“B”，与上图中的“A”“B”“C”不同。因此，下图中时间步4生成各个词的条件概率也与上图中的不同。此时的输出序列“A”“C”“B”“&lt;eos&gt;”的条件概率是\(0.5\times0.3\times0.6\times0.6=0.054\)，大于贪婪搜索得到的输出序列的条件概率。因此，贪婪搜索得到的输出序列“A”“B”“C”“&lt;eos&gt;”并非最优输出序列。</p>
<p><img src="/2020/07/20/deeplearning/beam_search/10.10_s2s_prob2.svg" alt="在时间步2选取条件概率第二大的词“C”"></p>
<h1 id="qiong-ju-sou-suo">穷举搜索</h1>
<p>如果目标是得到最优输出序列，我们可以考虑穷举搜索（exhaustive search）：穷举所有可能的输出序列，输出条件概率最大的序列。</p>
<p>虽然穷举搜索可以得到最优输出序列，但它的计算开销\(\mathcal{O}(\left|\mathcal{Y}\right|^{T'})\)很容易过大。例如，当\(|\mathcal{Y}|=10000\)且\(T'=10\)时，我们将评估\(10000^{10} = 10^{40}\)个序列：这几乎不可能完成。而贪婪搜索的计算开销是\(\mathcal{O}(\left|\mathcal{Y}\right|T')\)，通常显著小于穷举搜索的计算开销。例如，当\(|\mathcal{Y}|=10000\)且\(T'=10\)时，我们只需评估\(10000\times10=10^5\)个序列。</p>
<h1 id="shu-sou-suo">束搜索</h1>
<p>束搜索（beam search）是对贪婪搜索的一个改进算法。它有一个束宽超参数。我们将它设为\(k\)。在时间步1时，选取当前时间步条件概率最大的\(k\)个词，分别组成\(k\)个候选输出序列的首词。在之后的每个时间步，基于上个时间步的\(k\)个候选输出序列，从\(k\left|\mathcal{Y}\right|\)个可能的输出序列中选取条件概率最大的\(k\)个，作为该时间步的候选输出序列。最终，从各个时间步的候选输出序列中筛选出包含特殊符号“&lt;eos&gt;”的序列，并将它们中所有特殊符号“&lt;eos&gt;”后面的子序列舍弃，得到最终候选输出序列的集合。</p>
<p><img src="/2020/07/20/deeplearning/beam_search/10.10_beam_search.svg" alt="束搜索的过程。束宽为2，输出序列最大长度为3。候选输出序列有A、C、AB、CE、ABD和CED"></p>
<p>通过上图一个例子演示了束搜索的过程。假设输出序列的词典中只包含5个元素，即\(\mathcal{Y} = \{A, B, C, D, E\}\)，且其中一个为特殊符号“&lt;eos&gt;”。设束搜索的束宽等于2，输出序列最大长度为3。在输出序列的时间步1时，假设条件概率\(P(y_1 \mid \boldsymbol{c})\)最大的2个词为\(A\)和\(C\)。在时间步2时将对所有的\(y_2 \in \mathcal{Y}\)都分别计算\(P(y_2 \mid A, \boldsymbol{c})\)和\(P(y_2 \mid C, \boldsymbol{c})\)，并从计算出的10个条件概率中取最大的2个，假设为\(P(B \mid A, \boldsymbol{c})\)和\(P(E \mid C, \boldsymbol{c})\)。那么，在时间步3时将对所有的\(y_3 \in \mathcal{Y}\)都分别计算\(P(y_3 \mid A, B, \boldsymbol{c})\)和\(P(y_3 \mid C, E, \boldsymbol{c})\)，并从计算出的10个条件概率中取最大的2个，假设为\(P(D \mid A, B, \boldsymbol{c})\)和\(P(D \mid C, E, \boldsymbol{c})\)。如此一来，得到6个候选输出序列：（1）\(A\)；（2）\(C\)；（3）\(A\)、\(B\)；（4）\(C\)、\(E\)；（5）\(A\)、\(B\)、\(D\)和（6）\(C\)、\(E\)、\(D\)。接下来，根据这6个序列得出最终候选输出序列的集合。在最终候选输出序列的集合中，取以下分数最高的序列作为输出序列：</p>
<p>\[
\frac{1}{L^\alpha} \log P(y_1, \ldots, y_{L}) = \frac{1}{L^\alpha} \sum_{t'=1}^L \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c})
\]</p>
<p>其中\(L\)为最终候选序列长度，\(\alpha\)一般可选为0.75。分母上的\(L^\alpha\)是为了惩罚较长序列在以上分数中较多的对数相加项。分析可知，束搜索的计算开销为\(\mathcal{O}(k\left|\mathcal{Y}\right|T')\)。这介于贪婪搜索和穷举搜索的计算开销之间。此外，贪婪搜索可看作是束宽为1的束搜索。束搜索通过灵活的束宽\(k\)来权衡计算开销和搜索质量。</p>
]]></content>
      <categories>
        <category>技术/深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>tensorflow计算性能</title>
    <url>/2020/07/19/deeplearning/tf_computational_performance/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/07/19/deeplearning/tf_computational_performance/201510141003.jpg" alt></p>
<a id="more"></a>
<h1 id="ming-ling-shi-he-fu-hao-shi-hun-he-bian-cheng">命令式和符号式混合编程</h1>
<p>考虑下面这段简单的命令式程序。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fancy_func</span><span class="params">(a, b, c, d)</span>:</span></span><br><span class="line">    e = add(a, b)</span><br><span class="line">    f = add(c, d)</span><br><span class="line">    g = add(e, f)</span><br><span class="line">    <span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line">fancy_func(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>) <span class="comment"># 10</span></span><br></pre></td></tr></table></figure>
<p>和预期的一样，在运行语句<code>e = add(a, b)</code>时，Python会做加法运算并将结果存储在变量<code>e</code>中，从而令程序的状态发生改变。类似地，后面的两条语句<code>f = add(c, d)</code>和<code>g = add(e, f)</code>会依次做加法运算并存储变量。</p>
<p>虽然使用命令式编程很方便，但它的运行可能很慢。一方面，即使<code>fancy_func</code>函数中的<code>add</code>是被重复调用的函数，Python也会逐一执行这3条函数调用语句。另一方面，我们需要保存变量<code>e</code>和<code>f</code>的值直到<code>fancy_func</code>中所有语句执行结束。这是因为在执行<code>e = add(a, b)</code>和<code>f = add(c, d)</code>这2条语句之后我们并不知道变量<code>e</code>和<code>f</code>是否会被程序的其他部分使用。</p>
<p>与命令式编程不同，符号式编程通常在计算流程完全定义好后才被执行。多个深度学习框架，如<strong>Theano和TensorFlow，都使用了符号式编程</strong>。通常，符号式编程的程序需要下面3个步骤：</p>
<ol>
<li>定义计算流程；</li>
<li>把计算流程编译成可执行的程序；</li>
<li>给定输入，调用编译好的程序执行。</li>
</ol>
<p>下面我们用符号式编程重新实现本节开头给出的命令式编程代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_str</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'''</span></span><br><span class="line"><span class="string">def add(a, b):</span></span><br><span class="line"><span class="string">    return a + b</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fancy_func_str</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'''</span></span><br><span class="line"><span class="string">def fancy_func(a, b, c, d):</span></span><br><span class="line"><span class="string">    e = add(a, b)</span></span><br><span class="line"><span class="string">    f = add(c, d)</span></span><br><span class="line"><span class="string">    g = add(e, f)</span></span><br><span class="line"><span class="string">    return g</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evoke_str</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> add_str() + fancy_func_str() + <span class="string">'''</span></span><br><span class="line"><span class="string">print(fancy_func(1, 2, 3, 4))</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">prog = evoke_str()</span><br><span class="line">print(prog)</span><br><span class="line">y = compile(prog, <span class="string">''</span>, <span class="string">'exec'</span>)</span><br><span class="line">exec(y)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line">def <span class="built_in">add</span>(<span class="keyword">a</span>, b):</span><br><span class="line">    <span class="literal">return</span> <span class="keyword">a</span> + b</span><br><span class="line"></span><br><span class="line">def fancy_func(<span class="keyword">a</span>, b, c, d):</span><br><span class="line">    e = <span class="built_in">add</span>(<span class="keyword">a</span>, b)</span><br><span class="line">    f = <span class="built_in">add</span>(c, d)</span><br><span class="line">    g = <span class="built_in">add</span>(e, f)</span><br><span class="line">    <span class="literal">return</span> g</span><br><span class="line"></span><br><span class="line">print(fancy_func(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>以上定义的3个函数都仅以字符串的形式返回计算流程。最后，通过<code>compile</code>函数编译完整的计算流程并运行。由于在编译时系统能够完整地获取整个程序，因此有更多空间优化计算。例如，编译的时候可以将程序改写成<code>print((1 + 2) + (3 + 4))</code>，甚至直接改写成<code>print(10)</code>。这样不仅减少了函数调用，还节省了内存。</p>
<p>对比这两种编程方式，可以看到以下两点。</p>
<ul>
<li>
<p>命令式编程更方便。当在Python里使用命令式编程时，大部分代码编写起来都很直观。同时，命令式编程更容易调试。这是因为可以很方便地获取并打印所有的中间变量值，或者使用Python的调试工具。</p>
</li>
<li>
<p>符号式编程更高效并更容易移植。一方面，在编译的时候系统容易做更多优化；另一方面，符号式编程可以将程序变成一个与Python无关的格式，从而可以使程序在非Python环境下运行，以避开Python解释器的性能问题。</p>
</li>
</ul>
<h2 id="hun-he-shi-bian-cheng-qu-liang-zhe-zhi-chang">混合式编程取两者之长</h2>
<p>大部分深度学习框架在命令式编程和符号式编程之间二选一。例如，Theano和受其启发的后来者<code>TensorFlow1.x</code>使用了符号式编程，<code>Chainer</code>和它的追随者<code>PyTorch</code>使用了命令式编程。开发人员在设计<code>Tensorflow2.x</code>时思考了这个问题：有没有可能既得到命令式编程的好处，又享受符号式编程的优势？开发者们认为，用户应该用纯命令式编程进行开发和调试；当需要产品级别的计算性能和部署时，用户可以将大部分命令式程序转换成符号式程序来运行。Tensorflow通过提供静态图转换器<code>tf.function</code>,实现对两种编程方式的支持。在不使用静态图转换器<code>tf.function</code>时，用户编写的<code>python</code>函数默认会采用命令式编程<strong>逐行执行</strong>，符合<code>python</code>编程的直觉，便于调试，但因为框架不能获得完整的静态运算图，不能进行优化，且动态图不能序列化。官方由此推出了静态图转换器<code>tf.function</code>，其作用在<code>python_function</code>后会将这个函数&quot;编译&quot;成一个运算图，接受<code>input_tensors</code>为输入并用图执行的方式计算结果，可以加速函数执行，并且可以被序列化后供任何其他语言（如C++和Java）调用，这样用户只需要在使用动态图运算编写和测试完毕函数之后使用tf.function装饰一下就能够获得静态图的所有优点。</p>
<h2 id="tf-function-de-shi-yong">tf.function 的使用</h2>
<h3 id="ji-chu">基础</h3>
<p><code>tf.function</code>可以定义一个<code>Tensorflow</code>操作，既可以命令式的执行它，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> a+b</span><br><span class="line"></span><br><span class="line">add(tf.ones([<span class="number">2</span>, <span class="number">2</span>]), tf.ones([<span class="number">2</span>, <span class="number">2</span>]))  <span class="comment">#  [[2., 2.], [2., 2.]]</span></span><br></pre></td></tr></table></figure>
<pre><code>&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[2., 2.],
       [2., 2.]], dtype=float32)&gt;
</code></pre>
<p>也可以在图中执行它，并求其梯度，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    result = add(v, <span class="number">1.0</span>)</span><br><span class="line">tape.gradient(result, v)</span><br></pre></td></tr></table></figure>
<pre><code>  &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt;
</code></pre>
<p>也可以定义嵌套定义（当然，在实际使用中，可以直接在顶层定义，会自动对子图进行转换），</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_layer</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> add(tf.matmul(x, w), b)</span><br><span class="line"></span><br><span class="line">dense_layer(tf.ones([<span class="number">3</span>, <span class="number">2</span>]), tf.ones([<span class="number">2</span>, <span class="number">2</span>]), tf.ones([<span class="number">2</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=
array([[3., 3.],
       [3., 3.],
       [3., 3.]], dtype=float32)&gt;
</code></pre>
<h3 id="zhui-zong-yu-duo-tai">追踪与多态</h3>
<p><code>Python</code> 的动态类型意味着用户可以传递多种类型的参数，这也许会导致函数产生不同的行为。</p>
<p>在另一方面，<code>Tensorflow</code>的静态图需要确定的数据类型和维度。<code>tf.function</code> 通过<code>retracing</code>函数调用来弥补这一差距，并在必要的时候产生正确的计算图。<code>tf.function</code>大多数微妙的行为都产生自<code>retracing</code>行为。</p>
<p>可以用不同的参数调用同一函数来观察<code>retracing</code>行为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Functions are polymorphic</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">double</span><span class="params">(a)</span>:</span></span><br><span class="line">  print(<span class="string">"Tracing with"</span>, a)</span><br><span class="line">  <span class="keyword">return</span> a + a</span><br><span class="line"></span><br><span class="line">print(double(tf.constant(<span class="number">1</span>)))</span><br><span class="line">print()</span><br><span class="line">print(double(tf.constant(<span class="number">1.1</span>)))</span><br><span class="line">print()</span><br><span class="line">print(double(tf.constant(<span class="string">"a"</span>)))</span><br><span class="line">print()</span><br></pre></td></tr></table></figure>
<pre><code>Tracing with Tensor(&quot;a:0&quot;, shape=(), dtype=int32)
tf.Tensor(2, shape=(), dtype=int32)

Tracing with Tensor(&quot;a:0&quot;, shape=(), dtype=float32)
tf.Tensor(2.2, shape=(), dtype=float32)

Tracing with Tensor(&quot;a:0&quot;, shape=(), dtype=string)
tf.Tensor(b'aa', shape=(), dtype=string)
</code></pre>
<p>如果希望控制 <code>tracing</code> 行为，可以用如下方式操作：</p>
<ul>
<li>创建新的 <code>tf.function</code>。分离 <code>tf.fucntion</code> 对象，保证没有共享的计算图引用。</li>
<li>使用 <code>get_concrete_function</code> 方法，得到特定的计算图。</li>
<li>声明 <code>input_signature</code> 当调用 <code>tf.function</code> 时，仅跟踪与输入签名一致的调用。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"Obtaining concrete trace"</span>)</span><br><span class="line">double_strings = double.get_concrete_function(tf.TensorSpec(shape=<span class="literal">None</span>, dtype=tf.string))</span><br><span class="line">print(<span class="string">"Executing traced function"</span>)</span><br><span class="line">print(double_strings(tf.constant(<span class="string">"a"</span>)))</span><br><span class="line">print(double_strings(a=tf.constant(<span class="string">"b"</span>)))</span><br><span class="line">print(<span class="string">"Using a concrete trace with incompatible types will throw an error"</span>)</span><br><span class="line"><span class="keyword">with</span> assert_raises(tf.errors.InvalidArgumentError):</span><br><span class="line">  double_strings(tf.constant(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">Obtaining concrete trace</span><br><span class="line">Tracing <span class="keyword">with</span> <span class="constructor">Tensor(<span class="string">"a:0"</span>, <span class="params">dtype</span>=<span class="params">string</span>)</span></span><br><span class="line">Executing traced <span class="keyword">function</span></span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">aa</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">bb</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line"></span><br><span class="line">Using a concrete trace <span class="keyword">with</span> incompatible types will throw an error</span><br><span class="line">Caught expected <span class="keyword">exception</span> </span><br><span class="line">  &lt;<span class="keyword">class</span> 'tensorflow.python.framework.errors_impl.InvalidArgumentError'&gt;:</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;ipython-input-3-73d0ca52e838&gt;"</span>, line <span class="number">8</span>, <span class="keyword">in</span> assert_raises</span><br><span class="line">    yield</span><br><span class="line">  File <span class="string">"&lt;ipython-input-8-5351d0a2eda2&gt;"</span>, line <span class="number">8</span>, <span class="keyword">in</span> &lt;<span class="keyword">module</span>&gt;</span><br><span class="line">    double<span class="constructor">_strings(<span class="params">tf</span>.<span class="params">constant</span>(1)</span>)</span><br><span class="line">tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute __inference_double_87 <span class="keyword">as</span> input #<span class="number">0</span>(zero-based) was expected <span class="keyword">to</span> be a <span class="built_in">string</span> tensor but is a <span class="built_in">int32</span> tensor <span class="literal">[O<span class="identifier">p</span>:<span class="identifier">__inference_double_87</span>]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32),))</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_collatz</span><span class="params">(x)</span>:</span></span><br><span class="line">  print(<span class="string">"Tracing with"</span>, x)</span><br><span class="line">  <span class="keyword">return</span> tf.where(x % <span class="number">2</span> == <span class="number">0</span>, x // <span class="number">2</span>, <span class="number">3</span> * x + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(next_collatz(tf.constant([<span class="number">1</span>, <span class="number">2</span>])))</span><br><span class="line"><span class="comment"># We specified a 1-D tensor in the input signature, so this should fail.</span></span><br><span class="line"><span class="keyword">with</span> assert_raises(ValueError):</span><br><span class="line">  next_collatz(tf.constant([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]))</span><br></pre></td></tr></table></figure>
<pre><code>Tracing with Tensor(&quot;x:0&quot;, shape=(None,), dtype=int32)
tf.Tensor([4 1], shape=(2,), dtype=int32)

Caught expected exception 
  &lt;class 'ValueError'&gt;:

Traceback (most recent call last):
  File &quot;&lt;ipython-input-3-73d0ca52e838&gt;&quot;, line 8, in assert_raises
    yield
  File &quot;&lt;ipython-input-9-9939c82c1507&gt;&quot;, line 9, in &lt;module&gt;
    next_collatz(tf.constant([[1, 2], [3, 4]]))
ValueError: Python inputs incompatible with input_signature:
  inputs: (
    tf.Tensor(
[[1 2]
 [3 4]], shape=(2, 2), dtype=int32))
  input_signature: (
    TensorSpec(shape=(None,), dtype=tf.int32, name=None))
</code></pre>
<h3 id="zhui-zong-hong-fa-de-shi-ji">追踪触发的时机</h3>
<p>多态函数 <code>tf.function</code> 会缓存之前追踪行为触发生成过的具体函数。缓存的键由传入的参数确定，对于 <code>tf.Tensor</code> 参数而言，是其维度和类型，而对于 <code>Python</code> 元语，是其值。对于其它 Python类型，使用对象 id，即对每个不同的类实例都会触发独立的追踪行为，并生成相应的静态图。</p>
<h3 id="shu-ru-can-shu-de-xuan-ze-python-or-tensor">输入参数的选择 Python or Tensor</h3>
<p>通常，<code>Python</code> 参数被用作超参数，如 <code>num_layers=10</code>、<code>training=True</code>以及<code>nonlinearity='relu'</code>。此时，<code>Python</code> 参数的改变触发 <code>retrace</code> 行为来构建新的计算图是合理的。然而，在另一些情况下，<code>Python</code> 参数并不改变计算图，是不需要触发 <code>retrace</code> 重新构建计算图的。例如，在训练过程中控制步数，AutoGraph 会自动动态展开，因此传入不同的步数，其生成图是一致的，这时如果触发多个 <code>trace</code> 生成同样的计算图，是很低效的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_one_step</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(num_steps)</span>:</span></span><br><span class="line">  print(<span class="string">"Tracing with num_steps = &#123;&#125;"</span>.format(num_steps))</span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> tf.range(num_steps):</span><br><span class="line">    train_one_step()</span><br><span class="line"></span><br><span class="line">train(num_steps=<span class="number">10</span>)</span><br><span class="line">train(num_steps=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Tracing with num_steps = 10
Tracing with num_steps = 20
</code></pre>
<p>一种简单的绕过方式是，将参数转换为 Tensor，这样不改变 shape，就不会触发生成计算图。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(num_steps=tf.constant(<span class="number">10</span>))</span><br><span class="line">train(num_steps=tf.constant(<span class="number">20</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Tracing with num_steps = Tensor(&quot;num_steps:0&quot;, shape=(), dtype=int32)
</code></pre>
<h3 id="tf-function-de-fu-dai-xiao-ying">tf.function 的附带效应</h3>
<p>通常，<code>Python</code> 的附带效应（<code>print</code> 或改变对象）仅发生在 <code>tracing</code> 行为中。何时应该触发 <code>tf.function</code> 的附带效应呢？</p>
<p>经验上推荐仅使用附带效应来 debug <code>trace</code> 行为。其它情况则建议使用 <code>Tensorflow</code> 运算如 <code>tf.Variable.assign</code>、<code>tf.print</code>和<code>tf.summary</code>来在 <code>Tensorflow runtime</code> 保证代码跟踪和执行。帮助调试的最佳实践是使用函数式风格。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">  print(<span class="string">"Traced with"</span>, x)</span><br><span class="line">  tf.print(<span class="string">"Executed with"</span>, x)</span><br><span class="line"></span><br><span class="line">f(<span class="number">1</span>)</span><br><span class="line">f(<span class="number">1</span>)</span><br><span class="line">f(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Traced with 1
Executed with 1
Executed with 1
Traced with 2
Executed with 2
</code></pre>
<p>如果想要在每次调用 <code>tf.function</code> 时执行 <code>Python</code> 代码，<code>tf.py_function</code> 提供了这种方式的支持。使用 <code>tf.py_function</code> 的一个缺点是性能，不能很好的工作在分布式环境下（多GPU/TPU）。由于 <code>tf.py_function</code> 需要被可微的连入计算图，输入和输出都会被转换为 <code>tf.Tensor</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">external_list = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">side_effect</span><span class="params">(x)</span>:</span></span><br><span class="line">  print(<span class="string">'Python side effect'</span>)</span><br><span class="line">  external_list.append(x)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">  tf.py_function(side_effect, inp=[x], Tout=[])</span><br><span class="line"></span><br><span class="line">f(<span class="number">1</span>)</span><br><span class="line">f(<span class="number">1</span>)</span><br><span class="line">f(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> len(external_list) == <span class="number">3</span></span><br><span class="line"><span class="comment"># .numpy() call required because py_function casts 1 to tf.constant(1)</span></span><br><span class="line"><span class="keyword">assert</span> external_list[<span class="number">0</span>].numpy() == <span class="number">1</span></span><br></pre></td></tr></table></figure>
<pre><code>Python side effect
Python side effect
Python side effect
</code></pre>
<h3 id="zhu-yi-python-de-zhuang-tai">注意 Python 的状态</h3>
<p>许多 Python 特性，如生成器和迭代器，依赖于 Python 运行时跟踪其状态。通常，这些构件在动态图模式下工作正常，但由于 <code>tracing</code> 行为，在 <code>tf.function</code> 内会发生预期外行为。</p>
<p>例如，迭代器状态被视为一种 Python 附带效应，因此只在 <code>tracing</code> 时触发一次。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">external_var = tf.Variable(<span class="number">0</span>)</span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">buggy_consume_next</span><span class="params">(iterator)</span>:</span></span><br><span class="line">  external_var.assign_add(next(iterator))</span><br><span class="line">  tf.print(<span class="string">"Value of external_var:"</span>, external_var)</span><br><span class="line"></span><br><span class="line">iterator = iter([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">buggy_consume_next(iterator)</span><br><span class="line"><span class="comment"># This reuses the first value from the iterator, rather than consuming the next value.</span></span><br><span class="line">buggy_consume_next(iterator)</span><br><span class="line">buggy_consume_next(iterator)</span><br></pre></td></tr></table></figure>
<pre><code>Value of external_var: 0
Value of external_var: 0
Value of external_var: 0
</code></pre>
<p>如果一个迭代器生成和消费完全在 <code>tf.function</code> 中，它会正确地工作，但会产生巨大地计算图，这也许不符合你的预期，更严重的是，对于以 Python List 表示的内存中大型数据集，相应的大型计算图也并不能带来性能提升。</p>
<p>如果想要在 <code>Python</code> 数据集上迭代，最安全的方式是使用 <code>tf.data.Dataset</code> 封装，并以 <code>for x in y</code> 方式遍历。AutoGraph 对 <code>for</code> 循环中 <code>y</code> 是一个 <code>tf.Tensor</code> 或 <code>tf.data.Dataset</code> 有特殊支持。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">measure_graph_size</span><span class="params">(f, *args)</span>:</span></span><br><span class="line">  g = f.get_concrete_function(*args).graph</span><br><span class="line">  print(<span class="string">"&#123;&#125;(&#123;&#125;) contains &#123;&#125; nodes in its graph"</span>.format(</span><br><span class="line">      f.__name__, <span class="string">', '</span>.join(map(str, args)), len(g.as_graph_def().node)))</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(dataset)</span>:</span></span><br><span class="line">  loss = tf.constant(<span class="number">0</span>)</span><br><span class="line">  <span class="keyword">for</span> x, y <span class="keyword">in</span> dataset:</span><br><span class="line">    loss += tf.abs(y - x) <span class="comment"># Some dummy computation.</span></span><br><span class="line">  <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">small_data = [(<span class="number">1</span>, <span class="number">1</span>)] * <span class="number">2</span></span><br><span class="line">big_data = [(<span class="number">1</span>, <span class="number">1</span>)] * <span class="number">10</span></span><br><span class="line">measure_graph_size(train, small_data)</span><br><span class="line">measure_graph_size(train, big_data)</span><br><span class="line"></span><br><span class="line">measure_graph_size(train, tf.data.Dataset.from_generator(</span><br><span class="line">    <span class="keyword">lambda</span>: small_data, (tf.int32, tf.int32)))</span><br><span class="line">measure_graph_size(train, tf.data.Dataset.from_generator(</span><br><span class="line">    <span class="keyword">lambda</span>: big_data, (tf.int32, tf.int32)))</span><br></pre></td></tr></table></figure>
<pre><code>train([(1, 1), (1, 1)]) contains 8 nodes in its graph
train([(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]) contains 32 nodes in its graph
train(&lt;FlatMapDataset shapes: (&lt;unknown&gt;, &lt;unknown&gt;), types: (tf.int32, tf.int32)&gt;) contains 9 nodes in its graph
train(&lt;FlatMapDataset shapes: (&lt;unknown&gt;, &lt;unknown&gt;), types: (tf.int32, tf.int32)&gt;) contains 9 nodes in its graph
</code></pre>
<p>当封装 <code>Python/Numpy</code> 数据为 <code>tf.data.Dataset</code> 时，要注意 <code>tf.data.Dataset.from_generator</code> 与 <code>tf.data.Dataset.from_tensors</code> 的区别。前者保持数据在 <code>Python runtime</code> 中，通过 <code>tf.py_function</code> 取数，会造成一定的性能瓶颈。而后者会复制一份到 <code>Tensorflow runtime</code> 成为计算图的一个 <code>tf.constant()</code> 节点，会占用更多的内存。</p>
<p>从文件中读数据，如 <code>TFRecordDataset/CsvDataset</code> 等，是读取数据最高效的方式，<code>Tensorflow</code> 可以自行管理异步读取和预加载数据，而不需要 <code>Python</code> 的参与。</p>
<h3 id="zi-dong-kong-zhi-yi-lai">自动控制依赖</h3>
<p>作为编程模型，<code>tf.function</code> 一个非常吸引人的特性是，在通常的数据流图之上，为运行时环境提供了更多关于代码预期行为的信息。</p>
<p>例如，当写代码是多次读写同一变量，数据流图也许不会编码原本预期的操作顺序。在 <code>tf.function</code> 中，则会依照在 <code>Python</code> 代码中的声明顺序消除执行顺序的歧义。这使得 <code>tf.function</code> 支持状态操作，可以复制动态图模式的语义。</p>
<p>这意味着不再需要手动添加控制依赖，而可以交由 <code>tf.function</code> 自动添加控制依赖。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Automatic control dependencies</span></span><br><span class="line"></span><br><span class="line">a = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line">b = tf.Variable(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x, y)</span>:</span></span><br><span class="line">  a.assign(y * b)</span><br><span class="line">  b.assign_add(x * a)</span><br><span class="line">  <span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line">f(<span class="number">1.0</span>, <span class="number">2.0</span>)  <span class="comment"># 10.0</span></span><br><span class="line">&lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">10.0</span>&gt;</span><br></pre></td></tr></table></figure>
<h3 id="bian-liang">变量</h3>
<p>变量也可能使动态图模式的执行结果和静态图模式产生差异。当每次调用创建一个新变量时，由于 <code>tracing</code> 语义，<code>tf.function</code> 会在每次调用时重用同一变量，但动态图时会在每次调用时新建相应的变量。为了避免类似的问题，<code>tf.function</code> 将在监测到危险的变量创建行为时抛出异常。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">  v = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line">  v.assign_add(x)</span><br><span class="line">  <span class="keyword">return</span> v</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> assert_raises(ValueError):</span><br><span class="line">  f(<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;ipython-input-17-73e410646579&gt;:3 f  *
    v = tf.Variable(1.0)
/tmpfs/src/tf_docs_env/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:260 __call__
    return cls._variable_v2_call(*args, **kwargs)
/tmpfs/src/tf_docs_env/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:254 _variable_v2_call
    shape=shape)
/tmpfs/src/tf_docs_env/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:65 getter
    return captured_getter(captured_previous, **kwargs)
/tmpfs/src/tf_docs_env/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py:502 invalid_creator_scope
    &quot;tf.function-decorated function tried to create &quot;

ValueError: tf.function-decorated function tried to create variables on non-first call.
</code></pre>
<p>而无歧义的代码就不会触发相应的行为</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> v.assign_add(x)</span><br><span class="line"></span><br><span class="line">print(f(<span class="number">1.0</span>))  <span class="comment"># 2.0</span></span><br><span class="line">print(f(<span class="number">2.0</span>))  <span class="comment"># 4.0</span></span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(2.0, shape=(), dtype=float32)
tf.Tensor(4.0, shape=(), dtype=float32)
</code></pre>
<p>只要可以证明<code>tf.function</code>中的变量只在函数初次执行时被创建，也是可以通过检查的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span>:</span></span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">obj = C()</span><br><span class="line">obj.v = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> obj.v <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    obj.v = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line">  <span class="keyword">return</span> obj.v.assign_add(x)</span><br><span class="line"></span><br><span class="line">print(g(<span class="number">1.0</span>))  <span class="comment"># 2.0</span></span><br><span class="line">print(g(<span class="number">2.0</span>))  <span class="comment"># 4.0</span></span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(2.0, shape=(), dtype=float32)
tf.Tensor(4.0, shape=(), dtype=float32)
</code></pre>
<h2 id="auto-graph">AutoGraph</h2>
<p><code>AutoGraph</code> 集成在 <code>tf.function</code> 中，用来重写依赖与张量的条件分支和循环，以支持计算图动态改变结构。</p>
<p><code>tf.cond</code> 和 <code>tf.while_loop</code> 依旧与 <code>tf.function</code> 兼容，但以命令式写控制流更简单直观。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Simple loop</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">while</span> tf.reduce_sum(x) &gt; <span class="number">1</span>:</span><br><span class="line">    tf.print(x)</span><br><span class="line">    x = tf.tanh(x)</span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">f(tf.random.uniform([<span class="number">5</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>[0.654489756 0.378447413 0.843570352 0.706569076 0.899703264]
[0.57468468 0.361358374 0.687695503 0.608520865 0.716153383]
[0.518791437 0.346409947 0.596499443 0.543085039 0.614520967]
[0.476766676 0.333187848 0.534554 0.495319664 0.54730171]
[0.443650424 0.321382284 0.488854468 0.458428353 0.498495191]
[0.416665703 0.310756236 0.453306764 0.428802431 0.460932881]
[0.394117773 0.301124901 0.424613386 0.40432 0.430844247]
[0.374904692 0.292341709 0.400809854 0.383639246 0.406026632]
[0.358274311 0.284288675 0.380641699 0.36586377 0.385093749]
[0.343693078 0.276869684 0.36326462 0.3503685 0.367122918]
[0.330770403 0.270005435 0.348086357 0.336702287 0.351472586]
[0.319212824 0.263629884 0.334677339 0.324530154 0.337680846]
[0.308794975 0.257687539 0.322717279 0.313597322 0.325405359]
[0.299340427 0.252131343 0.31196183 0.303706169 0.314386249]
[0.290708899 0.246921107 0.302220792 0.294700593 0.30442214]
[0.282787144 0.242022276 0.293343604 0.286455452 0.295354247]
[0.275482684 0.237404957 0.285209358 0.278869152 0.287055373]
[0.268719077 0.233043134 0.277719557 0.271858126 0.279422313]
[0.262432516 0.228914022 0.27079317 0.265352964 0.272370338]
[0.256569326 0.22499761 0.264362723 0.259295493 0.265829057]
[0.25108391 0.221276194 0.258371592 0.25363645 0.259739518]
[0.245937288 0.217734098 0.252771795 0.248333916 0.254051864]
[0.241095871 0.214357331 0.247522414 0.243351877 0.248723671]
[0.236530572 0.211133406 0.242588282 0.238659233 0.24371852]
[0.23221606 0.2080511 0.237939 0.234228939 0.239004955]
[0.228130147 0.205100328 0.233548105 0.230037391 0.234555662]
[0.224253282 0.202271983 0.229392484 0.226063833 0.230346799]
[0.220568195 0.199557811 0.225451797 0.222289979 0.226357415]
[0.217059553 0.196950331 0.221708104 0.218699604 0.222569034]
[0.213713691 0.194442704 0.21814549 0.215278283 0.218965292]
[0.210518375 0.192028716 0.214749783 0.212013125 0.215531647]
[0.207462624 0.189702675 0.211508334 0.208892584 0.21225509]
[0.204536542 0.18745935 0.208409771 0.205906287 0.209123984]
[0.201731205 0.185293958 0.205443889 0.203044847 0.206127867]

&lt;tf.Tensor: shape=(5,), dtype=float32, numpy=
array([0.19903852, 0.18320207, 0.20260146, 0.20029978, 0.20325728],
      dtype=float32)&gt;
</code></pre>
<p>如下代码可以观察 <code>autograph</code> 生成的代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">while</span> tf.reduce_sum(x) &gt; <span class="number">1</span>:</span><br><span class="line">    tf.print(x)</span><br><span class="line">    x = tf.tanh(x)</span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">print(tf.autograph.to_code(f))</span><br></pre></td></tr></table></figure>
<pre><code>def tf__f(x):
  do_return = False
  retval_ = ag__.UndefinedReturnValue()
  with ag__.FunctionScope('f', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:

    def get_state():
      return ()

    def set_state(_):
      pass

    def loop_body(x):
      ag__.converted_call(tf.print, (x,), None, fscope)
      x = ag__.converted_call(tf.tanh, (x,), None, fscope)
      return x,

    def loop_test(x):
      return ag__.converted_call(tf.reduce_sum, (x,), None, fscope) &gt; 1
    x, = ag__.while_stmt(loop_test, loop_body, get_state, set_state, (x,), ('x',), ())
    do_return = True
    retval_ = fscope.mark_return_value(x)
  do_return,
  return ag__.retval(retval_)
</code></pre>
<h3 id="auto-graph-tiao-jian-fen-zhi">AutoGraph：条件分支</h3>
<p><code>AutoGraph</code> 将 <code>if</code> 语句转换为等效的 <code>tf.cond</code> 调用。这一替换发生在条件变量为张量时，除此之外的条件，在 <code>tracing</code> 时确定。<code>test_tf_cond</code> 函数用来检查函数中是否使用了 <code>tf.cond</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_tf_cond</span><span class="params">(f, *args)</span>:</span></span><br><span class="line">  g = f.get_concrete_function(*args).graph</span><br><span class="line">  <span class="keyword">if</span> any(node.name == <span class="string">'cond'</span> <span class="keyword">for</span> node <span class="keyword">in</span> g.as_graph_def().node):</span><br><span class="line">    print(<span class="string">"&#123;&#125;(&#123;&#125;) uses tf.cond."</span>.format(</span><br><span class="line">        f.__name__, <span class="string">', '</span>.join(map(str, args))))</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"&#123;&#125;(&#123;&#125;) executes normally."</span>.format(</span><br><span class="line">        f.__name__, <span class="string">', '</span>.join(map(str, args))))</span><br><span class="line"></span><br><span class="line">  print(<span class="string">"  result: "</span>,f(*args).numpy())</span><br></pre></td></tr></table></figure>
<p>当参数为 python <code>True</code> 时，正常地执行条件：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(x, training=True)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> training:</span><br><span class="line">    x = tf.nn.dropout(x, rate=<span class="number">0.5</span>)</span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">test_tf_cond(dropout, tf.ones([<span class="number">10</span>], dtype=tf.float32), <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>dropout(tf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(10,), dtype=float32), True) executes normally.
  result:  [2. 2. 2. 2. 0. 0. 2. 0. 0. 2.]
</code></pre>
<p>但传递一个张量则会使 python <code>if</code> 替换为 <code>tf.cond</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_tf_cond(dropout, tf.ones([<span class="number">10</span>], dtype=tf.float32), tf.constant(<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<pre><code>dropout(tf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(10,), dtype=float32), tf.Tensor(True, shape=(), dtype=bool)) uses tf.cond.
  result:  [0. 2. 2. 2. 0. 0. 0. 0. 2. 2.]
</code></pre>
<h3 id="auto-graph-yu-xun-huan">AutoGraph 与循环</h3>
<p><code>AutoGraph</code> 有一些转换循环的简单规则。</p>
<ul>
<li><code>for</code>：迭代器是张量时转换</li>
<li><code>while</code>：循环条件与张量有关时转换</li>
</ul>
<p>如果一个循环被转换，它将由 <code>tf.while</code> 动态展开，或在 <code>for x in tf.data.Dataset</code> 情况下，将循环转换为 <code>tf.data.Dataset.reduce</code>。</p>
<p>如果循环没有被转换，则静态展开。</p>
<p><code>test_dynamically_unrolled(f, *args)</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_dynamically_unrolled</span><span class="params">(f, *args)</span>:</span></span><br><span class="line">  g = f.get_concrete_function(*args).graph</span><br><span class="line">  <span class="keyword">if</span> any(node.name == <span class="string">'while'</span> <span class="keyword">for</span> node <span class="keyword">in</span> g.as_graph_def().node):</span><br><span class="line">    print(<span class="string">"&#123;&#125;(&#123;&#125;) uses tf.while_loop."</span>.format(</span><br><span class="line">        f.__name__, <span class="string">', '</span>.join(map(str, args))))</span><br><span class="line">  <span class="keyword">elif</span> any(node.name == <span class="string">'ReduceDataset'</span> <span class="keyword">for</span> node <span class="keyword">in</span> g.as_graph_def().node):</span><br><span class="line">    print(<span class="string">"&#123;&#125;(&#123;&#125;) uses tf.data.Dataset.reduce."</span>.format(</span><br><span class="line">        f.__name__, <span class="string">', '</span>.join(map(str, args))))</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"&#123;&#125;(&#123;&#125;) gets unrolled."</span>.format(</span><br><span class="line">        f.__name__, <span class="string">', '</span>.join(map(str, args))))</span><br></pre></td></tr></table></figure>
<h4 id="for-xun-huan">For 循环</h4>
<p><code>tf.function</code> 的静态展开</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">for_in_range</span><span class="params">()</span>:</span></span><br><span class="line">  x = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    x += i</span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">test_dynamically_unrolled(for_in_range)</span><br></pre></td></tr></table></figure>
<pre><code>for_in_range() gets unrolled.
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">for_in_tfrange</span><span class="params">()</span>:</span></span><br><span class="line">  x = tf.constant(<span class="number">0</span>, dtype=tf.int32)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> tf.range(<span class="number">5</span>):</span><br><span class="line">    x += i</span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">test_dynamically_unrolled(for_in_tfrange)</span><br></pre></td></tr></table></figure>
<pre><code>for_in_tfrange() uses tf.while_loop.
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">for_in_tfdataset</span><span class="params">()</span>:</span></span><br><span class="line">  x = tf.constant(<span class="number">0</span>, dtype=tf.int64)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> tf.data.Dataset.range(<span class="number">5</span>):</span><br><span class="line">    x += i</span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">test_dynamically_unrolled(for_in_tfdataset)</span><br></pre></td></tr></table></figure>
<pre><code>for_in_tfdataset() uses tf.data.Dataset.reduce.
</code></pre>
<h4 id="while-xun-huan">While 循环</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">while_py_cond</span><span class="params">()</span>:</span></span><br><span class="line">  x = <span class="number">5</span></span><br><span class="line">  <span class="keyword">while</span> x &gt; <span class="number">0</span>:</span><br><span class="line">    x -= <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">test_dynamically_unrolled(while_py_cond)</span><br></pre></td></tr></table></figure>
<pre><code>while_py_cond() gets unrolled.
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">while_tf_cond</span><span class="params">()</span>:</span></span><br><span class="line">  x = tf.constant(<span class="number">5</span>)</span><br><span class="line">  <span class="keyword">while</span> x &gt; <span class="number">0</span>:</span><br><span class="line">    x -= <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">test_dynamically_unrolled(while_tf_cond)</span><br></pre></td></tr></table></figure>
<pre><code>while_tf_cond() uses tf.while_loop.
</code></pre>
<h1 id="yi-bu-ji-suan">异步计算</h1>
<p>Tensorflow使用异步计算来提升计算性能。理解它的工作原理既有助于开发更高效的程序，又有助于在内存资源有限的情况下主动降低计算性能从而减小内存开销。我们先导入本节中实验需要的包或模块。</p>
<h2 id="tensorflow-zhong-de-yi-bu-ji-suan">Tensorflow 中的异步计算</h2>
<p>广义上讲，<code>Tensorflow</code>包括用户直接用来交互的前端和系统用来执行计算的后端。例如，用户可以使用不同的前端语言编写<code>Tensorflow</code>程序，如<code>Python</code>、<code>C++</code>和<code>Javascript</code>。无论使用何种前端编程语言，<code>Tensorflow</code>程序的执行主要都发生在<code>C++</code>实现的后端。换句话说，用户写好的前端<code>Tensorflow</code>程序会传给后端执行计算。后端有自己的线程在队列中不断收集任务并执行它们。</p>
<p><code>Tensorflow</code>通过前端线程和后端线程的交互实现异步计算。异步计算指，前端线程无须等待当前指令从后端线程返回结果就继续执行后面的指令。为了便于解释，假设<code>Python</code>前端线程调用以下4条指令。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.ones((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">b = tf.ones((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">c = a * b + <span class="number">2</span></span><br><span class="line">c <span class="comment">#   &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[3., 3.]], dtype=float32)&gt;</span></span><br></pre></td></tr></table></figure>
<p>在异步计算中，<code>Python</code>前端线程执行前3条语句的时候，仅仅是把任务放进后端的队列里就返回了。当最后一条语句需要打印计算结果时，<code>Python</code>前端线程会等待<code>C++</code>后端线程把变量<code>c</code>的结果计算完。此设计的一个好处是，这里的Python前端线程不需要做实际计算。因此，无论<code>Python</code>的性能如何，它对整个程序性能的影响很小。只要<code>C++</code>后端足够高效，那么不管前端编程语言性能如何，<code>Tensorflow</code>都可以提供一致的高性能。</p>
<p>为了演示异步计算的性能，先实现一个简单的计时类。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Benchmark</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, prefix=None)</span>:</span></span><br><span class="line">    self.prefix = prefix + <span class="string">' '</span> <span class="keyword">if</span> prefix <span class="keyword">else</span> <span class="string">''</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.start = time.time()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">    print(<span class="string">'%stime: %.4f sec'</span> % (self.prefix, time.time() - self.start))</span><br></pre></td></tr></table></figure>
<p>下面的例子通过计时来展示<code>Tensorflow2.x</code>的计算行为。可以看到，当<code>y = tf.keras.backend.sum(tf.transpose(x) * x)</code>返回的时候需等待变量y真正被计算完，以便<code>pdb</code>在命令模式下调试。这里的行为不同于<code>MXNet</code>。在<code>MXNet</code>中，计算行为发送到C++后端，由<code>print</code>触发同步行为，等待完成计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'Workloads are queued.'</span>):</span><br><span class="line">  x = tf.random.uniform(shape=(<span class="number">2000</span>, <span class="number">2000</span>))</span><br><span class="line">  y = tf.keras.backend.sum(tf.transpose(x) * x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'Workloads are finished.'</span>):</span><br><span class="line">  print(<span class="string">'sum ='</span>, y)</span><br></pre></td></tr></table></figure>
<pre><code>    Workloads are queued. time: 0.0808 sec
    sum = tf.Tensor(999325.0, shape=(), dtype=float32)
    Workloads are finished. time: 0.0001 sec
</code></pre>
<p>的确，除非需要打印或者保存计算结果，否则基本无须关心目前结果在内存中是否已经计算好了。<code>Tensorflow</code>默认使用命令模式，如果需要提高性能，需要利用<code>tf.function</code>和<code>AutoGraph</code>创建比一行命令对应的单独命令节点更大的计算图，使<code>C++</code>后端更少和前端交互，从而获得更好的性能。</p>
<h2 id="yong-tong-bu-han-shu-rang-qian-duan-deng-dai-ji-suan-jie-guo">用同步函数让前端等待计算结果</h2>
<p>除了刚刚介绍的<code>print</code>函数外，<code>MXNet</code>还有其他方法让前端线程等待后端的计算结果完成。我们可以使用  <code>wait_to_read</code>函数让前端等待某个的<code>NDArray</code>的计算结果完成，再执行前端中后面的语句。或者，我们可以用<code>waitall</code>函数令前端等待前面所有计算结果完成。后者是性能测试中常用的方法。</p>
<h2 id="shi-yong-yi-bu-ji-suan-ti-sheng-ji-suan-xing-neng">使用异步计算提升计算性能</h2>
<p>在下面的例子中，我们用<code>for</code>循环不断对变量<code>y</code>赋值。当在<code>for</code>循环内执行<code>y = x + 1</code>时，每次赋值不使用异步计算；当在<code>for</code>循环外使用<code>tf.function</code>装饰时，则使用异步计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'synchronous.'</span>):</span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    y = x + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    y = x + <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'asynchronous.'</span>):</span><br><span class="line">  y = loop()</span><br></pre></td></tr></table></figure>
<pre><code>    synchronous. time: 3.5589 sec
    asynchronous. time: 1.0457 sec
</code></pre>
<p>观察到，使用异步计算能提升一定的计算性能。为了解释这一现象，对Python前端线程和C++后端线程的交互稍作简化。在每一次循环中，前端和后端的交互大约可以分为3个阶段：</p>
<ol>
<li>前端令后端将计算任务y = x + 1放进队列；</li>
<li>后端从队列中获取计算任务并执行真正的计算；</li>
<li>后端将计算结果返回给前端。</li>
</ol>
<p>将这3个阶段的耗时分别设为 t1,t2,t3 。如果不使用异步计算，执行1000次计算的总耗时大约为 1000(t1+t2+t3) ；如果使用异步计算，由于每次循环中前端都无须等待后端返回计算结果，执行1000次计算的总耗时可以降为 t1+1000t2+t3 （假设 1000t2&gt;999t1 ）。</p>
<h2 id="yi-bu-ji-suan-dui-nei-cun-de-ying-xiang">异步计算对内存的影响</h2>
<p>在实现的模型训练过程中，通常会在每个小批量上评测一下模型，如模型的损失或者精度。细心的读者也许已经发现了，而<code>keras model</code>的<code>compile</code>方法会隐式调用<code>tf.function</code>，触发<code>AutoGraph</code>，前端会在极短的时间内使后端生成完整的计算图，从而可能导致占用更多内存。当我们使用命令执行模式时，前端在每次迭代时仅会将一个小批量的任务丢给后端执行计算，并通常会减小内存占用。</p>
<p>由于深度学习模型通常比较大，而内存资源通常有限，建议在训练模型时对每个小批量操作使用<code>tf.function</code>函数，而不是整个训练过程。类似地，在使用模型预测时，为了减小内存的占用，也建议对每个小批量预测时都使用同步函数。</p>
<p>下面演示异步计算对内存的影响。先定义一个数据获取函数<code>data_iter</code>，它会从被调用时开始计时，并定期打印到目前为止获取数据批量的总耗时。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">()</span>:</span></span><br><span class="line">  start = time.time()</span><br><span class="line">  num_batches, batch_size = <span class="number">100</span>, <span class="number">1024</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(num_batches):</span><br><span class="line">    X = tf.random.normal(shape=(batch_size, <span class="number">512</span>))</span><br><span class="line">    y = tf.ones((batch_size,))</span><br><span class="line">    <span class="keyword">yield</span> X, y</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">      print(<span class="string">'batch %d, time %f sec'</span> % (i+<span class="number">1</span>, time.time()-start))</span><br></pre></td></tr></table></figure>
<p>下面定义多层感知机、优化算法和损失函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = keras.Sequential()</span><br><span class="line">net.add(keras.layers.Dense(<span class="number">2048</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">net.add(keras.layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">net.add(keras.layers.Dense(<span class="number">1</span>))</span><br><span class="line">optimizer=keras.optimizers.SGD(<span class="number">0.05</span>)</span><br><span class="line">loss = keras.losses.MeanSquaredError()</span><br></pre></td></tr></table></figure>
<p>这里定义辅助函数来监测内存的使用。</p>
<blockquote>
<p>这个函数只能在Linux或macOS上运行。需要支持<code>ps</code>指令</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_mem</span><span class="params">()</span>:</span></span><br><span class="line">  res = subprocess.check_output([<span class="string">'ps'</span>, <span class="string">'u'</span>, <span class="string">'-p'</span>, str(os.getpid())])</span><br><span class="line">  <span class="keyword">return</span> int(str(res).split()[<span class="number">15</span>]) / <span class="number">1e3</span></span><br></pre></td></tr></table></figure>
<p>对于训练模型<code>net</code>来说，可以自然地使用命令式方式实现。此时，每个小批量的生成间隔较长，不过内存开销较小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l_sum, mem = <span class="number">0</span>, get_mem()</span><br><span class="line">dense_1 = keras.layers.Dense(<span class="number">2048</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">dense_2 = keras.layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">dense_3 = keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line">trainable_variables = (dense_1.trainable_variables + </span><br><span class="line">                       dense_2.trainable_variables +</span><br><span class="line">                       dense_3.trainable_variables)</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter():</span><br><span class="line">  <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    logits = net(X)</span><br><span class="line">    loss_value = loss(y, logits)</span><br><span class="line"></span><br><span class="line">  grads = tape.gradient(loss_value, trainable_variables)</span><br><span class="line">  optimizer.apply_gradients(zip(grads, trainable_variables))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'increased memory: %f MB'</span> % (get_mem() - mem))</span><br></pre></td></tr></table></figure>
<pre><code>  batch 50, time 7.880550 sec
  batch 100, time 15.700529 sec
  increased memory: 14.336000 MB
</code></pre>
<p>如果转而使用预生成计算图，虽然每个小批量的生成间隔较短，但训练过程中可能会导致内存占用较高。这是因为在默认异步计算下，前端会将所有计算图在短时间内由后端完整生成。这使得在内存保存大量中间计算节点无法释放，从而占用额外内存。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l_sum, mem = <span class="number">0</span>, get_mem()</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter():</span><br><span class="line">  <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    logits = net(X)</span><br><span class="line">    loss_value = loss(y, logits)</span><br><span class="line"></span><br><span class="line">  grads = tape.gradient(loss_value, net.trainable_weights)</span><br><span class="line">  optimizer.apply_gradients(zip(grads, net.trainable_weights))</span><br></pre></td></tr></table></figure>
<pre><code>  batch 50, time 7.976524 sec
  batch 100, time 15.683179 sec
  increased memory: 12.268000 MB
</code></pre>
<h1 id="zi-dong-bing-xing-ji-suan">自动并行计算</h1>
<p>Tensorflow后端会自动构建计算图。通过计算图，系统可以知道所有计算的依赖关系，并可以选择将没有依赖关系的多个任务并行执行来获得计算性能的提升。例如“异步计算”第一个例子里依次执行了a = tf.ones((1, 2))和b = tf.ones((1, 2))。这两步计算之间并没有依赖关系，因此系统可以选择并行执行它们。</p>
<p>通常，一个运算符会用到所有CPU或单块GPU上全部的计算资源。例如，dot运算符会用到所有CPU（即使是一台机器上有多个CPU处理器）或单块GPU上所有的线程。如果每个运算符的计算量足够大，只在CPU上或者单块GPU上并行运行多个运算符时，每个运算符的运行只分到CPU或单块GPU上部分计算资源。即使这些计算可以并行，最终计算性能的提升可能也并不明显。</p>
<h2 id="cpu-he-gpu-de-bing-xing-ji-suan">CPU和GPU的并行计算</h2>
<p>程序中的计算既发生在CPU上，又发生在GPU上。先定义<code>run</code>函数，令它做10次矩阵乘法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> [tf.matmul(x, x) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br></pre></td></tr></table></figure>
<p>接下来，分别在CPU和GPU上创建<code>Tensor</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/CPU:0'</span>):</span><br><span class="line">  x_cpu = tf.random.uniform(shape=(<span class="number">2000</span>, <span class="number">2000</span>))</span><br><span class="line">  </span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/GPU:0'</span>):</span><br><span class="line">  x_gpu = tf.random.uniform(shape=(<span class="number">6000</span>, <span class="number">6000</span>))</span><br></pre></td></tr></table></figure>
<p>然后，分别使用它们在CPU和GPU上运行<code>run</code>函数并打印运行所需时间。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">run(x_cpu)</span><br><span class="line">run(x_gpu)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'Run on CPU.'</span>):</span><br><span class="line">  run(x_cpu)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'Then Run on GPU.'</span>):</span><br><span class="line">  run(x_gpu)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line">Run <span class="keyword">on</span> CPU. <span class="built_in">time</span>: <span class="number">1.2657</span> sec</span><br><span class="line">Then Run <span class="keyword">on</span> GPU. <span class="built_in">time</span>: <span class="number">0.0005</span> sec</span><br></pre></td></tr></table></figure>
<p>尝试系统能自动并行这两个任务：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'Run on both CPU and GPU in parallel.'</span>):</span><br><span class="line">  run(x_cpu)</span><br><span class="line">  run(x_gpu)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">Run on both CPU <span class="keyword">and</span> GPU <span class="keyword">in</span> parallel. time: <span class="number">1.2364</span> sec</span><br></pre></td></tr></table></figure>
<p>可以看到，当两个计算任务一起执行时，执行总时间小于它们分开执行的总和。这表明，Tensorflow能有效地在CPU和GPU上自动并行计算。</p>
<h2 id="ji-suan-he-tong-xin-de-bing-xing-ji-suan">计算和通信的并行计算</h2>
<p>在同时使用CPU和GPU的计算中，经常需要在内存和显存之间复制数据，造成数据的通信。在下面的例子中，在GPU上计算，然后将结果复制回CPU使用的内存。分别打印GPU上计算时间和显存到内存的通信时间。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">copy_to_cpu</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">with</span> tf.device(<span class="string">'/CPU:0'</span>):</span><br><span class="line">    <span class="keyword">return</span> [y <span class="keyword">for</span> y <span class="keyword">in</span> x]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'Run on GPU.'</span>):</span><br><span class="line">  y = run(x_gpu)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'Then copy to CPU.'</span>):</span><br><span class="line">  copy_to_cpu(y)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">Run on GPU. time: <span class="number">0.0047</span> sec</span><br><span class="line">Then copy to CPU. time: <span class="number">0.0007</span> sec</span><br></pre></td></tr></table></figure>
<p>打印这两个任务完成的总时间。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'Run and copy in parallel.'</span>):</span><br><span class="line">    y = run(x_gpu)</span><br><span class="line">    copy_to_cpu(y)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">Run <span class="keyword">and</span> copy <span class="keyword">in</span> parallel. time: <span class="number">0.0024</span> sec</span><br></pre></td></tr></table></figure>
<h1 id="zong-jie">总结</h1>
<ul>
<li>Tensorflow包括用户直接用来交互的前端和系统用来执行计算的后端。</li>
<li>Tensorflow能够通过生成更大规模的计算图，使后端异步计算时间更长，更少被打断，从而提升计算性能。</li>
<li>建议使用每个小批量训练或预测时以<code>batch</code>为单位生成计算图，从而避免在短时间内将过多计算任务丢给后端</li>
</ul>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://zh.d2l.ai/chapter_computational-performance/hybridize.html" target="_blank" rel="noopener">dive into deeplearning</a></li>
<li><a href="https://www.tensorflow.org/tutorials/customization/performance" target="_blank" rel="noopener">Better performance with tf.function</a></li>
<li><a href="https://github.com/tensorflow/community/blob/master/rfcs/20180918-functions-not-sessions-20.md" target="_blank" rel="noopener">Functions, not Sessions</a></li>
<li><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md" target="_blank" rel="noopener">AutoGraph Reference</a></li>
<li><a href="https://github.com/tensorflow/community/blob/master/rfcs/20190117-tf-module.md" target="_blank" rel="noopener">tf.Module</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/74441082" target="_blank" rel="noopener">Tensorflow2.0 学习笔记之静态图转换器</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/73575776" target="_blank" rel="noopener">Tensorflow2.0 学习笔记之状态容器</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/tensorflow</category>
      </categories>
  </entry>
  <entry>
    <title>数值稳定性和模型初始化</title>
    <url>/2020/07/16/deeplearning/numberical_stability_and_init/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/07/16/deeplearning/numberical_stability_and_init/1200px-Two_red_dice_01.svg.png" alt></p>
<a id="more"></a>
<h1 id="shuai-jian-he-bao-zha">衰减和爆炸</h1>
<p>深度模型有关数值稳定性的典型问题是衰减和爆炸。当神经网络的层数较多时，模型的数值稳定性容易变差。假设一个层数为\(L\)的多层感知机的第\(l\)层\(\boldsymbol{H}^{(l)}\)的权重参数为\(\boldsymbol{W}^{(l)}\)，输出层\(\boldsymbol{H}^{(L)}\)的权重参数为\(\boldsymbol{W}^{(L)}\)。不考虑偏差参数，设所有隐藏层的激活函数为恒等映射\(\phi(x) = x\)。给定输入\(\boldsymbol{X}\)，多层感知机的第\(l\)层的输出\(\boldsymbol{H}^{(l)} = \boldsymbol{X} \boldsymbol{W}^{(1)} \boldsymbol{W}^{(2)} \ldots \boldsymbol{W}^{(l)}\)。此时，如果层数\(l\)较大，\(\boldsymbol{H}^{(l)}\)的计算可能会出现衰减或爆炸。举个例子，假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入\(\boldsymbol{X}\)分别与\(0.2^{30} \approx 1 \times 10^{-21}\)（衰减）和\(5^{30} \approx 9 \times 10^{20}\)（爆炸）的乘积。类似地，当层数较多时，梯度的计算也更容易出现衰减或爆炸。</p>
<h1 id="sui-ji-chu-shi-hua-mo-xing-can-shu">随机初始化模型参数</h1>
<p>在神经网络中，通常需要随机初始化模型参数。</p>
<p>如下图所示结构，为了方便解释，假设输出层只保留一个输出单元\(o_1\)（删去\(o_2\)和\(o_3\)以及指向它们的箭头），且隐藏层使用相同的激活函数。如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。因此，正如在前面的实验中所做的那样，通常将神经网络的模型参数，特别是权重参数，进行随机初始化。</p>
<p><img src="/2020/07/16/deeplearning/numberical_stability_and_init/3.8_mlp.svg" alt></p>
<h2 id="tensorflow-2-0-de-mo-ren-sui-ji-chu-shi-hua">Tensorflow2.0的默认随机初始化</h2>
<p>随机初始化模型参数的方法有很多。如使用<code>kernel_initializer=init.RandomNormal(stddev=0.01)</code>使模型<code>model</code>的权重参数采用正态分布的随机初始化方式。不过，Tensorflow中<code>initializers</code>的模块参数都采取了较为合理的初始化策略，因此一般不用我们考虑。</p>
<h2 id="xavier-sui-ji-chu-shi-hua">Xavier随机初始化</h2>
<p>还有一种比较常用的随机初始化方法叫作Xavier随机初始化[1]。假设某全连接层的输入个数为\(a\)，输出个数为\(b\)，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布</p>
<p>\[
U\left(-\sqrt{\frac{6}{a+b}}, \sqrt{\frac{6}{a+b}}\right)
\]</p>
<p>它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。</p>
<h1 id="xiao-jie">小结</h1>
<ul>
<li>深度模型有关数值稳定性的典型问题是衰减和爆炸。当神经网络的层数较多时，模型的数值稳定性容易变差。</li>
<li>通常需要随机初始化神经网络的模型参数，如权重参数。</li>
</ul>
<h1 id="can-kao-wen-xian">参考文献</h1>
<p>[1] Glorot, X., &amp; Bengio, Y. (2010, March). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249-256).</p>
]]></content>
      <categories>
        <category>技术/深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>事件抽取模型复现核心代码</title>
    <url>/2020/06/26/research/ee_model/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/06/26/research/ee_model/image-20200626162628002.png" alt></p>
<a id="more"></a>
<h1 id="plmee">plmee</h1>
<p>plmee事件抽取用于裁判文书事件抽取</p>
<h2 id="hong-fa-qi-de-chou-qu">触发器的抽取</h2>
<p>触发器抽取器的目的是预测出触发了事件的token，形式化为token级别的多类别分类任务，分类标签是事件类型。在BERT上添加一个多类分类器就构成了触发器抽取器。</p>
<p>触发器抽取器的输入和BERT的一样，是WordPiece嵌入、位置嵌入和segment嵌入的和。因为输入只有一个句子，所以所有的segment ids设为0。句子首尾的token分别是[CLS]和[SEP]。</p>
<p>触发词有时不是一个单词而是一个词组。因此，作者令连续的tokens共享同一个预测标签，作为一个完整的触发词。</p>
<p>采用交叉熵损失函数用于微调（fine-tune）。</p>
<p><img src="/2020/06/26/research/ee_model/image-20200626162628002.png" alt></p>
<h3 id="mo-xing">模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TriggerExtractor</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bert_path, bert_train, dropout,event_type_num)</span>:</span></span><br><span class="line">        super(TriggerExtractor,self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="comment"># 对bert进行训练</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.bert.named_parameters():</span><br><span class="line">            param.requires_grad = bert_train</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.taggers = nn.ModuleList([nn.Linear(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],<span class="number">2</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(event_type_num)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text_lengths, text_ids, masks)</span>:</span></span><br><span class="line">        bert_out,bert_cls = self.bert(text_ids,attention_mask=masks)</span><br><span class="line">        bert_out = self.dropout(bert_out)</span><br><span class="line">        outputs = []</span><br><span class="line">        <span class="keyword">for</span> tagger <span class="keyword">in</span> self.taggers:</span><br><span class="line">            out = tagger(bert_out).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">            outputs.append(out[:,<span class="number">1</span>:,:])</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs,dim=<span class="number">-2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="ping-ce-zhi-biao">评测指标</h3>
<ol>
<li>位置正确</li>
<li>位置正确且类别正确</li>
</ol>
<h2 id="yuan-su-de-chou-qu">元素的抽取</h2>
<p>给定触发器的条件下，元素抽取器的目的是抽取出和触发器所对应事件相关的元素，以及这些元素扮演的角色。</p>
<p>和触发器抽取相比较，元素的抽取更加复杂，主要有3个原因：</p>
<ol>
<li>元素对触发器的依赖；</li>
<li>大多数元素是较长的名词短语；</li>
<li>角色重叠问题。</li>
</ol>
<p>和触发器抽取器一样，元素抽取器也需要3种嵌入相加作为输入，但还需要知道哪些tokens组成了触发器，因此特征表示输入的segment将触发词所在的span设为1。</p>
<p>为了克服元素抽取面临的后2个问题，作者在BERT上添加了多组二类分类器（多组分类器设为所有角色标签的集合，对每个元素判断所有类型角色的概率）。每组分类器服务于一个角色，以确定所有属于它的元素的范围（span：每个span都包括start和end）。</p>
<p>由于预测和角色是分离的，所以一个元素可以扮演多个角色（对一个元素使用一组二类分类器，一组二类分类器中有多个分类器，对应多个角色），一个token可以属于不同的元素。这就缓解了角色重叠问题。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArgumentExtractor</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bert_path, bert_train, role_type_num, dropout)</span>:</span></span><br><span class="line">        super(ArgumentExtractor, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="comment"># 对bert进行训练</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.bert.named_parameters():</span><br><span class="line">            param.requires_grad = bert_train</span><br><span class="line"></span><br><span class="line">        self.start_taggers = nn.ModuleList(</span><br><span class="line">            [nn.Linear(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>], <span class="number">2</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(role_type_num)])</span><br><span class="line">        self.end_taggers = nn.ModuleList(</span><br><span class="line">            [nn.Linear(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>], <span class="number">2</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(role_type_num)])</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text_lengths, text_ids, masks, type_ids)</span>:</span></span><br><span class="line">        bert_out, bert_cls = self.bert(text_ids, attention_mask=masks, token_type_ids=type_ids)</span><br><span class="line">        bert_out = self.dropout(bert_out)</span><br><span class="line">        start_outputs = []</span><br><span class="line">        end_outputs = []</span><br><span class="line">        <span class="keyword">for</span> start_tagger, end_tagger <span class="keyword">in</span> zip(self.start_tagger, self.end_taggers):</span><br><span class="line">            start_out = start_tagger(bert_out).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">            end_out = end_tagger(bert_out).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">            start_outputs.append(start_out[:, <span class="number">1</span>:, :])</span><br><span class="line">            end_outputs.append(end_out[:, <span class="number">1</span>:, :])</span><br><span class="line">        <span class="keyword">return</span> torch.cat(start_outputs, dim=<span class="number">-2</span>), torch.cat(end_outputs, dim=<span class="number">-2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="ping-ce-zhi-biao-1">评测指标</h3>
<ol>
<li>位置正确</li>
<li>位置正确且类别正确</li>
</ol>
<h2 id="af-ief">AF-IEF</h2>
<p>Role Frequency （RF）<br>
RF定义为角色r在类型为v的事件中出现的频率：<br>
\[
\operatorname{RF}(r, v)=\frac{N_{v}^{r}}{\sum_{k \in \mathcal{R}} N_{v}^{k}}
\]<br>
Inverse Event Frequency （IEF）</p>
<p>log内的分母表示论元角色r在多少个事件从出现<br>
\[
\operatorname{IEF}(r)=\log \frac{|\mathcal{V}|}{|\{v \in \mathcal{V}: r \in v\}|}
\]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_AF_IEF</span><span class="params">(self, dataset)</span>:</span></span><br><span class="line">        event_num = len(self.schema.event_type_2_id)</span><br><span class="line">        role_num = len(self.schema.role_type_2_id)</span><br><span class="line"></span><br><span class="line">        self.rf = np.zeros((event_num, role_num), dtype=float)</span><br><span class="line">        self.ief = np.zeros((role_num, ), dtype=float)</span><br><span class="line">    </span><br><span class="line">         <span class="keyword">with</span> open(os.path.join(self.data_dir, file), <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">             <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                json_item = json.loads(line)</span><br><span class="line">                <span class="keyword">for</span> event <span class="keyword">in</span> json_item[<span class="string">'events'</span>].values():</span><br><span class="line">                    event_id = self.schema.event_type_2_id[event[<span class="string">'event_type'</span>]]</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">for</span> argu <span class="keyword">in</span> event[<span class="string">'argument'</span>]:</span><br><span class="line">                        role_id = self.schema.role_type_2_id[argu[<span class="string">'role_type'</span>]]</span><br><span class="line">                        self.rf[event_id, role_id] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> idx <span class="keyword">in</span> range(role_num):</span><br><span class="line">                    self.ief[idx] = np.log(event_num/np.sum(self.af[:,idx] != <span class="number">0</span>))</span><br><span class="line">                role_count_per_event = np.sum(self.rf, axis=<span class="number">1</span>)</span><br><span class="line">                self.rf = self.rf / (role_count_per_event+<span class="number">1e-13</span>)</span><br></pre></td></tr></table></figure>
<p>将\(RF(r,v)\)和\(IEF(r)\)相乘得到\(RF−IEF(r,v)\)，使用RF-IEF度量角色r对于v类型事件的重要性：</p>
<p>\[
I(r, v)=\frac{\exp ^{\operatorname{RF}-\operatorname{IEF}}(r, v)}{\sum_{r^{\prime} \in R} \exp ^{\operatorname{RF}-\operatorname{IEF}}\left(r^{\prime}, v\right)}
\]<br>
给定输入的事件类型\(v\)，根据每个角色对于\(v\)类型事件的重要性，计算损失\(L_s\)和\(L_e\) ，将两者取平均就得到最终的损失。<br>
\[
\begin{aligned}
\mathcal{L}_{s} &amp;=\sum_{r \in \mathcal{R}} \frac{I(r, v)}{|\mathcal{S}|} \mathrm{CE}\left(P_{s}^{r}, \boldsymbol{y}_{s}^{r}\right) \\
\mathcal{L}_{e} &amp;=\sum_{r \in \mathcal{R}} \frac{I(r, v)}{|\mathcal{S}|} \mathrm{CE}\left(P_{e}^{r}, \boldsymbol{y}_{e}^{r}\right)
\end{aligned}
\]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rf_ief = ief * rf</span><br><span class="line">batch_weight = torch.exp(rf_ief)</span><br><span class="line">batch_weight_sum = torch.sum(batch_weight)</span><br><span class="line">batch_weight = batch_weight / batch_weight_sum</span><br><span class="line">batch_loss = batch_loss * batch_weight</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>技术/论文复现</category>
      </categories>
      <tags>
        <tag>信息抽取</tag>
        <tag>事件抽取</tag>
      </tags>
  </entry>
  <entry>
    <title>词向量</title>
    <url>/2020/06/17/word_embedding/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/06/17/word_embedding/Word2Vec-Training-Models.png" alt></p>
<a id="more"></a>
<h1 id="ci-qian-word-2-vec">词嵌⼊（word2vec）</h1>
<p>⾃然语⾔是⼀套⽤来表达含义的复杂系统。在这套系统中，词是表义的基本单元。顾名思义，词向量是⽤来表⽰词的向量，也可被认为是词的特征向量或表征。把词映射为实数域向量的技术也叫词嵌⼊（word embedding）。</p>
<h2 id="wei-he-bu-cai-one-hot-xiang-liang">为何不采⽤one-hot向量</h2>
<p>假设词典中不同词的数量（词典⼤小）为\(N\)，每个词可以和从\(0\)到\(N −1\)的连续整数⼀⼀对应。这些与词对应的整数叫作词的索引。假设⼀个词的索引为\(i\)，为了得到该词的one-hot向量表⽰，我们创建⼀个全0的⻓为\(N\)的向量，并将其第\(i\)位设成1。这样⼀来，每个词就表⽰成了⼀个⻓度为\(N\)的向量，可以直接被神经⽹络使⽤。</p>
<p>虽然one-hot词向量构造起来很容易，但通常并不是⼀个好选择。⼀个主要的原因是，one-hot词向量⽆法准确表达不同词之间的相似度，如我们常常使⽤的余弦相似度。对于向量\(x,y ∈ R^d\) ，它们的余弦相似度是它们之间夹⻆的余弦值:</p>
<p>\[
\frac{\boldsymbol{x}^\top \boldsymbol{y}}{\|\boldsymbol{x}\| \|\boldsymbol{y}\|} \in [-1, 1].
\]</p>
<p>由于任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度难以通过one-hot向量准确地体现出来。</p>
<p>word2vec工具的提出正是为了解决上面这个问题。它将每个词表示成一个定长的向量，并使得这些向量能较好地表达不同词之间的相似和类比关系。word2vec工具包含了两个模型，即跳字模型(skip-gram)和连续词袋模型(continuous bag of words，CBOW)。</p>
<h2 id="skip-gram">skip-gram</h2>
<h3 id="yuan-li">原理</h3>
<p>跳字模型假设基于某个词来生成它在文本序列周围的词。举个例子，假设文本序列是the man loves his son。以“loves”作为中心词，设背景窗口大小为2。如下图所示，跳字模型所关心的是，给定中心词“loves”，生成与它距离不超过2个词的背景词the、man、his、son的条件概率，即</p>
<p>\[
P(\textrm{the},\textrm{man},\textrm{his},\textrm{son}\mid\textrm{loves})
\]</p>
<p>假设给定中心词的情况下，背景词的生成是相互独立的，那么上式可以改写成</p>
<p>\[
P(\textrm{the}\mid\textrm{loves})\cdot P(\textrm{man}\mid\textrm{loves})\cdot P(\textrm{his}\mid\textrm{loves})\cdot P(\textrm{son}\mid\textrm{loves})
\]</p>
<p><img src="/2020/06/17/word_embedding/skip-gram.svg" alt></p>
<p>在跳字模型中，每个词被表示成两个\(d\)维向量，用来计算条件概率。假设这个词在词典中索引为\(i\)，当它为中心词时向量表示为\(\boldsymbol{v}_i\in\mathbb{R}^d\)，而为背景词时向量表示为\(\boldsymbol{u}_i\in\mathbb{R}^d\)。设中心词\(w_c\)在词典中索引为\(c\)，背景词\(w_o\)在词典中索引为\(o\)，给定中心词生成背景词的条件概率可以通过对向量内积做softmax运算而得到：</p>
<p>\[
P(w_o \mid w_c) = \frac{\text{exp}(\boldsymbol{u}_o^\top \boldsymbol{v}_c)}{ \sum_{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}
\]</p>
<p>其中词典索引集\(\mathcal{V} = \{0, 1, \ldots, |\mathcal{V}|-1\}\)。假设给定一个长度为\(T\)的文本序列，设时间步\(t\)的词为\(w^{(t)}\)。假设给定中心词的情况下背景词的生成相互独立，当背景窗口大小为\(m\)时，跳字模型的似然函数即给定任一中心词生成所有背景词的概率：</p>
<p>\[
\prod_{t=1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} P(w^{(t+j)} \mid w^{(t)})
\]</p>
<p>这里小于1或大于\(T\)的时间步可以被忽略。</p>
<h3 id="xun-lian-skip-gram">训练skip-gram</h3>
<p>跳字模型的参数是每个词所对应的中心词向量和背景词向量。训练中我们通过最大化似然函数来学习模型参数，即最大似然估计。这等价于最小化以下损失函数：</p>
<p>\[
-\sum_{t=1}^{T} \sum_{-m \leq j \leq m,\ j \neq 0} \text{log}\, P(w^{(t+j)} \mid w^{(t)})
\]</p>
<p>如果使用随机梯度下降，那么在每一次迭代里，随机采样一个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数。梯度计算的关键是条件概率的对数有关中心词向量和背景词向量的梯度。根据定义，</p>
<p>\[
\log P(w_o \mid w_c) =
\boldsymbol{u}_o^\top \boldsymbol{v}_c - \log\left(\sum_{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)\right)
\]</p>
<p>通过微分，我们可以得到上式中\(\boldsymbol{v}_c\)的梯度</p>
<p>\[
\begin{aligned}
\frac{\partial \text{log}\, P(w_o \mid w_c)}{\partial \boldsymbol{v}_c} 
&amp;= \boldsymbol{u}_o - \frac{\sum_{j \in \mathcal{V}} \exp(\boldsymbol{u}_j^\top \boldsymbol{v}_c)\boldsymbol{u}_j}{\sum_{i \in \mathcal{V}} \exp(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}\\
&amp;= \boldsymbol{u}_o - \sum_{j \in \mathcal{V}} \left(\frac{\text{exp}(\boldsymbol{u}_j^\top \boldsymbol{v}_c)}{ \sum_{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}\right) \boldsymbol{u}_j\\ 
&amp;= \boldsymbol{u}_o - \sum_{j \in \mathcal{V}} P(w_j \mid w_c) \boldsymbol{u}_j.
\end{aligned}
\]</p>
<p>它的计算需要词典中所有词以\(w_c\)为中心词的条件概率。有关其他词向量的梯度同理可得。</p>
<p>训练结束后，对于词典中的任一索引为\(i\)的词，均得到该词作为中心词和背景词的两组词向量\(\boldsymbol{v}_i\)和\(\boldsymbol{u}_i\)。在自然语言处理应用中，一般使用跳字模型的<strong>中心词向量作为词的表征向量</strong>。</p>
<h2 id="cbow">CBOW</h2>
<h3 id="yuan-li-1">原理</h3>
<p>连续词袋模型与跳字模型类似。与跳字模型最大的不同在于，连续词袋模型假设基于某中心词在文本序列前后的背景词来生成该中心词。在同样的文本序列the man loves his son里，以“loves”作为中心词，且背景窗口大小为2时，连续词袋模型关心的是，给定背景词the、man、his、son生成中心词“loves”的条件概率，也就是</p>
<p>\[
P(\textrm{loves}\mid\textrm{the},\textrm{man},\textrm{his},\textrm{son})
\]</p>
<p><img src="/2020/06/17/word_embedding/cbow.svg" alt></p>
<p>因为连续词袋模型的背景词有多个，将这些背景词向量取<strong>平均</strong>，然后使用和跳字模型一样的方法来计算条件概率。设\(\boldsymbol{v_i}\in\mathbb{R}^d\)和\(\boldsymbol{u_i}\in\mathbb{R}^d\)分别表示词典中索引为\(i\)的词作为背景词和中心词的向量（注意符号的含义与跳字模型中的相反）。设中心词\(w_c\)在词典中索引为\(c\)，背景词\(w_{o_1}, \ldots, w_{o_{2m}}\)在词典中索引为\(o_1, \ldots, o_{2m}\)，那么给定背景词生成中心词的条件概率</p>
<p>\[
P(w_c \mid w_{o_1}, \ldots, w_{o_{2m}}) = \frac{\text{exp}\left(\frac{1}{2m}\boldsymbol{u}_c^\top (\boldsymbol{v}_{o_1} + \ldots + \boldsymbol{v}_{o_{2m}}) \right)}{ \sum_{i \in \mathcal{V}} \text{exp}\left(\frac{1}{2m}\boldsymbol{u}_i^\top (\boldsymbol{v}_{o_1} + \ldots + \boldsymbol{v}_{o_{2m}}) \right)}
\]</p>
<p>为了让符号更加简单，我们记\(\mathcal{W}_o= \{w_{o_1}, \ldots, w_{o_{2m}}\}\)，且\(\bar{\boldsymbol{v}}_o = \left(\boldsymbol{v}_{o_1} + \ldots + \boldsymbol{v}_{o_{2m}} \right)/(2m)\)，那么上式可以简写成</p>
<p>\[
P(w_c \mid \mathcal{W}_o) = \frac{\exp\left(\boldsymbol{u}_c^\top \bar{\boldsymbol{v}}_o\right)}{\sum_{i \in \mathcal{V}} \exp\left(\boldsymbol{u}_i^\top \bar{\boldsymbol{v}}_o\right)}
\]</p>
<p>给定一个长度为\(T\)的文本序列，设时间步\(t\)的词为\(w^{(t)}\)，背景窗口大小为\(m\)。连续词袋模型的似然函数是由背景词生成任一中心词的概率<br>
\[
\prod_{t=1}^{T}  P(w^{(t)} \mid  w^{(t-m)}, \ldots,  w^{(t-1)},  w^{(t+1)}, \ldots,  w^{(t+m)})
\]</p>
<h3 id="xun-lian-cbow">训练CBOW</h3>
<p>训练连续词袋模型同训练跳字模型基本一致。连续词袋模型的最大似然估计等价于最小化损失函数</p>
<p>\[
-\sum_{t=1}^T  \text{log}\, P(w^{(t)} \mid  w^{(t-m)}, \ldots,  w^{(t-1)},  w^{(t+1)}, \ldots,  w^{(t+m)})
\]</p>
<p>注意到</p>
<p>\[
\log\,P(w_c \mid \mathcal{W}_o) = \boldsymbol{u}_c^\top \bar{\boldsymbol{v}}_o - \log\,\left(\sum_{i \in \mathcal{V}} \exp\left(\boldsymbol{u}_i^\top \bar{\boldsymbol{v}}_o\right)\right)
\]</p>
<p>通过微分，可以计算出上式中条件概率的对数有关任一背景词向量\(\boldsymbol{v}_{o_i}\)（\(i = 1, \ldots, 2m\)）的梯度</p>
<p>\[
\frac{\partial \log\, P(w_c \mid \mathcal{W}_o)}{\partial \boldsymbol{v}_{o_i}} = \frac{1}{2m} \left(\boldsymbol{u}_c - \sum_{j \in \mathcal{V}} \frac{\exp(\boldsymbol{u}_j^\top \bar{\boldsymbol{v}}_o)\boldsymbol{u}_j}{ \sum_{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \bar{\boldsymbol{v}}_o)} \right) = \frac{1}{2m}\left(\boldsymbol{u}_c - \sum_{j \in \mathcal{V}} P(w_j \mid \mathcal{W}_o) \boldsymbol{u}_j \right)
\]</p>
<p>有关其他词向量的梯度同理可得。同跳字模型不一样的一点在于，一般使用连续词袋模型的<strong>背景词向量作为词的表征向量</strong>。</p>
<h2 id="jin-si-xun-lian">近似训练</h2>
<p>跳字模型的核心在于使用softmax运算得到给定中心词\(w_c\)来生成背景词\(w_o\)的条件概率</p>
<p>\[
P(w_o \mid w_c) = \frac{\text{exp}(\boldsymbol{u}_o^\top \boldsymbol{v}_c)}{ \sum_{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}
\]</p>
<p>该条件概率相应的对数损失</p>
<p>\[
-\log P(w_o \mid w_c) =
-\boldsymbol{u}_o^\top \boldsymbol{v}_c + \log\left(\sum_{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)\right)
\]</p>
<p>由于softmax运算考虑了背景词可能是词典\(\mathcal{V}\)中的任一词，以上损失包含了词典大小数目的项的累加。不论是跳字模型还是连续词袋模型，由于条件概率使用了softmax运算，每一步的梯度计算都包含词典大小数目的项的累加。对于含几十万或上百万词的较大词典，每次的梯度计算开销可能过大。为了降低该计算复杂度，介绍两种近似训练方法，即负采样或层序softmax。由于跳字模型和连续词袋模型类似，仅以跳字模型为例介绍这两种方法。</p>
<h3 id="fu-cai-yang">负采样</h3>
<p>负采样修改了原来的<strong>目标函数</strong>。给定中心词\(w_c\)的一个背景窗口，把背景词\(w_o\)出现在该背景窗口看作一个事件，并将该事件的概率计算为</p>
<p>\[
P(D=1\mid w_c, w_o) = \sigma(\boldsymbol{u}_o^\top \boldsymbol{v}_c)
\]</p>
<p>其中的\(\sigma\)函数与sigmoid激活函数的定义相同：</p>
<p>\[
\sigma(x) = \frac{1}{1+\exp(-x)}
\]</p>
<p>先考虑最大化文本序列中所有该事件的联合概率来训练词向量。具体来说，给定一个长度为\(T\)的文本序列，设时间步\(t\)的词为\(w^{(t)}\)且背景窗口大小为\(m\)，考虑最大化联合概率</p>
<p>\[
\prod_{t=1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} P(D=1\mid w^{(t)}, w^{(t+j)})
\]</p>
<p>然而，以上模型中包含的事件仅考虑了正类样本。这导致当所有词向量相等且值为无穷大时，以上的联合概率才被最大化为1。很明显，这样的词向量毫无意义。负采样通过采样并添加负类样本使目标函数更有意义。设背景词\(w_o\)出现在中心词\(w_c\)的一个背景窗口为事件\(P\)，我们根据分布\(P(w)\)采样\(K\)个未出现在该背景窗口中的词，即噪声词。设噪声词\(w_k\)（\(k=1, \ldots, K\)）不出现在中心词\(w_c\)的该背景窗口为事件\(N_k\)。假设同时含有正类样本和负类样本的事件\(P, N_1, \ldots, N_K\)相互独立，负采样将以上需要最大化的仅考虑正类样本的联合概率改写为</p>
<p>\[
\prod_{t=1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} P(w^{(t+j)} \mid w^{(t)})
\]</p>
<p>其中条件概率被近似表示为<br>
\[
P(w^{(t+j)} \mid w^{(t)}) =P(D=1\mid w^{(t)}, w^{(t+j)})\prod_{k=1,\ w_k \sim P(w)}^K P(D=0\mid w^{(t)}, w_k)
\]</p>
<p>设文本序列中时间步\(t\)的词\(w^{(t)}\)在词典中的索引为\(i_t\)，噪声词\(w_k\)在词典中的索引为\(h_k\)。有关以上条件概率的对数损失为</p>
<p>\[
\begin{aligned}
-\log P(w^{(t+j)} \mid w^{(t)})
=&amp; -\log P(D=1\mid w^{(t)}, w^{(t+j)}) - \sum_{k=1,\ w_k \sim P(w)}^K \log P(D=0\mid w^{(t)}, w_k)\\
=&amp;-  \log\, \sigma\left(\boldsymbol{u}_{i_{t+j}}^\top \boldsymbol{v}_{i_t}\right) - \sum_{k=1,\ w_k \sim P(w)}^K \log\left(1-\sigma\left(\boldsymbol{u}_{h_k}^\top \boldsymbol{v}_{i_t}\right)\right)\\
=&amp;-  \log\, \sigma\left(\boldsymbol{u}_{i_{t+j}}^\top \boldsymbol{v}_{i_t}\right) - \sum_{k=1,\ w_k \sim P(w)}^K \log\sigma\left(-\boldsymbol{u}_{h_k}^\top \boldsymbol{v}_{i_t}\right).
\end{aligned}
\]</p>
<p>现在，训练中每一步的梯度计算开销不再与词典大小相关，而与\(K\)线性相关。当\(K\)取较小的常数时，负采样在每一步的梯度计算开销较小。</p>
<h3 id="ceng-xu-softmax">层序softmax</h3>
<p>层序softmax是另一种近似训练法。它使用了二叉树这一数据结构，树的每个叶结点代表词典\(\mathcal{V}\)中的每个词。</p>
<p><img src="/2020/06/17/word_embedding/10.2_hi-softmax.svg" alt="层序softmax,二叉树的每个叶结点代表着词典的每个词"></p>
<p>假设\(L(w)\)为从二叉树的根结点到词\(w\)的叶结点的路径（包括根结点和叶结点）上的结点数。设\(n(w,j)\)为该路径上第\(j\)个结点，并设该结点的背景词向量为\(\boldsymbol{u}_{n(w,j)}\)。以上图为例，\(L(w_3) = 4\)。层序softmax将跳字模型中的条件概率近似表示为<br>
\[
P(w_o \mid w_c) = \prod_{j=1}^{L(w_o)-1} \sigma\left( [\![  n(w_o, j+1) = \text{leftChild}(n(w_o,j)) ]\!] \cdot \boldsymbol{u}_{n(w_o,j)}^\top \boldsymbol{v}_c\right)
\]</p>
<p>其中\(\sigma\)函数与sigmoid激活函数的定义相同，\(\text{leftChild}(n)\)是结点\(n\)的左子结点：如果判断\(x\)为真，\([\![x]\!] = 1\)；反之\([\![x]\!] = -1\)。<br>
计算图中给定词\(w_c\)生成词\(w_3\)的条件概率。需要将\(w_c\)的词向量\(\boldsymbol{v}_c\)和根结点到\(w_3\)路径上的非叶结点向量一一求内积。由于在二叉树中由根结点到叶结点\(w_3\)的路径上需要向左、向右再向左地遍历（图中加粗的路径），得到<br>
\[
P(w_3 \mid w_c) = \sigma(\boldsymbol{u}_{n(w_3,1)}^\top \boldsymbol{v}_c) \cdot \sigma(-\boldsymbol{u}_{n(w_3,2)}^\top \boldsymbol{v}_c) \cdot \sigma(\boldsymbol{u}_{n(w_3,3)}^\top \boldsymbol{v}_c)
\]</p>
<p>由于\(\sigma(x)+\sigma(-x) = 1\)，给定中心词\(w_c\)生成词典\(\mathcal{V}\)中任一词的条件概率之和为1这一条件也将满足：</p>
<p>\[
\sum_{w \in \mathcal{V}} P(w \mid w_c) = 1
\]</p>
<p>此外，由于\(L(w_o)-1\)的数量级为\(\mathcal{O}(\text{log}_2|\mathcal{V}|)\)，当词典\(\mathcal{V}\)很大时，层序softmax在训练中每一步的梯度计算开销相较未使用近似训练时大幅降低。</p>
<h2 id="dai-ma-shi-xian">代码实现</h2>
<h3 id="pytorch">pytorch</h3>
<h4 id="zhun-bei">准备</h4>
<p>PyTorch 中的 nn.Embedding中，有两个必选的参数：<code>num_embeddings</code>表示单词的总数目，<code>embedding_dim</code>表示每个单词需要用什么维度的向量表示。而nn.Embedding权重的维度也是(num_embeddings, embedding_dim)，默认是随机初始化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">embeds = nn.Embedding(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">embeds.weight</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[<span class="number">-1.1454</span>,  <span class="number">0.3675</span>, <span class="number">-0.3718</span>,  <span class="number">0.3733</span>,  <span class="number">0.5979</span>],</span><br><span class="line">        [<span class="number">-0.7952</span>, <span class="number">-0.9794</span>,  <span class="number">0.6292</span>, <span class="number">-0.3633</span>, <span class="number">-0.2037</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果使用预训练好的词向量</span></span><br><span class="line">pretrained_weight = np.array(pretrained_weight)</span><br><span class="line">embeds.weight.data.copy_(torch.from_numpy(pretrained_weight))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 想要查看某个词的词向量，需要传入这个词在词典中的 index，并且这个 index 得是 LongTensor 型的</span></span><br><span class="line">embeds = nn.Embedding(<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line">embeds(torch.LongTensor([<span class="number">50</span>]))</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([[<span class="number">-1.9562e-03</span>,  <span class="number">1.8971e+00</span>,  <span class="number">7.0230e-01</span>, <span class="number">-6.3762e-01</span>, <span class="number">-1.9426e-01</span>,</span><br><span class="line">          <span class="number">3.4200e-01</span>, <span class="number">-2.0908e+00</span>, <span class="number">-3.0827e-01</span>,  <span class="number">9.6250e-01</span>, <span class="number">-7.2700e-01</span>]],</span><br><span class="line">       grad_fn=&lt;EmbeddingBackward&gt;)</span><br></pre></td></tr></table></figure>
<p>首先 Embedding 层输入的 shape 是<code>(batchsize, seq_len)</code>，输出的 shape 是<code>(batchsize, embedding_dim)</code></p>
<p><img src="/2020/06/17/word_embedding/GOImi8.png" alt></p>
<p>上图的流程是把文章中的单词使用词向量来表示</p>
<ol>
<li>提取文章所有的单词，把所有的单词按照频次降序排序（取前 4999 个，表示常出现的单词。其余所有单词均用’&lt;UNK&gt;'表示。所以一共有 5000 个单词）</li>
<li>5000 个单词使用 one-hot 编码</li>
<li>通过训练会生成一个 \(5000×300\) 的矩阵，每一行向量表示一个词的词向量。这里的 <code>300</code> 是人为指定，想要每个词最终编码为词向量的维度，也可以设置成别的。</li>
</ol>
<p>这个矩阵如何获得呢？在 Skip-gram 模型中，首先会随机初始化这个矩阵，然后通过一层神经网络来训练。最终这个一层神经网络的所有权重，就是要求的词向量的矩阵</p>
<p><img src="/2020/06/17/word_embedding/GOHgHA.png" alt></p>
<p>从上面的图中看到，我们所学习的 embedding 层是一个训练任务的一小部分，根据任务目标反向传播，学习到 embedding 层里的权重 weight。</p>
<h4 id="shu-ju">数据</h4>
<p><a href="https://share.weiyun.com/jYYyKpON" target="_blank" rel="noopener">训练语料</a>下载，文件中的内容是英文文本，去除了标点符号，每个单词之间用空格隔开。</p>
<p><img src="/2020/06/17/word_embedding/Gv38YD.png" alt></p>
<figure class="highlight python"><figcaption><span>数据处理</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> tud</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line"></span><br><span class="line">random.seed(<span class="number">1</span>)</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># parameters</span></span><br><span class="line">C = <span class="number">3</span> <span class="comment"># context window，论文中选取左右多少个单词作为背景词</span></span><br><span class="line">K = <span class="number">15</span> <span class="comment"># number of negative samples，表示随机选取 15 个噪声词。</span></span><br><span class="line">epochs = <span class="number">20</span></span><br><span class="line">MAX_VOCAB_SIZE = <span class="number">50000</span> <span class="comment"># 训练 50000 个词的词向量，但实际上只会选出语料库中出现次数最多的 49999 个词，还有一个词是&lt;UNK&gt;用来表示所有的其它词。</span></span><br><span class="line">EMBEDDING_SIZE = <span class="number">300</span> <span class="comment"># 词向量维度</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">lr = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取文本数据</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'text8.train.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    text = f.read() <span class="comment"># 得到文本内容</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line">text = text.lower().split() <span class="comment">#　分割成单词列表</span></span><br><span class="line">vocab_dict = dict(Counter(text).most_common(MAX_VOCAB_SIZE - <span class="number">1</span>)) <span class="comment"># 得到单词字典表，key是单词，value是次数</span></span><br><span class="line">vocab_dict[<span class="string">'&lt;UNK&gt;'</span>] = len(text) - np.sum(list(vocab_dict.values())) <span class="comment"># 把不常用的单词都编码为"&lt;UNK&gt;"</span></span><br><span class="line">idx2word = [word <span class="keyword">for</span> word <span class="keyword">in</span> vocab_dict.keys()]</span><br><span class="line">word2idx = &#123;word:i <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(idx2word)&#125;</span><br><span class="line">word_counts = np.array([count <span class="keyword">for</span> count <span class="keyword">in</span> vocab_dict.values()], dtype=np.float32) <span class="comment"># 频次</span></span><br><span class="line">word_freqs = word_counts / np.sum(word_counts) <span class="comment"># 频率</span></span><br><span class="line">word_freqs = word_freqs ** (<span class="number">3.</span>/<span class="number">4.</span>)</span><br></pre></td></tr></table></figure>
<p>最后一行代码，<code>word_freqs</code>存储了每个单词的频率，然后又将所有的频率变为原来的 0.75 次方，这是因为 word2vec 论文里面推荐这么做。</p>
<blockquote>
<p>（频繁词的二次采样）根据论文描述在大的语料库中，频繁词如容易出现很多次的 the\in\a 提供的信息量远没有罕见词提供的信息量多，因此在后续的训练中频繁词无法提供更多的信息甚至会将网络带偏，因此提出了频繁词二次采样方式：即在每次训练时按照如下公式对训练集的单词\(w_i\)进行丢弃：<br>
\[
P\left(w_{i}\right)=1-\sqrt{\frac{t}{f\left(w_{i}\right)}}
\]</p>
</blockquote>
<p><img src="/2020/06/17/word_embedding/GvGoQJ.png" alt></p>
<figure class="highlight python"><figcaption><span>DataLoader</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordEmbeddingDataset</span><span class="params">(tud.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, text, word2idx, idx2word, word_freqs, word_counts)</span>:</span></span><br><span class="line">        <span class="string">''' text: a list of words, all text from the training dataset</span></span><br><span class="line"><span class="string">            word2idx: the dictionary from word to index</span></span><br><span class="line"><span class="string">            idx2word: index to word mapping</span></span><br><span class="line"><span class="string">            word_freqs: the frequency of each word</span></span><br><span class="line"><span class="string">            word_counts: the word counts</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super(WordEmbeddingDataset, self).__init__() <span class="comment"># #通过父类初始化模型，然后重写两个方法</span></span><br><span class="line">        self.text_encoded = [word2idx.get(word, word2idx[<span class="string">'&lt;UNK&gt;'</span>]) <span class="keyword">for</span> word <span class="keyword">in</span> text] <span class="comment"># 把单词数字化表示。如果不在词典中，也表示为unk</span></span><br><span class="line">        self.text_encoded = torch.LongTensor(self.text_encoded) <span class="comment"># nn.Embedding需要传入LongTensor类型</span></span><br><span class="line">        self.word2idx = word2idx</span><br><span class="line">        self.idx2word = idx2word</span><br><span class="line">        self.word_freqs = torch.Tensor(word_freqs)</span><br><span class="line">        self.word_counts = torch.Tensor(word_counts)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.text_encoded) <span class="comment"># 返回所有单词的总数，即item的总数</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="string">''' 这个function返回以下数据用于训练</span></span><br><span class="line"><span class="string">            - 中心词</span></span><br><span class="line"><span class="string">            - 这个单词附近的positive word</span></span><br><span class="line"><span class="string">            - 随机采样的K个单词作为negative word</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        center_words = self.text_encoded[idx] <span class="comment"># 取得中心词</span></span><br><span class="line">        pos_indices = list(range(idx - C, idx)) + list(range(idx + <span class="number">1</span>, idx + C + <span class="number">1</span>)) <span class="comment"># 先取得中心左右各C个词的索引</span></span><br><span class="line">        pos_indices = [i % len(self.text_encoded) <span class="keyword">for</span> i <span class="keyword">in</span> pos_indices] <span class="comment"># 为了避免索引越界，所以进行取余处理</span></span><br><span class="line">        pos_words = self.text_encoded[pos_indices] <span class="comment"># tensor(list)</span></span><br><span class="line">        </span><br><span class="line">        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[<span class="number">0</span>], <span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># torch.multinomial作用是对self.word_freqs做K * pos_words.shape[0]次取值，输出的是self.word_freqs对应的下标</span></span><br><span class="line">        <span class="comment"># 取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大</span></span><br><span class="line">        <span class="comment"># 每采样一个正确的单词(positive word)，就采样K个错误的单词(negative word)，pos_words.shape[0]是正确单词数量</span></span><br><span class="line">        <span class="keyword">return</span> center_words, pos_words, neg_words</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 构建数据集和dataloader</span></span><br><span class="line">dataset = WordEmbeddingDataset(text, word2idx, idx2word, word_freqs, word_counts)</span><br><span class="line">dataloader = tud.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>假设所有文本分词，转为索引之后的 list 如下图所示</p>
<p><img src="/2020/06/17/word_embedding/AZoONacteCBhF67.png" alt></p>
<p>根据论文所述，假定 window size=2，即每个中心词左右各取 2 个词作为背景词，那么对于上面的 list，窗口每次滑动，选定的中心词和背景词如下图所示</p>
<p><img src="/2020/06/17/word_embedding/K1VA8E7lHtOohme.png" alt></p>
<p>那么 skip_grams 变量里存的就是中心词和背景词一一配对后的 list，例如中心词 2，有背景词 0,1,0,1，一一配对以后就会产生 [2,0],[2,1],[2,0],[2,1]。skip_grams 如下图所示</p>
<p><img src="/2020/06/17/word_embedding/idWyM2YgruoGzUa.png" alt></p>
<figure class="highlight python"><figcaption><span>模型</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbeddingModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size)</span>:</span></span><br><span class="line">        super(EmbeddingModel, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        </span><br><span class="line">        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size)</span><br><span class="line">        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_labels, pos_labels, neg_labels)</span>:</span></span><br><span class="line">        input_embedding = self.in_embed(input_labels) <span class="comment"># [batch_size, 1,embed_size],中心词：1个</span></span><br><span class="line">        pos_embedding = self.out_embed(pos_labels)<span class="comment"># [batch_size, (window * 2), embed_size]</span></span><br><span class="line">        neg_embedding = self.out_embed(neg_labels) <span class="comment"># [batch_size, (window * 2 * K), embed_size]</span></span><br><span class="line">        </span><br><span class="line">        input_embedding = input_embedding.unsqueeze(<span class="number">2</span>) <span class="comment"># [batch_size, embed_size, 1]</span></span><br><span class="line">        </span><br><span class="line">        pos_dot = torch.bmm(pos_embedding, input_embedding) <span class="comment"># [batch_size, (window * 2), 1]</span></span><br><span class="line">        pos_dot = pos_dot.squeeze(<span class="number">2</span>) <span class="comment"># [batch_size, (window * 2)]</span></span><br><span class="line">        </span><br><span class="line">        neg_dot = torch.bmm(neg_embedding, -input_embedding) <span class="comment"># [batch_size, (window * 2 * K), 1]</span></span><br><span class="line">        neg_dot = neg_dot.squeeze(<span class="number">2</span>) <span class="comment"># batch_size, (window * 2 * K)]</span></span><br><span class="line">        </span><br><span class="line">        log_pos = F.logsigmoid(pos_dot).sum(<span class="number">1</span>) <span class="comment"># .sum()结果只为一个数，.sum(1)结果是一维的张量</span></span><br><span class="line">        log_neg = F.logsigmoid(neg_dot).sum(<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        loss = log_pos + log_neg</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> -loss</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">input_embedding</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.in_embed.weight.numpy()</span><br></pre></td></tr></table></figure>
<p>这里为什么要分两个 embedding 层来训练？</p>
<p>很明显，对于任一一个词，它既有可能作为中心词出现，也有可能作为背景词出现，所以每个词需要用两个向量去表示。<code>in_embed</code>训练出来的权重就是每个词作为中心词的权重。<code>out_embed</code>训练出来的权重就是每个词作为背景词的权重。那么最后到底用什么向量来表示一个词呢？是中心词向量？还是背景词向量？按照 Word2Vec 论文所写，推荐使用中心词向量，所以这里最后返回的是<code>in_embed.weight</code>。</p>
<blockquote>
<p>介绍一个比较常用的batch 矩阵乘法：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch1 = torch.randn(<span class="number">10</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">batch2 = torch.randn(<span class="number">10</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">res = torch.bmm(batch1, batch2)</span><br><span class="line">print(res.size())</span><br><span class="line"><span class="comment"># torch.Size([10, 3, 5])</span></span><br></pre></td></tr></table></figure>
</blockquote>
<figure class="highlight python"><figcaption><span>训练模型</span></figcaption><table><tr><td class="code"><pre><span class="line">model = EmbeddingModel(MAX_VOCAB_SIZE,EMBEDDING_SIZE)</span><br><span class="line">model = model.cuda()</span><br><span class="line">params = filter(<span class="keyword">lambda</span> p: p.requires_grad, model.parameters())</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(params,lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> i, (input_labels, pos_labels, neg_labels) <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">        input_labels = input_labels.long().cuda()</span><br><span class="line">        pos_labels = pos_labels.long().cuda()</span><br><span class="line">        neg_labels = neg_labels.long().cuda()</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss = model(input_labels, pos_labels, neg_labels).mean()</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch'</span>, e, <span class="string">'iteration'</span>, i, loss.item())</span><br><span class="line"></span><br><span class="line">embedding_weights = model.input_embeddings()</span><br><span class="line">torch.save(model.state_dict(), <span class="string">"embedding-&#123;&#125;.th"</span>.format(EMBEDDING_SIZE))</span><br></pre></td></tr></table></figure>
<h4 id="ci-xiang-liang-ying-yong">词向量应用</h4>
<p>找出与某个词相近的一些词，比方说输入 good，他能找出 nice，better，best 之类的词。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_nearest</span><span class="params">(word)</span>:</span></span><br><span class="line">    index = word2idx[word]</span><br><span class="line">    embedding = embedding_weights[index]</span><br><span class="line">    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) <span class="keyword">for</span> e <span class="keyword">in</span> embedding_weights])</span><br><span class="line">    <span class="keyword">return</span> [idx_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> cos_dis.argsort()[:<span class="number">10</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> [<span class="string">"two"</span>, <span class="string">"america"</span>, <span class="string">"computer"</span>]:</span><br><span class="line">    print(word, find_nearest(word))</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">two [<span class="string">'two'</span>, <span class="string">'zero'</span>, <span class="string">'four'</span>, <span class="string">'one'</span>, <span class="string">'six'</span>, <span class="string">'five'</span>, <span class="string">'three'</span>, <span class="string">'nine'</span>, <span class="string">'eight'</span>, <span class="string">'seven'</span>]</span><br><span class="line">america [<span class="string">'america'</span>, <span class="string">'states'</span>, <span class="string">'japan'</span>, <span class="string">'china'</span>, <span class="string">'usa'</span>, <span class="string">'west'</span>, <span class="string">'africa'</span>, <span class="string">'italy'</span>, <span class="string">'united'</span>, <span class="string">'kingdom'</span>]</span><br><span class="line">computer [<span class="string">'computer'</span>, <span class="string">'machine'</span>, <span class="string">'earth'</span>, <span class="string">'pc'</span>, <span class="string">'game'</span>, <span class="string">'writing'</span>, <span class="string">'board'</span>, <span class="string">'result'</span>, <span class="string">'code'</span>, <span class="string">'website'</span>]</span><br></pre></td></tr></table></figure>
<h4 id="nn-linear-vs-nn-embedding">nn.Linear vs nn.Embedding</h4>
<p>Word2Vec 论文中给出的架构其实就一个单层神经网络，那么为什么直接用<code>nn.Linear()</code>来训练呢？<code>nn.Linear()</code>不是也能训练出一个 weight 吗？</p>
<p>答案是可以的，当然可以直接使用<code>nn.Linear()</code>，只不过输入要改为 one-hot Encoding，而不能像<code>nn.Embedding()</code>这种方式直接传入一个 index。还有就是需要设置<code>bias</code>，因为只需要训练一个权重矩阵，不训练偏置。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Word2Vec</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    super(Word2Vec, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># W and V is not Traspose relationship</span></span><br><span class="line">    self.W = nn.Parameter(torch.randn(voc_size, embedding_size).type(dtype))</span><br><span class="line">    self.V = nn.Parameter(torch.randn(embedding_size, voc_size).type(dtype))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="comment"># X : [batch_size, voc_size] one-hot</span></span><br><span class="line">    <span class="comment"># torch.mm only for 2 dim matrix, but torch.matmul can use to any dim</span></span><br><span class="line">    hidden_layer = torch.matmul(X, self.W) <span class="comment"># hidden_layer : [batch_size, embedding_size]</span></span><br><span class="line">    output_layer = torch.matmul(hidden_layer, self.V) <span class="comment"># output_layer : [batch_size, voc_size]</span></span><br><span class="line">    <span class="keyword">return</span> output_layer</span><br></pre></td></tr></table></figure>
<h3 id="tensorflow">tensorflow</h3>
<p>以跳字模型和近似训练中的负采样为例，介绍在语料库上训练词嵌入模型的实现。</p>
<h4 id="shu-ju-1">数据</h4>
<p><a href="https://share.weiyun.com/jYYyKpON" target="_blank" rel="noopener">ptb数据下载</a></p>
<p>PTB（Penn Tree Bank）是一个常用的小型语料库。它采样自《华尔街日报》的文章，包括训练集、验证集和测试集。该数据集的每一行作为一个句子。句子中的每个词由空格隔开。这个数据集中句尾符为&quot;&lt;eos&gt;&quot;，生僻词全用&quot;&lt;unk&gt;“表示，数字则被替换成了&quot;N”。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">".."</span>)</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'ptb.train.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    lines = f.readlines()</span><br><span class="line">    <span class="comment"># st是sentence的缩写</span></span><br><span class="line">    raw_dataset = [st.split() <span class="keyword">for</span> st <span class="keyword">in</span> lines]</span><br><span class="line">print(<span class="string">'# sentences: &#123;&#125;'</span>.format(len(raw_dataset))) <span class="comment"># 输出 '# sentences: 42068'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立词语索引 为了计算简单，只保留在数据集中至少出现5次的词</span></span><br><span class="line">counter = collections.Counter([tk <span class="keyword">for</span> st <span class="keyword">in</span> raw_dataset <span class="keyword">for</span> tk <span class="keyword">in</span> st])</span><br><span class="line">counter = dict(filter(<span class="keyword">lambda</span> x: x[<span class="number">1</span>] &gt;= <span class="number">5</span>, counter.items()))</span><br><span class="line"><span class="comment"># 然后将词映射到整数索引。</span></span><br><span class="line">idx_to_token = [tk <span class="keyword">for</span> tk, _ <span class="keyword">in</span> counter.items()]</span><br><span class="line">token_to_idx = &#123;tk: idx <span class="keyword">for</span> idx, tk <span class="keyword">in</span> enumerate(idx_to_token)&#125;</span><br><span class="line">dataset = [[token_to_idx[tk] <span class="keyword">for</span> tk <span class="keyword">in</span> st <span class="keyword">if</span> tk <span class="keyword">in</span> token_to_idx] <span class="keyword">for</span> st <span class="keyword">in</span> raw_dataset]</span><br><span class="line">num_tokens = sum([len(st) <span class="keyword">for</span> st <span class="keyword">in</span> dataset])</span><br><span class="line">print(<span class="string">'# tokens: &#123;&#125;'</span>.format(num_tokens)) <span class="comment"># 输出 '# tokens: 887100'</span></span><br></pre></td></tr></table></figure>
<h4 id="er-ci-cai-yang">二次采样</h4>
<p>文本数据中一般会出现一些高频词，如英文中的“the”“a”和“in”。通常来说，在一个背景窗口中，一个词（如“chip”）和较低频词（如“microprocessor”）同时出现比和较高频词（如“the”）同时出现对训练词嵌入模型更有益。因此，训练词嵌入模型时可以对词进行二次采样。<br>
具体来说，数据集中每个被索引词\(w_i\)将有一定概率被丢弃，该丢弃概率为<br>
\[
P(w_i) = \max\left(1 - \sqrt{\frac{t}{f(w_i)}}, 0\right),
\]</p>
<p>其中 \(f(w_i)\) 是数据集中词\(w_i\)的个数与总词数之比，常数\(t\)是一个超参数（实验中设为\(10^{-4}\)）。可见，只有当\(f(w_i) > t\)时，才有可能在二次采样中丢弃词\(w_i\)，并且越高频的词被丢弃的概率越大。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discard</span><span class="params">(idx)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> random.uniform(<span class="number">0</span>, <span class="number">1</span>) &lt; <span class="number">1</span> - math.sqrt(<span class="number">1e-4</span> / counter[idx_to_token[idx]] * num_tokens)</span><br><span class="line"></span><br><span class="line">subsampled_dataset = [[tk <span class="keyword">for</span> tk <span class="keyword">in</span> st <span class="keyword">if</span> <span class="keyword">not</span> discard(tk)] <span class="keyword">for</span> st <span class="keyword">in</span> dataset]</span><br><span class="line">print(<span class="string">'# tokens: &#123;&#125;'</span>.format(sum([len(st) <span class="keyword">for</span> st <span class="keyword">in</span> subsampled_dataset]))) <span class="comment"># '# tokens: 376200'</span></span><br><span class="line"><span class="comment"># 可以看到，二次采样后去掉了一半左右的词。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面比较一个词在二次采样前后出现在数据集中的次数。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compare_counts</span><span class="params">(token)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'# &#123;&#125;: before=&#123;&#125;, after=&#123;&#125;'</span>.format(token, </span><br><span class="line">                                          sum([st.count(token_to_idx[token]) <span class="keyword">for</span> st <span class="keyword">in</span> dataset]), </span><br><span class="line">                                          sum([st.count(token_to_idx[token]) <span class="keyword">for</span> st <span class="keyword">in</span> subsampled_dataset]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可见高频词“the”的采样率不足1/20。</span></span><br><span class="line">print(compare_counts(<span class="string">'the'</span>)) <span class="comment"># '# the: before=50770, after=2013'</span></span><br><span class="line"><span class="comment"># 但低频词“join”则完整地保留了下来。</span></span><br><span class="line">print(compare_counts(<span class="string">'join'</span>)) <span class="comment"># '# join: before=45, after=45'</span></span><br></pre></td></tr></table></figure>
<h4 id="ti-qu-zhong-xin-ci-he-bei-jing-ci">提取中心词和背景词</h4>
<p>将与中心词距离不超过背景窗口大小的词作为它的背景词。下面定义函数提取出所有中心词和它们的背景词。它每次在整数1和<code>max_window_size</code>（最大背景窗口）之间随机均匀采样一个整数作为背景窗口大小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_centers_and_contexts</span><span class="params">(dataset, max_window_size)</span>:</span></span><br><span class="line">    centers, contexts = [], []</span><br><span class="line">    <span class="keyword">for</span> st <span class="keyword">in</span> dataset:</span><br><span class="line">        <span class="keyword">if</span> len(st) &lt; <span class="number">2</span>:  <span class="comment"># 每个句子至少要有2个词才可能组成一对“中心词-背景词”</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        centers += st</span><br><span class="line">        <span class="keyword">for</span> center_i <span class="keyword">in</span> range(len(st)):</span><br><span class="line">            window_size = random.randint(<span class="number">1</span>, max_window_size)</span><br><span class="line">            indices = list(range(max(<span class="number">0</span>, center_i - window_size),</span><br><span class="line">                                 min(len(st), center_i + <span class="number">1</span> + window_size)))</span><br><span class="line">            indices.remove(center_i)  <span class="comment"># 将中心词排除在背景词之外</span></span><br><span class="line">            contexts.append([st[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> indices])</span><br><span class="line">    <span class="keyword">return</span> centers, contexts</span><br><span class="line"><span class="comment"># 实验中，设最大背景窗口大小为5。下面提取数据集中所有的中心词及其背景词。</span></span><br><span class="line">all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h4 id="fu-cai-yang-1">负采样</h4>
<p>使用负采样来进行近似训练。对于一对中心词和背景词，随机采样\(K\)个噪声词（实验中设\(K=5\)）。根据word2vec论文的建议，噪声词采样概率\(P(w)\)设为\(w\)词频与总词频之比的0.75次方。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_negatives</span><span class="params">(all_contexts, sampling_weights, K)</span>:</span></span><br><span class="line">    all_negatives, neg_candidates, i = [], [], <span class="number">0</span></span><br><span class="line">    population = list(range(len(sampling_weights)))</span><br><span class="line">    <span class="keyword">for</span> contexts <span class="keyword">in</span> all_contexts:</span><br><span class="line">        negatives = []</span><br><span class="line">        <span class="keyword">while</span> len(negatives) &lt; len(contexts) * K:</span><br><span class="line">            <span class="keyword">if</span> i == len(neg_candidates):</span><br><span class="line">                <span class="comment"># 根据每个词的权重（sampling_weights）随机生成k个词的索引作为噪声词。</span></span><br><span class="line">                <span class="comment"># 为了高效计算，可以将k设得稍大一点</span></span><br><span class="line">                i, neg_candidates = <span class="number">0</span>, random.choices(</span><br><span class="line">                    population, sampling_weights, k=int(<span class="number">1e5</span>))</span><br><span class="line">            neg, i = neg_candidates[i], i + <span class="number">1</span></span><br><span class="line">            <span class="comment"># 噪声词不能是背景词</span></span><br><span class="line">            <span class="keyword">if</span> neg <span class="keyword">not</span> <span class="keyword">in</span> set(contexts):</span><br><span class="line">                negatives.append(neg)</span><br><span class="line">        all_negatives.append(negatives)</span><br><span class="line">    <span class="keyword">return</span> all_negatives</span><br><span class="line"></span><br><span class="line">sampling_weights = [counter[w]**<span class="number">0.75</span> <span class="keyword">for</span> w <span class="keyword">in</span> idx_to_token]</span><br><span class="line">all_negatives = get_negatives(all_contexts, sampling_weights, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h4 id="du-qu-shu-ju">读取数据</h4>
<p>从数据集中提取所有中心词<code>all_centers</code>，以及每个中心词对应的背景词<code>all_contexts</code>和噪声词<code>all_negatives</code>。</p>
<p>通过随机小批量来读取它们。在一个小批量数据中，第\(i\)个样本包括一个中心词以及它所对应的\(n_i\)个背景词和\(m_i\)个噪声词。由于每个样本的背景窗口大小可能不一样，其中背景词与噪声词个数之和\(n_i+m_i\)也会不同。在构造小批量时，将每个样本的背景词和噪声词连结在一起，并添加填充项0直至连结后的长度相同，即长度均为\(\max_i n_i+m_i\)（<code>max_len</code>变量）。为了避免填充项对损失函数计算的影响，构造了掩码变量<code>masks</code>，其每一个元素分别与连结后的背景词和噪声词<code>contexts_negatives</code>中的元素一一对应。当<code>contexts_negatives</code>变量中的某个元素为填充项时，相同位置的掩码变量<code>masks</code>中的元素取0，否则取1。为了区分正类和负类，我们还需要将<code>contexts_negatives</code>变量中的背景词和噪声词区分开来。依据掩码变量的构造思路，我们只需创建与<code>contexts_negatives</code>变量形状相同的标签变量<code>labels</code>，并将与背景词（正类）对应的元素设1，其余清0。</p>
<p>下面我们实现这个小批量读取函数<code>batchify</code>。它的小批量输入<code>data</code>是一个长度为批量大小的列表，其中每个元素分别包含中心词<code>center</code>、背景词<code>context</code>和噪声词<code>negative</code>。该函数返回的小批量数据符合我们需要的格式，例如，包含了掩码变量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchify</span><span class="params">(data)</span>:</span></span><br><span class="line">    max_len = max(len(c) + len(n) <span class="keyword">for</span> _, c, n <span class="keyword">in</span> data)</span><br><span class="line">    centers, contexts_negatives, masks, labels = [], [], [], []</span><br><span class="line">    <span class="keyword">for</span> center, context, negative <span class="keyword">in</span> data:</span><br><span class="line">        center=center.numpy().tolist()</span><br><span class="line">        context=context.numpy().tolist()</span><br><span class="line">        negative=negative.numpy().tolist()</span><br><span class="line">        cur_len = len(context) + len(negative)</span><br><span class="line">        centers += [center]</span><br><span class="line">        contexts_negatives += [context + negative + [<span class="number">0</span>] * (max_len - cur_len)]</span><br><span class="line">        masks += [[<span class="number">1</span>] * cur_len + [<span class="number">0</span>] * (max_len - cur_len)]</span><br><span class="line">        labels += [[<span class="number">1</span>] * len(context) + [<span class="number">0</span>] * (max_len - len(context))]</span><br><span class="line">    <span class="keyword">return</span> tf.data.Dataset.from_tensor_slices((tf.reshape(tf.convert_to_tensor(centers),shape=(<span class="number">-1</span>, <span class="number">1</span>)), </span><br><span class="line">                                               tf.convert_to_tensor(contexts_negatives),tf.convert_to_tensor(masks), </span><br><span class="line">                                               tf.convert_to_tensor(labels)))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> cent, cont, neg <span class="keyword">in</span> zip(all_centers,all_contexts,all_negatives):</span><br><span class="line">        <span class="keyword">yield</span> (cent, cont, neg)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">512</span></span><br><span class="line">dataset=tf.data.Dataset.from_generator(generator=generator,output_types=(tf.int32,tf.int32, tf.int32))</span><br><span class="line">dataset = dataset.apply(batchify).shuffle(len(all_centers)).batch(batch_size)</span><br></pre></td></tr></table></figure>
<h4 id="tiao-zi-mo-xing">跳字模型</h4>
<h5 id="qian-ru-ceng">嵌入层</h5>
<p>获取词嵌入的层称为嵌入层，在Tensorflow2中可以通过创建<code>tf.keras.layers.Embedding</code>实例得到。嵌入层的权重是一个矩阵，其行数为词典大小（<code>num_embeddings</code>），列数为每个词向量的维度（<code>embedding_dim</code>）。嵌入层的输入为词的索引。输入一个词的索引\(i\)，嵌入层返回权重矩阵的第\(i\)行作为它的词向量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embed = tf.keras.layers.Embedding(input_dim=<span class="number">20</span>, output_dim=<span class="number">4</span>)</span><br><span class="line">embed.build(input_shape=(<span class="number">1</span>,<span class="number">20</span>))</span><br></pre></td></tr></table></figure>
<h5 id="tiao-zi-mo-xing-qian-xiang-ji-suan">跳字模型前向计算</h5>
<p>在前向计算中，跳字模型的输入包含中心词索引<code>center</code>以及连结的背景词与噪声词索引<code>contexts_and_negatives</code>。其中<code>center</code>变量的形状为(批量大小, 1)，而<code>contexts_and_negatives</code>变量的形状为(批量大小, <code>max_len</code>)。这两个变量先通过词嵌入层分别由词索引变换为词向量，再通过小批量乘法得到形状为(批量大小, 1, <code>max_len</code>)的输出。输出中的每个元素是中心词向量与背景词向量或噪声词向量的内积。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">skip_gram</span><span class="params">(center, contexts_and_negatives, embed_v, embed_u)</span>:</span></span><br><span class="line">    v = embed_v(center)</span><br><span class="line">    u = embed_u(contexts_and_negatives)</span><br><span class="line">    pred = tf.matmul(v, tf.transpose(u,perm=[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure>
<h4 id="xun-lian-mo-xing">训练模型</h4>
<p>在训练词嵌入模型之前，定义模型的损失函数,由于tensorflow中没有二元交叉损失函数中的mask参数，故要重新定义。</p>
<h5 id="er-yuan-jiao-cha-shang-sun-shi-han-shu">二元交叉熵损失函数</h5>
<p>根据负采样中损失函数的定义，我们可以使用二元交叉熵损失函数,下面定义<code>SigmoidBinaryCrossEntropyLoss</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SigmoidBinaryCrossEntropyLoss</span><span class="params">(tf.keras.losses.Loss)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span> <span class="comment"># none mean sum</span></span><br><span class="line">        super(SigmoidBinaryCrossEntropyLoss, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, targets, mask=None)</span>:</span></span><br><span class="line">        <span class="comment">#tensorflow中使用tf.nn.weighted_cross_entropy_with_logits设置mask并没有起到作用</span></span><br><span class="line">        <span class="comment">#直接与mask按元素相乘回实现当mask为0时不计损失的效果</span></span><br><span class="line">        inputs=tf.cast(inputs,dtype=tf.float32)</span><br><span class="line">        targets=tf.cast(targets,dtype=tf.float32)</span><br><span class="line">        mask=tf.cast(mask,dtype=tf.float32)</span><br><span class="line">        res=tf.nn.sigmoid_cross_entropy_with_logits(inputs, targets)*mask</span><br><span class="line">        <span class="keyword">return</span> tf.reduce_mean(res,axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">loss = SigmoidBinaryCrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<p>值得一提的是，可以通过掩码变量指定小批量中参与损失函数计算的部分预测值和标签：当掩码为1时，相应位置的预测值和标签将参与损失函数的计算；当掩码为0时，相应位置的预测值和标签则不参与损失函数的计算。掩码变量可用于避免填充项对损失函数计算的影响。</p>
<h5 id="chu-shi-hua-mo-xing-can-shu">初始化模型参数</h5>
<p>分别构造中心词和背景词的嵌入层，并将超参数词向量维度<code>embed_size</code>设置成100。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embed_size = <span class="number">100</span></span><br><span class="line">net = tf.keras.Sequential([</span><br><span class="line">    tf.keras.layers.Embedding(input_dim=len(idx_to_token), output_dim=embed_size),</span><br><span class="line">    tf.keras.layers.Embedding(input_dim=len(idx_to_token), output_dim=embed_size)</span><br><span class="line">])</span><br><span class="line">net.get_layer(index=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h5 id="ding-yi-xun-lian-han-shu">定义训练函数</h5>
<p>下面定义训练函数。由于填充项的存在，与之前的训练函数相比，损失函数的计算稍有不同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(net, lr, num_epochs)</span>:</span></span><br><span class="line">    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        start, l_sum, n = time.time(), <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> dataset:</span><br><span class="line">            center, context_negative, mask, label = [d <span class="keyword">for</span> d <span class="keyword">in</span> batch]</span><br><span class="line">            mask=tf.cast(mask,dtype=tf.float32)</span><br><span class="line">            <span class="keyword">with</span> tf.GradientTape(persistent=<span class="literal">True</span>) <span class="keyword">as</span> tape:</span><br><span class="line">                pred = skip_gram(center, context_negative, net.get_layer(index=<span class="number">0</span>), net.get_layer(index=<span class="number">1</span>))</span><br><span class="line">                <span class="comment"># 使用掩码变量mask来避免填充项对损失函数计算的影响</span></span><br><span class="line">                l = (loss(label, tf.reshape(pred,label.shape), mask) *</span><br><span class="line">                     mask.shape[<span class="number">1</span>] / tf.reduce_sum(mask,axis=<span class="number">1</span>))</span><br><span class="line">                l=tf.reduce_mean(l)<span class="comment"># 一个batch的平均loss</span></span><br><span class="line">                </span><br><span class="line">            grads = tape.gradient(l, net.variables)</span><br><span class="line">            optimizer.apply_gradients(zip(grads, net.variables))</span><br><span class="line">            l_sum += np.array(l).item()</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">        print(<span class="string">'epoch %d, loss %.2f, time %.2fs'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, l_sum / n, time.time() - start))</span><br></pre></td></tr></table></figure>
<p>使用负采样训练跳字模型了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(net, <span class="number">0.01</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h4 id="ying-yong-ci-qian-ru-mo-xing">应用词嵌入模型</h4>
<p>训练好词嵌入模型之后，我们可以根据两个词向量的余弦相似度表示词与词之间在语义上的相似度。可以看到，使用训练得到的词嵌入模型时，与词“chip”语义最接近的词大多与芯片有关。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_similar_tokens</span><span class="params">(query_token, k, embed)</span>:</span></span><br><span class="line">    W = embed.get_weights()</span><br><span class="line">    W = tf.convert_to_tensor(W[<span class="number">0</span>])</span><br><span class="line">    x = W[token_to_idx[query_token]]</span><br><span class="line">    x = tf.reshape(x,shape=[<span class="number">-1</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 添加的1e-9是为了数值稳定性</span></span><br><span class="line">    cos = tf.reshape(tf.matmul(W, x),shape=[<span class="number">-1</span>])/ tf.sqrt(tf.reduce_sum(W * W, axis=<span class="number">1</span>) * tf.reduce_sum(x * x) + <span class="number">1e-9</span>)</span><br><span class="line">    _, topk = tf.math.top_k(cos, k=k+<span class="number">1</span>)</span><br><span class="line">    topk=topk.numpy().tolist()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> topk[<span class="number">1</span>:]:  <span class="comment"># 除去输入词</span></span><br><span class="line">        print(<span class="string">'cosine sim=%.3f: %s'</span> % (cos[i], (idx_to_token[i])))</span><br><span class="line">        </span><br><span class="line">get_similar_tokens(<span class="string">'chip'</span>, <span class="number">3</span>, net.get_layer(index=<span class="number">0</span>))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">cosine sim=<span class="number">0.539</span>: nec</span><br><span class="line">cosine sim=<span class="number">0.494</span>: a.g.</span><br><span class="line">cosine sim=<span class="number">0.493</span>: microprocessor</span><br></pre></td></tr></table></figure>
<h2 id="word-2-vec-zong-jie">word2vec总结</h2>
<ol>
<li>word2vec包含跳字模型和连续词袋模型。</li>
<li>跳字模型假设基于中心词来生成背景词。在应用中，使用中心词的词向量作为词的表征向量</li>
<li>连续词袋模型假设基于背景词来生成中心词。在应用中，使用背景词的词向量作为词的表征向量。</li>
<li>负采样通过考虑同时含有正类样本和负类样本的相互独立事件来构造损失函数。其训练中每一步的梯度计算开销与采样的噪声词的个数线性相关。</li>
<li>层序softmax使用了二叉树，并根据根结点到叶结点的路径来构造损失函数。其训练中每一步的梯度计算开销与词典大小的对数相关。</li>
<li>word2vec是静态词向量预训练模型，词向量是固定的，不能解决多义词问题，无法考虑预料全局信息。</li>
<li>二次采样试图尽可能减轻高频词对训练词嵌入模型的影响。</li>
<li>可以将长度不同的样本填充至长度相同的小批量，并通过掩码变量区分非填充和填充，然后只令非填充参与损失函数的计算。</li>
</ol>
<h1 id="quan-ju-xiang-liang-de-ci-qian-ru-glo-ve">全局向量的词嵌入（GloVe）</h1>
<p>先回顾一下word2vec中的跳字模型。将跳字模型中使用softmax运算表达的条件概率\(P(w_j\mid w_i)\)记作\(q_{ij}\)，即</p>
<p>\[
q_{ij}=\frac{\exp(\boldsymbol{u}_j^\top \boldsymbol{v}_i)}{ \sum_{k \in \mathcal{V}} \text{exp}(\boldsymbol{u}_k^\top \boldsymbol{v}_i)},
\]</p>
<p>其中\(\boldsymbol{v}_i\)和\(\boldsymbol{u}_i\)分别是索引为\(i\)的词\(w_i\)作为中心词和背景词时的向量表示，\(\mathcal{V} = \{0, 1, \ldots, |\mathcal{V}|-1\}\)为词典索引集。</p>
<p>对于词\(w_i\)，它在数据集中可能多次出现。<strong>将每一次以它作为中心词的所有背景词全部汇总并保留重复元素</strong>，记作多重集\(\mathcal{C}_i\)。一个元素在多重集中的个数称为该元素的重数。举例来说，假设词\(w_i\)在数据集中出现2次：文本序列中以这2个\(w_i\)作为中心词的背景窗口分别包含背景词索引\(2,1,5,2\)和\(2,3,2,1\)。那么多重集\(\mathcal{C}_i = \{1,1,2,2,2,2,3,5\}\)，其中元素1的重数为2，元素2的重数为4，元素3和5的重数均为1。将多重集\(\mathcal{C}_i\)中元素\(j\)的重数记作\(x_{ij}\)：它表示了整个数据集中所有以\(w_i\)为中心词的背景窗口中词\(w_j\)的个数。那么，跳字模型的损失函数还可以用另一种方式表达：</p>
<p>\[
-\sum_{i\in\mathcal{V}}\sum_{j\in\mathcal{V}} x_{ij} \log\,q_{ij}.
\]</p>
<p>将数据集中所有以词\(w_i\)为中心词的背景词的数量之和\(\left|\mathcal{C}_i\right|\)记为\(x_i\)，并将以\(w_i\)为中心词生成背景词\(w_j\)的条件概率\(x_{ij}/x_i\)记作\(p_{ij}\)。进一步改写跳字模型的损失函数为:</p>
<p>\[
-\sum_{i\in\mathcal{V}} x_i \sum_{j\in\mathcal{V}} p_{ij} \log\,q_{ij}.
\]</p>
<p>上式中，\(-\sum_{j\in\mathcal{V}} p_{ij} \log\,q_{ij}\)计算的是以\(w_i\)为中心词的背景词条件概率分布\(p_{ij}\)和模型预测的条件概率分布\(q_{ij}\)的交叉熵，且损失函数使用所有以词\(w_i\)为中心词的背景词的数量之和来加权。最小化上式中的损失函数会令预测的条件概率分布尽可能接近真实的条件概率分布。</p>
<p>然而，作为常用损失函数的一种，交叉熵损失函数有时并不是好的选择。一方面，令模型预测\(q_{ij}\)成为合法概率分布的代价是它在分母中基于整个词典的累加项。这很容易带来过大的计算开销。另一方面，词典中往往有大量生僻词，它们在数据集中出现的次数极少。而有关大量生僻词的条件概率分布在交叉熵损失函数中的最终预测往往并不准确。</p>
<h2 id="glo-ve-mo-xing">GloVe模型</h2>
<p>鉴于此，作为在word2vec之后提出的词嵌入模型，GloVe模型采用了平方损失，并基于该损失对跳字模型做了3点改动：</p>
<ol>
<li>使用非概率分布的变量\(p'_{ij}=x_{ij}\)和\(q'_{ij}=\exp(\boldsymbol{u}_j^\top \boldsymbol{v}_i)\)，并对它们取对数。因此，平方损失项是\(\left(\log\,p'_{ij} - \log\,q'_{ij}\right)^2 = \left(\boldsymbol{u}_j^\top \boldsymbol{v}_i - \log\,x_{ij}\right)^2\)。</li>
<li>为每个词\(w_i\)增加两个为标量的模型参数：中心词偏差项\(b_i\)和背景词偏差项\(c_i\)。</li>
<li>将每个损失项的权重替换成函数\(h(x_{ij})\)。权重函数\(h(x)\)是值域在\([0,1]\)的单调递增函数。</li>
</ol>
<p>如此一来，GloVe模型的目标是最小化损失函数</p>
<p>\[
\sum_{i\in\mathcal{V}} \sum_{j\in\mathcal{V}} h(x_{ij}) \left(\boldsymbol{u}_j^\top \boldsymbol{v}_i + b_i + c_j - \log\,x_{ij}\right)^2
\]</p>
<p>其中权重函数\(h(x)\)的一个建议选择是：当\(x &lt; c\)时（如\(c = 100\)），令\(h(x) = (x/c)^\alpha\)（如\(\alpha = 0.75\)），反之令\(h(x) = 1\)。因为\(h(0)=0\)，所以对于\(x_{ij}=0\)的平方损失项可以直接忽略。当使用小批量随机梯度下降来训练时，每个时间步我们随机采样小批量非零\(x_{ij}\)，然后计算梯度来迭代模型参数。这些非零\(x_{ij}\)是预先基于整个数据集计算得到的，包含了数据集的全局统计信息。因此，GloVe模型的命名取“全局向量”（Global Vectors）之意。</p>
<p>需要强调的是，如果词\(w_i\)出现在词\(w_j\)的背景窗口里，那么词\(w_j\)也会出现在词\(w_i\)的背景窗口里。也就是说，\(x_{ij}=x_{ji}\)。不同于word2vec中拟合的是非对称的条件概率\(p_{ij}\)，GloVe模型拟合的是对称的\(\log\, x_{ij}\)。因此，任意词的中心词向量和背景词向量在GloVe模型中是等价的。但由于初始化值的不同，同一个词最终学习到的两组词向量可能不同。当学习得到所有词向量以后，<strong>GloVe模型使用中心词向量与背景词向量之和作为该词的最终词向量</strong>。</p>
<h2 id="cong-tiao-jian-gai-lu-bi-zhi-li-jie-glo-ve-mo-xing">从条件概率比值理解GloVe模型</h2>
<p>还可以从另外一个角度来理解GloVe模型。沿用前面的符号，\(P(w_j \mid w_i)\)表示数据集中以\(w_i\)为中心词生成背景词\(w_j\)的条件概率，并记作\(p_{ij}\)。作为源于某大型语料库的真实例子，以下列举了两组分别以“ice”（冰）和“steam”（蒸汽）为中心词的条件概率以及它们之间的比值：</p>
<table>
<thead>
<tr>
<th style="text-align:right">\(w_k\)=</th>
<th style="text-align:center">“solid”</th>
<th style="text-align:center">“gas”</th>
<th style="text-align:center">“water”</th>
<th style="text-align:center">“fashion”</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">\(p_1=P(w_k\mid\) “ice” \()\)</td>
<td style="text-align:center">0.00019</td>
<td style="text-align:center">0.000066</td>
<td style="text-align:center">0.003</td>
<td style="text-align:center">0.000017</td>
</tr>
<tr>
<td style="text-align:right">\(p_2=P(w_k\mid\) “steam” \()\)</td>
<td style="text-align:center">0.000022</td>
<td style="text-align:center">0.00078</td>
<td style="text-align:center">0.0022</td>
<td style="text-align:center">0.000018</td>
</tr>
<tr>
<td style="text-align:right">\(p_1/p_2\)</td>
<td style="text-align:center">8.9</td>
<td style="text-align:center">0.085</td>
<td style="text-align:center">1.36</td>
<td style="text-align:center">0.96</td>
</tr>
</tbody>
</table>
<p>可以观察到以下现象。</p>
<ul>
<li>对于与“ice”相关而与“steam”不相关的词\(w_k\)，如\(w_k=\)“solid”（固体），我们期望条件概率比值较大，如上表最后一行中的值8.9；</li>
<li>对于与“ice”不相关而与“steam”相关的词\(w_k\)，如\(w_k=\)“gas”（气体），我们期望条件概率比值较小，如上表最后一行中的值0.085；</li>
<li>对于与“ice”和“steam”都相关的词\(w_k\)，如\(w_k=\)“water”（水），我们期望条件概率比值接近1，如上表最后一行中的值1.36；</li>
<li>对于与“ice”和“steam”都不相关的词\(w_k\)，如\(w_k=\)“fashion”（时尚），我们期望条件概率比值接近1，如上表最后一行中的值0.96。</li>
</ul>
<p>由此可见，条件概率比值能比较直观地表达词与词之间的关系。可以构造一个词向量函数使它能有效拟合条件概率比值。任意一个这样的比值需要3个词\(w_i\)、\(w_j\)和\(w_k\)。以\(w_i\)作为中心词的条件概率比值为\({p_{ij}}/{p_{ik}}\)。我们可以找一个函数，它使用词向量来拟合这个条件概率比值</p>
<p>\[
f(\boldsymbol{u}_j, \boldsymbol{u}_k, {\boldsymbol{v}}_i) \approx \frac{p_{ij}}{p_{ik}}.
\]</p>
<p>这里函数\(f\)可能的设计并不唯一，我们只需考虑一种较为合理的可能性。注意到条件概率比值是一个标量，我们可以将\(f\)限制为一个标量函数：\(f(\boldsymbol{u}_j, \boldsymbol{u}_k, {\boldsymbol{v}}_i) = f\left((\boldsymbol{u}_j - \boldsymbol{u}_k)^\top {\boldsymbol{v}}_i\right)\)。交换索引\(j\)和\(k\)后可以看到函数\(f\)应该满足\(f(x)f(-x)=1\)，因此一种可能是\(f(x)=\exp(x)\)，于是</p>
<p>\[
f(\boldsymbol{u}_j, \boldsymbol{u}_k, {\boldsymbol{v}}_i) = \frac{\exp\left(\boldsymbol{u}_j^\top {\boldsymbol{v}}_i\right)}{\exp\left(\boldsymbol{u}_k^\top {\boldsymbol{v}}_i\right)} \approx \frac{p_{ij}}{p_{ik}}.
\]<br>
满足最右边约等号的一种可能是\(\exp\left(\boldsymbol{u}_j^\top {\boldsymbol{v}}_i\right) \approx \alpha p_{ij}\)，这里\(\alpha\)是一个常数。考虑到\(p_{ij}=x_{ij}/x_i\)，取对数后\(\boldsymbol{u}_j^\top {\boldsymbol{v}}_i \approx \log\,\alpha + \log\,x_{ij} - \log\,x_i\)。我们使用额外的偏差项来拟合\(- \log\,\alpha + \log\,x_i\)，例如，中心词偏差项\(b_i\)和背景词偏差项\(c_j\)：<br>
\[
\boldsymbol{u}_j^\top \boldsymbol{v}_i + b_i + c_j \approx \log(x_{ij}).
\]</p>
<p>对上式左右两边取平方误差并加权，我们可以得到GloVe模型的损失函数。</p>
<h2 id="glo-ve-zong-jie">GloVe 总结</h2>
<ul>
<li>在有些情况下，交叉熵损失函数有劣势。GloVe模型采用了平方损失，并通过词向量拟合预先基于整个数据集计算得到的全局统计信息。</li>
<li>任意词的中心词向量和背景词向量在GloVe模型中是等价的。</li>
</ul>
<h1 id="can-kao">参考</h1>
<ol>
<li>
<p>Penn Tree Bank. <a href="https://catalog.ldc.upenn.edu/LDC99T42" target="_blank" rel="noopener">https://catalog.ldc.upenn.edu/LDC99T42</a></p>
</li>
<li>
<p>Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119).</p>
</li>
<li>
<p><a href="https://www.wmathor.com/index.php/archives/1435/" target="_blank" rel="noopener">PyTorch 实现 Word2Vec</a></p>
</li>
<li>
<p><a href="https://www.bilibili.com/video/BV12W411v7Ga?from=search&amp;seid=9314405103672286153" target="_blank" rel="noopener">[MXNet/Gluon] 动手学深度学习第十六课：词向量（word2vec）</a></p>
</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>Circle Loss</title>
    <url>/2020/06/07/deeplearning/loss/circle_loss/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/06/07/deeplearning/loss/circle_loss/image-20200607122847009.png" alt></p>
<a id="more"></a>
<p>旷世在CVPR 2020上的一篇<a href="https://arxiv.org/abs/2002.10857" target="_blank" rel="noopener">论文</a>。</p>
<p>简单来讲，原来特征学习有 2 种基本范式，分类学习和 pairwise 学习，人们普遍都觉得这两者虽然有联系，但是总体上仍是割裂的。旷视在这项工作中首次将两者放在一个统一的框架下，用一个general 的公式定义了这两种范式，且在这统一的公式下，获得了比两者各自最高水平方法都要好的性能。这项工作已经发表在CVPR 2020。</p>
<h1 id="dong-ji">动机</h1>
<p>深度特征学习有两种基本范式，分别是使用类标签和使用正负样本对标签进行学习。使用类标签时，一般需要用分类损失函数（比如 softmax + cross entropy）优化样本和权重向量之间的相似度；使用样本对标签时，通常用度量损失函数（比如 triplet 损失）来优化样本之间的相似度。</p>
<blockquote>
<p>softmax : \(S_j = \frac{e^{a_j}}{\sum^N_{k=1}{e^{a_k}}}\)</p>
<p>softmax 能够保证</p>
<ol>
<li>所有的值都是 [0, 1] 之间的（因为概率必须是 [0, 1]）</li>
<li>所有的值加起来等于 1</li>
</ol>
<p>Cross-Entropy：\(H(p_{y^\prime},q)=-\sum_{j=1}^K{p_{y^{\prime}_j}log(q_j)}\),其中\(log(q(k))\)是logSoftmax。</p>
<p>Cross-Entropy是用来衡量两个概率分布之间的距离的,也就是预测类别和正确类别之间的损失。如：<code>a=[1,2,3]</code>,<code>b=[2,4,6]</code>，那么 cross entropy距离就是0。</p>
<p>在pytorch中，<code>torch.nn.CrossEntropyLoss = torch.nn.LogSoftmax+torch.nn.NLLLoss</code></p>
<p>triplet loss</p>
<p>Triplet loss 最初是在人脸识别中为了学习一个较好的人脸embedding，而<code>softmax</code>最终的类别数是确定的，而<code>Triplet loss</code>学到的是一个好的<code>embedding</code>，相似的图像在<code>embedding</code>空间里是相近的，可以判断是否是同一个人脸。</p>
<p>输入是一个三元组 <code>&lt;a, p, n&gt;</code></p>
<ul>
<li><code>a： anchor</code></li>
<li><code>p： positive</code>, 与 <code>a</code> 是同一类别的样本</li>
<li><code>n： negative</code>, 与 <code>a</code> 是不同类别的样本</li>
</ul>
<p>公式是：<br>
\[
L = max(d(a,p)-d(a,n)+margin,0)
\]<br>
最终的优化目标是拉近 <code>a, p</code> 的距离， 拉远 <code>a, n</code> 的距离</p>
</blockquote>
<p>这两种学习方法之间并无本质区别，其目标都是最大化类内相似度\(S_p\)和最小化类间相似度\(S_n\)。从这个角度看，很多常用的损失函数（如 triplet 损失、softmax 损失及其变体）有着相似的优化模式：它们会将\(S_n\)和\(S_p\)组合成相似对来优化，并试图减小\((S_n-S_p)\)。在\((S_n - S_p)\)中，增大\(S_p\)等效于降低\(S_n\)。这种对称的优化方法容易出现下面两个问题：</p>
<p><img src="/2020/06/07/deeplearning/loss/circle_loss/image-20200607122847009.png" alt></p>
<h2 id="1-you-hua-que-fa-ling-huo-xing">1.  优化缺乏灵活性</h2>
<p>\(S_n\)和\(S_p\)上的惩罚力度是严格相等的。换而言之，给定指定的损失函数，在\(S_n\)和\(S_p\)上的梯度的幅度总是一样的。例如图<code>(a)</code>中A点，它的\(S_n\)已经很小了，但是\(S_n\)会不断受到较大的梯度。这样的现象低效切不合理。</p>
<h2 id="2-shou-lian-zhuang-tai-bu-ming-que">2. 收敛状态不明确</h2>
<p>优化\((S_n-S_p)\)得到的决策边界为\(S_n-S_p=m\)。这个决策边界平行于\(S_n=S_p\),维持边界上任意两个点(比如\(T=(0.4,0.7)\)和\(T^\prime (0.2,0.5)\))的对应难度相等，这种决策边界允许模棱两可的收敛状态。比如，\(T\)和\(T^\prime\)都满足了\(S_p-S_n=0.3\)的目标，可是比较两者时，会发现二者之间的分离量只有0.1\((S_p^\prime -S-n) = 0.1\),从而降低了特征空间的可分性。</p>
<h1 id="jian-jie">简介</h1>
<p>为此，旷视研究院仅仅做了一项非常简单的改变，把\((S_n-S_p)\)泛化为\((\alpha_nS_n-\alpha_pS_p)\),从而允许\(S_n\)和\(S_p\)能以各自不同的步调学习。</p>
<p>具体来讲，把\(\alpha_n\)和\(\alpha_p\) 分别实现为\(S_n\) 和\(S_p\) 各自的线性函数，使学习速度与优化状态相适应。相似度分数偏离最优值越远，加权因子就越大。如此优化得到的决策边界为\(\alpha_nS_n-\alpha_pS_p=m\),能够证明这个分界面是\((S_n,S_p)\) 空间中的一段圆弧，因此，这一新提出的损失函数称之为 <code>Circle Loss</code>，即圆损失函数。<br>
由图 (a) 可知，降低\(S_n-S_p\) 容易导致优化不灵活（A、B、C 相较于\(S_n\)和\(S_p\)的梯度都相等）以及收敛状态不明确（决策边界上的 \(T\) 和 \(T\prime\) 都可接受）；而在 <code>Circle Loss</code> 所对应的图 (b) 中，减小\((\alpha_nS_n-\alpha_pS_p)\)  会动态调整其在\(S_n\) 和\(S_p\) 上的梯度，由此能使优化过程更加灵活。</p>
<p>对于状态 A，它的\(S_p\)很小（而\(S_n\) 已经足够小），因此其重点是增大\(S_p\) ；对于 B，它的 \(S_n\) 很大 （而\(S_p\)已经足够大），因此其重点是降低\(S_n\) 。此外，圆形决策边界上的特定点 \(T\)（圆弧与45度斜线的切点）更有利于收敛。因此，<code>Circle Loss</code> 设计了一个更灵活的优化途径，通向一个更明确的优化目标。</p>
<p><code>Circle Loss</code> 非常简单，而它对深度特征学习的意义却非常本质，表现为以下三个方面:</p>
<ol>
<li>统一的（广义）损失函数。从统一的相似度配对优化角度出发，它为两种基本学习范式（即使用类别标签和使用样本对标签的学习）提出了一种统一的损失函数；</li>
<li>灵活的优化方式。在训练期间，向 \(S_n\) 或\(S_p\) 的梯度反向传播会根据权重\(\alpha_n\) 或\(\alpha_p\) 来调整幅度大小。那些优化状态不佳的相似度分数，会被分配更大的权重因子，并因此获得更大的更新梯度。如图 (b) 所示，在 <code>Circle Loss</code> 中，A、B、C 三个状态对应的优化各有不同。</li>
<li>明确的收敛状态。在这个圆形的决策边界上，Circle Loss 更偏爱特定的收敛状态（图  (b) 中的 \(T\)）。这种明确的优化目标有利于提高特征鉴别力。</li>
</ol>
<h1 id="tong-yi-de-xiang-si-xing-you-hua-shi-jiao">统一的相似性优化视角</h1>
<p>深度特征学习的优化目标是最大化 \(S_p\) ，最小化\(S_n\) 。在两种基本学习范式中，采用的损失函数通常大相径庭，比如sofmax loss 和 triplet loss。<br>
这里不去在意相似性计算的具体方式——无论是样本对之间的相似性（相似性对标签情况下）还是样本与类别代理之间的相似性（类别标签情况下）。本文仅仅做这样一个假设定义：给定特征空间中的单个样本 \(x\)，假设与\(x\)相关的类内相似度分数有\(K\) 个，与 \(x\) 相关的类间相似度分数有\(L\) 个，分别记为 \(\{S^i_p\}(i=1,2,\ldots,K)\) 和 \(\{S^j_n\}(j=1,2,\ldots,L)\)。</p>
<p>为了实现最大化\(S_p\)最小化\(S_n\)的优化目标，本文提出把所有的\(S_p\)和\(S_n\) 两两配对，并通过在所有的相似性对上穷举、减小二者之差，来获得以下的统一损失函数：<br>
\[
\begin{aligned}
\mathcal{L}_{u n i} &amp;=\log \left[1+\sum_{i=1}^{K} \sum_{j=1}^{L} \exp \left(\gamma\left(s_{n}^{j}-s_{p}^{i}+m\right)\right)\right] \\
&amp;=\log \left[1+\sum_{j=1}^{L} \exp \left(\gamma\left(s_{n}^{j}+m\right)\right) \sum_{i=1}^{K} \exp \left(\gamma\left(-s_{p}^{i}\right)\right)\right]
\end{aligned}
\]<br>
这个公式仅需少量修改就能降级得到常见的 triplet 损失或分类损失，比如得到:</p>
<p>AM-Softmax 损失。</p>
<blockquote>
<p>Softmax loss通常擅长优化类间差异（即，分离不同的类），但是不善于减少类内的变化（即，使相同类的特征紧凑）。AM-Softmax 将角度间距（angular margin）引入到softmax中，来最小化类内变化。</p>
</blockquote>
<p>\[
\begin{aligned}
\mathcal{L}_{a m} &amp;=\log \left[1+\sum_{j=1}^{N-1} \exp \left(\gamma\left(s_{n}^{j}+m\right)\right) \exp \left(-\gamma s_{p}\right)\right] \\
&amp;=-\log \frac{\exp \left(\gamma\left(s_{p}-m\right)\right)}{\exp \left(\gamma\left(s_{p}-m\right)\right)+\sum_{j=1}^{N-1} \exp \left(\gamma s_{n}^{j}\right)}
\end{aligned}
\]</p>
<p>或 triplet 损失：<br>
\[
\begin{aligned}
\mathcal{L}_{t r i} &amp;=\lim _{\gamma \rightarrow+\infty} \frac{1}{\gamma} \mathcal{L}_{u n i} \\
&amp;=\lim _{\gamma \rightarrow+\infty} \frac{1}{\gamma} \log \left[1+\sum_{i=1}^{K} \sum_{j=1}^{L} \exp \left(\gamma\left(s_{n}^{j}-s_{p}^{i}+m\right)\right)\right] \\
&amp;=\max \left[s_{n}^{j}-s_{p}^{i}\right]_{+}
\end{aligned}
\]</p>
<h1 id="circle-loss-zi-ding-bu-diao-de-jia-quan-fang-shi">Circle Loss：自定步调的加权方式</h1>
<p>忽略上文<code>Circle Loss</code>等式中的余量项 \(m\)并对\(S_n\) 和\(S_p\)  进行加权，可得到新提出的 <code>Circle Loss</code>：<br>
\[
\begin{aligned}
\mathcal{L}_{\text {circle }} &amp;=\log \left[1+\sum_{i=1}^{K} \sum_{j=1}^{L} \exp \left(\gamma\left(\alpha_{n}^{j} s_{n}^{j}-\alpha_{p}^{i} s_{p}^{i}\right)\right)\right] \\
&amp;=\log \left[1+\sum_{j=1}^{L} \exp \left(\gamma \alpha_{n}^{j} s_{n}^{j}\right) \sum_{i=1}^{K} \exp \left(-\gamma \alpha_{p}^{i} s_{p}^{i}\right)\right.
\end{aligned}
\]<br>
再定义\(S_p\)  的最优值为\(O_p\)，\(S_n\) 的最优值为\(O_n\)；\(O_n &lt; O_p\) 。当一个相似性得分与最优值偏离较远，<code>Circle Loss</code> 将分配较大的权重，从而对它进行强烈的优化更新。自定步调（self-paced）的方式给出了如下定义：<br>
\[
\left\{\begin{array}{c}
\alpha_{p}^{i}=\left[O_{p}-s_{p}^{i}\right]_{+} \\
\alpha_{n}^{j}=\left[s_{n}^{j}-O_{n}\right]_{+}
\end{array}\right.
\]</p>
<h1 id="lei-nei-yu-liang-he-lei-jian-yu-liang">类内余量和类间余量</h1>
<p>不同于优化\(S_n-S_p\) 的损失函数，在 <code>Circle Loss</code> 中， \(S_n\) 和 \(S_p\) 是不对称的，论文为其各自定义了余量\(\Delta_n\)和\(\Delta_p\),这样可得到最终带余量的 <code>Circle Loss</code>：<br>
\[
\mathcal{L}_{\text {circle}}=\log \left[1+\sum_{j=1}^{L} \exp \left(\gamma \alpha_{n}^{j}\left(s_{n}^{j}-\Delta_{n}\right)\right) \sum_{i=1}^{K} \exp \left(-\gamma \alpha_{p}^{i}\left(s_{p}^{i}-\Delta_{p}\right)\right)\right]
\]<br>
通过推导决策边界，论文进一步分析\(\Delta_n\)和\(\Delta_p\)。为简单起见，这里以二元分类的情况进行说明，其中决策边界是在\(\alpha_n(S_n-\Delta_n)-\alpha_p(s_p-\Delta_p)=0\)处得到。结合:<br>
\[
\left\{\begin{array}{c}
\alpha_{p}^{i}=\left[O_{p}-s_{p}^{i}\right]_{+} \\
\alpha_{n}^{j}=\left[s_{n}^{j}-O_{n}\right]_{+}
\end{array}\right.
\]<br>
可得决策边界：<br>
\[
\left(s_{n}-\frac{O_{n}+\Delta_{n}}{2}\right)^{2}+\left(s_{p}-\frac{O_{p}+\Delta_{p}}{2}\right)^{2}=C
\]<br>
其中，\(C=\frac{\left(O_{n}-\Delta_{n}\right)^{2}+\left(O_{p}-\Delta_{p}\right)^{2}}{4}\)。</p>
<p><code>Circle Loss</code> 有 5 个超参数，即\(O_p\) 、\(O_n\)、\(\gamma\) 、\(\Delta_n\)和\(\Delta_p\)。通过将\(O_p=1+m,O_n=-m,\Delta_p=1-m,\Delta_n=m\) 。可将上式约简为：<br>
\[
\left(s_{n}-0\right)^{2}+\left(s_{p}-1\right)^{2}=2 m^{2}
\]<br>
基于上式定义的决策边界，可对 <code>Circle Loss</code> 进行另外一番解读。其目标是优化 \(S_p\) --&gt; 1 和\(S_n\) --&gt; 0。参数 \(m\)控制着决策边界的半径，并可被视为一个松弛因子。换句话说，<code>Circle Loss</code> 期望\(S_p^i > 1-m\)且 \(s^i_n &lt; m\)。因此，超参数仅有 2 个，即扩展因子 \(\gamma\) 和松弛因子\(m\) 。</p>
<blockquote>
<p>\(m\)(即半径)越大，损失越小。</p>
</blockquote>
<h1 id="you-shi">优势</h1>
<p><code>Circle Loss</code> 在\(S_n^j\) 和 \(S^i_p\)上的梯度分别为：<br>
\[
\begin{array}{l}
\frac{\partial \mathcal{L}_{\text {circle}}}{\partial s_{n}^{j}}=Z \frac{\exp \left(\gamma\left(\left(s_{n}^{j}\right)^{2}-m^{2}\right)\right)}{\sum_{l=1}^{L} \exp \left(\gamma\left(\left(s_{n}^{l}\right)^{2}-m^{2}\right)\right)} \gamma\left(s_{n}^{j}+m\right) \\
\frac{\partial \mathcal{L}_{\text {circle}}}{\partial s_{p}^{i}}=Z \frac{\exp \left(\gamma\left(m^{2}-\left(s_{p}^{i}-1\right)^{2}\right)\right)}{\sum_{k=1}^{K} \exp \left(\gamma\left(m^{2}-\left(s_{p}^{k}-1\right)^{2}\right)\right)} \gamma\left(s_{p}^{i}-1-m\right)
\end{array}
\]<br>
下图在二元分类的实验场景中可视化了不同 \(m\)值设置下的梯度情况，对比图中(a) 和 (b) 的triplet 损失和 AMSoftmax 损失的梯度，可知 Circle Loss 有这些优势：</p>
<ol>
<li>在\(S_n\) 和 \(S_p\)上能进行平衡的优化</li>
<li>梯度会逐渐减弱</li>
<li>收敛目标更加明确</li>
</ol>
<p><img src="/2020/06/07/deeplearning/loss/circle_loss/image-20200607211829089.png" alt></p>
<p>上图的可视化结果表明，triplet 损失和 AMSoftmax 损失都缺乏优化的灵活性。它们相对于 \(S_n\)（左图）和 \(S_p\)（右图）的梯度严格相等，而且在收敛方面出现了陡然的下降（相似度配对 B）。比如，在 A 处，类内相似度分数\(S_p\) 已接近 1 ，但仍出现了较大的梯度。此外，决策边界平行于\(S_p=S_n\) ，这会导致收敛不明确。</p>
<p>相对而言，新提出的 <code>Circle Loss</code> 可根据相似性得分与最优值的距离，动态地为相似度分数分配不同的梯度。对于 A（ \(S_n\)和\(S_p\)都很大），<code>Circle Loss</code> 的重点是优化 \(S_n\) ；对于 B，因为\(S_n\)  显著下降，<code>Circle Loss</code> 会降低它的梯度，并因此会施加温和的优化。</p>
<p><code>Circle Loss</code> 的决策边界是圆形的，与 \(S_n-S_p=m\) 直线有着明确的切点，而这个切点将成为明确的收敛目标。这是因为，对于同样的损失值，该切点具有最小的类间-类内差距，是最容易维持的。</p>
<h1 id="dai-ma">代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_label_to_similarity</span><span class="params">(normed_feature: Tensor, label: Tensor)</span> -&gt; Tuple[Tensor, Tensor]:</span></span><br><span class="line">    similarity_matrix = normed_feature @ normed_feature.transpose(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    label_matrix = label.unsqueeze(<span class="number">1</span>) == label.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    positive_matrix = label_matrix.triu(diagonal=<span class="number">1</span>)</span><br><span class="line">    negative_matrix = torch.ByteTensor(np.logical_not(label_matrix.cpu().detach().numpy())).cuda().triu(diagonal=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    similarity_matrix = similarity_matrix.view(<span class="number">-1</span>)</span><br><span class="line">    positive_matrix = positive_matrix.view(<span class="number">-1</span>)</span><br><span class="line">    negative_matrix = negative_matrix.view(<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> similarity_matrix[positive_matrix], similarity_matrix[negative_matrix]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CircleLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, m: float, gamma: float)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        super(CircleLoss, self).__init__()</span><br><span class="line">        self.m = m</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.soft_plus = nn.Softplus()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, sp: Tensor, sn: Tensor)</span> -&gt; Tensor:</span></span><br><span class="line">        ap = torch.clamp_min(- sp.detach() + <span class="number">1</span> + self.m, min=<span class="number">0.</span>)</span><br><span class="line">        an = torch.clamp_min(sn.detach() + self.m, min=<span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line">        delta_p = <span class="number">1</span> - self.m</span><br><span class="line">        delta_n = self.m</span><br><span class="line"></span><br><span class="line">        logit_p = - ap * (sp - delta_p) * self.gamma</span><br><span class="line">        logit_n = an * (sn - delta_n) * self.gamma</span><br><span class="line"></span><br><span class="line">        loss = self.soft_plus(torch.logsumexp(logit_n, dim=<span class="number">0</span>) + torch.logsumexp(logit_p, dim=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    feat = nn.functional.normalize(torch.rand(<span class="number">256</span>, <span class="number">64</span>, requires_grad=<span class="literal">True</span>))</span><br><span class="line">    lbl = torch.randint(high=<span class="number">10</span>, size=(<span class="number">256</span>,))</span><br><span class="line"></span><br><span class="line">    inp_sp, inp_sn = convert_label_to_similarity(feat, lbl)</span><br><span class="line"></span><br><span class="line">    criterion = CircleLoss(m=<span class="number">0.25</span>, gamma=<span class="number">256</span>)</span><br><span class="line">    circle_loss = criterion(inp_sp, inp_sn)</span><br><span class="line"></span><br><span class="line">    print(circle_loss)</span><br></pre></td></tr></table></figure>
<h1 id="jie-lun">结论</h1>
<p>本文对深度特征学习做出了两项深刻理解。第一，包括 triplet 损失和常用的分类损失函数在内的大多数损失函数具有统一的内在形式，它们都将类间相似度与类内相似度嵌入到相似性配对中进行优化。第二，在相似度配对内部，考虑各个相似度得分偏离理想状态的程度不同，应该给予它们不同的优化强度。将这两项理解联合起来，便得到 <code>Circle Loss</code>。</p>
<p>通过让每个相似性得分以不同的步调学习，<code>Circle Loss</code> 赋予深度特征学习的更灵活的优化途径，以及更明确的收敛目标；并且，它为两种基本学习范式（样本对和分类学习）提供了统一的解读以及统一的数学公式。</p>
<h1 id="shi-ji">实际</h1>
<ol>
<li>使用fasttext在文本分类中，使用该loss没有达到直接使用softmax + cross entropy 的效果差了0.07%左右。可能是我参数没有调好。</li>
<li>该loss需要花费额外的精力去进行调参，在实在没有办法的情况下，可以尝试使用该loss。</li>
</ol>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://blog.csdn.net/u013602059/article/details/105202917" target="_blank" rel="noopener">Circle Loss: A Unified Perspective of Pair Similarity Optimization 圆损失函数，统一优化视角，革新深度特征学习范式 CVPR 2020</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/优化算法</category>
      </categories>
      <tags>
        <tag>损失</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title>latex</title>
    <url>/2020/06/06/basic_skills/latex/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/06/06/basic_skills/latex/timg.jpeg" alt></p>
<a id="more"></a>
<h1 id="an-zhuang-ji-pei-zhi-mac">安装及配置（mac）</h1>
<h2 id="an-zhuang">安装</h2>
<ol>
<li><a href="http://www.tug.org/mactex/mactex-download.html" target="_blank" rel="noopener">Downloading MacTex</a>，并进行安装。</li>
<li><a href="https://code.visualstudio.com/download" target="_blank" rel="noopener">Downloading VScode</a>,并进行安装。</li>
</ol>
<h2 id="pei-zhi">配置</h2>
<ol>
<li>
<p>打开 vscode 的扩展，搜索插件 LaTeX WorkShop 并进行安装。<img src="/2020/06/06/basic_skills/latex/image-20200606223402471.png" alt></p>
</li>
<li>
<p>安装完成后，即可编译英文 Tex 文件。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line">    Hello World.</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p>如新建<code>HelloWorld.tex</code>文件，选择<code>Build Latex Project</code> 即可得到如下图结果：</p>
<p><img src="/2020/06/06/basic_skills/latex/image-20200606223733559.png" alt></p>
</li>
<li>
<p>配置中文环境：LaTeX WorkShop 插件默认只提供<code>PDFLaTeX</code>，而中文编译需要<code>XeFLaTeX</code>，所以还需另行配置。</p>
<ol>
<li>
<p>依次选择<code>Code &gt; Preferences &gt; Settings</code>，搜索latex.tool，点击在settings.json中编辑。</p>
<p><img src="/2020/06/06/basic_skills/latex/image-20200606224100719.png" alt></p>
</li>
<li>
<p>修改对应代码如下，或者对相应部分进行替换</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">"latex-workshop.latex.tools": [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"xelatex"</span>,</span><br><span class="line">          <span class="attr">"command"</span>: <span class="string">"xelatex"</span>,</span><br><span class="line">          <span class="attr">"args"</span>: [</span><br><span class="line">            <span class="string">"-synctex=1"</span>,</span><br><span class="line">            <span class="string">"-interaction=nonstopmode"</span>,</span><br><span class="line">            <span class="string">"-file-line-error"</span>,</span><br><span class="line">            <span class="string">"%DOC%"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"pdflatex"</span>,</span><br><span class="line">          <span class="attr">"command"</span>: <span class="string">"pdflatex"</span>,</span><br><span class="line">          <span class="attr">"args"</span>: [</span><br><span class="line">            <span class="string">"-synctex=1"</span>,</span><br><span class="line">            <span class="string">"-interaction=nonstopmode"</span>,</span><br><span class="line">            <span class="string">"-file-line-error"</span>,</span><br><span class="line">            <span class="string">"%DOC%"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"latexmk"</span>,</span><br><span class="line">          <span class="attr">"command"</span>: <span class="string">"latexmk"</span>,</span><br><span class="line">          <span class="attr">"args"</span>: [</span><br><span class="line">            <span class="string">"-synctex=1"</span>,</span><br><span class="line">            <span class="string">"-interaction=nonstopmode"</span>,</span><br><span class="line">            <span class="string">"-file-line-error"</span>,</span><br><span class="line">            <span class="string">"-pdf"</span>,</span><br><span class="line">            <span class="string">"%DOC%"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"bibtex"</span>,</span><br><span class="line">          <span class="attr">"command"</span>: <span class="string">"bibtex"</span>,</span><br><span class="line">          <span class="attr">"args"</span>: [</span><br><span class="line">            <span class="string">"%DOCFILE%"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;</span><br><span class="line">      ],</span><br><span class="line"></span><br><span class="line">      "latex-workshop.latex.recipes": [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"XeLaTeX"</span>,</span><br><span class="line">          <span class="attr">"tools"</span>: [</span><br><span class="line">            <span class="string">"xelatex"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"PDFLaTeX"</span>,</span><br><span class="line">          <span class="attr">"tools"</span>: [</span><br><span class="line">            <span class="string">"pdflatex"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"latexmk"</span>,</span><br><span class="line">          <span class="attr">"tools"</span>: [</span><br><span class="line">            <span class="string">"latexmk"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"BibTeX"</span>,</span><br><span class="line">          <span class="attr">"tools"</span>: [</span><br><span class="line">            <span class="string">"bibtex"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"xelatex -&gt; bibtex -&gt; xelatex*2"</span>,</span><br><span class="line">          <span class="attr">"tools"</span>: [</span><br><span class="line">            <span class="string">"xelatex"</span>,</span><br><span class="line">            <span class="string">"bibtex"</span>,</span><br><span class="line">            <span class="string">"xelatex"</span>,</span><br><span class="line">            <span class="string">"xelatex"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"pdflatex -&gt; bibtex -&gt; pdflatex*2"</span>,</span><br><span class="line">          <span class="attr">"tools"</span>: [</span><br><span class="line">            <span class="string">"pdflatex"</span>,</span><br><span class="line">            <span class="string">"bibtex"</span>,</span><br><span class="line">            <span class="string">"pdflatex"</span>,</span><br><span class="line">            <span class="string">"pdflatex"</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">    ],</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>新建Hello.tex 如下：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line">中文</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609133419903.png" alt></p>
</li>
</ol>
</li>
</ol>
<h1 id="latex-yu-fa">latex 语法</h1>
<h2 id="hello-world">Hello world</h2>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 这里是导言区</span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line">Hello, world!</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p>此处的第一行 <code>\documentclass{article}</code> 中包含了一个控制序列（或称命令/标记）。所谓控制序列，是以反斜杠 <code>\</code> 开头，以第一个<strong>空格或非字母</strong>的字符结束的一串文字。它们不被输出，但是他们会影响输出文档的效果。这里的控制序列是 <code>documentclass</code>，它后面紧跟着的 <code>{article}</code> 代表这个控制序列有一个必要的参数，该参数的值为 <code>article</code>。这个控制序列的作用，是调用名为 <code>article</code> 的文档类。</p>
<span class="label danger">注意，TeX 对控制序列的大小写是敏感的。</span>
<blockquote>
<p>部分控制序列还有被方括号 <code>[]</code> 包括的可选参数。</p>
<p>所谓文档类，即是 TeX 系统预设的（或是用户自定的）一些格式的集合。不同的文档类在输出效果上会有差别。</p>
</blockquote>
<p>处的第二行以 <code>%</code> 开头。TeX 以百分号 <code>%</code> 作为注释标记。具体来说，TeX 会忽略从 <code>%</code> 开始到当前行末尾的所有内容。这些内容不会被输出，也不影响最终排版效果，只供人类阅读。若要输出 <code>%</code> 字符本身，则需要在 <code>%</code> 之前加上反斜杠 <code>\</code> 进行转义（escape）。例如：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">今年的净利润为 20<span class="tag">\<span class="name">%</span></span>，比去年高。</span><br></pre></td></tr></table></figure>
<p>此处 <code>%</code> 被当做正常的百分号处理，其后的文字也将被正常输出。</p>
<p>在注释行之后出现了控制序列 <code>begin</code>。这个控制序列总是与 <code>end</code> 成对出现。这两个控制序列以及他们中间的内容被称为「环境」；它们之后的第一个必要参数总是<strong>一致的</strong>，被称为环境名。</p>
<p>只有在 <code>document</code> 环境中的内容，才会被正常输出到文档中去或是作为控制序列对文档产生影响。也就是说，在 <code>\end{document}</code> 之后插入任何内容都是无效的。</p>
<p>从 <code>\documentclass{article}</code> 开始到 <code>\begin{document}</code> 之前的部分被称为导言区。导言区是对整篇文档进行设置的区域——在导言区出现的控制序列，往往会影响整篇文档的格式。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">比如，通常在导言区设置页面大小、页眉页脚样式、章节标题样式等等。</span><br></pre></td></tr></table></figure>
<h2 id="zhong-ying-wen-hun-pai">中英文混排</h2>
<p>TeX 系统是高教授开发的。在 TeX 开发当初并没有考虑到亚洲文字的问题。因此早期的 TeX 系统并不能直接支持中文，必须要用其他工具先处理一下（或者是一些宏包之类的）。但是现在，XeTeX 原生支持 Unicode，并且可以方便地调用系统字体。至此，只需要使用几个简单的宏包，就能完成中文支持了。</p>
<p>所谓宏包，就是一系列控制序列的合集。这些控制序列太常用，以至于人们会觉得每次将他们写在导言区太过繁琐，于是将他们打包放在同一个文件中，成为所谓的宏包。<code>\usepackage{}</code> 可以用来调用宏包。</p>
<p>除去中文支持，中文的版式处理和标点则也是不小的挑战。好在 <code>CTeX</code> 宏集一次性解决了这些问题。<code>CTeX</code> 宏集的优势在于，它能适配于多种编译方式；在内部处理好了中文和中文版式的支持，隐藏了这些细节；并且，提供了不少中文用户需要的功能接口。</p>
<blockquote>
<p><code>CTeX</code> 宏集和 <code>CTeX</code> 套装是两个不同的东西。<code>CTeX</code> 宏集本质是 LaTeX 宏的集合，包含若干文档类（<code>.cls</code> 文件）和宏包（<code>.sty</code> 文件）。<code>CTeX</code> 套装是一个<strong>过时的</strong> TeX 系统。</p>
<p>新版 <code>CTeX</code> 宏集的默认能够自动检测用户的操作系统，并为之配置合适的字库。对于 Windows 用户、Mac OS X 用户和 Linux 用户，都无需做任何配置，就能使用 <code>CTeX</code> 宏集来排版中文。</p>
</blockquote>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;ctexart&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line">你好，world!</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609124748583.png" alt></p>
<p>相较于之前的例子，这份代码只有细微的差异：</p>
<ol>
<li>文档类从 <code>article</code> 变为 <code>ctexart</code>；</li>
<li>增加了文档类选项 <code>UTF8</code>。</li>
</ol>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line">你好，world!</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609124719918.png" alt></p>
<h2 id="zuo-zhe-biao-ti-ri-qi">作者、标题、日期</h2>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">title</span><span class="string">&#123;this is a title&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">author</span><span class="string">&#123;Jeffery&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">date</span><span class="string">&#123;\today&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">maketitle</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">% body</span></span><br><span class="line">你好，world!</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609122604811.png" alt></p>
<p>导言区复杂了很多，但和之前的文档主要的区别只有一处：定义了标题、作者、日期。</p>
<p>在 <code>document</code> 环境中，除了原本的<code>你好，world!</code>，还多了一个控制序列 <code>maketitle</code>。这个控制序列能将在导言区中定义的标题、作者、日期按照预定的格式展现出来。</p>
<blockquote>
<p>使用<code>titling</code>宏包可以修改上述默认格式。</p>
</blockquote>
<h2 id="zhang-jie-he-duan-luo">章节和段落</h2>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">title</span><span class="string">&#123;this is a title&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">author</span><span class="string">&#123;Jeffery&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">date</span><span class="string">&#123;\today&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">maketitle</span></span></span><br><span class="line"><span class="tag">\<span class="name">section</span><span class="string">&#123;你好中国&#125;</span></span></span><br><span class="line">中国在East Asia.</span><br><span class="line"><span class="tag">\<span class="name">subsection</span><span class="string">&#123;Hello Beijing&#125;</span></span></span><br><span class="line">北京是capital of China.</span><br><span class="line"><span class="tag">\<span class="name">subsubsection</span><span class="string">&#123;Hello Dongcheng District&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">paragraph</span><span class="string">&#123;Tian'anmen Square&#125;</span></span></span><br><span class="line">is in the center of Beijing</span><br><span class="line"><span class="tag">\<span class="name">subparagraph</span><span class="string">&#123;Chairman Mao&#125;</span></span></span><br><span class="line">is in the center of 天安门广场。</span><br><span class="line"><span class="tag">\<span class="name">subsection</span><span class="string">&#123;Hello 江苏&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">paragraph</span><span class="string">&#123;东南大学&#125;</span></span> is one of the best university in 江苏。</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609123108736.png" alt></p>
<p>在文档类 <code>article</code>/<code>ctexart</code> 中，定义了五个控制序列来调整行文组织结构。他们分别是</p>
<ul>
<li><code>\section{·}</code></li>
<li><code>\subsection{·}</code></li>
<li><code>\subsubsection{·}</code></li>
<li><code>\paragraph{·}</code></li>
<li><code>\subparagraph{·}</code></li>
</ul>
<blockquote>
<p>在<code>report</code>/<code>ctexrep</code>中，还有<code>\chapter{·}</code>；</p>
<p>在文档类<code>book</code>/<code>ctexbook</code>中，还定义了<code>\part{·}</code>。</p>
</blockquote>
<h2 id="cha-ru-mu-lu">插入目录</h2>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">title</span><span class="string">&#123;this is a title&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">author</span><span class="string">&#123;Jeffery&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">date</span><span class="string">&#123;\today&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="comment">% \tableofcontents 用于生成目录</span></span><br><span class="line"><span class="tag">\<span class="name">tableofcontents</span></span></span><br><span class="line"><span class="tag">\<span class="name">maketitle</span></span></span><br><span class="line"><span class="tag">\<span class="name">section</span><span class="string">&#123;你好中国&#125;</span></span></span><br><span class="line">中国在East Asia.</span><br><span class="line"><span class="tag">\<span class="name">subsection</span><span class="string">&#123;Hello Beijing&#125;</span></span></span><br><span class="line">北京是capital of China.</span><br><span class="line"><span class="tag">\<span class="name">subsubsection</span><span class="string">&#123;Hello Dongcheng District&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">paragraph</span><span class="string">&#123;Tian'anmen Square&#125;</span></span></span><br><span class="line">is in the center of Beijing</span><br><span class="line"><span class="tag">\<span class="name">subparagraph</span><span class="string">&#123;Chairman Mao&#125;</span></span></span><br><span class="line">is in the center of 天安门广场。</span><br><span class="line"><span class="tag">\<span class="name">subsection</span><span class="string">&#123;Hello 江苏&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">paragraph</span><span class="string">&#123;东南大学&#125;</span></span> is one of the best university in 江苏。</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609123511051.png" alt></p>
<h2 id="cha-ru-shu-xue-gong-shi">插入数学公式</h2>
<p>为了使用 AMS-LaTeX 提供的数学功能，需要在导言区加载 <code>amsmath</code> 宏包：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br></pre></td></tr></table></figure>
<h3 id="shu-xue-mo-shi">数学模式</h3>
<p>LaTeX 的数学模式有两种：行内模式 (inline) 和行间模式 (display)。前者在正文的行文中，插入数学公式；后者独立排列单独成行，并自动居中。</p>
<p>在行文中，使用 <code>$ ... $</code> 可以插入行内公式，使用 <code>\[ ... \]</code> 可以插入行间公式，如果需要对行间公式进行编号，则可以使用 <code>equation</code> 环境：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;equation&#125;</span></span></span><br><span class="line">...</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;equation&#125;</span></span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>行内公式也可以使用 <code>\(...\)</code> 或者 <code>\begin{math} ... \end{math}</code> 来插入，但略显麻烦。</p>
<p>无编号的行间公式也可以使用 <code>\begin{displaymath} ... \end{displaymath}</code> 或者 <code>\begin{equation*} ... \end{equation*}</code> 来插入，但略显麻烦。（<code>equation*</code> 中的 <code>*</code> 表示环境不编号）<br>
也有 plainTeX 风格的 <code>$$ ... $$</code> 来插入不编号的行间公式。但是在 LaTeX 中这样做会改变行文的默认行间距，不推荐。</p>
</blockquote>
<h3 id="shang-xia-biao">上下标</h3>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="comment">% 行内公式</span></span><br><span class="line">Einstein 's <span class="formula">$E=mc^2$</span>.</span><br><span class="line"><span class="comment">% 行间公式 不带编号</span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> E=mc^2. <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="comment">% 行间公式 带编号</span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;equation&#125;</span></span></span><br><span class="line">E=mc^2.</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;equation&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609124558745.png" alt></p>
<blockquote>
<p>行内公式和行间公式对标点的要求是不同的：行内公式的标点，应该放在数学模式的限定符之外，而行间公式则应该放在数学模式限定符之内。</p>
<p>数学模式中，需要表示上标，可以使用 <code>^</code> 来实现（下标则是 <code>_</code>）。<strong>它默认只作用于之后的一个字符</strong>，如果想对连续的几个字符起作用，请将这些字符用花括号 <code>{}</code> 括起来</p>
</blockquote>
<h3 id="gen-shi-yu-fen-shi">根式与分式</h3>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">sqrt</span><span class="string">&#123;x&#125;</span></span>$</span>, <span class="formula">$<span class="tag">\<span class="name">frac</span><span class="string">&#123;1&#125;</span><span class="string">&#123;2&#125;</span></span>$</span>.</span><br><span class="line"><span class="comment">% 根号</span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">sqrt</span><span class="string">&#123;x&#125;</span></span>, <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="comment">% 分式</span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">frac</span><span class="string">&#123;1&#125;</span><span class="string">&#123;2&#125;</span></span>. <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609125352216.png" alt></p>
<p>在行间公式和行内公式中，分式的输出效果是有差异的。如果要强制行内模式的分式显示为行间模式的大小，可以使用 <code>\dfrac</code>(变大), 反之可以使用 <code>\tfrac</code>。</p>
<blockquote>
<p>在行内写分式，推荐 <code>xfrac</code> 宏包提供的 <code>\sfrac</code> 命令的效果。</p>
<p>排版繁分式，使用 <code>\cfrac</code> 命令。</p>
</blockquote>
<h3 id="yun-suan-fu">运算符</h3>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="comment">% 加减</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">pm</span></span>$</span>  </span><br><span class="line"></span><br><span class="line"><span class="comment">%乘</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">times</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 除</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">div</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 点乘</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">cdot</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 交</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">cap</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 并</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">cup</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 大于等于</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">geq</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 小于等于</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">leq</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 不等于</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">neq</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 约等于</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">approx</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 恒等于</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">equiv</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 求和</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">sum</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 连乘</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">prod</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 极限</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">lim</span></span>$</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 积分</span></span><br><span class="line"><span class="formula">$<span class="tag">\<span class="name">int</span></span>$</span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609130721090.png" alt></p>
<p>连加、连乘、极限、积分等大型运算符分别用 <code>\sum</code>, <code>\prod</code>, <code>\lim</code>, <code>\int</code> 生成。他们的上下标在行内公式中被压缩，以适应行高。我们可以用 <code>\limits</code> 和 <code>\nolimits</code> 来强制显式地指定是否压缩这些上下标。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="formula">$ <span class="tag">\<span class="name">sum</span></span>_&#123;i=1&#125;^n i<span class="tag">\<span class="name">quad</span></span> <span class="tag">\<span class="name">prod</span></span>_&#123;i=1&#125;^n $</span></span><br><span class="line"><span class="formula">$ <span class="tag">\<span class="name">sum</span></span><span class="tag">\<span class="name">limits</span></span> _&#123;i=1&#125;^n i<span class="tag">\<span class="name">quad</span></span> <span class="tag">\<span class="name">prod</span></span><span class="tag">\<span class="name">limits</span></span> _&#123;i=1&#125;^n $</span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">lim</span></span>_&#123;x<span class="tag">\<span class="name">to</span></span>0&#125;x^2 <span class="tag">\<span class="name">quad</span></span> <span class="tag">\<span class="name">int</span></span>_a^b x^2 dx <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">lim</span></span><span class="tag">\<span class="name">nolimits</span></span> _&#123;x<span class="tag">\<span class="name">to</span></span>0&#125;x^2<span class="tag">\<span class="name">quad</span></span> <span class="tag">\<span class="name">int</span></span><span class="tag">\<span class="name">nolimits</span></span>_a^b x^2 dx <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="comment">% 多重积分可以使用 `\iint`, `\iiint`, `\iiiint`, `\idotsint` 等命令输入。</span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">iint</span></span><span class="tag">\<span class="name">quad</span></span> <span class="tag">\<span class="name">iiint</span></span><span class="tag">\<span class="name">quad</span></span> <span class="tag">\<span class="name">iiiint</span></span><span class="tag">\<span class="name">quad</span></span> <span class="tag">\<span class="name">idotsint</span></span> <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609130934688.png" alt="image-20200609130934688"></p>
<h3 id="ding-jie-fu">定界符</h3>
<p>各种括号用 <code>()</code>, <code>[]</code>, <code>\{\}</code>, <code>\langle\rangle</code> 等命令表示；注意花括号通常用来输入命令和环境的参数，所以在数学公式中它们前面要加 <code>\</code>。因为 LaTeX 中 <code>|</code> 和 <code>\|</code> 的应用过于随意，amsmath 宏包推荐用 <code>\lvert\rvert</code> 和 <code>\lVert\rVert</code> 取而代之。</p>
<p>为了调整这些定界符的大小，amsmath 宏包推荐使用 <code>\big</code>, <code>\Big</code>, <code>\bigg</code>, <code>\Bigg</code> 等一系列命令放在上述括号前面调整大小。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">Biggl</span></span>(<span class="tag">\<span class="name">biggl</span></span>(<span class="tag">\<span class="name">Bigl</span></span>(<span class="tag">\<span class="name">bigl</span></span>((x)<span class="tag">\<span class="name">bigr</span></span>)<span class="tag">\<span class="name">Bigr</span></span>)<span class="tag">\<span class="name">biggr</span></span>)<span class="tag">\<span class="name">Biggr</span></span>) <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">Biggl</span><span class="string">[\biggl[\Bigl[\bigl[[x]</span></span><span class="tag">\<span class="name">bigr</span></span>]<span class="tag">\<span class="name">Bigr</span></span>]<span class="tag">\<span class="name">biggr</span></span>]<span class="tag">\<span class="name">Biggr</span></span>] <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">Biggl</span></span> <span class="tag">\<span class="name">&#123;</span></span><span class="tag">\<span class="name">biggl</span></span> <span class="tag">\<span class="name">&#123;</span></span><span class="tag">\<span class="name">Bigl</span></span> <span class="tag">\<span class="name">&#123;</span></span><span class="tag">\<span class="name">bigl</span></span> <span class="tag">\<span class="name">&#123;</span></span><span class="tag">\<span class="name">&#123;</span></span>x<span class="tag">\<span class="name">&#125;</span></span><span class="tag">\<span class="name">bigr</span></span> <span class="tag">\<span class="name">&#125;</span></span><span class="tag">\<span class="name">Bigr</span></span> <span class="tag">\<span class="name">&#125;</span></span><span class="tag">\<span class="name">biggr</span></span> <span class="tag">\<span class="name">&#125;</span></span><span class="tag">\<span class="name">Biggr</span></span><span class="tag">\<span class="name">&#125;</span></span> <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">Biggl</span></span><span class="tag">\<span class="name">langle</span></span><span class="tag">\<span class="name">biggl</span></span><span class="tag">\<span class="name">langle</span></span><span class="tag">\<span class="name">Bigl</span></span><span class="tag">\<span class="name">langle</span></span><span class="tag">\<span class="name">bigl</span></span><span class="tag">\<span class="name">langle</span></span><span class="tag">\<span class="name">langle</span></span> x</span><br><span class="line"><span class="tag">\<span class="name">rangle</span></span><span class="tag">\<span class="name">bigr</span></span><span class="tag">\<span class="name">rangle</span></span><span class="tag">\<span class="name">Bigr</span></span><span class="tag">\<span class="name">rangle</span></span><span class="tag">\<span class="name">biggr</span></span><span class="tag">\<span class="name">rangle</span></span><span class="tag">\<span class="name">Biggr</span></span><span class="tag">\<span class="name">rangle</span></span> <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">Biggl</span></span><span class="tag">\<span class="name">lvert</span></span><span class="tag">\<span class="name">biggl</span></span><span class="tag">\<span class="name">lvert</span></span><span class="tag">\<span class="name">Bigl</span></span><span class="tag">\<span class="name">lvert</span></span><span class="tag">\<span class="name">bigl</span></span><span class="tag">\<span class="name">lvert</span></span><span class="tag">\<span class="name">lvert</span></span> x</span><br><span class="line"><span class="tag">\<span class="name">rvert</span></span><span class="tag">\<span class="name">bigr</span></span><span class="tag">\<span class="name">rvert</span></span><span class="tag">\<span class="name">Bigr</span></span><span class="tag">\<span class="name">rvert</span></span><span class="tag">\<span class="name">biggr</span></span><span class="tag">\<span class="name">rvert</span></span><span class="tag">\<span class="name">Biggr</span></span><span class="tag">\<span class="name">rvert</span></span> <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">Biggl</span></span><span class="tag">\<span class="name">lVert</span></span><span class="tag">\<span class="name">biggl</span></span><span class="tag">\<span class="name">lVert</span></span><span class="tag">\<span class="name">Bigl</span></span><span class="tag">\<span class="name">lVert</span></span><span class="tag">\<span class="name">bigl</span></span><span class="tag">\<span class="name">lVert</span></span><span class="tag">\<span class="name">lVert</span></span> x</span><br><span class="line"><span class="tag">\<span class="name">rVert</span></span><span class="tag">\<span class="name">bigr</span></span><span class="tag">\<span class="name">rVert</span></span><span class="tag">\<span class="name">Bigr</span></span><span class="tag">\<span class="name">rVert</span></span><span class="tag">\<span class="name">biggr</span></span><span class="tag">\<span class="name">rVert</span></span><span class="tag">\<span class="name">Biggr</span></span><span class="tag">\<span class="name">rVert</span></span> <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609131349870.png" alt></p>
<h3 id="sheng-lue-hao">省略号</h3>
<p>省略号用 <code>\dots</code>, <code>\cdots</code>, <code>\vdots</code>, <code>\ddots</code> 等命令表示。</p>
<p><code>\dots</code> 和 <code>\cdots</code> 的纵向位置不同，前者一般用于有下标的序列。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> x_1,x_2,<span class="tag">\<span class="name">dots</span></span> ,x_n<span class="tag">\<span class="name">quad</span></span> 1,2,<span class="tag">\<span class="name">cdots</span></span> ,n<span class="tag">\<span class="name">quad</span></span></span><br><span class="line"><span class="tag">\<span class="name">vdots</span></span><span class="tag">\<span class="name">quad</span></span> <span class="tag">\<span class="name">ddots</span></span> <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609131538918.png" alt></p>
<h3 id="ju-zhen">矩阵</h3>
<p><code>amsmath</code> 的 <code>pmatrix</code>, <code>bmatrix</code>, <code>Bmatrix</code>, <code>vmatrix</code>, <code>Vmatrix</code> 等环境可以在矩阵两边加上各种分隔符。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> <span class="tag">\<span class="name">begin</span><span class="string">&#123;pmatrix&#125;</span></span> a&amp;b<span class="tag">\<span class="name">\</span></span>c&amp;d <span class="tag">\<span class="name">end</span><span class="string">&#123;pmatrix&#125;</span></span> <span class="tag">\<span class="name">quad</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;bmatrix&#125;</span></span> a&amp;b<span class="tag">\<span class="name">\</span></span>c&amp;d <span class="tag">\<span class="name">end</span><span class="string">&#123;bmatrix&#125;</span></span> <span class="tag">\<span class="name">quad</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;Bmatrix&#125;</span></span> a&amp;b<span class="tag">\<span class="name">\</span></span>c&amp;d <span class="tag">\<span class="name">end</span><span class="string">&#123;Bmatrix&#125;</span></span> <span class="tag">\<span class="name">quad</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;vmatrix&#125;</span></span> a&amp;b<span class="tag">\<span class="name">\</span></span>c&amp;d <span class="tag">\<span class="name">end</span><span class="string">&#123;vmatrix&#125;</span></span> <span class="tag">\<span class="name">quad</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;Vmatrix&#125;</span></span> a&amp;b<span class="tag">\<span class="name">\</span></span>c&amp;d <span class="tag">\<span class="name">end</span><span class="string">&#123;Vmatrix&#125;</span></span> <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609131725854.png" alt></p>
<p>使用 <code>smallmatrix</code> 环境，可以生成行内公式的小矩阵。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">Marry has a little matrix <span class="formula">$ ( <span class="tag">\<span class="name">begin</span><span class="string">&#123;smallmatrix&#125;</span></span> a&amp;b<span class="tag">\<span class="name">\</span></span>c&amp;d <span class="tag">\<span class="name">end</span><span class="string">&#123;smallmatrix&#125;</span></span> ) $</span>.</span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609131818971.png" alt></p>
<h3 id="duo-xing-gong-shi">多行公式</h3>
<p>有的公式特别长，我们需要手动为他们换行；有几个公式是一组，我们需要将他们放在一起；还有些类似分段函数，我们需要给它加上一个左边的花括号。</p>
<h4 id="chang-gong-shi">长公式</h4>
<h5 id="bu-dui-qi">不对齐</h5>
<p>无须对齐的长公式可以使用 <code>multline</code> 环境(如果不需要编号，可以使用 <code>multline*</code> 环境代替。)。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line">    <span class="tag">\<span class="name">begin</span><span class="string">&#123;multline&#125;</span></span></span><br><span class="line">        x = a+b+c+&#123;&#125; <span class="tag">\<span class="name">\</span></span></span><br><span class="line">        d+e+f+g</span><br><span class="line">    <span class="tag">\<span class="name">end</span><span class="string">&#123;multline&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609132032239.png" alt></p>
<h5 id="dui-qi">对齐</h5>
<p>需要对齐的公式，可以使用 <code>aligned</code> <em>次环境</em>来实现，它必须包含在数学环境之内。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line">    <span class="tag">\<span class="name">[</span></span><span class="tag">\<span class="name">begin</span><span class="string">&#123;aligned&#125;</span></span></span><br><span class="line">        x =&#123;&#125;&amp; a+b+c+&#123;&#125; <span class="tag">\<span class="name">\</span></span></span><br><span class="line">        &amp;d+e+f+g</span><br><span class="line">    <span class="tag">\<span class="name">end</span><span class="string">&#123;aligned&#125;</span></span><span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609132211903.png" alt></p>
<h4 id="gong-shi-zu">公式组</h4>
<p>无需对齐的公式组可以使用 <code>gather</code> 环境，需要对齐的公式组可以使用 <code>align</code> 环境。他们都带有编号，如果不需要编号可以使用带星花的版本。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;gather&#125;</span></span></span><br><span class="line">    a = b+c+d <span class="tag">\<span class="name">\</span></span></span><br><span class="line">    x = y+z</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;gather&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;align&#125;</span></span></span><br><span class="line">    a &amp;= b+c+d <span class="tag">\<span class="name">\</span></span></span><br><span class="line">    x &amp;= y+z</span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;align&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609132406290.png" alt></p>
<h3 id="fen-duan-han-shu">分段函数</h3>
<p>分段函数可以用<code>cases</code>次环境来实现，它必须包含在数学环境之内。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">[</span></span> y= <span class="tag">\<span class="name">begin</span><span class="string">&#123;cases&#125;</span></span></span><br><span class="line">    -x,<span class="tag">\<span class="name">quad</span></span> x<span class="tag">\<span class="name">leq</span></span> 0 <span class="tag">\<span class="name">\</span></span></span><br><span class="line">    x,<span class="tag">\<span class="name">quad</span></span> x&gt;0</span><br><span class="line">    <span class="tag">\<span class="name">end</span><span class="string">&#123;cases&#125;</span></span> <span class="tag">\<span class="name">]</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609132647535.png" alt></p>
<h3 id="shu-xue-gong-shi-gong-ju">数学公式工具</h3>
<ul>
<li><a href="https://mathpix.com/" target="_blank" rel="noopener">https://mathpix.com/</a> 通过热键呼出截屏，而后将截屏中的公式转换成 LaTeX 数学公式的代码。</li>
<li><a href="http://detexify.kirelabs.org/classify.html" target="_blank" rel="noopener">http://detexify.kirelabs.org/classify.html</a> 允许用户用鼠标在输入区绘制单个数学符号的样式，系统会根据样式返回对应的 LaTeX 代码（和所需的宏包）。这在查询不熟悉的数学符号时特别有用。</li>
</ul>
<h2 id="cha-ru-tu-pian">插入图片</h2>
<p>在 LaTeX 中插入图片，有很多种方式。最好用的应当属利用 <code>graphicx</code> 宏包提供的 <code>\includegraphics</code> 命令。比如你在你的 TeX 源文件同目录下，有名为 <code>a.jpg</code> 的图片，你可以用这样的方式将它插入到输出文档中：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;graphicx&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">includegraphics</span><span class="string">&#123;a.jpg&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609133258012.png" alt></p>
<p>图片很大，超过了输出文件的纸张大小，这时候可以用 <code>\includegraphics</code> 控制序列的可选参数来控制。比如</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;graphicx&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="comment">% 这样图片的宽度会被缩放至页面宽度的百分之六十，图片的总高度会按比例缩放。</span></span><br><span class="line"><span class="tag">\<span class="name">includegraphics</span><span class="string">[width = .6\textwidth]</span><span class="string">&#123;a.jpg&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609133717627.png" alt></p>
<h2 id="biao-ge">表格</h2>
<p><code>tabular</code> 环境提供了最简单的表格功能。它用 <code>\hline</code> 命令表示横线，在列格式中用 <code>|</code> 表示竖线；用 <code>&amp;</code> 来分列，用 <code>\\</code> 来换行；每列可以采用居左、居中、居右等横向对齐方式，分别用 <code>l</code>、<code>c</code>、<code>r</code> 来表示。（<a href="https://www.tablesgenerator.com/" target="_blank" rel="noopener">表格在线生成</a>）</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;graphicx&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;tabular&#125;</span><span class="string">&#123;|l|c|r|&#125;</span></span></span><br><span class="line">    <span class="tag">\<span class="name">hline</span></span></span><br><span class="line">   操作系统&amp; 发行版&amp; 编辑器<span class="tag">\<span class="name">\</span></span></span><br><span class="line">    <span class="tag">\<span class="name">hline</span></span></span><br><span class="line">   Windows &amp; MikTeX &amp; TexMakerX <span class="tag">\<span class="name">\</span></span></span><br><span class="line">    <span class="tag">\<span class="name">hline</span></span></span><br><span class="line">   Unix/Linux &amp; teTeX &amp; Kile <span class="tag">\<span class="name">\</span></span></span><br><span class="line">    <span class="tag">\<span class="name">hline</span></span></span><br><span class="line">   Mac OS &amp; MacTeX &amp; TeXShop <span class="tag">\<span class="name">\</span></span></span><br><span class="line">    <span class="tag">\<span class="name">hline</span></span></span><br><span class="line">   通用&amp; TeX Live &amp; TeXworks <span class="tag">\<span class="name">\</span></span></span><br><span class="line">    <span class="tag">\<span class="name">hline</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;tabular&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609134020824.png" alt></p>
<h2 id="fu-dong-ti">浮动体</h2>
<p>插图和表格通常需要占据大块空间，所以在文字处理软件中经常需要调整他们的位置。<code>figure</code> 和 <code>table</code> 环境可以自动完成这样的任务；这种自动调整位置的环境称作浮动体(float)。我们以 <code>figure</code> 为例。</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[UTF8]</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="comment">% 希望 ctex 只提供中文支持的功能不对版式做任何修改，可以这样使用：</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">[UTF8, heading = false, scheme = plain]</span><span class="string">&#123;ctex&#125;</span></span></span><br><span class="line"><span class="comment">% 插入对数学公式的支持</span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;graphicx&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;figure&#125;</span><span class="string">[htbp]</span></span></span><br><span class="line">    <span class="tag">\<span class="name">centering</span></span></span><br><span class="line">    <span class="tag">\<span class="name">includegraphics</span><span class="string">[width = .4\textwidth]</span><span class="string">&#123;a.jpg&#125;</span></span></span><br><span class="line">    <span class="tag">\<span class="name">caption</span><span class="string">&#123;夏&#125;</span></span></span><br><span class="line">    <span class="tag">\<span class="name">label</span><span class="string">&#123;fig:myphoto&#125;</span></span></span><br><span class="line">    <span class="tag">\<span class="name">end</span><span class="string">&#123;figure&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/06/basic_skills/latex/image-20200609134518050.png" alt></p>
<blockquote>
<p><code>htbp</code> 选项用来指定插图的理想位置，这几个字母分别代表 here, top, bottom, float page，也就是就这里、页顶、页尾、浮动页（专门放浮动体的单独页面或分栏）。</p>
<p><code>\centering</code> 用来使插图居中；</p>
<p><code>\caption</code> 命令设置插图标题，LaTeX 会自动给浮动体的标题加上编号。</p>
<p><code>\label</code> 应该放在标题命令之后。</p>
</blockquote>
<h2 id="ban-mian-she-zhi">版面设置</h2>
<h3 id="ye-bian-ju">页边距</h3>
<p>设置页边距，推荐使用 <code>geometry</code> 宏包。比如希望，将纸张的长度设置为 20cm、宽度设置为 15cm、左边距 1cm、右边距 2cm、上边距 3cm、下边距 4cm，可以在导言区加上这样几行：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;geometry&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">geometry</span><span class="string">&#123;papersize=&#123;20cm,15cm&#125;</span></span>&#125;</span><br><span class="line"><span class="tag">\<span class="name">geometry</span><span class="string">&#123;left=1cm,right=2cm,top=3cm,bottom=4cm&#125;</span></span></span><br></pre></td></tr></table></figure>
<h3 id="ye-mei-ye-jiao">页眉页脚</h3>
<p>设置页眉页脚，推荐使用 <code>fancyhdr</code> 宏包。比如希望，在页眉左边写上我的名字，中间写上今天的日期，右边写上我的电话；页脚的正中写上页码；页眉和正文之间有一道宽为 0.4pt 的横线分割，可以在导言区加上如下几行：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;fancyhdr&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">pagestyle</span><span class="string">&#123;fancy&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">lhead</span><span class="string">&#123;\author&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">chead</span><span class="string">&#123;\date&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">rhead</span><span class="string">&#123;166xxxxxxxx&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">lfoot</span><span class="string">&#123;&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">cfoot</span><span class="string">&#123;\thepage&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">rfoot</span><span class="string">&#123;&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">renewcommand</span><span class="string">&#123;\headrulewidth&#125;</span><span class="string">&#123;0.4pt&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">renewcommand</span><span class="string">&#123;\headwidth&#125;</span><span class="string">&#123;\textwidth&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">renewcommand</span><span class="string">&#123;\footrulewidth&#125;</span><span class="string">&#123;0pt&#125;</span></span></span><br></pre></td></tr></table></figure>
<h3 id="shou-xing-suo-jin">首行缩进</h3>
<p>CTeX 宏集已经处理好了首行缩进的问题（自然段前空两格汉字宽度）。因此，使用 CTeX 宏集进行中西文混合排版时，不需要关注首行缩进的问题。</p>
<h3 id="xing-jian-ju">行间距</h3>
<p>可以通过 <code>setspace</code> 宏包提供的命令来调整行间距。比如在导言区添加如下内容，可以将行距设置为字号的 1.5 倍：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;setspace&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">onehalfspacing</span></span></span><br></pre></td></tr></table></figure>
<h3 id="duan-jian-ju">段间距</h3>
<p>可以通过修改长度 <code>\parskip</code> 的值来调整段间距。例如在导言区添加以下内容</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"><span class="tag">\<span class="name">addtolength</span><span class="string">&#123;\parskip&#125;</span><span class="string">&#123;.4em&#125;</span></span></span><br></pre></td></tr></table></figure>
<p>可以在原有的基础上，增加段间距 0.4em。如果需要减小段间距，只需将该数值改为负值即可。</p>
<h1 id="zhu-yao-fu-hao-yi-lan">主要符号一览</h1>
<h2 id="shu">数</h2>
<ul>
<li>\(x\)：标量</li>
<li>\(\boldsymbol{x}\)：向量</li>
<li>\(\boldsymbol{X}\)：矩阵</li>
<li>\(\mathsf{X}\)：张量</li>
</ul>
<h2 id="ji-he">集合</h2>
<ul>
<li>\(\mathcal{X}\)：集合</li>
<li>\(\mathbb{R}\)：实数集合</li>
<li>\(\mathbb{R}^n\)：\(n\)维的实数向量集合</li>
<li>\(\mathbb{R}^{x\times y}\)：\(x\)行\(y\)列的实数矩阵集合</li>
</ul>
<h2 id="cao-zuo-fu">操作符</h2>
<ul>
<li>\(\boldsymbol{(\cdot)}^\top\)：向量或矩阵的转置</li>
<li>\(\odot\)：按元素相乘，即阿达马（Hadamard）积</li>
<li>\(\lvert\mathcal{X}\rvert\)：集合\(\mathcal{X}\)中元素个数</li>
<li>\(\|\cdot\|_p\)：\(L_p\)范数</li>
<li>\(\|\cdot\|\)：\(L_2\)范数</li>
<li>\(\sum\)：连加</li>
<li>\(\prod\)：连乘</li>
</ul>
<h2 id="han-shu">函数</h2>
<ul>
<li>\(f(\cdot)\)：函数</li>
<li>\(\log(\cdot)\)：自然对数函数</li>
<li>\(\exp(\cdot)\)：指数函数</li>
</ul>
<h2 id="dao-shu-he-ti-du">导数和梯度</h2>
<ul>
<li>\(\frac{dy}{dx}\)：\(y\)关于\(x\)的导数</li>
<li>\(\frac{\partial y}{\partial x}\)：\(y\)关于\(x\)的偏导数</li>
<li>\(\nabla_{\cdot} y\)：\(y\)关于\(\cdot\)的梯度</li>
</ul>
<h2 id="gai-lu-he-tong-ji">概率和统计</h2>
<ul>
<li>\(P(\cdot)\)：概率分布</li>
<li>\(\cdot\sim P\)：随机变量\(\cdot\)的概率分布是\(P\)</li>
<li>\(P(\cdot \mid\cdot)\)：条件概率分布</li>
<li>\(E_{\cdot}\left(f(\cdot)\right)\)：函数\(f(\cdot)\)对\(\cdot\)的数学期望</li>
</ul>
<h2 id="fu-za-du">复杂度</h2>
<ul>
<li>\(\mathcal{O}\)：大O符号（渐进符号）</li>
</ul>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://liam.page/2014/09/08/latex-introduction/" target="_blank" rel="noopener">一份其实很短的 LaTeX 入门文档</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/基本技能</category>
      </categories>
      <tags>
        <tag>latex</tag>
      </tags>
  </entry>
  <entry>
    <title>SVM</title>
    <url>/2020/06/03/machine_learning/SVM/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/06/03/machine_learning/SVM/00630Defly1g4w7oendezj30gz0aojrq.jpg" alt></p>
<a id="more"></a>
<h1 id="jiang-jiang-svm">讲讲SVM</h1>
<h2 id="yi-ge-guan-yu-svm-de-tong-hua-gu-shi">一个关于SVM的童话故事</h2>
<p>支持向量机（Support Vector Machine，SVM）是众多监督学习方法中十分出色的一种，几乎所有讲述经典机器学习方法的教材都会介绍。关于SVM，流传着一个关于天使与魔鬼的故事。</p>
<p>传说魔鬼和天使玩了一个游戏，魔鬼在桌上放了两种颜色的球。魔鬼让天使用一根木棍将它们分开。这对天使来说，似乎太容易了。天使不假思索地一摆，便完成了任务。魔鬼又加入了更多的球。随着球的增多，似乎有的球不能再被原来的木棍正确分开，如下图所示。</p>
<p><img src="/2020/06/03/machine_learning/SVM/00630Defgy1g4vn6ow8vtj30i40ddgpt.jpg" alt></p>
<p>SVM实际上是在为天使找到木棒的最佳放置位置，使得两边的球都离分隔它们的木棒足够远。依照SVM为天使选择的木棒位置，魔鬼即使按刚才的方式继续加入新球，木棒也能很好地将两类不同的球分开。</p>
<p>看到天使已经很好地解决了用木棒线性分球的问题，魔鬼又给了天使一个新的挑战，如下图所示。</p>
<p><img src="/2020/06/03/machine_learning/SVM/00630Defgy1g4vn9xl3pcj30iy0dswgr.jpg" alt></p>
<p>按照这种球的摆法，世界上貌似没有一根木棒可以将它们 完美分开。但天使毕竟有法力，他一拍桌子，便让这些球飞到了空中，然后凭借念力抓起一张纸片，插在了两类球的中间。从魔鬼的角度看这些 球，则像是被一条曲线完美的切开了。</p>
<p><img src="/2020/06/03/machine_learning/SVM/00630Defgy1g4vnbaltf7j30mo0ar77n.jpg" alt></p>
<p>后来，“无聊”的科学家们把这些球称为“数据”，把木棍称为“分类面”，找到最大间隔的木棒位置的过程称为“优化”，拍桌子让球飞到空中的念力叫“核映射”，在空中分隔球的纸片称为“分类超平面”。这便是SVM的童话故事。</p>
<h2 id="li-jie-svm-di-yi-ceng">理解SVM：第一层</h2>
<p>支持向量机，因其英文名为support vector machine，故一般简称SVM，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是<strong>间隔最大化</strong>，最终可转化为一个凸二次规划问题的求解。</p>
<p>**线性分类器：**给定一些数据点，它们分别属于两个不同的类，现在要找到一个线性分类器把这些数据分成两类。如果用x表示数据点，用y表示类别（y可以取1或者0，分别代表两个不同的类），一个线性分类器的学习目标便是要在n维的数据空间中找到一个超平面，这个超平面的方程可以表示为：<br>
\[
w^Tx+b=0
\]<br>
这个超平面可以用分类函数  \(f(x)=w^Tx+b\)表示，当\(f(x)\) 等于0的时候，\(x\)便是位于超平面上的点，而\(f(x)\)大于0的点对应 \(y=1\) 的数据点，\(f(x)\)小于0的点对应\(y=-1\)的点，如下图所示：</p>
<p><img src="/2020/06/03/machine_learning/SVM/00630Defly1g4w7oendezj30gz0aojrq.jpg" alt></p>
<h3 id="han-shu-jian-ge-yu-ji-he-jian-ge">函数间隔与几何间隔</h3>
<p>在分离超平面固定为\(𝑤^T𝑥+𝑏=0\)的时候，\(|wTx+b|\)表示点x到超平面的相对距离。通过观察\(w^Tx+b\)和\(y\)是否同号，我们判断分类是否正确。这里引入函数间隔的概念，定义函数间隔\(\gamma^{\prime}\) 为：<br>
\[
\gamma^{\prime}=y(w^Tx+b)
\]<br>
可以看到，就是感知机模型里面的误分类点到超平面距离的分子。对于训练集中\(m\)个样本点对应的\(m\)个函数间隔的最小值，就是整个训练集的函数间隔。</p>
<p>函数间隔并不能正常反应点到超平面的距离，当分子成比例的增长时，分母也是成倍增长。为了统一度量，需要对法向量\(w\)加上约束条件，这样就得到了几何间隔\(\gamma\),定义为：<br>
\[
\gamma=\frac{y\left(w^{T} x+b\right)}{\|w\|_{2}}=\frac{\gamma^{\prime}}{\|w\|_{2}}
\]<br>
几何间隔才是点到超平面的真正距离，感知机模型里用到的距离就是几何距离。</p>
<h3 id="zhi-chi-xiang-liang">支持向量</h3>
<p>在感知机模型中，可以找到多个可以分类的超平面将数据分开，并且优化时希望所有的点都被准确分类。但是实际上离超平面很远的点已经被正确分类，它对超平面的位置没有影响。我们最关心是那些离超平面很近的点，这些点很容易被误分类。如果可以让离超平面比较近的点尽可能的远离超平面，最大化几何间隔，那么分类效果会更好一些。SVM的思想起源正起于此。</p>
<p>如下图所示，分离超平面为\(w^Tx+b=0\)，如果所有的样本不光可以被超平面分开，还和超平面保持一定的函数距离（下图函数距离为1），那么这样的分类超平面是比感知机的分类超平面优的。可以证明，这样的超平面只有一个。和超平面平行的保持一定的函数距离的这两个超平面对应的向量，我们定义为支持向量，如下图虚线所示。</p>
<p><img src="/2020/06/03/machine_learning/SVM/1042406-20161124144326487-1331861308.jpg" alt></p>
<p>支持向量到超平面的距离为\(\frac{1}{||w||_2}\),两个支持向量之间的距离为\(\frac{2}{||w||_2}\)。</p>
<h3 id="mu-biao-han-shu-yu-you-hua">目标函数与优化</h3>
<p>SVM的模型是让所有点到超平面的距离大于一定的距离，也就是所有的分类点要在各自类别的支持向量两边。用数学式子表示为：<br>
\[
\begin{array}{ll}
\max &amp; \gamma=\frac{y\left(w^{T} x+b\right)}{\|w\|_{2}} \\ \text { s.t } &amp; y_{i}\left(w^{T} x_{i}+b\right)=\gamma^{\prime}_{i} \geq \gamma^{\prime}(i=1,2, \ldots m)
\end{array}
\]<br>
一般我们都取函数间隔\(\gamma\)为1，这样我们的优化函数定义为:<br>
\[
\begin{array}{ll}
\max &amp; \frac{1}{\|w\|_{2}}  \\ \text { s.t } &amp; y_{i}\left(w^{T} x_{i}+b\right) \geq 1(i=1,2, \ldots m)
\end{array}
\]<br>
也就是说，我们要在约束条件\(y_{i}\left(w^{T} x_{i}+b\right) \geq 1(i=1,2, \ldots m)\)下，最大化\(\frac{1}{||w||_2}\)。可以看出，这个感知机的优化方式不同，感知机是固定分母优化分子，而SVM是固定分子优化分母，同时加上了支持向量的限制。</p>
<p>由于\(\frac{1}{||w||_2}\)最大化等价于\(\frac{||w||_2^2}{2}\)最小化。这样SVM的优化函数等价于：<br>
\[
\begin{array}{ll}
\min &amp;\frac{1}{2}\|w\|_{2}^{2} \\ \text { s.t } &amp;y_{i}\left(w^{T} x_{i}+b\right) \geq 1(i=1,2, \ldots m)
\end{array}
\]<br>
由于目标函数\(\frac{||w||_2^2}{2}\)是凸函数，同时约束条件不等式是仿射的，根据凸优化理论，可以通过拉格朗日函数将优化目标转化为无约束的优化函数<br>
\[
\begin{array}{ll}
L(w, b, \alpha)&amp;=\frac{1}{2}\|w\|_{2}^{2}-\sum_{i=1}^{m} \alpha_{i}\left[y_{i}\left(w^{T} x_{i}+b\right)-1\right] \\ \text { s.t } &amp; \alpha_{i} \geq 0
\end{array}
\]<br>
由于引入了朗格朗日乘子，我们的优化目标变成：<br>
\[
\underbrace{\min }_{w, b} \underbrace{\max }_{\alpha_i \ge0} L(w, b, \alpha)
\]<br>
可以通过拉格朗日对偶将我们的优化问题转化为等价的对偶问题来求解。也就是说，现在要求的是<br>
\[
\underbrace{\max }_{\alpha_i \ge0} \underbrace{\min }_{w, b}  L(w, b, \alpha)
\]<br>
从上式中，可以先求优化函数对于\(𝑤\)和\(𝑏\)的极小值(这个极值可以通过对\(w\)和\(b\)分别求偏导数得到)。接着再求拉格朗日乘子\(\alpha\)的极大值。<br>
\[
\begin{aligned}
\frac{\partial L}{\partial w}=0 &amp; \Rightarrow w=\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i} \\
\frac{\partial L}{\partial b}=0 &amp; \Rightarrow \sum_{i=1}^{m} \alpha_{i} y_{i}=0
\end{aligned}
\]<br>
从上两式子可以看出，已经求得了\(w\)和\(\alpha\)的关系，只要后面接着能够求出优化函数极大化对应的\(\alpha\)，就可以求出\(w\)了，至于，由于上两式已经没有\(b\)，所以最后的\(b\)可以有多个。既然已经求出\(w\)和\(\alpha\)的关系，就可以带入优化函数\(L(w,b,\alpha)\)消去\(w\)。定义:</p>
<p>\[
\psi(\alpha)=\underbrace{\min }_{w,b} L(w, b, \alpha)
\]<br>
从上面可以看出，通过对\(w,b\)极小化以后，优化函数\(\psi(\alpha)\)仅仅只有\(\alpha\)向量做参数。只要能够(利用SMO算法)极大化\(\psi(\alpha)\)，就可以求出此时对应的\(\alpha\)，进而求出\(w,b\).</p>
<h3 id="zui-da-jian-ge-sun-shi-han-shu-hinge-loss">最大间隔损失函数Hinge loss</h3>
<p>SVM 求解使通过建立二次规划原始问题，引入拉格朗日乘子法，然后转换成对偶的形式去求解，这是一种理论非常充实的解法。这里换一种角度来思考，在机器学习领域，一般的做法是经验风险最小化 ，即构建假设函数为输入输出间的映射，然后采用损失函数来衡量模型的优劣。求得使损失最小化的模型即为最优的假设函数，采用不同的损失函数也会得到不同的机器学习算法。SVM采用的就是Hinge Loss，用于“最大间隔”分类。</p>
<p>Hinge Loss中文名叫合页损失函数，因为它的图像是这样的：</p>
<p><img src="/2020/06/03/machine_learning/SVM/v2-3c6aa9626ee8e4609b0d7c5712baf624_1440w.jpg" alt></p>
<p>hinge-loss的公式是：<br>
\[
\begin{array}{c}
\sum_{i=1}^{N}\left[1-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}+\lambda\|w\|^{2} \\
{[z]_{+}=\left\{\begin{array}{l}
z, z>0 \\
0 . z \leq 0
\end{array}\right.}
\end{array}
\]<br>
第一项是损失，第二项是正则化项。这个公式就是说 \(y_i(w * x_i+b)\) 大于1时loss为0， 否则loss为 \(1-y_i(w*x_i+b)\) 。对比感知机的损失函数\([-y_i(w*x_i+b)]\)  来说，hinge loss不仅要分类正确，而且置信度足够高的时候，损失才为0，对学习有更高的要求。对比感知机损失和hinge loss的图像，明显Hinge loss更加严格。</p>
<p><img src="/2020/06/03/machine_learning/SVM/v2-0884e199d2198ec716f3b09536b10213_r.jpg" alt="preview"></p>
<p>如下图中，点 \(x_4\)被分类正确了，但是它的损失不是0。其实这个道理和SVM中的Margin是一样的，不仅要分类正确，还要使得Margin最大化，所以说hinge loss的另外一种解释。</p>
<p><img src="/2020/06/03/machine_learning/SVM/v2-a04fdb43f41bd9e4fd06306558ab61f2_1440w.jpg" alt="img"></p>
<h2 id="shen-ru-svm-di-er-ceng">深入SVM：第二层</h2>
<h3 id="cong-xian-xing-ke-fen-dao-xian-xing-bu-ke-fen">从线性可分到线性不可分</h3>
<p>有时候本来数据的确是可分的，也就是说可以用 线性分类SVM的学习方法来求解，但是却因为混入了异常点，导致不能线性可分，比如下图，本来数据是可以按下面的实线来做超平面分离的，可由于一个橙色和一个蓝色的异常点导致我们没法用上述方法来分类。</p>
<p><img src="/2020/06/03/machine_learning/SVM/1042406-20161125104106409-1177897648.png" alt="img"></p>
<p>另外一种情况没有这么糟糕到不可分，但是会严重影响我们模型的泛化预测效果，比如下图，本来如果我们不考虑异常点，SVM的超平面应该是下图中的红色线所示，但是由于有一个蓝色的异常点，导致我们学习到的超平面是下图中的粗虚线所示，这样会严重影响我们的分类模型预测效果。</p>
<p><img src="/2020/06/03/machine_learning/SVM/1042406-20161125104737206-364720074.png" alt="img"></p>
<p>如何解决这些问题呢？SVM引入了软间隔最大化的方法来解决。回顾硬间隔最大化的条件：</p>
<p>\[
\begin{array}{ll}
\min &amp; \frac{1}{2}\|w\|_{2}^{2} \\ \text { s.t } &amp; y_{i}\left(w^{T} x_{i}+b\right) \geq 1(i=1,2, \ldots m)
\end{array}
\]<br>
接着如何软间隔最大化呢？</p>
<p>SVM对训练集里面的每个样本\((x_i,y_i)\)引入了一个松弛变量\(\xi_i \ge 0\)使函数间隔加上松弛变量大于等于1，也就是说：<br>
\[
y_{i}\left(w \cdot x_{i}+b\right) \geq 1-\xi_{i}
\]<br>
对比硬间隔最大化，可以看到对样本到超平面的函数距离的要求放松了，之前是一定要大于等于1，现在只需要加上一个大于等于0的松弛变量能大于等于1就可以了。当然，对于加入的松弛变量是有成本的，每一个松弛变量\(\xi_i\), 对应了一个代价\(\xi_i\)，这个就得到了的软间隔最大化的SVM学习条件如下：</p>
<p>\[
\begin{aligned}
&amp;\min \frac{1}{2}\|w\|_{2}^{2}+C \sum_{i=1}^{m} \xi_{i}\\
\text {s.t. } &amp; y_{i}\left(w^{T} x_{i}+b\right) \geq 1-\xi_{i}(i=1,2, \ldots m)\\
&amp;\xi_{i} \geq 0 \quad(i=1,2, \ldots m)
\end{aligned}
\]<br>
这里,\(C > 0\)为惩罚参数，可以理解为我们一般回归和分类问题正则化时候的参数。\(C\)越大，对误分类的惩罚越大，\(C\)越小，对误分类的惩罚越小。也就是说，我们希望\(\frac{||w||_2^2}{2}\)尽量小，误分类的点尽可能的少。\(C\)是协调两者关系的正则化惩罚系数。在实际应用中，需要调参来选择。</p>
<p><strong>如果数据中出现了离群点outliers，那么就可以使用松弛变量来解决。</strong></p>
<h3 id="he-han-shu-kernel">核函数Kernel</h3>
<p>事实上，大部分时候数据并不是线性可分的，这个时候满足这样条件的超平面就根本不存在。对于非线性的情况，SVM 的处理方法是选择一个核函数 κ(⋅,⋅) ，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题。</p>
<p>具体来说，**在线性不可分的情况下，支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开。**如图所示，一堆数据在二维空间无法划分，从而映射到三维空间里划分：</p>
<p><img src="/2020/06/03/machine_learning/SVM/00630Defly1g4w7t050xfj30nl0bajrv.jpg" alt></p>
<p><img src="/2020/06/03/machine_learning/SVM/quesbase6415311358438441728.gif" alt></p>
<p>通常人们会从一些常用的核函数中选择（根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数），例如：<strong>多项式核、高斯核、线性核。</strong></p>
<p>核函数简要概括，即以下三点：</p>
<ol>
<li>实际中，我们会经常遇到线性不可分的样例，此时，我们的常用做法是把样例特征映射到高维空间中去(映射到高维空间后，<strong>相关特征便被分开了</strong>，也就达到了分类的目的)；</li>
<li>但进一步，如果凡是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小是会高到可怕的。那咋办呢？</li>
<li>此时，核函数就隆重登场了，核函数的价值在于它虽然也是将特征进行从低维到高维的转换，但核函数能够<strong>事先在低维上进行计算</strong>，而将实质上的分类效果表现在了高维上，避免了直接在高维空间中的复杂计算。</li>
</ol>
<h3 id="zong-jie">总结</h3>
<p>SVM它本质上即是一个分类方法，用 \(w^T+b\) 定义分类函数，于是求\(w,b\)，为寻最大间隔，引出\(\frac{||w||^2_2}{2}\),继而引入拉格朗日因子，化为对拉格朗日乘子a的求解（求解过程中会涉及到一系列最优化或凸二次规划等问题），如此，求\(w,b\)与求\(a\)等价，而\(a\)的求解可以用一种快速学习算法SMO，至于核函数，是为处理非线性情况，若直接映射到高维计算恐维度爆炸，故在低维计算，等效高维表现。</p>
<h2 id="svm-de-ying-yong">SVM的应用</h2>
<p>SVM在很多诸如<strong>文本分类，图像分类，生物序列分析和生物数据挖掘，手写字符识别等领域有很多的应用</strong>。</p>
<h1 id="zhi-chi-xiang-liang-ji-yuan-shi-zui-you-hua-wen-ti-he-he-ye-sun-shi">支持向量机原始最优化问题和合页损失</h1>
<p>线性支持向量机原始最优化问题<br>
\[
\begin{array}{ll}
\min _{w, b, \xi} &amp; \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}   \\
\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N \\
&amp; \xi_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
\]<br>
等价于(合页损失)最优化问题：<br>
\[
\min _{w, b} \sum_{i=1}^{N}\left[1-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}+\lambda\|w\|^{2}
\]<br>
证明，令合页损失中：<br>
\[
1-y_{i}\left(w \cdot x_{i}+b\right)=\xi_{i}, \quad \xi_{i} \geqslant 0
\]<br>
则：\(y_i(w*x_i+b) \ge 1\).于是\(w,b,\xi\)满足支持向量机原始优化问题的约束条件。由\([1-y_i(w*x_i+b)]_+=[\xi_i]_+=\xi_i\)，所以合页损失可以写成:<br>
\[
\min _{w, b} \sum_{i=1}^{N} \xi_{i}+\lambda\|w\|^{2}
\]<br>
取：\(\lambda=\frac{1}{2C}\)，则：<br>
\[
\min _{w, b} \frac{1}{C}\left(\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}\right)
\]<br>
与原始问题等价。</p>
<h2 id="svm-de-yi-xie-wen-ti">SVM的一些问题</h2>
<ol>
<li>
<p>是否存在一组参数使SVM训练误差为0？</p>
<p>答：存在</p>
</li>
<li>
<p>训练误差为0的SVM分类器一定存在吗？</p>
<p>答：一定存在</p>
</li>
<li>
<p>加入松弛变量的SVM的训练误差可以为0吗？</p>
<p>答：使用SMO算法训练的线性分类器并不一定能得到训练误差为0的模型。这是由于优化目标改变了，并不再是使训练误差最小。</p>
</li>
<li>
<p><strong>带核的SVM为什么能分类非线性问题?</strong></p>
<p>答：核函数的本质是两个函数的內积，通过核函数将其隐射到高维空间，在高维空间非线性问题转化为线性问题, SVM得到超平面是高维空间的线性分类平面。其分类结果也视为低维空间的非线性分类结果, 因而带核的SVM就能分类非线性问题。</p>
</li>
<li>
<p><strong>如何选择核函数？</strong></p>
<ul>
<li>如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM；</li>
<li>如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数；</li>
<li>如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况。</li>
</ul>
</li>
</ol>
<h1 id="lr-he-svm-de-lian-xi-yu-qu-bie">LR和SVM的联系与区别</h1>
<h2 id="xiang-tong-dian">相同点</h2>
<ul>
<li>都是线性分类器。本质上都是求一个最佳分类超平面。</li>
<li>都是监督学习算法。</li>
<li>都是判别模型。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。</li>
</ul>
<h2 id="bu-tong-dian">不同点</h2>
<ul>
<li>LR是参数模型，svm是<strong>非参数模型</strong>，linear和rbf则是针对数据线性可分和不可分的区别；</li>
<li>从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。</li>
<li>SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。</li>
<li>逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。</li>
<li>logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。</li>
</ul>
<h1 id="xian-xing-fen-lei-qi-yu-fei-xian-xing-fen-lei-qi-de-qu-bie-yi-ji-you-lie">线性分类器与非线性分类器的区别以及优劣</h1>
<p>线性和非线性是针对模型参数和输入特征来讲的。比如输入\(x\)，模型\(y=ax+ax^2\) 那么就是非线性模型，如果输入是\(x\)和\(x^2\)则模型是线性的。</p>
<ul>
<li>
<p>线性分类器可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。</p>
<p>LR,贝叶斯分类，单层感知机、线性回归</p>
</li>
<li>
<p>非线性分类器效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好。</p>
<p>决策树、RF、GBDT、多层感知机</p>
</li>
</ul>
<p><strong>SVM两种都有（看线性核还是高斯核）</strong></p>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>LightGBM</title>
    <url>/2020/06/03/machine_learning/LightGBM/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/06/03/machine_learning/LightGBM/img_lightgbm.jpg" alt></p>
<a id="more"></a>
<h1 id="light-gbm-shi-shi-yao">LightGBM是什么</h1>
<p>不久前微软DMTK(分布式机器学习工具包)团队在GitHub上开源了性能超越其他boosting工具的LightGBM，在三天之内GitHub上被star了1000次，fork了200次。知乎上有近千人关注“如何看待微软开源的LightGBM？”问题，被评价为“速度惊人”，“非常有启发”，“支持分布式”，“代码清晰易懂”，“占用内存小”等。</p>
<p><a href="%5Bhttps://github.com/Microsoft/LightGBM%5D(https://github.com/Microsoft/LightGBM)">LightGBM （Light Gradient Boosting Machine）</a>是一个实现GBDT算法的框架，支持高效率的并行训练。</p>
<p>LightGBM在Higgs数据集上LightGBM比XGBoost快将近10倍，内存占用率大约为XGBoost的1/6，并且准确率也有提升。GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的GBDT算法是不能满足其需求的。</p>
<p>LightGBM提出的主要原因就<strong>是为了解决GBDT在海量数据遇到的问题</strong>，让GBDT可以更好更快地用于工业实践。</p>
<h2 id="light-gbm-zai-na-xie-di-fang-jin-xing-liao-you-hua-qu-bie-xg-boost">LightGBM在哪些地方进行了优化    (区别XGBoost)？</h2>
<ul>
<li>基于Histogram的决策树算法</li>
<li>带深度限制的Leaf-wise的叶子生长策略</li>
<li>直方图做差加速直接</li>
<li>支持类别特征(Categorical Feature)</li>
<li>Cache命中率优化</li>
<li>基于直方图的稀疏特征优化多线程优化。</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>预排序算法</th>
<th>LightGBM</th>
</tr>
</thead>
<tbody>
<tr>
<td>内存占用</td>
<td><code>2 * #feature * #data #4Bytes</code></td>
<td><code>#feature * #data #1Bytes</code></td>
</tr>
<tr>
<td>统计量累积</td>
<td><code>O(#feature * #data)</code></td>
<td><code>O(#feature * #data)</code></td>
</tr>
<tr>
<td>分割增益计算</td>
<td><code>O(#feature * #data)</code></td>
<td><code>O(#feature * #k)</code></td>
</tr>
<tr>
<td>直方图作差</td>
<td>N/A</td>
<td>加速1倍</td>
</tr>
<tr>
<td>直接支持类别特征</td>
<td>N/A</td>
<td>在Expo数据上加速8倍</td>
</tr>
<tr>
<td>Cache优化</td>
<td>N/A</td>
<td>在Higgs数据上加速40%</td>
</tr>
<tr>
<td>带深度限制的Leaf-wise的决策树算法</td>
<td>N/A</td>
<td>精度更好的模型</td>
</tr>
</tbody>
</table>
<h2 id="histogram-suan-fa">Histogram算法</h2>
<p>直方图算法的基本思想是先把连续的浮点特征值离散化成k个整数（其实又是分桶的思想，而这些桶称为bin，比如[0,0.1)→0, [0.1,0.3)→1），同时构造一个宽度为k的直方图。</p>
<p>在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</p>
<p><img src="/2020/06/03/machine_learning/LightGBM/histogram-1.png" alt></p>
<p>优点：</p>
<ol>
<li>最明显就是内存消耗的降低，直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用8位整型存储就足够了，内存消耗可以降低为原来的1/8。</li>
<li>在计算上的代价也大幅降低，预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次（k可以认为是常数），时间复杂度从<code>O(#data*#feature)</code>优化到<code>O(k*#features)</code>。</li>
</ol>
<h2 id="dai-shen-du-xian-zhi-de-leaf-wise-de-xie-zi-sheng-chang-ce-lue">带深度限制的Leaf-wise的叶子生长策略</h2>
<p>在XGBoost中，树是按层生长的，称为Level-wise tree growth，同一层的所有节点都做分裂，最后剪枝，如下图所示：</p>
<p><img src="https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155197509149646916.png" alt></p>
<p>Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。</p>
<p>在Histogram算法之上，LightGBM进行进一步的优化。首先它抛弃了大多数GBDT工具使用的按层生长 (level-wise)<br>
的决策树生长策略，而使用了带有深度限制的按叶子生长 (leaf-wise)算法。</p>
<p><img src="https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155197520844369289.png" alt></p>
<p>Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</p>
<h2 id="zhi-fang-tu-chai-jia-su">直方图差加速</h2>
<p>LightGBM另一个优化是Histogram（直方图）做差加速。一个容易观察到的现象：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。</p>
<p>利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。</p>
<h2 id="zhi-jie-zhi-chi-lei-bie-te-zheng">直接支持类别特征</h2>
<p>实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，转化到多维的0/1特征，降低了空间和时间的效率。而类别特征的使用是在实践中很常用的。基于这个考虑，LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。并在决策树算法上增加了类别特征的决策规则。在Expo数据集上的实验，相比0/1展开的方法，训练速度可以加速8倍，并且精度一致。据我们所知，LightGBM是第一个直接支持类别特征的GBDT工具。</p>
<h1 id="light-gbm-you-dian">LightGBM优点</h1>
<p>LightGBM （Light Gradient Boosting Machine）是一个实现GBDT算法的框架，支持高效率的并行训练，并且具有以下优点：</p>
<ul>
<li>更快的训练速度</li>
<li>更低的内存消耗</li>
<li>更好的准确率</li>
<li>分布式支持，可以快速处理海量数据</li>
</ul>
<h1 id="dai-ma-shi-xian">代码实现</h1>
<p>为了演示LightGBM在Python中的用法，本代码以sklearn包中自带的鸢尾花数据集为例，用lightgbm算法实现鸢尾花种类的分类任务。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span>  make_classification</span><br><span class="line"></span><br><span class="line">iris = load_iris()   <span class="comment"># 载入鸢尾花数据集</span></span><br><span class="line">data=iris.data</span><br><span class="line">target = iris.target</span><br><span class="line">X_train,X_test,y_train,y_test =train_test_split(data,target,test_size=<span class="number">0.2</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 加载你的数据</span></span><br><span class="line"><span class="comment"># print('Load data...')</span></span><br><span class="line"><span class="comment"># df_train = pd.read_csv('../regression/regression.train', header=None, sep='\t')</span></span><br><span class="line"><span class="comment"># df_test = pd.read_csv('../regression/regression.test', header=None, sep='\t')</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># y_train = df_train[0].values</span></span><br><span class="line"><span class="comment"># y_test = df_test[0].values</span></span><br><span class="line"><span class="comment"># X_train = df_train.drop(0, axis=1).values</span></span><br><span class="line"><span class="comment"># X_test = df_test.drop(0, axis=1).values</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建成lgb特征的数据集格式</span></span><br><span class="line">lgb_train = lgb.Dataset(X_train, y_train) <span class="comment"># 将数据保存到LightGBM二进制文件将使加载更快</span></span><br><span class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)  <span class="comment"># 创建验证数据</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 将参数写成字典下形式</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'task'</span>: <span class="string">'train'</span>,</span><br><span class="line">    <span class="string">'boosting_type'</span>: <span class="string">'gbdt'</span>,  <span class="comment"># 设置提升类型</span></span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'regression'</span>, <span class="comment"># 目标函数</span></span><br><span class="line">    <span class="string">'metric'</span>: &#123;<span class="string">'l2'</span>, <span class="string">'auc'</span>&#125;,  <span class="comment"># 评估函数</span></span><br><span class="line">    <span class="string">'num_leaves'</span>: <span class="number">31</span>,   <span class="comment"># 叶子节点数</span></span><br><span class="line">    <span class="string">'learning_rate'</span>: <span class="number">0.05</span>,  <span class="comment"># 学习速率</span></span><br><span class="line">    <span class="string">'feature_fraction'</span>: <span class="number">0.9</span>, <span class="comment"># 建树的特征选择比例</span></span><br><span class="line">    <span class="string">'bagging_fraction'</span>: <span class="number">0.8</span>, <span class="comment"># 建树的样本采样比例</span></span><br><span class="line">    <span class="string">'bagging_freq'</span>: <span class="number">5</span>,  <span class="comment"># k 意味着每 k 次迭代执行bagging</span></span><br><span class="line">    <span class="string">'verbose'</span>: <span class="number">1</span> <span class="comment"># &lt;0 显示致命的, =0 显示错误 (警告), &gt;0 显示信息</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">print(<span class="string">'Start training...'</span>)</span><br><span class="line"><span class="comment"># 训练 cv and train</span></span><br><span class="line">gbm = lgb.train(params,lgb_train,num_boost_round=<span class="number">20</span>,valid_sets=lgb_eval,early_stopping_rounds=<span class="number">5</span>) <span class="comment"># 训练数据需要参数列表和数据集</span></span><br><span class="line"> </span><br><span class="line">print(<span class="string">'Save model...'</span>) </span><br><span class="line"> </span><br><span class="line">gbm.save_model(<span class="string">'model.txt'</span>)   <span class="comment"># 训练后保存模型到文件</span></span><br><span class="line"> </span><br><span class="line">print(<span class="string">'Start predicting...'</span>)</span><br><span class="line"><span class="comment"># 预测数据集</span></span><br><span class="line">y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration) <span class="comment">#如果在训练期间启用了早期停止，可以通过best_iteration方式从最佳迭代中获得预测</span></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line">print(<span class="string">'The rmse of prediction is:'</span>, mean_squared_error(y_test, y_pred) ** <span class="number">0.5</span>) <span class="comment"># 计算真实值和预测值之间的均方根误差</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>XGBoost</title>
    <url>/2020/06/01/machine_learning/XGBoost/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/06/01/machine_learning/XGBoost/u=2735111756,2432069868&amp;fm=26&amp;gp=0.jpg.png" alt></p>
<a id="more"></a>
<h1 id="shi-yao-shi-xg-boost">什么是XGBoost</h1>
<p>XGBoost是陈天奇等人开发的一个开源机器学习项目，高效地实现了GBDT算法并进行了算法和工程上的许多改进，被广泛应用在Kaggle竞赛及其他许多机器学习竞赛中并取得了不错的成绩。</p>
<p>说到XGBoost，不得不提GBDT(Gradient Boosting Decision Tree)。因为XGBoost本质上还是一个GBDT，但是力争<strong>把速度和效率发挥到极致</strong>，所以叫X (Extreme) GBoosted。包括前面说过，两者都是boosting方法。</p>
<h2 id="xg-boost-shu-de-ding-yi">XGBoost树的定义</h2>
<p>先来举个<strong>例子</strong>，我们要预测一家人对电子游戏的喜好程度，考虑到年轻和年老相比，年轻更可能喜欢电子游戏，以及男性和女性相比，男性更喜欢电子游戏，故先根据年龄大小区分小孩和大人，然后再通过性别区分开是男是女，逐一给各人在电子游戏喜好程度上打分，就这样，训练出了2棵树tree1和tree2，类似之前GBDT的原理，两棵树的结论累加起来便是最终的结论，所以小孩的预测分数就是两棵树中小孩所落到的结点的分数相加：2 + 0.9 = 2.9。爷爷的预测分数同理：-1 + （-0.9）= -1.9。具体如下图所示：</p>
<p><img src="/2020/06/01/machine_learning/XGBoost/quesbase64153438578739198433.png" alt></p>
<p>惊呼，这不是跟上文介绍的GBDT乃异曲同工么？</p>
<p>事实上，如果不考虑工程实现、解决问题上的一些差异，XGBoost与GBDT<strong>比较大的不同就是目标函数的定义</strong>。XGBoost的目标函数如下图所示：</p>
<p><img src="/2020/06/01/machine_learning/XGBoost/quesbase64153438580139159593.png" alt></p>
<p>其中：</p>
<ul>
<li>红色箭头所指向的L 即为损失函数（比如平方损失函数：\(l(y_i,y^i)=(y_i-y^i)^2\)</li>
<li>红色方框所框起来的是正则项（包括L1正则、L2正则）</li>
<li>红色圆圈所圈起来的为常数项</li>
<li>对于\(f(x)\)，XGBoost利用泰勒展开三项，做一个近似。<strong>\(f(x)\)表示的是其中一颗回归树。</strong></li>
</ul>
<p>XGBoost的<strong>核心算法思想</strong>，基本就是：</p>
<ol>
<li>不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数**\(f(x)\)**，去拟合上次预测的残差。</li>
<li>当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数</li>
<li>最后只需要将每棵树对应的分数加起来就是该样本的预测值。</li>
</ol>
<p>显然，我们的目标是要使得树群的预测值\(y_i^{\prime}\)尽量接近真实值\(y_i\)，而且有尽量大的泛化能力。类似之前GBDT的套路，XGBoost也是需要将多棵树的得分累加得到最终的预测得分（每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差）。</p>
<p><img src="/2020/06/01/machine_learning/XGBoost/quesbase64153438657261833493.png" alt></p>
<p>那接下来，如何选择每一轮加入什么 \(f\) 呢？答案是非常直接的，选取一个 \(f\) 来使得我们的目标函数尽量最大地降低。这里 \(f\) 可以使用泰勒展开公式近似。</p>
<h2 id="zheng-ze-xiang-shu-de-fu-za-du">正则项：树的复杂度</h2>
<p>XGBoost对树的复杂度包含了两个部分：</p>
<ul>
<li>一个是树里面叶子节点的个数T</li>
<li>一个是树上叶子节点的得分w的L2模平方（对w进行L2正则化，相当于针对每个叶结点的得分增加L2平滑，目的是为了避免过拟合）</li>
</ul>
<p><img src="https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64153438674199471483.png" alt></p>
<p>再来看一下XGBoost的目标函数（损失函数揭示训练误差 + 正则化定义复杂度）：</p>
<p>\[
L(\phi)=\sum_{i}l(y_i^{\prime}-y_i)+\sum_k\Omega(f_t)
\]<br>
正则化公式也就是目标函数的后半部分，对于上式而言，\(y_i^{\prime}\)是整个累加模型的输出，正则化项\(\sum \Omega (f_t)\)是则表示树的复杂度的函数，值越小复杂度越低，泛化能力越强。</p>
<h2 id="shu-gai-zen-yao-chang">树该怎么长</h2>
<p>从头到尾了解了xgboost如何优化、如何计算，但树到底长啥样，我们却一直没看到。很显然，一棵树的生成是由一个节点一分为二，然后不断分裂最终形成为整棵树。那么树怎么分裂的就成为了接下来我们要探讨的关键。对于一个叶子节点如何进行分裂，XGBoost作者在其原始论文中给出了一种分裂节点的方法：<strong>枚举所有不同树结构的贪心法</strong></p>
<p>不断地枚举不同树的结构，然后利用打分函数来寻找出一个最优结构的树，接着加入到模型中，不断重复这样的操作。这个寻找的过程使用的就是<strong>贪心算法</strong>。选择一个feature分裂，计算loss function最小值，然后再选一个feature分裂，又得到一个loss function最小值，你枚举完，找一个效果最好的，把树给分裂，就得到了小树苗。</p>
<p>总而言之，XGBoost使用了和CART回归树一样的想法，利用贪婪算法，遍历所有特征的所有特征划分点，不同的是使用的目标函数不一样。具体做法就是分裂后的目标函数值比单子叶子节点的目标函数的增益，同时为了限制树生长过深，还加了个阈值，只有当增益大于该阈值才进行分裂。从而继续分裂，形成一棵树，再形成一棵树，<strong>每次在上一次的预测基础上取最优进一步分裂/建树。</strong></p>
<h2 id="ru-he-ting-zhi-shu-de-xun-huan-sheng-cheng">如何停止树的循环生成</h2>
<p>凡是这种循环迭代的方式必定有停止条件，什么时候停止呢？简言之，设置树的最大深度、当样本权重和小于设定阈值时停止生长以防止过拟合。具体而言，则</p>
<ol>
<li>当引入的分裂带来的增益小于设定阀值的时候，可以忽略掉这个分裂，所以并不是每一次分裂loss function整体都会增加的，有点预剪枝的意思，阈值参数为（即正则项里叶子节点数T的系数）；</li>
<li>当树达到最大深度时则停止建立决策树，设置一个超参数max_depth，避免树太深导致学习局部样本，从而过拟合；</li>
<li>样本权重和小于设定阈值时则停止建树。什么意思呢，即涉及到一个超参数-最小的样本权重和min_child_weight，和GBM的 min_child_leaf 参数类似，但不完全一样。大意就是一个叶子节点样本太少了，也终止同样是防止过拟合；</li>
</ol>
<h1 id="xg-boost-yu-gbdt-you-shi-yao-bu-tong">XGBoost与GBDT有什么不同</h1>
<p>除了算法上与传统的GBDT有一些不同外，XGBoost还在工程实现上做了大量的优化。总的来说，两者之间的区别和联系可以总结成以下几个方面。</p>
<ol>
<li>GBDT是机器学习算法，XGBoost是该算法的工程实现。</li>
<li>在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。</li>
<li>GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代 价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。</li>
<li>传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。</li>
<li>传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。</li>
<li>传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略。</li>
</ol>
<h1 id="wei-shi-yao-xg-boost-yao-yong-tai-le-zhan-kai-you-shi-zai-na-li">为什么XGBoost要用泰勒展开，优势在哪里？</h1>
<p>XGBoost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式, 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了XGBoost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归。</p>
<h2 id="dai-ma-shi-xian">代码实现</h2>
<figure class="highlight nix"><table><tr><td class="code"><pre><span class="line"><span class="built_in">import</span> xgboost</span><br><span class="line"><span class="comment"># First XGBoost model for Pima Indians dataset</span></span><br><span class="line">from numpy <span class="built_in">import</span> loadtxt</span><br><span class="line">from xgboost <span class="built_in">import</span> XGBClassifier </span><br><span class="line">from sklearn.model_selection <span class="built_in">import</span> train_test_split</span><br><span class="line">from sklearn.metrics <span class="built_in">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line"><span class="attr">dataset</span> = loadtxt('pima-indians-diabetes.csv', <span class="attr">delimiter=",")</span></span><br><span class="line"><span class="comment"># split data into X and y</span></span><br><span class="line"><span class="attr">X</span> = dataset[:,<span class="number">0</span>:<span class="number">8</span>]</span><br><span class="line"><span class="attr">Y</span> = dataset[:,<span class="number">8</span>]</span><br><span class="line"><span class="comment"># split data into train and test sets</span></span><br><span class="line"><span class="attr">seed</span> = <span class="number">7</span></span><br><span class="line"><span class="attr">test_size</span> = <span class="number">0.33</span></span><br><span class="line">X_train, X_test, y_train, <span class="attr">y_test</span> = train_test_split(X, Y, <span class="attr">test_size=test_size,</span> <span class="attr">random_state=seed)</span></span><br><span class="line"><span class="comment"># fit model no training data</span></span><br><span class="line"><span class="attr">model</span> = XGBClassifier()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># make predictions for test data</span></span><br><span class="line"><span class="attr">y_pred</span> = model.predict(X_test)</span><br><span class="line"><span class="attr">predictions</span> = [round(value) for value <span class="keyword">in</span> y_pred]</span><br><span class="line"><span class="comment"># evaluate predictions</span></span><br><span class="line"><span class="attr">accuracy</span> = accuracy_score(y_test, predictions)</span><br><span class="line">print(<span class="string">"Accuracy: %.2f%%"</span> % (accuracy * <span class="number">100.0</span>))</span><br></pre></td></tr></table></figure>
<h1 id="can-kao-wen-xian">参考文献</h1>
<ol>
<li><a href="https://blog.csdn.net/v_JULY_v/article/details/81410574" target="_blank" rel="noopener">通俗理解kaggle比赛大杀器xgboost</a></li>
<li><a href="https://www.cnblogs.com/cassielcode/p/12469053.html" target="_blank" rel="noopener">XGBoost20题</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>GBDT</title>
    <url>/2020/05/31/machine_learning/gbdt/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/31/machine_learning/gbdt/v2-e9d180e7afd45c9cee0c094039df4eb7_1200x500.jpg" alt></p>
<a id="more"></a>
<h1 id="gbdt-suan-fa-de-guo-cheng">GBDT算法的过程</h1>
<p>GBDT(Gradient Boosting Decision Tree)，全名叫梯度提升决策树，使用的是<strong>Boosting</strong>的思想。</p>
<p>GBDT利用最速下降法的近似方法来实现每一步的优化,用损失函数的<strong>负梯度</strong>作为回归问题中提升树算法中的残差的<strong>近似值</strong>,每一步以此来估计回归树叶结点区域以拟合残差的近似值。</p>
<h2 id="boosting-si-xiang">Boosting思想</h2>
<p>Boosting方法训练基分类器时采用串行的方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。</p>
<p>Bagging与Boosting的串行训练方式不同，Bagging方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。</p>
<h2 id="gbdt-yuan-li">GBDT原理</h2>
<p>GBDT的原理很简单，就是所有弱分类器的结果相加等于预测值，然后下一个弱分类器去拟合误差函数对预测值的残差(这个残差就是预测值与真实值之间的误差)。它里面的弱分类器的表现形式就是各棵树。</p>
<p>举一个非常简单的例子，比如我今年30岁了，但计算机或者模型GBDT并不知道我今年多少岁，那GBDT咋办呢？</p>
<ul>
<li>它会在第一个弱分类器（或第一棵树中）随便用一个年龄比如20岁来拟合，然后发现误差有10岁；</li>
<li>接下来在第二棵树中，用6岁去拟合剩下的损失，发现差距还有4岁；</li>
<li>接着在第三棵树中用3岁拟合剩下的差距，发现差距只有1岁了；</li>
<li>最后在第四课树中用1岁拟合剩下的残差，完美。</li>
<li>最终，四棵树的结论加起来，就是真实年龄30岁（实际工程中，gbdt是计算负梯度，用负梯度近似残差）。</li>
</ul>
<h1 id="wei-shi-yao-gbdt-ke-yi-yong-fu-ti-du-jin-si-can-chai">为什么gbdt可以用负梯度近似残差</h1>
<p>回归任务下，GBDT 在每一轮的迭代时对每个样本都会有一个预测值，此时的损失函数为均方差损失函数，<br>
\[
l(y_i,y^i)=\frac{1}{2}(y_i - y^i)^2
\]<br>
此时的负梯度是这样计算的<br>
\[
-[\frac{ \partial l(y_i,y^i)}{\partial y^i}] = (y_i-y^i)
\]<br>
所以，当损失函数选用均方损失函数是时，每一次拟合的值就是（真实值 - 当前模型预测的值），即残差。此时的变量是\(y^i\)，即“当前预测模型的值”，也就是对它求负梯度。</p>
<h1 id="ti-du-ti-sheng-he-ti-du-xia-jiang-de-qu-bie-he-lian-xi-shi-shi-yao">梯度提升和梯度下降的区别和联系是什么？</h1>
<p>下表是梯度提升算法和梯度下降算法的对比情况。</p>
<ul>
<li>相同之处：两者都是在每一轮迭代中，利用损失函数相对于模型的负梯度方向的信息来对当前模型进行更新。</li>
<li>不同之处：在梯度下降中，模型是以参数化形式表示，从而模型的更新等价于参数的更新。而在梯度提升中，模型并不需要进行参数化表示，而是直接定义在函数空间中，从而大大扩展了可以使用的模型种类。</li>
</ul>
<table>
<thead>
<tr>
<th>名称</th>
<th>学习空间</th>
<th>更新方式</th>
<th>损失函数</th>
</tr>
</thead>
<tbody>
<tr>
<td>梯度提升</td>
<td>函数空间\(F\)</td>
<td>\(F=F_{t-1}-\rho \nabla_F L \mid_{F=F_{t-1}}\)</td>
<td>\(L = \sum_{i}{l(y_i,F(x_i))}\)</td>
</tr>
<tr>
<td>梯度下降</td>
<td>参数空间\(W\)</td>
<td>\(W_t = w_{t-1}-\rho \nabla_wL\mid_{w=w_{t-1}}\)</td>
<td>\(L = \sum_{i}{l(y_i,F(x_i))}\)</td>
</tr>
</tbody>
</table>
<h1 id="strong-gbdt-strong-de-you-dian-he-ju-xian-xing-you-na-xie"><strong>GBDT</strong>的优点和局限性有哪些？</h1>
<h2 id="you-dian">优点</h2>
<ol>
<li>预测阶段的计算速度快，树与树之间可并行化计算。</li>
<li>在分布稠密的数据集上，泛化能力和表达能力都很好，这使得GBDT在Kaggle的众多竞赛中，经常名列榜首。</li>
<li>采用决策树作为弱分类器使得GBDT模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系。</li>
</ol>
<h2 id="ju-xian-xing">局限性</h2>
<ol>
<li>GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。</li>
<li>GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。</li>
<li>训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提高训练速度。</li>
</ol>
<h1 id="rf-sui-ji-sen-lin-yu-gbdt-zhi-jian-de-qu-bie-yu-lian-xi">RF(随机森林)与GBDT之间的区别与联系</h1>
<p><strong>相同点</strong>：</p>
<ul>
<li>都是由多棵树组成，最终的结果都是由多棵树一起决定。</li>
<li>RF和GBDT在使用CART树时，可以是分类树或者回归树。</li>
</ul>
<p><strong>不同点</strong>：</p>
<ul>
<li>组成随机森林的树可以并行生成，而GBDT是串行生成</li>
<li>随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和</li>
<li>随机森林对异常值不敏感，而GBDT对异常值比较敏感</li>
<li>随机森林是减少模型的方差，而GBDT是减少模型的偏差</li>
<li>随机森林不需要进行特征归一化。而GBDT<strong>则需要进行特征归一化</strong></li>
</ul>
<h1 id="dai-ma">代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="comment"># 训练数据</span></span><br><span class="line">train_feature = np.genfromtxt(<span class="string">"train_feat.txt"</span>,dtype=np.float32)</span><br><span class="line">num_feature = len(train_feature[<span class="number">0</span>])</span><br><span class="line">train_feature = pd.DataFrame(train_feature)</span><br><span class="line"></span><br><span class="line">train_label = train_feature.iloc[:, num_feature - <span class="number">1</span>]</span><br><span class="line">train_feature = train_feature.iloc[:, <span class="number">0</span>:num_feature - <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据</span></span><br><span class="line">test_feature = np.genfromtxt(<span class="string">"test_feat.txt"</span>,dtype=np.float32)</span><br><span class="line">num_feature = len(test_feature[<span class="number">0</span>])</span><br><span class="line">test_feature = pd.DataFrame(test_feature)</span><br><span class="line"></span><br><span class="line">test_label = test_feature.iloc[:, num_feature - <span class="number">1</span>]</span><br><span class="line">test_feature = test_feature.iloc[:, <span class="number">0</span>:num_feature - <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">gbdt = GradientBoostingRegressor(</span><br><span class="line">  loss = <span class="string">'ls'</span></span><br><span class="line">, learning_rate = <span class="number">0.1</span></span><br><span class="line">, n_estimators = <span class="number">100</span></span><br><span class="line">, subsample = <span class="number">1</span></span><br><span class="line">, min_samples_split = <span class="number">2</span></span><br><span class="line">, min_samples_leaf = <span class="number">1</span></span><br><span class="line">, max_depth = <span class="number">3</span></span><br><span class="line">, init = <span class="literal">None</span></span><br><span class="line">, random_state = <span class="literal">None</span></span><br><span class="line">, max_features = <span class="literal">None</span></span><br><span class="line">, alpha = <span class="number">0.9</span></span><br><span class="line">, verbose = <span class="number">0</span></span><br><span class="line">, max_leaf_nodes = <span class="literal">None</span></span><br><span class="line">, warm_start = <span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">gbdt.fit(train_feature, train_label)</span><br><span class="line">pred = gbdt.predict(test_feature)</span><br><span class="line">total_err = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(pred.shape[<span class="number">0</span>]):</span><br><span class="line">    print(<span class="string">'pred:'</span>, pred[i], <span class="string">' label:'</span>, test_label[i])</span><br><span class="line">print(<span class="string">'均方误差:'</span>, np.sqrt(((pred - test_label) ** <span class="number">2</span>).mean()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">pred: <span class="number">320.0008173984891</span>  label: <span class="number">320.0</span></span><br><span class="line">pred: <span class="number">360.99965033119537</span>  label: <span class="number">361.0</span></span><br><span class="line">pred: <span class="number">363.99928183902097</span>  label: <span class="number">364.0</span></span><br><span class="line">pred: <span class="number">336.0002344322584</span>  label: <span class="number">336.0</span></span><br><span class="line">pred: <span class="number">358.0000159974151</span>  label: <span class="number">358.0</span></span><br><span class="line">均方误差: <span class="number">0.0005218003748239915</span></span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>随机森林</title>
    <url>/2020/05/31/machine_learning/random-forest/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/31/machine_learning/random-forest/v2-88dddf356f60e384e3bae3561e8c19f4_1200x500.jpg" alt></p>
<a id="more"></a>
<h1 id="shi-yao-shi-sui-ji-sen-lin">什么是随机森林</h1>
<h2 id="bagging-si-xiang">Bagging思想</h2>
<p>Bagging是bootstrap aggregating。思想就是从总体样本当中随机取一部分样本进行训练，通过多次这样的结果，进行投票获取平均值作为结果输出，这就极大可能的避免了不好的样本数据，从而提高准确度。因为有些是不好的样本，相当于噪声，模型学入噪声后会使准确度不高。</p>
<p><strong>举个例子</strong>：</p>
<p>假设有1000个样本，如果按照以前的思维，是直接把这1000个样本拿来训练，但现在不一样，先抽取800个样本来进行训练，假如噪声点是这800个样本以外的样本点，就很有效的避开了。重复以上操作，提高模型输出的平均值。</p>
<h2 id="ji-cheng-fang-fa">集成方法</h2>
<h3 id="bagging">bagging</h3>
<p>装袋法的核心思想是构建多个相互独立的评估器，然后对其预测进行平均或多数表决原则来决定集成评估器的结果。</p>
<ol>
<li>评估器：相互独立，同时运行</li>
<li>抽样：有放回抽样</li>
<li>如何决定集成的结果：平均或者少数服从多数</li>
<li>目标：降低方差</li>
<li>基学习器过拟合：能够一定程度上解决基学习器过拟合的问题</li>
<li>基学习器学习能力弱：不是很有帮助</li>
<li>代表算法：随机森林</li>
</ol>
<h3 id="boosting">boosting</h3>
<p>提升方法中，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本进行预测，从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树。</p>
<ol>
<li>评估器：相互关联，按顺序依次构建，后建的模型在先建模型预测失败的样本上有更多的权重</li>
<li>抽样：有放回的采样，但会确认数据的权重，每次抽样都会给预测失败的样本更多的权重</li>
<li>如何决定集成的结果：加权平均，在训练集上表现好的模型会有更大的权重</li>
<li>目标：降低偏差，提高模型整体的精确度</li>
<li>基学习器过拟合：加剧过拟合问题</li>
<li>基学习器学习能力弱：提升模型表现</li>
<li>代表算法：GBDT,Adaboost</li>
</ol>
<h3 id="stacking">stacking</h3>
<p>Stacking模型是指将多种分类器组合在一起来取得更好表现的一种集成学习模型。一般情况下，Stacking模型分为两层。第一层中我们训练多个不同的模型，然后再以第一层训练的各个模型的输出作为输入来训练第二层的模型，以得到一个最终的输出。</p>
<h2 id="sui-ji-sen-lin">随机森林</h2>
<p>Random Forest(随机森林)是一种基于树模型的Bagging的优化版本，一棵树的生成肯定还是不如多棵树，因此就有了随机森林，解决决策树泛化能力弱的特点。(可以理解成三个臭皮匠顶过诸葛亮)</p>
<p>而同一批数据，用同样的算法只能产生一棵树，这时Bagging策略可以帮助我们产生不同的数据集。<strong>Bagging</strong>策略来源于bootstrap aggregation：从样本集（假设样本集\(N\)个数据点）中重采样选出\(n\)个样本（有放回的采样，样本数据点个数仍然不变为N），在所有样本上，对这\(n\)个样本建立分类器（ID3\C4.5\CART\SVM\LOGISTIC），重复以上两步\(m\)次，获得\(m\)个分类器，最后根据这\(m\)个分类器的投票结果，决定数据属于哪一类。</p>
<p><strong>每棵树的按照如下规则生成：</strong></p>
<ol>
<li>如果训练集大小为N，对于每棵树而言，<strong>随机</strong>且有放回地从训练集中的抽取N个训练样本，作为该树的训练集；</li>
<li>如果每个样本的特征维度为M，指定一个常数m&lt;&lt;M，<strong>随机</strong>地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；</li>
<li>每棵树都尽最大程度的生长，并且没有剪枝过程。</li>
</ol>
<p>一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。</p>
<p>总的来说就是随机选择样本数，随机选取特征，随机选择分类器，建立多颗这样的决策树，然后通过这几课决策树来投票，决定数据属于哪一类(<strong>投票机制有一票否决制、少数服从多数、加权多数</strong>)</p>
<h2 id="sui-ji-sen-lin-fen-lei-xiao-guo-de-ying-xiang-yin-su">随机森林分类效果的影响因素</h2>
<ul>
<li>森林中<strong>任意两棵树的相关性</strong>：相关性越大，错误率越大；</li>
<li>森林中<strong>每棵树的分类能力</strong>：每棵树的分类能力越强，整个森林的错误率越低。</li>
</ul>
<p>减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。</p>
<h2 id="sui-ji-sen-lin-you-shi-yao-you-que-dian">随机森林有什么优缺点</h2>
<p><strong>优点：</strong></p>
<ul>
<li>在当前的很多数据集上，相对其他算法有着很大的优势，表现良好。</li>
<li>它能够处理很高维度（feature很多）的数据，并且不用做特征选择(因为特征子集是随机选择的)。</li>
<li>在训练完后，它能够给出哪些feature比较重要。</li>
<li>训练速度快，容易做成并行化方法(训练时树与树之间是相互独立的)。</li>
<li>在训练过程中，能够检测到feature间的互相影响。</li>
<li>对于不平衡的数据集来说，它可以平衡误差。</li>
<li>如果有很大一部分的特征遗失，仍可以维持准确度。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>随机森林已经被证明在某些<strong>噪音较大</strong>的分类或回归问题上会过拟合。</li>
<li>对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。</li>
</ul>
<h2 id="sui-ji-sen-lin-ru-he-chu-li-que-shi-zhi">随机森林如何处理缺失值？</h2>
<p>根据随机森林创建和训练的特点，随机森林对缺失值的处理还是比较特殊的。</p>
<ul>
<li>首先，给缺失值预设一些估计值，比如数值型特征，选择其余数据的中位数或众数作为当前的估计值</li>
<li>然后，根据估计的数值，建立随机森林，把所有的数据放进随机森林里面跑一遍。记录每一组数据在决策树中一步一步分类的路径.</li>
<li>判断哪组数据和缺失数据路径最相似，引入一个相似度矩阵，来记录数据之间的相似度，比如有N组数据，相似度矩阵大小就是N*N</li>
<li>如果缺失值是类别变量，通过权重投票得到新估计值，如果是数值型变量，通过加权平均得到新的估计值，如此迭代，直到得到稳定的估计值。</li>
</ul>
<p>其实，该缺失值填补过程类似于推荐系统中采用协同过滤进行评分预测，先计算缺失特征与其他特征的相似度，再加权得到缺失值的估计，而随机森林中计算相似度的方法（数据在决策树中一步一步分类的路径）乃其独特之处。</p>
<h2 id="shi-yao-shi-oob-sui-ji-sen-lin-zhong-oob-shi-ru-he-ji-suan-de-ta-you-shi-yao-you-que-dian">什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？</h2>
<p><strong>OOB</strong>：</p>
<p>上面我们提到，构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob error（out-of-bag error）。</p>
<p>bagging方法中Bootstrap每次约有1/3的样本不会出现在Bootstrap所采集的样本集合中，当然也就没有参加决策树的建立，把这1/3的数据称为<strong>袋外数据oob（out of bag）</strong>,它可以用于取代测试集误差估计方法。</p>
<p><strong>袋外数据(oob)误差的计算方法如下：</strong></p>
<ul>
<li>对于已经生成的随机森林,用袋外数据测试其性能,假设袋外数据总数为O,用这O个袋外数据作为输入,带进之前已经生成的随机森林分类器,分类器会给出O个数据相应的分类</li>
<li>因为这O条数据的类型是已知的,则用正确的分类与随机森林分类器的结果进行比较,统计随机森林分类器分类错误的数目,设为X,则袋外数据误差大小=X/O</li>
</ul>
<p><strong>优缺点</strong>：</p>
<p>这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。</p>
<h2 id="dai-ma">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载波士顿房价数据集</span></span><br><span class="line">boston_house = load_boston()</span><br><span class="line"></span><br><span class="line">boston_feature_name = boston_house.feature_names</span><br><span class="line">boston_features = boston_house.data</span><br><span class="line">boston_target = boston_house.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看类的用法</span></span><br><span class="line">help(RandomForestRegressor)</span><br><span class="line">rgs = RandomForestRegressor(n_estimators=<span class="number">15</span>)  <span class="comment">##随机森林模型</span></span><br><span class="line">rgs = rgs.fit(boston_features, boston_target)</span><br><span class="line">rgs.predict(boston_features)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>随机森林</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2020/05/30/machine_learning/decision-tree/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/30/machine_learning/decision-tree/v2-39d109b46ea4f34d5efbf67edc11d57d_1440w.png" alt></p>
<a id="more"></a>
<h1 id="shi-yao-shi-jue-ce-shu">什么是决策树</h1>
<h2 id="jue-ce-shu-de-ji-ben-si-xiang">决策树的基本思想</h2>
<p><img src="/2020/05/30/machine_learning/decision-tree/00630Defly1g4q286viibj30pk0pfk09.jpg" alt></p>
<p>LR模型是一股脑儿的把所有特征塞入学习，决策树更像是编程语言中的if-else一样，去做条件判断，这是两者根本性的区别。主要优点是模型具有可读性，分类速度快。其主要围绕着两个问题：</p>
<ol>
<li>如何从数据表中找出最佳节点和最佳分枝？</li>
<li>如何让决策树停止生长，防止过拟合？</li>
</ol>
<h2 id="ce-lue">策略</h2>
<p>决策树学习本质上是从训练数据集中归纳出一组分类规则。我们需要的是一个与训练数据<strong>矛盾较小</strong>,同时具有很好的<strong>泛化能力</strong>的决策树。</p>
<h2 id="suan-fa">算法</h2>
<p>决策树学习算法包含<strong>特征选择</strong>，<strong>决策树的生成</strong>与<strong>决策树的剪枝过程</strong>。生成只考虑局部最优，剪枝则考虑全局最优。</p>
<h3 id="te-zheng-xuan-ze">特征选择</h3>
<p>如果利用一个特征进行分类的结果与随机分类的结果没有很大差别,则称这个特征是<strong>没有分类能力</strong>的.扔掉这样的特征对决策树学习的精度影响不大.</p>
<ol>
<li><strong>信息熵</strong>：熵是衡量<strong>随机变量不确定性</strong>的度量.熵越大,随机变量的不确定性就越大.信息熵是信息量的期望，\(\left.H(X)=-\sum_{x \in X} P(x) \log P(x)\right)\)</li>
<li><strong>条件熵</strong>：条件熵表示在已知随机变量X的条件下随机变量Y的不确定性.\(H(Y | X)=\sum_{i=1}^{n} p_{i} H\left(Y | X=x_{i}\right)\)</li>
<li><strong>信息增益</strong>：表示得知特征X的信息而使得类Y的信息的<strong>不确定性减少</strong>的程度.定义为集合D的经验熵与特征A在给定条件下D的经验条件熵之差\(g(D,A)=H(D)-H(D|A)\),也就是训练数据集中类与特征的<strong>互信息</strong>.</li>
<li><strong>信息增益算法</strong>:
<ol>
<li>计算数据集D的经验熵\(H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|_{\mathrm{log}_{2}}\left|C_{k}\right|}{|D|}\)。</li>
<li>计算特征A对数据集D的经验条件熵\(H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{\mu}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{k}\right|}{\left|D_{i}\right|}\)。</li>
<li>计算信息增益,选取信息增益最大的特征.</li>
</ol>
</li>
<li><strong>信息增益比</strong>:信息增益值的大小是相对于训练数据集而言的,并无绝对意义.使用信息增益比,\(g_{R}(D, A)=\frac{g(D, A)}{H(D)}\)可以对这一问题进行校正.</li>
</ol>
<h3 id="jue-ce-shu-de-sheng-cheng">决策树的生成</h3>
<h4 id="id-3-suan-fa">ID3 算法</h4>
<p>在根节点处计算信息熵，然后根据特征依次划分并计算其节点的信息熵，用 信息增益 = 根节点信息熵–属性节点的信息熵，根据信息增益进行降序排列，排在前面的就是第一个划分属性，其后依次类推，这就得到了决策树的形状，也就是怎么“长”了。</p>
<p><img src="/2020/05/30/machine_learning/decision-tree/image39e7b.png" alt></p>
<p><img src="/2020/05/30/machine_learning/decision-tree/image61cdc.png" alt></p>
<p><img src="/2020/05/30/machine_learning/decision-tree/image9e194.png" alt></p>
<p><img src="/2020/05/30/machine_learning/decision-tree/image09288.png" alt></p>
<p>信息增益存在一个问题：对可取值数目较多的属性有所偏好，例如：考虑将“编号”作为一个属性。为了解决这个问题，引出了另一个 算法C4.5。</p>
<h4 id="c-4-5-suan-fa">C4.5算法</h4>
<p>为了解决信息增益的问题，引入一个信息增益比：</p>
<p>\[
Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}
\]</p>
<p>其中：</p>
<p>\[
IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}
\]</p>
<p>特征a(比如身份id)的可能取值数目越多(即V越大)，则IV(a)的值通常就越大。<strong>信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数。特征取值个数较多时，惩罚参数较小（也就是除以一个较大的值）；特征个数较少时，惩罚参数较大（除以一个较小的值）</strong>。不过有一个缺点：信息增益率<strong>偏向取值较少的特征</strong>。</p>
<p>使用信息增益率：基于以上缺点，并不是直接选择信息增益率最大的特征，而是现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。</p>
<h4 id="strong-cart-suan-fa-strong"><strong>CART算法</strong></h4>
<ol>
<li>
<p>CART既可以用于<strong>分类也</strong>可以用于<strong>回归</strong>.它假设决策树是<strong>二叉树</strong>,内部结点特征的取值为&quot;是&quot;和&quot;否&quot;.递归地构建二叉树,对回归树用<strong>平方误差</strong>最小化准则,对分类数用<strong>基尼指数</strong>最小化准则.</p>
</li>
<li>
<p><strong>回归树的生成</strong>:</p>
<p>CART回归树是假设树为二叉树，通过不断将特征进行分裂。比如当前树结点是基于第j个特征值进行分裂的，设该特征值小于s的样本划分为左子树，大于s的样本划分为右子树。<br>
\[
R_{1}(j, s)=\left\{x | x^{(j)} \leqslant s\right\} \quad \quad R_{2}(j, s)=\left\{x | x^{(l)}>s\right\}
\]</p>
<p>而CART回归树实质上就是在该特征维度对样本空间进行划分，而这种空间划分的优化是一种NP难问题，因此，在决策树模型中是使用启发式方法解决。典型CART回归树产生的目标函数为：<br>
\[
\sum_{x_i \in R_m}{(y_i - f(x_i))^2}
\]<br>
因此，当我们为了求解最优的切分特征j和最优的切分点s，就转化为求解这么一个目标函数：<br>
\[
\min _{j, z}\left[\min _{c_1} \sum_{x \in R_{1}(j, x)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x \in R_{2}(j, x)}\left(y_{i}-c_{2}\right)^{2}\right]
\]<br>
所以我们只要遍历所有特征的的所有切分点，就能找到最优的切分特征和切分点。最终得到一棵回归树。</p>
</li>
<li>
<p><strong>基尼指数</strong>:假设有K个类,样本属于第k类的概率为\(p_k\),则概率分布的基尼指数为:\(\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}\),表示不确定性.在特征A的条件下集合D的基尼指数定义为\(\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)\),表示分割后集合D的不确定性.基尼指数越大,样本集合的<strong>不确定性</strong>也就越大.</p>
</li>
<li>
<p><strong>分类树的生成</strong></p>
<ol>
<li>从根结点开始,设结点的训练数据集为D,对每个特征A和其可能取的每个值a,计算A=a时的基尼指数,</li>
<li>选择<strong>基尼指数最小</strong>的特征及其对应的切分点作为<strong>最优特征</strong>与<strong>最优切分点</strong>,生成两个子结点</li>
<li>递归进行以上操作,直至满足<strong>停止条件</strong>.停止条件一般是结点中的样本个数小于阈值,或样本集的基尼指数小于阈值,或没有更多特征.</li>
</ol>
</li>
<li>
<p><strong>CART剪枝</strong></p>
<p>\(T_t\)表示以t为根结点的子树,\(|T_t|\)是\(T_t\)的叶结点个数.可以证明当\(\alpha=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}\)时,\(T_t\)与\(t\)有相同的损失函数值,且\(t\)的结点少,因此\(t\)比\(T_t\)更可取,对\(T_t\)进行剪枝.<strong>自下而上</strong>地对各内部结点t计算\(g(t)=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}\),并令\(a=min(g(t))\),<strong>自上而下</strong>地访问内部节点t,如果有\(g(t)=a\),进行剪枝,并对t以<strong>多数表决法</strong>决定其类,得到子树T,如此循环地生成一串<strong>子树序列</strong>,直到新生成的T是由根结点单独构成的树为止.利用<strong>交叉验证法</strong>在子树序列中选取最优子树.</p>
<p>如果是<strong>连续值</strong>的情况,一般用<strong>二分法</strong>作为结点来划分.</p>
</li>
</ol>
<h4 id="san-chong-bu-tong-de-jue-ce-shu">三种不同的决策树</h4>
<ul>
<li>
<p><strong>ID3</strong>：取值多的属性，更容易使数据更纯，其信息增益更大。训练得到的是一棵庞大且深度浅的树：不合理。</p>
</li>
<li>
<p><strong>C4.5</strong>：采用信息增益率替代信息增益。</p>
</li>
<li>
<p><strong>CART</strong>：以基尼系数替代熵，最小化不纯度，而不是最大化信息增益。</p>
</li>
</ul>
<h3 id="shu-chang-dao-shi-yao-shi-hou-ting">“树”长到什么时候停</h3>
<ul>
<li>当前结点包含的<strong>样本全属于同一类别</strong>，无需划分；</li>
<li>当前<strong>属性集为空</strong>，或是所有样本在所有属性上取值相同，无法划分；例如：所有的样本特征都是一样的，就造成无法划分了，训练集太单一。</li>
<li>当前结点包含的<strong>样本集合为空</strong>，不能划分。</li>
</ul>
<h3 id="jue-ce-shu-de-jian-zhi">决策树的剪枝</h3>
<ol>
<li>在学习时过多考虑如何提高对训练数据的正确分类,从而构建出过于复杂的决策树,产生<strong>过拟合</strong>现象.解决方法是对已生成的决策树进行简化,称为剪枝.</li>
<li>设树的叶结点个数为\(|T|\),每个叶结点有\(N_t\)个样本点,其中\(k\)类样本点有\(N_{tk}\)个,剪枝往往通过极小化决策树整体的损失函数\(C_{\alpha}(T)=\sum_{i=1}^{\pi} N_{i} H_{i}(T)+\alpha|T|\)来实现,其中经验熵\(H_{t}(T)=-\sum_{k} \frac{N_{a}}{N_{t}} \log \frac{N_{u}}{N_{t}}\).剪枝通过加入\(a|T|\)项来考虑模型复杂度,实际上就是用正则化的极大似然估计进行模型选择.</li>
<li><strong>剪枝算法</strong>:剪去某一子结点,如果生成的新的整体树的<strong>损失函数值</strong>小于原树,则进行剪枝,直到不能继续为止.具体可以由动态规划实现.</li>
</ol>
<h1 id="shu-xing-jie-gou-wei-shi-yao-bu-xu-yao-gui-yi-hua">树形结构为什么不需要归一化?</h1>
<p>根本原因在于：特征选择依赖于在某个特征值下的样本数量带来的信息增益，和特征值的大小无关。所以数值缩放不影响分裂点位置，对树模型的结构不造成影响。<br>
按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。</p>
<p>树模型是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。</p>
<p>既然树形结构（如决策树、RF）不需要归一化，那为何非树形结构比如Adaboost、SVM、LR、Knn、KMeans之类则需要归一化。</p>
<p>对于线性模型，特征值差别很大时，运用梯度下降的时候，损失等高线是椭圆形，需要进行多次迭代才能到达最优点。<br>
但是如果进行了归一化，那么等高线就是圆形的，促使SGD往原点迭代，从而导致需要的迭代次数较少。</p>
<h1 id="jue-ce-shu-ru-he-jian-zhi">决策树如何剪枝</h1>
<p>决策树的剪枝基本策略有 预剪枝 (Pre-Pruning) 和 后剪枝 (Post-Pruning)。</p>
<ul>
<li><strong>预剪枝</strong>：其中的核心思想就是，在每一次实际对结点进行进一步划分之前，先采用验证集的数据来验证如果划分是否能提高划分的准确性。如果不能，就把结点标记为叶结点并退出进一步划分；如果可以就继续递归生成节点。</li>
<li><strong>后剪枝</strong>：后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来泛化性能提升，则将该子树替换为叶结点。</li>
</ul>
<h1 id="dai-ma">代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#用于数据处理和分析的工具包</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment">#引入用于数据预处理/特征工程的工具包</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="comment">#import决策树建模包</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">adult_data = pd.read_csv(<span class="string">'./DecisionTree.csv'</span>)</span><br><span class="line">adult_data.info()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pandas</span>.<span class="title">core</span>.<span class="title">frame</span>.<span class="title">DataFrame</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">RangeIndex</span>:</span> <span class="number">32561</span> entries, <span class="number">0</span> to <span class="number">32560</span></span><br><span class="line">Data columns (total <span class="number">9</span> columns):</span><br><span class="line">workclass         <span class="number">32561</span> non-null object</span><br><span class="line">education         <span class="number">32561</span> non-null object</span><br><span class="line">marital-status    <span class="number">32561</span> non-null object</span><br><span class="line">occupation        <span class="number">32561</span> non-null object</span><br><span class="line">relationship      <span class="number">32561</span> non-null object</span><br><span class="line">race              <span class="number">32561</span> non-null object</span><br><span class="line">gender            <span class="number">32561</span> non-null object</span><br><span class="line">native-country    <span class="number">32561</span> non-null object</span><br><span class="line">income            <span class="number">32561</span> non-null object</span><br><span class="line">dtypes: object(<span class="number">9</span>)</span><br><span class="line">memory usage: <span class="number">2.2</span>+ MB</span><br><span class="line">    </span><br><span class="line">adult_data.shape</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">(<span class="number">32561</span>, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列</span></span><br><span class="line">adult_data.columns</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">Index([<span class="string">u'workclass'</span>, <span class="string">u'education'</span>, <span class="string">u'marital-status'</span>, <span class="string">u'occupation'</span>,</span><br><span class="line">       <span class="string">u'relationship'</span>, <span class="string">u'race'</span>, <span class="string">u'gender'</span>, <span class="string">u'native-country'</span>, <span class="string">u'income'</span>],</span><br><span class="line">      dtype=<span class="string">'object'</span>)</span><br><span class="line"><span class="comment"># 特征</span></span><br><span class="line">feature_columns = [<span class="string">u'workclass'</span>, <span class="string">u'education'</span>, <span class="string">u'marital-status'</span>, <span class="string">u'occupation'</span>, <span class="string">u'relationship'</span>, <span class="string">u'race'</span>, <span class="string">u'gender'</span>, <span class="string">u'native-country'</span>]</span><br><span class="line"><span class="comment"># 标签</span></span><br><span class="line">label_column = [<span class="string">'income'</span>]</span><br><span class="line"><span class="comment">#区分特征和目标列</span></span><br><span class="line">features = adult_data[feature_columns]</span><br><span class="line">label = adult_data[label_column]</span><br><span class="line"><span class="comment"># 把每一个离散的类别特征的取值用one-hot 表示（因为数据都是用字符串表示，需要转化成数字）</span></span><br><span class="line">features = pd.get_dummies(features)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化一个决策树分类器</span></span><br><span class="line">clf = tree.DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>, max_depth=<span class="number">4</span>)</span><br><span class="line"><span class="comment">#用决策树分类器拟合数据</span></span><br><span class="line">clf = clf.fit(features.values, label.values)</span><br><span class="line"><span class="comment"># inference</span></span><br><span class="line">clf.predict(features.values)</span><br></pre></td></tr></table></figure>
<h2 id="ke-shi-hua-jue-ce-shu">可视化决策树</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pydotplus</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display, Image</span><br><span class="line"></span><br><span class="line">dot_data = tree.export_graphviz(clf, </span><br><span class="line">                                out_file=<span class="literal">None</span>, </span><br><span class="line">                                feature_names=features.columns,</span><br><span class="line">                                class_names = [<span class="string">'&lt;=50k'</span>, <span class="string">'&gt;50k'</span>],</span><br><span class="line">                                filled = <span class="literal">True</span>,</span><br><span class="line">                                rounded =<span class="literal">True</span></span><br><span class="line">                               )</span><br><span class="line">                               </span><br><span class="line">graph = pydotplus.graph_from_dot_data(dot_data)</span><br><span class="line">display(Image(graph.create_png()))</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/30/machine_learning/decision-tree/download.png" alt></p>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>命名实体识别</title>
    <url>/2020/05/26/named_entity_recognition/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/26/named_entity_recognition/3c5ea3534a4143adbca52823c5b757f1.jpg" alt></p>
<a id="more"></a>
<h1 id="jian-jie">简介</h1>
<p>命名实体识别(Named Entity Recognition,NER)的主要任务是识别出文本中的人名、地名等专有名称和有意义的时间、日期等数量短语并加以归类。命名实体识别技术是信息抽取、信息检索、机器翻译、问答系统等多种自然语言处理技术必不可少的组成部分。</p>
<p>中文命名实体识别的难度要比英文的难度大（<strong>英文专有名词会大写</strong>）。</p>
<h1 id="yan-jiu-zhu-ti">研究主体</h1>
<p>命名实体是命名实体识别的研究主体,一般包括3大类(实体类、时间类和数字类)和7小类(人名、地名、机构名、时间、日期、货币和百分比)命名实体。实际研究中,命名实体的确切含义需要根据具体应用来确定。</p>
<h1 id="te-dian-ji-nan-dian">特点及难点</h1>
<p>评判一个命名实体是否被正确识别包括两个方面:</p>
<ol>
<li>实体的边界是否正确</li>
<li>实体的类型是否标注正确。</li>
</ol>
<p>英语中的命名实体具有比较明显的形态标志,如人名、地名等实体中的每个词的第一个字母要大写等,所以实体边界识别相对汉语来说比较容易,任务的重点是确定实体的类型。</p>
<p>和英语相比,汉语命名实体识别任务更加复杂,一方面由于分词等因素的影响难度较大,另一方面，其难点主要表现在如下几个方面:</p>
<ol>
<li>命名实体类型多样,数量众多,不断有新的命名实体涌现,如新的人名、地名等,<strong>难以建立大而全的</strong>姓氏库、名字库、地址库等<strong>数据库</strong>。</li>
<li>命名实体构成结构比较复杂,<strong>存在大量的嵌套、别名、缩略词等问题</strong>,没有严格的规律可以遵循,对这类命名实体识别的<strong>召回率</strong>相对偏低。</li>
<li><strong>存在大量的交叉和互相包含现象</strong>,组织名称中也存在大量的人名、地名、数字的现象,要正确标注这些命名实体类型,要涉及上下文语义层面的分析,这些都给命名实体的识别带来困难。</li>
<li>误差传播，<strong>分词、语法分析系统的可靠性</strong>也直接决定命名实体识别的有效性,使得中文命名实体识别更加困难。</li>
</ol>
<h1 id="zhu-yao-fang-fa">主要方法</h1>
<p>命名实体识别的主要技术方法分为:基于规则和词典的方法、无监督学习方法、监督学习方法（基于统计的方法、基于深度学习的方法）。</p>
<h2 id="ji-yu-gui-ze-he-ci-dian-de-fang-fa">基于规则和词典的方法</h2>
<ol>
<li>特定领域词典，其中还包括同义林词典；</li>
<li>句法词汇模板；</li>
<li>正则表达式；</li>
</ol>
<p>总的来说，当词汇表足够大时，基于规则的方法能够取得不错效果。但总结规则模板花费大量时间，且词汇表规模小，且实体识别结果普遍高精度、低召回。</p>
<h2 id="wu-jian-du-xue-xi-fang-fa">无监督学习方法</h2>
<p>主要是基于聚类的方法，根据文本相似度得到不同的簇，表示不同的实体类别组。常用到的特征或者辅助信息有词汇资源、语料统计信息（TF-IDF）、浅层语义信息等。</p>
<h2 id="jian-du-xue-xi-fang-fa">监督学习方法</h2>
<h3 id="ji-yu-tong-ji-de-fang-fa">基于统计的方法</h3>
<p>NER 任务可以是看作是 <strong>token 级别的多分类任务</strong>或<strong>序列标注任务</strong>。</p>
<p>**特征工程：**word 级别特征（词法特征、词性标注等），词汇特征（维基百科、DBpdia 知识），文档及语料级别特征。</p>
<p>**机器学习算法：**隐马尔可夫模型 HMM、决策树 DT、最大熵模型 MEM、最大熵马尔科夫模型 HEMM、支持向量机 SVM、条件随机场 CRF。</p>
<h4 id="yin-ma-er-ke-fu-mo-xing">隐马尔科夫模型</h4>
<p>隐马尔可夫模型描述由一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。隐马尔可夫模型由初始状态分布，状态转移概率矩阵以及观测概率矩阵所确定。</p>
<p>NER本质上可以看成是一种序列标注问题，在使用HMM解决NER这种序列标注问题的时候，我们所能观测到的是字组成的序列（观测序列），观测不到的是每个字对应的标注（状态序列）。对应的，HMM的<strong>三个要素</strong>可以解释为，<strong>初始状态分布</strong>就是每一个标注作为句子第一个字的标注的概率，<strong>状态转移概率矩阵</strong>就是由某一个标注转移到下一个标注的概率，<strong>观测概率矩阵</strong>就是指在某个标注下，生成某个词的概率。根据HMM的三个要素，可以定义如下的HMM模型:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HMM</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, N, M)</span>:</span></span><br><span class="line">        <span class="string">"""Args:</span></span><br><span class="line"><span class="string">            N: 状态数，这里对应存在的标注的种类</span></span><br><span class="line"><span class="string">            M: 观测数，这里对应有多少不同的字</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.N = N</span><br><span class="line">        self.M = M</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 状态转移概率矩阵 A[i][j]表示从i状态转移到j状态的概率</span></span><br><span class="line">        self.A = torch.zeros(N, N)</span><br><span class="line">        <span class="comment"># 观测概率矩阵, B[i][j]表示i状态下生成j观测的概率</span></span><br><span class="line">        self.B = torch.zeros(N, M)</span><br><span class="line">        <span class="comment"># 初始状态概率  Pi[i]表示初始时刻为状态i的概率</span></span><br><span class="line">        self.Pi = torch.zeros(N)</span><br></pre></td></tr></table></figure>
<p>HMM模型的训练过程对应隐马尔可夫模型的学习问题，根据训练数据利用<strong>最大似然</strong>的方法估计模型的三个要素：初始状态分布、状态转移概率矩阵以及观测概率矩阵。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HMM</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, N, M)</span>:</span></span><br><span class="line">        ....</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, word_lists, tag_lists, word2id, tag2id)</span>:</span></span><br><span class="line">        <span class="string">"""HMM的训练，即根据训练语料对模型参数进行估计,</span></span><br><span class="line"><span class="string">           因为我们有观测序列以及其对应的状态序列，所以我们</span></span><br><span class="line"><span class="string">           可以使用极大似然估计的方法来估计隐马尔可夫模型的参数</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            word_lists: 列表，其中每个元素由字组成的列表，如 ['担','任','科','员']</span></span><br><span class="line"><span class="string">            tag_lists: 列表，其中每个元素是由对应的标注组成的列表，如 ['O','O','B-TITLE', 'E-TITLE']</span></span><br><span class="line"><span class="string">            word2id: 将字映射为ID</span></span><br><span class="line"><span class="string">            tag2id: 字典，将标注映射为ID</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> len(tag_lists) == len(word_lists)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 估计转移概率矩阵</span></span><br><span class="line">        <span class="keyword">for</span> tag_list <span class="keyword">in</span> tag_lists:</span><br><span class="line">            seq_len = len(tag_list)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(seq_len - <span class="number">1</span>):</span><br><span class="line">                current_tagid = tag2id[tag_list[i]]</span><br><span class="line">                next_tagid = tag2id[tag_list[i+<span class="number">1</span>]]</span><br><span class="line">                self.A[current_tagid][next_tagid] += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 一个重要的问题：如果某元素没有出现过，该位置为0，这在后续的计算中是不允许的</span></span><br><span class="line">        <span class="comment"># 解决方法：我们将等于0的概率加上很小的数</span></span><br><span class="line">        self.A[self.A == <span class="number">0.</span>] = <span class="number">1e-10</span></span><br><span class="line">        self.A = self.A / self.A.sum(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 估计观测概率矩阵</span></span><br><span class="line">        <span class="keyword">for</span> tag_list, word_list <span class="keyword">in</span> zip(tag_lists, word_lists):</span><br><span class="line">            <span class="keyword">assert</span> len(tag_list) == len(word_list)</span><br><span class="line">            <span class="keyword">for</span> tag, word <span class="keyword">in</span> zip(tag_list, word_list):</span><br><span class="line">                tag_id = tag2id[tag]</span><br><span class="line">                word_id = word2id[word]</span><br><span class="line">                self.B[tag_id][word_id] += <span class="number">1</span></span><br><span class="line">        self.B[self.B == <span class="number">0.</span>] = <span class="number">1e-10</span></span><br><span class="line">        self.B = self.B / self.B.sum(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 估计初始状态概率</span></span><br><span class="line">        <span class="keyword">for</span> tag_list <span class="keyword">in</span> tag_lists:</span><br><span class="line">            init_tagid = tag2id[tag_list[<span class="number">0</span>]]</span><br><span class="line">            self.Pi[init_tagid] += <span class="number">1</span></span><br><span class="line">        self.Pi[self.Pi == <span class="number">0.</span>] = <span class="number">1e-10</span></span><br><span class="line">        self.Pi = self.Pi / self.Pi.sum()</span><br></pre></td></tr></table></figure>
<p>模型训练完毕之后，要利用训练好的模型进行解码，就是对给定的模型未见过的句子，求句子中的每个字对应的标注，针对解码问题，使用的是维特比（viterbi）算法。实现的细节如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HMM</span><span class="params">(object)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decoding</span><span class="params">(self, word_list, word2id, tag2id)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        使用维特比算法对给定观测序列求状态序列， 这里就是对字组成的序列,求其对应的标注。</span></span><br><span class="line"><span class="string">        维特比算法实际是用动态规划解隐马尔可夫模型预测问题，即用动态规划求概率最大路径（最优路径）</span></span><br><span class="line"><span class="string">        这时一条路径对应着一个状态序列</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 问题:整条链很长的情况下，十分多的小概率相乘，最后可能造成下溢</span></span><br><span class="line">        <span class="comment"># 解决办法：采用对数概率，这样源空间中的很小概率，就被映射到对数空间的大的负数</span></span><br><span class="line">        <span class="comment">#  同时相乘操作也变成简单的相加操作</span></span><br><span class="line">        A = torch.log(self.A)</span><br><span class="line">        B = torch.log(self.B)</span><br><span class="line">        Pi = torch.log(self.Pi)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化 维比特矩阵viterbi 它的维度为[状态数, 序列长度]</span></span><br><span class="line">        <span class="comment"># 其中viterbi[i, j]表示标注序列的第j个标注为i的所有单个序列(i_1, i_2, ..i_j)出现的概率最大值</span></span><br><span class="line">        seq_len = len(word_list)</span><br><span class="line">        viterbi = torch.zeros(self.N, seq_len)</span><br><span class="line">        <span class="comment"># backpointer是跟viterbi一样大小的矩阵</span></span><br><span class="line">        <span class="comment"># backpointer[i, j]存储的是 标注序列的第j个标注为i时，第j-1个标注的id</span></span><br><span class="line">        <span class="comment"># 等解码的时候，我们用backpointer进行回溯，以求出最优路径</span></span><br><span class="line">        backpointer = torch.zeros(self.N, seq_len).long()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.Pi[i] 表示第一个字的标记为i的概率</span></span><br><span class="line">        <span class="comment"># Bt[word_id]表示字为word_id的时候，对应各个标记的概率</span></span><br><span class="line">        <span class="comment"># self.A.t()[tag_id]表示各个状态转移到tag_id对应的概率</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 所以第一步为</span></span><br><span class="line">        start_wordid = word2id.get(word_list[<span class="number">0</span>], <span class="literal">None</span>)</span><br><span class="line">        Bt = B.t()</span><br><span class="line">        <span class="keyword">if</span> start_wordid <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 如果字不再字典里，则假设状态的概率分布是均匀的</span></span><br><span class="line">            bt = torch.log(torch.ones(self.N) / self.N)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            bt = Bt[start_wordid]</span><br><span class="line">        viterbi[:, <span class="number">0</span>] = Pi + bt</span><br><span class="line">        backpointer[:, <span class="number">0</span>] = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 递推公式：</span></span><br><span class="line">        <span class="comment"># viterbi[tag_id, step] = max(viterbi[:, step-1]* self.A.t()[tag_id] * Bt[word])</span></span><br><span class="line">        <span class="comment"># 其中word是step时刻对应的字</span></span><br><span class="line">        <span class="comment"># 由上述递推公式求后续各步</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1</span>, seq_len):</span><br><span class="line">            wordid = word2id.get(word_list[step], <span class="literal">None</span>)</span><br><span class="line">            <span class="comment"># 处理字不在字典中的情况</span></span><br><span class="line">            <span class="comment"># bt是在t时刻字为wordid时，状态的概率分布</span></span><br><span class="line">            <span class="keyword">if</span> wordid <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 如果字不再字典里，则假设状态的概率分布是均匀的</span></span><br><span class="line">                bt = torch.log(torch.ones(self.N) / self.N)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                bt = Bt[wordid]  <span class="comment"># 否则从观测概率矩阵中取bt</span></span><br><span class="line">            <span class="keyword">for</span> tag_id <span class="keyword">in</span> range(len(tag2id)):</span><br><span class="line">                max_prob, max_id = torch.max(</span><br><span class="line">                    viterbi[:, step<span class="number">-1</span>] + A[:, tag_id],</span><br><span class="line">                    dim=<span class="number">0</span></span><br><span class="line">                )</span><br><span class="line">                viterbi[tag_id, step] = max_prob + bt[tag_id]</span><br><span class="line">                backpointer[tag_id, step] = max_id</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 终止， t=seq_len 即 viterbi[:, seq_len]中的最大概率，就是最优路径的概率</span></span><br><span class="line">        best_path_prob, best_path_pointer = torch.max(</span><br><span class="line">            viterbi[:, seq_len<span class="number">-1</span>], dim=<span class="number">0</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 回溯，求最优路径</span></span><br><span class="line">        best_path_pointer = best_path_pointer.item()</span><br><span class="line">        best_path = [best_path_pointer]</span><br><span class="line">        <span class="keyword">for</span> back_step <span class="keyword">in</span> range(seq_len<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            best_path_pointer = backpointer[best_path_pointer, back_step]</span><br><span class="line">            best_path_pointer = best_path_pointer.item()</span><br><span class="line">            best_path.append(best_path_pointer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将tag_id组成的序列转化为tag</span></span><br><span class="line">        <span class="keyword">assert</span> len(best_path) == len(word_list)</span><br><span class="line">        id2tag = dict((id_, tag) <span class="keyword">for</span> tag, id_ <span class="keyword">in</span> tag2id.items())</span><br><span class="line">        tag_list = [id2tag[id_] <span class="keyword">for</span> id_ <span class="keyword">in</span> reversed(best_path)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tag_list</span><br></pre></td></tr></table></figure>
<h4 id="tiao-jian-sui-ji-chang">条件随机场</h4>
<p>HMM模型中存在两个假设，一是输出观察值之间严格独立（观测独立性假设），二是状态转移过程中当前状态只与前一状态有关（齐次马尔科夫性假设）。在命名实体识别的场景下，HMM认为观测到的句子中的每个字都是相互独立的，而且当前时刻的标注只与前一时刻的标注相关。但实际上，命名实体识别往往需要更多的特征，比如词性，词的上下文等等，同时当前时刻的标注应该与前一时刻以及后一时刻的标注都相关联。由于这两个假设的存在，显然HMM模型在解决命名实体识别的问题上是存在缺陷的。</p>
<p>而条件随机场就没有这种问题，它通过引入自定义的特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖，可以有效克服HMM模型面临的问题。</p>
<p>为了建立一个条件随机场，首先要定义一个<strong>特征函数集</strong>，该函数集内的每个特征函数都以标注序列作为输入，提取的特征作为输出。假设该函数集为：<br>
\[
\Phi\left(x_{1}, \ldots, x_{m}, s_{1}, \ldots, s_{m}\right) \in \mathbb{R}^{d}
\]<br>
其中 \(x=(x_1,x_2,\ldots,x_m)\)表示观测序列， \(s=(s_1,s_2,\ldots,s_m)\)表示状态序列。然后，条件随机场使用对数线性模型来计算给定观测序列下状态序列的条件概率：<br>
\[
p(s | x ; w)=\frac{\exp (w \cdot \Phi(x, s))}{\sum_{s^{\prime}} \exp \left(w \cdot \Phi\left(x, s^{\prime}\right)\right)}
\]<br>
其中 \(s^\prime\)是是所有可能的状态序列，\(w\)是条件随机场模型的参数，可以把它看成是每个特征函数的权重。CRF模型的训练其实就是对参数 <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]"> 的估计。假设有 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 个已经标注好的数据 <img src="https://www.zhihu.com/equation?tex=%7B%28x%5Ei%2C+s%5Ei%29%7D_%7Bi%3D1%7D%5En" alt="[公式]"> ，则其对数似然函数的正则化形式如下：<br>
\[
L(w)=\sum_{i=1}^{n} \log p\left(s^{i} | x^{i} ; w\right)-\frac{\lambda_{2}}{2}\|w\|_{2}^{2}-\lambda_{1}\|w\|_{1}
\]<br>
那么，最优参数 \(w^\star\) 就是：<br>
\[
w^{*}=\arg \max _{w \in \mathbb{R}^{d}} L\left(w\right)
\]<br>
解码采用维特比算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn_crfsuite <span class="keyword">import</span> CRF   <span class="comment"># 借助一个外部的库</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word2features</span><span class="params">(sent, i)</span>:</span></span><br><span class="line">    <span class="string">"""抽取单个字的特征"""</span></span><br><span class="line">    word = sent[i]</span><br><span class="line">    prev_word = <span class="string">"&lt;s&gt;"</span> <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> sent[i<span class="number">-1</span>]</span><br><span class="line">    next_word = <span class="string">"&lt;/s&gt;"</span> <span class="keyword">if</span> i == (len(sent)<span class="number">-1</span>) <span class="keyword">else</span> sent[i+<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 因为每个词相邻的词会影响这个词的标记</span></span><br><span class="line">    <span class="comment"># 所以我们使用：</span></span><br><span class="line">    <span class="comment"># 前一个词，当前词，后一个词，</span></span><br><span class="line">    <span class="comment"># 前一个词+当前词， 当前词+后一个词</span></span><br><span class="line">    <span class="comment"># 作为特征</span></span><br><span class="line">    features = &#123;</span><br><span class="line">        <span class="string">'w'</span>: word,</span><br><span class="line">        <span class="string">'w-1'</span>: prev_word,</span><br><span class="line">        <span class="string">'w+1'</span>: next_word,</span><br><span class="line">        <span class="string">'w-1:w'</span>: prev_word+word,</span><br><span class="line">        <span class="string">'w:w+1'</span>: word+next_word,</span><br><span class="line">        <span class="string">'bias'</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sent2features</span><span class="params">(sent)</span>:</span></span><br><span class="line">    <span class="string">"""抽取序列特征"""</span></span><br><span class="line">    <span class="keyword">return</span> [word2features(sent, i) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(sent))]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRFModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 algorithm=<span class="string">'lbfgs'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 c1=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 c2=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_iterations=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 all_possible_transitions=False</span></span></span><br><span class="line"><span class="function"><span class="params">                 )</span>:</span></span><br><span class="line"></span><br><span class="line">        self.model = CRF(algorithm=algorithm,</span><br><span class="line">                         c1=c1,</span><br><span class="line">                         c2=c2,</span><br><span class="line">                         max_iterations=max_iterations,</span><br><span class="line">                         all_possible_transitions=all_possible_transitions)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, sentences, tag_lists)</span>:</span></span><br><span class="line">        <span class="string">"""训练模型"""</span></span><br><span class="line">        features = [sent2features(s) <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">        self.model.fit(features, tag_lists)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, sentences)</span>:</span></span><br><span class="line">        <span class="string">"""解码,对给定句子预测其标注"""</span></span><br><span class="line">        features = [sent2features(s) <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">        pred_tag_lists = self.model.predict(features)</span><br><span class="line">        <span class="keyword">return</span> pred_tag_lists</span><br></pre></td></tr></table></figure>
<h3 id="strong-shen-du-xue-xi-de-fang-fa-strong"><strong>深度学习的方法</strong></h3>
<h4 id="fen-bu-shi-biao-shi">分布式表示</h4>
<ol>
<li>词级别表示：使用词嵌入方式，对不同语料进行训练，如生物医学领域PubMed、NYT 之类。</li>
<li>字符级别表示：字符嵌入主要可以降低 OOV 率。</li>
<li>混合信息表示：除了词级别表示、字符级别表示外，一些研究工作还嵌入了其他一些语义信息，如词汇相似 度、词性标注、分块、语义依赖、汉字偏旁、汉字拼音以及位置嵌入，词嵌入，段嵌入等。</li>
</ol>
<h4 id="shang-xia-wen-bian-ma">上下文编码</h4>
<p>卷积网络 CNN、循环网络 RNN、递归网络、Transformer</p>
<h5 id="cnn">CNN</h5>
<p>句子经过 embedding 层，一个 word 被表示为 N 维度的向量，随后整个句子表示使用卷积（通常为一维卷积）编码，进而得到每个 word 的局部特征，再使用最大池化操作得到整个句子的全局特征，可以直接将其送入解码层输出标签，也可以将其和局部特征向量一起送入解码层。</p>
<p><img src="/2020/05/26/named_entity_recognition/640.png" alt></p>
<h5 id="rnn">RNN</h5>
<p>LSTM也常常被用来解决序列标注问题。和HMM、CRF不同的是，LSTM是依靠神经网络超强的非线性拟合能力，在训练时将样本通过高维空间中的复杂非线性变换，学习到从样本到标注的函数，之后使用这个函数为指定的样本预测每个token的标注。LSTM比起CRF模型最大的好处就是<strong>简单粗暴</strong>，不需要做繁杂的特征工程，直接训练即可.</p>
<p>常用的循环神经网络包括 LSTM 和 GRU，在 NLP 中常使用双向网络 BiRNN，从左到右和从右到左两个方向提取问题特征。</p>
<p><img src="/2020/05/26/named_entity_recognition/640-20200527211629345.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_packed_sequence, pack_padded_sequence</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiLSTM</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, emb_size, hidden_size, out_size)</span>:</span></span><br><span class="line">        <span class="string">"""初始化参数：</span></span><br><span class="line"><span class="string">            vocab_size:字典的大小</span></span><br><span class="line"><span class="string">            emb_size:词向量的维数</span></span><br><span class="line"><span class="string">            hidden_size：隐向量的维数</span></span><br><span class="line"><span class="string">            out_size:标注的种类</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(BiLSTM, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, emb_size)</span><br><span class="line">        self.bilstm = nn.LSTM(emb_size, hidden_size,</span><br><span class="line">                              batch_first=<span class="literal">True</span>,</span><br><span class="line">                              bidirectional=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.lin = nn.Linear(<span class="number">2</span>*hidden_size, out_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, sents_tensor, lengths)</span>:</span></span><br><span class="line">        emb = self.embedding(sents_tensor)  <span class="comment"># [B, L, emb_size]</span></span><br><span class="line"></span><br><span class="line">        packed = pack_padded_sequence(emb, lengths, batch_first=<span class="literal">True</span>)</span><br><span class="line">        rnn_out, _ = self.bilstm(packed)</span><br><span class="line">        <span class="comment"># rnn_out:[B, L, hidden_size*2]</span></span><br><span class="line">        rnn_out, _ = pad_packed_sequence(rnn_out, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        scores = self.lin(rnn_out)  <span class="comment"># [B, L, out_size]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, sents_tensor, lengths, _)</span>:</span></span><br><span class="line">        <span class="string">"""解码"""</span></span><br><span class="line">        logits = self.forward(sents_tensor, lengths)  <span class="comment"># [B, L, out_size]</span></span><br><span class="line">        _, batch_tagids = torch.max(logits, dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> batch_tagid</span><br></pre></td></tr></table></figure>
<h5 id="rnn-yu-cnn-xiang-jie-he-de-fang-shi">RNN与CNN 相结合的方式</h5>
<p>BiLSTM-CNN 的网络结构，有 ID-CNNs 迭代膨胀卷积：</p>
<p><img src="/2020/05/26/named_entity_recognition/640-20200527211818522.png" alt></p>
<h5 id="lstm-crf">LSTM + CRF</h5>
<p>简单的LSTM的优点是能够通过<strong>双向</strong>的设置学习到观测序列（输入的字）之间的依赖，在训练过程中，LSTM能够根据目标自动提取观测序列的特征，但是缺点是无法学习到状态序列（输出的标注）之间的关系，而在命名实体识别任务中，标注之间是有一定的关系的，比如O类标注后面不会接一个I类标注，所以LSTM在解决NER这类序列标注任务时，虽然可以省去很繁杂的特征工程，但是也存在无法学习到标注上下文的缺点。</p>
<p>相反，CRF的优点就是能对隐含状态建模，学习状态序列的特点，但它的缺点是需要手动提取序列特征。所以一般的做法是，在LSTM后面再加一层CRF，以获得两者的优点。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiLSTM_CRF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, emb_size, hidden_size, out_size)</span>:</span></span><br><span class="line">        <span class="string">"""初始化参数：</span></span><br><span class="line"><span class="string">            vocab_size:字典的大小</span></span><br><span class="line"><span class="string">            emb_size:词向量的维数</span></span><br><span class="line"><span class="string">            hidden_size：隐向量的维数</span></span><br><span class="line"><span class="string">            out_size:标注的种类</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(BiLSTM_CRF, self).__init__()</span><br><span class="line">        <span class="comment"># 这里的BiLSTM就是LSTM模型部分所定义的BiLSTM模型</span></span><br><span class="line">        self.bilstm = BiLSTM(vocab_size, emb_size, hidden_size, out_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># CRF实际上就是多学习一个转移矩阵 [out_size, out_size] 初始化为均匀分布</span></span><br><span class="line">        self.transition = nn.Parameter(</span><br><span class="line">            torch.ones(out_size, out_size) * <span class="number">1</span>/out_size)</span><br><span class="line">        <span class="comment"># self.transition.data.zero_()</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, sents_tensor, lengths)</span>:</span></span><br><span class="line">        <span class="comment"># [B, L, out_size]</span></span><br><span class="line">        emission = self.bilstm(sents_tensor, lengths)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算CRF scores, 这个scores大小为[B, L, out_size, out_size]</span></span><br><span class="line">        <span class="comment"># 也就是每个字对应对应一个 [out_size, out_size]的矩阵</span></span><br><span class="line">        <span class="comment"># 这个矩阵第i行第j列的元素的含义是：上一时刻tag为i，这一时刻tag为j的分数</span></span><br><span class="line">        batch_size, max_len, out_size = emission.size()</span><br><span class="line">        crf_scores = emission.unsqueeze(</span><br><span class="line">            <span class="number">2</span>).expand(<span class="number">-1</span>, <span class="number">-1</span>, out_size, <span class="number">-1</span>) + self.transition.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> crf_scores</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, test_sents_tensor, lengths, tag2id)</span>:</span></span><br><span class="line">        <span class="string">"""使用维特比算法进行解码"""</span></span><br><span class="line">        start_id = tag2id[<span class="string">'&lt;start&gt;'</span>]</span><br><span class="line">        end_id = tag2id[<span class="string">'&lt;end&gt;'</span>]</span><br><span class="line">        pad = tag2id[<span class="string">'&lt;pad&gt;'</span>]</span><br><span class="line">        tagset_size = len(tag2id)</span><br><span class="line"></span><br><span class="line">        crf_scores = self.forward(test_sents_tensor, lengths)</span><br><span class="line">        device = crf_scores.device</span><br><span class="line">        <span class="comment"># B:batch_size, L:max_len, T:target set size</span></span><br><span class="line">        B, L, T, _ = crf_scores.size()</span><br><span class="line">        <span class="comment"># viterbi[i, j, k]表示第i个句子，第j个字对应第k个标记的最大分数</span></span><br><span class="line">        viterbi = torch.zeros(B, L, T).to(device)</span><br><span class="line">        <span class="comment"># backpointer[i, j, k]表示第i个句子，第j个字对应第k个标记时前一个标记的id，用于回溯</span></span><br><span class="line">        backpointer = (torch.zeros(B, L, T).long() * end_id).to(device)</span><br><span class="line">        lengths = torch.LongTensor(lengths).to(device)</span><br><span class="line">        <span class="comment"># 向前递推</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(L):</span><br><span class="line">            batch_size_t = (lengths &gt; step).sum().item()</span><br><span class="line">            <span class="keyword">if</span> step == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 第一个字它的前一个标记只能是start_id</span></span><br><span class="line">                viterbi[:batch_size_t, step,</span><br><span class="line">                        :] = crf_scores[: batch_size_t, step, start_id, :]</span><br><span class="line">                backpointer[: batch_size_t, step, :] = start_id</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                max_scores, prev_tags = torch.max(</span><br><span class="line">                    viterbi[:batch_size_t, step<span class="number">-1</span>, :].unsqueeze(<span class="number">2</span>) +</span><br><span class="line">                    crf_scores[:batch_size_t, step, :, :],     <span class="comment"># [B, T, T]</span></span><br><span class="line">                    dim=<span class="number">1</span></span><br><span class="line">                )</span><br><span class="line">                viterbi[:batch_size_t, step, :] = max_scores</span><br><span class="line">                backpointer[:batch_size_t, step, :] = prev_tags</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在回溯的时候我们只需要用到backpointer矩阵</span></span><br><span class="line">        backpointer = backpointer.view(B, <span class="number">-1</span>)  <span class="comment"># [B, L * T]</span></span><br><span class="line">        tagids = []  <span class="comment"># 存放结果</span></span><br><span class="line">        tags_t = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(L<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            batch_size_t = (lengths &gt; step).sum().item()</span><br><span class="line">            <span class="keyword">if</span> step == L<span class="number">-1</span>:</span><br><span class="line">                index = torch.ones(batch_size_t).long() * (step * tagset_size)</span><br><span class="line">                index = index.to(device)</span><br><span class="line">                index += end_id</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                prev_batch_size_t = len(tags_t)</span><br><span class="line"></span><br><span class="line">                new_in_batch = torch.LongTensor(</span><br><span class="line">                    [end_id] * (batch_size_t - prev_batch_size_t)).to(device)</span><br><span class="line">                offset = torch.cat(</span><br><span class="line">                    [tags_t, new_in_batch],</span><br><span class="line">                    dim=<span class="number">0</span></span><br><span class="line">                )  <span class="comment"># 这个offset实际上就是前一时刻的</span></span><br><span class="line">                index = torch.ones(batch_size_t).long() * (step * tagset_size)</span><br><span class="line">                index = index.to(device)</span><br><span class="line">                index += offset.long()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            tags_t = backpointer[:batch_size_t].gather(</span><br><span class="line">                dim=<span class="number">1</span>, index=index.unsqueeze(<span class="number">1</span>).long())</span><br><span class="line">            tags_t = tags_t.squeeze(<span class="number">1</span>)</span><br><span class="line">            tagids.append(tags_t.tolist())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># tagids:[L-1]（L-1是因为扣去了end_token),大小的liebiao</span></span><br><span class="line">        <span class="comment"># 其中列表内的元素是该batch在该时刻的标记</span></span><br><span class="line">        <span class="comment"># 下面修正其顺序，并将维度转换为 [B, L]</span></span><br><span class="line">        tagids = list(zip_longest(*reversed(tagids), fillvalue=pad))</span><br><span class="line">        tagids = torch.Tensor(tagids).long()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 返回解码的结果</span></span><br><span class="line">        <span class="keyword">return</span> tagids</span><br></pre></td></tr></table></figure>
<h5 id="di-gui-shen-jing-wang-luo">递归神经网络</h5>
<p>递归神经网络相较循环神经网络，最大区别是具有树状阶层结构。循环神经网络一个很好的特性是通过神经元循环结构处理变长序列，而对于具有树状或图结构的数据很难建模（如语法解析树）。还有一点特别在于其训练算法不同于常规的后向传播算法，而是采用 BPTS (Back Propagation Through Structure)。虽然递归神经网络理论上感觉效果不错，但实际应用中效果一般，并且很难训练。</p>
<p><img src="/2020/05/26/named_entity_recognition/640-20200527213037128.png" alt="img"></p>
<h5 id="transformer">Transformer</h5>
<p>transformer 在长距离文本依赖上相较 RNN 有更好的效果。</p>
<p><img src="/2020/05/26/named_entity_recognition/over_all.png" alt></p>
<h5 id="bert-lstm-crf">BERT+（LSTM）+CRF</h5>
<p>BERT中蕴含了大量的通用知识，利用预训练好的BERT模型，再用少量的标注数据进行FINETUNE是一种快速的获得效果不错的NER的方法。</p>
<h5 id="strong-yu-yan-mo-xing-strong"><strong>语言模型</strong></h5>
<ol>
<li>word2vec</li>
<li>Glove</li>
<li>fasttext</li>
<li>ELMO</li>
<li>BERT</li>
<li>GPT</li>
<li>GPT2</li>
<li>XLNET</li>
<li>ALBERT</li>
<li>RoBERTa</li>
</ol>
<h4 id="strong-jie-ma-ceng-strong"><strong>解码层</strong></h4>
<ol>
<li>MLP+softmax</li>
<li>CRF</li>
<li>RNN</li>
<li>Pointer Network</li>
</ol>
<p>1和2比较常见。3使用 RNN 解码，框架图如下所示。文中所述当前输出（并非隐藏层输出）经过 softmax 损失函数后输入至下一时刻 LSTM 单元，所以这是一个局部归一化模型。</p>
<p><img src="/2020/05/26/named_entity_recognition/640-20200527213946340.png" alt></p>
<p>使用指针网络解码，是将 NER 任务当作先识别“块”即实体范围，然后再对其进行分类。指针网络通常是在 Seq2seq 框架中，如下图所示。</p>
<p><img src="/2020/05/26/named_entity_recognition/640-20200527214031105.png" alt></p>
<h2 id="strong-qi-ta-yan-jiu-fang-xiang-de-ner-fang-fa-strong"><strong>其他研究方向的NER方法</strong></h2>
<ol>
<li>多任务学习 Multi-task Learning</li>
<li>深度迁移学习 Deep Transfer Learning</li>
<li>深度主动学习 Deep Active Learning</li>
<li>深度强化学习 Deep Reinforcement Learning</li>
<li>深度对抗学习 Deep Adversarial Learning</li>
<li>注意力机制 Neural Attention</li>
</ol>
<h1 id="shu-ju-ji">数据集</h1>
<ol>
<li>医疗数据集</li>
<li>kaggle数据集</li>
<li>BosonNLP命名实体识别数据</li>
</ol>
<h1 id="ping-ce-zhi-biao">评测指标</h1>
<p>NER 评测指标 P R F1 分为两类，这也是比赛和论文中通用评测方式：</p>
<ol>
<li><strong>Exact-match</strong>严格匹配，范围与类别都正确。其中 F1 值又可以分为 macro-averaged 和 micro-averaged，前者是按照不同实体类别计算 F1，然后取平均；后者是把所有识别结果合在一起，再计算 F1。这两者的区别在于实体类别数目不均衡，因为通常语料集中类别数量分布不均衡，模型往往对于大类别的实体学习较好。</li>
<li><strong>relaxed match</strong> 宽松匹配，简言之，可视为实体位置区间部分重叠，或位置正确类别错误的，都记为正确或按照匹配的位置区间大小评测。</li>
</ol>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/61227299" target="_blank" rel="noopener">NLP实战-中文命名实体识别</a></li>
<li><a href="https://blog.csdn.net/u014033218/article/details/89304699" target="_blank" rel="noopener">命名实体识别研究综述</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
      <tags>
        <tag>ner</tag>
        <tag>命名实体识别</tag>
      </tags>
  </entry>
  <entry>
    <title>过拟合-欠拟合</title>
    <url>/2020/05/24/machine_learning/overfit_underfit/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/24/machine_learning/overfit_underfit/3.11_capacity_vs_error.svg" alt></p>
<a id="more"></a>
<h1 id="qian-ni-he-he-guo-ni-he">欠拟合和过拟合</h1>
<p>当模型在训练数据集上更准确时，它在测试数据集上却不一定更准确。这是为什么呢？</p>
<h2 id="xun-lian-wu-chai-he-fan-hua-wu-chai">训练误差和泛化误差</h2>
<p>在解释上述现象之前，需要区分训练误差和泛化误差。通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。</p>
<p>在机器学习里，通常假设训练数据集和测试数据集里的每一个样本都是从同一个概率分布中相互独立地生成的。基于该独立同分布假设，给定任意一个机器学习模型，它的训练误差的期望和泛化误差都是一样的。例如，如果将模型参数设成随机值，那么训练误差和泛化误差会非常相近。但是，模型的参数是通过在训练数据集上训练模型而学习出的，参数的选择依据了最小化训练误差。所以，训练误差的期望小于或等于泛化误差。也就是说，一般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于或等于在测试数据集上的表现。由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低。机器学习模型应关注降低泛化误差。</p>
<h2 id="qian-ni-he-he-guo-ni-he-1">欠拟合和过拟合</h2>
<p>接下来，将探究模型训练中经常出现的两类典型问题：一类是模型无法得到较低的训练误差，这一现象被称作欠拟合；另一类是模型的训练误差远小于它在测试数据集上的误差，称该现象为过拟合。在实践中，要尽可能同时应对欠拟合和过拟合。</p>
<h3 id="mo-xing-fu-za-du">模型复杂度</h3>
<p>为了解释模型复杂度，以多项式函数拟合为例。给定一个由标量数据特征\(x\)和对应的标量标签\(y\)组成的训练数据集，多项式函数拟合的目标是找一个\(K\)阶多项式函数</p>
<p>\[
\hat{y} = b + \sum_{k=1}^K x^k w_k
\]</p>
<p>来近似 \(y\)。在上式中，\(w_k\)是模型的权重参数，\(b\)是偏差参数。与线性回归相同，多项式函数拟合也使用平方损失函数。特别地，一阶多项式函数拟合又叫线性函数拟合。</p>
<p>因为高阶多项式函数模型参数更多，模型函数的选择空间更大，所以高阶多项式函数比低阶多项式函数的复杂度更高。因此，高阶多项式函数比低阶多项式函数更容易在相同的训练数据集上得到更低的训练误差。给定训练数据集，模型复杂度和误差之间的关系通常如下图所示。给定训练数据集，如果模型的复杂度过低，很容易出现欠拟合；如果模型复杂度过高，很容易出现过拟合。应对欠拟合和过拟合的一个办法是针对数据集选择合适复杂度的模型。</p>
<p><img src="/2020/05/24/machine_learning/overfit_underfit/3.11_capacity_vs_error.svg" alt></p>
<h3 id="xun-lian-shu-ju-ji-da-xiao">训练数据集大小</h3>
<p>影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。如果训练数据集中样本数过少，特别是比模型参数数量更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。</p>
<h1 id="jie-jue-guo-ni-he-de-fang-fa">解决过拟合的方法</h1>
<h2 id="quan-zhong-shuai-jian">权重衰减</h2>
<p>当模型过拟合时，模型的训练误差远小于它在测试集上的误差。虽然增大训练数据集可能会减轻过拟合，但是获取额外的训练数据往往代价高昂。应对过拟合问题的常用方法：权重衰减。</p>
<h3 id="fang-fa">方法</h3>
<p>权重衰减等价于 \(L_2\) 范数正则化。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。\(L_2\)范数正则化在模型原损失函数基础上添加\(L_2\)范数惩罚项，从而得到训练所需要最小化的函数。\(L_2\)范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。以的线性回归损失函数</p>
<p>\[
\ell(w_1, w_2, b) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2
\]</p>
<p>为例，其中\(w_1, w_2\)是权重参数，\(b\)是偏差参数，样本\(i\)的输入为\(x_1^{(i)}, x_2^{(i)}\)，标签为\(y^{(i)}\)，样本数为\(n\)。将权重参数用向量\(\boldsymbol{w} = [w_1, w_2]\)表示，带有\(L_2\)范数惩罚项的新损失函数为</p>
<p>\[
\ell(w_1, w_2, b) + \frac{\lambda}{2n} \|\boldsymbol{w}\|^2
\]</p>
<p>其中超参数\(\lambda > 0\)。当权重参数均为0时，惩罚项最小。当\(\lambda\)较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当\(\lambda\)设为0时，惩罚项完全不起作用。上式中\(L_2\)范数平方\(\|\boldsymbol{w}\|^2\)展开后得到\(w_1^2 + w_2^2\)。有了\(L_2\)范数惩罚项后，在小批量随机梯度下降中，我们将线性回归一节中权重\(w_1\)和\(w_2\)的迭代方式更改为</p>
<p>\[
\begin{aligned}
w_1 &amp;\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\
w_2 &amp;\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right).
\end{aligned}
\]</p>
<p>可见，\(L_2\)范数正则化令权重\(w_1\)和\(w_2\)先自乘小于1的数，再减去不含惩罚项的梯度。因此，\(L_2\)范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。实际场景中，有时也在惩罚项中添加偏差元素的平方和。</p>
<h3 id="dai-ma-shi-xian">代码实现</h3>
<p>首先，定义随机初始化模型参数的函数。该函数为每个参数都附上梯度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_params</span><span class="params">()</span>:</span></span><br><span class="line">    w = tf.Variable(tf.random.normal(mean=<span class="number">1</span>, shape=(num_inputs, <span class="number">1</span>)))</span><br><span class="line">    b = tf.Variable(tf.zeros(shape=(<span class="number">1</span>,)))</span><br><span class="line">    <span class="keyword">return</span> [w, b]</span><br></pre></td></tr></table></figure>
<p>定义\(L_2\)范数惩罚项。这里只惩罚模型的权重参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">l2_penalty</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_sum((w**<span class="number">2</span>)) / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>定义如何在训练数据集和测试数据集上分别训练和测试模型。这里在计算最终的损失函数时添加了\(L_2\)范数惩罚项。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size, num_epochs, lr = <span class="number">1</span>, <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">net, loss = linreg, squared_loss</span><br><span class="line">optimizer = tf.keras.optimizers.SGD()</span><br><span class="line">train_iter = tf.data.Dataset.from_tensor_slices((train_features, train_labels)).batch(batch_size).shuffle(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot</span><span class="params">(lambd)</span>:</span></span><br><span class="line">    w, b = init_params()</span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="keyword">with</span> tf.GradientTape(persistent=<span class="literal">True</span>) <span class="keyword">as</span> tape:</span><br><span class="line">                <span class="comment"># 添加了L2范数惩罚项</span></span><br><span class="line">                l = loss(net(X, w, b), y) + lambd * l2_penalty(w)</span><br><span class="line">            grads = tape.gradient(l, [w, b])</span><br><span class="line">            sgd([w, b], lr, batch_size, grads)</span><br><span class="line">        train_ls.append(tf.reduce_mean(loss(net(train_features, w, b),</span><br><span class="line">                             train_labels)).numpy())</span><br><span class="line">        test_ls.append(tf.reduce_mean(loss(net(test_features, w, b),</span><br><span class="line">                            test_labels)).numpy())</span><br><span class="line">    semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'loss'</span>,</span><br><span class="line">                 range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    print(<span class="string">'L2 norm of w:'</span>, tf.norm(w).numpy())</span><br></pre></td></tr></table></figure>
<p>当<code>lambd</code>设为0时，我们没有使用权重衰减。结果训练误差远小于测试集上的误差。这是典型的过拟合现象。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">fit_and_plot</span><span class="params">(lambd=<span class="number">0</span>)</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/24/machine_learning/overfit_underfit/3.12_output1.png" alt></p>
<p>下面使用权重衰减。可以看出，训练误差虽然有所提高，但测试集上的误差有所下降。过拟合现象得到一定程度的缓解。另外，权重参数的\(L_2\)范数比不使用权重衰减时的更小，此时的权重参数更接近0。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit_and_plot(lambd=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/24/machine_learning/overfit_underfit/3.12_output2.png" alt></p>
<h3 id="tensorflow-kuai-su-shi-xian">tensorflow快速实现</h3>
<p>直接在构造优化器实例时通过<code>weight_decay</code>参数来指定权重衰减超参数。默认下，tensorflow会对权重和偏差同时衰减。可以分别对权重和偏差构造优化器实例，从而只对权重衰减。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot_tf2</span><span class="params">(wd, lr=<span class="number">1e-3</span>)</span>:</span></span><br><span class="line">    net = models.Sequential()</span><br><span class="line">    net.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">    net.build(input_shape=(<span class="number">1</span>, <span class="number">200</span>))</span><br><span class="line">    w, b = net.trainable_variables</span><br><span class="line">    optimizer = optimizers.SGD(learning_rate=lr)</span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">                l = loss(net(X), y) + wd * l2_penalty(w)</span><br><span class="line">            grads = tape.gradient(l, net.trainable_variables)</span><br><span class="line">            optimizer.apply_gradients(zip(grads, net.trainable_variables))</span><br><span class="line">        train_ls.append(tf.reduce_mean(loss(net(train_features),</span><br><span class="line">                             train_labels)).numpy())</span><br><span class="line">        test_ls.append(tf.reduce_mean(loss(net(test_features),</span><br><span class="line">                            test_labels)).numpy())</span><br><span class="line">    semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'loss'</span>,</span><br><span class="line">                 range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    print(<span class="string">'L2 norm of w:'</span>, tf.norm(w).numpy())</span><br></pre></td></tr></table></figure>
<h2 id="dropout">Dropout</h2>
<p>除了权重衰减以外，深度学习模型常常使用dropout 来应对过拟合问题。</p>
<p>多层感知机描述了一个单隐藏层的多层感知机。其中输入个数为4，隐藏单元个数为5，且隐藏单元\(h_i\)（\(i=1, \ldots, 5\)）的计算表达式为</p>
<p>\[
h_i = \phi\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\right)
\]</p>
<p>这里\(\phi\)是激活函数，\(x_1, \ldots, x_4\)是输入，隐藏单元\(i\)的权重参数为\(w_{1i}, \ldots, w_{4i}\)，偏差参数为\(b_i\)。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为\(p\)，那么有\(p\)的概率\(h_i\)会被清零，有\(1-p\)的概率\(h_i\)会除以\(1-p\)做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量\(\xi_i\)为0和1的概率分别为\(p\)和\(1-p\)。使用丢弃法时我们计算新的隐藏单元\(h_i'\)</p>
<p>\[
h_i' = \frac{\xi_i}{1-p} h_i
\]</p>
<p>由于\(E(\xi_i) = 1-p\)，因此</p>
<p>\[
E(h_i') = \frac{E(\xi_i)}{1-p}h_i = h_i
\]</p>
<p>即<strong>丢弃法不改变其输入的期望值</strong>。对图1中的隐藏层使用丢弃法，一种可能的结果如图2所示，其中\(h_2\)和\(h_5\)被清零。这时输出值的计算不再依赖\(h_2\)和\(h_5\)，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即\(h_1, \ldots, h_5\)都有可能被清零，输出层的计算无法过度依赖\(h_1, \ldots, h_5\)中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法。</p>
<p><img src="/2020/05/24/machine_learning/overfit_underfit/3.8_mlp.svg" alt="图1"></p>
<p><img src="/2020/05/24/machine_learning/overfit_underfit/3.13_dropout.svg" alt="图2"></p>
<h3 id="dai-ma-shi-xian-1">代码实现</h3>
<p>根据丢弃法的定义，可以很容易地实现它。下面的<code>dropout</code>函数将以<code>drop_prob</code>的概率丢弃<code>X</code>中的元素。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras, nn， losses</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dropout, Flatten, Dense</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(X, drop_prob)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= drop_prob &lt;= <span class="number">1</span></span><br><span class="line">    keep_prob = <span class="number">1</span> - drop_prob</span><br><span class="line">    <span class="comment"># 这种情况下把全部元素都丢弃</span></span><br><span class="line">    <span class="keyword">if</span> keep_prob == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> tf.zeros_like(X)</span><br><span class="line">    <span class="comment">#初始mask为一个bool型数组，故需要强制类型转换</span></span><br><span class="line">    mask = tf.random.uniform(shape=X.shape, minval=<span class="number">0</span>, maxval=<span class="number">1</span>) &lt; keep_prob</span><br><span class="line">    <span class="keyword">return</span> tf.cast(mask, dtype=tf.float32) * tf.cast(X, dtype=tf.float32) / keep_prob</span><br></pre></td></tr></table></figure>
<p>定义一个包含两个隐藏层的多层感知机，其中两个隐藏层的输出个数都是256。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = tf.Variable(tf.random.normal(stddev=<span class="number">0.01</span>, shape=(num_inputs, num_hiddens1)))</span><br><span class="line">b1 = tf.Variable(tf.zeros(num_hiddens1))</span><br><span class="line">W2 = tf.Variable(tf.random.normal(stddev=<span class="number">0.1</span>, shape=(num_hiddens1, num_hiddens2)))</span><br><span class="line">b2 = tf.Variable(tf.zeros(num_hiddens2))</span><br><span class="line">W3 = tf.Variable(tf.random.truncated_normal(stddev=<span class="number">0.01</span>, shape=(num_hiddens2, num_outputs)))</span><br><span class="line">b3 = tf.Variable(tf.zeros(num_outputs))</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2, W3, b3]</span><br></pre></td></tr></table></figure>
<p>下面定义的模型将全连接层和激活函数ReLU串起来，并对每个激活函数的输出使用丢弃法。可以分别设置各个层的丢弃概率。通常是把靠近输入层的丢弃概率设得小一点。在实验中，把第一个隐藏层的丢弃概率设为0.2，把第二个隐藏层的丢弃概率设为0.5。通过参数<code>is_training</code>函数来判断运行模式为训练还是测试，并只需在训练模式下使用丢弃法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">drop_prob1, drop_prob2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X, is_training=False)</span>:</span></span><br><span class="line">    X = tf.reshape(X, shape=(<span class="number">-1</span>,num_inputs))</span><br><span class="line">    H1 = tf.nn.relu(tf.matmul(X, W1) + b1)</span><br><span class="line">    <span class="keyword">if</span> is_training:<span class="comment"># 只在训练模型时使用丢弃法</span></span><br><span class="line">      H1 = dropout(H1, drop_prob1)  <span class="comment"># 在第一层全连接后添加丢弃层</span></span><br><span class="line">    H2 = nn.relu(tf.matmul(H1, W2) + b2)</span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">      H2 = dropout(H2, drop_prob2)  <span class="comment"># 在第二层全连接后添加丢弃层</span></span><br><span class="line">    <span class="keyword">return</span> tf.math.softmax(tf.matmul(H2, W3) + b3)</span><br></pre></td></tr></table></figure>
<p>训练和测试模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> fashion_mnist</span><br><span class="line"></span><br><span class="line">batch_size=<span class="number">256</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()</span><br><span class="line">x_train = tf.cast(x_train, tf.float32) / <span class="number">255</span> <span class="comment">#在进行矩阵相乘时需要float型，故强制类型转换为float型</span></span><br><span class="line">x_test = tf.cast(x_test,tf.float32) / <span class="number">255</span> <span class="comment">#在进行矩阵相乘时需要float型，故强制类型转换为float型</span></span><br><span class="line">train_iter = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)</span><br><span class="line">test_iter = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iter, net)</span>:</span></span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> _, (X, y) <span class="keyword">in</span> enumerate(data_iter):</span><br><span class="line">        y = tf.cast(y,dtype=tf.int64)</span><br><span class="line">        acc_sum += np.sum(tf.cast(tf.argmax(net(X), axis=<span class="number">1</span>), dtype=tf.int64) == y)</span><br><span class="line">        n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(net, train_iter, test_iter, loss, num_epochs, batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">              params=None, lr=None, trainer=None)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> sample_grads</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">                y_hat = net(X, is_training=<span class="literal">True</span>)</span><br><span class="line">                l = tf.reduce_sum(loss(y_hat, tf.one_hot(y, depth=<span class="number">10</span>, axis=<span class="number">-1</span>, dtype=tf.float32)))</span><br><span class="line">            </span><br><span class="line">            grads = tape.gradient(l, params)</span><br><span class="line">            <span class="keyword">if</span> trainer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                </span><br><span class="line">                sample_grads = grads</span><br><span class="line">                params[<span class="number">0</span>].assign_sub(grads[<span class="number">0</span>] * lr)</span><br><span class="line">                params[<span class="number">1</span>].assign_sub(grads[<span class="number">1</span>] * lr)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                trainer.apply_gradients(zip(grads, params))  <span class="comment"># “softmax回归的简洁实现”一节将用到</span></span><br><span class="line"></span><br><span class="line">            y = tf.cast(y, dtype=tf.float32)</span><br><span class="line">            train_l_sum += l.numpy()</span><br><span class="line">            train_acc_sum += tf.reduce_sum(tf.cast(tf.argmax(y_hat, axis=<span class="number">1</span>) == tf.cast(y, dtype=tf.int64), dtype=tf.int64)).numpy()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">num_epochs, lr, batch_size = <span class="number">5</span>, <span class="number">0.5</span>, <span class="number">256</span></span><br><span class="line">loss = tf.losses.CategoricalCrossentropy()</span><br><span class="line">train(net, train_iter, test_iter, loss, num_epochs, batch_size,</span><br><span class="line">              params, lr)</span><br></pre></td></tr></table></figure>
<h3 id="tensorflow-kuai-su-shi-xian-1">tensorflow快速实现</h3>
<p>在Tensorflow2.0中，只需要在全连接层后添加<code>Dropout</code>层并指定丢弃概率。在训练模型时，<code>Dropout</code>层将以指定的丢弃概率随机丢弃上一层的输出元素；在测试模型时（即<code>model.eval()</code>后），<code>Dropout</code>层并不发挥作用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = keras.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=(<span class="number">28</span>, <span class="number">28</span>)),</span><br><span class="line">    keras.layers.Dense(<span class="number">256</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    Dropout(<span class="number">0.2</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">256</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    Dropout(<span class="number">0.5</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">10</span>,activation=tf.nn.softmax)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=tf.keras.optimizers.Adam(),</span><br><span class="line">              loss = <span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(x_train,y_train,epochs=<span class="number">5</span>,batch_size=<span class="number">256</span>,validation_data=(x_test, y_test),</span><br><span class="line">                    validation_freq=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="shu-ju-zeng-qiang-ji-zhu">数据增强技术</h2>
<h1 id="xiao-jie">小结</h1>
<ul>
<li>正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。</li>
<li>权重衰减等价于\(L_2\)范数正则化，通常会使学到的权重参数的元素较接近0。</li>
<li>权重衰减可以通过优化器中的<code>weight_decay</code>超参数来指定。</li>
<li>可以定义多个优化器实例对不同的模型参数使用不同的迭代方法。</li>
<li>可以通过使用丢弃法应对过拟合。丢弃法只在训练模型时使用。</li>
<li></li>
</ul>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>逻辑回归与最大熵模型</title>
    <url>/2020/05/23/machine_learning/logistic_regression_and_maximum_entropy_mode/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/23/machine_learning/logistic_regression_and_maximum_entropy_mode/Logistic-Regression-learning.png" alt></p>
<a id="more"></a>
<h1 id="luo-ji-hui-gui">逻辑回归</h1>
<h2 id="shi-yao-shi-luo-ji-hui-gui">什么是逻辑回归</h2>
<p>逻辑回归是用来做分类算法的，线性回归，一般形式是\(y=w \ast x + b\) ，y的取值范围是\([- \infty, +\infty]\)，有这么多取值，把\(y\)的结果带入一个非线性变换的<strong>Sigmoid函数</strong>中，即可得到[0,1]之间取值范围的数\(S\)，\(S\)可以看成是一个概率值，如果设置概率阈值为0.5，那么\(S\)大于0.5可以看成是正样本，小于0.5看成是负样本，这样就可以进行分类了。</p>
<h2 id="shi-yao-shi-dui-shu-ji-lu">什么是对数几率</h2>
<p>如果某事件发生的概率是p,则该事件发生的<strong>几率</strong>(此处几率指该事件发生概率与不发生概率之比)是\(\frac{p}{1-p}\), <strong>对数几率</strong> 是\(log(\frac{p}{1-p})\),那么\(\log \frac{P(Y=1 | x)}{1-P(Y=1 | x)}=w \cdot x\)，也就是说在逻辑斯谛回归模型中,输出Y=1的对数几率是输入x的<strong>线性函数</strong>,线性函数值越接近正无穷,概率值就越接近1,反之则越接近0.</p>
<h2 id="shi-yao-shi-sigmoid-han-shu">什么是Sigmoid函数</h2>
<p>公式及图像如下：</p>
<p><img src="/2020/05/23/machine_learning/logistic_regression_and_maximum_entropy_mode/00630Defly1g4pvk2ctatj30cw0b63yq.jpg" alt></p>
<p>函数中\(t\)无论取什么值，其结果都在\([0,1]\)的区间内，一个分类问题就有两种答案，一种是“是”，一种是“否”，那0对应着“否”，1对应着“是”，假设分类的<strong>阈值</strong>是0.5，那么超过0.5的归为1分类，低于0.5的归为0分类，阈值是可以自己设定的。把\(w \ast x + b\)带入t中就得到了逻辑回归的一般模型方程：</p>
<p>\[
H_{w,b}(x)=\frac{1}{1+e^{(w \ast x+b)}}
\]<br>
结果可以理解为概率，概率大于0.5的属于1分类，概率小于0.5的属于0分类，这样就达到了分类的目的。</p>
<h2 id="sun-shi-han-shu-shi-shi-yao">损失函数是什么</h2>
<p>逻辑回归的损失函数是 <strong>log loss</strong>，也就是<strong>对数似然函数</strong>，函数公式如下：</p>
<p>\[
\operatorname{cost}\left(h_{\theta}(x), y\right)=\left\{\begin{aligned}
-\log \left(h_{\theta}(x)\right) &amp; \text { if } y=1 \\
-\log \left(1-h_{\theta}(x)\right) &amp; \text { if } y=0
\end{aligned}\right.
\]</p>
<p>公式中的 \(y=1\) 表示的是真实值为1时用第一个公式，真实 \(y=0\) 用第二个公式计算损失。当真实样本为\(y=1\)时，但\(h=0\)概率，那么\(log0=\infty\)，这就对模型最大的惩罚力度；当真实样本\(y=1\)，并且\(h=1\)时，那么\(log1=0\)，相当于没有惩罚，也就是没有损失，达到最优结果。</p>
<p>上面公式也可以表示成：<br>
\[
\begin{aligned} L(w) &amp;=\sum_{i=1}^{N}\left[y_{i} \log \pi\left(x_{i}\right)+\left(1-y_{i}\right)\log\left(1-\pi\left(x_{i}\right)\right)\right]\\ &amp;=\sum_{i=1}^{N}\left[y_{i} \log \frac{\pi\left(x_{i}\right)}{1-\pi\left(x_{i}\right)}+\log \left(1-\pi\left(x_{i}\right)\right)\right]\\&amp;=\sum_{i=1}^{N}\left[y_{i}\left(w \cdot x_{i}\right)-\log \left(1+\exp \left(w \cdot x_{i}\right)\right]\right.\end{aligned}
\]</p>
<p>最后按照梯度下降法一样，求解极小值点，得到想要的模型效果。</p>
<h2 id="ru-he-jin-xing-duo-xiang-luo-ji-hui-gui">如何进行多项逻辑回归</h2>
<p>当问题是多分类问题时,可以作如下推广:设Y有K类可能取值,\(P(Y=k | x)=\frac{\exp \left(w_{k} \cdot x\right)}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}, \quad k=1,2, \cdots, K-1 \quad P(Y=K | x)=\frac{1}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}\),实际上就是<strong>one-vs-all</strong>的思想,将其他所有类当作一个类,问题转换为二分类问题.</p>
<p>使用最大似然法衡量模型输出的概率与真实概率的差别，假设样本一共有N个，那么这组样本发生的总概率可以表示为：<br>
\[
P(\boldsymbol{W})=\prod_{n=1}^{N}\left(\frac{e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}}{\sum_{k^{\prime}}^{C} e^{\boldsymbol{w}_{k^{\prime}}^{T} \boldsymbol{x}_{n}}}\right)
\]<br>
对函数取对数再乘以-1，推导得到：<br>
\[
\begin{aligned}
F(\boldsymbol{W})=-\ln (P(\boldsymbol{W})) &amp;=\sum_{n=1}^{N} \ln \left(\frac{\sum_{k^{\prime}}^{C} e^{\boldsymbol{w}_{k^{\prime}}^{T} \boldsymbol{x}_{n}}}{e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}}\right) \\
&amp;=\sum_{n=1}^{N} \ln \left(\frac{e^{\boldsymbol{w}_{1}^{T} \boldsymbol{x}}+e^{\boldsymbol{w}_{2}^{T} \boldsymbol{x}}+\ldots+e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}+\ldots+e^{\boldsymbol{w}_{c}^{T} \boldsymbol{x}}}{e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}}\right) \\
&amp;=\sum_{n=1}^{N} \ln \left(1+\sum_{k \neq y_{n}} e^{\boldsymbol{w}_{k} \boldsymbol{x}_{n}-\boldsymbol{w}_{y_{n}} \boldsymbol{x}_{n}}\right)
\end{aligned}
\]</p>
<h2 id="luo-ji-hui-gui-you-shi-yao-you-dian">逻辑回归有什么优点</h2>
<ul>
<li>LR能以概率的形式输出结果，而非只是0,1判定。</li>
<li>LR的可解释性强，可控度高(你要给老板讲的嘛…)。</li>
<li>训练快，feature engineering之后效果赞。</li>
<li>因为结果是概率，可以做ranking model。</li>
</ul>
<h2 id="luo-ji-hui-gui-chang-yong-de-you-hua-fang-fa-you-na-xie">逻辑回归常用的优化方法有哪些</h2>
<h3 id="yi-jie-fang-fa-ti-du-xia-jiang">一阶方法：梯度下降</h3>
<p>梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。</p>
<h3 id="er-jie-fang-fa-niu-dun-fa-ni-niu-dun-fa">二阶方法：牛顿法、拟牛顿法</h3>
<p>牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。</p>
<p>实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。</p>
<p>缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。</p>
<p>拟牛顿法： 不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。</p>
<h2 id="luo-ji-si-te-hui-gui-wei-shi-yao-yao-dui-te-zheng-jin-xing-chi-san-hua">逻辑斯特回归为什么要对特征进行离散化</h2>
<ol>
<li>非线性！逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代；</li>
<li>速度快！稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</li>
<li>鲁棒性！离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</li>
<li>方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</li>
<li>稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；</li>
<li>简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</li>
</ol>
<h2 id="luo-ji-hui-gui-de-mu-biao-han-shu-zhong-zeng-da-l-1-zheng-ze-hua-hui-shi-shi-yao-jie-guo">逻辑回归的目标函数中增大L1正则化会是什么结果</h2>
<p>所有的参数w都会变成0。</p>
<h1 id="zui-da-shang">最大熵</h1>
<h2 id="strong-zui-da-shang-yuan-li-strong"><strong>最大熵原理</strong></h2>
<p>学习概率模型时,在所有可能的概率模型中,<strong>熵最大</strong>的模型是最好的模型.直观地,最大熵原理认为模型首先要满足已有的事实,即<strong>约束条件</strong>.在没有更多信息的情况下,那些不确定的部分都是&quot;<strong>等可能的</strong>&quot;.</p>
<h2 id="strong-zui-da-shang-mo-xing-strong"><strong>最大熵模型</strong></h2>
<p>给定训练数据集,可以确定联合分布P(X,Y)的经验分布\(\tilde{P}(X=x, Y=y)=\frac{v(X=x, Y=y)}{N}\)和边缘分布P(X)的经验分布\(\tilde{P}(X=x)=\frac{v(X=x)}{N}\),其中v表示频数,N表示样本容量.用<strong>特征函数\(f(x,y)\)</strong>=1描述x与y满足某一事实,可以得到特征函数关于P(X,Y)的经验分布的期望值和关于模型P(Y|X)与P(X)的经验分布的期望值,假设两者相等,就得到了<strong>约束条件</strong>\(\sum_{x, y} \tilde{P}(x) P(y | x) f(x, y)=\sum_{x, y} \tilde{P}(x, y) f(x, y)\).定义在条件概率分布P(Y|X)上的条件熵为\(H(P)=-\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)\),则<strong>条件熵最大</strong>的模型称为最大熵模型.</p>
<h2 id="strong-zui-da-shang-mo-xing-de-xue-xi-strong"><strong>最大熵模型的学习</strong></h2>
<p>就是求解最大熵模型的过程.等价于<strong>约束最优化问题</strong><br>
\[
\begin{aligned}
&amp;\max _{P_{e c}} H(P)=-\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)\\
&amp;\text { s.t. } \quad E_{p}\left(f_{i}\right)=E_{p}\left(f_{i}\right), \quad i=1,2, \cdots, n\\
     &amp;\sum_{y} P(y | x)=1
\end{aligned}
\]<br>
,将求最大值问题改为等价的求最小值问题<br>
\[
\begin{aligned}
&amp;\min _{R \in C}-H(P)=\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)\\
&amp;\text { s.t. } \quad E_{P}\left(f_{i}\right)-E_{\beta}\left(f_{i}\right)=0, \quad i=1,2, \cdots, n\\
&amp;\sum_{y} P(y | x)=1
\end{aligned}
\]</p>
<p>引入<strong>拉格朗日乘子</strong><br>
\[
\begin{aligned}
L(P, w) &amp; \equiv-H(P)+w_{0}\left(1-\sum_{y} P(y | x)\right)+\sum_{i=1}^{n} w_{i}\left(E_{p}\left(f_{i}\right)-E_{P}\left(f_{i}\right)\right) \\
&amp;=\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)+w_{0}\left(1-\sum_{y} P(y | x)\right) \\
&amp;+\sum_{i=1}^{n} w_{i}\left(\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P(y | x) f_{i}(x, y)\right)
\end{aligned}
\]<br>
将原始问题\(\min _{p \in C} \max _{w} L(P, w)\)转换为无约束最优化的<strong>对偶问题</strong>\(\max _{w} \min _{P \in \mathbf{C}} L(P, w)\).首先求解内部的<strong>极小化问题</strong>,即求\(L(P,W)\)对\(P(y|x)\)的偏导数.<br>
\[
\begin{aligned}
\frac{\partial L(P, w)}{\partial P(y | x)} &amp;=\sum_{x, y} \tilde{P}(x)(\log P(y | x)+1)-\sum_{y} w_{0}-\sum_{x, y}\left(\tilde{P}(x) \sum_{i=1}^{n} w_{i} f_{i}(x, y)\right) \\
&amp;=\sum_{x, y} \tilde{P}(x)\left(\log P(y | x)+1-w_{0}-\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
\end{aligned}
\]<br>
,并令偏导数等于0,解得\(Z_{w}(x)=\sum_{y} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)\).可以证明对偶函数等价于对数似然函数,那么对偶函数极大化等价于最大熵模型的<strong>极大似然估计</strong>\(L(w)=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)\).之后可以用最优化算法求解得到w.</p>
<h2 id="luo-ji-hui-gui-yu-zui-da-shang-mo-xing-de-gong-tong-dian">逻辑回归与最大熵模型的共同点</h2>
<p>最大熵模型与逻辑斯谛回归模型有类似的形式,它们又称为<strong>对数线性模型</strong>.模型学习就是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计.</p>
<h2 id="you-hua-suan-fa">优化算法</h2>
<p>似然函数是<strong>光滑的凸函数</strong>,因此多种最优化方法都适用.</p>
<ol>
<li><strong>改进的迭代尺度法(IIS)</strong>:假设当前的参数向量是w,如果能找到一种方法<strong>w-&gt;w+δ</strong>使对数似然函数值变大,就可以<strong>重复</strong>使用这一方法,直到找到最大值.</li>
<li>逻辑斯谛回归常应用梯度下降法,牛顿法或拟牛顿法.</li>
</ol>
<h1 id="softmax-duo-fen-lei">softmax多分类</h1>
<p>softmax回归的输出单元从一个变成了多个，且引入了softmax运算使输出更适合离散值的预测和训练。</p>
<h2 id="fen-lei-wen-ti">分类问题</h2>
<p>让我们考虑一个简单的图像分类问题，其输入图像的高和宽均为2像素，且色彩为灰度。这样每个像素值都可以用一个标量表示。我们将图像中的4像素分别记为\(x_1, x_2, x_3, x_4\)。假设训练数据集中图像的真实标签为狗、猫或鸡，这些标签分别对应离散值\(y_1, y_2, y_3\)。使用离散的数值来表示类别，例如\(y_1=1, y_2=2, y_3=3\)。如此，一张图像的标签为1、2和3这3个数值中的一个。虽然我们仍然可以使用回归模型来进行建模，并将预测值就近定点化到1、2和3这3个离散值之一，但这种连续值到离散值的转化通常会影响到分类质量。因此一般使用更加适合离散值输出的模型来解决分类问题。</p>
<h2 id="softmax-hui-gui-mo-xing">softmax回归模型</h2>
<p>softmax回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，softmax回归的输出值个数等于标签里的类别数。因为一共有4种特征和3种输出动物类别，所以权重包含12个标量、偏差包含3个标量，且对每个输入计算\(o_1, o_2, o_3\)这3个输出：</p>
<p>\[
\begin{aligned}
o_1 &amp;= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1,\\
o_2 &amp;= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2,\\
o_3 &amp;= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3.
\end{aligned}
\]</p>
<p>下图用神经网络图描绘了上面的计算。softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出\(o_1, o_2, o_3\)的计算都要依赖于所有的输入\(x_1, x_2, x_3, x_4\)，softmax回归的输出层也是一个全连接层。</p>
<p><img src="/2020/05/23/machine_learning/logistic_regression_and_maximum_entropy_mode/3.4_softmaxreg.svg" alt></p>
<p>既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值\(o_i\)当作预测类别是\(i\)的置信度，并将值最大的输出所对应的类作为预测输出，即输出 \(\underset{i}{\arg\max} o_i\)。例如，如果\(o_1,o_2,o_3\)分别为\(0.1,10,0.1\)，由于\(o_2\)最大，那么预测类别为2，其代表猫。</p>
<p>然而，直接使用输出层的输出有两个问题。</p>
<ol>
<li>一方面，由于输出层的输出值的范围不确定，难以直观上判断这些值的意义。例如，刚才举的例子中的输出值10表示“很置信”图像类别为猫，因为该输出值是其他两类的输出值的100倍。但如果\(o_1=o_3=10^3\)，那么输出值10却又表示图像类别为猫的概率很低。</li>
<li>另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。</li>
</ol>
<p>softmax运算符解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布：</p>
<p>\[
\hat{y}_1, \hat{y}_2, \hat{y}_3 = \text{softmax}(o_1, o_2, o_3)
\]</p>
<p>其中</p>
<p>\[
\hat{y}_1 = \frac{ \exp(o_1)}{\sum_{i=1}^3 \exp(o_i)},\quad
\hat{y}_2 = \frac{ \exp(o_2)}{\sum_{i=1}^3 \exp(o_i)},\quad
\hat{y}_3 = \frac{ \exp(o_3)}{\sum_{i=1}^3 \exp(o_i)}.
\]</p>
<p>容易看出\(\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1\)且\(0 \leq \hat{y}_1, \hat{y}_2, \hat{y}_3 \leq 1\)，因此\(\hat{y}_1, \hat{y}_2, \hat{y}_3\)是一个合法的概率分布。这时候，如果\(\hat{y}_2=0.8\)，不管\(\hat{y}_1\)和\(\hat{y}_3\)的值是多少，我们都知道图像类别为猫的概率是80%。此外，注意到</p>
<p>\[
\underset{i}{\arg\max} o_i = \underset{i}{\arg\max} \hat{y}_i
\]</p>
<p>因此softmax运算不改变预测类别输出。</p>
<h2 id="dan-yang-ben-fen-lei-de-shi-liang-ji-suan-biao-da-shi">单样本分类的矢量计算表达式</h2>
<p>为了提高计算效率，可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为</p>
<p>\[
\boldsymbol{W} = 
\begin{bmatrix}
    w_{11} &amp; w_{12} &amp; w_{13} \\
    w_{21} &amp; w_{22} &amp; w_{23} \\
    w_{31} &amp; w_{32} &amp; w_{33} \\
    w_{41} &amp; w_{42} &amp; w_{43}
\end{bmatrix},\quad
\boldsymbol{b} = 
\begin{bmatrix}
    b_1 &amp; b_2 &amp; b_3
\end{bmatrix},
\]</p>
<p>设高和宽分别为2个像素的图像样本\(i\)的特征为<br>
\[
\boldsymbol{x}^{(i)} = \begin{bmatrix}x_1^{(i)} &amp; x_2^{(i)} &amp; x_3^{(i)} &amp; x_4^{(i)}\end{bmatrix},
\]</p>
<p>输出层的输出为<br>
\[
\boldsymbol{o}^{(i)} = \begin{bmatrix}o_1^{(i)} &amp; o_2^{(i)} &amp; o_3^{(i)}\end{bmatrix},
\]</p>
<p>预测为狗、猫或鸡的概率分布为<br>
\[
\boldsymbol{\hat{y}}^{(i)} = \begin{bmatrix}\hat{y}_1^{(i)} &amp; \hat{y}_2^{(i)} &amp; \hat{y}_3^{(i)}\end{bmatrix}.
\]</p>
<p>softmax回归对样本\(i\)分类的矢量计算表达式为</p>
<p>\[
\begin{aligned}
\boldsymbol{o}^{(i)} &amp;= \boldsymbol{x}^{(i)} \boldsymbol{W} + \boldsymbol{b},\\
\boldsymbol{\hat{y}}^{(i)} &amp;= \text{softmax}(\boldsymbol{o}^{(i)}).
\end{aligned}
\]</p>
<h2 id="xiao-pi-liang-yang-ben-fen-lei-de-shi-liang-ji-suan-biao-da-shi">小批量样本分类的矢量计算表达式</h2>
<p>为了进一步提升计算效率，对小批量数据做矢量计算。广义上讲，给定一个小批量样本，其批量大小为\(n\)，输入个数（特征数）为\(d\)，输出个数（类别数）为\(q\)。设批量特征为\(\boldsymbol{X} \in \mathbb{R}^{n \times d}\)。假设softmax回归的权重和偏差参数分别为\(\boldsymbol{W} \in \mathbb{R}^{d \times q}\)和\(\boldsymbol{b} \in \mathbb{R}^{1 \times q}\)。softmax回归的矢量计算表达式为</p>
<p>\[
\begin{aligned}
\boldsymbol{O} &amp;= \boldsymbol{X} \boldsymbol{W} + \boldsymbol{b},\\
\boldsymbol{\hat{Y}} &amp;= \text{softmax}(\boldsymbol{O}),
\end{aligned}
\]</p>
<p>其中的加法运算使用了广播机制，\(\boldsymbol{O}, \boldsymbol{\hat{Y}} \in \mathbb{R}^{n \times q}\)且这两个矩阵的第\(i\)行分别为样本\(i\)的输出\(\boldsymbol{o}^{(i)}\)和概率分布\(\boldsymbol{\hat{y}}^{(i)}\)。</p>
<h2 id="jiao-cha-shang-sun-shi-han-shu">交叉熵损失函数</h2>
<p>使用softmax运算后可以更方便地与离散标签计算误差。softmax运算将输出变换成一个合法的类别预测分布。实际上，真实标签也可以用类别分布表达：对于样本\(i\)，构造向量\(\boldsymbol{y}^{(i)}\in \mathbb{R}^{q}\) ，使其第\(y^{(i)}\)个元素为1，其余为0。这样我们的训练目标可以设为使预测概率分布\(\boldsymbol{\hat y}^{(i)}\)尽可能接近真实的标签概率分布\(\boldsymbol{y}^{(i)}\)。</p>
<p>可以像线性回归那样使用平方损失函数\(\|\boldsymbol{\hat y}^{(i)}-\boldsymbol{y}^{(i)}\|^2/2\)。然而，想要预测分类结果正确，并不需要预测概率完全等于标签概率。例如，在图像分类的例子里，如果\(y^{(i)}=3\)，那么我们只需要\(\hat{y}^{(i)}_3\)比其他两个预测值\(\hat{y}^{(i)}_1\)和\(\hat{y}^{(i)}_2\)大就行了。即使\(\hat{y}^{(i)}_3\)值为0.6，不管其他两个预测值为多少，类别预测均正确。而平方损失则过于严格，例如\(\hat y^{(i)}_1=\hat y^{(i)}_2=0.2\)比\(\hat y^{(i)}_1=0, \hat y^{(i)}_2=0.4\)的损失要小很多，虽然两者都有同样正确的分类预测结果。</p>
<p>改善上述问题的一个方法是使用更适合<strong>衡量两个概率分布差异</strong>的测量函数。其中，交叉熵是一个常用的衡量方法：</p>
<p>\[
H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)}
\]</p>
<p>其中带下标的\(y_j^{(i)}\)是向量\(\boldsymbol y^{(i)}\)中非0即1的元素。在上式中，假设向量\(\boldsymbol y^{(i)}\)中只有第\(j\)个元素\(y^{(i)}_{j}\)为1，其余全为0，于是\(H(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}) = -\log \hat y_{j}^{(i)}\)。也就是说，交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。当然，遇到一个样本有多个标签时，例如图像里含有不止一个物体时，我们并不能做这一步简化。但即便对于这种情况，交叉熵同样只关心对图像中出现的物体类别的预测概率。</p>
<p>假设训练数据集的样本数为\(n\)，交叉熵损失函数定义为<br>
\[
\ell(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right )
\]</p>
<p>其中\(\boldsymbol{\Theta}\)代表模型参数。同样地，如果每个样本只有一个标签，那么交叉熵损失可以简写成\(\ell(\boldsymbol{\Theta}) = -(1/n)  \sum_{i=1}^n \log \hat y_{j}^{(i)}\)。从另一个角度来看，我们知道最小化\(\ell(\boldsymbol{\Theta})\)等价于最大化\(\exp(-n\ell(\boldsymbol{\Theta}))=\prod_{i=1}^n \hat y_{j}^{(i)}\)，即最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。</p>
<p>优点：</p>
<ol>
<li>它是<strong>非负</strong>的,并且当实际输出接近目标值时它<strong>接近0</strong>,因此可以作为代价函数.</li>
<li>它关于权重的<strong>偏导数</strong>是\(\frac{\partial C}{\partial w_{j}}=\frac{1}{n} \sum_{x} x_{j}(\sigma(z)-y)\),也就是误差越大,学习速度越快.而<strong>二次代价函数</strong>关于权重和偏置的<strong>偏导数</strong>是\(\frac{\partial C}{\partial w}=(a-y) \sigma^{\prime}(z) x=a \sigma^{\prime}(z)\)和\(\frac{\partial C}{\partial b}=(a-y) \sigma^{\prime}(z)=a \sigma^{\prime}(z)\)，而\(\sigma\)函数容易饱和，梯度小，导致模型难以快速学习参数。</li>
<li>如果输出层是<strong>线性神经元</strong>,那么<strong>二次代价函数</strong>不再会导致学习速度下降的问题,可以选用.如果输出神经元是<strong>S型神经元</strong>,<strong>交叉熵</strong>一般都是更好的选择.</li>
<li>对于多分类交叉熵代价函数为：\(C=-\sum_{j=1}^{T} y_{j} \log {p_j}\)</li>
</ol>
<blockquote>
<p>二分类问题<strong>交叉熵代价函数</strong>:\(C=-\frac{1}{n} \sum_{x}[y \ln a+(1-y) \ln (1-a)]\)</p>
</blockquote>
<h2 id="dai-ma-shi-xian">代码实现</h2>
<h3 id="shu-ju">数据</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> fashion_mnist</span><br><span class="line"></span><br><span class="line">batch_size=<span class="number">256</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()</span><br><span class="line">x_train = tf.cast(x_train, tf.float32) / <span class="number">255</span> <span class="comment">#在进行矩阵相乘时需要float型，故强制类型转换为float型</span></span><br><span class="line">x_test = tf.cast(x_test,tf.float32) / <span class="number">255</span> <span class="comment">#在进行矩阵相乘时需要float型，故强制类型转换为float型</span></span><br><span class="line">train_iter = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)</span><br><span class="line">test_iter = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)</span><br></pre></td></tr></table></figure>
<h3 id="chu-shi-hua-mo-xing-can-shu">初始化模型参数</h3>
<p>使用向量表示每个样本。已知每个样本输入是高和宽均为28像素的图像。模型的输入向量的长度是 \(28 \times 28 = 784\)：该向量的每个元素对应图像中每个像素。由于图像有10个类别，单层神经网络输出层的输出个数为10，因此softmax回归的权重和偏差参数分别为\(784 \times 10\)和\(1 \times 10\)的矩阵。<code>Variable</code>来标注需要记录梯度的向量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line">W = tf.Variable(tf.random.normal(shape=(num_inputs, num_outputs), mean=<span class="number">0</span>, stddev=<span class="number">0.01</span>, dtype=tf.float32))</span><br><span class="line">b = tf.Variable(tf.zeros(num_outputs, dtype=tf.float32))</span><br></pre></td></tr></table></figure>
<h3 id="shi-xian-softmax-yun-suan">实现 softmax 运算</h3>
<p>在下面的函数中，矩阵<code>logits</code>的行数是样本数，列数是输出个数。为了表达样本预测各个输出的概率，softmax运算先通过<code>exp</code>函数对每个元素做指数运算，再对<code>exp</code>矩阵同行元素求和，最后令矩阵每行各元素与该行元素之和相除。这样一来，最终得到的矩阵每行元素和为1且非负，该矩阵每行都是合法的概率分布。softmax运算的输出矩阵中的任意一行元素代表了一个样本在各个输出类别上的预测概率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(logits, axis=<span class="number">-1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.exp(logits)/tf.reduce_sum(tf.exp(logits), axis, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>给定一个<code>Tensor</code>矩阵<code>X</code>。只对其中同一列（<code>axis=0</code>）或同一行（<code>axis=1</code>）的元素求和，并在结果中保留行和列这两个维度（<code>keepdims=True</code>）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">tf.reduce_sum(X, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">tf.reduce_sum(X, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">462401</span>, shape=(<span class="number">1</span>, <span class="number">3</span>), dtype=int32, numpy=array([[<span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]], dtype=int32)&gt;</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">462403</span>, shape=(<span class="number">2</span>, <span class="number">1</span>), dtype=int32, numpy=array([[ <span class="number">6</span>],[<span class="number">15</span>]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="ding-yi-mo-xing-ji-sun-shi-han-shu">定义模型及损失函数</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    logits = tf.matmul(tf.reshape(X, shape=(<span class="number">-1</span>, W.shape[<span class="number">0</span>])), W) + b</span><br><span class="line">    <span class="keyword">return</span> softmax(logits)</span><br></pre></td></tr></table></figure>
<p>下面实现了softmax回归中的交叉熵损失函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    y = tf.cast(tf.reshape(y, shape=[<span class="number">-1</span>, <span class="number">1</span>]),dtype=tf.int32)</span><br><span class="line">    y = tf.one_hot(y, depth=y_hat.shape[<span class="number">-1</span>])</span><br><span class="line">    y = tf.cast(tf.reshape(y, shape=[<span class="number">-1</span>, y_hat.shape[<span class="number">-1</span>]]),dtype=tf.int32)</span><br><span class="line">    <span class="keyword">return</span> -tf.math.log(tf.boolean_mask(y_hat, y)+<span class="number">1e-8</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>为了得到标签的预测概率，可以使用<code>boolean_mask</code>函数和<code>one_hot</code>函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_hat = np.array([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.6</span>], [<span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]])</span><br><span class="line">y = np.array([<span class="number">0</span>, <span class="number">2</span>], dtype=<span class="string">'int32'</span>)</span><br><span class="line">tf.boolean_mask(y_hat, tf.one_hot(y, depth=<span class="number">3</span>))</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">462449</span>, shape=(<span class="number">2</span>,), dtype=float64, numpy=array([<span class="number">0.1</span>, <span class="number">0.5</span>])&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="ji-suan-fen-lei-zhun-que-lu">计算分类准确率</h3>
<p>下面定义准确率<code>accuracy</code>函数。其中<code>tf.argmax(y_hat, axis=1)</code>返回矩阵<code>y_hat</code>每行中最大元素的索引，且返回结果与变量<code>y</code>形状相同。相等条件判断式<code>(tf.argmax(y_hat, axis=1) == y)</code>是一个数据类型为<code>bool</code>的<code>Tensor</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.mean((tf.argmax(y_hat, axis=<span class="number">1</span>) == y))</span><br></pre></td></tr></table></figure>
<p>评价模型<code>net</code>在数据集<code>data_iter</code>上的准确率:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 描述,对于tensorflow2中，比较的双方必须类型都是int型，所以要将输出和标签都转为int型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iter, net)</span>:</span></span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> _, (X, y) <span class="keyword">in</span> enumerate(data_iter):</span><br><span class="line">        y = tf.cast(y,dtype=tf.int64)</span><br><span class="line">        acc_sum += np.sum(tf.cast(tf.argmax(net(X), axis=<span class="number">1</span>), dtype=tf.int64) == y)</span><br><span class="line">        n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br></pre></td></tr></table></figure>
<h3 id="xun-lian-mo-xing-ji-yu-ce">训练模型及预测</h3>
<p>使用小批量随机梯度下降来优化模型的损失函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">0.1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None, lr=None, trainer=None)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">                y_hat = net(X)</span><br><span class="line">                l = tf.reduce_sum(loss(y_hat, y))</span><br><span class="line">            grads = tape.gradient(l, params)</span><br><span class="line">            <span class="keyword">if</span> trainer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 如果没有传入优化器，则编写的小批量随机梯度下降</span></span><br><span class="line">                <span class="keyword">for</span> i, param <span class="keyword">in</span> enumerate(params):</span><br><span class="line">                    param.assign_sub(lr * grads[i] / batch_size)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># tf.keras.optimizers.SGD 直接使用是随机梯度下降 theta(t+1) = theta(t) - learning_rate * gradient</span></span><br><span class="line">                <span class="comment"># 这里使用批量梯度下降，需要对梯度除以 batch_size, </span></span><br><span class="line">                trainer.apply_gradients(zip([grad / batch_size <span class="keyword">for</span> grad <span class="keyword">in</span> grads], params))  </span><br><span class="line">                </span><br><span class="line">            y = tf.cast(y, dtype=tf.float32)</span><br><span class="line">            train_l_sum += l.numpy()</span><br><span class="line">            train_acc_sum += tf.reduce_sum(tf.cast(tf.argmax(y_hat, axis=<span class="number">1</span>) == tf.cast(y, dtype=tf.int64), dtype=tf.int64)).numpy()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span>% (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br><span class="line"></span><br><span class="line">trainer = tf.keras.optimizers.SGD(lr)</span><br><span class="line">train(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.8969</span>, train acc <span class="number">0.736</span>, test acc <span class="number">0.813</span></span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.5987</span>, train acc <span class="number">0.806</span>, test acc <span class="number">0.826</span></span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.5524</span>, train acc <span class="number">0.820</span>, test acc <span class="number">0.832</span></span><br><span class="line">epoch <span class="number">4</span>, loss <span class="number">0.5297</span>, train acc <span class="number">0.826</span>, test acc <span class="number">0.834</span></span><br><span class="line">epoch <span class="number">5</span>, loss <span class="number">0.5139</span>, train acc <span class="number">0.830</span>, test acc <span class="number">0.836</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">predict_label = tf.argmax(net(X)</span><br></pre></td></tr></table></figure>
<h3 id="tensorflow-kuai-su-shi-xian-ban-ben">tensorflow 快速实现版本</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="comment"># 数据</span></span><br><span class="line">fashion_mnist = keras.datasets.fashion_mnist</span><br><span class="line">(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()</span><br><span class="line">x_train = x_train / <span class="number">255.0</span></span><br><span class="line">x_test = x_test / <span class="number">255.0</span></span><br><span class="line"><span class="comment"># 模型</span></span><br><span class="line">model = keras.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=(<span class="number">28</span>, <span class="number">28</span>)),</span><br><span class="line">    keras.layers.Dense(<span class="number">10</span>, activation=tf.nn.softmax)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失</span></span><br><span class="line">loss = <span class="string">'sparse_categorical_crossentropy'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">model.compile(optimizer=tf.keras.optimizers.SGD(<span class="number">0.1</span>),</span><br><span class="line">              loss = <span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(x_train,y_train,epochs=<span class="number">5</span>,batch_size=<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估</span></span><br><span class="line">test_loss, test_acc = model.evaluate(x_test, y_test)</span><br><span class="line">print(<span class="string">'Test Acc:'</span>,test_acc)</span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">predict_labels = tf.argmax(model.predict(x_test),axis=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="xiao-jie">小结</h2>
<ul>
<li>softmax回归适用于分类问题。它使用softmax运算输出类别的概率分布。</li>
<li>softmax回归是一个单层神经网络，输出个数等于分类问题中的类别个数。</li>
<li><strong>交叉熵适合衡量两个概率分布的差异</strong>。</li>
</ul>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>logistic</tag>
      </tags>
  </entry>
  <entry>
    <title>线性回归</title>
    <url>/2020/05/23/machine_learning/linear_regression/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/23/machine_learning/linear_regression/71505f8a532d4d5e9b259634ff1c99c3.jpeg" alt></p>
<a id="more"></a>
<h1 id="shi-yao-shi-xian-xing-hui-gui">什么是线性回归</h1>
<ul>
<li>线性：两个变量之间的关系是一次函数关系的——图象<strong>是直线</strong>，叫做线性。</li>
<li>非线性：两个变量之间的关系不是一次函数关系的——图象<strong>不是直线</strong>，叫做非线性。</li>
<li>回归：人们在测量事物的时候因为客观条件所限，求得的都是测量值，而不是事物真实的值，为了能够得到真实值，无限次的进行测量，最后通过这些测量数据计算<strong>回归到真实值</strong>，这就是回归的由来。</li>
</ul>
<h2 id="neng-gou-jie-jue-shi-yao-yang-de-wen-ti">能够解决什么样的问题</h2>
<p>线性回归输出是一个连续值，适用于回归问题。通过对大量的观测数据进行处理，从而得到比较符合事物内部规律的数学表达式。寻找到数据与数据之间的规律所在，从而就可以模拟出结果，也就是对结果进行预测。解决的就是通过已知的数据得到未知的结果。例如：对房价的预测、判断信用评价、电影票房预估等。</p>
<h2 id="biao-da-shi-shi-shi-yao">表达式是什么</h2>
<p>\[
Y=wx+b
\]<br>
\(w\)叫做\(x\)的系数，\(b\)叫做偏置项。</p>
<h2 id="ru-he-ji-suan">如何计算</h2>
<h3 id="sun-shi-han-shu">损失函数</h3>
<p>在模型训练中，需要衡量预测值与真实值之间的误差。通常会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。它在评估索引为 \(i\) 的样本误差的表达式为</p>
<p>\[
\ell^{(i)}(W, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2
\]</p>
<p>其中常数 \(\frac 1 2\) 使对平方项求导后的常数系数为1，这样在形式上稍微简单一些。显然，误差越小表示预测价格与真实价格越相近，且当二者相等时误差为0。给定训练数据集，这个误差只与模型参数相关，因此我们将它记为以模型参数为参数的函数。通常，用训练数据集中所有样本误差的<strong>平均</strong>来衡量模型预测的质量，即</p>
<p>\[
\ell(W, b) =\frac{1}{n} \sum_{i=1}^n \ell^{(i)}(W, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(X^{(i)} W +  b - y^{(i)}\right)^2
\]</p>
<p>在模型训练中，利用梯度下降法一组模型参数，记为 \(W^*, b^*\)，来使训练样本平均损失最小：</p>
<p>\[
W^*, b^* = \underset{W, b}{\arg\min} \ell(W, b)
\]</p>
<h3 id="you-hua-suan-fa">优化算法</h3>
<p>当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作<strong>解析解</strong>。线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作<strong>数值解</strong>。</p>
<p>在求数值解的优化算法中，小批量随机梯度下降在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量\(\mathcal{B}\)，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。以\(\hat{y}^{(i)} = x_1^{(i)} w_1 + x_2^{(i)} w_2 + b\)为例在训练的线性回归模型的过程中，模型的每个参数将作如下迭代：<br>
\[
\begin{aligned}
w_1 &amp;\leftarrow w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial w_1} = w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\
w_2 &amp;\leftarrow w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial w_2} = w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\
b &amp;\leftarrow b -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial b} = b -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right).
\end{aligned}
\]</p>
<p>在上式中，\(|\mathcal{B}|\) 代表每个小批量中的样本个数<code>(batch size)</code>，\(\eta\) 称作学习率<code>(learning rate)</code>并取正数。需要强调的是，这里的批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作超参数。</p>
<h1 id="guo-ni-he-qian-ni-he-ru-he-jie-jue">过拟合、欠拟合如何解决</h1>
<p>使用正则化项，也就是给loss function加上一个参数项，正则化项有<strong>L1正则化、L2正则化、ElasticNet</strong>。加入这个正则化项好处：</p>
<ul>
<li>控制参数幅度，不让模型“无法无天”。</li>
<li>限制参数搜索空间</li>
<li>解决欠拟合与过拟合的问题。</li>
</ul>
<h2 id="shi-yao-shi-l-2-zheng-ze-hua-ling-hui-gui">什么是L2正则化(岭回归)</h2>
<p>方程：<br>
\[
J=J_0+\lambda\sum_{w}w^2
\]<br>
\(J_0\)表示上面的 loss function ，在loss function的基础上加入\(w\)参数的平方和乘以 \(\lambda\) ，假设：<br>
\[
L=\lambda({w_1}^2 + {w_2}^2)
\]<br>
回忆以前学过的单位元的方程：<br>
\[
x^2+y^2=1
\]<br>
和L2正则化项一样，此时的任务变成在\(L\)约束下求出\(J\)取最小值的解。求解\(J_0\)的过程可以画出等值线。同时L2正则化的函数L也可以在\(w_1w_2\)的二维平面上画出来。如下图：</p>
<p><img src="/2020/05/23/machine_learning/linear_regression/00630Defgy1g4ns9qha1nj308u089aav.jpg" alt></p>
<p>L表示为图中的黑色圆形，随着梯度下降法的不断逼近，与圆第一次产生交点，而这个交点很难出现在坐标轴上。这就说明了L2正则化不容易得到稀疏矩阵，同时为了求出损失函数的最小值，使得\(w_1\)和\(w_2\)无限接近于0，达到防止过拟合的问题。</p>
<h2 id="shi-yao-chang-jing-xia-yong-l-2-zheng-ze-hua">什么场景下用L2正则化</h2>
<p>只要数据线性相关，用LinearRegression拟合的不是很好，<strong>需要正则化</strong>，可以考虑使用岭回归(L2), 如果输入特征的维度很高,而且是稀疏线性关系的话， 岭回归就不太合适,考虑使用Lasso回归。</p>
<h2 id="shi-yao-shi-l-1-zheng-ze-hua-lasso-hui-gui">什么是L1正则化(Lasso回归)</h2>
<p>L1正则化与L2正则化的区别在于惩罚项的不同：<br>
\[
J=J_0+\lambda(|w_1|+|w_2|)
\]<br>
求解J0的过程可以画出等值线。同时L1正则化的函数也可以在w1w2的二维平面上画出来。如下图：</p>
<p><img src="/2020/05/23/machine_learning/linear_regression/00630Defgy1g4nse7rf9xj308u089gme.jpg" alt></p>
<p>惩罚项表示为图中的黑色棱形，随着梯度下降法的不断逼近，与棱形第一次产生交点，而这个交点很容易出现在坐标轴上。<strong>这就说明了L1正则化容易得到稀疏矩阵。</strong></p>
<h2 id="shi-yao-chang-jing-xia-shi-yong-l-1-zheng-ze-hua">什么场景下使用L1正则化</h2>
<p><strong>L1正则化(Lasso回归)可以使得一些特征的系数变小,甚至还使一些绝对值较小的系数直接变为0</strong>，从而增强模型的泛化能力 。对于高的特征数据,尤其是线性关系是稀疏的，就采用L1正则化(Lasso回归),或者是要在一堆特征里面找出主要的特征，那么L1正则化(Lasso回归)更是首选了。</p>
<h2 id="shi-yao-shi-elastic-net-hui-gui">什么是ElasticNet回归</h2>
<p><strong>ElasticNet综合了L1正则化项和L2正则化项</strong>，它的公式是:<br>
\[
min(\frac{1}{2m}[\sum_{i=1}^{m}({y_i}^{'}-y_i)^2+\lambda\sum_{j=1}^{n}\theta_j^2]+\lambda\sum_{j=1}^{n}|\theta|
\]</p>
<h2 id="shi-yao-chang-jing-xia-shi-yong-elastic-net-hui-gui">什么场景下使用ElasticNet回归</h2>
<p>ElasticNet在我们发现用Lasso回归太过(太多特征被稀疏为0),而岭回归也正则化的不够(回归系数衰减太慢)的时候，可以考虑使用ElasticNet回归来综合，得到比较好的结果。</p>
<h1 id="xian-xing-hui-gui-yao-qiu-yin-bian-liang-fu-cong-zheng-tai-fen-bu">线性回归要求因变量服从正态分布？</h1>
<p>假设线性回归的噪声服从均值为0的正态分布。 当噪声符合正态分布\(N(0,\delta^2)\)时，因变量则符合正态分布\(N(wx(i)+b,\delta^2)\)，其中预测函数\(y=wx(i)+b\)。这个结论可以由正态分布的概率密度函数得到。也就是说当噪声符合正态分布时，其因变量必然也符合正态分布。 <strong>在用线性回归模型拟合数据之前，首先要求数据应符合或近似符合正态分布，否则得到的拟合函数不正确。</strong></p>
<h1 id="yu-shen-jing-wang-luo-zhi-jian-de-guan-xi">与神经网络之间的关系</h1>
<p>在深度学习中，可以使用神经网络图直观地表现模型结构。为了更清晰地展示线性回归作为神经网络的结构，下图使用神经网络图表示本节中介绍的线性回归模型。神经网络图隐去了模型参数权重和偏差。</p>
<p><img src="/2020/05/23/machine_learning/linear_regression/3.1_linreg.svg" alt></p>
<p>在上图所示的神经网络中，输入分别为 \(x_1\) 和 \(x_2\)，也叫特征数或特征向量维度。图中网络的输出为 \(o\)，输出层的输出个数为1。需要注意的是，我们直接将图3.1中神经网络的输出 \(o\) 作为线性回归的输出，即 \(\hat{y} = o\)。由于输入层并不涉及计算，按照惯例，图示的神经网络的层数为1。所以，线性回归是一个单层神经网络。输出层中负责计算 \(o\) 的单元又叫神经元。在线性回归中，\(o\) 的计算依赖于 \(x_1\) 和 \(x_2\)。也就是说，输出层中的神经元和输入层中各个输入完全连接。因此，这里的输出层又叫全连接层或稠密层。</p>
<h1 id="dai-ma-shi-xian">代码实现</h1>
<p><code><a class="btn" href="kc_data.zip">
            <i class="fa fa-download"></i>数据下载
          </a></code></p>
<h2 id="ji-qi-xue-xi-fang-fa">机器学习方法</h2>
<h3 id="shu-ju-jie-shao">数据介绍</h3>
<p>数据主要包括2014年5月至2015年5月美国King County的房屋销售价格以及房屋的基本信息。 数据分为训练数据和测试数据，分别保存在kc_train.csv和kc_test.csv两个文件中。 其中训练数据主要包括10000条记录，14个字段，主要字段说明如下：</p>
<blockquote>
<ol>
<li>第一列“销售日期”：2014年5月到2015年5月房屋出售时的日期</li>
<li>第二列“销售价格”：房屋交易价格，单位为美元，是目标预测值</li>
<li>第三列“卧室数”：房屋中的卧室数目</li>
<li>第四列“浴室数”：房屋中的浴室数目</li>
<li>第五列“房屋面积”：房屋里的生活面积</li>
<li>第六列“停车面积”：停车坪的面积</li>
<li>第七列“楼层数”：房屋的楼层数</li>
<li>第八列“房屋评分”：King County房屋评分系统对房屋的总体评分</li>
<li>第九列“建筑面积”：除了地下室之外的房屋建筑面积</li>
<li>第十列“地下室面积”：地下室的面积</li>
<li>第十一列“建筑年份”：房屋建成的年份</li>
<li>第十二列“修复年份”：房屋上次修复的年份</li>
<li>第十三列&quot;纬度&quot;：房屋所在纬度</li>
<li>第十四列“经度”：房屋所在经度。</li>
</ol>
</blockquote>
<p>测试数据主要包括3000条记录，13个字段，跟训练数据的不同是测试数据并不包括房屋销售价格，需要通过由训练数据所建立的模型以及所给的测试数据，得出测试数据相应的房屋销售价格预测值。</p>
<h3 id="bu-zou">步骤</h3>
<p><img src="/2020/05/23/machine_learning/linear_regression/687474703a2f2f7777772e7761696c69616e2e776f726b2f696d616765732f323031382f31322f31302f3132343030663535342e706e67.png" alt></p>
<blockquote>
<ol>
<li>选择合适的模型，对模型的好坏进行评估和选择。</li>
<li>对缺失的值进行补齐操作，可以使用均值的方式补齐数据，使得准确度更高。</li>
<li>数据的取值一般跟属性有关系，但世界万物的属性是很多的，有些值小，但不代表不重要，所有为了提高预测的准确度，统一数据维度进行计算，方法有特征缩放和归一法等。</li>
<li>数据处理好之后就可以进行调用模型库进行训练了。</li>
<li>使用测试数据进行目标函数预测输出，观察结果是否符合预期。或者通过画出对比函数进行结果线条对比。</li>
</ol>
</blockquote>
<h3 id="mo-xing-xuan-ze">模型选择</h3>
<p>一元线性回归<br>
\[
y:h(x)=wx+b
\]<br>
n元线性回归<br>
\[
y:h(x)=\sum^n_{i=1}w_ix^i+b=WX+b
\]<br>
这里，y表示要求的销售价格，x表示特征值。需要调用sklearn库来进行训练。</p>
<h3 id="shu-ju-chu-li">数据处理</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读取训练数据</span></span><br><span class="line">train_set = pd.read_csv(<span class="string">'kc_train.csv'</span>)</span><br><span class="line"><span class="comment">#销售价格</span></span><br><span class="line">target=pd.read_csv(<span class="string">'kc_train2.csv'</span>)  </span><br><span class="line"><span class="comment">#测试数据</span></span><br><span class="line">test=pd.read_csv(<span class="string">'kc_test.csv'</span>)   </span><br><span class="line"></span><br><span class="line">train_set.info()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/23/machine_learning/linear_regression/image-20200524102417564.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#特征缩放</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">minmax_scaler=MinMaxScaler()</span><br><span class="line"><span class="comment">#进行拟合，内部参数会发生变化</span></span><br><span class="line">minmax_scaler.fit(train_set)   </span><br><span class="line">scaler_train_set=minmax_scaler.transform(train_set)</span><br><span class="line">scaler_test_set = minmax_scaler.transform(test_set)</span><br><span class="line">scaler_train_set=pd.DataFrame(scaler_train_set,columns=train_set.columns)</span><br><span class="line">scaler_test_set = pd.DataFrame(scaler_test_set,columns=train_set.columns)</span><br><span class="line">scaler_train_set.head()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/23/machine_learning/linear_regression/image-20200524103226890.png" alt></p>
<h3 id="xun-lian-mo-xing">训练模型</h3>
<p>使用sklearn库的线性回归函数进行调用训练。梯度下降法获得误差最小值。最后使用均方误差法来评价模型的好坏程度，并画图进行比较。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#选择基于梯度下降的线性回归模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">LR_reg=LinearRegression()</span><br><span class="line"><span class="comment">#进行拟合</span></span><br><span class="line">LR_reg.fit(scaler_housing,target)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#使用均方误差用于评价模型好坏</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">preds=LR_reg.predict(scaler_housing)   <span class="comment">#输入数据进行预测得到结果</span></span><br><span class="line">mse=mean_squared_error(preds,target)   <span class="comment">#使用均方误差来评价模型好坏，可以输出mse进行查看评价值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#绘图进行比较</span></span><br><span class="line">plot.figure(figsize=(<span class="number">10</span>,<span class="number">7</span>))       <span class="comment">#画布大小</span></span><br><span class="line">num=<span class="number">100</span></span><br><span class="line">x=np.arange(<span class="number">1</span>,num+<span class="number">1</span>)              <span class="comment">#取100个点进行比较</span></span><br><span class="line">plot.plot(x,target[:num],label=<span class="string">'target'</span>)      <span class="comment">#目标取值</span></span><br><span class="line">plot.plot(x,preds[:num],label=<span class="string">'preds'</span>)        <span class="comment">#预测取值</span></span><br><span class="line">plot.legend(loc=<span class="string">'upper right'</span>)  <span class="comment">#线条显示位置</span></span><br><span class="line">plot.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/23/machine_learning/linear_regression/687474703a2f2f7777772e7761696c69616e2e776f726b2f696d616765732f323031382f31322f31302f3132343039346539362e706e67.png" alt></p>
<p>这张结果对比图中就可以看出模型是否得到精确的目标函数，是否能够精确预测房价。如果想要预测test文件里的数据，那就把test文件里的数据进行读取，并且进行特征缩放，调用： <strong>LR_reg.predict(test)</strong> 就可以得到预测结果，并进行输出操作。</p>
<h2 id="shen-jing-wang-luo-de-fang-fa-tensorflow">神经网络的方法(tensorflow)</h2>
<p>以房价预测问题为例(只为了简便使用其中两个特征，记为\(x_1,x_2\))。如果对训练数据集里的3个房屋样本(记为:\(x^{(1)},x^{(2)},x^{(3)},\))逐一预测价格，将得到:<br>
\[
\begin{aligned}
\hat{y}^{(1)} &amp;= x_1^{(1)} w_1 + x_2^{(1)} w_2 + b,\\
\hat{y}^{(2)} &amp;= x_1^{(2)} w_1 + x_2^{(2)} w_2 + b,\\
\hat{y}^{(3)} &amp;= x_1^{(3)} w_1 + x_2^{(3)} w_2 + b.
\end{aligned}
\]<br>
将上面3个等式转化成矢量计算:<br>
\[
\boldsymbol{\hat{y}} =
\begin{bmatrix}
    \hat{y}^{(1)} \\
    \hat{y}^{(2)} \\
    \hat{y}^{(3)}
\end{bmatrix},\quad
\boldsymbol{X} =
\begin{bmatrix}
    x_1^{(1)} &amp; x_2^{(1)} \\
    x_1^{(2)} &amp; x_2^{(2)} \\
    x_1^{(3)} &amp; x_2^{(3)}
\end{bmatrix},\quad
\boldsymbol{w} =
\begin{bmatrix}
    w_1 \\
    w_2
\end{bmatrix}
\]<br>
广义上讲，当数据样本数为 \(n\)，特征数为 \(d\) 时，线性回归的矢量计算表达式为\(\boldsymbol{\hat{y}} = \boldsymbol{X} \boldsymbol{w} + b\),其中模型输出 \(\boldsymbol{\hat{y}} \in \mathbb{R}^{n \times 1}\) 批量数据样本特征 \(\boldsymbol{X} \in \mathbb{R}^{n \times d}\)，权重 \(\boldsymbol{w} \in \mathbb{R}^{d \times 1}\)， 偏差 \(b \in \mathbb{R}\)。相应地，批量数据样本标签 \(\boldsymbol{y} \in \mathbb{R}^{n \times 1}\)。假设模型参数 \(\boldsymbol{\theta} = [w_1, w_2,\ldots, b]^\top\)，可以重写损失函数为<br>
\[
\ell(\boldsymbol{\theta})=\frac{1}{2n}(\boldsymbol{\hat{y}}-\boldsymbol{y})^\top(\boldsymbol{\hat{y}}-\boldsymbol{y})
\]</p>
<p>小批量随机梯度下降的迭代步骤将相应地改写为<br>
\[
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}   \nabla_{\boldsymbol{\theta}} \ell^{(i)}(\boldsymbol{\theta}),
\]</p>
<p>其中梯度是损失有关标量模型参数的偏导数组成的向量：<br>
\[
\nabla_{\boldsymbol{\theta}} \ell^{(i)}(\boldsymbol{\theta})=
\begin{bmatrix}
    \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial w_1} \\
    \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial w_2} \\
    \ldots \\
    \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial b}
\end{bmatrix} =
\begin{bmatrix}
    x_1^{(i)} (x_1^{(i)} w_1 + x_2^{(i)} w_2 +\ldots+ b - y^{(i)}) \\
    x_2^{(i)} (x_1^{(i)} w_1 + x_2^{(i)} w_2 +\ldots+ b - y^{(i)}) \\
        \ldots \\
    x_1^{(i)} w_1 + x_2^{(i)} w_2 +\ldots+ b - y^{(i)}
\end{bmatrix}=
\begin{bmatrix}
    x_1^{(i)} \\
    x_2^{(i)} \\
        \ldots \\
    1
\end{bmatrix}
(\hat{y}^{(i)} - y^{(i)})
\]</p>
<h3 id="du-qu-shu-ju">读取数据</h3>
<p>在训练模型的时候，需要遍历数据集并不断读取小批量数据样本。这里定义一个函数：它每次返回<code>batch_size</code>个随机样本的特征和标签。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(batch_size, features, labels)</span>:</span></span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        j = indices[i: min(i+batch_size, num_examples)]</span><br><span class="line">        <span class="keyword">yield</span> tf.gather(np.array(features,dtype=np.float32), axis=<span class="number">0</span>, indices=j), tf.gather(np.array(labels,dtype=np.float32), axis=<span class="number">0</span>, indices=j)</span><br></pre></td></tr></table></figure>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">batch_size</span> = <span class="number">10</span></span><br><span class="line"><span class="comment"># 将训练数据的特征和标签组合</span></span><br><span class="line"><span class="attr">dataset</span> = tfdata.Dataset.from_tensor_slices((features, labels))</span><br><span class="line"><span class="comment"># 随机读取小批量</span></span><br><span class="line"><span class="attr">dataset</span> = dataset.shuffle(buffer_size=num_examples) </span><br><span class="line"><span class="attr">dataset</span> = dataset.batch(batch_size)</span><br><span class="line"><span class="attr">data_iter</span> = iter(dataset)</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing">定义模型</h3>
<p>初始化模型参数:将权重初始化成均值为0、标准差为0.01的正态随机数，偏差则初始化成0。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_features = len(train_set.columns)</span><br><span class="line">w = tf.Variable(tf.random.normal((num_features, <span class="number">1</span>), stddev=<span class="number">0.01</span>),dtype=tf.float32)</span><br><span class="line">b = tf.Variable(tf.zeros((<span class="number">1</span>,)),dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<p>下面是线性回归的矢量计算表达式的实现。使用<code>matmul</code>函数做矩阵乘法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linreg</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.matmul(X, w) + b</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-sun-shi-han-shu">定义损失函数</h3>
<p>使用平方损失来定义线性回归的损失函数。在实现中，我们需要把真实值<code>y</code>变形成预测值<code>y_hat</code>的形状。以下函数返回的结果也将和<code>y_hat</code>的形状相同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squared_loss</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - tf.reshape(y, y_hat.shape)) ** <span class="number">2</span> /<span class="number">2</span></span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-you-hua-suan-fa">定义优化算法</h3>
<p><code>sgd</code>函数实现了小批量随机梯度下降算法。它通过不断迭代模型参数来优化损失函数。这里自动求梯度模块计算得来的梯度是一个批量样本的梯度和，将它除以批量大小来得到平均值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size, grads)</span>:</span></span><br><span class="line">    <span class="string">"""Mini-batch stochastic gradient descent."""</span></span><br><span class="line">    <span class="keyword">for</span> i, param <span class="keyword">in</span> enumerate(params):</span><br><span class="line">        param.assign_sub(lr * grads[i] / batch_size)</span><br></pre></td></tr></table></figure>
<h3 id="xun-lian-mo-xing-1">训练模型</h3>
<p>在训练中，将多次迭代模型参数。在每次迭代中，根据当前读取的小批量数据样本（特征<code>X</code>和标签<code>y</code>），通过调用反向函数<code>t.gradients</code>计算小批量随机梯度，并调用优化算法<code>sgd</code>迭代模型参数。由于之前设批量大小<code>batch_size</code>为10，每个小批量的损失<code>l</code>的形状为(10, 1)。由于变量<code>l</code>并不是一个标量，所以我们可以调用<code>reduce_sum()</code>将其求和得到一个标量，再运行<code>t.gradients</code>得到该变量有关模型参数的梯度。</p>
<blockquote>
<p>注意在每次更新完参数后不要忘了将参数的梯度清零。</p>
</blockquote>
<p>在一个epoch中，将完整遍历一遍<code>data_iter</code>函数，并对训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。迭代周期个数<code>num_epochs</code>和学习率<code>lr</code>都是超参数，在实践中，大多超参数都需要通过反复试错来不断调节。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">num_epochs = <span class="number">30</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, train_set, target):</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> t:</span><br><span class="line">            t.watch([w,b])</span><br><span class="line">            l = loss(net(X, w, b), y)</span><br><span class="line">        grads = t.gradient(l, [w, b])</span><br><span class="line">        sgd([w, b], lr, batch_size, grads)</span><br><span class="line">    train_l = loss(net(X, w, b), y)</span><br><span class="line">    print(<span class="string">'epoch %d, loss %f'</span> % (epoch + <span class="number">1</span>, tf.reduce_mean(train_l)))</span><br></pre></td></tr></table></figure>
<h3 id="tensorflow-kuai-su-shi-xian-ban-ben">tensorflow快速实现版本</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> initializers <span class="keyword">as</span> init</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> losses</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line"><span class="comment"># 模型</span></span><br><span class="line">model = keras.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, kernel_initializer=init.RandomNormal(stddev=<span class="number">0.01</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失</span></span><br><span class="line">loss = losses.MeanSquaredError()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">trainer = optimizers.SGD(learning_rate=<span class="number">0.03</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">num_epochs = <span class="number">30</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, num_epochs + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> (batch, (X, y)) <span class="keyword">in</span> enumerate(dataset):</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            l = loss(model(X, training=<span class="literal">True</span>), y)</span><br><span class="line">        </span><br><span class="line">        grads = tape.gradient(l, model.trainable_variables)</span><br><span class="line">        trainer.apply_gradients(zip(grads, model.trainable_variables))</span><br><span class="line">    </span><br><span class="line">    l = loss(model(features), labels)</span><br><span class="line">    print(<span class="string">'epoch %d, loss: %f'</span> % (epoch, l))</span><br></pre></td></tr></table></figure>
<h2 id="xiao-jie">小结</h2>
<ul>
<li>可以看出，仅使用<code>Variables</code>和<code>GradientTape</code>模块就可以很容易地实现一个模型。</li>
<li>和大多数深度学习模型一样，对于线性回归这样一种单层神经网络，它的基本要素包括模型、训练数据、损失函数和优化算法。</li>
<li>既可以用神经网络图表示线性回归，又可以用矢量计算表示该模型。</li>
</ul>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title>多层感知机</title>
    <url>/2020/05/22/machine_learning/multi-layer-perceptron/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/22/machine_learning/multi-layer-perceptron/3.8_mlp.svg" alt></p>
<a id="more"></a>
<h1 id="duo-ceng-gan-zhi-ji">多层感知机</h1>
<p>深度学习主要关注多层模型。本节介绍多层神经网络的概念。</p>
<h2 id="yin-cang-ceng">隐藏层</h2>
<p>多层感知机在单层神经网络的基础上引入了一到多个隐藏层。隐藏层位于输入层和输出层之间。下图展示了一个多层感知机的神经网络图，它含有一个隐藏层，该层中有5个隐藏单元。</p>
<p><img src="/2020/05/22/machine_learning/multi-layer-perceptron/3.8_mlp.svg" alt></p>
<p>在上图图所示的多层感知机中，输入和输出个数分别为<code>4</code>和<code>3</code>，中间的隐藏层中包含了<code>5</code>个隐藏单元。由于输入层不涉及计算，图中的多层感知机的层数为<code>2</code>，隐藏层中的神经元和输入层中各个输入完全连接，输出层中的神经元和隐藏层中的各个神经元也完全连接。因此，多层感知机中的隐藏层和输出层都是全连接层。</p>
<p>具体来说，给定一个小批量样本\(\boldsymbol{X} \in \mathbb{R}^{n \times d}\)，其批量大小为\(n\)，输入个数为\(d\)。假设多层感知机只有一个隐藏层，其中隐藏单元个数为\(h\)。记隐藏层的输出为\(\boldsymbol{H}\)，有\(\boldsymbol{H} \in \mathbb{R}^{n \times h}\)。因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为\(\boldsymbol{W}_h \in \mathbb{R}^{d \times h}\)和 \(\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}\)，输出层的权重和偏差参数分别为\(\boldsymbol{W}_o \in \mathbb{R}^{h \times q}\)和\(\boldsymbol{b}_o \in \mathbb{R}^{1 \times q}\)。其输出\(\boldsymbol{O} \in \mathbb{R}^{n \times q}\)的计算为:</p>
<p>\[
\begin{aligned}
\boldsymbol{H} &amp;= \boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h,\\
\boldsymbol{O} &amp;= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o,
\end{aligned}      
\]</p>
<p>如果将以上两个式子联立起来，可以得到</p>
<p>\[
\boldsymbol{O} = (\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h)\boldsymbol{W}_o + \boldsymbol{b}_o = \boldsymbol{X} \boldsymbol{W}_h\boldsymbol{W}_o + \boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o.
\]</p>
<p>从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络：其中输出层权重参数为\(\boldsymbol{W}_h\boldsymbol{W}_o\)，偏差参数为\(\boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o\)。不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。</p>
<h2 id="ji-huo-han-shu">激活函数</h2>
<p>上述问题的根源在于全连接层只是对数据做仿射变换，而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数<code>(activation function)</code>。</p>
<h3 id="re-lu-han-shu">ReLU函数</h3>
<p>ReLU<code>(rectified linear unit)</code>函数提供了一个很简单的非线性变换。给定元素\(x\)，该函数定义为</p>
<p>\[
\text{ReLU}(x) = \max(x, 0)
\]</p>
<p><img src="/2020/05/22/machine_learning/multi-layer-perceptron/3.8_relu.png" alt></p>
<p>当输入为负数时，ReLU函数的导数为0；当输入为正数时，ReLU函数的导数为1。尽管输入为0时ReLU函数不可导，但是可以取此处的导数为0。</p>
<p><img src="/2020/05/22/machine_learning/multi-layer-perceptron/3.8_relu_grad.png" alt></p>
<h3 id="sigmoid-han-shu">sigmoid函数</h3>
<p>sigmoid函数可以将元素的值变换到0和1之间：</p>
<p>\[
\text{sigmoid}(x) = \frac{1}{1 + \exp(-x)}
\]</p>
<p>sigmoid函数在早期的神经网络中较为普遍，但它目前逐渐被更简单的ReLU函数取代。</p>
<p><img src="/2020/05/22/machine_learning/multi-layer-perceptron/3.8_sigmoid.png" alt></p>
<p>依据链式法则，sigmoid函数的导数</p>
<p>\[
\text{sigmoid}'(x) = \text{sigmoid}(x)\left(1-\text{sigmoid}(x)\right)
\]</p>
<p>下面绘制了sigmoid函数的导数。当输入为0时，sigmoid函数的导数达到最大值0.25；当输入越偏离0时，sigmoid函数的导数越接近0。</p>
<p><img src="/2020/05/22/machine_learning/multi-layer-perceptron/3.8_sigmoid_grad.png" alt></p>
<h3 id="tanh-han-shu">tanh函数</h3>
<p>tanh（双曲正切）函数可以将元素的值变换到-1和1之间：</p>
<p>\[
\text{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}
\]</p>
<p>当输入接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。</p>
<p><img src="/2020/05/22/machine_learning/multi-layer-perceptron/3.8_tanh.png" alt></p>
<p>依据链式法则，tanh函数的导数</p>
<p>\[
\text{tanh}'(x) = 1 - \text{tanh}^2(x)
\]</p>
<p>下面绘制了tanh函数的导数。当输入为0时，tanh函数的导数达到最大值1；当输入越偏离0时，tanh函数的导数越接近0。</p>
<p><img src="/2020/05/22/machine_learning/multi-layer-perceptron/3.8_tanh_grad.png" alt></p>
<h1 id="dai-ma-shi-xian-tensorflow">代码实现(tensorflow)</h1>
<h2 id="zhun-bei-shu-ju-shu-ju">准备数据数据</h2>
<p>使用Fashion-MNIST数据集，使用多层感知机对图像进行分类。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> fashion_mnist</span><br><span class="line">(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()</span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">x_train = tf.cast(x_train, tf.float32)</span><br><span class="line">x_test = tf.cast(x_test, tf.float32)</span><br><span class="line">x_train = x_train/<span class="number">255.0</span></span><br><span class="line">x_test = x_test/<span class="number">255.0</span></span><br><span class="line">train_iter = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)</span><br><span class="line">test_iter = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)</span><br></pre></td></tr></table></figure>
<h2 id="ding-yi-mo-xing">定义模型</h2>
<p>Fashion-MNIST数据集中图像形状为 \(28 \times 28\)，类别数为10，使用长度为 \(28 \times 28 = 784\) 的向量表示每一张图像。因此，输入个数为784，输出个数为10。设超参数隐藏单元个数为256。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">W1 = tf.Variable(tf.random.normal(shape=(num_inputs, num_hiddens),mean=<span class="number">0</span>, stddev=<span class="number">0.01</span>, dtype=tf.float32))</span><br><span class="line">b1 = tf.Variable(tf.zeros(num_hiddens, dtype=tf.float32))</span><br><span class="line">W2 = tf.Variable(tf.random.normal(shape=(num_hiddens, num_outputs),mean=<span class="number">0</span>, stddev=<span class="number">0.01</span>, dtype=tf.float32))</span><br><span class="line">b2 = tf.Variable(tf.random.normal([num_outputs], stddev=<span class="number">0.1</span>))</span><br></pre></td></tr></table></figure>
<h2 id="ding-yi-ji-huo-han-shu">定义激活函数</h2>
<p>这里使用基础的<code>max</code>函数来实现ReLU，而非直接调用<code>relu</code>函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.math.maximum(x,<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="ding-yi-mo-xing-1">定义模型</h2>
<p>通过<code>reshape</code>函数将每张原始图像改成长度为<code>num_inputs</code>的向量，然后实现多层感知机的计算表达式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    X = tf.reshape(X, shape=[<span class="number">-1</span>, num_inputs])</span><br><span class="line">    h = relu(tf.matmul(X, W1) + b1)</span><br><span class="line">    <span class="keyword">return</span> tf.math.softmax(tf.matmul(h, W2) + b2)</span><br></pre></td></tr></table></figure>
<h2 id="ding-yi-sun-shi-han-shu">定义损失函数</h2>
<p>为了得到更好的数值稳定性，直接使用Tensorflow提供的包括softmax运算和交叉熵损失计算的函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(y_hat,y_true)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.losses.sparse_categorical_crossentropy(y_true,y_hat)</span><br></pre></td></tr></table></figure>
<h2 id="xun-lian-mo-xing">训练模型</h2>
<p>训练多层感知机，在这里设超参数迭代周期数为5，学习率为0.5。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">0.5</span></span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None, lr=None, trainer=None)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">                y_hat = net(X)</span><br><span class="line">                l = tf.reduce_sum(loss(y_hat, y))</span><br><span class="line">            grads = tape.gradient(l, params)</span><br><span class="line">            <span class="keyword">if</span> trainer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 如果没有传入优化器，则使用原先编写的小批量随机梯度下降</span></span><br><span class="line">                <span class="keyword">for</span> i, param <span class="keyword">in</span> enumerate(params):</span><br><span class="line">                    param.assign_sub(lr * grads[i] / batch_size)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># tf.keras.optimizers.SGD 直接使用是随机梯度下降 theta(t+1) = theta(t) - learning_rate * gradient</span></span><br><span class="line">                <span class="comment"># 这里使用批量梯度下降，需要对梯度除以 batch_size, 对应原书代码的 trainer.step(batch_size)</span></span><br><span class="line">                trainer.apply_gradients(zip([grad / batch_size <span class="keyword">for</span> grad <span class="keyword">in</span> grads], params))  </span><br><span class="line">                </span><br><span class="line">            y = tf.cast(y, dtype=tf.float32)</span><br><span class="line">            train_l_sum += l.numpy()</span><br><span class="line">            train_acc_sum += tf.reduce_sum(tf.cast(tf.argmax(y_hat, axis=<span class="number">1</span>) == tf.cast(y, dtype=tf.int64), dtype=tf.int64)).numpy()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span>% (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br><span class="line"></span><br><span class="line">train(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.8208</span>, train acc <span class="number">0.693</span>, test acc <span class="number">0.804</span></span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.4784</span>, train acc <span class="number">0.822</span>, test acc <span class="number">0.832</span></span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.4192</span>, train acc <span class="number">0.843</span>, test acc <span class="number">0.850</span></span><br><span class="line">epoch <span class="number">4</span>, loss <span class="number">0.3874</span>, train acc <span class="number">0.857</span>, test acc <span class="number">0.858</span></span><br><span class="line">epoch <span class="number">5</span>, loss <span class="number">0.3651</span>, train acc <span class="number">0.864</span>, test acc <span class="number">0.860</span></span><br></pre></td></tr></table></figure>
<h2 id="tensorflow-kuai-su-shi-xian-ban-ben">tensorflow快速实现版本</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">".."</span>) </span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="comment"># 数据</span></span><br><span class="line">fashion_mnist = keras.datasets.fashion_mnist</span><br><span class="line">(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()</span><br><span class="line">x_train = x_train / <span class="number">255.0</span></span><br><span class="line">x_test = x_test / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型</span></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>, <span class="number">28</span>)),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>,),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line">model.compile(optimizer=tf.keras.optimizers.SGD(lr=<span class="number">0.5</span>), <span class="comment"># 优化器</span></span><br><span class="line">             loss = <span class="string">'sparse_categorical_crossentropy'</span>, <span class="comment"># 损失</span></span><br><span class="line">             metrics=[<span class="string">'accuracy'</span>]) <span class="comment"># 评测指标</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">5</span>,</span><br><span class="line">              batch_size=<span class="number">256</span>,</span><br><span class="line">              validation_data=(x_test, y_test),</span><br><span class="line">              validation_freq=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 评测</span></span><br><span class="line">model.evaluate(x_test,y_test)</span><br></pre></td></tr></table></figure>
<h1 id="xiao-jie">小结</h1>
<p>多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。以单隐藏层为例并沿用本节之前定义的符号，多层感知机按以下方式计算输出：<br>
\[
\begin{aligned}
\boldsymbol{H} &amp;= \phi(\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h),\\
\boldsymbol{O} &amp;= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o,
\end{aligned}
\]</p>
<p>其中\(\phi\)表示激活函数。在分类问题中，可以对输出\(\boldsymbol{O}\)做softmax运算，并使用softmax回归中的交叉熵损失函数。<br>
在回归问题中，将输出层的输出个数设为1，并将输出\(\boldsymbol{O}\)直接提供给线性回归中使用的平方损失函数。</p>
<p>多层感知机在输出层与输入层之间加入了一个或多个全连接隐藏层，并通过激活函数对隐藏层输出进行变换。</p>
<p>常用的激活函数包括ReLU函数、sigmoid函数和tanh函数。</p>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>Pytorch Bugs</title>
    <url>/2020/05/20/pytorch_bugs/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/20/pytorch_bugs/computer-bug-850x479.jpg" alt></p>
<a id="more"></a>
<h1 id="xian-qia-xiang-guan">显卡相关</h1>
<h2 id="cuda-error-invalid-device-ordinal">CUDA error: invalid device ordinal</h2>
<h3 id="bug-miao-shu">bug 描述</h3>
<p>这个问题是在有多张显卡的机器上，由于0卡被占用，而选择使用除0卡外的其他卡时出现的错误。</p>
<h3 id="yuan-yin">原因</h3>
<p>当<code>CUDA_VISIBLE_DEVICES</code>被赋值为单个值的时候(即使你使用的是1卡)，pytorch 会默认你的gpu_id为0。</p>
<p>所以如果想使用非0的单张卡，<code>CUDA_VISIBLE_DEVICES</code>应该被设置为多个值，比如<code>1,0</code>，这样主卡的id会被设置为1。从而解决问题。</p>
<h2 id="runtime-error-cu-dnn-error-cudnn-status-bad-param">RuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM</h2>
<h3 id="bug-miao-shu-1">bug 描述</h3>
<p>这个问题是因为在处理cnews 的数据的时候，序列长度太长，超出了缓存容量，解决方案是把rnn的输入tensor的float64转成了float32.</p>
<h1 id="shu-ju-lei-xing-bu-yi-zhi-xiang-guan-bugs">数据类型不一致相关bugs</h1>
<h2 id="expected-tensor-for-argument-1-indices-to-have-scalar-type-long-but-got-cuda-type-instead-while-checking-arguments-for-embedding">Expected tensor for argument #1 ‘indices’ to have scalar type Long; but got CUDAType instead (while checking arguments for embedding)</h2>
<h3 id="bug-miao-shu-2">bug 描述</h3>
<p>在使用nn.embedding ，forward的时候</p>
<h3 id="yuan-yin-1">原因</h3>
<p>由于对应的token_id是从使用<code>torch.from_numpy(np.array(token_ids))</code>转换过来的。所以token_ids 的类型从int转成了float，而nn.embedding的需要的输入是 int 或者 Long类型</p>
<h3 id="jie-jue">解决</h3>
<p><code>torch.from_numpy(np.array(token_ids))--&gt; torch.from_numpy(np.array(token_ids,**dtype=np.int**))</code></p>
<h1 id="yue-jie-wen-ti">越界问题</h1>
<h2 id="device-side-assert-triggered-unable-to-get-repr-for-xx">device-side assert triggered/unable to get repr for xx</h2>
<ol>
<li>在使用nn.Embedding()时，token_id出现负数，或者超出nn.Embedding的最大值会出现错误。</li>
<li>Bert 输入长度超过512</li>
</ol>
<h3 id="jie-jue-ban-fa">解决办法</h3>
<p>检查 token_id 值是否有越界。</p>
<h1 id="other">Other</h1>
<h2 id="bug-miao-shu-3">bug 描述</h2>
<p>torch.tensor(input_size,output_size)</p>
<p>tensor() takes 1 positional argument but 2 were given</p>
<h2 id="fen-xi">分析</h2>
<p>在Pytorch中，Tensor和tensor都用于生成新的张量。</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;  a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>])</span><br><span class="line">&gt;&gt;&gt; a=torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>区别：</p>
<p>torch.Tensor()是Python类，更明确的说，是默认张量类型torch.FloatTensor()的别名，torch.Tensor([1,2]) 会调用Tensor类的构造函数__init__，<strong>生成单精度浮点类型的张量</strong>。</p>
<p>torch.tensor()仅仅是Python的函数，<code>torch.tensor(data, dtype=None, device=None, requires_grad=False)</code>其中data可以是：list, tuple, array, scalar等类型。</p>
]]></content>
      <categories>
        <category>技术/pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>bugs</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构-屠龙14式</title>
    <url>/2020/05/15/data-structures-and-algorithms/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/15/data-structures-and-algorithms/0.png" alt></p>
<a id="more"></a>
<h1 id="shuang-zhi-zhen-mo-shi">双指针模式</h1>
<p><strong>基本思想</strong>:使用两个指针以不同的速度在数组或链表中移动。在处理<strong>循环链表</strong>或<strong>数组</strong>时，此方法非常有用。</p>
<p>通过以不同的速度移动（例如，在循环链表中），算法证明两个指针必然会相遇。 一旦两个指针都处于循环循环中，快速指针就应该捕获慢速指针。这种方法在解决有环的链表和数组时特别有用。</p>
<h2 id="kuai-man-zhi-zhen">快慢指针</h2>
<p>快慢指针一般都初始化指向链表的头结点 head，前进时快指针 fast 在前，慢指针 slow 在后，巧妙解决一些链表中的问题。</p>
<h3 id="pan-ding-lian-biao-zhong-shi-fou-han-you-huan">判定链表中是否含有环</h3>
<p>如果链表中不含环，那么这个指针最终会遇到空指针 null 表示链表到头了，可以判断该链表不含环。但是如果链表中含有环，那么这个指针就会陷入死循环，因为环形数组中没有 null 指针作为尾部节点。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">def has_cycle(head) &#123;</span><br><span class="line">    <span class="keyword">while</span> head:</span><br><span class="line">        head = head.next</span><br><span class="line">    <span class="keyword">return</span> false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>经典解法就是用两个指针，一个跑得快，一个跑得慢。如果不含有环，跑得快的那个指针最终会遇到 null，说明链表不含环；如果含有环，快指针最终会超慢指针一圈，和慢指针相遇，说明链表含有环。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">def has_cycle(head) &#123;</span><br><span class="line">    </span><br><span class="line">    fast = head</span><br><span class="line">    slow = head</span><br><span class="line">    <span class="keyword">while</span> fast <span class="keyword">and</span> fast.next:</span><br><span class="line">        fast = fast.next.next</span><br><span class="line">        slow = slow.next</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> fast == slow: </span><br><span class="line">    		<span class="keyword">return</span> true</span><br><span class="line">    <span class="keyword">return</span> false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="yi-zhi-lian-biao-zhong-han-you-huan-fan-hui-zhe-ge-huan-de-qi-shi-wei-zhi">已知链表中含有环，返回这个环的起始位置</h3>
<p><img src="/2020/05/15/data-structures-and-algorithms/fast_slow_pointer.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">def detect_cycle(head) &#123;</span><br><span class="line">    </span><br><span class="line">    fast = head</span><br><span class="line">    slow = head</span><br><span class="line">    <span class="keyword">while</span> fast <span class="keyword">and</span> fast.next:</span><br><span class="line">        fast = fast.next.next</span><br><span class="line">        slow = slow.next</span><br><span class="line">        <span class="keyword">if</span> fast == slow:</span><br><span class="line">        	<span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 上面的代码类似 hasCycle 函数</span></span><br><span class="line">    slow = head;</span><br><span class="line">    <span class="keyword">while</span> slow != fast:</span><br><span class="line">        fast = fast.next</span><br><span class="line">        slow = slow.next</span><br><span class="line">    <span class="keyword">return</span> slow</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当快慢指针相遇时，让其中任一个指针指向头节点，然后让它俩以相同速度前进，再次相遇时所在的节点位置就是环开始的位置。</p>
<p>第一次相遇时，假设慢指针 <code>slow</code> 走了 <code>k</code> 步，那么快指针 <code>fast</code> 一定走了 <code>2k</code> 步，也就是说比 <code>slow</code> 多走了 <code>k</code> 步（也就是环的长度）。</p>
<p><img src="/2020/05/15/data-structures-and-algorithms/fast_slow_pointer_2.png" alt></p>
<p>设相遇点距环的起点的距离为 <code>m</code>，那么环的起点距头结点 <code>head</code> 的距离为 <code>k - m</code>，也就是说如果从 <code>head</code> 前进 <code>k - m</code> 步就能到达环起点。如果从相遇点继续前进 <code>k - m</code> 步，也恰好到达环起点。</p>
<p><img src="/2020/05/15/data-structures-and-algorithms/fast_slow_pointer_3.png" alt></p>
<p>所以，只要把快慢指针中的任一个重新指向 <code>head</code>，然后两个指针同速前进，<code>k - m</code> 步后就会相遇，相遇之处就是环的起点了。</p>
<h3 id="xun-zhao-lian-biao-de-zhong-dian">寻找链表的中点</h3>
<p>让快指针一次前进两步，慢指针一次前进一步，当快指针到达链表尽头时，慢指针就处于链表的中间位置。当链表的长度是奇数时，slow 恰巧停在中点位置(此时fast.next=null)；如果长度是偶数，<span class="label danger">slow 最终的位置是中间偏右</span>(此时fast = null)：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> fast <span class="keyword">and</span> fast.next:</span><br><span class="line">    fast = fast.next.next</span><br><span class="line">    slow = slow.next</span><br><span class="line"></span><br><span class="line"><span class="comment"># slow 就在中间位置</span></span><br><span class="line"><span class="keyword">return</span> slow</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/15/data-structures-and-algorithms/image-20200630183112564.png" alt="image-20200630183112564"></p>
<blockquote>
<p>寻找链表中点的一个重要作用是对链表进行归并排序。</p>
<p>归并排序：求中点索引递归地把数组二分，最后合并两个有序数组。对于链表，合并两个有序链表是很简单的，难点就在于二分。</p>
</blockquote>
<h3 id="xun-zhao-lian-biao-de-dao-shu-di-k-ge-yuan-su">寻找链表的倒数第 k 个元素</h3>
<p>使用快慢指针，让快指针先走 k 步，然后快慢指针开始同速前进。这样当快指针走到链表末尾 null 时，慢指针所在的位置就是倒数第 k 个链表节点:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_k</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">  slow = head</span><br><span class="line">    fast = head</span><br><span class="line">    <span class="keyword">while</span> k &gt; <span class="number">0</span>:</span><br><span class="line">        fast = fast.next</span><br><span class="line">        k -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> fast :</span><br><span class="line">        slow = slow.next;</span><br><span class="line">        fast = fast.next;</span><br><span class="line">  <span class="keyword">return</span> slow;</span><br></pre></td></tr></table></figure>
<h3 id="ying-yong-chang-jing">应用场景</h3>
<ul>
<li>链表或数组循环</li>
<li>用于找中间元素</li>
<li>需要知道某个元素的位置或链表的总长度</li>
</ul>
<h3 id="ti-mu">题目</h3>
<ul class="contains-task-list">
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox0" checked="true" disabled="true"><label for="checkbox0"> <a href="https://leetcode-cn.com/problems/linked-list-cycle/" target="_blank" rel="noopener">环形链表（LEETCODE）</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox1" checked="true" disabled="true"><label for="checkbox1"> <a href="https://leetcode-cn.com/problems/intersection-of-two-linked-lists/" target="_blank" rel="noopener">相交链表（LEETCODE）</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox2" checked="true" disabled="true"><label for="checkbox2"> <a href="https://leetcode-cn.com/problems/linked-list-cycle-ii/" target="_blank" rel="noopener">环形链表入口节点（LEETCODE）</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox3" checked="true" disabled="true"><label for="checkbox3"> <a href="https://leetcode-cn.com/problems/lian-biao-zhong-dao-shu-di-kge-jie-dian-lcof/" target="_blank" rel="noopener">链表中倒数第k个节点</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox4" disabled="true"><label for="checkbox4"> <a href="https://leetcode-cn.com/problems/palindrome-linked-list/" target="_blank" rel="noopener">回文链表</a></label></div></p>
</li>
</ul>
<h2 id="zuo-you-zhi-zhen">左右指针</h2>
<p>左右指针在数组中实际是指两个索引值，一般初始化为 <code>left = 0, right = nums.length - 1</code>。</p>
<h3 id="er-fen-cha-zhao">二分查找</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span><span class="params">(nums, target)</span>:</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = len(nums) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        mid = left + (right - left) / <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] &gt; target:</span><br><span class="line">            right = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] == target:</span><br><span class="line">            <span class="comment"># 直接返回（适合列表中不存在重复元素）</span></span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<h3 id="liang-shu-he">两数和</h3>
<p><img src="/2020/05/15/data-structures-and-algorithms/2.png" alt></p>
<blockquote>
<p>只要数组有序，就应该想到双指针技巧。</p>
</blockquote>
<h3 id="fan-zhuan-shu-zu">反转数组</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">def reverse(nums) &#123;</span><br><span class="line">    left = <span class="number">0</span>;</span><br><span class="line">    right = len(nums) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        <span class="comment"># swap(nums[left], nums[right])</span></span><br><span class="line">        temp = nums[left]</span><br><span class="line">        nums[left] = nums[right]</span><br><span class="line">        nums[right] = temp</span><br><span class="line">        left += <span class="number">1</span></span><br><span class="line">    	right -= <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="hua-dong-chuang-kou-suan-fa">滑动窗口算法</h3>
<p>滑动窗口模式用于对给定数组或链表的特定窗口大小执行所需操作.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">slide_window</span><span class="params">(s,t)</span>:</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 窗口: 字典/list/set</span></span><br><span class="line">    window = defaultdict(int)</span><br><span class="line">    <span class="comment"># gold: 字典/list</span></span><br><span class="line">    need = defaultdict(int)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化need</span></span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> t:</span><br><span class="line">        <span class="comment"># need do something</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> right &lt; len(s):</span><br><span class="line">        <span class="comment"># 要添加的字符</span></span><br><span class="line">        c = s[right]</span><br><span class="line">        <span class="comment"># right 指针右移，扩大窗口</span></span><br><span class="line">        right += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对窗口内的数据进行更新操作 1</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判断左侧窗口是否要收缩</span></span><br><span class="line">        <span class="keyword">while</span> (window needs shrink):</span><br><span class="line">            <span class="comment"># d 是将移出窗口的字符</span></span><br><span class="line">            d = s[left]</span><br><span class="line">            <span class="comment"># 左移窗口</span></span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 进行窗口内数据的一系列更新操作 2（与1处保持对称）</span></span><br><span class="line">            <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<h3 id="ying-yong-chang-jing-1">应用场景</h3>
<ul>
<li>问题为排序数组或链表，并且需要满足某些约束的一组元素问题</li>
<li>数组中的元素集是一对，三元组，甚至是子数组</li>
</ul>
<h3 id="ti-mu-1">题目</h3>
<ul class="contains-task-list">
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox5" disabled="true"><label for="checkbox5"> [N-sum问题(leetcode)]</label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox6" checked="true" disabled="true"><label for="checkbox6"> <a href="https://leetcode-cn.com/problems/find-closest-lcci/" target="_blank" rel="noopener">单词距离</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox7" checked="true" disabled="true"><label for="checkbox7"> <a href="https://leetcode-cn.com/problems/reverse-string/" target="_blank" rel="noopener">反转字符串</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox8" checked="true" disabled="true"><label for="checkbox8"> <a href="https://leetcode-cn.com/problems/merge-sorted-array/" target="_blank" rel="noopener">合并两个有序数组</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox9" checked="true" disabled="true"><label for="checkbox9"> <a href="https://leetcode-cn.com/problems/valid-palindrome/" target="_blank" rel="noopener">验证回文串</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox10" disabled="true"><label for="checkbox10"> <a href="https://leetcode-cn.com/problems/binary-subarrays-with-sum/" target="_blank" rel="noopener">和相同的二元子数组</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox11" checked="true" disabled="true"><label for="checkbox11"> <a href="https://leetcode-cn.com/problems/longest-substring-without-repeating-characters/" target="_blank" rel="noopener">无重复字符的最长子串(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox12" disabled="true"><label for="checkbox12"> <a href="https://leetcode-cn.com/problems/trapping-rain-water/" target="_blank" rel="noopener">接雨水(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox13" disabled="true"><label for="checkbox13"> <a href="https://leetcode-cn.com/problems/minimum-size-subarray-sum/" target="_blank" rel="noopener">长度最小的子数组(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox14" disabled="true"><label for="checkbox14"> [输出一个排好序的数组的平方数组（简单）]</label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox15" disabled="true"><label for="checkbox15"> [3-Sum（中等）]</label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox16" disabled="true"><label for="checkbox16"> [比较两个字符是否相等，字符中包括得有退格键（中等）]</label></div></p>
</li>
</ul>
<h1 id="hua-dong-chuang-kou">滑动窗口</h1>
<p>滑动窗口模式用于对给定数组或链表的特定窗口大小执行所需操作，例如查找包含所有1的最长子序列。滑动窗口从第一个元素开始，每次向右移动一个元素并根据要解决的问题调整窗口的长度。在某些情况下，窗口的大小保持不变，而在其他情况下，大小会增大或缩小。</p>
<p><img src="/2020/05/15/data-structures-and-algorithms/1.png" alt></p>
<h2 id="ying-yong-chang-jing-2">应用场景</h2>
<ul>
<li>问题输入是线性数据结构，如链表、数组或字符串</li>
<li>题目要求查找最长/最短的子字符串、子数组或所需的值</li>
</ul>
<h2 id="ti-mu-2">题目</h2>
<ul class="contains-task-list">
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox17" checked="true" disabled="true"><label for="checkbox17"> <a href="https://www.nowcoder.com/practice/1624bc35a45c42c0bc17d17fa0cba788?tpId=13&amp;tqId=11217&amp;tPage=4&amp;rp=4&amp;ru=/ta/coding-interviews&amp;qru=/ta/coding-interviews/question-ranking" target="_blank" rel="noopener">滑动窗口的最大值(剑指offer)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox18" checked="true" disabled="true"><label for="checkbox18"> <a href="https://leetcode-cn.com/problems/sliding-window-median/" target="_blank" rel="noopener">滑动窗口中位数(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox19" checked="true" disabled="true"><label for="checkbox19"> <a href="https://leetcode-cn.com/problems/minimum-window-substring/" target="_blank" rel="noopener">最小覆盖子串(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox20" checked="true" disabled="true"><label for="checkbox20"> <a href="https://leetcode-cn.com/problems/find-all-anagrams-in-a-string/" target="_blank" rel="noopener">找到字符串中所有字母异位词(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox21" checked="true" disabled="true"><label for="checkbox21"> <a href="https://leetcode-cn.com/problems/longest-substring-without-repeating-characters/" target="_blank" rel="noopener">无重复字符的最长子串(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox22" checked="true" disabled="true"><label for="checkbox22"> <a href="https://leetcode-cn.com/problems/permutation-in-string/" target="_blank" rel="noopener">字符串的排列(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox23" checked="true" disabled="true"><label for="checkbox23"> <a href="https://leetcode-cn.com/problems/partition-labels/" target="_blank" rel="noopener">划分字母区间(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox24" checked="true" disabled="true"><label for="checkbox24"> <a href="https://leetcode-cn.com/problems/fruit-into-baskets/" target="_blank" rel="noopener">水果成篮(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox25" disabled="true"><label for="checkbox25"> <a href="https://leetcode-cn.com/problems/subarrays-with-k-different-integers/" target="_blank" rel="noopener">k个不同整数的子数组</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox26" disabled="true"><label for="checkbox26"> [窗口大小为K的最大子数组和]</label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox27" disabled="true"><label for="checkbox27"> [拥有K个不同的字母的最长子串]</label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox28" disabled="true"><label for="checkbox28"> [字符串的同字母异序词]</label></div></p>
</li>
</ul>
<h2 id="fen-xi-yu-zong-jie">分析与总结</h2>
<ol>
<li>考虑左右边界值的变化。</li>
<li>关注的元素是否有效（有效是指：元素的index是否在窗口范围内）</li>
</ol>
<figure class="highlight python"><figcaption><span>滑动窗口模板</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">slide_window</span><span class="params">(s,t)</span>:</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 窗口: 字典/list/set</span></span><br><span class="line">    window = defaultdict(int)</span><br><span class="line">    <span class="comment"># gold: 字典/list</span></span><br><span class="line">    need = defaultdict(int)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化need</span></span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> t:</span><br><span class="line">        <span class="comment"># need do something</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> right &lt; len(s):</span><br><span class="line">        <span class="comment"># 要添加的字符</span></span><br><span class="line">        c = s[right]</span><br><span class="line">        <span class="comment"># right 指针右移，扩大窗口</span></span><br><span class="line">        right += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对窗口内的数据进行更新操作 1</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判断左侧窗口是否要收缩</span></span><br><span class="line">        <span class="keyword">while</span> (window needs shrink):</span><br><span class="line">            <span class="comment"># d 是将移出窗口的字符</span></span><br><span class="line">            d = s[left]</span><br><span class="line">            <span class="comment"># 左移窗口</span></span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 进行窗口内数据的一系列更新操作 2（与1处保持对称）</span></span><br><span class="line">            <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<h1 id="qu-jian-he-bing">区间合并</h1>
<p>合并间隔模式是<strong>处理重叠间隔</strong>的有效技术。 在涉及间隔的许多问题中，你可以需要找到重叠间隔或合并间隔（如果它们重叠）。给定两个间隔aaa和bbb，可能存在6中不同的间隔交互情况：<br>
<img src="/2020/05/15/data-structures-and-algorithms/4.png" alt></p>
<h2 id="ying-yong-chang-jing-3">应用场景</h2>
<ul>
<li>要求生成仅具有互斥间隔的列表</li>
<li>出现“overlapping intervals”一词</li>
</ul>
<h2 id="ti-mu-3">题目</h2>
<ol>
<li><a href="https://leetcode-cn.com/problems/merge-intervals/" target="_blank" rel="noopener">合并区间（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/meeting-rooms-ii/" target="_blank" rel="noopener">会议室（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/range-module/" target="_blank" rel="noopener">Range模块（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/interval-list-intersections/" target="_blank" rel="noopener">区间列表的交集（LEETCODE）</a></li>
<li>区间交集（中等）</li>
<li>最大化CPU负载（困难）</li>
</ol>
<h1 id="xun-huan-pai-xu">循环排序</h1>
<p>循环排序模式描述了一种处理涉及包含给定范围内的数字的数组问题的有趣方法。其一次遍历数组一个数字，如果正在迭代的当前数字不是正确的索引，则将其与正确索引处的数字交换。</p>
<p><img src="/2020/05/15/data-structures-and-algorithms/5.png" alt></p>
<h2 id="ying-yong-chang-jing-4">应用场景</h2>
<ul>
<li>涉及给定范围内的数字的排序数组</li>
<li>要求在已排序/旋转的数组中找到缺失/重复/最小的数字</li>
</ul>
<h2 id="ti-mu-4">题目</h2>
<ol>
<li><a href="https://leetcode-cn.com/problems/missing-number/" target="_blank" rel="noopener">缺失数字（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/find-the-duplicate-number/" target="_blank" rel="noopener">寻找重复数（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/first-missing-positive/" target="_blank" rel="noopener">缺失的第一个正数（LEETCODE）</a></li>
<li>需要数组中没出现的数字 （简单）</li>
<li>寻找最小的没出现的正整数 （中等）</li>
</ol>
<h1 id="yuan-di-lian-biao-fan-zhuan">原地链表翻转</h1>
<p>在许多问题中，可能会要求我们反转链表的一组节点之间的链接。 通常，约束就是需要就地执行此操作，即使用现有节点对象而不使用额外内存。 这是上述模式有用的地方。</p>
<p>此模式一次反转一个节点，从一个指向链表头部的变量（当前）开始，一个变量（上一个）将指向已处理的上一个节点。 以锁步方式，将通过将当前节点指向前一个节点，然后再转到下一个节点来反转当前节点。 此外，更新变量“previous”以始终指向您已处理的上一个节点。<br>
<img src="/2020/05/15/data-structures-and-algorithms/6.png" alt></p>
<h2 id="ying-yong-chang-jing-5">应用场景</h2>
<ul>
<li>就地反转链表</li>
</ul>
<h2 id="ti-mu-5">题目</h2>
<ol>
<li><a href="https://leetcode-cn.com/problems/reverse-linked-list/" target="_blank" rel="noopener">反转链表（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/reverse-linked-list-ii/" target="_blank" rel="noopener">反转链表II（LEETCODE）</a></li>
</ol>
<h1 id="shu">树</h1>
<h2 id="shu-de-kuan-du-you-xian-sou-suo-tree-bfs">树的宽度优先搜索（Tree BFS）</h2>
<p>该模式基于广度优先搜索（BFS）技术来遍历树，并<span class="label danger">使用队列</span>在跳到下一层之前记录下该层的所有节点。使用这种方法可以有效地解决涉及以逐级顺序遍历树的任何问题。Tree BFS模式的基本思想是将根节点push到队列然后不断迭代直到队列为空。对于每次迭代，删除队列头部的节点并“访问”该节点。从队列中删除每个节点后，我们还将其所有子节点push进队列。</p>
<h3 id="ying-yong-chang-jing-6">应用场景</h3>
<ul>
<li>涉及到层序遍历树</li>
</ul>
<h3 id="ti-mu-6">题目</h3>
<ul class="contains-task-list">
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox29" checked="true" disabled="true"><label for="checkbox29"> <a href="https://leetcode-cn.com/problems/n-ary-tree-level-order-traversal/" target="_blank" rel="noopener">N叉树的层序遍历(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox30" checked="true" disabled="true"><label for="checkbox30"> <a href="https://leetcode-cn.com/problems/binary-tree-level-order-traversal/" target="_blank" rel="noopener">二叉树的层序遍历(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox31" checked="true" disabled="true"><label for="checkbox31"> <a href="https://leetcode-cn.com/problems/binary-tree-zigzag-level-order-traversal/" target="_blank" rel="noopener">二叉树的锯齿形层次遍历(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox32" checked="true" disabled="true"><label for="checkbox32"> <a href="https://leetcode-cn.com/problems/cong-shang-dao-xia-da-yin-er-cha-shu-lcof/" target="_blank" rel="noopener">从上到下打印二叉树(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox33" checked="true" disabled="true"><label for="checkbox33"> <a href="https://leetcode-cn.com/problems/cong-shang-dao-xia-da-yin-er-cha-shu-ii-lcof/" target="_blank" rel="noopener">从上到下打印二叉树 II(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox34" checked="true" disabled="true"><label for="checkbox34"> <a href="https://leetcode-cn.com/problems/cong-shang-dao-xia-da-yin-er-cha-shu-iii-lcof/" target="_blank" rel="noopener">从上到下打印二叉树 III(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox35" checked="true" disabled="true"><label for="checkbox35"> <a href="https://leetcode-cn.com/problems/find-largest-value-in-each-tree-row/" target="_blank" rel="noopener">在每个树行中找最大值(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox36" checked="true" disabled="true"><label for="checkbox36"> <a href="https://leetcode-cn.com/problems/list-of-depth-lcci/" target="_blank" rel="noopener">特定深度节点链表(leetcode)</a></label></div></p>
</li>
</ul>
<h3 id="dai-ma-mo-ban">代码模板</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">levelOrder</span><span class="params">(root)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :type root: TreeNode</span></span><br><span class="line"><span class="string">    :rtype: List[List[int]]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    q = []</span><br><span class="line">    res = []</span><br><span class="line">    q.append(root)</span><br><span class="line">    <span class="keyword">while</span> q:</span><br><span class="line">        temp_q = []</span><br><span class="line">        temp_res = []</span><br><span class="line">        <span class="keyword">while</span> q:</span><br><span class="line">            node = q.pop(<span class="number">0</span>)</span><br><span class="line">            temp_res.append(node.val)</span><br><span class="line">            <span class="keyword">if</span> node.left:</span><br><span class="line">                temp_q.append(node.left)</span><br><span class="line">            <span class="keyword">if</span> node.right:</span><br><span class="line">                temp_q.append(node.right)</span><br><span class="line">        q = temp_q</span><br><span class="line">        res.append(temp_res)</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"><span class="comment"># 如果需要层级信息，加上level</span></span><br></pre></td></tr></table></figure>
<h2 id="shu-de-shen-du-you-xian-sou-suo-tree-dfs">树的深度优先搜索（Tree DFS）</h2>
<p>基于深度优先搜索(DFS)技术来实现树的遍历。可以用递归（如果用迭代方式的话，需要用栈）来记录遍历过程中访问过的父节点。</p>
<p>该模式的运行方式是从根节点开始，如果该节点不是叶子节点，我们需要干三件事：</p>
<ol>
<li>需要区别我们是先处理根节点（pre-order，前序），处理孩子节点之间处理根节点（in-order，中序），还是处理完所有孩子再处理根节点（post-order，后序）。</li>
<li>递归处理当前节点的左右孩子。</li>
</ol>
<h3 id="ying-yong-chang-jing-7">应用场景</h3>
<ul>
<li>涉及树的先序、中序或者后续遍历问题</li>
<li>如果问题涉及搜索节点离叶子更近的目标</li>
</ul>
<h3 id="ti-mu-7">题目</h3>
<ul class="contains-task-list">
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox37" checked="true" disabled="true"><label for="checkbox37"> <a href="https://leetcode-cn.com/problems/sum-root-to-leaf-numbers/" target="_blank" rel="noopener">求根到叶子节点数字之和（leetcode）</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox38" checked="true" disabled="true"><label for="checkbox38"> <a href="https://leetcode-cn.com/problems/path-sum/" target="_blank" rel="noopener">路径总和(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox39" checked="true" disabled="true"><label for="checkbox39"> <a href="https://leetcode-cn.com/problems/path-sum-ii/" target="_blank" rel="noopener">路径总和II(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox40" checked="true" disabled="true"><label for="checkbox40"> <a href="https://leetcode-cn.com/problems/path-sum-iii/" target="_blank" rel="noopener">路径总和III(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox41" checked="true" disabled="true"><label for="checkbox41"> <a href="https://leetcode-cn.com/problems/maximum-depth-of-binary-tree/" target="_blank" rel="noopener">二叉树的最大深度（leetcode）</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox42" checked="true" disabled="true"><label for="checkbox42"> <a href="https://leetcode-cn.com/problems/minimum-depth-of-binary-tree/" target="_blank" rel="noopener">二叉树的最小深度(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox43" checked="true" disabled="true"><label for="checkbox43"> <a href="https://leetcode-cn.com/problems/leaf-similar-trees/" target="_blank" rel="noopener">叶子相似的树(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox44" checked="true" disabled="true"><label for="checkbox44"> <a href="https://leetcode-cn.com/problems/maximum-depth-of-n-ary-tree/" target="_blank" rel="noopener">N叉树的最大深度(leetcode)</a></label></div></p>
</li>
<li class="task-list-item">
<p><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox45" disabled="true"><label for="checkbox45"> <a href="https://leetcode-cn.com/problems/construct-binary-tree-from-inorder-and-postorder-traversal/" target="_blank" rel="noopener">从中序与后序遍历序列构造二叉树（leetcode）</a></label></div></p>
</li>
</ul>
<h3 id="dai-ma">代码</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">res = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self,node,temp_res)</span>:</span></span><br><span class="line">    <span class="comment"># 出口</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 对当前节点的操作</span></span><br><span class="line">  <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 到达叶子节点</span></span><br><span class="line">    <span class="keyword">if</span> node.left <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> node.right <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        res.append(temp_res)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 左节点不为空</span></span><br><span class="line">    <span class="keyword">if</span> node.left:</span><br><span class="line">        dfs(node.left,copy.deepcopy(temp_res))</span><br><span class="line">    <span class="comment"># 有节点不为空</span></span><br><span class="line">    <span class="keyword">if</span> node.right:</span><br><span class="line">        dfs(node.right,copy.deepcopy(temp_res))</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="comment"># 双重递归        </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.status = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pathSum</span><span class="params">(self, root, target)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :type target: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> len(self.status)</span><br><span class="line">        self.dfs(root,target)</span><br><span class="line">        <span class="keyword">if</span> root.left:</span><br><span class="line">            self.pathSum(root.left,target)</span><br><span class="line">        <span class="keyword">if</span> root.right:</span><br><span class="line">            self.pathSum(root.right,target)</span><br><span class="line">        <span class="keyword">return</span> len(self.status)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self, node,target)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        target -= node.val</span><br><span class="line">        <span class="keyword">if</span> target == <span class="number">0</span>:</span><br><span class="line">            self.status.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> node.left:</span><br><span class="line">            self.dfs(node.left, target)</span><br><span class="line">        <span class="keyword">if</span> node.right:</span><br><span class="line">            self.dfs(node.right, target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 非递归解法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxDepth</span><span class="params">(self, root)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :type root: TreeNode</span></span><br><span class="line"><span class="string">    :rtype: int</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># return self.dfs(root,0)</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    stack = []</span><br><span class="line">    max_depth = <span class="number">0</span> <span class="comment"># 记录最优值</span></span><br><span class="line">    stack.append((root,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">while</span> stack:</span><br><span class="line">        node,depth = stack.pop()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> node.right:</span><br><span class="line">            stack.append((node.right,depth+<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">if</span> node.left:</span><br><span class="line">            stack.append((node.left,depth+<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 叶子</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        max_depth = max(depth,max_depth)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> max_depth</span><br></pre></td></tr></table></figure>
<h1 id="shuang-dui-mo-shi">双堆模式</h1>
<p>在许多问题中，给出了一系列元素，需要我们将其分成两部分。 为了解决这个问题，我们想要知道一个部分中的最小元素和另一个部分中的最大元素。 这种模式是解决此类问题的有效方法。</p>
<p>这种模式使用两个堆：找到最小元素的Min Heap和找到最大元素的Max Heap。 该模式的工作原理是将前半部分的数字存储在Max Heap中，这是因为我们希望在上半部分找到最大的数字。 然后将数字的后半部分存储在Min Heap中，因为我们希望在后半部分找到最小的数字。 在任何时候，可以从两个堆的顶部元素计算当前数字列表的中值。</p>
<h2 id="ying-yong-chang-jing-8">应用场景</h2>
<ul>
<li>优先队列，调度等情况</li>
<li>找到集合中的最小/最大/中值元素</li>
<li>有时，在以二叉树数据结构为特征的问题中很有用</li>
</ul>
<h2 id="ti-mu-8">题目</h2>
<ol>
<li><a href="https://leetcode-cn.com/problems/find-median-from-data-stream/" target="_blank" rel="noopener">数据流的中位数（LEETCODE）</a></li>
<li><a href="https://www.nowcoder.com/practice/1624bc35a45c42c0bc17d17fa0cba788?tpId=13&amp;tqId=11217&amp;tPage=4&amp;rp=4&amp;ru=/ta/coding-interviews&amp;qru=/ta/coding-interviews/question-ranking" target="_blank" rel="noopener">滑动窗口的最大值（剑指offer）</a></li>
</ol>
<h1 id="zi-ji-wen-ti-mo-shi">子集问题模式</h1>
<p>大量的编程面试问题涉及处理一组给定元素的排列和组合。 Subsets模式描述了一种有效的广度优先搜索（BFS）方法来处理所有这些问题。</p>
<p>例如给定一个数组 [1, 5, 3]，</p>
<ol>
<li>首先初始化一个空数组： [[ ]]</li>
<li>将第一个数字(1)添加到所有现有子集，以创建新的子集: [[], [1]]</li>
<li>继续添加[[], [1], [5], [1, 5]]</li>
<li>[[], [1], [5], [1, 5], [3], [1, 3], [5, 3], [1, 5, 3]]</li>
</ol>
<p><img src="/2020/05/15/data-structures-and-algorithms/10.png" alt></p>
<h2 id="ying-yong-chang-jing-9">应用场景</h2>
<ul>
<li>需要找到给定集合的组合或排列的问题</li>
</ul>
<h2 id="ti-mu-9">题目</h2>
<ol>
<li><a href="https://leetcode-cn.com/problems/subsets/" target="_blank" rel="noopener">子集系列（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/letter-case-permutation/" target="_blank" rel="noopener">字母大小写全排列（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/generalized-abbreviation/" target="_blank" rel="noopener">列举单词的全部缩写（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/word-subsets/" target="_blank" rel="noopener">单词子集（LEETCODE）</a></li>
</ol>
<h1 id="er-fen-bian-chong">二分变种</h1>
<p>无论何时给定排序数组，链表或矩阵，并要求查找某个元素，你可以使用的最佳算法是二分搜索。此模式描述了处理涉及二分搜索的所有问题的有效方法。这种模式的步骤是这样的：</p>
<ol>
<li>首先，算出左右端点的中点。最简单的方式是这样的：middle = (start + end) / 2。但这种计算方式有不小的概率会出现整数越界。因此一般都推荐另外这种写法：middle = start + (end — start) / 2</li>
<li>如果要找的目标改好和中点所在的数值相等，我们返回中点的下标就行</li>
<li>如果目标不等的话：我们就有两种移动方式了</li>
<li>如果目标比中点在的值小（key &lt; arr[middle]）：将下一步搜索空间放到左边（end = middle - 1）</li>
<li>如果比中点的值大，则继续在右边搜索，丢弃左边：left = middle + 1</li>
</ol>
<p><img src="/2020/05/15/data-structures-and-algorithms/11.png" alt></p>
<h2 id="ti-mu-10">题目</h2>
<ul class="contains-task-list">
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox46" checked="true" disabled="true"><label for="checkbox46"> <a href="https://leetcode-cn.com/problems/binary-search/" target="_blank" rel="noopener">二分查找(leetcode)</a></label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox47" checked="true" disabled="true"><label for="checkbox47"> <a href="https://leetcode-cn.com/problems/search-in-rotated-sorted-array/" target="_blank" rel="noopener">搜索旋转排序数组（leetcode）</a></label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox48" checked="true" disabled="true"><label for="checkbox48"> <a href="https://leetcode-cn.com/problems/search-in-rotated-sorted-array-ii" target="_blank" rel="noopener">搜索旋转排序数组 II(leetcode)</a></label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox49" checked="true" disabled="true"><label for="checkbox49"> <a href="https://leetcode-cn.com/problems/find-first-and-last-position-of-element-in-sorted-array/" target="_blank" rel="noopener">在排序数组中查找元素的第一个和最后一个位置(leetcode)</a></label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox50" disabled="true"><label for="checkbox50"> <a href="https://leetcode-cn.com/problems/median-of-two-sorted-arrays/" target="_blank" rel="noopener">寻找两个有序数组的中位数（leetcode）</a></label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox51" checked="true" disabled="true"><label for="checkbox51"> <a href="https://leetcode-cn.com/problems/find-minimum-in-rotated-sorted-array/" target="_blank" rel="noopener">寻找旋转排序数组中的最小值（leetcode）</a></label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox52" disabled="true"><label for="checkbox52"> <a href="https://leetcode-cn.com/problems/find-minimum-in-rotated-sorted-array-ii" target="_blank" rel="noopener">寻找旋转排序数组中的最小值 II(leetcode)</a></label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox53" disabled="true"><label for="checkbox53"> 顺序未知的二分（可能翻转过了，简单）</label></div></li>
<li class="task-list-item"><div class="task-list-item-checkbox"><input type="checkbox" id="checkbox54" disabled="true"><label for="checkbox54"> 无界排序数组的二分（中等）</label></div></li>
</ul>
<h2 id="dai-ma-mo-ban-1">代码模板</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span><span class="params">(nums, target)</span>:</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = len(nums) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        mid = left + (right - left) / <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] &gt; target:</span><br><span class="line">            right = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] == target:</span><br><span class="line">            <span class="comment"># 直接返回（适合列表中不存在重复元素）</span></span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search_left</span><span class="params">(nums, target)</span>:</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = len(nums) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        mid = left + (right - left) / <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] &gt; target:</span><br><span class="line">            right = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] == target:</span><br><span class="line">            <span class="comment"># 向左边界搜索,用right</span></span><br><span class="line">            right = mid <span class="number">-1</span></span><br><span class="line">    <span class="comment"># 检查是否越界</span></span><br><span class="line">    <span class="keyword">if</span> left &gt;= len(nums) <span class="keyword">or</span> <span class="keyword">not</span> nums[left] == target:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">return</span> left</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search_right</span><span class="params">(nums,target)</span>:</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = len(nums) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        mid = left + (right - left) / <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] &gt; target:</span><br><span class="line">            right = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] == target:</span><br><span class="line">            <span class="comment"># 向右边界搜索，用left</span></span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">    <span class="comment"># 检查是否越界</span></span><br><span class="line">    <span class="keyword">if</span> right &lt; <span class="number">0</span> <span class="keyword">or</span> <span class="keyword">not</span> nums[right] == target:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">return</span> right</span><br></pre></td></tr></table></figure>
<h1 id="top-k">Top K</h1>
<p>任何要求我们在给定集合中找到最大/最小/频繁“K”元素的问题都属于这种模式。</p>
<p>跟踪<code>K</code>元素的最佳数据结构是Heap。这种模式将利用Heap来解决从一组给定元素一次处理<code>K</code>元素的多个问题。大致思路是这样的：</p>
<ul>
<li>根据问题将<code>K</code>元素插入到最小堆或最大堆中</li>
<li>迭代剩余的数字，如果找到一个比堆中的数字大的数字，则删除该数字并插入较大的数字</li>
</ul>
<p><img src="/2020/05/15/data-structures-and-algorithms/12.png" alt></p>
<h2 id="ying-yong-chang-jing-10">应用场景</h2>
<ul>
<li>要求找到给定集合的最大/最小/频繁<code>K</code>元素；</li>
<li>要求对数组进行排序以找到确切的元素</li>
</ul>
<h2 id="ju-ge-li-zi">举个栗子</h2>
<ol>
<li><a href="https://leetcode-cn.com/problems/top-k-frequent-elements/" target="_blank" rel="noopener">前K个高频元素（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/top-k-frequent-words/" target="_blank" rel="noopener">前K个高频单词（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/permutation-sequence/" target="_blank" rel="noopener">第k个排列（LEETCODE）</a></li>
<li>前K大的数（简单）</li>
<li>前K个最常出现的数字（中等）</li>
</ol>
<h2 id="fen-xi-zong-jie">分析总结</h2>
<ol>
<li>当涉及到top k问题的时候，可以考虑一下<code>堆</code>这种数据结构<code>python: import heapq</code>，python构建最大堆，可以通过给list中的元素加 <code>负号</code> 的方式实现。</li>
</ol>
<h1 id="k-lu-gui-bing">K路归并</h1>
<p>K路归并能解决那些涉及到多组排好序的数组的问题。</p>
<p>该模式是这样的运行的：</p>
<ol>
<li>把每个数组中的第一个元素都加入最小堆中</li>
<li>取出堆顶元素（全局最小），将该元素放入排好序的结果集合里面</li>
<li>将刚取出的元素所在的数组里面的下一个元素加入堆</li>
<li>重复步骤2，3，直到处理完所有数字</li>
</ol>
<p><img src="/2020/05/15/data-structures-and-algorithms/13.png" alt></p>
<h2 id="ying-yong-chang-jing-11">应用场景</h2>
<ul>
<li>适用于排序的数组，列表或矩阵</li>
<li>问题要求合并排序列表，在排序列表中查找最小元素等</li>
</ul>
<h2 id="ti-mu-11">题目</h2>
<ol>
<li><a href="https://leetcode-cn.com/problems/merge-two-sorted-lists/" target="_blank" rel="noopener">合并两个有序链表（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/merge-k-sorted-lists/" target="_blank" rel="noopener">合并K个排序链表（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/ugly-number-ii/" target="_blank" rel="noopener">丑数系列（LEETCODE）</a></li>
<li>合并K个排好序的链表（中等）</li>
<li>K对数和最大（困难）</li>
</ol>
<h1 id="tuo-bu-pai-xu-mo-shi">拓扑排序模式</h1>
<p>拓扑排序用于查找彼此依赖的元素的线性排序。例如，如果事件“B”依赖于事件“A”，则“A”在拓扑排序中位于“B”之前。流程大概是这样的：</p>
<ol>
<li>初始化
<ul>
<li>借助于HashMap将图保存成邻接表形式。</li>
<li>找到所有的起点，用HashMap来帮助记录每个节点的入度</li>
</ul>
</li>
<li>创建图，找到每个节点的入度
<ul>
<li>利用输入，把图建好，然后遍历一下图，将入度信息记录在HashMap中</li>
</ul>
</li>
<li>找所有的起点
<ul>
<li>所有入度为0的节点，都是有效的起点，而且我们讲他们都加入到一个队列中</li>
</ul>
</li>
<li>排序
<ul>
<li>对每个起点，执行以下步骤
<ul>
<li>把它加到结果的顺序中</li>
<li>将其在图中的孩子节点取到</li>
<li>将其孩子的入度减少1</li>
<li>如果孩子的入度变为0，则改孩子节点成为起点，将其加入队列中</li>
</ul>
</li>
<li>重复上面4步过程，直到起点队列为空。</li>
</ul>
</li>
</ol>
<p><img src="/2020/05/15/data-structures-and-algorithms/14.png" alt></p>
<h2 id="ying-yong-chang-jing-12">应用场景</h2>
<ul>
<li>需要处理没有定向循环的图</li>
<li>要求按排序顺序更新所有对象</li>
<li>如果有一组遵循特定顺序的对象</li>
</ul>
<h2 id="ti-mu-12">题目</h2>
<ol>
<li><a href="https://leetcode-cn.com/problems/course-schedule/" target="_blank" rel="noopener">课程表系列（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/longest-increasing-path-in-a-matrix/" target="_blank" rel="noopener">矩阵中的最长递增路径（LEETCODE）</a></li>
<li><a href="https://leetcode-cn.com/problems/sequence-reconstruction/" target="_blank" rel="noopener">序列重建（LEETCODE）</a></li>
<li>任务执行顺序安排（中等）</li>
<li>树的最小高度（困难）</li>
</ol>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://medium.com/hackernoon/14-patterns-to-ace-any-coding-interview-question-c5bb3357f6ed" target="_blank" rel="noopener">14 Patterns to Ace Any Coding Interview Question</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/90664857" target="_blank" rel="noopener">码农找工作之：秒杀算法面试必须掌握的14种模式</a></li>
<li><a href="https://labuladong.gitbook.io/algo/" target="_blank" rel="noopener">labuladong的算法小抄</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>bert源码</title>
    <url>/2020/05/13/research/bert_code/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/13/research/bert_code/bert.jpg" alt></p>
<a id="more"></a>
<h1 id="jian-jie">简介</h1>
<p>BERT——来自<strong>Transformer的双向编码器表征</strong>。与最近的语言表征模型不同，BERT旨在基于<strong>所有层</strong>的<strong>左、右语境</strong>来预训练深度双向表征。BERT是首个在<strong>大批句子层面</strong>和<strong>token层面</strong>任务中取得当前最优性能的<strong>基于微调的表征模型</strong>，其性能超越许多使用任务特定架构的系统，刷新了11项NLP任务的最优性能记录。</p>
<h2 id="motivation">motivation</h2>
<p>作者认为现有的技术严重制约了预训练表征的能力，其主要局限在于语言模型是<strong>单向</strong>的，例如，OpenAI GPT使用的是<strong>从左到右</strong>的架构，其中<strong>每个token只能注意Transformer自注意力层中的先前token</strong>。这些局限对于<strong>句子层面的任务</strong>而言不是最佳选择，对于<strong>token级任务</strong>则可能是毁灭性的，<strong>因为在这种任务中，结合两个方向的语境至关重要</strong>BERT（Bidirectional Encoder Representations from Transformers）改进了<strong>基于微调的策略</strong>。</p>
<h2 id="solution">solution</h2>
<p>BERT提出一种新的<strong>预训练目标</strong>——<strong>遮蔽语言模型（masked language model，MLM）</strong>，来克服上文提到的单向局限。MLM<strong>随机遮蔽输入中的一些token</strong>，通过遮蔽词的语境来<strong>预测其原始词汇id</strong>。与从左到右的语言模型预训练不同，MLM目标<strong>允许表征融合左右两侧的语境</strong>，从而预训练一个深度<strong>双向Transformer</strong>。除了 MLM，还引入了一个**“下一句预测”（next sentence prediction）任务**，该任务<strong>联合预训练</strong>文本对表征。</p>
<h2 id="contribution">contribution</h2>
<ol>
<li>Bert模型的<strong>双向特性是最重要的一项新贡献</strong></li>
<li>BERT是首个在大批句子层面和token层面任务中取得当前最优性能的基于微调的表征模型，其性能超越许多使用任务特定架构的系统。证明了<strong>预训练表征</strong>可以<strong>消除对许多精心设计的任务特定架构的需求</strong>。</li>
</ol>
<p><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">论文地址</a></p>
<h1 id="mo-xing-jia-gou">模型架构</h1>
<p>BERT 旨在基于所有层的左、右语境来预训练深度双向表征。因此，预训练的 BERT 表征可以仅用一个额外的输出层进行微调，进而为很多任务创建当前最优模型，无需对任务特定架构做出大量修改。</p>
<h2 id="base-amp-large">Base &amp; Large</h2>
<blockquote>
<p>\(BERT_{Base}:L=12,H=768,A=12,Total Parameters=110M\)</p>
<p>\(BERT_{Large}:L=24,H=1024,A=16,Total Parameters=340M\)</p>
</blockquote>
<p>其中，\(L\):表示层数，\(H\):表示隐藏层的size，\(A\):表示自注意力head的个数。feed-forward的size为\(4H\),即\(H=768\)时为3072，\(H=1024\)时为4096。</p>
<h2 id="bert-amp-open-ai-gpt-amp-el-mo">BERT &amp; OpenAI GPT &amp; ELMo</h2>
<p>\(BERT_{Base}\)和OpenAI GPT的大小是一样的。BERT Transformer使用<strong>双向自注意力机制</strong>，而GPT Transformer使用受限的自注意力机制，导致每个token只能关注其左侧的语境。双向Transformer在文献中通常称为**“Transformer 编码器”<strong>，而只</strong>关注左侧语境的版本<strong>则因能用于文本生成而被称为</strong>“Transformer 解码器”**。</p>
<p><img src="/2020/05/13/research/bert_code/bert-gpt-transformer-elmo.png" alt></p>
<ul>
<li>BERT 使用双向Transformer</li>
<li>OpenAI GPT 使用从左到右的Transformer</li>
<li>ELMo 使用独立训练的从左到右和从右到左LSTM的级联来生成下游任务的特征。</li>
</ul>
<h2 id="pre-training-tasks">Pre-training Tasks</h2>
<h3 id="task-1-masked-lm">Task1:Masked LM</h3>
<p>BERT训练双向语言模型时以较小的概率把少量的词替成了Mask或者另一个随机的词。其目的在于使模型被迫增加对上下文的记忆。标准语言模型只能从左到右或从右到左进行训练，使得每个单词在多层上下文中<strong>间接</strong>地“see itself”。</p>
<p>为了训练一个深度双向表示，研究团队采用了一种简单的方法，即随机屏蔽（masking）部分输入token，然后只预测那些被屏蔽的token。论文将这个过程称为“masked LM”(MLM)。与masked token对应的最终隐藏向量被输入到词汇表上的输出softmax中。</p>
<p>在实验中，随机地Mask每个序列中15%的WordPiece token。</p>
<p>虽然这确实能获得双向预训练模型，但这种方法有两个缺点。</p>
<ul>
<li>
<p>缺点1：预训练和finetuning之间不匹配，因为在finetuning期间从未看到<code>[MASK]</code>token。</p>
<p>为了解决这个问题，并不总是用实际的<code>[MASK]</code>token替换被“masked”的词汇。使用训练数据生成器<strong>随机选择15％的token</strong>。例如在这个句子“my dog is hairy”中，它选择的token是“hairy”。然后，执行以下过程：</p>
<ul>
<li>80％的时间：<strong>用<code>[MASK]</code>标记替换单词</strong>，例如，<code>my dog is hairy → my dog is [MASK]</code></li>
<li>10％的时间：用一个<strong>随机的单词</strong>替换该单词，例如，<code>my dog is hairy → my dog is apple</code></li>
<li>10％的时间：<strong>保持单词不变</strong>，例如，<code>my dog is hairy → my dog is hairy</code>. 这样做的目的是将表示偏向于实际观察到的单词。</li>
</ul>
<p>由于Transformer encoder不知道它将被要求预测哪些单词或哪些单词已被随机单词替换，因此它<strong>被迫保持每个输入token的分布式上下文表示</strong>。此外，因为随机替换只发生在所有token的1.5％（即<strong>15％的10％</strong>），这不会损害模型的语言理解能力。</p>
</li>
<li>
<p>缺点2：每个batch只预测了15％的token，这表明模型可能需要更多的预训练步骤才能收敛。</p>
<p>MLM的收敛速度略慢于 left-to-right的模型（预测每个token），但MLM模型在实验上获得的提升远远超过增加的训练成本。</p>
</li>
</ul>
<h3 id="task-2-next-sentence-prediction">Task2:Next Sentence Prediction</h3>
<p>为了训练一个<strong>理解句子关系</strong>的模型，预先训练一个二分类的下一句测任务，这一任务可以从任何单语语料库中生成。具体地说，当选择句子A和B作为预训练样本时，B有50％的可能是A的下一个句子，也有50％的可能是来自语料库的随机句子。（实际bert后续的研究中，表明这个任务在理解句子关系表现的不是很理想。）例如：</p>
<figure class="highlight inform7"><table><tr><td class="code"><pre><span class="line">Input = <span class="comment">[CLS]</span> the <span class="keyword">man</span> went to <span class="comment">[MASK]</span> store <span class="comment">[SEP]</span> he bought a gallon <span class="comment">[MASK]</span> milk <span class="comment">[SEP]</span></span><br><span class="line">Label = IsNext</span><br><span class="line"></span><br><span class="line">Input = <span class="comment">[CLS]</span> the <span class="keyword">man</span> <span class="comment">[MASK]</span> to the store <span class="comment">[SEP]</span> penguin <span class="comment">[MASK]</span> <span class="keyword">are</span> flight ##less birds <span class="comment">[SEP]</span></span><br><span class="line">Label = NotNext</span><br></pre></td></tr></table></figure>
<h1 id="dai-ma-shi-xian-pytorch">代码实现（pytorch）</h1>
<p>参考<a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener">transformers</a></p>
<h2 id="yu-xun-lian-mo-xing">预训练模型</h2>
<p><a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">bert中文预训练模型下载</a>，解压缩之后，会有三个文件：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bert_config.json <span class="comment"># 模型配置文件</span></span><br><span class="line">pytorch_model.bin <span class="comment"># 预训练好的模型</span></span><br><span class="line">vocab.txt <span class="comment">#词汇表</span></span><br></pre></td></tr></table></figure>
<p>vocab.txt是模型的词典，这个文件会经常要用到。<em>bert_config.json</em>是BERT的配置(超参数)，比如网络的层数，通常不需要修改，如果自己显存小的话，也可以调小一下bert的层数。pytorch_model是预训练好的模型的模型参数，Fine-Tuning模型的初始值就是来自这个文件，然后根据不同的任务进行Fine-Tuning。</p>
<figure class="highlight"><figcaption><span>bert_config.json</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "attention_probs_dropout_prob": 0.1,  #乘法attention时，softmax后dropout概率 </span><br><span class="line">  "directionality": "bidi", </span><br><span class="line">  "hidden_act": "gelu",  #激活函数 </span><br><span class="line">  "hidden_dropout_prob": 0.1,  #隐藏层dropout概率 </span><br><span class="line">  "hidden_size": 768,  #隐藏单元数 (embedding_size)</span><br><span class="line">  "initializer_range": 0.02,   #初始化范围 </span><br><span class="line">  "intermediate_size": 3072,  #升维维度</span><br><span class="line">  "max_position_embeddings": 512,  #用于生成position_embedding。输入序列长度（seq_len）不能超过512</span><br><span class="line">  "num_attention_heads": 12,  #每个隐藏层中的attention head数 (则，每个head的embedding_size=768/12=64)</span><br><span class="line">  "num_hidden_layers": 12, #隐藏层数 </span><br><span class="line">  "pooler_fc_size": 768, </span><br><span class="line">  "pooler_num_attention_heads": 12, </span><br><span class="line">  "pooler_num_fc_layers": 3, </span><br><span class="line">  "pooler_size_per_head": 128, </span><br><span class="line">  "pooler_type": "first_token_transform", </span><br><span class="line">  "type_vocab_size": 2,  #segment_ids类别 [0,1] </span><br><span class="line">  "vocab_size": 21128 #词典中词数</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>vocab.txt中的部分内容</p>
<figure class="highlight plain"><figcaption><span>vocab.txt</span></figcaption><table><tr><td class="code"><pre><span class="line">馬</span><br><span class="line">高</span><br><span class="line">龍</span><br><span class="line">龸</span><br><span class="line">ﬁ</span><br><span class="line">ﬂ</span><br><span class="line">！</span><br><span class="line">（</span><br><span class="line">）</span><br><span class="line">，</span><br><span class="line">－</span><br><span class="line">．</span><br><span class="line">／</span><br><span class="line">：</span><br><span class="line">？</span><br><span class="line">～</span><br><span class="line">the</span><br><span class="line">of</span><br><span class="line">and</span><br><span class="line">in</span><br><span class="line">to</span><br></pre></td></tr></table></figure>
<h2 id="data">Data</h2>
<h3 id="step-1-read-example-from-file">step 1: read example from file</h3>
<figure class="highlight python"><figcaption><span>read_examples_from_file</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InputExample</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A single training/test example for token classification.</span></span><br><span class="line"><span class="string">	用于保存当个样本实例。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        guid: 样本id.</span></span><br><span class="line"><span class="string">        words: 文本序列，list类型：[word1,word2,...wordn]. </span></span><br><span class="line"><span class="string">        labels: 序列对应的label， This should be specified for train and dev examples, but not for test examples.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 由于语法糖@dataclass,会自动对该类添加__init__()函数。</span></span><br><span class="line">    guid: str</span><br><span class="line">    words: List[str]</span><br><span class="line">    labels: Optional[List[str]]</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_examples_from_file</span><span class="params">(data_dir, mode: Union[Split, str])</span> -&gt; List[InputExample]:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    data_dir: 原始数据所在的文件夹</span></span><br><span class="line"><span class="string">    model：‘train’(‘valid’)对应的数据会有label，‘test’数据的label为0</span></span><br><span class="line"><span class="string">    return：</span></span><br><span class="line"><span class="string">    	examples：list类型，包含train or valid or test 中的所有样本。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(mode, Split):</span><br><span class="line">        mode = mode.value</span><br><span class="line">    file_path = os.path.join(data_dir, <span class="string">f"<span class="subst">&#123;mode&#125;</span>.txt"</span>)</span><br><span class="line">    guid_index = <span class="number">1</span></span><br><span class="line">    examples = []</span><br><span class="line">    <span class="keyword">with</span> open(file_path, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        words = []</span><br><span class="line">        labels = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            <span class="keyword">if</span> line.startswith(<span class="string">"-DOCSTART-"</span>) <span class="keyword">or</span> line == <span class="string">""</span> <span class="keyword">or</span> line == <span class="string">"\n"</span>:</span><br><span class="line">                <span class="keyword">if</span> words:</span><br><span class="line">                    examples.append(InputExample(guid=<span class="string">f"<span class="subst">&#123;mode&#125;</span>-<span class="subst">&#123;guid_index&#125;</span>"</span>, words=words, labels=labels))</span><br><span class="line">                    guid_index += <span class="number">1</span></span><br><span class="line">                    words = []</span><br><span class="line">                    labels = []</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                splits = line.split(<span class="string">" "</span>)</span><br><span class="line">                words.append(splits[<span class="number">0</span>])</span><br><span class="line">                <span class="keyword">if</span> len(splits) &gt; <span class="number">1</span>:</span><br><span class="line">                    labels.append(splits[<span class="number">-1</span>].replace(<span class="string">"\n"</span>, <span class="string">""</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># Examples could have no label for mode = "test"</span></span><br><span class="line">                    labels.append(<span class="string">"O"</span>)</span><br><span class="line">        <span class="keyword">if</span> words:</span><br><span class="line">            examples.append(InputExample(guid=<span class="string">f"<span class="subst">&#123;mode&#125;</span>-<span class="subst">&#123;guid_index&#125;</span>"</span>, words=words, labels=labels))</span><br><span class="line">    <span class="keyword">return</span> examples</span><br></pre></td></tr></table></figure>
<h3 id="step-2-convert-example-into-feature">step 2:convert example into feature</h3>
<h4 id="basic-tokenizer">BasicTokenizer</h4>
<p>在转化成feature之前需要先了解一下bert的分词：</p>
<ol>
<li>对于文本序列中不想被切分开的词可以放到一个列表中，通过never_split传入分词函数中，就不会对列表中的文字进行分词。</li>
<li>中文序列标注任务需要注意：<code>'\t', '\n', '\r'，' '</code>会被替换成空白字符<code>' '</code>添加到文本中，英文文本对文本进行分词的时候又会根据空白字符来进行分词，导致空白字符会被直接清洗掉，这样就会导致中文序列标注的标签中的<code>'\t', '\n', '\r'，' '</code>在分词之后找不到对应的位置。</li>
</ol>
<figure class="highlight python"><figcaption><span>BasicTokenizer</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Runs basic tokenization (punctuation splitting, lower casing, etc.)."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, do_lower_case=True, never_split=None, tokenize_chinese_chars=True)</span>:</span></span><br><span class="line">        <span class="string">""" Constructs a BasicTokenizer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            **do_lower_case**: Whether to lower case the input.</span></span><br><span class="line"><span class="string">            **never_split**: (`optional`) List of token not to split.</span></span><br><span class="line"><span class="string">            **tokenize_chinese_chars**: (`optional`),是否对中文分词，默认是true</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> never_split <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            never_split = []</span><br><span class="line">        self.do_lower_case = do_lower_case</span><br><span class="line">        self.never_split = never_split</span><br><span class="line">        self.tokenize_chinese_chars = tokenize_chinese_chars</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text, never_split=None)</span>:</span></span><br><span class="line">        <span class="string">""" Basic Tokenization of a piece of text.</span></span><br><span class="line"><span class="string">            仅仅根据 空白字符 来进行分词</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            **never_split**: (`optional`)，对哪些文字不进行分词，[person,people,..],如果正常分词的话，person 会被切成per ##son，但是person如果被放入这个列表中，就不会被切分开</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        never_split = self.never_split + (never_split <span class="keyword">if</span> never_split <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> [])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 注意： 非法中文字符会被从文本中删除，以及‘\t’, ‘\n’, and ‘\r’，‘ ’会被替换成空白字符‘ ’添加到文本中，对文本进行分词的时候又会根据空白字符来进行分词，导致空白字符会被直接清洗掉，对于序列标注任务来说，会造成文本与标签对不齐的问题。</span></span><br><span class="line">        text = self._clean_text(text)</span><br><span class="line">        <span class="keyword">if</span> self.tokenize_chinese_chars:</span><br><span class="line">            <span class="comment"># 对中文字符按 字 进行分词，对中文分词过滤一遍之后在对英文字符进行分词。</span></span><br><span class="line">            text = self._tokenize_chinese_chars(text)</span><br><span class="line">            </span><br><span class="line">        orig_tokens = whitespace_tokenize(text) <span class="comment"># 先按照空白字符进行分词。</span></span><br><span class="line">        split_tokens = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> orig_tokens:</span><br><span class="line">            <span class="keyword">if</span> self.do_lower_case <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> never_split:</span><br><span class="line">                token = token.lower()</span><br><span class="line">                token = self._run_strip_accents(token)</span><br><span class="line">            split_tokens.extend(self._run_split_on_punc(token, never_split))</span><br><span class="line">        output_tokens = whitespace_tokenize(<span class="string">" "</span>.join(split_tokens))</span><br><span class="line">        <span class="keyword">return</span> output_tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run_strip_accents</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="string">"""Strips accents from a piece of text."""</span></span><br><span class="line">        text = unicodedata.normalize(<span class="string">"NFD"</span>, text)</span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">            cat = unicodedata.category(char)</span><br><span class="line">            <span class="keyword">if</span> cat == <span class="string">"Mn"</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            output.append(char)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>.join(output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run_split_on_punc</span><span class="params">(self, text, never_split=None)</span>:</span></span><br><span class="line">        <span class="string">"""Splits punctuation on a piece of text."""</span></span><br><span class="line">        <span class="keyword">if</span> never_split <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> text <span class="keyword">in</span> never_split:</span><br><span class="line">            <span class="keyword">return</span> [text]</span><br><span class="line">        chars = list(text)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        start_new_word = <span class="literal">True</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">while</span> i &lt; len(chars):</span><br><span class="line">            char = chars[i]</span><br><span class="line">            <span class="keyword">if</span> _is_punctuation(char):</span><br><span class="line">                output.append([char])</span><br><span class="line">                start_new_word = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> start_new_word:</span><br><span class="line">                    output.append([])</span><br><span class="line">                start_new_word = <span class="literal">False</span></span><br><span class="line">                output[<span class="number">-1</span>].append(char)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [<span class="string">""</span>.join(x) <span class="keyword">for</span> x <span class="keyword">in</span> output]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_tokenize_chinese_chars</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="string">"""对中文合法字符按字进行分割。"""</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">            cp = ord(char)</span><br><span class="line">            <span class="keyword">if</span> self._is_chinese_char(cp):</span><br><span class="line">                output.append(<span class="string">" "</span>)</span><br><span class="line">                output.append(char)</span><br><span class="line">                output.append(<span class="string">" "</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output.append(char)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>.join(output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_is_chinese_char</span><span class="params">(self, cp)</span>:</span></span><br><span class="line">        <span class="string">"""判断cp是否是中文合法字符</span></span><br><span class="line"><span class="string">        0x4e00-0x9fff cjk 统一字型 常用字 共 20992个（实际只定义到0x9fc3)</span></span><br><span class="line"><span class="string">        0x3400-0x4dff cjk 统一字型扩展表a 少用字 共 6656个</span></span><br><span class="line"><span class="string">        0x20000-0x2a6df cjk 统一字型扩展表b 少用字，历史上使用 共42720个</span></span><br><span class="line"><span class="string">        0xf900-0xfaff cjk 兼容字型 重复字，可统一变体，共同字 共512个</span></span><br><span class="line"><span class="string">        0x2f800-0x2fa1f cjk 兼容字型补遗 可统一变体 共544个</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> (</span><br><span class="line">            (cp &gt;= <span class="number">0x4E00</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x9FFF</span>)</span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x3400</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x4DBF</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x20000</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2A6DF</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2A700</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2B73F</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2B740</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2B81F</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2B820</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2CEAF</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0xF900</span> <span class="keyword">and</span> cp &lt;= <span class="number">0xFAFF</span>)</span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2F800</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2FA1F</span>)  <span class="comment">#</span></span><br><span class="line">        ):  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_clean_text</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="string">"""过滤掉非法字符，把`'\t', '\n', '\r'，' '`会被替换成空白字符`' '`"""</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">            cp = ord(char)</span><br><span class="line">            <span class="keyword">if</span> cp == <span class="number">0</span> <span class="keyword">or</span> cp == <span class="number">0xFFFD</span> <span class="keyword">or</span> _is_control(char):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> _is_whitespace(char):</span><br><span class="line">                output.append(<span class="string">" "</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output.append(char)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>.join(output)</span><br></pre></td></tr></table></figure>
<h4 id="wordpiece-tokenizer">WordpieceTokenizer</h4>
<p>经过<code>BasicTokenizer</code>处理成空格隔开的单词之后，还需要在经过WordpieceTokenizer对英文单词进行更细粒度的切分：</p>
<blockquote>
<p>input = “unaffable”</p>
<p>output = [“un”, “##aff”, “##able”]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordpieceTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Runs WordPiece tokenization."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab, unk_token, max_input_chars_per_word=<span class="number">100</span>)</span>:</span></span><br><span class="line">        self.vocab = vocab</span><br><span class="line">        self.unk_token = unk_token</span><br><span class="line">        self.max_input_chars_per_word = max_input_chars_per_word</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        For example:input = "unaffable"output = ["un", "##aff", "##able"]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          text: 经过`BasicTokenizer`分词之后，</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">          A list of wordpiece tokens.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        output_tokens = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> whitespace_tokenize(text):</span><br><span class="line">            chars = list(token)</span><br><span class="line">            <span class="keyword">if</span> len(chars) &gt; self.max_input_chars_per_word:</span><br><span class="line">                output_tokens.append(self.unk_token)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            is_bad = <span class="literal">False</span></span><br><span class="line">            start = <span class="number">0</span></span><br><span class="line">            sub_tokens = []</span><br><span class="line">            <span class="keyword">while</span> start &lt; len(chars):</span><br><span class="line">                end = len(chars)</span><br><span class="line">                cur_substr = <span class="literal">None</span></span><br><span class="line">                <span class="keyword">while</span> start &lt; end:</span><br><span class="line">                    substr = <span class="string">""</span>.join(chars[start:end])</span><br><span class="line">                    <span class="keyword">if</span> start &gt; <span class="number">0</span>:</span><br><span class="line">                        substr = <span class="string">"##"</span> + substr</span><br><span class="line">                    <span class="keyword">if</span> substr <span class="keyword">in</span> self.vocab: <span class="comment"># 如果在词汇表中存在该wordpiece</span></span><br><span class="line">                        cur_substr = substr</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    end -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> cur_substr <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    is_bad = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                sub_tokens.append(cur_substr)</span><br><span class="line">                start = end</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> is_bad:</span><br><span class="line">                output_tokens.append(self.unk_token)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output_tokens.extend(sub_tokens)</span><br><span class="line">        <span class="keyword">return</span> output_tokens</span><br></pre></td></tr></table></figure>
<h4 id="bert-tokenizer">BertTokenizer</h4>
<p><code>BertTokenizer</code>基于<code>WordPiece</code>，需要注意的地方是，对于中文文本来说，中文文本是没有空格分隔的文本，所以是需要在WordPiece之前do_basic_tokenize，也就是对于中文来说，需要<code>do_basic_tokenize=True</code>,不然对于没有分隔的中文字符会由于其长度超过词的最大长度，被修改为<code>[UNK]</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertTokenizer</span><span class="params">(PreTrainedTokenizer)</span>:</span></span><br><span class="line">    <span class="string">r"""</span></span><br><span class="line"><span class="string">    Constructs a BERT tokenizer. Based on WordPiece.</span></span><br><span class="line"><span class="string">    这是一个替换原文本中符号，检测元文本中的单词是否在预训练字典中，将单词替换成字典中对应的id，对文本的长度进行padding的一个类。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab_file:词汇表文件</span></span><br><span class="line"><span class="string">        do_lower_case:是否大写变小写，默认True</span></span><br><span class="line"><span class="string">        do_basic_tokenize:是否在WordPiece之前做basci tokenize,对于中文字符来说</span></span><br><span class="line"><span class="string">        never_split：哪些词不需要在进行更细粒度的切分，前提需要`do_basic_tokenize=True`，才会起作用。</span></span><br><span class="line"><span class="string">        unk_token：对于不在词汇表中的词汇被修改成unk_token，默认是 "[UNK]"，无特殊需要，使用默认就好。</span></span><br><span class="line"><span class="string">        sep_token：分隔句子的标识，默认是"[SEP]"，无特殊需要，使用默认就好。</span></span><br><span class="line"><span class="string">        pad_token:超出句子长度的内容被填补pad_token, 默认是"[PAD]"，无特殊需要，使用默认就好。</span></span><br><span class="line"><span class="string">        cls_token：句子分类，默认"[CLS]"，原用于NSP任务，可用该字符对应的输出向量做文本分类任务</span></span><br><span class="line"><span class="string">        mask_token：对文本进行mask的字符，用于MAKS LM 任务，被mask住的字符需要在训练阶段进行predict，默认是"[MASK]"，无特殊需要，使用默认就好。</span></span><br><span class="line"><span class="string">        tokenize_chinese_chars：是否对中文字符进行分词。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    vocab_files_names = VOCAB_FILES_NAMES <span class="comment"># 词汇表文件名</span></span><br><span class="line">    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP</span><br><span class="line">    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION</span><br><span class="line">    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        vocab_file,</span></span></span><br><span class="line"><span class="function"><span class="params">        do_lower_case=True,</span></span></span><br><span class="line"><span class="function"><span class="params">        do_basic_tokenize=True,</span></span></span><br><span class="line"><span class="function"><span class="params">        never_split=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        unk_token=<span class="string">"[UNK]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        sep_token=<span class="string">"[SEP]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        pad_token=<span class="string">"[PAD]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        cls_token=<span class="string">"[CLS]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        mask_token=<span class="string">"[MASK]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        tokenize_chinese_chars=True,</span></span></span><br><span class="line"><span class="function"><span class="params">        **kwargs</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">        super().__init__(</span><br><span class="line">            unk_token=unk_token,</span><br><span class="line">            sep_token=sep_token,</span><br><span class="line">            pad_token=pad_token,</span><br><span class="line">            cls_token=cls_token,</span><br><span class="line">            mask_token=mask_token,</span><br><span class="line">            **kwargs,</span><br><span class="line">        )</span><br><span class="line">        self.max_len_single_sentence = self.max_len - <span class="number">2</span>  <span class="comment"># take into account special tokens</span></span><br><span class="line">        self.max_len_sentences_pair = self.max_len - <span class="number">3</span>  <span class="comment"># roberta，take into account special tokens</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(vocab_file):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">"Can't find a vocabulary file at path '&#123;&#125;'. To load the vocabulary from a Google pretrained "</span></span><br><span class="line">                <span class="string">"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"</span>.format(vocab_file)</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 加载词汇表文件</span></span><br><span class="line">        self.vocab = load_vocab(vocab_file)</span><br><span class="line">        <span class="comment"># id 到 token 的映射</span></span><br><span class="line">        self.ids_to_tokens = collections.OrderedDict([(ids, tok) <span class="keyword">for</span> tok, ids <span class="keyword">in</span> self.vocab.items()])</span><br><span class="line">        self.do_basic_tokenize = do_basic_tokenize</span><br><span class="line">        <span class="keyword">if</span> do_basic_tokenize:</span><br><span class="line">            self.basic_tokenizer = BasicTokenizer(</span><br><span class="line">                do_lower_case=do_lower_case, never_split=never_split, tokenize_chinese_chars=tokenize_chinese_chars</span><br><span class="line">            )</span><br><span class="line">        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">vocab_size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_vocab</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> dict(self.vocab, **self.added_tokens_encoder)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        split_tokens = []</span><br><span class="line">        <span class="keyword">if</span> self.do_basic_tokenize:</span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):</span><br><span class="line">                <span class="keyword">for</span> sub_token <span class="keyword">in</span> self.wordpiece_tokenizer.tokenize(token):</span><br><span class="line">                    split_tokens.append(sub_token)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            split_tokens = self.wordpiece_tokenizer.tokenize(text)</span><br><span class="line">        <span class="keyword">return</span> split_tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_convert_token_to_id</span><span class="params">(self, token)</span>:</span></span><br><span class="line">        <span class="string">""" 把词映射到id"""</span></span><br><span class="line">        <span class="keyword">return</span> self.vocab.get(token, self.vocab.get(self.unk_token))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_convert_id_to_token</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="string">"""把token id 映射成词"""</span></span><br><span class="line">        <span class="keyword">return</span> self.ids_to_tokens.get(index, self.unk_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convert_tokens_to_string</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">        <span class="string">""" 英文：把wordpiece之后的tokens 还原成文本。没有处理中文的换行等"""</span></span><br><span class="line">        out_string = <span class="string">" "</span>.join(tokens).replace(<span class="string">" ##"</span>, <span class="string">""</span>).strip()</span><br><span class="line">        <span class="keyword">return</span> out_string</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_inputs_with_special_tokens</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span> -&gt; List[int]:</span></span><br><span class="line">        <span class="string">"""为分词后的句子添加special token：[CLS]、[SEP]</span></span><br><span class="line"><span class="string">        - single sequence: ``[CLS] X [SEP]``</span></span><br><span class="line"><span class="string">        - pair of sequences: ``[CLS] A [SEP] B [SEP]``</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> [self.cls_token_id] + token_ids_0 + [self.sep_token_id]</span><br><span class="line">        cls = [self.cls_token_id]</span><br><span class="line">        sep = [self.sep_token_id]</span><br><span class="line">        <span class="keyword">return</span> cls + token_ids_0 + sep + token_ids_1 + sep</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_special_tokens_mask</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span> -&gt; List[int]:</span></span><br><span class="line">        <span class="string">"""判断token_ids 是否已经添加过special token，用mask序列来表示：[0,1,1,1,1,0],0表示special token，返回mask 序列</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> already_has_special_tokens:</span><br><span class="line">            <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">"You should not supply a second sequence if the provided sequence of "</span></span><br><span class="line">                    <span class="string">"ids is already formated with special tokens for the model."</span></span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">return</span> list(map(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x <span class="keyword">in</span> [self.sep_token_id, self.cls_token_id] <span class="keyword">else</span> <span class="number">0</span>, token_ids_0))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> [<span class="number">1</span>] + ([<span class="number">0</span>] * len(token_ids_0)) + [<span class="number">1</span>] + ([<span class="number">0</span>] * len(token_ids_1)) + [<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> [<span class="number">1</span>] + ([<span class="number">0</span>] * len(token_ids_0)) + [<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_token_type_ids_from_sequences</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span> -&gt; List[int]:</span></span><br><span class="line">        <span class="string">"""根据序列创建 token type ids，用于表示句子分割，如果只有一个句子，返回全为0的列表，如果是两个句子，则表示成如下：</span></span><br><span class="line"><span class="string">            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1</span></span><br><span class="line"><span class="string">            | first sequence    | second sequence |</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        sep = [self.sep_token_id]</span><br><span class="line">        cls = [self.cls_token_id]</span><br><span class="line">        <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> len(cls + token_ids_0 + sep) * [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> len(cls + token_ids_0 + sep) * [<span class="number">0</span>] + len(token_ids_1 + sep) * [<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_vocabulary</span><span class="params">(self, vocab_path)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Save the sentencepiece vocabulary (copy original file) and special tokens file to a directory.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> os.path.isdir(vocab_path):</span><br><span class="line">            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[<span class="string">"vocab_file"</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            vocab_file = vocab_path</span><br><span class="line">        <span class="keyword">with</span> open(vocab_file, <span class="string">"w"</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> writer:</span><br><span class="line">            <span class="keyword">for</span> token, token_index <span class="keyword">in</span> sorted(self.vocab.items(), key=<span class="keyword">lambda</span> kv: kv[<span class="number">1</span>]):</span><br><span class="line">                <span class="keyword">if</span> index != token_index:</span><br><span class="line">                    logger.warning(</span><br><span class="line">                        <span class="string">"Saving vocabulary to &#123;&#125;: vocabulary indices are not consecutive."</span></span><br><span class="line">                        <span class="string">" Please check that the vocabulary is not corrupted!"</span>.format(vocab_file)</span><br><span class="line">                    )</span><br><span class="line">                    index = token_index</span><br><span class="line">                writer.write(token + <span class="string">"\n"</span>)</span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> (vocab_file,)</span><br></pre></td></tr></table></figure>
<p>由于BertTokenizer继承了PreTrainedTokenizer，对PreTrainedTokenizer感兴趣可以通过这个<a href="https://github.com/huggingface/transformers/blob/ef46ccb05c601f413a774d43524591816406778d/src/transformers/tokenization_utils.py#L693" target="_blank" rel="noopener">链接</a>在进行研究。</p>
<h4 id="input-feature">Input feature</h4>
<figure class="highlight python"><figcaption><span>convert_example_to_features</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InputFeatures</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    InputExample 对应的 feature，该类变量名称与model中的变量是对应的。</span></span><br><span class="line"><span class="string">    @dataclass 自动为该类添加初始化函数__init__().</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    input_ids: List[int] <span class="comment"># 输入样本的id</span></span><br><span class="line">    attention_mask: List[int] <span class="comment"># </span></span><br><span class="line">    <span class="comment"># 用来指示第几个句子，比如：</span></span><br><span class="line">    <span class="comment"># tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]</span></span><br><span class="line">    <span class="comment">#  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1</span></span><br><span class="line">    token_type_ids: Optional[List[int]] = <span class="literal">None</span> </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 由于是序列标注任务，所以label对应的也应该是一个序列。</span></span><br><span class="line">    label_ids: Optional[List[int]] = <span class="literal">None</span> </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="comment"># 实际上该函数没有处理中文分词中，换行（\n）,空格字符对齐的问题。    </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_examples_to_features</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    examples: List[InputExample], <span class="comment"># 样本集</span></span></span></span><br><span class="line"><span class="function"><span class="params">    label_list: List[str], <span class="comment"># 样本集对应的标签集</span></span></span></span><br><span class="line"><span class="function"><span class="params">    max_seq_length: int, <span class="comment"># 最大序列长度，但是不应该超过510（512应该包含[CLS],[SEP]两个字符）</span></span></span></span><br><span class="line"><span class="function"><span class="params">    tokenizer: PreTrainedTokenizer, <span class="comment"># 分词器</span></span></span></span><br><span class="line"><span class="function"><span class="params">    cls_token_at_end=False, <span class="comment"># [CLS]字符是否添加在序列最后，默认是放在序列最前面，False：[CLS] + A + [SEP] + B + [SEP]，True: A + [SEP] + B + [SEP] + [CLS]</span></span></span></span><br><span class="line"><span class="function"><span class="params">    cls_token=<span class="string">"[CLS]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    cls_token_segment_id=<span class="number">1</span>, <span class="comment"># `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)</span></span></span></span><br><span class="line"><span class="function"><span class="params">    sep_token=<span class="string">"[SEP]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    sep_token_extra=False, <span class="comment"># roberta 中会有extra sep token</span></span></span></span><br><span class="line"><span class="function"><span class="params">    pad_on_left=False, <span class="comment"># 是否在序列的左边进行pad</span></span></span></span><br><span class="line"><span class="function"><span class="params">    pad_token=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    pad_token_segment_id=<span class="number">0</span>, <span class="comment"># padding token_ids 的值为0</span></span></span></span><br><span class="line"><span class="function"><span class="params">    pad_token_label_id=<span class="number">-100</span>, <span class="comment"># 序列标注的tag，超出序列长度的部分，pad成：-100</span></span></span></span><br><span class="line"><span class="function"><span class="params">    sequence_a_segment_id=<span class="number">0</span>, <span class="comment"># 第一句对应的seg id：0  --&gt; type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1</span></span></span></span><br><span class="line"><span class="function"><span class="params">    mask_padding_with_zero=True, <span class="comment"># 对padding的部分，mask对应的值为0</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span> -&gt; List[InputFeatures]:</span></span><br><span class="line">        </span><br><span class="line">  <span class="comment"># label 到 index 的映射 ： [B,M,E,S]--&gt;[0,1,2,3]</span></span><br><span class="line">  label_map = &#123;label: i <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(label_list)&#125; </span><br><span class="line">  </span><br><span class="line">  features = []</span><br><span class="line">  <span class="keyword">for</span> (ex_index, example) <span class="keyword">in</span> enumerate(examples):</span><br><span class="line">      <span class="keyword">if</span> ex_index % <span class="number">10</span>_000 == <span class="number">0</span>:</span><br><span class="line">          logger.info(<span class="string">"Writing example %d of %d"</span>, ex_index, len(examples))</span><br><span class="line">  </span><br><span class="line">      tokens = []  <span class="comment"># 文本序列分词之后的列表:sentence --&gt; [word1,wor2,word3,....]</span></span><br><span class="line">      label_ids = [] <span class="comment"># 由于分词之后，label 和 tokens 会产生错位现象（[CLS],[SEP],空格,换行等字符导致的问题），需要重新和分词之后的内容对齐。</span></span><br><span class="line">      <span class="keyword">for</span> word, label <span class="keyword">in</span> zip(example.words, example.labels):</span><br><span class="line">          word_tokens = tokenizer.tokenize(word)</span><br><span class="line">  </span><br><span class="line">          <span class="keyword">if</span> len(word_tokens) &gt; <span class="number">0</span>:</span><br><span class="line">              tokens.extend(word_tokens)</span><br><span class="line">              label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - <span class="number">1</span>))</span><br><span class="line">  </span><br><span class="line">      <span class="comment"># Account for [CLS] and [SEP] with "- 2" and with "- 3" for RoBERTa.</span></span><br><span class="line">      special_tokens_count = tokenizer.num_special_tokens_to_add()</span><br><span class="line">      <span class="comment"># 先取出不添加special token的序列，再在这个序列的基础上，添加special token</span></span><br><span class="line">      <span class="keyword">if</span> len(tokens) &gt; max_seq_length - special_tokens_count:</span><br><span class="line">          tokens = tokens[: (max_seq_length - special_tokens_count)]</span><br><span class="line">          label_ids = label_ids[: (max_seq_length - special_tokens_count)]</span><br><span class="line">  </span><br><span class="line">      <span class="comment"># 在序列末尾添加[SEP]</span></span><br><span class="line">      tokens += [sep_token]</span><br><span class="line">      label_ids += [pad_token_label_id]</span><br><span class="line">      </span><br><span class="line">      <span class="comment"># roberta 的extra token</span></span><br><span class="line">      <span class="keyword">if</span> sep_token_extra:</span><br><span class="line">          <span class="comment"># roberta uses an extra separator b/w pairs of sentences</span></span><br><span class="line">          tokens += [sep_token]</span><br><span class="line">          label_ids += [pad_token_label_id]</span><br><span class="line">          </span><br><span class="line">      <span class="comment"># 第一句对应的seg id：0  --&gt; type_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1</span></span><br><span class="line">      segment_ids = [sequence_a_segment_id] * len(tokens)</span><br><span class="line">    </span><br><span class="line">      <span class="comment"># [CLS] 添加到句首还是句末</span></span><br><span class="line">      <span class="keyword">if</span> cls_token_at_end:</span><br><span class="line">          tokens += [cls_token]</span><br><span class="line">          label_ids += [pad_token_label_id]</span><br><span class="line">          segment_ids += [cls_token_segment_id]</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          tokens = [cls_token] + tokens</span><br><span class="line">          label_ids = [pad_token_label_id] + label_ids</span><br><span class="line">          segment_ids = [cls_token_segment_id] + segment_ids</span><br><span class="line">    </span><br><span class="line">      <span class="comment"># 把词转化成token id</span></span><br><span class="line">      input_ids = tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line">  </span><br><span class="line">      <span class="comment"># The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.</span></span><br><span class="line">      input_mask = [<span class="number">1</span> <span class="keyword">if</span> mask_padding_with_zero <span class="keyword">else</span> <span class="number">0</span>] * len(input_ids)</span><br><span class="line">  </span><br><span class="line">      <span class="comment"># 添加0padding到句子最大长度</span></span><br><span class="line">      padding_length = max_seq_length - len(input_ids)</span><br><span class="line">      <span class="comment"># 在句子的左边padding 0 还是在句子的右边padding 0</span></span><br><span class="line">      <span class="keyword">if</span> pad_on_left:</span><br><span class="line">          input_ids = ([pad_token] * padding_length) + input_ids</span><br><span class="line">          input_mask = ([<span class="number">0</span> <span class="keyword">if</span> mask_padding_with_zero <span class="keyword">else</span> <span class="number">1</span>] * padding_length) + input_mask</span><br><span class="line">          segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids</span><br><span class="line">          label_ids = ([pad_token_label_id] * padding_length) + label_ids</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          input_ids += [pad_token] * padding_length</span><br><span class="line">          input_mask += [<span class="number">0</span> <span class="keyword">if</span> mask_padding_with_zero <span class="keyword">else</span> <span class="number">1</span>] * padding_length</span><br><span class="line">          segment_ids += [pad_token_segment_id] * padding_length</span><br><span class="line">          label_ids += [pad_token_label_id] * padding_length</span><br><span class="line">  </span><br><span class="line">      <span class="keyword">assert</span> len(input_ids) == max_seq_length</span><br><span class="line">      <span class="keyword">assert</span> len(input_mask) == max_seq_length</span><br><span class="line">      <span class="keyword">assert</span> len(segment_ids) == max_seq_length</span><br><span class="line">      <span class="keyword">assert</span> len(label_ids) == max_seq_length</span><br><span class="line">  </span><br><span class="line">      <span class="keyword">if</span> ex_index &lt; <span class="number">5</span>:</span><br><span class="line">          logger.info(<span class="string">"*** Example ***"</span>)</span><br><span class="line">          logger.info(<span class="string">"guid: %s"</span>, example.guid)</span><br><span class="line">          logger.info(<span class="string">"tokens: %s"</span>, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> tokens]))</span><br><span class="line">          logger.info(<span class="string">"input_ids: %s"</span>, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> input_ids]))</span><br><span class="line">          logger.info(<span class="string">"input_mask: %s"</span>, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> input_mask]))</span><br><span class="line">          logger.info(<span class="string">"segment_ids: %s"</span>, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> segment_ids]))</span><br><span class="line">          logger.info(<span class="string">"label_ids: %s"</span>, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> label_ids]))</span><br><span class="line">  </span><br><span class="line">      <span class="keyword">if</span> <span class="string">"token_type_ids"</span> <span class="keyword">not</span> <span class="keyword">in</span> tokenizer.model_input_names:</span><br><span class="line">          segment_ids = <span class="literal">None</span></span><br><span class="line">  </span><br><span class="line">      features.append(</span><br><span class="line">          InputFeatures(</span><br><span class="line">              input_ids=input_ids, attention_mask=input_mask, token_type_ids=segment_ids, label_ids=label_ids</span><br><span class="line">          )</span><br><span class="line">      )</span><br><span class="line">  <span class="keyword">return</span> features</span><br></pre></td></tr></table></figure>
<h3 id="step-3-dataset">step 3:Dataset</h3>
<p>有了样本和特征，构建数据集类。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NerDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    features: List[InputFeatures]</span><br><span class="line">    <span class="comment"># 对于pad的部分不计算损失 </span></span><br><span class="line">    pad_token_label_id: int = nn.CrossEntropyLoss().ignore_index</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        data_dir: str,</span></span></span><br><span class="line"><span class="function"><span class="params">        tokenizer: PreTrainedTokenizer,</span></span></span><br><span class="line"><span class="function"><span class="params">        labels: List[str],</span></span></span><br><span class="line"><span class="function"><span class="params">        model_type: str,</span></span></span><br><span class="line"><span class="function"><span class="params">        max_seq_length: Optional[int] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">        overwrite_cache=False,</span></span></span><br><span class="line"><span class="function"><span class="params">        mode: Split = Split.train,</span></span></span><br><span class="line"><span class="function"><span class="params">        local_rank=<span class="number">-1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        )</span>:</span></span><br><span class="line">        <span class="comment"># 加载数据集文件</span></span><br><span class="line">        cached_features_file = os.path.join(data_dir, <span class="string">"cached_&#123;&#125;_&#123;&#125;_&#123;&#125;"</span>.format(mode.value, tokenizer.__class__.__name__, str(max_seq_length)), )</span><br><span class="line">        <span class="keyword">with</span> torch_distributed_zero_first(local_rank):</span><br><span class="line">        <span class="comment"># Make sure only the first process in distributed training processes the dataset,</span></span><br><span class="line">        <span class="comment"># and the others will use the cache.</span></span><br><span class="line">            <span class="keyword">if</span> os.path.exists(cached_features_file) <span class="keyword">and</span> <span class="keyword">not</span> overwrite_cache:</span><br><span class="line">                logger.info(<span class="string">f"Loading features from cached file <span class="subst">&#123;cached_features_file&#125;</span>"</span>)</span><br><span class="line">                self.features = torch.load(cached_features_file)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                logger.info(<span class="string">f"Creating features from dataset file at <span class="subst">&#123;data_dir&#125;</span>"</span>)</span><br><span class="line">                examples = read_examples_from_file(data_dir, mode)</span><br><span class="line">                <span class="comment"># TODO clean up all this to leverage built-in features of tokenizers</span></span><br><span class="line">                self.features = convert_examples_to_features(</span><br><span class="line">                    examples,</span><br><span class="line">                    labels,</span><br><span class="line">                    max_seq_length,</span><br><span class="line">                    tokenizer,</span><br><span class="line">                    cls_token_at_end=bool(model_type <span class="keyword">in</span> [<span class="string">"xlnet"</span>]),</span><br><span class="line">                    <span class="comment"># xlnet has a cls token at the end</span></span><br><span class="line">                    cls_token=tokenizer.cls_token,</span><br><span class="line">                    cls_token_segment_id=<span class="number">2</span> <span class="keyword">if</span> model_type <span class="keyword">in</span> [<span class="string">"xlnet"</span>] <span class="keyword">else</span> <span class="number">0</span>,</span><br><span class="line">                    sep_token=tokenizer.sep_token,</span><br><span class="line">                    sep_token_extra=bool(model_type <span class="keyword">in</span> [<span class="string">"roberta"</span>]),</span><br><span class="line">                    <span class="comment"># roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805</span></span><br><span class="line">                    pad_on_left=bool(tokenizer.padding_side == <span class="string">"left"</span>),</span><br><span class="line">                    pad_token=tokenizer.pad_token_id,</span><br><span class="line">                    pad_token_segment_id=tokenizer.pad_token_type_id,</span><br><span class="line">                    pad_token_label_id=self.pad_token_label_id,</span><br><span class="line">                )</span><br><span class="line">                <span class="comment"># 保存0卡上的数据到缓存</span></span><br><span class="line">                <span class="keyword">if</span> local_rank <span class="keyword">in</span> [<span class="number">-1</span>, <span class="number">0</span>]:</span><br><span class="line">                    logger.info(<span class="string">f"Saving features into cached file <span class="subst">&#123;cached_features_file&#125;</span>"</span>)</span><br><span class="line">                    torch.save(self.features, cached_features_file)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.features)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, i)</span> -&gt; InputFeatures:</span></span><br><span class="line">        <span class="keyword">return</span> self.features[i]</span><br></pre></td></tr></table></figure>
<h2 id="bert-model">BertModel</h2>
<p><img src="/2020/05/13/research/bert_code/BertModel.png" alt></p>
<p>从整体来看BertModel由三部分组成：BertEmbeddings、BertEncoder、BertPooler,需要注意<code>attention_mask和head_mask</code>的处理。</p>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>inputs，segment，mask'，position_ids'，head_mask'</code></p>
</li>
<li>
<p>输出：<code>元组 (最后一层的隐变量，最后一层第一个token的隐变量，最后一层的隐变量或每一层attentions 权重参数)</code></p>
</li>
<li>
<p>过程:<code>embedding-&gt;encoder-&gt;pooler</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertModel</span><span class="params">(BertPreTrainedModel)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__(config)</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        self.embeddings = BertEmbeddings(config)</span><br><span class="line">        self.encoder = BertEncoder(config)</span><br><span class="line">        self.pooler = BertPooler(config)</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_input_embeddings</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.embeddings.word_embeddings</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_input_embeddings</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        self.embeddings.word_embeddings = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_prune_heads</span><span class="params">(self, heads_to_prune)</span>:</span></span><br><span class="line">        <span class="string">""" E.g. &#123;1: [0, 2], 2: [2, 3]&#125; will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> layer, heads <span class="keyword">in</span> heads_to_prune.items():</span><br><span class="line">            self.encoder.layer[layer].attention.prune_heads(heads)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">        head_mask: head_mask has shape [num_heads] or [num_hidden_layers x num_heads]</span></span><br><span class="line"><span class="string">        encoder_hidden_states: encoder 的输出</span></span><br><span class="line"><span class="string">        encoder_attention_mask:</span></span><br><span class="line"><span class="string">        返回值：</span></span><br><span class="line"><span class="string">        last_hidden_state :[batch_size, sequence_length, hidden_size]，是序列在模型最后一层的输出的隐藏层</span></span><br><span class="line"><span class="string">        pooler_output :[batch_size, hidden_size]:[CLS]对应的隐状态的输出，由于这个token是用来做NSP任务的，这个输出通常不能很好的summary 整个序列的语义，如果想要获取整句话的语义通常需要对整个序列取平均或者池化。</span></span><br><span class="line"><span class="string">        hidden_states:[batch_size, sequence_length, hidden_size],embedding + output of each layer.</span></span><br><span class="line"><span class="string">        attention:[batch_size, num_heads, sequence_length, sequence_length],矩阵的权值是经过softmax的。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"You cannot specify both input_ids and inputs_embeds at the same time"</span>)</span><br><span class="line">        <span class="keyword">elif</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">elif</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"You have to specify either input_ids or inputs_embeds"</span>)</span><br><span class="line"></span><br><span class="line">        device = input_ids.device <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> inputs_embeds.device</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 全部mask成 1</span></span><br><span class="line">            attention_mask = torch.ones(input_shape, device=device)</span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 全表示成0（第一句）</span></span><br><span class="line">            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. attention_mask:[bs,seq_len] --&gt; [bs,num_heads,seq_len,seq_len]</span></span><br><span class="line">        <span class="comment"># 2. attention_value:1 --&gt; 0, 0--&gt;-10000,这样做的目的是为了和算出来的score（没有经过softmax）相加，相当于从序列中移除掉了mask的内容。</span></span><br><span class="line">        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, self.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.config.is_decoder <span class="keyword">and</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()</span><br><span class="line">            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length) <span class="comment"># [bs,seq_len]</span></span><br><span class="line">            <span class="keyword">if</span> encoder_attention_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 1. attention_mask:[bs,seq_len] --&gt; [bs,num_heads,seq_len,seq_len]</span></span><br><span class="line">            <span class="comment"># 2. attention_value:1 --&gt; 0, 0--&gt;-10000,这样做的目的是为了和算出来的score（没有经过softmax）相加，相当于从序列中移除掉了mask的内容。</span></span><br><span class="line">            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            encoder_extended_attention_mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># input head_mask： [num_heads] or [num_hidden_layers x num_heads]，1表示保留这个head。</span></span><br><span class="line">        <span class="comment"># 因为有多个head，每个head都需要mask序列中哪些是token，哪些是padding，所以需要转化head_mask为：[num_hidden_layers,bsz,num_heads,seq_length,seq_length]</span></span><br><span class="line">        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)</span><br><span class="line"></span><br><span class="line">        embedding_output = self.embeddings(</span><br><span class="line">            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds</span><br><span class="line">        )</span><br><span class="line">        encoder_outputs = self.encoder(</span><br><span class="line">            embedding_output,</span><br><span class="line">            attention_mask=extended_attention_mask,</span><br><span class="line">            head_mask=head_mask,</span><br><span class="line">            encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">            encoder_attention_mask=encoder_extended_attention_mask,</span><br><span class="line">        )</span><br><span class="line">        sequence_output = encoder_outputs[<span class="number">0</span>]</span><br><span class="line">        pooled_output = self.pooler(sequence_output)</span><br><span class="line">        <span class="comment"># add hidden_states and attentions if they are here</span></span><br><span class="line">        outputs = (sequence_output, pooled_output,) + encoder_outputs[<span class="number">1</span>:]  </span><br><span class="line">        <span class="keyword">return</span> outputs  <span class="comment"># sequence_output, pooled_output, (hidden_states), (attentions)</span></span><br></pre></td></tr></table></figure>
<h3 id="bert-embeddings">BertEmbeddings</h3>
<p>从Bert的论文中可以知道，Bert的词向量主要是由三个向量相加组合而成，分别是单词本身的向量，单词所在句子中位置的向量和句子所在单个训练文本中位置的向量。这样做的好处主要可以解决只有词向量时碰见多义词时模型预测不准的问题。</p>
<p><img src="/2020/05/13/research/bert_code/bert-input-representation.png" alt></p>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>inputs，segment'，position_ids'</code></p>
</li>
<li>
<p>输出：<code>words+position+segment的embedding</code></p>
</li>
<li>
<p>过程:<code>调用nn.Embedding构造words、position、segment的embedding -&gt; 三个embedding相加 -&gt; 规范化 LayerNorm（关联类BertLayerNorm）-&gt; dropout</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEmbeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Construct the embeddings from word, position and token_type embeddings.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)</span><br><span class="line">        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)</span><br><span class="line">        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)</span><br><span class="line">        </span><br><span class="line">        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">        device = input_ids.device <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> inputs_embeds.device</span><br><span class="line">        <span class="keyword">if</span> position_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 生成token 的 index 信息。[seq_len]</span></span><br><span class="line">            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)</span><br><span class="line">            position_ids = position_ids.unsqueeze(<span class="number">0</span>).expand(input_shape) <span class="comment">#[bs,seq_len]</span></span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs_embeds <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            inputs_embeds = self.word_embeddings(input_ids)</span><br><span class="line">        position_embeddings = self.position_embeddings(position_ids)</span><br><span class="line">        token_type_embeddings = self.token_type_embeddings(token_type_ids)</span><br><span class="line"></span><br><span class="line">        embeddings = inputs_embeds + position_embeddings + token_type_embeddings</span><br><span class="line">        <span class="comment"># BertLayerNorm = torch.nn.LayerNorm</span></span><br><span class="line">        embeddings = self.LayerNorm(embeddings)</span><br><span class="line">        embeddings = self.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure>
<p>从forward函数开始看，先有一个torch.arange函数。<code>torch.arange(seq_length, dtype=torch.long, device=input_ids.device)</code>来生成token 的index 信息,<span class="label danger">和transformer中的Positional Embedding不同，bert这里的positional embedding 需要通过训练进行学习，transformer中的positional embedding是使用公式计算出来的</span>(可以参考 的Positional Embedding来进行比较)。此外，LayerNorm的数学表达为：\(y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\)，\(\gamma,\beta\)可以通过仿射变换来进行学习。</p>
<h3 id="bert-encoder">BertEncoder</h3>
<p>下面代码将原有的BertLayer一层一层剥开，如果想要输出每层的状态（output_hidden_states=True），则对hidden_states 进行累加，如果（output_hidden_states=False）则只与最后一层的状态相加。BertEncoder类，其实只是用来输出BertLayer类的状态的一个函数。真正模型内部的东西在BertLayer类中。</p>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states（由BetEmbeddings输出），attention_mask，head_mask</code></p>
</li>
<li>
<p>输出：<code>元组 (最后一层隐变量+每层隐变量) 或者 (最后一层attention+每一层attention)</code></p>
</li>
<li>
<p>过程:<code>调用modulelist类实例layer使得每一层输出（关联类BertLayer）-&gt; 保存所有层的attention输出 和 隐变量 -&gt; 返回元组，元组第一个是最后一层的attention或hidden，再往后是每层的。</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.output_attentions = config.output_attentions</span><br><span class="line">        self.output_hidden_states = config.output_hidden_states</span><br><span class="line">        self.layer = nn.ModuleList([BertLayer(config) <span class="keyword">for</span> _ <span class="keyword">in</span> range(config.num_hidden_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,)</span>:</span></span><br><span class="line">        all_hidden_states = ()</span><br><span class="line">        all_attentions = ()</span><br><span class="line">        <span class="comment"># 将原有的BertLayer一层一层剥开</span></span><br><span class="line">        <span class="comment"># 如果想要输出每层的状态（output_hidden_states=True），则对hidden_states 进行累加</span></span><br><span class="line">        <span class="comment"># 如果（output_hidden_states=False）则只与最后一层的状态相加。</span></span><br><span class="line">        <span class="keyword">for</span> i, layer_module <span class="keyword">in</span> enumerate(self.layer):</span><br><span class="line">            <span class="keyword">if</span> self.output_hidden_states:</span><br><span class="line">                all_hidden_states = all_hidden_states + (hidden_states,)</span><br><span class="line"></span><br><span class="line">            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask)</span><br><span class="line">            hidden_states = layer_outputs[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.output_attentions:</span><br><span class="line">                all_attentions = all_attentions + (layer_outputs[<span class="number">1</span>],)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add last layer</span></span><br><span class="line">        <span class="keyword">if</span> self.output_hidden_states:</span><br><span class="line">            all_hidden_states = all_hidden_states + (hidden_states,)</span><br><span class="line"></span><br><span class="line">        outputs = (hidden_states,)</span><br><span class="line">        <span class="keyword">if</span> self.output_hidden_states:</span><br><span class="line">            outputs = outputs + (all_hidden_states,)</span><br><span class="line">        <span class="keyword">if</span> self.output_attentions:</span><br><span class="line">            outputs = outputs + (all_attentions,)</span><br><span class="line">        <span class="keyword">return</span> outputs  <span class="comment"># last-layer hidden state, (all hidden states), (all attentions)</span></span><br></pre></td></tr></table></figure>
<h4 id="bert-layer">BertLayer</h4>
<p>BertLayer 由三部分组成：<code>BertAttention，BertIntermeidiate，BertOutput</code>。</p>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states（由上层BertLayer输出），attention_mask，head_mask</code></p>
</li>
<li>
<p>输出：<code>元组，(本层输出的隐变量，本层输出的attention)</code></p>
</li>
<li>
<p>过程:<code>调用attention得到attention_outputs -&gt; 取第一维attention_output[0]作为intermediate的参数 -&gt;调用intermediate-&gt; 调用output得到layer_output -&gt; layer_output 和 attention_outputs[1:]合并成元组返回</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.attention = BertAttention(config)</span><br><span class="line">        self.is_decoder = config.is_decoder</span><br><span class="line">        <span class="keyword">if</span> self.is_decoder:</span><br><span class="line">            self.crossattention = BertAttention(config)</span><br><span class="line">        self.intermediate = BertIntermediate(config)</span><br><span class="line">        self.output = BertOutput(config)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,)</span>:</span></span><br><span class="line">        self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)</span><br><span class="line">        attention_output = self_attention_outputs[<span class="number">0</span>]</span><br><span class="line">        outputs = self_attention_outputs[<span class="number">1</span>:]  <span class="comment"># add self attentions if we output attention weights</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.is_decoder <span class="keyword">and</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            cross_attention_outputs = self.crossattention(</span><br><span class="line">                attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask</span><br><span class="line">            )</span><br><span class="line">            attention_output = cross_attention_outputs[<span class="number">0</span>]</span><br><span class="line">            outputs = outputs + cross_attention_outputs[<span class="number">1</span>:]  <span class="comment"># add cross attentions if we output attention weights</span></span><br><span class="line"></span><br><span class="line">        intermediate_output = self.intermediate(attention_output)</span><br><span class="line">        layer_output = self.output(intermediate_output, attention_output)</span><br><span class="line">        outputs = (layer_output,) + outputs</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>从forward开始看，依次进入BertAttention，BertIntermediate和BertOutput这三个类。</p>
<h5 id="bert-attention">BertAttention</h5>
<p>这个类由两个类组成：<code>BertSelfAttention,BertSelfOutput</code>.</p>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>input_tensor(就是BertLayer的hidden_states)，mask，head_mask'</code></p>
</li>
<li>
<p>输出：<code>返回元组（attention_output，self_outputs[1:]）第一个是语义向量，第二个是概率</code></p>
</li>
<li>
<p>过程:<code>selfattention得到 self_outputs-&gt; 以self_outputs[0]作为参数调用selfoutput得到 attention_output-&gt; 返回元组（attention_output，self_outputs[1:]）第一个是语义向量，第二个是概率</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.self_att = BertSelfAttention(config)</span><br><span class="line">        self.output = BertSelfOutput(config)</span><br><span class="line">        self.pruned_heads = set()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prune_heads</span><span class="params">(self, heads)</span>:</span> <span class="comment"># </span></span><br><span class="line">        <span class="keyword">if</span> len(heads) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        mask = torch.ones(self.self_att.num_attention_heads, self.self_att.attention_head_size)</span><br><span class="line">        heads = set(heads) - self.pruned_heads  </span><br><span class="line">        <span class="keyword">for</span> head <span class="keyword">in</span> heads:</span><br><span class="line">            <span class="comment"># Compute how many pruned heads are before the head and move the index accordingly</span></span><br><span class="line">            head = head - sum(<span class="number">1</span> <span class="keyword">if</span> h &lt; head <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> h <span class="keyword">in</span> self.pruned_heads)</span><br><span class="line">            mask[head] = <span class="number">0</span></span><br><span class="line">        mask = mask.view(<span class="number">-1</span>).contiguous().eq(<span class="number">1</span>)</span><br><span class="line">        index = torch.arange(len(mask))[mask].long()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Prune linear layers</span></span><br><span class="line">        self.self_att.query = prune_linear_layer(self.self_att.query, index)</span><br><span class="line">        self.self_att.key = prune_linear_layer(self.self_att.key, index)</span><br><span class="line">        self.self_att.value = prune_linear_layer(self.self_att.value, index)</span><br><span class="line">        self.output.dense = prune_linear_layer(self.output.dense, index, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update hyper params and store pruned heads</span></span><br><span class="line">        self.self_att.num_attention_heads = self.self_att.num_attention_heads - len(heads)</span><br><span class="line">        self.self_att.all_head_size = self.self_att.attention_head_size * self.self_att.num_attention_heads</span><br><span class="line">        self.pruned_heads = self.pruned_heads.union(heads)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,)</span>:</span></span><br><span class="line">        self_outputs = self.self_att(</span><br><span class="line">            hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask</span><br><span class="line">        )</span><br><span class="line">        attention_output = self.output(self_outputs[<span class="number">0</span>], hidden_states)</span><br><span class="line">        outputs = (attention_output,) + self_outputs[<span class="number">1</span>:]  <span class="comment"># add attentions if we output them</span></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h6 id="bert-self-attention">BertSelfAttention</h6>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states(由BertLayer输出),mask，head_mask</code></p>
</li>
<li>
<p>输出：<code>返回元组（context_layer语义向量，attention_prob概率）第一个是语义向量，第二个是概率</code></p>
</li>
<li>
<p>过程:<code>self-attention 过程</code></p>
</li>
</ul>
<p><img src="/2020/05/13/research/bert_code/image-20200515100926049.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="keyword">if</span> config.hidden_size % config.num_attention_heads != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> hasattr(config, <span class="string">"embedding_size"</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">"The hidden size (%d) is not a multiple of the number of attention "</span></span><br><span class="line">                <span class="string">"heads (%d)"</span> % (config.hidden_size, config.num_attention_heads)</span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        self.output_attentions = config.output_attentions</span><br><span class="line">        <span class="comment"># 12</span></span><br><span class="line">        self.num_attention_heads = config.num_attention_heads</span><br><span class="line">        <span class="comment"># 64 = 768/12 </span></span><br><span class="line">        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)</span><br><span class="line">        <span class="comment"># 768 = 12 * 64</span></span><br><span class="line">        self.all_head_size = self.num_attention_heads * self.attention_head_size</span><br><span class="line"></span><br><span class="line">        self.query = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.key = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.value = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># x:[bs,seq_len,768]--&gt; x:[bs,seq_len, 12, 64]   multi-head</span></span><br><span class="line">        new_x_shape = x.size()[:<span class="number">-1</span>] + (self.num_attention_heads, self.attention_head_size)</span><br><span class="line">        x = x.view(*new_x_shape)</span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None, )</span>:</span></span><br><span class="line">        mixed_query_layer = self.query(hidden_states)</span><br><span class="line">    <span class="comment"># q,k,v</span></span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mixed_key_layer = self.key(encoder_hidden_states)</span><br><span class="line">            mixed_value_layer = self.value(encoder_hidden_states)</span><br><span class="line">            attention_mask = encoder_attention_mask</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mixed_key_layer = self.key(hidden_states)</span><br><span class="line">            mixed_value_layer = self.value(hidden_states)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># multi-head </span></span><br><span class="line">        query_layer = self.transpose_for_scores(mixed_query_layer) <span class="comment"># [bs,seq_len, 12, 64]</span></span><br><span class="line">        key_layer = self.transpose_for_scores(mixed_key_layer) <span class="comment"># [bs,seq_len, 12, 64]</span></span><br><span class="line">        value_layer = self.transpose_for_scores(mixed_value_layer) <span class="comment"># [bs,seq_len, 12, 64]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到权值矩阵</span></span><br><span class="line">        attention_scores = torch.matmul(query_layer, key_layer.transpose(<span class="number">-1</span>, <span class="number">-2</span>))</span><br><span class="line">        attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 用求和的方式，因为在attention_mask 中：1--&gt;0,0--&gt;-10000.</span></span><br><span class="line">            attention_scores = attention_scores + attention_mask</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对最后一个维度用 softmax 计算概率</span></span><br><span class="line">        attention_probs = nn.Softmax(dim=<span class="number">-1</span>)(attention_scores)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">        <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper .</span></span><br><span class="line">        attention_probs = self.dropout(attention_probs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="keyword">if</span> head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_probs = attention_probs * head_mask</span><br><span class="line"></span><br><span class="line">        context_layer = torch.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">        context_layer = context_layer.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        <span class="comment"># [bs,seq_len,12,64]--&gt; x:[bs,seq_len, 768]   把multi-head 再拼接回去</span></span><br><span class="line">        new_context_layer_shape = context_layer.size()[:<span class="number">-2</span>] + (self.all_head_size,)</span><br><span class="line">        context_layer = context_layer.view(*new_context_layer_shape)</span><br><span class="line"></span><br><span class="line">        outputs = (context_layer, attention_probs) <span class="keyword">if</span> self.output_attentions <span class="keyword">else</span> (context_layer,)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h6 id="bert-self-output">BertSelfOutput</h6>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states（由BertSelfAttention输出）, input_tensor（就是BertAttention的input_tensor，也就是BertSelfAttention的输入）</code></p>
</li>
<li>
<p>输出：<code>hidden_states</code></p>
</li>
<li>
<p>过程:<code>对hidden_states加一层dense -&gt; dropout -&gt; 得到的hidden_states与input_tensor相加做LayerNorm #这种做法说是为了避免梯度消失，也就是曾经的残差网络解决办法：output=output+Q</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfOutput</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, hidden_states, input_tensor)</span>:</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<h5 id="bert-intermediate">BertIntermediate</h5>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states(BertSelfOutput的输出)</code></p>
</li>
<li>
<p>输出：<code>hidden_states</code></p>
</li>
<li>
<p>过程:<code>对hidden_states加一层dense，向量输出大小为intermedia_size -&gt; 调用intermediate_act_fn，这个函数是由config.hidden_act得来，是gelu、relu、swish方法中的一个 #中间层存在的意义：推测是能够使模型从低至高学习到多层级信息，从表面信息到句法到语义。还有人研究说中间层的可迁移性更好。</code></p>
</li>
</ul>
<p>ACT2FN：激活函数:<code>{&quot;gelu&quot;: gelu, &quot;relu&quot;: torch.nn.functional.relu, &quot;swish&quot;: swish}</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertIntermediate</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)</span><br><span class="line">        <span class="keyword">if</span> isinstance(config.hidden_act, str):</span><br><span class="line">            <span class="comment"># ACT2FN = &#123;"gelu": gelu, "relu": torch.nn.functional.relu, "swish": swish&#125;</span></span><br><span class="line">            self.intermediate_act_fn = ACT2FN[config.hidden_act]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.intermediate_act_fn = config.hidden_act</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, hidden_states)</span>:</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.intermediate_act_fn(hidden_states)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<h5 id="bert-output">BertOutput</h5>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states（由BertAttention输出）, input_tensor</code></p>
</li>
<li>
<p>输出：<code>hidden_states</code></p>
</li>
<li>
<p>过程:<code>对hidden_states加一层dense ，由intermedia_size 又变回hidden_size -&gt; dropout -&gt; 得到的hidden_states与input_tensor相加做LayerNorm #这种做法说是为了避免梯度消失，也就是曾经的残差网络解决办法：output=output+Q。</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertOutput</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, hidden_states, input_tensor)</span>:</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<p>可以看到BertOutput是一个输入size是<code>config.intermediate_size</code>，输出size是<code>config.hidden_size</code>。又把size从BertIntermediate中的<code>config.intermediate_size</code>变回<code>config.hidden_size</code>。然后又接了一个Dropout和一个归一化。</p>
<h3 id="bert-pooler">BertPooler</h3>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>接收参数：<code>hidden_states（由BertAttention输出）, input_tensor</code></li>
<li>输出：<code>pooled_output</code></li>
<li>过程:<code>简单取第一个token -&gt; 加一层dense -&gt; Tanh激活函数输出</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertPooler</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.activation = nn.Tanh()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, hidden_states)</span>:</span></span><br><span class="line">        first_token_tensor = hidden_states[:, <span class="number">0</span>]</span><br><span class="line">        pooled_output = self.dense(first_token_tensor)</span><br><span class="line">        pooled_output = self.activation(pooled_output)</span><br><span class="line">        <span class="keyword">return</span> pooled_output</span><br></pre></td></tr></table></figure>
<h2 id="zong-jie">总结</h2>
<p>数据流：</p>
<ol>
<li>输入的数据首先经过<strong>BertEmbeddings</strong>类。在BertEmbeddings中将每个单词变为words_embeddings + position_embeddings +token_type_embeddings三项embeddings的和。</li>
<li>然后，把已经变为词向量的数据输入BertSelfAttention类中。BertSelfAttention类中是一个Multi-Head Attention（少一个Linear层）， 也就是说数据流入这个<strong>少一个Linear层的Multi-Head Attention</strong>。</li>
<li>之后，数据流入BertSelfOutput类。BertSelfOutput是一个Linear+Dropout+LayerNorm。<strong>补齐了BertSelfAttention中少的那个Linear层</strong>，并且进行一次LayerNorm。</li>
<li>再之后，数据经过BertIntermediate(Linear层+激活函数)和BertOutput(Linear+Dropout+LayerNorm)。这样整个Transformer的部分就算完成了。</li>
<li>最后，取出最后一层的<code>[CLS]</code>对应的向量，经过<code>(Linear+Tanh)</code>,得到pooled_out</li>
</ol>
<p>关于bert的训练优化参考<a href="https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py" target="_blank" rel="noopener">链接</a></p>
<p>其余类：</p>
<ol>
<li>
<p>BertConfig</p>
<p>保存BERT的各种参数配置</p>
</li>
<li>
<p>BertOnlyMLMHead<br>
使用mask 方法训练语言模型时用的，返回预测值<br>
过程：调用BertLMPredictionHead，返回的就是prediction_scores</p>
</li>
<li>
<p>BertLMPredictionHead<br>
decode功能<br>
过程：调用BertPredictionHeadTransform -&gt; linear层，输出维度是vocab_size</p>
</li>
<li>
<p>BertPredictionHeadTransform<br>
过程：dense -&gt; 激活(gelu or relu or swish) -&gt; LayerNorm</p>
</li>
<li>
<p>BertOnlyNSPHead<br>
NSP策略训练模型用的，返回0或1<br>
过程：添加linear层输出size是2，返回seq_relationship_score</p>
</li>
<li>
<p>BertPreTrainingHeads<br>
MLM和NSP策略都写在里面，输入的是Encoder的输出sequence_output, pooled_output<br>
返回（prediction_scores, seq_relationship_score）分别是MLM和NSP下的分值</p>
</li>
<li>
<p>BertPreTrainedModel<br>
从全局变量BERT_PRETRAINED_MODEL_ARCHIVE_MAP加载BERT模型的权重</p>
</li>
<li>
<p>BertForPreTraining<br>
计算score和loss<br>
通过BertPreTrainingHeads，得到prediction后计算loss，然后反向传播。</p>
</li>
<li>
<p>BertForMaskedLM<br>
只有MLM策略的loss</p>
</li>
<li>
<p>BertForNextSentencePrediction<br>
只有NSP策略的loss</p>
</li>
<li>
<p>BertForSequenceClassification<br>
计算句子分类任务的loss</p>
</li>
<li>
<p>BertForMultipleChoice<br>
计算句子选择任务的loss</p>
</li>
<li>
<p>BertForTokenClassification<br>
计算对token分类or标注任务的loss</p>
</li>
<li>
<p>BertForQuestionAnswering<br>
计算问答任务的loss</p>
</li>
</ol>
<h3 id="shi-yong-yu-xun-lian-mo-xing">使用预训练模型</h3>
<p>在BertModel class中有两个函数。get_pool_output表示获取每个batch第一个词的[CLS]表示结果。BERT认为这个词包含了整条语料的信息；适用于句子级别的分类问题。get_sequence_output表示BERT最终的输出结果,shape为[batch_size,seq_length,hidden_size]。可以直观理解为对每条语料的最终表示，适用于seq2seq问题。<br>
对于其它序列标注或生成任务，也可以使用 BERT 对应的输出信息作出预测，例如每一个时间步输出一个标注或词等。下图展示了 BERT 在 11 种任务中的微调方法，它们都只添加了一个额外的输出层。在下图中，Tok 表示不同的词、E 表示输入的嵌入向量、\(T_i\)表示第\(i\) 个词在经过 BERT 处理后输出的上下文向量。</p>
<p><img src="/2020/05/13/research/bert_code/bert-specific-models.png" alt></p>
<h1 id="can-kao">参考</h1>
<ol start="3">
<li><a href="http://fancyerii.github.io/2019/03/09/bert-codes/" target="_blank" rel="noopener">BERT代码阅读</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/56103665" target="_blank" rel="noopener">一起读Bert文本分类代码 </a></li>
<li><a href="https://daiwk.github.io/posts/nlp-bert.html#pytorch%E7%89%88%E6%9C%AC" target="_blank" rel="noopener">bert</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/75558363" target="_blank" rel="noopener">快速掌握BERT源代码（pytorch）</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
      <tags>
        <tag>transformer</tag>
        <tag>bert</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/2020/05/11/transformers/transformer/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/11/transformers/transformer/Cg-4jlOszDCIbmRNAAo_4amaeggAAOn7APf2hoACj_5550.jpg" alt></p>
<a id="more"></a>
<h1 id="jian-jie">简介</h1>
<p>自2017年提出以来，Transformer在众多自然语言处理问题中取得了非常好的效果。它不但训练速度更快，而且更适合建模长距离依赖关系，因此大有取代循环或卷积神经网络，一统自然语言处理的深度模型江湖之势。本文结合<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">《Attention is all you need》</a>论文与Harvard的代码<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">《Annotated Transformer》</a>深入理解transformer模型。</p>
<h1 id="transformer">Transformer</h1>
<p>Transformer的整体结构如下图所示，在Encoder和Decoder中都使用了Self-attention, Point-wise和全连接层。Encoder和decoder的大致结构分别如下图的左半部分和右半部分所示。</p>
<p><img src="/2020/05/11/transformers/transformer/over_all.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Helper: Construct a model from hyperparameters.</span></span><br><span class="line"><span class="string">    src_vocab: 输入词汇表大小</span></span><br><span class="line"><span class="string">    tgt_vocab:输出词汇表大小</span></span><br><span class="line"><span class="string">    N：堆叠个数</span></span><br><span class="line"><span class="string">    d_model:embedding_size</span></span><br><span class="line"><span class="string">    d_ff: 线形层输出维度</span></span><br><span class="line"><span class="string">    h:multi-head中 head 的个数</span></span><br><span class="line"><span class="string">    dropout：</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N), <span class="comment"># encoder</span></span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N), <span class="comment"># decoder</span></span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)), <span class="comment"># src_embed</span></span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)), <span class="comment"># tgt_embed</span></span><br><span class="line">        Generator(d_model, tgt_vocab) <span class="comment"># generator</span></span><br><span class="line">    		)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h2 id="embedding">Embedding</h2>
<p>transformer的输入是<strong>Word Embedding + Position Embedding</strong>。</p>
<h3 id="word-embedding">Word Embedding</h3>
<p>Word embedding在pytorch中通常用 nn.Embedding 实现，其权重矩阵通常有两种选择：</p>
<ol>
<li>使用 Pre-trained的<strong>Embeddings并freeze</strong>，这种情况下实际就是一个 Lookup Table。</li>
<li>对其进行随机初始化(当然也可以选择 Pre-trained 的结果)，但<strong>设为 Trainable</strong>。这样在 training 过程中不断地对 Embeddings 进行改进。</li>
</ol>
<p>transformer选择后者，代码实现如下：</p>
<figure class="highlight python"><figcaption><span>word_embedding</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model) <span class="comment"># vocab 词汇表大小</span></span><br><span class="line">        self.d_model = d_model  <span class="comment">#表示embedding的维度</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure>
<h3 id="positional-embedding">Positional Embedding</h3>
<p>在RNN中，对句子的处理是一个个word按顺序输入的。但在 Transformer 中，输入句子的所有word是同时处理的，没有考虑词的排序和位置信息。因此，Transformer 的作者提出了加入 <code>positional encoding</code>的方法来解决这个问题。<code>positional encoding</code>使得 Transformer 可以衡量 word 位置有关的信息。</p>
<p><strong>如何实现具有位置信息的encoding？</strong></p>
<p>作者提供了两种思路：</p>
<ul>
<li>通过训练学习 positional encoding 向量；</li>
<li>使用公式来计算 positional encoding向量。</li>
</ul>
<p>试验后发现两种选择的结果是相似的，所以采用了第2种方法，优点是不需要训练参数，而且即使<strong>在训练集中没有出现过的句子长度上也能用</strong>。Positional Encoding的公式如下：<br>
\[
\begin{aligned}
P E_{(p o s, 2 i)} &amp;=\sin \left(p o s / 10000^{2 i / d_{\text {model }}}\right) \\
P E_{(p o s, 2 i+1)} &amp;=\cos \left(p o s / 10000^{2 i / d_{\text {model }}}\right.
\end{aligned}
\]<br>
其中，\(pos\)指的是这个 word 在这个句子中的位置；\(2i\)指的是 embedding 词向量的偶数维度，\(2i+1\)指的是embedding 词向量的奇数维度。具体实现如下：</p>
<figure class="highlight python"><figcaption><span>PositionalEncoding</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>) <span class="comment"># [max_len, 1]</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * -(math.log(<span class="number">10000.0</span>) / d_model)) </span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term) <span class="comment"># 偶数列</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term) <span class="comment"># 奇数列</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)         <span class="comment"># [1, max_len, d_model]</span></span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe) <span class="comment"># 上述代码只需计算一次，然后放到寄存器里，随用随取。</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x) <span class="comment"># 据说dropout=0.1</span></span><br></pre></td></tr></table></figure>
<p><code>x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)</code> 这行代码表示；输入模型的整个Embedding是Word Embedding与Positional Embedding直接相加之后的结果。</p>
<p>为什么上面的两个公式能体现单词的相对位置信息呢？</p>
<p>下面一段代码取词向量的4个维度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在位置编码下方，将基于位置添加正弦波。对于每个维度，波的频率和偏移都不同。</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line">y = pe.forward(Variable(torch.zeros(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>))) <span class="comment"># [bs=1,seq_len=100,embed_size=20]</span></span><br><span class="line">plt.plot(np.arange(<span class="number">100</span>), y[<span class="number">0</span>, :, <span class="number">4</span>:<span class="number">8</span>].data.numpy())</span><br><span class="line">plt.legend([<span class="string">"dim %d"</span> %p <span class="keyword">for</span> p <span class="keyword">in</span> [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/11/transformers/transformer/position_embedding.png" alt></p>
<p>可以看到某个序列中不同位置的单词，在某一维度上的位置编码数值不一样，即同一序列的不同单词在单个纬度符合某个正弦或者余弦，可认为他们的具有相对关系。</p>
<h2 id="encoder">Encoder</h2>
<p>Encoder部分是由个层相同小Encoder Layer串联而成。小Encoder Layer可以简化为两个部分：</p>
<ol>
<li>Multi-Head Self Attention</li>
<li>Feed-Forward Network</li>
</ol>
<p><code>Multi-Head Self Attention</code> 和<code>Feed-Forward Network</code>之后都接了一层<code>Add</code> 和<code>Norm</code></p>
<p>示意图如下:</p>
<p><img src="/2020/05/11/transformers/transformer/encoder_layer.png" alt></p>
<h3 id="muti-head-attention">Muti-Head-Attention</h3>
<p>Multi-Head Self Attention 实际上是<strong>由h个Self Attention 层并行组成，原文中h=8</strong>。</p>
<h4 id="self-attention">Self-Attention</h4>
<p>self-attention的输入是序列词向量<code>x</code>。<code>x</code>经过一个线性变换得到<code>query(Q)</code>, <code>x</code>经过第二个线性变换得到<code>key(K)</code>,<code> x</code>经过第三个线性变换得到<code>value(V)</code>。也就是：</p>
<ul>
<li>Q = linear_q(x)</li>
<li>K = linear_k(x)</li>
<li>V = linear_v(x)</li>
</ul>
<p>即：</p>
<p><img src="/2020/05/11/transformers/transformer/qkv.png" alt></p>
<p>linear_k, linear_q, linear_v是相互独立、权重\(𝑊^𝑄,𝑊^𝐾,W^V\))是不同的，通过训练可得到。得到query(Q)，key(K)，value(V)之后按照下面的公式计算attention(Q, K, V)：<br>
\[
\text {Attention}(Q, K, V)=\text {Softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
\]<br>
<img src="/2020/05/11/transformers/transformer/attention.png" alt></p>
<p>这里<code>Z</code>就是<code>attention(Q, K, V)</code>，\(𝑑_𝑘=\frac{𝑑_{𝑚𝑜𝑑𝑒𝑙}}{ℎ}=\frac{512}{8}=64\)。</p>
<ol>
<li>
<p>为什么要用\(\sqrt{d_k}\) 对 \(𝑄𝐾^𝑇\)进行缩放呢？</p>
<p>\(d_k\)实际上是<code>Q/K/V</code>的最后一个维度，当\(d_k\)越大，\(QK^T\)就越大，可能会将softmax函数推入梯度极小的区域。</p>
</li>
<li>
<p>softmax之后值都介于0到1之间，可以理解成得到了 attention weights。然后基于这个 attention weights 对 V 求 weighted sum 值 Attention(Q, K, V)。</p>
</li>
</ol>
<p>Multi-Head-Attention 就是将<code>embedding</code>之后的X按维度\(𝑑_{𝑚𝑜𝑑𝑒𝑙}=512\) 切割成\(ℎ=8\)个，分别做self-attention之后再合并在一起。</p>
<p><img src="/2020/05/11/transformers/transformer/attention_1.png" alt></p>
<figure class="highlight python"><figcaption><span>Attention</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute 'Scaled Dot Product Attention</span></span><br><span class="line"><span class="string">    qkv :[batch, h, seq_len, embed_size/h(d_k)]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)  <span class="comment"># mask </span></span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)     <span class="comment"># dropout=0.1</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/11/transformers/transformer/multi_head.png" alt></p>
<figure class="highlight python"><figcaption><span>MultiHeadedAttention</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        实现MultiHeadedAttention。</span></span><br><span class="line"><span class="string">           输入的q，k，v是形状 [batch, seq_len, embed_size(d_model)]。</span></span><br><span class="line"><span class="string">           输出的x 的形状同上。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        <span class="comment">#    [batch, seq_len, embed_size] -&gt;[batch, h, seq_len, embed_size/h]</span></span><br><span class="line">        query, key, value = [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch.</span></span><br><span class="line">        <span class="comment">#    计算注意力attn 得到attn*v 与attn </span></span><br><span class="line">        <span class="comment">#    qkv :[batch, h, seq_len, embed_size/h] --&gt;</span></span><br><span class="line">        <span class="comment">#              x:[batch, h, seq_len, embed_size/h], attn[batch, h, seq_len, seq_len]</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) "Concat" using a view and apply a final linear.</span></span><br><span class="line">        <span class="comment">#     上一步的结果合并在一起还原成原始输入序列的形状</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)  <span class="comment"># 最后再过一个线性层</span></span><br></pre></td></tr></table></figure>
<h4 id="self-attention-de-you-dian">Self-Attention的优点</h4>
<ol>
<li>因为每个词都和周围所有词做attention，所以任意两个位置都相当于有直连线路，可捕获长距离依赖。</li>
<li>Attention的可解释性更好，根据Attention score可以知道一个词和哪些词的关系比较大。</li>
<li>易于并行化，当前层的Attention计算只和前一层的值有关，所以一层的所有节点可并行执行self-attention操作。计算效率高，一次Self-Attention只需要两次矩阵运算，速度很快。</li>
</ol>
<h3 id="add-amp-norm">Add &amp; Norm</h3>
<p><code>x</code> 序列经过<code>Multi-Head-Self-Attention</code> 之后实际经过一个<code>Add+Norm</code>层，再进入<code>feed-forward network(FFN)</code>，在<code>FFN</code>之后又经过一个<code>norm</code>再输入下一个encoder layer。几乎每个sub layer之后都会经过一个归一化，然后再加在原来的输入上。</p>
<figure class="highlight python"><figcaption><span>Norm</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features)) </span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features)) </span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Add</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="string">"Apply residual connection to any sublayer with the same size."</span></span><br><span class="line">        <span class="comment"># sublayer &lt;-- encoder layer</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure>
<h3 id="feed-forward-network">Feed-Forward Network</h3>
<p>Feed-Forward Network可以细分为有两层，第一层是一个线性激活函数，第二层是激活函数是ReLU。可以表示为：<br>
\[
F F N=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}
\]</p>
<figure class="highlight python"><figcaption><span>PositionwiseFeedForward</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Implements FFN equation.</span></span><br><span class="line"><span class="string">    positionwise体现每个单词上做linear，单词之间各不相同</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">      <span class="string">"""</span></span><br><span class="line"><span class="string">      d_model: embedding_size</span></span><br><span class="line"><span class="string">      d_ff: 线形层输出维度</span></span><br><span class="line"><span class="string">      """</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure>
<h3 id="zu-he-chu-encoder">组合出Encoder</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span><span class="params">(module, N)</span>:</span></span><br><span class="line">    <span class="string">"Produce N identical layers."</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)  <span class="comment"># sub layer</span></span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Pass the input (and mask) through each layer in turn."</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<h3 id="xiao-jie">小结</h3>
<p>总的来说Encoder 是由上述小encoder layer 6个串行叠加组成。encoder sub layer主要包含两个部分：</p>
<ul>
<li>SubLayer-1 做 Multi-Headed Attention</li>
<li>SubLayer-2 做 Feed Forward Neural Network</li>
</ul>
<h2 id="decoder">Decoder</h2>
<p>Decoder与Encoder有所不同，Encoder与Decoder的关系可以用下图描述：</p>
<p><img src="/2020/05/11/transformers/transformer/decoder.png" alt></p>
<figure class="highlight python"><figcaption><span>Decoder</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Generic N layer decoder with masking."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span> </span><br><span class="line">      	<span class="comment"># memory Encoder最后的输出。 </span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<p>Decoder 子结构（sub layer）：</p>
<p><img src="/2020/05/11/transformers/transformer/decoder_layer.png" alt></p>
<p>Decoder 也是N=6层堆叠的结构。被分为3个 SubLayer，Encoder与Decoder有三大主要的不同：</p>
<ol>
<li>Decoder SubLayer-1 使用的是 “Masked” Multi-Headed Attention 机制，防止为了模型看到要预测的数据，防止泄露。</li>
<li>SubLayer-2 是一个 Encoder-Decoder Multi-head Attention。</li>
<li>LinearLayer 和 SoftmaxLayer 作用于 SubLayer-3 的输出后面，来预测对应的 word 的 probabilities 。</li>
</ol>
<h3 id="mask-multi-head-attention">Mask-Multi-Head-Attention</h3>
<p>Mask 的目的是防止 Decoder “seeing the future”，防止提前知道预测的内容（ensures that the predictions for position \(i\) can depend only on the known outputs at positions less than \(i\).）,这也说明Transformer只是在Encoder阶段可以并行化，Decoder阶段需要一个个词顺序翻译，依然是<strong>串行</strong>的(参考Greedy Decoding代码部分。这里mask是一个下三角矩阵，对角线以及对角线左下都是1，其余都是0。</p>
<figure class="highlight python"><figcaption><span>subsequent_mask</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="string">""""Mask out subsequent positions.</span></span><br><span class="line"><span class="string">    mask后续的位置，返回[size, size]尺寸下三角Tensor</span></span><br><span class="line"><span class="string">    对角线及其左下角全是1，右上角全是0</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    <span class="comment"># np.triu()  Upper triangle of an array: 返回矩阵上三角</span></span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span> <span class="comment"># 等于0，返回的是矩阵的下三角</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.imshow(subsequent_mask(<span class="number">20</span>)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="/2020/05/11/transformers/transformer/subsequent_mask.png" alt></p>
<p>subsequent_mask 返回结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]], dtype=torch.uint8)</span><br></pre></td></tr></table></figure>
<p>当mask不为空的时候，attention计算需要将x做一个操作：scores = scores.masked_fill(mask == 0, -1e9)。即将mask==0的替换为-1e9,其余不变。</p>
<h3 id="encoder-decoder-multi-head-attention">Encoder-Decoder Multi-head Attention</h3>
<p>这部分和Multi-head Attention的区别是该层的输入来自encoder和上一次decoder的结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (right) for connections."</span></span><br><span class="line">      	<span class="comment"># 将decoder的三个Sublayer串联起来</span></span><br><span class="line">        m = memory <span class="comment"># 为encoder 最后的输出</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>
<h3 id="linear-and-softmax-to-produce-output-probabilities">Linear and Softmax to Produce Output Probabilities</h3>
<p>Decoder的最后一个部分是过一个linear layer将decoder的输出扩展到与vocabulary size一样的维度上。经过softmax 后，选择概率最高的一个word作为预测结果。假设我们有一个已经训练好的网络，在做预测时，步骤如下：</p>
<ol>
<li>给 decoder 输入 encoder 对整个句子 embedding 的结果和一个特殊的开始符号 。decoder 将产生预测，在例子中应该是 <code>I</code>。</li>
<li>给 decoder 输入 encoder 的 embedding 结果和 “ I”，在这一步 decoder 应该产生预测 <code>am</code>。</li>
<li>给 decoder 输入 encoder 的 embedding 结果和 “ I am”，在这一步 decoder 应该产生预测 <code>a</code>。</li>
<li>给 decoder 输入 encoder 的 embedding 结果和 “ I am a”，在这一步 decoder 应该产生预测 <code>student</code>。</li>
<li>给 decoder 输入 encoder 的 embedding 结果和 “I am a student”, decoder应该生成句子结尾的标记，decoder 应该输出 <code>&lt;/eos&gt;</code>。</li>
<li>然后 decoder 生成了 ，翻译完成。</li>
</ol>
<p>这部分的代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Define standard linear + softmax generation step."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">    		<span class="string">"""</span></span><br><span class="line"><span class="string">    		d_model:decoder 输出的embedding size</span></span><br><span class="line"><span class="string">    		vocab：目标的词汇表大小</span></span><br><span class="line"><span class="string">    		"""</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="encoder-decoder">EncoderDecoder</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many </span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed <span class="comment"># embedding</span></span><br><span class="line">        self.tgt_embed = tgt_embed <span class="comment"># embedding</span></span><br><span class="line">        self.generator = generator <span class="comment"># 用于生成翻译目标（Linear + softmax --&gt; prob）</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Take in and process masked src and target sequences."</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure>
<h2 id="full-model">Full Model</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout) <span class="comment"># linear(relu(linear(x)))</span></span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h1 id="training">Training</h1>
<p>这部分主要根据自己的理解对代码加注释</p>
<h2 id="batches-and-masking">Batches and Masking</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Batch</span>:</span></span><br><span class="line">    <span class="string">"Object for holding a batch of data with mask during training."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, src, trg=None, pad=<span class="number">0</span>)</span>:</span></span><br><span class="line">        self.src = src</span><br><span class="line">        self.src_mask = (src != pad).unsqueeze(<span class="number">-2</span>) <span class="comment"># 对超出句子长度部分mask</span></span><br><span class="line">        <span class="keyword">if</span> trg <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.trg = trg[:, :<span class="number">-1</span>]</span><br><span class="line">            self.trg_y = trg[:, <span class="number">1</span>:]</span><br><span class="line">            self.trg_mask = self.make_std_mask(self.trg, pad) <span class="comment"># mask to hide padding and future words</span></span><br><span class="line">            self.ntokens = (self.trg_y != pad).data.sum()</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_std_mask</span><span class="params">(tgt, pad)</span>:</span></span><br><span class="line">        <span class="string">"Create a mask to hide padding and future words."</span></span><br><span class="line">        tgt_mask = (tgt != pad).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">        tgt_mask = tgt_mask &amp; Variable(</span><br><span class="line">            subsequent_mask(tgt.size(<span class="number">-1</span>)).type_as(tgt_mask.data))</span><br><span class="line">        <span class="keyword">return</span> tgt_mask</span><br></pre></td></tr></table></figure>
<h2 id="training-loop">Training Loop</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(data_iter, model, loss_compute)</span>:</span></span><br><span class="line">    <span class="string">"Standard Training and Logging Function"</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    total_tokens = <span class="number">0</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    tokens = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(data_iter):</span><br><span class="line">        out = model.forward(batch.src, batch.trg, </span><br><span class="line">                            batch.src_mask, batch.trg_mask)</span><br><span class="line">        loss = loss_compute(out, batch.trg_y, batch.ntokens)</span><br><span class="line">        total_loss += loss</span><br><span class="line">        total_tokens += batch.ntokens</span><br><span class="line">        tokens += batch.ntokens</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">1</span>:</span><br><span class="line">            elapsed = time.time() - start</span><br><span class="line">            print(<span class="string">"Epoch Step: %d Loss: %f Tokens per Sec: %f"</span> %</span><br><span class="line">                    (i, loss / batch.ntokens, tokens / elapsed))</span><br><span class="line">            start = time.time()</span><br><span class="line">            tokens = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> total_loss / total_tokens</span><br></pre></td></tr></table></figure>
<h2 id="training-data">Training Data</h2>
<p>使用标准WMT 2014英语-德语数据集进行了训练，该数据集包含大约450万个句子对。 使用字节对的编码方法对句子进行编码，该编码具有大约37000个词的共享源-目标词汇表。 对于英语-法语，使用了WMT 2014 英语-法语数据集，该数据集由36M个句子组成，并将词分成32000个词片(Word-piece)的词汇表。句子对按照近似的序列长度进行批处理。每个训练批包含一组句子对，包含大约25000个源词和25000个目标词。</p>
<p>使用torch text来创建batch。在torchtext的一个函数中创建batch，确保填充到最大batch训练长度的大小不超过阈值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">global</span> max_src_in_batch, max_tgt_in_batch</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_size_fn</span><span class="params">(new, count, sofar)</span>:</span></span><br><span class="line">    <span class="string">"Keep augmenting batch and calculate total number of tokens + padding."</span></span><br><span class="line">    <span class="keyword">global</span> max_src_in_batch, max_tgt_in_batch</span><br><span class="line">    <span class="keyword">if</span> count == <span class="number">1</span>: </span><br><span class="line">        max_src_in_batch = <span class="number">0</span></span><br><span class="line">        max_tgt_in_batch = <span class="number">0</span></span><br><span class="line">    max_src_in_batch = max(max_src_in_batch,  len(new.src))</span><br><span class="line">    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + <span class="number">2</span>)</span><br><span class="line">    src_elements = count * max_src_in_batch</span><br><span class="line">    tgt_elements = count * max_tgt_in_batch</span><br><span class="line">    <span class="keyword">return</span> max(src_elements, tgt_elements)</span><br></pre></td></tr></table></figure>
<h2 id="optimizer">Optimizer</h2>
<p>选择Adam作为优化器，其参数为\(\beta_1 = 0.9, \beta_2=0.98,\epsilon=10^{-9}\)。根据<br>
\[
lrate = d_{model}^{- \frac{1}{2}} * min(step\_num^{- \frac{1}{2}},step\_num \cdot warmup\_steps^{-1.5})
\]<br>
在训练过程中改变了学习率。在<code>warm_up</code>中随步数线性地增加学习速率，随后与步数的反平方根成比例地减小它。预热<code>warmup_steps</code>为4000。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NoamOpt</span>:</span></span><br><span class="line">    <span class="string">"Optim wrapper that implements rate."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model_size, factor, warmup, optimizer)</span>:</span></span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self._step = <span class="number">0</span></span><br><span class="line">        self.warmup = warmup</span><br><span class="line">        self.factor = factor</span><br><span class="line">        self.model_size = model_size</span><br><span class="line">        self._rate = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"Update parameters and rate"</span></span><br><span class="line">        self._step += <span class="number">1</span></span><br><span class="line">        rate = self.rate()</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.optimizer.param_groups:</span><br><span class="line">            p[<span class="string">'lr'</span>] = rate</span><br><span class="line">        self._rate = rate</span><br><span class="line">        self.optimizer.step()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rate</span><span class="params">(self, step = None)</span>:</span></span><br><span class="line">        <span class="string">"Implement `lrate` above"</span></span><br><span class="line">        <span class="keyword">if</span> step <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            step = self._step</span><br><span class="line">        <span class="comment"># 对应上面公式</span></span><br><span class="line">        <span class="keyword">return</span> self.factor * (self.model_size ** (<span class="number">-0.5</span>) * min(step ** (<span class="number">-0.5</span>), step * self.warmup ** (<span class="number">-1.5</span>)))</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_std_opt</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">2</span>, <span class="number">4000</span>,torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">opts = [NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="literal">None</span>), </span><br><span class="line">        NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">8000</span>, <span class="literal">None</span>),</span><br><span class="line">        NoamOpt(<span class="number">256</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="literal">None</span>)]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">20000</span>), [[opt.rate(i) <span class="keyword">for</span> opt <span class="keyword">in</span> opts] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">20000</span>)])</span><br><span class="line">plt.legend([<span class="string">"512:4000"</span>, <span class="string">"512:8000"</span>, <span class="string">"256:4000"</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/11/transformers/transformer/warm_up.png" alt></p>
<p>也就是说：</p>
<ol>
<li>在embedding size相同的情况下，warm up步数越少，前期的学习率曲线越陡峭，学的越快。</li>
<li>在warm up步数相同的时候，embedding size 越小，前期的学习率曲线越陡峭，学的越快。</li>
</ol>
<h2 id="regularization">Regularization</h2>
<h3 id="label-smoothing">Label Smoothing</h3>
<h4 id="bei-jing-jie-shao">背景介绍</h4>
<p>在多分类训练任务中，输入经过神级网络的计算，会得到当前输入对应于各个类别的置信度分数，这些分数会被softmax进行归一化处理，最终得到当前输入属于每个类别的概率。<br>
\[
q_{i}=\frac{\exp \left(z_{i}\right)}{\sum_{j=1}^{K} \exp \left(z_{j}\right)}
\]<br>
之后在使用交叉熵函数来计算损失值：<br>
\[
\begin{aligned}
&amp;L o s s=-\sum_{i=1}^{K} p_{i} \log q_{i}\\
&amp;p_{i}=\left\{\begin{array}{l}
1, \text { if }(i=y) \\
0, i f(i \neq y)
\end{array}\right.
\end{aligned}
\]<br>
其中，i表示多类中的某一类。</p>
<p>最终在训练网络时，最小化预测概率和标签真实概率的交叉熵，从而得到最优的预测概率分布。在此过程中，为了达到最好的拟合效果，最优的预测概率分布为：<br>
\[
Z_{i}=\left\{\begin{array}{l}
+\infty, i f(i=y) \\
0, i f(i \neq y)
\end{array}\right.
\]<br>
也就是说，网络会驱使自身往正确标签和错误标签差值大的方向学习，在训练数据不足以表征所以的样本特征的情况下，就<strong>会导致网络过拟合</strong>。</p>
<h4 id="label-smoothing-yuan-li">label smoothing原理</h4>
<p>label smoothing的提出就是为了解决上述问题，是一种正则化的策略。其通过&quot;软化&quot;传统的one-hot类型标签，使得在计算损失值时能够有效抑制过拟合现象。label smoothing相当于减少真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。</p>
<p>1.label smoothing将真实概率分布作如下改变：<br>
\[
P_{i}=\left\{\begin{array}{l}
1, i f(i=y) \\
0, i f(i \neq y)
\end{array} \Longrightarrow  P_{i}=\left\{\begin{array}{l}
(1-\varepsilon), i f(i=y) \\
\frac{\varepsilon}{K-1}, i f(i \neq y)
\end{array}\right.\right.
\]<br>
其实更新后的分布就相当于往真实分布中加入了噪声，为了便于计算，该噪声服从简单的均匀分布。</p>
<p>2.与之对应，label smoothing将交叉熵损失函数作如下改变：<br>
\[
L o s s=-\sum_{i=1}^{K} p_{i} \log q_{i} \Longrightarrow \operatorname{Loss}_{i}=\left\{\begin{array}{l}
(1-\varepsilon)^{*} \operatorname{Loss}, \text {if}(i=y) \\
\varepsilon^{*} \operatorname{Loss}, \text {if}(i \neq y)
\end{array}\right.
\]<br>
3.与之对应，label smoothing将最优的预测概率分布作如下改变：<br>
\[
Z_{i}=\left\{\begin{array}{l}
+\infty, i f(i=y) \\
0, i f(i \neq y)
\end{array} \Longrightarrow Z_{i}=\left\{\begin{array}{l}
\log \frac{(k-1)(1-\varepsilon)}{\varepsilon+\alpha}, i f(i=y) \\
\alpha, i f(i \neq y)
\end{array}\right.\right.
\]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelSmoothing</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement label smoothing."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, padding_idx, smoothing=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(size_average=<span class="literal">False</span>)</span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, target)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>)) <span class="comment"># 减2，是减去目标序列的&lt;s&gt; &lt;e&gt;标识位置。</span></span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)</span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, Variable(true_dist, requires_grad=<span class="literal">False</span>))</span><br></pre></td></tr></table></figure>
<p>在训练期间，采用了值 \(\epsilon_{ls}=0.1\)的标签平滑。 这种做法提高了困惑度，因为模型变得更加不确定，但提高了准确性和BLEU分数。使用KL div loss实现标签平滑。 相比使用独热目标分布，其包含正确单词的置信度和整个词汇表中分布的其余平滑项。可以看到标签平滑的示例:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Example of label smoothing.</span></span><br><span class="line"><span class="comment"># embed_size = 5, padding_idx=0,smoothing=0.4</span></span><br><span class="line">crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.4</span>)</span><br><span class="line">predict = torch.FloatTensor([[<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">                             [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>], </span><br><span class="line">                             [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>]])</span><br><span class="line">v = crit(Variable(predict.log()), </span><br><span class="line">         Variable(torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the target distributions expected by the system.</span></span><br><span class="line">plt.imshow(crit.true_dist)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/11/transformers/transformer/label_smooth.png" alt></p>
<p>以紫色为例：0号标签应该以<code>-0.5</code>为目标，<code>1,2,3,4</code>以<code>1.5</code>为目标</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(x)</span>:</span></span><br><span class="line">    d = x + <span class="number">3</span> * <span class="number">1</span></span><br><span class="line">    predict = torch.FloatTensor([[<span class="number">0</span>, x / d, <span class="number">1</span> / d, <span class="number">1</span> / d, <span class="number">1</span> / d],</span><br><span class="line">                                 ])</span><br><span class="line">    <span class="comment">#print(predict)</span></span><br><span class="line">    <span class="keyword">return</span> crit(Variable(predict.log()),</span><br><span class="line">                 Variable(torch.LongTensor([<span class="number">1</span>]))).data[<span class="number">0</span>]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">100</span>), [loss(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>)])</span><br></pre></td></tr></table></figure>
<p>如果对给定的选择非常有信心，标签平滑实际上会开始惩罚模型。</p>
<p><img src="/2020/05/11/transformers/transformer/penate.png" alt></p>
<h2 id="examples">Examples</h2>
<h3 id="copy-task">Copy Task</h3>
<h4 id="synthetic-data">Synthetic Data</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_gen</span><span class="params">(V, batch, nbatches)</span>:</span></span><br><span class="line">    <span class="string">"Generate random data for a src-tgt copy task."</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nbatches):</span><br><span class="line">        data = torch.from_numpy(np.random.randint(<span class="number">1</span>, V, size=(batch, <span class="number">10</span>)))</span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        src = Variable(data, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        tgt = Variable(data, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> Batch(src, tgt, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="loss-computation">Loss Computation</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleLossCompute</span>:</span></span><br><span class="line">    <span class="string">"A simple loss compute and train function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, generator, criterion, opt=None)</span>:</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        self.criterion = criterion</span><br><span class="line">        self.opt = opt</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x, y, norm)</span>:</span></span><br><span class="line">        x = self.generator(x)</span><br><span class="line">        loss = self.criterion(x.contiguous().view(<span class="number">-1</span>, x.size(<span class="number">-1</span>)), </span><br><span class="line">                              y.contiguous().view(<span class="number">-1</span>)) / norm</span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">if</span> self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.opt.step()</span><br><span class="line">            self.opt.optimizer.zero_grad()</span><br><span class="line">        <span class="keyword">return</span> loss.data[<span class="number">0</span>] * norm</span><br></pre></td></tr></table></figure>
<h4 id="span-id-greedy-greedy-decoding-span"><span id="greedy">Greedy Decoding </span></h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Train the simple copy task.</span></span><br><span class="line">V = <span class="number">11</span></span><br><span class="line">criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line">model = make_model(V, V, N=<span class="number">2</span>)</span><br><span class="line">model_opt = NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">1</span>, <span class="number">400</span>,</span><br><span class="line">        torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    run_epoch(data_gen(V, <span class="number">30</span>, <span class="number">20</span>), model, </span><br><span class="line">              SimpleLossCompute(model.generator, criterion, model_opt))</span><br><span class="line">    model.eval()</span><br><span class="line">    print(run_epoch(data_gen(V, <span class="number">30</span>, <span class="number">5</span>), model, </span><br><span class="line">                    SimpleLossCompute(model.generator, criterion, <span class="literal">None</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greedy_decode</span><span class="params">(model, src, src_mask, max_len, start_symbol)</span>:</span></span><br><span class="line">    memory = model.encode(src, src_mask)</span><br><span class="line">    ys = torch.ones(<span class="number">1</span>, <span class="number">1</span>).fill_(start_symbol).type_as(src.data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_len<span class="number">-1</span>):</span><br><span class="line">        out = model.decode(memory, src_mask, </span><br><span class="line">                           Variable(ys), </span><br><span class="line">                           Variable(subsequent_mask(ys.size(<span class="number">1</span>))</span><br><span class="line">                                    .type_as(src.data)))</span><br><span class="line">        prob = model.generator(out[:, <span class="number">-1</span>])</span><br><span class="line">        _, next_word = torch.max(prob, dim = <span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat([ys, </span><br><span class="line">                        torch.ones(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ys</span><br><span class="line"></span><br><span class="line">model.eval()</span><br><span class="line">src = Variable(torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]]) )</span><br><span class="line">src_mask = Variable(torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>) )</span><br><span class="line">print(greedy_decode(model, src, src_mask, max_len=<span class="number">10</span>, start_symbol=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>result</span></figcaption><table><tr><td class="code"><pre><span class="line">  <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span>     <span class="number">4</span>     <span class="number">5</span>     <span class="number">6</span>     <span class="number">7</span>     <span class="number">8</span>     <span class="number">9</span>    <span class="number">10</span></span><br><span class="line">[torch.LongTensor of size <span class="number">1</span>x10]</span><br></pre></td></tr></table></figure>
<p>翻译的例子涉及GPU并行比较复杂，不做介绍。</p>
<h1 id="attention-visualization">Attention Visualization</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tgt_sent = trans.split()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(data, x, y, ax)</span>:</span></span><br><span class="line">    seaborn.heatmap(data, </span><br><span class="line">                    xticklabels=x, square=<span class="literal">True</span>, yticklabels=y, vmin=<span class="number">0.0</span>, vmax=<span class="number">1.0</span>, </span><br><span class="line">                    cbar=<span class="literal">False</span>, ax=ax)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    print(<span class="string">"Encoder Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.encoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data, sent, sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    print(<span class="string">"Decoder Self Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.decoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data[:len(tgt_sent), :len(tgt_sent)], tgt_sent, tgt_sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"Decoder Src Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.decoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data[:len(tgt_sent), :len(sent)],sent, tgt_sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="encoder-visualization">Encoder visualization</h2>
<div class="note info">
            <p>Encoder Layer 2</p>
          </div>
<p><img src="/2020/05/11/transformers/transformer/encoder_layer_2.png" alt></p>
<div class="note info">
            <p>Encoder Layer 2</p>
          </div>
<p><img src="/2020/05/11/transformers/transformer/encoder_layer_4.png" alt></p>
<div class="note info">
            <p>Encoder Layer 6</p>
          </div>
<p><img src="/2020/05/11/transformers/transformer/image-20200511115252714.png" alt></p>
<p>同一行，比较不同的head,可以看出，不同的head，attention到的内容是各不相同的。</p>
<h2 id="decoder-visualization">Decoder visualization</h2>
<div class="note info">
            <p>decoder Self Layer 2 :<code>&lt;s&gt;</code>会attention到所有单词，单词大多会attention到自己。</p>
          </div>
<p><img src="/2020/05/11/transformers/transformer/image-20200511120009561.png" alt></p>
<div class="note info">
            <p>decoder Src Layer 2</p>
          </div>
<p><img src="/2020/05/11/transformers/transformer/image-20200511120705604.png" alt></p>
<div class="note info">
            <p>decoder Self Layer 4</p>
          </div>
<p><img src="/2020/05/11/transformers/transformer/image-20200511120927309.png" alt></p>
<div class="note info">
            <p>decoder Src Layer 4</p>
          </div>
<p><img src="/2020/05/11/transformers/transformer/image-20200511120946538.png" alt></p>
<div class="note info">
            <p>decoder Self Layer 6</p>
          </div>
<p><img src="/2020/05/11/transformers/transformer/image-20200511121002344.png" alt></p>
<div class="note info">
            <p>decoder Src Layer 6</p>
          </div>
<p><img src="/2020/05/11/transformers/transformer/image-20200511121021648.png" alt></p>
<p>上三角全黑：表示decoder是一个串行计算，每一个词只会attention到其前面的词。</p>
<h1 id="zong-jie">总结</h1>
<h2 id="tricks">tricks</h2>
<p>在训练过程中，模型没有收敛得很好时，Decoder预测产生的词很可能不是我们想要的。这个时候如果再把错误的数据再输给Decoder，就会越跑越偏。这个时候怎么办？</p>
<ul>
<li>在训练过程中可以使用 “teacher forcing”。因为我们知道应该预测的word是什么，那么可以给Decoder喂一个正确的结果作为输入。</li>
<li>除了选择最高概率的词 (greedy search)，还可以选择是比如 “Beam Search”，可以保留topK个预测的word。 Beam Search 方法不再是只得到一个输出放到下一步去训练了，我们可以设定一个值，拿多个值放到下一步去训练，这条路径的概率等于每一步输出的概率的乘积。</li>
</ul>
<h2 id="transformer-de-you-que-dian">Transformer的优缺点</h2>
<h3 id="you-dian">优点</h3>
<ol>
<li>每层计算复杂度比RNN要低。</li>
<li>可以进行<strong>并行计算</strong>。</li>
<li>从计算一个序列长度为n的信息要经过的路径长度来看, CNN需要增加卷积层数来扩大视野，RNN需要从1到n逐个进行计算，而Self-attention只需要一步矩阵计算就可以。Self-Attention可以比RNN更好地解决长时依赖问题。当然如果计算量太大，比如序列长度N大于序列维度D这种情况，也可以用窗口限制Self-Attention的计算数量。</li>
<li>从作者在附录中给出的栗子可以看出，Self-Attention模型更可解释，Attention结果的分布表明了该模型学习到了一些语法和语义信息。</li>
</ol>
<h3 id="que-dian">缺点</h3>
<ol>
<li>实践上：有些RNN轻易可以解决的问题transformer没做到，比如<strong>复制string</strong>，或者推理时碰到的sequence长度比训练时更长（因为<strong>碰到了没见过的position embedding</strong>）。</li>
<li>理论上：transformers不是computationally universal(图灵完备)，这种非RNN式的模型是非图灵完备的的，<strong>无法单独完成NLP中推理、决策等计算问题</strong>（包括使用transformer的bert模型等等）。</li>
</ol>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">The Annotated Transformer</a></li>
<li><a href="https://www.cnblogs.com/zingp/p/11696111.html" target="_blank" rel="noopener">深入理解Transformer及其源码</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>smoothing_method</title>
    <url>/2020/05/08/smoothing_method/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/08/smoothing_method/2gramsmooth.png" alt></p>
<a id="more"></a>
<h3 id="jian-jie">简介</h3>
<p>在一个概率模型中，也就是 p(e)在 event space E 下的概率分布，模型很可能会用最大似然估计(MLE)：<br>
\[
P_{M L E}=\frac{c(x)}{\sum_{e} c(e)}
\]<br>
然而，由于并没有足够的数据，很多事件 \(x\) 并没有在训练数据中出现，也就是 \(c(x)=0\)，\(P_{MLE}=0\)这是有问题的，没有在训练数据中出现的数据，并不代表不会在测试数据中出现，如果没有考虑到数据稀疏性，模型就显得太简单了。</p>
<p>Data sparsity 是 smoothing 的最大原因。Chen &amp; Goodman 在1998 年提到过，几乎所有数据稀疏的场景下，smoothing 都可以帮助提高 performance，而数据稀疏性几乎是所有统计模型都会遇到的问题。而如果你有足够多的训练数据，所有的 parameters 都可以在没有 smoothing 的情况下被准确的估计，那么你总是可以扩展模型，如原来是 bigram，没有数据稀疏，完全可以扩展到 trigram 来提高 performance，如果还没有出现稀疏，就再往高层推，当 parameters 越来越多的时候，数据稀疏再次成为了问题，这时候，用合适的smoothing_method可以得到更准确的模型。实际上，无论有多少的数据，平滑几乎总是可以以很小的代价来提高 performance。</p>
<h3 id="smoothing-method">smoothing_method</h3>
<h4 id="add-one-smoothing">add-one smoothing</h4>
<p>(也叫laplace smoothing),以bigram为例：</p>
<p>MLE estimate：<br>
\[
P_{M L E}\left(w_{i} | w_{i-1}\right)=\frac{c\left(w_{i-1} w_{i}\right)}{c\left(w_{i-1}\right)}
\]<br>
Add-one estimate:<br>
\[
P_{A d d-1}\left(w_{i} | w_{i-1}\right)=\frac{c\left(w_{i-1} w_{i}\right)+1}{c\left(w_{i-1}\right)+V}
\]<br>
其中，\(V\)表示词典大小。</p>
<p>假设语料库为：</p>
<blockquote>
<ol>
<li>john read moby dick</li>
<li>mary read a different book</li>
<li>she read a book by cher</li>
</ol>
</blockquote>
<p>那么，john read a book 这个句子的概率为：</p>
<p>\[
\begin{aligned}
p(\text { john read a book })&amp;=p(\text { john } |&lt;s>)  p(\text { read } | \text { john })  p(\text { a } | \text { read })  p(\text { book } | a )  p(&lt;e> | \text {book}) \\
&amp;=\frac{c(&lt;s>,john)}{\sum_{w} c(&lt;s>,w)} \frac{c(john,read)}{\sum_{w} c(john,w)} \frac{c(read,a)}{\sum_{w} c(read,w)} \frac{c(a,book)}{\sum_{w} c(a,w)} \frac{c(book,&lt;e>)}{\sum_{w} c(book,w)}\\
&amp;= \frac{1}{3} * \frac{1}{1} * \frac{2}{3} * \frac{1}{2} * \frac{1}{2} \\
&amp;\approx 0.006
\end{aligned}
\]</p>
<p>而，cher read a book 这个句子出现的概率是：</p>
<p>\[
\begin{aligned}
p(\text { cher read a book })&amp;=p(\text { cher } |&lt;s>)  p(\text { read } | \text { cher })  p(\text { a } | \text { read })  p(\text { book } | a )  p(&lt;e> | \text {book}) \\
&amp;=\frac{c(&lt;s>,cher)}{\sum_{w} c(&lt;s>,w)} \frac{c(cher,read)}{\sum_{w} c(cher,w)} \frac{c(read,a)}{\sum_{w} c(read,w)} \frac{c(a,book)}{\sum_{w} c(a,w)} \frac{c(book,&lt;e>)}{\sum_{w} c(book,w)}\\
&amp;= \frac{0}{3} * \frac{0}{1} * \frac{2}{3} * \frac{1}{2} * \frac{1}{2} \\
&amp;\approx 0
\end{aligned}
\]<br>
如果使用add-one smoothing：<br>
\[
\begin{aligned}
p(\text { john read a book })&amp;=p(\text { john } |&lt;s>)  p(\text { read } | \text { john })  p(\text { a } | \text { read })  p(\text { book } | a )  p(&lt;e> | \text {book}) \\
&amp;= \frac{1+1}{3+11} * \frac{1+1}{1+11} * \frac{2+1}{3+11} * \frac{1+1}{2+11} * \frac{1+1}{2+11} \\
&amp;\approx 0.0001
\end{aligned}
\]</p>
<p>\[
\begin{aligned}
p(\text { cher read a book })&amp;=p(\text { cher } |&lt;s>)  p(\text { read } | \text { cher })  p(\text { a } | \text { read })  p(\text { book } | a )  p(&lt;e> | \text {book}) \\
&amp;= \frac{0+1}{3+11} * \frac{0+1}{1+11} * \frac{2+1}{3+11} * \frac{1+1}{2+11} * \frac{1+1}{2+11} \\
&amp;\approx 0.00003
\end{aligned}
\]</p>
<p>加 1 平滑通常情况下是一种很糟糕的算法，与其他smoothing_method相比显得非常差，可以把加 1 平滑用在其他任务中，如文本分类，或者非零计数没那么多的情况下。</p>
<h4 id="additive-smoothing">Additive smoothing</h4>
<p>对加 1 平滑的改进就是把 1 改成 \(\delta\)，且 \(0&lt;\delta\)<br>
\[
P_{A d d-\delta}\left(w_{i} | w_{i-1}\right)=\frac{c\left(w_{i-1} w_{i}\right)+\delta}{c\left(w_{i-1}\right)+\delta V}
\]<br>
可以把\(\delta\)看作是一个可以调节的超参数。</p>
<h4 id="good-turing-smoothing">Good-Turing smoothing</h4>
<p><strong>基本思想:</strong> 用观察计数较高的 N-gram 数量来重新估计概率量大小，并把它指派给那些具有零计数或较低计数的 N-gram.</p>
<p>举个例子：假设你在钓鱼，然后抓到了 18 条鱼，种类如下：10条鲤鱼, 3条黑鱼, 2条刀鱼, 1条鲨鱼, 1条草鱼, 1条鳗鱼，那么</p>
<ol>
<li>
<p>下一个钓到的鱼是鲨鱼的概率是多少？</p>
<p>1/18</p>
</li>
<li>
<p>下一条与是新鱼种（之前没有出现过）的概率是多少？</p>
<p>3/18(没有出现过的按出现过一次算)</p>
</li>
<li>
<p>下一条抓到的鱼是鲨鱼的概率是多少？</p>
<p>首先肯定的是概率小于1/18  --&gt; good turing</p>
</li>
</ol>
<p>在good turing 下,对参数的估计分两种情况：</p>
<ol>
<li>没有出现过的：\(P_{GT}=\frac{N_1}{N}\)</li>
<li>出现过的：\(P_{GT}=\frac{(c+1)N_{c+1}}{N_c*N}\)</li>
</ol>
<p>其中，\(N_c\)表示出现c次的单词个数,\(N\)代表样本总数。<br>
比如：</p>
<blockquote>
<p>sam i am i am sam i do not eat</p>
<p>sam：2次</p>
<p>i：3次</p>
<p>am：2次</p>
<p>do：1次</p>
<p>not：1次</p>
<p>eat：1次</p>
</blockquote>
<p>有：\(N_3=1,N_2=2,N_1=3\)</p>
<p>对于第3个问题：\(P_{GT}=\frac{(1+1)*1}{3*18}=\frac{1}{27}\).</p>
<h5 id="que-dian">缺点</h5>
<p>由公式：\(P_{GT}=\frac{(c+1)N_{c+1}}{N_c*N}\)在计算当前项\(c\) 的时候，需要计算到\(c+1\)的\(N_{c+1}\),若\(N_{c+1}=0\)，则会导致概率为0.</p>
<p>解决方法：可以利用机器学习中回归算法对\(N_1,N_2,\ldots,N_c,N_{c+1}\)进行拟合，进而得出\(N_{c+1}\)的值。</p>
<h4 id="interpolation">Interpolation</h4>
<p><strong>基本思想</strong>：在计算Trigram概率的同时，考虑Unigram，Bigram，Trigram出现的次数。</p>
<p>举个例子：有</p>
<p><code>C(in the kitchen)=0</code>,<code>C(the kitchen)=3</code>,<code>C(kitchen)=4</code>,<code>C(arboretum)=0</code>.在计算<code>p(kitchen|in the)</code>和<code>p(arboretum|in the)</code>的时候，从语料计算得出：\(p(kitchen|in,the)=p(arboretum|in,the)=0\),但是从经验上来讲\(p(kitchen|in,the)>p(arboretum|in,the)\)，因为kitchen要比arboretum常见的多。要实现这个，我们就希望把 bigram 和 unigram 结合起来，interpolate 就是这样一种方法。用线性差值把不同阶的 N-gram 结合起来，这里结合了 trigram，bigram 和 unigram。用 lambda 进行加权<br>
\[
\begin{aligned}
\mathrm{p}\left(\mathrm{w}_{\mathrm{n}} | \mathrm{w}_{\mathrm{n}-1}, \mathrm{w}_{\mathrm{n}-2}\right)=&amp; \lambda_{1} \mathrm{p}\left(\mathrm{w}_{\mathrm{n}} | \mathrm{w}_{\mathrm{n}-1}, \mathrm{w}_{\mathrm{n}-2}\right) \\
&amp;+\lambda_{2} \mathrm{p}\left(\mathrm{w}_{\mathrm{n}} | \mathrm{w}_{\mathrm{n}-1}\right) \\
&amp;+\lambda_{3} \mathrm{p}\left(\mathrm{w}_{\mathrm{n}}\right)\\
\sum_i\lambda_i=1
\end{aligned}
\]</p>
<h5 id="zen-yang-she-zhi-lambdas">怎样设置 lambdas？？？？</h5>
]]></content>
      <categories>
        <category>技术/数学</category>
      </categories>
      <tags>
        <tag>自然语言处理基础</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow2</title>
    <url>/2020/05/07/tensorflow2/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/05/07/tensorflow2/tensorflow_2-1200x600.png" alt></p>
<a id="more"></a>
<h1 id="tensor-flow-jian-mo-liu-cheng">TensorFlow建模流程</h1>
<p>尽管TensorFlow设计上足够灵活，可以用于进行各种复杂的数值计算。但通常人们使用TensorFlow来实现机器学习模型，尤其常用于实现神经网络模型。从原理上说可以使用张量构建计算图来定义神经网络，并通过自动微分机制训练模型。但为简洁起见，一般推荐使用TensorFlow的高层次keras接口来实现神经网络网模型。<br>
使用TensorFlow实现神经网络模型的一般流程包括：</p>
<p>1，准备数据</p>
<p>2，定义模型</p>
<p>3，训练模型</p>
<p>4，评估模型</p>
<p>5，使用模型</p>
<p>6，保存模型。</p>
<p>在实践中通常会遇到的数据类型包括结构化数据，图片数据，文本数据，时间序列数据。</p>
<h2 id="jie-gou-hua-shu-ju-jian-mo-liu-cheng">结构化数据建模流程</h2>
<h3 id="zhun-bei-shu-ju">准备数据</h3>
<p>titanic数据集的目标是根据乘客信息预测他们在Titanic号撞击冰山沉没后能否生存。</p>
<p>结构化数据一般会使用Pandas中的DataFrame进行预处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers</span><br><span class="line"></span><br><span class="line">dftrain_raw = pd.read_csv(<span class="string">'./data/titanic/train.csv'</span>)</span><br><span class="line">dftest_raw = pd.read_csv(<span class="string">'./data/titanic/test.csv'</span>)</span><br><span class="line">dftrain_raw.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-1-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B1%95%E7%A4%BA.jpg" alt></p>
<p>字段说明：</p>
<ul>
<li>Survived:0代表死亡，1代表存活【y标签】</li>
<li>Pclass:乘客所持票类，有三种值(1,2,3) 【转换成onehot编码】</li>
<li>Name:乘客姓名 【舍去】</li>
<li>Sex:乘客性别 【转换成bool特征】</li>
<li>Age:乘客年龄(有缺失) 【数值特征，添加“年龄是否缺失”作为辅助特征】</li>
<li>SibSp:乘客兄弟姐妹/配偶的个数(整数值) 【数值特征】</li>
<li>Parch:乘客父母/孩子的个数(整数值)【数值特征】</li>
<li>Ticket:票号(字符串)【舍去】</li>
<li>Fare:乘客所持票的价格(浮点数，0-500不等) 【数值特征】</li>
<li>Cabin:乘客所在船舱(有缺失) 【添加“所在船舱是否缺失”作为辅助特征】</li>
<li>Embarked:乘客登船港口:S、C、Q(有缺失)【转换成onehot编码，四维度 S,C,Q,nan】</li>
</ul>
<p>利用Pandas的数据可视化功能简单地进行探索性数据分析EDA（Exploratory Data Analysis）。</p>
<p>label分布情况</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'png'</span></span><br><span class="line">ax = dftrain_raw[<span class="string">'Survived'</span>].value_counts().plot(kind = <span class="string">'bar'</span>,</span><br><span class="line">     figsize = (<span class="number">12</span>,<span class="number">8</span>),fontsize=<span class="number">15</span>,rot = <span class="number">0</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Counts'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Survived'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-1-Label%E5%88%86%E5%B8%83.jpg" alt></p>
<p>年龄分布情况</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'png'</span></span><br><span class="line">ax = dftrain_raw[<span class="string">'Age'</span>].plot(kind = <span class="string">'hist'</span>,bins = <span class="number">20</span>,color= <span class="string">'purple'</span>,</span><br><span class="line">                    figsize = (<span class="number">12</span>,<span class="number">8</span>),fontsize=<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Age'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-1-%E5%B9%B4%E9%BE%84%E5%88%86%E5%B8%83.jpg" alt></p>
<p>年龄和label的相关性</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'png'</span></span><br><span class="line">ax = dftrain_raw.query(<span class="string">'Survived == 0'</span>)[<span class="string">'Age'</span>].plot(kind = <span class="string">'density'</span>,</span><br><span class="line">                      figsize = (<span class="number">12</span>,<span class="number">8</span>),fontsize=<span class="number">15</span>)</span><br><span class="line">dftrain_raw.query(<span class="string">'Survived == 1'</span>)[<span class="string">'Age'</span>].plot(kind = <span class="string">'density'</span>,</span><br><span class="line">                      figsize = (<span class="number">12</span>,<span class="number">8</span>),fontsize=<span class="number">15</span>)</span><br><span class="line">ax.legend([<span class="string">'Survived==0'</span>,<span class="string">'Survived==1'</span>],fontsize = <span class="number">12</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Density'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Age'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-1-%E5%B9%B4%E9%BE%84%E7%9B%B8%E5%85%B3%E6%80%A7.jpg" alt></p>
<p>下面为正式的数据预处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocessing</span><span class="params">(dfdata)</span>:</span></span><br><span class="line"></span><br><span class="line">    dfresult= pd.DataFrame()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Pclass</span></span><br><span class="line">    dfPclass = pd.get_dummies(dfdata[<span class="string">'Pclass'</span>])</span><br><span class="line">    dfPclass.columns = [<span class="string">'Pclass_'</span> +str(x) <span class="keyword">for</span> x <span class="keyword">in</span> dfPclass.columns ]</span><br><span class="line">    dfresult = pd.concat([dfresult,dfPclass],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Sex</span></span><br><span class="line">    dfSex = pd.get_dummies(dfdata[<span class="string">'Sex'</span>])</span><br><span class="line">    dfresult = pd.concat([dfresult,dfSex],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Age</span></span><br><span class="line">    dfresult[<span class="string">'Age'</span>] = dfdata[<span class="string">'Age'</span>].fillna(<span class="number">0</span>)</span><br><span class="line">    dfresult[<span class="string">'Age_null'</span>] = pd.isna(dfdata[<span class="string">'Age'</span>]).astype(<span class="string">'int32'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#SibSp,Parch,Fare</span></span><br><span class="line">    dfresult[<span class="string">'SibSp'</span>] = dfdata[<span class="string">'SibSp'</span>]</span><br><span class="line">    dfresult[<span class="string">'Parch'</span>] = dfdata[<span class="string">'Parch'</span>]</span><br><span class="line">    dfresult[<span class="string">'Fare'</span>] = dfdata[<span class="string">'Fare'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Carbin</span></span><br><span class="line">    dfresult[<span class="string">'Cabin_null'</span>] =  pd.isna(dfdata[<span class="string">'Cabin'</span>]).astype(<span class="string">'int32'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Embarked</span></span><br><span class="line">    dfEmbarked = pd.get_dummies(dfdata[<span class="string">'Embarked'</span>],dummy_na=<span class="literal">True</span>)</span><br><span class="line">    dfEmbarked.columns = [<span class="string">'Embarked_'</span> + str(x) <span class="keyword">for</span> x <span class="keyword">in</span> dfEmbarked.columns]</span><br><span class="line">    dfresult = pd.concat([dfresult,dfEmbarked],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>(dfresult)</span><br><span class="line"></span><br><span class="line">x_train = preprocessing(dftrain_raw)</span><br><span class="line">y_train = dftrain_raw[<span class="string">'Survived'</span>].values</span><br><span class="line"></span><br><span class="line">x_test = preprocessing(dftest_raw)</span><br><span class="line">y_test = dftest_raw[<span class="string">'Survived'</span>].values</span><br><span class="line"></span><br><span class="line">print(<span class="string">"x_train.shape ="</span>, x_train.shape )</span><br><span class="line">print(<span class="string">"x_test.shape ="</span>, x_test.shape )</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">x_train.shape = (<span class="number">712</span>, <span class="number">15</span>)</span><br><span class="line">x_test.shape = (<span class="number">179</span>, <span class="number">15</span>)</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing">定义模型</h3>
<p>使用Keras接口有以下3种方式构建模型：</p>
<ol>
<li>使用Sequential按层顺序构建模型</li>
<li>使用函数式API构建任意结构模型</li>
<li>继承Model基类构建自定义模型。</li>
</ol>
<p>此处选择使用最简单的Sequential，按层顺序模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">20</span>,activation = <span class="string">'relu'</span>,input_shape=(<span class="number">15</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>,activation = <span class="string">'relu'</span> ))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>,activation = <span class="string">'sigmoid'</span> ))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">dense (Dense)                (None, 20)                320       </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_1 (Dense)              (None, 10)                210       </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_2 (Dense)              (None, 1)                 11        </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 541</span><br><span class="line">Trainable params: 541</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<h3 id="xun-lian-mo-xing">训练模型</h3>
<p>训练模型通常有3种方法:</p>
<ol>
<li>内置fit方法</li>
<li>内置train_on_batch方法</li>
<li>以及自定义训练循环</li>
</ol>
<p>此处选择最常用也最简单的内置fit方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 二分类问题选择二元交叉熵损失函数</span></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">            loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'AUC'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train,y_train,</span><br><span class="line">                    batch_size= <span class="number">64</span>,</span><br><span class="line">                    epochs= <span class="number">30</span>,</span><br><span class="line">                    validation_split=<span class="number">0.2</span> <span class="comment">#分割一部分训练数据用于验证</span></span><br><span class="line">                   )</span><br></pre></td></tr></table></figure>
<h3 id="ping-gu-mo-xing">评估模型</h3>
<p>首先评估模型在训练集和验证集上的效果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">'val_'</span>+metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span>+ metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span>+metric, <span class="string">'val_'</span>+metric])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"loss"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-1-Loss%E6%9B%B2%E7%BA%BF.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"AUC"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-1-AUC%E6%9B%B2%E7%BA%BF.jpg" alt></p>
<p>再看一下模型在测试集上的效果.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.evaluate(x = x_test,y = y_test)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[<span class="number">0.5191367897907448</span>, <span class="number">0.8122605</span>]</span><br></pre></td></tr></table></figure>
<h3 id="shi-yong-mo-xing">使用模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#预测概率</span></span><br><span class="line">model.predict(x_test[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"><span class="comment">#model(tf.constant(x_test[0:10].values,dtype = tf.float32)) #等价写法</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">array([[<span class="number">0.26501188</span>],</span><br><span class="line">       [<span class="number">0.40970832</span>],</span><br><span class="line">       [<span class="number">0.44285864</span>],</span><br><span class="line">       [<span class="number">0.78408605</span>],</span><br><span class="line">       [<span class="number">0.47650957</span>],</span><br><span class="line">       [<span class="number">0.43849158</span>],</span><br><span class="line">       [<span class="number">0.27426785</span>],</span><br><span class="line">       [<span class="number">0.5962582</span> ],</span><br><span class="line">       [<span class="number">0.59476686</span>],</span><br><span class="line">       [<span class="number">0.17882936</span>]], dtype=float32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#预测类别</span></span><br><span class="line">model.predict_classes(x_test[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">array([[<span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>],</span><br><span class="line">       [<span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>],</span><br><span class="line">       [<span class="number">0</span>]], dtype=int32)</span><br></pre></td></tr></table></figure>
<h3 id="bao-cun-mo-xing">保存模型</h3>
<p>可以使用Keras方式保存模型，也可以使用TensorFlow原生方式保存。前者仅仅适合使用Python环境恢复模型，后者则可以跨平台进行模型部署。</p>
<p>推荐使用后一种方式进行保存。</p>
<h4 id="keras-fang-shi-bao-cun">Keras方式保存</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构及权重</span></span><br><span class="line"></span><br><span class="line">model.save(<span class="string">'./data/keras_model.h5'</span>)  </span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> model  <span class="comment">#删除现有模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># identical to the previous one</span></span><br><span class="line">model = models.load_model(<span class="string">'./data/keras_model.h5'</span>)</span><br><span class="line">model.evaluate(x_test,y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[<span class="number">0.5191367897907448</span>, <span class="number">0.8122605</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构</span></span><br><span class="line">json_str = model.to_json()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复模型结构</span></span><br><span class="line">model_json = models.model_from_json(json_str)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#保存模型权重</span></span><br><span class="line">model.save_weights(<span class="string">'./data/keras_model_weight.h5'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复模型结构</span></span><br><span class="line">model_json = models.model_from_json(json_str)</span><br><span class="line">model_json.compile(</span><br><span class="line">        optimizer=<span class="string">'adam'</span>,</span><br><span class="line">        loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">        metrics=[<span class="string">'AUC'</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载权重</span></span><br><span class="line">model_json.load_weights(<span class="string">'./data/keras_model_weight.h5'</span>)</span><br><span class="line">model_json.evaluate(x_test,y_test)</span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">[<span class="number">0.5191367897907448</span>, <span class="number">0.8122605</span>]</span><br></pre></td></tr></table></figure>
<h4 id="tensor-flow-yuan-sheng-fang-shi-bao-cun">TensorFlow原生方式保存</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存权重，该方式仅仅保存权重张量</span></span><br><span class="line">model.save_weights(<span class="string">'./data/tf_model_weights.ckpt'</span>,save_format = <span class="string">"tf"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构与模型参数到文件,该方式保存的模型具有跨平台性便于部署</span></span><br><span class="line">model.save(<span class="string">'./data/tf_model_savedmodel'</span>, save_format=<span class="string">"tf"</span>)</span><br><span class="line">print(<span class="string">'export saved model.'</span>)</span><br><span class="line"></span><br><span class="line">model_loaded = tf.keras.models.load_model(<span class="string">'./data/tf_model_savedmodel'</span>)</span><br><span class="line">model_loaded.evaluate(x_test,y_test)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[<span class="number">0.5191365896656527</span>, <span class="number">0.8122605</span>]</span><br></pre></td></tr></table></figure>
<h2 id="tu-pian-shu-ju-jian-mo-liu-cheng">图片数据建模流程</h2>
<h3 id="zhun-bei-shu-ju-1">准备数据</h3>
<p>cifar2数据集为cifar10数据集的子集，只包括前两种类别airplane和automobile。</p>
<p>训练集有airplane和automobile图片各5000张，测试集有airplane和automobile图片各1000张。</p>
<p>cifar2任务的目标是训练一个模型来对飞机airplane和机动车automobile两种图片进行分类。</p>
<p><img src="/2020/05/07/tensorflow2/cifar2.jpg" alt></p>
<p>在tensorflow中准备图片数据的常用方案有两种</p>
<ol>
<li>
<p>第一种是使用tf.keras中的ImageDataGenerator工具构建图片数据生成器。</p>
</li>
<li>
<p>第二种是使用tf.data.Dataset搭配tf.image中的一些图片处理方法构建数据管道。</p>
</li>
</ol>
<p>第一种方法更为简单。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line">train_dir = <span class="string">'cifar2_datasets/train'</span></span><br><span class="line">test_dir = <span class="string">'cifar2_datasets/test'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对训练集数据设置数据增强</span></span><br><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">            rescale = <span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">            rotation_range=<span class="number">40</span>,</span><br><span class="line">            width_shift_range=<span class="number">0.2</span>,</span><br><span class="line">            height_shift_range=<span class="number">0.2</span>,</span><br><span class="line">            shear_range=<span class="number">0.2</span>,</span><br><span class="line">            zoom_range=<span class="number">0.2</span>,</span><br><span class="line">            horizontal_flip=<span class="literal">True</span>,</span><br><span class="line">            fill_mode=<span class="string">'nearest'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对测试集数据无需使用数据增强</span></span><br><span class="line">test_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">                    train_dir,</span><br><span class="line">                    target_size=(<span class="number">32</span>, <span class="number">32</span>),</span><br><span class="line">                    batch_size=<span class="number">32</span>,</span><br><span class="line">                    shuffle = <span class="literal">True</span>,</span><br><span class="line">                    class_mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">test_generator = test_datagen.flow_from_directory(</span><br><span class="line">                    test_dir,</span><br><span class="line">                    target_size=(<span class="number">32</span>, <span class="number">32</span>),</span><br><span class="line">                    batch_size=<span class="number">32</span>,</span><br><span class="line">                    shuffle = <span class="literal">False</span>,</span><br><span class="line">                    class_mode=<span class="string">'binary'</span>)</span><br></pre></td></tr></table></figure>
<p>第二种方法是TensorFlow的原生方法，更加灵活，使用得当的话也可以获得更好的性能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets,layers,models</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(img_path,size = <span class="params">(<span class="number">32</span>,<span class="number">32</span>)</span>)</span>:</span></span><br><span class="line">    label = tf.constant(<span class="number">1</span>,tf.int8) <span class="keyword">if</span> tf.strings.regex_full_match(img_path,<span class="string">".*automobile.*"</span>) \</span><br><span class="line">            <span class="keyword">else</span> tf.constant(<span class="number">0</span>,tf.int8)</span><br><span class="line">    img = tf.io.read_file(img_path)</span><br><span class="line">    img = tf.image.decode_jpeg(img) <span class="comment">#注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img,size)/<span class="number">255.0</span></span><br><span class="line">    <span class="keyword">return</span>(img,label)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用并行化预处理num_parallel_calls 和预存数据prefetch来提升性能</span></span><br><span class="line">ds_train = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/train/*/*.jpg"</span>) \</span><br><span class="line">           .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">           .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">           .prefetch(tf.data.experimental.AUTOTUNE)  </span><br><span class="line"></span><br><span class="line">ds_test = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/test/*/*.jpg"</span>) \</span><br><span class="line">           .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">           .batch(BATCH_SIZE) \</span><br><span class="line">           .prefetch(tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看部分样本</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>)) </span><br><span class="line"><span class="keyword">for</span> i,(img,label) <span class="keyword">in</span> enumerate(ds_train.unbatch().take(<span class="number">9</span>)):</span><br><span class="line">    ax=plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    ax.imshow(img.numpy())</span><br><span class="line">    ax.set_title(<span class="string">"label = %d"</span>%label)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([]) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-2-%E5%9B%BE%E7%89%87%E9%A2%84%E8%A7%88.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> ds_train.take(<span class="number">1</span>):</span><br><span class="line">    print(x.shape,y.shape)</span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">(<span class="number">100</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>) (<span class="number">100</span>,)</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing-1">定义模型</h3>
<p>使用Keras接口有以下3种方式构建模型：</p>
<ol>
<li>使用Sequential按层顺序构建模型</li>
<li>使用函数式API构建任意结构模型</li>
<li>继承Model基类构建自定义模型。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session() <span class="comment">#清空会话</span></span><br><span class="line"></span><br><span class="line">inputs = layers.Input(shape=(<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>))</span><br><span class="line">x = layers.Conv2D(<span class="number">32</span>,kernel_size=(<span class="number">3</span>,<span class="number">3</span>))(inputs)</span><br><span class="line">x = layers.MaxPool2D()(x)</span><br><span class="line">x = layers.Conv2D(<span class="number">64</span>,kernel_size=(<span class="number">5</span>,<span class="number">5</span>))(x)</span><br><span class="line">x = layers.MaxPool2D()(x)</span><br><span class="line">x = layers.Dropout(rate=<span class="number">0.1</span>)(x)</span><br><span class="line">x = layers.Flatten()(x)</span><br><span class="line">x = layers.Dense(<span class="number">32</span>,activation=<span class="string">'relu'</span>)(x)</span><br><span class="line">outputs = layers.Dense(<span class="number">1</span>,activation = <span class="string">'sigmoid'</span>)(x)</span><br><span class="line"></span><br><span class="line">model = models.Model(inputs = inputs,outputs = outputs)</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">Model: "model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">input_1 (InputLayer)         [(None, 32, 32, 3)]       0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv2d (Conv2D)              (None, 30, 30, 32)        896       </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv2d_1 (Conv2D)            (None, 11, 11, 64)        51264     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max<span class="emphasis">_pooling2d_</span>1 (MaxPooling2 (None, 5, 5, 64)          0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dropout (Dropout)            (None, 5, 5, 64)          0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">flatten (Flatten)            (None, 1600)              0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 32)                51232     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_1 (Dense)              (None, 1)                 33        </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 103,425</span><br><span class="line">Trainable params: 103,425</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<h3 id="xun-lian-mo-xing-1">训练模型</h3>
<p>训练模型通常有3种方法，内置fit方法，内置train_on_batch方法，以及自定义训练循环。此处我们选择最常用也最简单的内置fit方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">stamp = datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">logdir = os.path.join(<span class="string">'data'</span>, <span class="string">'autograph'</span>, stamp)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 在 Python3 下建议使用 pathlib 修正各操作系统的路径</span></span><br><span class="line"><span class="comment"># from pathlib import Path</span></span><br><span class="line"><span class="comment"># stamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")</span></span><br><span class="line"><span class="comment"># logdir = str(Path('./data/autograph/' + stamp))</span></span><br><span class="line"></span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">model.compile(</span><br><span class="line">        optimizer=tf.keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>),</span><br><span class="line">        loss=tf.keras.losses.binary_crossentropy,</span><br><span class="line">        metrics=[<span class="string">"accuracy"</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">history = model.fit(ds_train,epochs= <span class="number">10</span>,validation_data=ds_test,</span><br><span class="line">                    callbacks = [tensorboard_callback],workers = <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h3 id="ping-gu-mo-xing-1">评估模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%load_ext tensorboard</span><br><span class="line"><span class="comment">#%tensorboard --logdir ./data/keras_model</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorboard <span class="keyword">import</span> notebook</span><br><span class="line">notebook.list()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#在tensorboard中查看模型</span></span><br><span class="line">notebook.start(<span class="string">"--logdir ./data/keras_model"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-2-tensorboard.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line">dfhistory = pd.DataFrame(history.history)</span><br><span class="line">dfhistory.index = range(<span class="number">1</span>,len(dfhistory) + <span class="number">1</span>)</span><br><span class="line">dfhistory.index.name = <span class="string">'epoch'</span></span><br><span class="line">dfhistory</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-2-dfhistory.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">'val_'</span>+metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span>+ metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span>+metric, <span class="string">'val_'</span>+metric])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"loss"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-2-Loss%E6%9B%B2%E7%BA%BF.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-2-Accuracy%E6%9B%B2%E7%BA%BF.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#可以使用evaluate对数据进行评估</span></span><br><span class="line">val_loss,val_accuracy = model.evaluate(ds_test,workers=<span class="number">4</span>)</span><br><span class="line">print(val_loss,val_accuracy)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="number">0.16139143370091916</span> <span class="number">0.9345</span></span><br></pre></td></tr></table></figure>
<h3 id="shi-yong-mo-xing-1">使用模型</h3>
<p>可以使用model.predict(ds_test)进行预测。</p>
<p>也可以使用model.predict_on_batch(x_test)对一个批量进行预测。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.predict(ds_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="built_in">array</span>([[<span class="number">9.9996173e-01</span>],</span><br><span class="line">       [<span class="number">9.5104784e-01</span>],</span><br><span class="line">       [<span class="number">2.8648047e-04</span>],</span><br><span class="line">       ...,</span><br><span class="line">       [<span class="number">1.1484033e-03</span>],</span><br><span class="line">       [<span class="number">3.5589080e-02</span>],</span><br><span class="line">       [<span class="number">9.8537153e-01</span>]], dtype=<span class="built_in">float</span>32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> ds_test.take(<span class="number">1</span>):</span><br><span class="line">    print(model.predict_on_batch(x[<span class="number">0</span>:<span class="number">20</span>]))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">3.8065155e-05</span>]</span><br><span class="line"> [<span class="number">8.8236779e-01</span>]</span><br><span class="line"> [<span class="number">9.1433197e-01</span>]</span><br><span class="line"> [<span class="number">9.9921846e-01</span>]</span><br><span class="line"> [<span class="number">6.4052093e-01</span>]</span><br><span class="line"> [<span class="number">4.9970779e-03</span>]</span><br><span class="line"> [<span class="number">2.6735585e-04</span>]</span><br><span class="line"> [<span class="number">9.9842811e-01</span>]</span><br><span class="line"> [<span class="number">7.9198682e-01</span>]</span><br><span class="line"> [<span class="number">7.4823302e-01</span>]</span><br><span class="line"> [<span class="number">8.7208226e-03</span>]</span><br><span class="line"> [<span class="number">9.3951421e-03</span>]</span><br><span class="line"> [<span class="number">9.9790359e-01</span>]</span><br><span class="line"> [<span class="number">9.9998581e-01</span>]</span><br><span class="line"> [<span class="number">2.1642199e-05</span>]</span><br><span class="line"> [<span class="number">1.7915063e-02</span>]</span><br><span class="line"> [<span class="number">2.5839690e-02</span>]</span><br><span class="line"> [<span class="number">9.7538447e-01</span>]</span><br><span class="line"> [<span class="number">9.7393811e-01</span>]</span><br><span class="line"> [<span class="number">9.7333014e-01</span>]], shape=(<span class="number">20</span>, <span class="number">1</span>), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="bao-cun-mo-xing-1">保存模型</h3>
<p>推荐使用TensorFlow原生方式保存模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存权重，该方式仅仅保存权重张量</span></span><br><span class="line">model.save_weights(<span class="string">'./data/tf_model_weights.ckpt'</span>,save_format = <span class="string">"tf"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构与模型参数到文件,该方式保存的模型具有跨平台性便于部署</span></span><br><span class="line">model.save(<span class="string">'./data/tf_model_savedmodel'</span>, save_format=<span class="string">"tf"</span>)</span><br><span class="line">print(<span class="string">'export saved model.'</span>)</span><br><span class="line"></span><br><span class="line">model_loaded = tf.keras.models.load_model(<span class="string">'./data/tf_model_savedmodel'</span>)</span><br><span class="line">model_loaded.evaluate(ds_test)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[<span class="number">0.16139124035835267</span>, <span class="number">0.9345</span>]</span><br></pre></td></tr></table></figure>
<h2 id="wen-ben-shu-ju-jian-mo-liu-cheng">文本数据建模流程</h2>
<h3 id="zhun-bei-shu-ju-2">准备数据</h3>
<p>imdb数据集的目标是根据电影评论的文本内容预测评论的情感标签。</p>
<p>训练集有20000条电影评论文本，测试集有5000条电影评论文本，其中正面评论和负面评论都各占一半。</p>
<p>文本数据预处理较为繁琐，包括中文切词，构建词典，编码转换，序列填充，构建数据管道等等。</p>
<p>在tensorflow中完成文本数据预处理的常用方案有两种</p>
<ol>
<li>第一种是利用tf.keras.preprocessing中的Tokenizer词典构建工具和tf.keras.utils.Sequence构建文本数据生成器管道。</li>
<li>第二种是使用tf.data.Dataset搭配.keras.layers.experimental.preprocessing.TextVectorization预处理层。</li>
</ol>
<p>第一种方法较为复杂，其使用范例可以参考以下代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm </span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集路径</span></span><br><span class="line">train_data_path = <span class="string">'imdb_datasets/xx_train_imdb'</span></span><br><span class="line">test_data_path = <span class="string">'imdb_datasets/xx_test_imdb'</span></span><br><span class="line"></span><br><span class="line">train_samples = <span class="number">20000</span> <span class="comment">#训练集样本数量</span></span><br><span class="line">test_samples = <span class="number">5000</span> <span class="comment">#测试集样本数量</span></span><br><span class="line"></span><br><span class="line">max_words = <span class="number">10000</span>  <span class="comment"># 保留词频最高的前10000个词</span></span><br><span class="line">maxlen = <span class="number">500</span>       <span class="comment"># 每个样本文本内容最多保留500个词</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建训练集文本生成器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">texts_gen</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(train_data_path,<span class="string">'r'</span>,encoding = <span class="string">'utf-8'</span>) <span class="keyword">as</span> f,\</span><br><span class="line">    tqdm(total = train_samples) <span class="keyword">as</span> pbar:      </span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            text = (f.readline().rstrip(<span class="string">'\n'</span>).split(<span class="string">'\t'</span>)[<span class="number">-1</span>])</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> text:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> len(text) &gt; maxlen:</span><br><span class="line">                text = text[<span class="number">0</span>:maxlen]</span><br><span class="line">            pbar.update(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">yield</span> text</span><br><span class="line"></span><br><span class="line">texts = texts_gen()</span><br><span class="line">tokenizer = Tokenizer(num_words=max_words)</span><br><span class="line">tokenizer.fit_on_texts(texts)</span><br></pre></td></tr></table></figure>
<p>分割样本</p>
<p>为了能够像ImageDataGenerator那样用数据管道多进程并行地读取数据，需要将数据集按样本分割成<strong>多个文件</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">scatter_train_data_path = <span class="string">'imdb_datasets/train/'</span></span><br><span class="line">scatter_test_data_path = <span class="string">'imdb_datasets/test/'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据按样本打散到多个文件</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scatter_data</span><span class="params">(data_file, scatter_data_path)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(scatter_data_path):</span><br><span class="line">        os.makedirs(scatter_data_path)</span><br><span class="line">    <span class="keyword">for</span> idx,line <span class="keyword">in</span> tqdm(enumerate(open(data_file,<span class="string">'r'</span>,encoding = <span class="string">'utf-8'</span>))):</span><br><span class="line">        <span class="keyword">with</span> open(scatter_data_path + str(idx) + <span class="string">'.txt'</span>,<span class="string">'w'</span>,encoding = <span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">             f.write(line)</span><br><span class="line"></span><br><span class="line">scatter_data(train_data_path,scatter_train_data_path)</span><br><span class="line">scatter_data(test_data_path,scatter_test_data_path)</span><br></pre></td></tr></table></figure>
<p>第二种方法为TensorFlow原生方式，相对也更加简单一些。</p>
<p><img src="/2020/05/07/tensorflow2/%E7%94%B5%E5%BD%B1%E8%AF%84%E8%AE%BA.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,preprocessing,optimizers,losses,metrics</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers.experimental.preprocessing <span class="keyword">import</span> TextVectorization</span><br><span class="line"><span class="keyword">import</span> re,string</span><br><span class="line"></span><br><span class="line">train_data_path = <span class="string">"./data/imdb/train.csv"</span></span><br><span class="line">test_data_path =  <span class="string">"./data/imdb/test.csv"</span></span><br><span class="line"></span><br><span class="line">MAX_WORDS = <span class="number">10000</span>  <span class="comment"># 仅考虑最高频的10000个词</span></span><br><span class="line">MAX_LEN = <span class="number">200</span>  <span class="comment"># 每个样本保留200个词的长度</span></span><br><span class="line">BATCH_SIZE = <span class="number">20</span> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#构建管道</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_line</span><span class="params">(line)</span>:</span></span><br><span class="line">    arr = tf.strings.split(line,<span class="string">"\t"</span>)</span><br><span class="line">    label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[<span class="number">0</span>]),tf.int32),axis = <span class="number">0</span>)</span><br><span class="line">    text = tf.expand_dims(arr[<span class="number">1</span>],axis = <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> (text,label)</span><br><span class="line"></span><br><span class="line">ds_train_raw =  tf.data.TextLineDataset(filenames = [train_data_path]) \</span><br><span class="line">   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \</span><br><span class="line">   .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">   .prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">ds_test_raw = tf.data.TextLineDataset(filenames = [test_data_path]) \</span><br><span class="line">   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \</span><br><span class="line">   .batch(BATCH_SIZE) \</span><br><span class="line">   .prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#构建词典</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_text</span><span class="params">(text)</span>:</span></span><br><span class="line">    lowercase = tf.strings.lower(text)</span><br><span class="line">    stripped_html = tf.strings.regex_replace(lowercase, <span class="string">'&lt;br /&gt;'</span>, <span class="string">' '</span>)</span><br><span class="line">    cleaned_punctuation = tf.strings.regex_replace(stripped_html,<span class="string">'[%s]'</span> % re.escape(string.punctuation),<span class="string">''</span>)</span><br><span class="line">    <span class="keyword">return</span> cleaned_punctuation</span><br><span class="line"></span><br><span class="line">vectorize_layer = TextVectorization(</span><br><span class="line">    standardize=clean_text,</span><br><span class="line">    split = <span class="string">'whitespace'</span>,</span><br><span class="line">    max_tokens=MAX_WORDS<span class="number">-1</span>, <span class="comment">#有一个留给占位符</span></span><br><span class="line">    output_mode=<span class="string">'int'</span>,</span><br><span class="line">    output_sequence_length=MAX_LEN)</span><br><span class="line"></span><br><span class="line">ds_text = ds_train_raw.map(<span class="keyword">lambda</span> text,label: text)</span><br><span class="line">vectorize_layer.adapt(ds_text)</span><br><span class="line">print(vectorize_layer.get_vocabulary()[<span class="number">0</span>:<span class="number">100</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#单词编码</span></span><br><span class="line">ds_train = ds_train_raw.map(<span class="keyword">lambda</span> text,label:(vectorize_layer(text),label)) \</span><br><span class="line">    .prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line">ds_test = ds_test_raw.map(<span class="keyword">lambda</span> text,label:(vectorize_layer(text),label)) \</span><br><span class="line">    .prefetch(tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure>
<figure class="highlight scheme"><table><tr><td class="code"><pre><span class="line">[<span class="name">b</span><span class="symbol">'the</span>', b<span class="symbol">'and</span>', b<span class="symbol">'a</span>', b<span class="symbol">'of</span>', b<span class="symbol">'to</span>', b<span class="symbol">'is</span>', b<span class="symbol">'in</span>', b<span class="symbol">'it</span>', b<span class="symbol">'i</span>', b<span class="symbol">'this</span>', b<span class="symbol">'that</span>', b<span class="symbol">'was</span>', b<span class="symbol">'as</span>', b<span class="symbol">'for</span>', b<span class="symbol">'with</span>', b<span class="symbol">'movie</span>', b<span class="symbol">'but</span>', b<span class="symbol">'film</span>', b<span class="symbol">'on</span>', b<span class="symbol">'not</span>', b<span class="symbol">'you</span>', b<span class="symbol">'his</span>', b<span class="symbol">'are</span>', b<span class="symbol">'have</span>', b<span class="symbol">'be</span>', b<span class="symbol">'he</span>', b<span class="symbol">'one</span>', b<span class="symbol">'its</span>', b<span class="symbol">'at</span>', b<span class="symbol">'all</span>', b<span class="symbol">'by</span>', b<span class="symbol">'an</span>', b<span class="symbol">'they</span>', b<span class="symbol">'from</span>', b<span class="symbol">'who</span>', b<span class="symbol">'so</span>', b<span class="symbol">'like</span>', b<span class="symbol">'her</span>', b<span class="symbol">'just</span>', b<span class="symbol">'or</span>', b<span class="symbol">'about</span>', b<span class="symbol">'has</span>', b<span class="symbol">'if</span>', b<span class="symbol">'out</span>', b<span class="symbol">'some</span>', b<span class="symbol">'there</span>', b<span class="symbol">'what</span>', b<span class="symbol">'good</span>', b<span class="symbol">'more</span>', b<span class="symbol">'when</span>', b<span class="symbol">'very</span>', b<span class="symbol">'she</span>', b<span class="symbol">'even</span>', b<span class="symbol">'my</span>', b<span class="symbol">'no</span>', b<span class="symbol">'would</span>', b<span class="symbol">'up</span>', b<span class="symbol">'time</span>', b<span class="symbol">'only</span>', b<span class="symbol">'which</span>', b<span class="symbol">'story</span>', b<span class="symbol">'really</span>', b<span class="symbol">'their</span>', b<span class="symbol">'were</span>', b<span class="symbol">'had</span>', b<span class="symbol">'see</span>', b<span class="symbol">'can</span>', b<span class="symbol">'me</span>', b<span class="symbol">'than</span>', b<span class="symbol">'we</span>', b<span class="symbol">'much</span>', b<span class="symbol">'well</span>', b<span class="symbol">'get</span>', b<span class="symbol">'been</span>', b<span class="symbol">'will</span>', b<span class="symbol">'into</span>', b<span class="symbol">'people</span>', b<span class="symbol">'also</span>', b<span class="symbol">'other</span>', b<span class="symbol">'do</span>', b<span class="symbol">'bad</span>', b<span class="symbol">'because</span>', b<span class="symbol">'great</span>', b<span class="symbol">'first</span>', b<span class="symbol">'how</span>', b<span class="symbol">'him</span>', b<span class="symbol">'most</span>', b<span class="symbol">'dont</span>', b<span class="symbol">'made</span>', b<span class="symbol">'then</span>', b<span class="symbol">'them</span>', b<span class="symbol">'films</span>', b<span class="symbol">'movies</span>', b<span class="symbol">'way</span>', b<span class="symbol">'make</span>', b<span class="symbol">'could</span>', b<span class="symbol">'too</span>', b<span class="symbol">'any</span>', b<span class="symbol">'after</span>', b<span class="symbol">'characters</span>']</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing-2">定义模型</h3>
<p>使用Keras接口有以下3种方式构建模型：</p>
<ol>
<li>使用Sequential按层顺序构建模型</li>
<li>使用函数式API构建任意结构模型</li>
<li>继承Model基类构建自定义模型。</li>
</ol>
<p>此处选择使用继承Model基类构建自定义模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 演示自定义模型范例，实际上应该优先使用Sequential或者函数式API</span></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CnnModel</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(CnnModel, self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self,input_shape)</span>:</span></span><br><span class="line">        self.embedding = layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN)</span><br><span class="line">        self.conv_1 = layers.Conv1D(<span class="number">16</span>, kernel_size= <span class="number">5</span>,name = <span class="string">"conv_1"</span>,activation = <span class="string">"relu"</span>)</span><br><span class="line">        self.pool_1 = layers.MaxPool1D(name = <span class="string">"pool_1"</span>)</span><br><span class="line">        self.conv_2 = layers.Conv1D(<span class="number">128</span>, kernel_size=<span class="number">2</span>,name = <span class="string">"conv_2"</span>,activation = <span class="string">"relu"</span>)</span><br><span class="line">        self.pool_2 = layers.MaxPool1D(name = <span class="string">"pool_2"</span>)</span><br><span class="line">        self.flatten = layers.Flatten()</span><br><span class="line">        self.dense = layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">        super(CnnModel,self).build(input_shape)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x = self.conv_1(x)</span><br><span class="line">        x = self.pool_1(x)</span><br><span class="line">        x = self.conv_2(x)</span><br><span class="line">        x = self.pool_2(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.dense(x)</span><br><span class="line">        <span class="keyword">return</span>(x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用于显示Output Shape</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">summary</span><span class="params">(self)</span>:</span></span><br><span class="line">        x_input = layers.Input(shape = MAX_LEN)</span><br><span class="line">        output = self.call(x_input)</span><br><span class="line">        model = tf.keras.Model(inputs = x_input,outputs = output)</span><br><span class="line">        model.summary()</span><br><span class="line">    </span><br><span class="line">model = CnnModel()</span><br><span class="line">model.build(input_shape =(<span class="literal">None</span>,MAX_LEN))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">Model: "model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">input_1 (InputLayer)         [(None, 200)]             0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">embedding (Embedding)        (None, 200, 7)            70000     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv_1 (Conv1D)              (None, 196, 16)           576       </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">pool_1 (MaxPooling1D)        (None, 98, 16)            0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv_2 (Conv1D)              (None, 97, 128)           4224      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">pool_2 (MaxPooling1D)        (None, 48, 128)           0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">flatten (Flatten)            (None, 6144)              0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 1)                 6145      </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 80,945</span><br><span class="line">Trainable params: 80,945</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<h3 id="xun-lian-mo-xing-2">训练模型</h3>
<p>训练模型通常有3种方法：</p>
<ol>
<li>内置fit方法</li>
<li>内置train_on_batch方法</li>
<li>以及自定义训练循环</li>
</ol>
<p>此处通过自定义训练循环训练模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line">    </span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = optimizers.Nadam()</span><br><span class="line">loss_func = losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">train_loss = metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_metric = metrics.BinaryAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">valid_loss = metrics.Mean(name=<span class="string">'valid_loss'</span>)</span><br><span class="line">valid_metric = metrics.BinaryAccuracy(name=<span class="string">'valid_accuracy'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features,training = <span class="literal">True</span>)</span><br><span class="line">        loss = loss_func(labels, predictions)</span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss.update_state(loss)</span><br><span class="line">    train_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">valid_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    predictions = model(features,training = <span class="literal">False</span>)</span><br><span class="line">    batch_loss = loss_func(labels, predictions)</span><br><span class="line">    valid_loss.update_state(batch_loss)</span><br><span class="line">    valid_metric.update_state(labels, predictions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,ds_train,ds_valid,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_train:</span><br><span class="line">            train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_valid:</span><br><span class="line">            valid_step(model,features,labels)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#此处logs模板需要根据metric具体情况修改</span></span><br><span class="line">        logs = <span class="string">'Epoch=&#123;&#125;,Loss:&#123;&#125;,Accuracy:&#123;&#125;,Valid Loss:&#123;&#125;,Valid Accuracy:&#123;&#125;'</span> </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">1</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(tf.strings.format(logs,</span><br><span class="line">            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))</span><br><span class="line">            tf.print(<span class="string">""</span>)</span><br><span class="line">        </span><br><span class="line">        train_loss.reset_states()</span><br><span class="line">        valid_loss.reset_states()</span><br><span class="line">        train_metric.reset_states()</span><br><span class="line">        valid_metric.reset_states()</span><br><span class="line"></span><br><span class="line">train_model(model,ds_train,ds_test,epochs = <span class="number">6</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">================================================================================<span class="number">13</span>:<span class="number">54</span>:<span class="number">08</span></span><br><span class="line">Epoch=<span class="number">1</span>,Loss:<span class="number">0.442317516</span>,Accuracy:<span class="number">0.7695</span>,Valid Loss:<span class="number">0.323672801</span>,Valid Accuracy:<span class="number">0.8614</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">54</span>:<span class="number">20</span></span><br><span class="line">Epoch=<span class="number">2</span>,Loss:<span class="number">0.245737702</span>,Accuracy:<span class="number">0.90215</span>,Valid Loss:<span class="number">0.356488883</span>,Valid Accuracy:<span class="number">0.8554</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">54</span>:<span class="number">32</span></span><br><span class="line">Epoch=<span class="number">3</span>,Loss:<span class="number">0.17360799</span>,Accuracy:<span class="number">0.93455</span>,Valid Loss:<span class="number">0.361132562</span>,Valid Accuracy:<span class="number">0.8674</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">54</span>:<span class="number">44</span></span><br><span class="line">Epoch=<span class="number">4</span>,Loss:<span class="number">0.113476314</span>,Accuracy:<span class="number">0.95975</span>,Valid Loss:<span class="number">0.483677238</span>,Valid Accuracy:<span class="number">0.856</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">54</span>:<span class="number">57</span></span><br><span class="line">Epoch=<span class="number">5</span>,Loss:<span class="number">0.0698405355</span>,Accuracy:<span class="number">0.9768</span>,Valid Loss:<span class="number">0.607856631</span>,Valid Accuracy:<span class="number">0.857</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">55</span>:<span class="number">15</span></span><br><span class="line">Epoch=<span class="number">6</span>,Loss:<span class="number">0.0366807655</span>,Accuracy:<span class="number">0.98825</span>,Valid Loss:<span class="number">0.745884955</span>,Valid Accuracy:<span class="number">0.854</span></span><br></pre></td></tr></table></figure>
<h3 id="ping-gu-mo-xing-2">评估模型</h3>
<p>通过自定义训练循环训练的模型没有经过编译，无法直接使用model.evaluate(ds_valid)方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_model</span><span class="params">(model,ds_valid)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_valid:</span><br><span class="line">         valid_step(model,features,labels)</span><br><span class="line">    logs = <span class="string">'Valid Loss:&#123;&#125;,Valid Accuracy:&#123;&#125;'</span> </span><br><span class="line">    tf.print(tf.strings.format(logs,(valid_loss.result(),valid_metric.result())))</span><br><span class="line">    </span><br><span class="line">    valid_loss.reset_states()</span><br><span class="line">    train_metric.reset_states()</span><br><span class="line">    valid_metric.reset_states()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">evaluate_model(model,ds_test)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">Valid Loss:<span class="number">0.745884418</span>,Valid Accuracy:<span class="number">0.854</span></span><br></pre></td></tr></table></figure>
<h3 id="shi-yong-mo-xing-2">使用模型</h3>
<p>可以使用以下方法:</p>
<ul>
<li>model.predict(ds_test)</li>
<li>model(x_test)</li>
<li>model.call(x_test)</li>
<li>model.predict_on_batch(x_test)</li>
</ul>
<p>推荐优先使用model.predict(ds_test)方法，既可以对Dataset，也可以对Tensor使用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.predict(ds_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="built_in">array</span>([[<span class="number">0.7864823</span> ],</span><br><span class="line">       [<span class="number">0.9999901</span> ],</span><br><span class="line">       [<span class="number">0.99944776</span>],</span><br><span class="line">       ...,</span><br><span class="line">       [<span class="number">0.8498302</span> ],</span><br><span class="line">       [<span class="number">0.13382755</span>],</span><br><span class="line">       [<span class="number">1.</span>        ]], dtype=<span class="built_in">float</span>32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> x_test,_ <span class="keyword">in</span> ds_test.take(<span class="number">1</span>):</span><br><span class="line">    print(model(x_test))</span><br><span class="line">    <span class="comment">#以下方法等价：</span></span><br><span class="line">    <span class="comment">#print(model.call(x_test))</span></span><br><span class="line">    <span class="comment">#print(model.predict_on_batch(x_test))</span></span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">7.8648227e-01</span>]</span><br><span class="line"> [<span class="number">9.9999011e-01</span>]</span><br><span class="line"> [<span class="number">9.9944776e-01</span>]</span><br><span class="line"> [<span class="number">3.7153201e-09</span>]</span><br><span class="line"> [<span class="number">9.4462049e-01</span>]</span><br><span class="line"> [<span class="number">2.3522753e-04</span>]</span><br><span class="line"> [<span class="number">1.2044354e-04</span>]</span><br><span class="line"> [<span class="number">9.3752089e-07</span>]</span><br><span class="line"> [<span class="number">9.9996352e-01</span>]</span><br><span class="line"> [<span class="number">9.3435925e-01</span>]</span><br><span class="line"> [<span class="number">9.8746723e-01</span>]</span><br><span class="line"> [<span class="number">9.9908626e-01</span>]</span><br><span class="line"> [<span class="number">4.1563155e-08</span>]</span><br><span class="line"> [<span class="number">4.1808244e-03</span>]</span><br><span class="line"> [<span class="number">8.0184749e-05</span>]</span><br><span class="line"> [<span class="number">8.3910513e-01</span>]</span><br><span class="line"> [<span class="number">3.5167937e-05</span>]</span><br><span class="line"> [<span class="number">7.2113985e-01</span>]</span><br><span class="line"> [<span class="number">4.5228912e-03</span>]</span><br><span class="line"> [<span class="number">9.9942589e-01</span>]], shape=(<span class="number">20</span>, <span class="number">1</span>), dtype=<span class="built_in">float</span>32)</span><br></pre></td></tr></table></figure>
<h3 id="bao-cun-mo-xing-2">保存模型</h3>
<p>推荐使用TensorFlow原生方式保存模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(<span class="string">'./data/tf_model_savedmodel'</span>, save_format=<span class="string">"tf"</span>)</span><br><span class="line">print(<span class="string">'export saved model.'</span>)</span><br><span class="line"></span><br><span class="line">model_loaded = tf.keras.models.load_model(<span class="string">'./data/tf_model_savedmodel'</span>)</span><br><span class="line">model_loaded.predict(ds_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">array([[<span class="number">0.7864823</span> ],</span><br><span class="line">       [<span class="number">0.9999901</span> ],</span><br><span class="line">       [<span class="number">0.99944776</span>],</span><br><span class="line">       ...,</span><br><span class="line">       [<span class="number">0.8498302</span> ],</span><br><span class="line">       [<span class="number">0.13382755</span>],</span><br><span class="line">       [<span class="number">1.</span>        ]], dtype=float32)</span><br></pre></td></tr></table></figure>
<h2 id="shi-jian-xu-lie-shu-ju-jian-mo-liu-cheng">时间序列数据建模流程</h2>
<h3 id="zhun-bei-shu-ju-3">准备数据</h3>
<p>数据<a href="https://github.com/BlankerL/DXY-COVID-19-Data/tree/master/json" target="_blank" rel="noopener">获取</a></p>
<p><img src="/2020/05/07/tensorflow2/1-4-%E6%96%B0%E5%A2%9E%E4%BA%BA%E6%95%B0.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,losses,metrics,callbacks</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">"./data/covid-19.csv"</span>,sep = <span class="string">"\t"</span>)</span><br><span class="line">df.plot(x = <span class="string">"date"</span>,y = [<span class="string">"confirmed_num"</span>,<span class="string">"cured_num"</span>,<span class="string">"dead_num"</span>],figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">plt.xticks(rotation=<span class="number">60</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-4-%E7%B4%AF%E7%A7%AF%E6%9B%B2%E7%BA%BF.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dfdata = df.set_index(<span class="string">"date"</span>)</span><br><span class="line">dfdiff = dfdata.diff(periods=<span class="number">1</span>).dropna()</span><br><span class="line">dfdiff = dfdiff.reset_index(<span class="string">"date"</span>)</span><br><span class="line"></span><br><span class="line">dfdiff.plot(x = <span class="string">"date"</span>,y = [<span class="string">"confirmed_num"</span>,<span class="string">"cured_num"</span>,<span class="string">"dead_num"</span>],figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">plt.xticks(rotation=<span class="number">60</span>)</span><br><span class="line">dfdiff = dfdiff.drop(<span class="string">"date"</span>,axis = <span class="number">1</span>).astype(<span class="string">"float32"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-4-%E6%96%B0%E5%A2%9E%E6%9B%B2%E7%BA%BF.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#用某日前8天窗口数据作为输入预测该日数据</span></span><br><span class="line">WINDOW_SIZE = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_dataset</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    dataset_batched = dataset.batch(WINDOW_SIZE,drop_remainder=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> dataset_batched</span><br><span class="line"></span><br><span class="line">ds_data = tf.data.Dataset.from_tensor_slices(tf.constant(dfdiff.values,dtype = tf.float32)) \</span><br><span class="line">   .window(WINDOW_SIZE,shift=<span class="number">1</span>).flat_map(batch_dataset)</span><br><span class="line"></span><br><span class="line">ds_label = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    tf.constant(dfdiff.values[WINDOW_SIZE:],dtype = tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据较小，可以将全部训练数据放入到一个batch中，提升性能</span></span><br><span class="line">ds_train = tf.data.Dataset.zip((ds_data,ds_label)).batch(<span class="number">38</span>).cache()</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing-3">定义模型</h3>
<p>使用Keras接口有以下3种方式构建模型：</p>
<ol>
<li>使用Sequential按层顺序构建模型</li>
<li>使用函数式API构建任意结构模型</li>
<li>继承Model基类构建自定义模型</li>
</ol>
<p>此处选择使用函数式API构建任意结构模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#考虑到新增确诊，新增治愈，新增死亡人数数据不可能小于0，设计如下结构</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span><span class="params">(layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(Block, self).__init__(**kwargs)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x_input,x)</span>:</span></span><br><span class="line">        x_out = tf.maximum((<span class="number">1</span>+x)*x_input[:,<span class="number">-1</span>,:],<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">return</span> x_out</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        config = super(Block, self).get_config()</span><br><span class="line">        <span class="keyword">return</span> config</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line">x_input = layers.Input(shape = (<span class="literal">None</span>,<span class="number">3</span>),dtype = tf.float32)</span><br><span class="line">x = layers.LSTM(<span class="number">3</span>,return_sequences = <span class="literal">True</span>,input_shape=(<span class="literal">None</span>,<span class="number">3</span>))(x_input)</span><br><span class="line">x = layers.LSTM(<span class="number">3</span>,return_sequences = <span class="literal">True</span>,input_shape=(<span class="literal">None</span>,<span class="number">3</span>))(x)</span><br><span class="line">x = layers.LSTM(<span class="number">3</span>,return_sequences = <span class="literal">True</span>,input_shape=(<span class="literal">None</span>,<span class="number">3</span>))(x)</span><br><span class="line">x = layers.LSTM(<span class="number">3</span>,input_shape=(<span class="literal">None</span>,<span class="number">3</span>))(x)</span><br><span class="line">x = layers.Dense(<span class="number">3</span>)(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#考虑到新增确诊，新增治愈，新增死亡人数数据不可能小于0，设计如下结构</span></span><br><span class="line"><span class="comment">#x = tf.maximum((1+x)*x_input[:,-1,:],0.0)</span></span><br><span class="line">x = Block()(x_input,x)</span><br><span class="line">model = models.Model(inputs = [x_input],outputs = [x])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">Model: "model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">input_1 (InputLayer)         [(None, None, 3)]         0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">lstm (LSTM)                  (None, None, 3)           84        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">lstm_1 (LSTM)                (None, None, 3)           84        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">lstm_2 (LSTM)                (None, None, 3)           84        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">lstm_3 (LSTM)                (None, 3)                 84        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 3)                 12        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">block (Block)                (None, 3)                 0         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 348</span><br><span class="line">Trainable params: 348</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<h3 id="xun-lian-mo-xing-3">训练模型</h3>
<p>训练模型通常有3种方法</p>
<ol>
<li>内置fit方法</li>
<li>内置train_on_batch方法</li>
<li>以及自定义训练循环</li>
</ol>
<p>此处我们选择最常用也最简单的内置fit方法。</p>
<blockquote>
<p>注：循环神经网络调试较为困难，需要设置多个不同的学习率多次尝试，以取得较好的效果。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#自定义损失函数，考虑平方差和预测目标的比值</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MSPE</span><span class="params">(losses.Loss)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self,y_true,y_pred)</span>:</span></span><br><span class="line">        err_percent = (y_true - y_pred)**<span class="number">2</span>/(tf.maximum(y_true**<span class="number">2</span>,<span class="number">1e-7</span>))</span><br><span class="line">        mean_err_percent = tf.reduce_mean(err_percent)</span><br><span class="line">        <span class="keyword">return</span> mean_err_percent</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span></span><br><span class="line">        config = super(MSPE, self).get_config()</span><br><span class="line">        <span class="keyword">return</span> config</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.Adam(learning_rate=<span class="number">0.01</span>)</span><br><span class="line">model.compile(optimizer=optimizer,loss=MSPE(name = <span class="string">"MSPE"</span>))</span><br><span class="line"></span><br><span class="line">stamp = datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">logdir = os.path.join(<span class="string">'data'</span>, <span class="string">'autograph'</span>, stamp)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 在 Python3 下建议使用 pathlib 修正各操作系统的路径</span></span><br><span class="line"><span class="comment"># from pathlib import Path</span></span><br><span class="line"><span class="comment"># stamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")</span></span><br><span class="line"><span class="comment"># logdir = str(Path('./data/autograph/' + stamp))</span></span><br><span class="line"></span><br><span class="line">tb_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#如果loss在100个epoch后没有提升，学习率减半。</span></span><br><span class="line">lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">"loss"</span>,factor = <span class="number">0.5</span>, patience = <span class="number">100</span>)</span><br><span class="line"><span class="comment">#当loss在200个epoch后没有提升，则提前终止训练。</span></span><br><span class="line">stop_callback = tf.keras.callbacks.EarlyStopping(monitor = <span class="string">"loss"</span>, patience= <span class="number">200</span>)</span><br><span class="line">callbacks_list = [tb_callback,lr_callback,stop_callback]</span><br><span class="line"></span><br><span class="line">history = model.fit(ds_train,epochs=<span class="number">500</span>,callbacks = callbacks_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">Epoch <span class="number">371</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">61</span>ms/step - loss: <span class="number">0.1184</span></span><br><span class="line">Epoch <span class="number">372</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">64</span>ms/step - loss: <span class="number">0.1177</span></span><br><span class="line">Epoch <span class="number">373</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.1169</span></span><br><span class="line">Epoch <span class="number">374</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">50</span>ms/step - loss: <span class="number">0.1161</span></span><br><span class="line">Epoch <span class="number">375</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.1154</span></span><br><span class="line">Epoch <span class="number">376</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.1147</span></span><br><span class="line">Epoch <span class="number">377</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">62</span>ms/step - loss: <span class="number">0.1140</span></span><br><span class="line">Epoch <span class="number">378</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">93</span>ms/step - loss: <span class="number">0.1133</span></span><br><span class="line">Epoch <span class="number">379</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">85</span>ms/step - loss: <span class="number">0.1126</span></span><br><span class="line">Epoch <span class="number">380</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">68</span>ms/step - loss: <span class="number">0.1119</span></span><br><span class="line">Epoch <span class="number">381</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">52</span>ms/step - loss: <span class="number">0.1113</span></span><br><span class="line">Epoch <span class="number">382</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">54</span>ms/step - loss: <span class="number">0.1107</span></span><br><span class="line">Epoch <span class="number">383</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.1100</span></span><br><span class="line">Epoch <span class="number">384</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.1094</span></span><br><span class="line">Epoch <span class="number">385</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">54</span>ms/step - loss: <span class="number">0.1088</span></span><br><span class="line">Epoch <span class="number">386</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">74</span>ms/step - loss: <span class="number">0.1082</span></span><br><span class="line">Epoch <span class="number">387</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.1077</span></span><br><span class="line">Epoch <span class="number">388</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">52</span>ms/step - loss: <span class="number">0.1071</span></span><br><span class="line">Epoch <span class="number">389</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">52</span>ms/step - loss: <span class="number">0.1066</span></span><br><span class="line">Epoch <span class="number">390</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.1060</span></span><br><span class="line">Epoch <span class="number">391</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">61</span>ms/step - loss: <span class="number">0.1055</span></span><br><span class="line">Epoch <span class="number">392</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.1050</span></span><br><span class="line">Epoch <span class="number">393</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">59</span>ms/step - loss: <span class="number">0.1045</span></span><br><span class="line">Epoch <span class="number">394</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">65</span>ms/step - loss: <span class="number">0.1040</span></span><br><span class="line">Epoch <span class="number">395</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">58</span>ms/step - loss: <span class="number">0.1035</span></span><br><span class="line">Epoch <span class="number">396</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">52</span>ms/step - loss: <span class="number">0.1031</span></span><br><span class="line">Epoch <span class="number">397</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">58</span>ms/step - loss: <span class="number">0.1026</span></span><br><span class="line">Epoch <span class="number">398</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.1022</span></span><br><span class="line">Epoch <span class="number">399</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.1017</span></span><br><span class="line">Epoch <span class="number">400</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">63</span>ms/step - loss: <span class="number">0.1013</span></span><br><span class="line">Epoch <span class="number">401</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">59</span>ms/step - loss: <span class="number">0.1009</span></span><br><span class="line">Epoch <span class="number">402</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">53</span>ms/step - loss: <span class="number">0.1005</span></span><br><span class="line">Epoch <span class="number">403</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.1001</span></span><br><span class="line">Epoch <span class="number">404</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0997</span></span><br><span class="line">Epoch <span class="number">405</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">58</span>ms/step - loss: <span class="number">0.0993</span></span><br><span class="line">Epoch <span class="number">406</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">53</span>ms/step - loss: <span class="number">0.0990</span></span><br><span class="line">Epoch <span class="number">407</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">59</span>ms/step - loss: <span class="number">0.0986</span></span><br><span class="line">Epoch <span class="number">408</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">63</span>ms/step - loss: <span class="number">0.0982</span></span><br><span class="line">Epoch <span class="number">409</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">67</span>ms/step - loss: <span class="number">0.0979</span></span><br><span class="line">Epoch <span class="number">410</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0976</span></span><br><span class="line">Epoch <span class="number">411</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">54</span>ms/step - loss: <span class="number">0.0972</span></span><br><span class="line">Epoch <span class="number">412</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0969</span></span><br><span class="line">Epoch <span class="number">413</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0966</span></span><br><span class="line">Epoch <span class="number">414</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">59</span>ms/step - loss: <span class="number">0.0963</span></span><br><span class="line">Epoch <span class="number">415</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0960</span></span><br><span class="line">Epoch <span class="number">416</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">62</span>ms/step - loss: <span class="number">0.0957</span></span><br><span class="line">Epoch <span class="number">417</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">69</span>ms/step - loss: <span class="number">0.0954</span></span><br><span class="line">Epoch <span class="number">418</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0951</span></span><br><span class="line">Epoch <span class="number">419</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">50</span>ms/step - loss: <span class="number">0.0948</span></span><br><span class="line">Epoch <span class="number">420</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0946</span></span><br><span class="line">Epoch <span class="number">421</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0943</span></span><br><span class="line">Epoch <span class="number">422</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0941</span></span><br><span class="line">Epoch <span class="number">423</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">62</span>ms/step - loss: <span class="number">0.0938</span></span><br><span class="line">Epoch <span class="number">424</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0936</span></span><br><span class="line">Epoch <span class="number">425</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">100</span>ms/step - loss: <span class="number">0.0933</span></span><br><span class="line">Epoch <span class="number">426</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">68</span>ms/step - loss: <span class="number">0.0931</span></span><br><span class="line">Epoch <span class="number">427</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0929</span></span><br><span class="line">Epoch <span class="number">428</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">50</span>ms/step - loss: <span class="number">0.0926</span></span><br><span class="line">Epoch <span class="number">429</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0924</span></span><br><span class="line">Epoch <span class="number">430</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0922</span></span><br><span class="line">Epoch <span class="number">431</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">75</span>ms/step - loss: <span class="number">0.0920</span></span><br><span class="line">Epoch <span class="number">432</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0918</span></span><br><span class="line">Epoch <span class="number">433</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">77</span>ms/step - loss: <span class="number">0.0916</span></span><br><span class="line">Epoch <span class="number">434</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">50</span>ms/step - loss: <span class="number">0.0914</span></span><br><span class="line">Epoch <span class="number">435</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0912</span></span><br><span class="line">Epoch <span class="number">436</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0911</span></span><br><span class="line">Epoch <span class="number">437</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0909</span></span><br><span class="line">Epoch <span class="number">438</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0907</span></span><br><span class="line">Epoch <span class="number">439</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">59</span>ms/step - loss: <span class="number">0.0905</span></span><br><span class="line">Epoch <span class="number">440</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0904</span></span><br><span class="line">Epoch <span class="number">441</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">68</span>ms/step - loss: <span class="number">0.0902</span></span><br><span class="line">Epoch <span class="number">442</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">73</span>ms/step - loss: <span class="number">0.0901</span></span><br><span class="line">Epoch <span class="number">443</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">50</span>ms/step - loss: <span class="number">0.0899</span></span><br><span class="line">Epoch <span class="number">444</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">58</span>ms/step - loss: <span class="number">0.0898</span></span><br><span class="line">Epoch <span class="number">445</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0896</span></span><br><span class="line">Epoch <span class="number">446</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">52</span>ms/step - loss: <span class="number">0.0895</span></span><br><span class="line">Epoch <span class="number">447</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0893</span></span><br><span class="line">Epoch <span class="number">448</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">64</span>ms/step - loss: <span class="number">0.0892</span></span><br><span class="line">Epoch <span class="number">449</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">70</span>ms/step - loss: <span class="number">0.0891</span></span><br><span class="line">Epoch <span class="number">450</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0889</span></span><br><span class="line">Epoch <span class="number">451</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">53</span>ms/step - loss: <span class="number">0.0888</span></span><br><span class="line">Epoch <span class="number">452</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">51</span>ms/step - loss: <span class="number">0.0887</span></span><br><span class="line">Epoch <span class="number">453</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0886</span></span><br><span class="line">Epoch <span class="number">454</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">58</span>ms/step - loss: <span class="number">0.0885</span></span><br><span class="line">Epoch <span class="number">455</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0883</span></span><br><span class="line">Epoch <span class="number">456</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">71</span>ms/step - loss: <span class="number">0.0882</span></span><br><span class="line">Epoch <span class="number">457</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">50</span>ms/step - loss: <span class="number">0.0881</span></span><br><span class="line">Epoch <span class="number">458</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0880</span></span><br><span class="line">Epoch <span class="number">459</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0879</span></span><br><span class="line">Epoch <span class="number">460</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0878</span></span><br><span class="line">Epoch <span class="number">461</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0878</span></span><br><span class="line">Epoch <span class="number">462</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0879</span></span><br><span class="line">Epoch <span class="number">463</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0879</span></span><br><span class="line">Epoch <span class="number">464</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">68</span>ms/step - loss: <span class="number">0.0888</span></span><br><span class="line">Epoch <span class="number">465</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">62</span>ms/step - loss: <span class="number">0.0875</span></span><br><span class="line">Epoch <span class="number">466</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0873</span></span><br><span class="line">Epoch <span class="number">467</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">49</span>ms/step - loss: <span class="number">0.0872</span></span><br><span class="line">Epoch <span class="number">468</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0872</span></span><br><span class="line">Epoch <span class="number">469</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0871</span></span><br><span class="line">Epoch <span class="number">470</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0871</span></span><br><span class="line">Epoch <span class="number">471</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">59</span>ms/step - loss: <span class="number">0.0870</span></span><br><span class="line">Epoch <span class="number">472</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">68</span>ms/step - loss: <span class="number">0.0871</span></span><br><span class="line">Epoch <span class="number">473</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0869</span></span><br><span class="line">Epoch <span class="number">474</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">61</span>ms/step - loss: <span class="number">0.0870</span></span><br><span class="line">Epoch <span class="number">475</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">47</span>ms/step - loss: <span class="number">0.0868</span></span><br><span class="line">Epoch <span class="number">476</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0868</span></span><br><span class="line">Epoch <span class="number">477</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">62</span>ms/step - loss: <span class="number">0.0866</span></span><br><span class="line">Epoch <span class="number">478</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">58</span>ms/step - loss: <span class="number">0.0867</span></span><br><span class="line">Epoch <span class="number">479</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0865</span></span><br><span class="line">Epoch <span class="number">480</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">65</span>ms/step - loss: <span class="number">0.0866</span></span><br><span class="line">Epoch <span class="number">481</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">58</span>ms/step - loss: <span class="number">0.0864</span></span><br><span class="line">Epoch <span class="number">482</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0865</span></span><br><span class="line">Epoch <span class="number">483</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">53</span>ms/step - loss: <span class="number">0.0863</span></span><br><span class="line">Epoch <span class="number">484</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0864</span></span><br><span class="line">Epoch <span class="number">485</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0862</span></span><br><span class="line">Epoch <span class="number">486</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0863</span></span><br><span class="line">Epoch <span class="number">487</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">52</span>ms/step - loss: <span class="number">0.0861</span></span><br><span class="line">Epoch <span class="number">488</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">68</span>ms/step - loss: <span class="number">0.0862</span></span><br><span class="line">Epoch <span class="number">489</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">62</span>ms/step - loss: <span class="number">0.0860</span></span><br><span class="line">Epoch <span class="number">490</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0861</span></span><br><span class="line">Epoch <span class="number">491</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">51</span>ms/step - loss: <span class="number">0.0859</span></span><br><span class="line">Epoch <span class="number">492</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">54</span>ms/step - loss: <span class="number">0.0860</span></span><br><span class="line">Epoch <span class="number">493</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">51</span>ms/step - loss: <span class="number">0.0859</span></span><br><span class="line">Epoch <span class="number">494</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">54</span>ms/step - loss: <span class="number">0.0860</span></span><br><span class="line">Epoch <span class="number">495</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">50</span>ms/step - loss: <span class="number">0.0858</span></span><br><span class="line">Epoch <span class="number">496</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">69</span>ms/step - loss: <span class="number">0.0859</span></span><br><span class="line">Epoch <span class="number">497</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">63</span>ms/step - loss: <span class="number">0.0857</span></span><br><span class="line">Epoch <span class="number">498</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0858</span></span><br><span class="line">Epoch <span class="number">499</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">54</span>ms/step - loss: <span class="number">0.0857</span></span><br><span class="line">Epoch <span class="number">500</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0858</span></span><br></pre></td></tr></table></figure>
<h3 id="ping-gu-mo-xing-3">评估模型</h3>
<p>评估模型一般要设置验证集或者测试集，由于此例数据较少，仅仅可视化损失函数在训练集上的迭代情况。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training '</span>+ metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span>+metric])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"loss"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-4-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%9B%B2%E7%BA%BF.png" alt></p>
<h3 id="shi-yong-mo-xing-3">使用模型</h3>
<p>此处我们使用模型预测疫情结束时间，即 新增确诊病例为0 的时间。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用dfresult记录现有数据以及此后预测的疫情数据</span></span><br><span class="line">dfresult = dfdiff[[<span class="string">"confirmed_num"</span>,<span class="string">"cured_num"</span>,<span class="string">"dead_num"</span>]].copy()</span><br><span class="line">dfresult.tail()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-4-%E6%97%A5%E6%9C%9F3%E6%9C%8810.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#预测此后100天的新增走势,将其结果添加到dfresult中</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    arr_predict = model.predict(tf.constant(tf.expand_dims(dfresult.values[<span class="number">-38</span>:,:],axis = <span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">    dfpredict = pd.DataFrame(tf.cast(tf.floor(arr_predict),tf.float32).numpy(),</span><br><span class="line">                columns = dfresult.columns)</span><br><span class="line">    dfresult = dfresult.append(dfpredict,ignore_index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dfresult.query(<span class="string">"confirmed_num==0"</span>).head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第55天开始新增确诊降为0，第45天对应3月10日，也就是10天后，即预计3月20日新增确诊降为0</span></span><br><span class="line"><span class="comment"># 注：该预测偏乐观</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-4-%E9%A2%84%E6%B5%8B%E7%A1%AE%E8%AF%8A.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dfresult.query(<span class="string">"cured_num==0"</span>).head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第164天开始新增治愈降为0，第45天对应3月10日，也就是大概4个月后，即7月10日左右全部治愈。</span></span><br><span class="line"><span class="comment"># 注: 该预测偏悲观，并且存在问题，如果将每天新增治愈人数加起来，将超过累计确诊人数。</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-4-%E9%A2%84%E6%B5%8B%E6%B2%BB%E6%84%88.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dfresult.query(<span class="string">"dead_num==0"</span>).head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第60天开始，新增死亡降为0，第45天对应3月10日，也就是大概15天后，即20200325</span></span><br><span class="line"><span class="comment"># 该预测较为合理</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/1-4-%E9%A2%84%E6%B5%8B%E6%AD%BB%E4%BA%A1.png" alt></p>
<h3 id="bao-cun-mo-xing-3">保存模型</h3>
<p>推荐使用TensorFlow原生方式保存模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(<span class="string">'./data/tf_model_savedmodel'</span>, save_format=<span class="string">"tf"</span>)</span><br><span class="line">print(<span class="string">'export saved model.'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_loaded = tf.keras.models.load_model(<span class="string">'./data/tf_model_savedmodel'</span>,compile=<span class="literal">False</span>)</span><br><span class="line">optimizer = tf.keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_loaded.compile(optimizer=optimizer,loss=MSPE(name = <span class="string">"MSPE"</span>))</span><br><span class="line">model_loaded.predict(ds_train)</span><br></pre></td></tr></table></figure>
<h1 id="tensor-flow-de-he-xin-gai-nian">TensorFlow的核心概念</h1>
<p>TensorFlow™ 是一个采用 <strong>数据流图</strong>（data flow graphs），用于数值计算的开源软件库。节点（Nodes）在图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。它灵活的架构让你可以<strong>在多种平台上展开计算</strong>，例如台式计算机中的一个或多个CPU（或GPU），服务器，移动设备等等。TensorFlow 最初由Google AI小组（隶属于Google机器智能研究机构）的研究员和工程师们开发出来，<strong>用于机器学习和深度神经网络</strong>方面的研究，但这个系统的通用性使其也可<strong>广泛用于其他计算领域</strong>。</p>
<p>TensorFlow的主要优点：</p>
<ul>
<li>
<p>灵活性：支持底层数值计算，C++自定义操作符</p>
</li>
<li>
<p>可移植性：从服务器到PC到手机，从CPU到GPU到TPU</p>
</li>
<li>
<p>分布式计算：分布式并行计算，可指定操作符对应计算设备</p>
</li>
</ul>
<p>俗话说，万丈高楼平地起，TensorFlow这座大厦也有它的地基。</p>
<p>Tensorflow底层最核心的概念是张量，计算图以及自动微分。</p>
<h2 id="zhang-liang-shu-ju-jie-gou">张量数据结构</h2>
<p>程序 = 数据结构+算法，TensorFlow程序 = 张量数据结构 + 计算图算法语言，张量和计算图是 TensorFlow的核心概念。</p>
<p>Tensorflow的基本数据结构是张量Tensor。张量即多维数组。Tensorflow的张量和numpy中的array很类似。</p>
<p>从行为特性来看，有两种类型的张量，常量constant和变量Variable，常量的值在计算图中不可以被重新赋值，变量可以在计算图中用assign等算子重新赋值。</p>
<h3 id="chang-liang-zhang-liang">常量张量</h3>
<p>张量的数据类型和numpy.array基本一一对应。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">i = tf.constant(<span class="number">1</span>) <span class="comment"># tf.int32 类型常量</span></span><br><span class="line">l = tf.constant(<span class="number">1</span>,dtype = tf.int64) <span class="comment"># tf.int64 类型常量</span></span><br><span class="line">f = tf.constant(<span class="number">1.23</span>) <span class="comment">#tf.float32 类型常量</span></span><br><span class="line">d = tf.constant(<span class="number">3.14</span>,dtype = tf.double) <span class="comment"># tf.double 类型常量</span></span><br><span class="line">s = tf.constant(<span class="string">"hello world"</span>) <span class="comment"># tf.string类型常量</span></span><br><span class="line">b = tf.constant(<span class="literal">True</span>) <span class="comment">#tf.bool类型常量</span></span><br><span class="line"></span><br><span class="line">print(tf.int64 == np.int64) </span><br><span class="line">print(tf.bool == np.bool)</span><br><span class="line">print(tf.double == np.float64)</span><br><span class="line">print(tf.string == np.unicode) <span class="comment"># tf.string类型和np.unicode类型不等价</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>不同类型的数据可以用不同维度(rank)的张量来表示，标量是0维张量，向量为1维张量，矩阵为2维张量。</p>
<p>彩色图像有rgb三个通道，可以表示为3维张量。视频还有时间维，可以表示为4维张量，可以简单地总结为：有几层中括号，就是多少维的张量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scalar = tf.constant(<span class="literal">True</span>)  <span class="comment">#标量，0维张量</span></span><br><span class="line"></span><br><span class="line">print(tf.rank(scalar))</span><br><span class="line">print(scalar.numpy().ndim)  <span class="comment"># tf.rank的作用和numpy的ndim方法相同</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="number">0</span>, shape=(), dtype=int32)</span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vector = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>]) <span class="comment">#向量，1维张量</span></span><br><span class="line"></span><br><span class="line">print(tf.rank(vector)) <span class="comment"># tf.Tensor(1, shape=(), dtype=int32)</span></span><br><span class="line">print(np.ndim(vector.numpy())) <span class="comment"># 1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">matrix = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]]) <span class="comment">#矩阵, 2维张量</span></span><br><span class="line"></span><br><span class="line">print(tf.rank(matrix).numpy()) <span class="comment"># 2</span></span><br><span class="line">print(np.ndim(matrix)) <span class="comment"># 2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor3 = tf.constant([[[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]],[[<span class="number">5.0</span>,<span class="number">6.0</span>],[<span class="number">7.0</span>,<span class="number">8.0</span>]]])  <span class="comment"># 3维张量</span></span><br><span class="line">print(tensor3)</span><br><span class="line">print(tf.rank(tensor3))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[[<span class="number">1.</span> <span class="number">2.</span>]</span><br><span class="line">  [<span class="number">3.</span> <span class="number">4.</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">5.</span> <span class="number">6.</span>]</span><br><span class="line">  [<span class="number">7.</span> <span class="number">8.</span>]]], shape=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">3</span>, shape=(), dtype=int32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor4 = tf.constant([[[[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">2.0</span>,<span class="number">2.0</span>]],[[<span class="number">3.0</span>,<span class="number">3.0</span>],[<span class="number">4.0</span>,<span class="number">4.0</span>]]],</span><br><span class="line">                        [[[<span class="number">5.0</span>,<span class="number">5.0</span>],[<span class="number">6.0</span>,<span class="number">6.0</span>]],[[<span class="number">7.0</span>,<span class="number">7.0</span>],[<span class="number">8.0</span>,<span class="number">8.0</span>]]]])  <span class="comment"># 4维张量</span></span><br><span class="line">print(tensor4)</span><br><span class="line">print(tf.rank(tensor4))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line">   [<span class="number">2.</span> <span class="number">2.</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">3.</span> <span class="number">3.</span>]</span><br><span class="line">   [<span class="number">4.</span> <span class="number">4.</span>]]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> [[[<span class="number">5.</span> <span class="number">5.</span>]</span><br><span class="line">   [<span class="number">6.</span> <span class="number">6.</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">7.</span> <span class="number">7.</span>]</span><br><span class="line">   [<span class="number">8.</span> <span class="number">8.</span>]]]], shape=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">4</span>, shape=(), dtype=int32)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以用tf.cast改变张量的数据类型。</p>
<p>可以用numpy方法将tensorflow中的张量转化成numpy中的张量。</p>
<p>可以用shape方法查看张量的尺寸。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">h = tf.constant([<span class="number">123</span>,<span class="number">456</span>],dtype = tf.int32)</span><br><span class="line">f = tf.cast(h,tf.float32)</span><br><span class="line">print(h.dtype, f.dtype) <span class="comment"># &lt;dtype: 'int32'&gt; &lt;dtype: 'float32'&gt;</span></span><br><span class="line"></span><br><span class="line">y = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]])</span><br><span class="line">print(y.numpy()) <span class="comment">#转换成np.array</span></span><br><span class="line">print(y.shape)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">1.</span> <span class="number">2.</span>]</span><br><span class="line"> [<span class="number">3.</span> <span class="number">4.</span>]]</span><br><span class="line">(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">u = tf.constant(<span class="string">u"你好 世界"</span>)</span><br><span class="line">print(u.numpy())  <span class="comment"># b'\xe4\xbd\xa0\xe5\xa5\xbd \xe4\xb8\x96\xe7\x95\x8c'</span></span><br><span class="line">print(u.numpy().decode(<span class="string">"utf-8"</span>)) <span class="comment"># 你好 世界</span></span><br></pre></td></tr></table></figure>
<h3 id="bian-liang-zhang-liang">变量张量</h3>
<p>模型中需要被训练的参数一般被设置成变量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 常量值不可以改变，常量的重新赋值相当于创造新的内存空间</span></span><br><span class="line">c = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>])</span><br><span class="line">print(c) <span class="comment"># tf.Tensor([1. 2.], shape=(2,), dtype=float32)</span></span><br><span class="line">print(id(c)) <span class="comment"># 5276289568</span></span><br><span class="line">c = c + tf.constant([<span class="number">1.0</span>,<span class="number">1.0</span>])</span><br><span class="line">print(c) <span class="comment"># tf.Tensor([2. 3.], shape=(2,), dtype=float32)</span></span><br><span class="line">print(id(c)) <span class="comment"># 5276290240</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 变量的值可以改变，可以通过assign, assign_add等方法给变量重新赋值</span></span><br><span class="line">v = tf.Variable([<span class="number">1.0</span>,<span class="number">2.0</span>],name = <span class="string">"v"</span>)</span><br><span class="line">print(v) <span class="comment"># &lt;tf.Variable 'v:0' shape=(2,) dtype=float32, numpy=array([1., 2.], dtype=float32)&gt;</span></span><br><span class="line">print(id(v)) <span class="comment"># 5276259888</span></span><br><span class="line">v.assign_add([<span class="number">1.0</span>,<span class="number">1.0</span>]) </span><br><span class="line">print(v) <span class="comment"># &lt;tf.Variable 'v:0' shape=(2,) dtype=float32, numpy=array([2., 3.], dtype=float32)&gt;</span></span><br><span class="line">print(id(v)) <span class="comment"># 5276259888</span></span><br></pre></td></tr></table></figure>
<h2 id="san-chong-ji-suan-tu">三种计算图</h2>
<p>有三种计算图的构建方式：</p>
<ol>
<li>静态计算图</li>
<li>动态计算图</li>
<li>Autograph</li>
</ol>
<p>在TensorFlow1.0时代，采用的是静态计算图，需要先使用TensorFlow的各种算子创建计算图，然后再开启一个会话Session，显式执行计算图。</p>
<p>而在TensorFlow2.0时代，采用的是动态计算图，即每使用一个算子后，该算子会被动态加入到隐含的默认计算图中立即执行得到结果，而无需开启Session。</p>
<p>使用动态计算图即Eager Excution的好处是方便调试程序，它会让TensorFlow代码的表现和Python原生代码的表现一样，写起来就像写numpy一样，各种日志打印，控制流全部都是可以使用的。</p>
<p>使用动态计算图的缺点是运行效率相对会低一些。因为使用动态图会有许多次Python进程和TensorFlow的C<ins>进程之间的通信。而静态计算图构建完成之后几乎全部在TensorFlow内核上使用C</ins>代码执行，效率更高。此外静态图会对计算步骤进行一定的优化，剪去和结果无关的计算步骤。</p>
<p>如果需要在TensorFlow2.0中使用静态图，可以使用@tf.function装饰器将普通Python函数转换成对应的TensorFlow计算图构建代码。运行该函数就相当于在TensorFlow1.0中用Session执行代码。使用tf.function构建静态图的方式叫做 Autograph.</p>
<h3 id="ji-suan-tu-jian-jie">计算图简介</h3>
<p>计算图由节点(nodes)和线(edges)组成。节点表示操作符Operator，或者称之为算子，线表示计算间的依赖。</p>
<p>实线表示有数据传递依赖，传递的数据即张量。虚线通常可以表示控制依赖，即执行先后顺序。</p>
<p><img src="/2020/05/07/tensorflow2/strjoin_graph.png" alt></p>
<h3 id="jing-tai-ji-suan-tu">静态计算图</h3>
<p>在TensorFlow1.0中，使用静态计算图分两步，第一步定义计算图，第二步在会话中执行计算图。<br>
<strong>TensorFlow 1.0静态计算图范例</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义计算图</span></span><br><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    <span class="comment">#placeholder为占位符，执行会话时候指定填充对象</span></span><br><span class="line">    x = tf.placeholder(name=<span class="string">'x'</span>, shape=[], dtype=tf.string)  </span><br><span class="line">    y = tf.placeholder(name=<span class="string">'y'</span>, shape=[], dtype=tf.string)</span><br><span class="line">    z = tf.string_join([x,y],name = <span class="string">'join'</span>,separator=<span class="string">' '</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph = g) <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(fetches = z,feed_dict = &#123;x:<span class="string">"hello"</span>,y:<span class="string">"world"</span>&#125;))</span><br></pre></td></tr></table></figure>
<p><strong>TensorFlow2.0 怀旧版静态计算图</strong></p>
<p>TensorFlow2.0为了确保对老版本tensorflow项目的兼容性，在tf.compat.v1子模块中保留了对TensorFlow1.0那种静态计算图构建风格的支持。(已经不推荐使用了)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">g = tf.compat.v1.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    x = tf.compat.v1.placeholder(name=<span class="string">'x'</span>, shape=[], dtype=tf.string)</span><br><span class="line">    y = tf.compat.v1.placeholder(name=<span class="string">'y'</span>, shape=[], dtype=tf.string)</span><br><span class="line">    z = tf.strings.join([x,y],name = <span class="string">"join"</span>,separator = <span class="string">" "</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session(graph = g) <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># fetches的结果非常像一个函数的返回值，而feed_dict中的占位符相当于函数的参数序列。</span></span><br><span class="line">    result = sess.run(fetches = z,feed_dict = &#123;x:<span class="string">"hello"</span>,y:<span class="string">"world"</span>&#125;)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure>
<figure class="highlight sml"><table><tr><td class="code"><pre><span class="line">b'hello world'</span><br></pre></td></tr></table></figure>
<h3 id="dong-tai-ji-suan-tu">动态计算图</h3>
<p>在TensorFlow2.0中，使用的是动态计算图和Autograph.</p>
<p>在TensorFlow1.0中，使用静态计算图分两步，第一步定义计算图，第二步在会话中执行计算图。</p>
<p>动态计算图已经不区分计算图的定义和执行了，而是定义后立即执行。因此称之为 Eager Excution.</p>
<p>Eager这个英文单词的原意是&quot;迫不及待的&quot;，也就是立即执行的意思。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 动态计算图在每个算子处都进行构建，构建后立即执行</span></span><br><span class="line"></span><br><span class="line">x = tf.constant(<span class="string">"hello"</span>)</span><br><span class="line">y = tf.constant(<span class="string">"world"</span>)</span><br><span class="line">z = tf.strings.join([x,y],separator=<span class="string">" "</span>)</span><br><span class="line"></span><br><span class="line">tf.print(z)</span><br></pre></td></tr></table></figure>
<figure class="highlight ebnf"><table><tr><td class="code"><pre><span class="line"><span class="attribute">hello world</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以将动态计算图代码的输入和输出关系封装成函数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strjoin</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    z =  tf.strings.join([x,y],separator = <span class="string">" "</span>)</span><br><span class="line">    tf.print(z)</span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">result = strjoin(tf.constant(<span class="string">"hello"</span>),tf.constant(<span class="string">"world"</span>))</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">hello world</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">hello</span> <span class="params">world</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br></pre></td></tr></table></figure>
<h3 id="tensor-flow-2-0-de-autograph">TensorFlow2.0的Autograph</h3>
<p>动态计算图运行效率相对较低。</p>
<p>可以用@tf.function装饰器将普通Python函数转换成和TensorFlow1.0对应的静态计算图构建代码。</p>
<p>在TensorFlow1.0中，使用计算图分两步，第一步定义计算图，第二步在会话中执行计算图。</p>
<p>在TensorFlow2.0中，如果采用Autograph的方式使用计算图，第一步定义计算图变成了定义函数，第二步执行计算图变成了调用函数。</p>
<p>不需要使用会话了，一些都像原始的Python语法一样自然。</p>
<p>实践中，我们一般会先用动态计算图调试代码，然后在需要提高性能的的地方利用@tf.function切换成Autograph获得更高的效率。</p>
<p>当然，@tf.function的使用需要遵循一定的规范，后面章节将重点介绍。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用autograph构建静态图</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strjoin</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    z =  tf.strings.join([x,y],separator = <span class="string">" "</span>)</span><br><span class="line">    tf.print(z)</span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">result = strjoin(tf.constant(<span class="string">"hello"</span>),tf.constant(<span class="string">"world"</span>))</span><br><span class="line"></span><br><span class="line">print(result)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">hello world</span><br><span class="line">tf.Tensor(<span class="string">b'hello world'</span>, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建日志</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">stamp = datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">logdir = os.path.join(<span class="string">'data'</span>, <span class="string">'autograph'</span>, stamp)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 在 Python3 下建议使用 pathlib 修正各操作系统的路径</span></span><br><span class="line"><span class="comment"># from pathlib import Path</span></span><br><span class="line"><span class="comment"># stamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")</span></span><br><span class="line"><span class="comment"># logdir = str(Path('./data/autograph/' + stamp))</span></span><br><span class="line"></span><br><span class="line">writer = tf.summary.create_file_writer(logdir)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开启autograph跟踪</span></span><br><span class="line">tf.summary.trace_on(graph=<span class="literal">True</span>, profiler=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#执行autograph</span></span><br><span class="line">result = strjoin(<span class="string">"hello"</span>,<span class="string">"world"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将计算图信息写入日志</span></span><br><span class="line"><span class="keyword">with</span> writer.as_default():</span><br><span class="line">    tf.summary.trace_export(</span><br><span class="line">        name=<span class="string">"autograph"</span>,</span><br><span class="line">        step=<span class="number">0</span>,</span><br><span class="line">        profiler_outdir=logdir)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#启动 tensorboard在jupyter中的魔法命令</span></span><br><span class="line">%load_ext tensorboard</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#启动tensorboard</span></span><br><span class="line">%tensorboard --logdir ./data/autograph/</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/2-2-tensorboard%E8%AE%A1%E7%AE%97%E5%9B%BE.jpg" alt></p>
<h2 id="zi-dong-wei-fen-ji-zhi">自动微分机制</h2>
<p>神经网络通常依赖反向传播求梯度来更新网络参数，求梯度过程通常是一件非常复杂而容易出错的事情。</p>
<p>而深度学习框架可以帮助我们自动地完成这种求梯度运算。</p>
<p>Tensorflow一般使用梯度磁带tf.GradientTape来记录正向运算过程，然后反播磁带自动得到梯度值。</p>
<p>这种利用tf.GradientTape求微分的方法叫做Tensorflow的自动微分机制。</p>
<h3 id="li-yong-ti-du-ci-dai-qiu-dao-shu">利用梯度磁带求导数</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的导数</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    </span><br><span class="line">dy_dx = tape.gradient(y,x)</span><br><span class="line">print(dy_dx) <span class="comment"># tf.Tensor(-2.0, shape=(), dtype=float32)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对常量张量也可以求导，需要增加watch</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    tape.watch([a,b,c])</span><br><span class="line">    y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    </span><br><span class="line">dy_dx,dy_da,dy_db,dy_dc = tape.gradient(y,[x,a,b,c])</span><br><span class="line">print(dy_da) <span class="comment"># tf.Tensor(0.0, shape=(), dtype=float32)</span></span><br><span class="line">print(dy_dc) <span class="comment"># tf.Tensor(1.0, shape=(), dtype=float32)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以求二阶导数</span></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape2:</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape1:   </span><br><span class="line">        y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    dy_dx = tape1.gradient(y,x)   </span><br><span class="line">dy2_dx2 = tape2.gradient(dy_dx,x)</span><br><span class="line"></span><br><span class="line">print(dy2_dx2) <span class="comment"># tf.Tensor(2.0, shape=(), dtype=float32)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以在autograph中使用</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span>   </span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 自变量转换成tf.float32</span></span><br><span class="line">    x = tf.cast(x,tf.float32)</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        tape.watch(x)</span><br><span class="line">        y = a*tf.pow(x,<span class="number">2</span>)+b*x+c</span><br><span class="line">    dy_dx = tape.gradient(y,x) </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span>((dy_dx,y))</span><br><span class="line"></span><br><span class="line">tf.print(f(tf.constant(<span class="number">0.0</span>))) <span class="comment"># (-2, 1)</span></span><br><span class="line">tf.print(f(tf.constant(<span class="number">1.0</span>))) <span class="comment"># (0, 0)</span></span><br></pre></td></tr></table></figure>
<h3 id="li-yong-ti-du-ci-dai-he-you-hua-qi-qiu-zui-xiao-zhi">利用梯度磁带和优化器求最小值</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"><span class="comment"># 使用optimizer.apply_gradients</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    dy_dx = tape.gradient(y,x)</span><br><span class="line">    optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])</span><br><span class="line">    </span><br><span class="line">tf.print(<span class="string">"y ="</span>,y,<span class="string">"; x ="</span>,x) <span class="comment"># y = 0 ; x = 0.999998569</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"><span class="comment"># 使用optimizer.minimize</span></span><br><span class="line"><span class="comment"># optimizer.minimize相当于先用tape求gradient,再apply_gradient</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment">#注意f()无参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">()</span>:</span>   </span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    y = a*tf.pow(x,<span class="number">2</span>)+b*x+c</span><br><span class="line">    <span class="keyword">return</span>(y)</span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)   </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    optimizer.minimize(f,[x])   </span><br><span class="line">    </span><br><span class="line">tf.print(<span class="string">"y ="</span>,f(),<span class="string">"; x ="</span>,x) <span class="comment"># y = 0 ; x = 0.999998569</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在autograph中完成最小值求解</span></span><br><span class="line"><span class="comment"># 使用optimizer.apply_gradients</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minimizef</span><span class="params">()</span>:</span></span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> tf.range(<span class="number">1000</span>): <span class="comment">#注意autograph时使用tf.range(1000)而不是range(1000)</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">        dy_dx = tape.gradient(y,x)</span><br><span class="line">        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])</span><br><span class="line">        </span><br><span class="line">    y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">tf.print(minimizef()) <span class="comment"># 0</span></span><br><span class="line">tf.print(x) <span class="comment"># 0.999998569</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在autograph中完成最小值求解</span></span><br><span class="line"><span class="comment"># 使用optimizer.minimize</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)   </span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">()</span>:</span>   </span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    y = a*tf.pow(x,<span class="number">2</span>)+b*x+c</span><br><span class="line">    <span class="keyword">return</span>(y)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epoch)</span>:</span>  </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> tf.range(epoch):  </span><br><span class="line">        optimizer.minimize(f,[x])</span><br><span class="line">    <span class="keyword">return</span>(f())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.print(train(<span class="number">1000</span>)) <span class="comment"># 0</span></span><br><span class="line">tf.print(x) <span class="comment"># 0.999998569</span></span><br></pre></td></tr></table></figure>
<h1 id="tensor-flow-de-ceng-ci-jie-gou">TensorFlow的层次结构</h1>
<p>本章介绍TensorFlow中5个不同的层次结构：</p>
<ol>
<li>硬件层</li>
<li>内核层</li>
<li>低阶API</li>
<li>中阶API</li>
<li>高阶API。</li>
</ol>
<p>并以线性回归和DNN二分类模型为例，直观对比展示在不同层级实现模型的特点。</p>
<p>TensorFlow的层次结构从低到高可以分成如下五层。</p>
<p>最底层为硬件层，TensorFlow支持CPU、GPU或TPU加入计算资源池。</p>
<p>第二层为C++实现的内核，kernel可以跨平台分布运行。</p>
<p>第三层为Python实现的操作符，提供了封装C++内核的低级API指令，主要包括各种张量操作算子、计算图、自动微分.如tf.Variable,tf.constant,tf.function,tf.GradientTape,tf.nn.softmax…如果把模型比作一个房子，那么第三层API就是【模型之砖】。</p>
<p>第四层为Python实现的模型组件，对低级API进行了函数封装，主要包括各种模型层，损失函数，优化器，数据管道，特征列等等。如tf.keras.layers,tf.keras.losses,tf.keras.metrics,tf.keras.optimizers,tf.data.DataSet,tf.feature_column…如果把模型比作一个房子，那么第四层API就是【模型之墙】。</p>
<p>第五层为Python实现的模型成品，一般为按照OOP方式封装的高级API，主要为tf.keras.models提供的模型的类接口。<br>
如果把模型比作一个房子，那么第五层API就是模型本身，即【模型之屋】。</p>
<img src="/2020/05/07/tensorflow2/../../../Downloads/eat_tensorflow2_in_30_days-master/data/tensorflow_structure.jpg">
<h2 id="di-jie-api">低阶API</h2>
<p>使用低阶API实现线性回归模型和DNN二分类模型。</p>
<p>低阶API主要包括：</p>
<ol>
<li>张量操作</li>
<li>计算图</li>
<li>自动微分。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure>
<h3 id="xian-xing-hui-gui-mo-xing">线性回归模型</h3>
<h4 id="zhun-bei-shu-ju-4">准备数据</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#样本数量</span></span><br><span class="line">n = <span class="number">400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成测试用数据集</span></span><br><span class="line">X = tf.random.uniform([n,<span class="number">2</span>],minval=<span class="number">-10</span>,maxval=<span class="number">10</span>) </span><br><span class="line">w0 = tf.constant([[<span class="number">2.0</span>],[<span class="number">-3.0</span>]])</span><br><span class="line">b0 = tf.constant([[<span class="number">3.0</span>]])</span><br><span class="line">Y = X@w0 + b0 + tf.random.normal([n,<span class="number">1</span>],mean = <span class="number">0.0</span>,stddev= <span class="number">2.0</span>)  <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], c = <span class="string">"b"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], c = <span class="string">"g"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/3-1-01-%E5%9B%9E%E5%BD%92%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建数据管道迭代器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(features, labels, batch_size=<span class="number">8</span>)</span>:</span></span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    np.random.shuffle(indices)  <span class="comment">#样本的读取顺序是随机的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        indexs = indices[i: min(i + batch_size, num_examples)]</span><br><span class="line">        <span class="keyword">yield</span> tf.gather(features,indexs), tf.gather(labels,indexs)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 测试数据管道效果   </span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">(features,labels) = next(data_iter(X,Y,batch_size))</span><br><span class="line">print(features)</span><br><span class="line">print(labels)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[ <span class="number">2.6161194</span>   <span class="number">0.11071014</span>]</span><br><span class="line"> [ <span class="number">9.79207</span>    <span class="number">-0.70180416</span>]</span><br><span class="line"> [ <span class="number">9.792343</span>    <span class="number">6.9149055</span> ]</span><br><span class="line"> [<span class="number">-2.4186516</span>  <span class="number">-9.375019</span>  ]</span><br><span class="line"> [ <span class="number">9.83749</span>    <span class="number">-3.4637213</span> ]</span><br><span class="line"> [ <span class="number">7.3953056</span>   <span class="number">4.374569</span>  ]</span><br><span class="line"> [<span class="number">-0.14686584</span> <span class="number">-0.28063297</span>]</span><br><span class="line"> [ <span class="number">0.49001217</span> <span class="number">-9.739792</span>  ]], shape=(<span class="number">8</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[ <span class="number">9.334667</span> ]</span><br><span class="line"> [<span class="number">22.058844</span> ]</span><br><span class="line"> [ <span class="number">3.0695205</span>]</span><br><span class="line"> [<span class="number">26.736238</span> ]</span><br><span class="line"> [<span class="number">35.292133</span> ]</span><br><span class="line"> [ <span class="number">4.2943544</span>]</span><br><span class="line"> [ <span class="number">1.6713585</span>]</span><br><span class="line"> [<span class="number">34.826904</span> ]], shape=(<span class="number">8</span>, <span class="number">1</span>), dtype=float32)</span><br></pre></td></tr></table></figure>
<h4 id="ding-yi-mo-xing-4">定义模型</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = tf.Variable(tf.random.normal(w0.shape))</span><br><span class="line">b = tf.Variable(tf.zeros_like(b0,dtype = tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span>:</span>     </span><br><span class="line">    <span class="comment">#正向传播</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self,x)</span>:</span> </span><br><span class="line">        <span class="keyword">return</span> x@w + b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 损失函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_func</span><span class="params">(self,y_true,y_pred)</span>:</span>  </span><br><span class="line">        <span class="keyword">return</span> tf.reduce_mean((y_true - y_pred)**<span class="number">2</span>/<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model = LinearRegression()</span><br></pre></td></tr></table></figure>
<h4 id="xun-lian-mo-xing-4">训练模型</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用动态图调试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = model.loss_func(labels, predictions)</span><br><span class="line">    <span class="comment"># 反向传播求梯度</span></span><br><span class="line">    dloss_dw,dloss_db = tape.gradient(loss,[w,b])</span><br><span class="line">    <span class="comment"># 梯度下降法更新参数</span></span><br><span class="line">    w.assign(w - <span class="number">0.001</span>*dloss_dw)</span><br><span class="line">    b.assign(b - <span class="number">0.001</span>*dloss_db)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试train_step效果</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">(features,labels) = next(data_iter(X,Y,batch_size))</span><br><span class="line">train_step(model,features,labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">tf.Tensor:</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">211.09982</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> data_iter(X,Y,<span class="number">10</span>):</span><br><span class="line">            loss = train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">50</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">"loss = "</span>,loss)</span><br><span class="line">            tf.print(<span class="string">"w ="</span>,w)</span><br><span class="line">            tf.print(<span class="string">"b ="</span>,b)</span><br><span class="line"></span><br><span class="line">train_model(model,epochs = <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">================================================================================<span class="number">16</span>:<span class="number">35</span>:<span class="number">56</span></span><br><span class="line">epoch = <span class="number">50</span> loss =  <span class="number">1.78806472</span></span><br><span class="line">w = <span class="string">[[1.97554708]</span></span><br><span class="line"><span class="string"> [-2.97719598]]</span></span><br><span class="line">b = <span class="string">[[2.60692883]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">00</span></span><br><span class="line">epoch = <span class="number">100</span> loss =  <span class="number">2.64588404</span></span><br><span class="line">w = <span class="string">[[1.97319281]</span></span><br><span class="line"><span class="string"> [-2.97810626]]</span></span><br><span class="line">b = <span class="string">[[2.95525956]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">04</span></span><br><span class="line">epoch = <span class="number">150</span> loss =  <span class="number">1.42576694</span></span><br><span class="line">w = <span class="string">[[1.96466208]</span></span><br><span class="line"><span class="string"> [-2.98337793]]</span></span><br><span class="line">b = <span class="string">[[3.00264144]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">08</span></span><br><span class="line">epoch = <span class="number">200</span> loss =  <span class="number">1.68992615</span></span><br><span class="line">w = <span class="string">[[1.97718477]</span></span><br><span class="line"><span class="string"> [-2.983814]]</span></span><br><span class="line">b = <span class="string">[[3.01013041]]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">##使用autograph机制转换成静态图加速</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = model.loss_func(labels, predictions)</span><br><span class="line">    <span class="comment"># 反向传播求梯度</span></span><br><span class="line">    dloss_dw,dloss_db = tape.gradient(loss,[w,b])</span><br><span class="line">    <span class="comment"># 梯度下降法更新参数</span></span><br><span class="line">    w.assign(w - <span class="number">0.001</span>*dloss_dw)</span><br><span class="line">    b.assign(b - <span class="number">0.001</span>*dloss_db)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> data_iter(X,Y,<span class="number">10</span>):</span><br><span class="line">            loss = train_step(model,features,labels)</span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">50</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">"loss = "</span>,loss)</span><br><span class="line">            tf.print(<span class="string">"w ="</span>,w)</span><br><span class="line">            tf.print(<span class="string">"b ="</span>,b)</span><br><span class="line"></span><br><span class="line">train_model(model,epochs = <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">35</span></span><br><span class="line">epoch = <span class="number">50</span> loss =  <span class="number">0.894210339</span></span><br><span class="line">w = <span class="string">[[1.96927285]</span></span><br><span class="line"><span class="string"> [-2.98914337]]</span></span><br><span class="line">b = <span class="string">[[3.00987792]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">36</span></span><br><span class="line">epoch = <span class="number">100</span> loss =  <span class="number">1.58621466</span></span><br><span class="line">w = <span class="string">[[1.97566223]</span></span><br><span class="line"><span class="string"> [-2.98550248]]</span></span><br><span class="line">b = <span class="string">[[3.00998402]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">37</span></span><br><span class="line">epoch = <span class="number">150</span> loss =  <span class="number">2.2695992</span></span><br><span class="line">w = <span class="string">[[1.96664226]</span></span><br><span class="line"><span class="string"> [-2.99248481]]</span></span><br><span class="line">b = <span class="string">[[3.01028705]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">38</span></span><br><span class="line">epoch = <span class="number">200</span> loss =  <span class="number">1.90848124</span></span><br><span class="line">w = <span class="string">[[1.98000824]</span></span><br><span class="line"><span class="string"> [-2.98888135]]</span></span><br><span class="line">b = <span class="string">[[3.01085401]]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], c = <span class="string">"b"</span>,label = <span class="string">"samples"</span>)</span><br><span class="line">ax1.plot(X[:,<span class="number">0</span>],w[<span class="number">0</span>]*X[:,<span class="number">0</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">"model"</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], c = <span class="string">"g"</span>,label = <span class="string">"samples"</span>)</span><br><span class="line">ax2.plot(X[:,<span class="number">1</span>],w[<span class="number">1</span>]*X[:,<span class="number">1</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">"model"</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/3-1-2-%E5%9B%9E%E5%BD%92%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<h3 id="dnn-er-fen-lei-mo-xing">DNN二分类模型</h3>
<h4 id="zhun-bei-shu-ju-5">准备数据</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#正负样本数量</span></span><br><span class="line">n_positive,n_negative = <span class="number">2000</span>,<span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成正样本, 小圆环分布</span></span><br><span class="line">r_p = <span class="number">5.0</span> + tf.random.truncated_normal([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_p = tf.random.uniform([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xp = tf.concat([r_p*tf.cos(theta_p),r_p*tf.sin(theta_p)],axis = <span class="number">1</span>)</span><br><span class="line">Yp = tf.ones_like(r_p)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成负样本, 大圆环分布</span></span><br><span class="line">r_n = <span class="number">8.0</span> + tf.random.truncated_normal([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_n = tf.random.uniform([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xn = tf.concat([r_n*tf.cos(theta_n),r_n*tf.sin(theta_n)],axis = <span class="number">1</span>)</span><br><span class="line">Yn = tf.zeros_like(r_n)</span><br><span class="line"></span><br><span class="line"><span class="comment">#汇总样本</span></span><br><span class="line">X = tf.concat([Xp,Xn],axis = <span class="number">0</span>)</span><br><span class="line">Y = tf.concat([Yp,Yn],axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line">plt.figure(figsize = (<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">plt.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">plt.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/3-1-03-%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建数据管道迭代器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(features, labels, batch_size=<span class="number">8</span>)</span>:</span></span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    np.random.shuffle(indices)  <span class="comment">#样本的读取顺序是随机的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        indexs = indices[i: min(i + batch_size, num_examples)]</span><br><span class="line">        <span class="keyword">yield</span> tf.gather(features,indexs), tf.gather(labels,indexs)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 测试数据管道效果   </span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">(features,labels) = next(data_iter(X,Y,batch_size))</span><br><span class="line">print(features)</span><br><span class="line">print(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[ <span class="number">0.03732629</span>  <span class="number">3.5783494</span> ]</span><br><span class="line"> [ <span class="number">0.542919</span>    <span class="number">5.035079</span>  ]</span><br><span class="line"> [ <span class="number">5.860281</span>   <span class="number">-2.4476354</span> ]</span><br><span class="line"> [ <span class="number">0.63657564</span>  <span class="number">3.194231</span>  ]</span><br><span class="line"> [<span class="number">-3.5072308</span>   <span class="number">2.5578873</span> ]</span><br><span class="line"> [<span class="number">-2.4109735</span>  <span class="number">-3.6621518</span> ]</span><br><span class="line"> [ <span class="number">4.0975413</span>  <span class="number">-2.4172943</span> ]</span><br><span class="line"> [ <span class="number">1.9393908</span>  <span class="number">-6.782317</span>  ]</span><br><span class="line"> [<span class="number">-4.7453732</span>  <span class="number">-0.5176727</span> ]</span><br><span class="line"> [<span class="number">-1.4057113</span>  <span class="number">-7.9775257</span> ]], shape=(<span class="number">10</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span>]], shape=(<span class="number">10</span>, <span class="number">1</span>), dtype=float32)</span><br></pre></td></tr></table></figure>
<h4 id="ding-yi-mo-xing-5">定义模型</h4>
<p>利用tf.Module来组织模型变量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DNNModel</span><span class="params">(tf.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name = None)</span>:</span></span><br><span class="line">        super(DNNModel, self).__init__(name=name)</span><br><span class="line">        self.w1 = tf.Variable(tf.random.truncated_normal([<span class="number">2</span>,<span class="number">4</span>]),dtype = tf.float32)</span><br><span class="line">        self.b1 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">4</span>]),dtype = tf.float32)</span><br><span class="line">        self.w2 = tf.Variable(tf.random.truncated_normal([<span class="number">4</span>,<span class="number">8</span>]),dtype = tf.float32)</span><br><span class="line">        self.b2 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">8</span>]),dtype = tf.float32)</span><br><span class="line">        self.w3 = tf.Variable(tf.random.truncated_normal([<span class="number">8</span>,<span class="number">1</span>]),dtype = tf.float32)</span><br><span class="line">        self.b3 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">1</span>]),dtype = tf.float32)</span><br><span class="line"></span><br><span class="line">     </span><br><span class="line">    <span class="comment"># 正向传播</span></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [None,2], dtype = tf.float32)])  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x = tf.nn.relu(x@self.w1 + self.b1)</span><br><span class="line">        x = tf.nn.relu(x@self.w2 + self.b2)</span><br><span class="line">        y = tf.nn.sigmoid(x@self.w3 + self.b3)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 损失函数(二元交叉熵)</span></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [None,1], dtype = tf.float32),</span></span><br><span class="line">                              tf.TensorSpec(shape = [<span class="literal">None</span>,<span class="number">1</span>], dtype = tf.float32)])  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_func</span><span class="params">(self,y_true,y_pred)</span>:</span>  </span><br><span class="line">        <span class="comment">#将预测值限制在 1e-7 以上, 1 - 1e-7 以下，避免log(0)错误</span></span><br><span class="line">        eps = <span class="number">1e-7</span></span><br><span class="line">        y_pred = tf.clip_by_value(y_pred,eps,<span class="number">1.0</span>-eps)</span><br><span class="line">        bce = - y_true*tf.math.log(y_pred) - (<span class="number">1</span>-y_true)*tf.math.log(<span class="number">1</span>-y_pred)</span><br><span class="line">        <span class="keyword">return</span>  tf.reduce_mean(bce)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 评估指标(准确率)</span></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [None,1], dtype = tf.float32),</span></span><br><span class="line">                              tf.TensorSpec(shape = [<span class="literal">None</span>,<span class="number">1</span>], dtype = tf.float32)]) </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">metric_func</span><span class="params">(self,y_true,y_pred)</span>:</span></span><br><span class="line">        y_pred = tf.where(y_pred&gt;<span class="number">0.5</span>,tf.ones_like(y_pred,dtype = tf.float32),</span><br><span class="line">                          tf.zeros_like(y_pred,dtype = tf.float32))</span><br><span class="line">        acc = tf.reduce_mean(<span class="number">1</span>-tf.abs(y_true-y_pred))</span><br><span class="line">        <span class="keyword">return</span> acc</span><br><span class="line">    </span><br><span class="line">model = DNNModel()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试模型结构</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">(features,labels) = next(data_iter(X,Y,batch_size))</span><br><span class="line"></span><br><span class="line">predictions = model(features)</span><br><span class="line"></span><br><span class="line">loss = model.loss_func(labels,predictions)</span><br><span class="line">metric = model.metric_func(labels,predictions)</span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"init loss:"</span>,loss) <span class="comment"># init loss: 1.76568353</span></span><br><span class="line">tf.print(<span class="string">"init metric"</span>,metric) <span class="comment"># init metric 0.6</span></span><br><span class="line">print(len(model.trainable_variables)) <span class="comment"># 6</span></span><br></pre></td></tr></table></figure>
<p><strong>3，训练模型</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">##使用autograph机制转换成静态图加速</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 正向传播求损失</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = model.loss_func(labels, predictions) </span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 反向传播求梯度</span></span><br><span class="line">    grads = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 执行梯度下降</span></span><br><span class="line">    <span class="keyword">for</span> p, dloss_dp <span class="keyword">in</span> zip(model.trainable_variables,grads):</span><br><span class="line">        p.assign(p - <span class="number">0.001</span>*dloss_dp)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 计算评估指标</span></span><br><span class="line">    metric = model.metric_func(labels,predictions)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss, metric</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> data_iter(X,Y,<span class="number">100</span>):</span><br><span class="line">            loss,metric = train_step(model,features,labels)</span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">"loss = "</span>,loss, <span class="string">"accuracy = "</span>, metric)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">train_model(model,epochs = <span class="number">600</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">35</span></span><br><span class="line">epoch = <span class="number">100</span> loss =  <span class="number">0.567795336</span> accuracy =  <span class="number">0.71</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">39</span></span><br><span class="line">epoch = <span class="number">200</span> loss =  <span class="number">0.50955683</span> accuracy =  <span class="number">0.77</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">43</span></span><br><span class="line">epoch = <span class="number">300</span> loss =  <span class="number">0.421476126</span> accuracy =  <span class="number">0.84</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">47</span></span><br><span class="line">epoch = <span class="number">400</span> loss =  <span class="number">0.330618203</span> accuracy =  <span class="number">0.9</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">51</span></span><br><span class="line">epoch = <span class="number">500</span> loss =  <span class="number">0.308296859</span> accuracy =  <span class="number">0.89</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">55</span></span><br><span class="line">epoch = <span class="number">600</span> loss =  <span class="number">0.279367268</span> accuracy =  <span class="number">0.96</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line">fig, (ax1,ax2) = plt.subplots(nrows=<span class="number">1</span>,ncols=<span class="number">2</span>,figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1.scatter(Xp[:,<span class="number">0</span>],Xp[:,<span class="number">1</span>],c = <span class="string">"r"</span>)</span><br><span class="line">ax1.scatter(Xn[:,<span class="number">0</span>],Xn[:,<span class="number">1</span>],c = <span class="string">"g"</span>)</span><br><span class="line">ax1.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax1.set_title(<span class="string">"y_true"</span>);</span><br><span class="line"></span><br><span class="line">Xp_pred = tf.boolean_mask(X,tf.squeeze(model(X)&gt;=<span class="number">0.5</span>),axis = <span class="number">0</span>)</span><br><span class="line">Xn_pred = tf.boolean_mask(X,tf.squeeze(model(X)&lt;<span class="number">0.5</span>),axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2.scatter(Xp_pred[:,<span class="number">0</span>],Xp_pred[:,<span class="number">1</span>],c = <span class="string">"r"</span>)</span><br><span class="line">ax2.scatter(Xn_pred[:,<span class="number">0</span>],Xn_pred[:,<span class="number">1</span>],c = <span class="string">"g"</span>)</span><br><span class="line">ax2.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax2.set_title(<span class="string">"y_pred"</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/3-1-04-%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<h2 id="zhong-jie-api">中阶API</h2>
<p>使用TensorFlow的中阶API实现线性回归模型和和DNN二分类模型。</p>
<p>TensorFlow的中阶API主要包括：</p>
<ol>
<li>各种模型层</li>
<li>损失函数</li>
<li>优化器</li>
<li>数据管道</li>
<li>特征列</li>
<li>…</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure>
<h3 id="xian-xing-hui-gui-mo-xing-1">线性回归模型</h3>
<h4 id="zhun-bei-shu-ju-6">准备数据</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,losses,metrics,optimizers</span><br><span class="line"></span><br><span class="line"><span class="comment">#样本数量</span></span><br><span class="line">n = <span class="number">400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成测试用数据集</span></span><br><span class="line">X = tf.random.uniform([n,<span class="number">2</span>],minval=<span class="number">-10</span>,maxval=<span class="number">10</span>) </span><br><span class="line">w0 = tf.constant([[<span class="number">2.0</span>],[<span class="number">-3.0</span>]])</span><br><span class="line">b0 = tf.constant([[<span class="number">3.0</span>]])</span><br><span class="line">Y = X@w0 + b0 + tf.random.normal([n,<span class="number">1</span>],mean = <span class="number">0.0</span>,stddev= <span class="number">2.0</span>)  <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], c = <span class="string">"b"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], c = <span class="string">"g"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/3-2-01-%E5%9B%9E%E5%BD%92%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#构建输入数据管道</span></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices((X,Y)) \</span><br><span class="line">     .shuffle(buffer_size = <span class="number">100</span>).batch(<span class="number">10</span>) \</span><br><span class="line">     .prefetch(tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure>
<h4 id="ding-yi-mo-xing-6">定义模型</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.build(input_shape = (<span class="number">2</span>,)) <span class="comment">#用build方法创建variables</span></span><br><span class="line">model.loss_func = losses.mean_squared_error</span><br><span class="line">model.optimizer = optimizers.SGD(learning_rate=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>
<h4 id="xun-lian-mo-xing-5">训练模型</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用autograph机制转换成静态图加速</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = model.loss_func(tf.reshape(labels,[<span class="number">-1</span>]), tf.reshape(predictions,[<span class="number">-1</span>]))</span><br><span class="line">    grads = tape.gradient(loss,model.variables)</span><br><span class="line">    model.optimizer.apply_gradients(zip(grads,model.variables))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试train_step效果</span></span><br><span class="line">features,labels = next(ds.as_numpy_iterator())</span><br><span class="line">train_step(model,features,labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        loss = tf.constant(<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds:</span><br><span class="line">            loss = train_step(model,features,labels)</span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">50</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">"loss = "</span>,loss)</span><br><span class="line">            tf.print(<span class="string">"w ="</span>,model.variables[<span class="number">0</span>])</span><br><span class="line">            tf.print(<span class="string">"b ="</span>,model.variables[<span class="number">1</span>])</span><br><span class="line">train_model(model,epochs = <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">01</span>:<span class="number">48</span></span><br><span class="line">epoch = <span class="number">50</span> loss =  <span class="number">2.56481647</span></span><br><span class="line">w = [[<span class="number">1.99355531</span>]</span><br><span class="line"> [<span class="number">-2.99061537</span>]]</span><br><span class="line">b = [<span class="number">3.09484935</span>]</span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">01</span>:<span class="number">51</span></span><br><span class="line">epoch = <span class="number">100</span> loss =  <span class="number">5.96198225</span></span><br><span class="line">w = [[<span class="number">1.98028314</span>]</span><br><span class="line"> [<span class="number">-2.96975136</span>]]</span><br><span class="line">b = [<span class="number">3.09501529</span>]</span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">01</span>:<span class="number">54</span></span><br><span class="line">epoch = <span class="number">150</span> loss =  <span class="number">4.79625702</span></span><br><span class="line">w = [[<span class="number">2.00056171</span>]</span><br><span class="line"> [<span class="number">-2.98774862</span>]]</span><br><span class="line">b = [<span class="number">3.09567738</span>]</span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">01</span>:<span class="number">58</span></span><br><span class="line">epoch = <span class="number">200</span> loss =  <span class="number">8.26704407</span></span><br><span class="line">w = [[<span class="number">2.00282311</span>]</span><br><span class="line"> [<span class="number">-2.99300027</span>]]</span><br><span class="line">b = [<span class="number">3.09406662</span>]</span><br></pre></td></tr></table></figure>
<h4 id="jie-guo-ke-shi-hua">结果可视化</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line">w,b = model.variables</span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], c = <span class="string">"b"</span>,label = <span class="string">"samples"</span>)</span><br><span class="line">ax1.plot(X[:,<span class="number">0</span>],w[<span class="number">0</span>]*X[:,<span class="number">0</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">"model"</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], c = <span class="string">"g"</span>,label = <span class="string">"samples"</span>)</span><br><span class="line">ax2.plot(X[:,<span class="number">1</span>],w[<span class="number">1</span>]*X[:,<span class="number">1</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">"model"</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/3-2-02-%E5%9B%9E%E5%BD%92%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<h3 id="dnn-er-fen-lei-mo-xing-1">DNN二分类模型</h3>
<h4 id="zhun-bei-shu-ju-7">准备数据</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,losses,metrics,optimizers</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#正负样本数量</span></span><br><span class="line">n_positive,n_negative = <span class="number">2000</span>,<span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成正样本, 小圆环分布</span></span><br><span class="line">r_p = <span class="number">5.0</span> + tf.random.truncated_normal([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_p = tf.random.uniform([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xp = tf.concat([r_p*tf.cos(theta_p),r_p*tf.sin(theta_p)],axis = <span class="number">1</span>)</span><br><span class="line">Yp = tf.ones_like(r_p)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成负样本, 大圆环分布</span></span><br><span class="line">r_n = <span class="number">8.0</span> + tf.random.truncated_normal([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_n = tf.random.uniform([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xn = tf.concat([r_n*tf.cos(theta_n),r_n*tf.sin(theta_n)],axis = <span class="number">1</span>)</span><br><span class="line">Yn = tf.zeros_like(r_n)</span><br><span class="line"></span><br><span class="line"><span class="comment">#汇总样本</span></span><br><span class="line">X = tf.concat([Xp,Xn],axis = <span class="number">0</span>)</span><br><span class="line">Y = tf.concat([Yp,Yn],axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line">plt.figure(figsize = (<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">plt.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">plt.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/3-1-03-%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96-4135196.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#构建输入数据管道</span></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices((X,Y)) \</span><br><span class="line">     .shuffle(buffer_size = <span class="number">4000</span>).batch(<span class="number">100</span>) \</span><br><span class="line">     .prefetch(tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure>
<h4 id="ding-yi-mo-xing-7">定义模型</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DNNModel</span><span class="params">(tf.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name = None)</span>:</span></span><br><span class="line">        super(DNNModel, self).__init__(name=name)</span><br><span class="line">        self.dense1 = layers.Dense(<span class="number">4</span>,activation = <span class="string">"relu"</span>) </span><br><span class="line">        self.dense2 = layers.Dense(<span class="number">8</span>,activation = <span class="string">"relu"</span>)</span><br><span class="line">        self.dense3 = layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>)</span><br><span class="line"></span><br><span class="line">     </span><br><span class="line">    <span class="comment"># 正向传播</span></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [None,2], dtype = tf.float32)])  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x = self.dense1(x)</span><br><span class="line">        x = self.dense2(x)</span><br><span class="line">        y = self.dense3(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line">model = DNNModel()</span><br><span class="line">model.loss_func = losses.binary_crossentropy</span><br><span class="line">model.metric_func = metrics.binary_accuracy</span><br><span class="line">model.optimizer = optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试模型结构</span></span><br><span class="line">(features,labels) = next(ds.as_numpy_iterator())</span><br><span class="line"></span><br><span class="line">predictions = model(features)</span><br><span class="line"></span><br><span class="line">loss = model.loss_func(tf.reshape(labels,[<span class="number">-1</span>]),tf.reshape(predictions,[<span class="number">-1</span>]))</span><br><span class="line">metric = model.metric_func(tf.reshape(labels,[<span class="number">-1</span>]),tf.reshape(predictions,[<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"init loss:"</span>,loss) <span class="comment"># init loss: 1.13653195</span></span><br><span class="line">tf.print(<span class="string">"init metric"</span>,metric) <span class="comment"># init metric 0.5</span></span><br></pre></td></tr></table></figure>
<h4 id="xun-lian-mo-xing-6">训练模型</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用autograph机制转换成静态图加速</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = model.loss_func(tf.reshape(labels,[<span class="number">-1</span>]), tf.reshape(predictions,[<span class="number">-1</span>]))</span><br><span class="line">    grads = tape.gradient(loss,model.trainable_variables)</span><br><span class="line">    model.optimizer.apply_gradients(zip(grads,model.trainable_variables))</span><br><span class="line">    </span><br><span class="line">    metric = model.metric_func(tf.reshape(labels,[<span class="number">-1</span>]), tf.reshape(predictions,[<span class="number">-1</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss,metric</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试train_step效果</span></span><br><span class="line">features,labels = next(ds.as_numpy_iterator())</span><br><span class="line">train_step(model,features,labels)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">(&lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">1.2033114</span>&gt;,&lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">0.47</span>&gt;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        loss, metric = tf.constant(<span class="number">0.0</span>),tf.constant(<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds:</span><br><span class="line">            loss,metric = train_step(model,features,labels)</span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">"loss = "</span>,loss, <span class="string">"accuracy = "</span>,metric)</span><br><span class="line">train_model(model,epochs = <span class="number">60</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">36</span></span><br><span class="line">epoch = <span class="number">10</span> loss =  <span class="number">0.556449413</span> accuracy =  <span class="number">0.79</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">38</span></span><br><span class="line">epoch = <span class="number">20</span> loss =  <span class="number">0.439187407</span> accuracy =  <span class="number">0.86</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">40</span></span><br><span class="line">epoch = <span class="number">30</span> loss =  <span class="number">0.259921253</span> accuracy =  <span class="number">0.95</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">42</span></span><br><span class="line">epoch = <span class="number">40</span> loss =  <span class="number">0.244920313</span> accuracy =  <span class="number">0.9</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">43</span></span><br><span class="line">epoch = <span class="number">50</span> loss =  <span class="number">0.19839409</span> accuracy =  <span class="number">0.92</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">45</span></span><br><span class="line">epoch = <span class="number">60</span> loss =  <span class="number">0.126151696</span> accuracy =  <span class="number">0.95</span></span><br></pre></td></tr></table></figure>
<h4 id="jie-guo-ke-shi-hua-1">结果可视化</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">fig, (ax1,ax2) = plt.subplots(nrows=<span class="number">1</span>,ncols=<span class="number">2</span>,figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">ax1.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">ax1.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax1.set_title(<span class="string">"y_true"</span>);</span><br><span class="line"></span><br><span class="line">Xp_pred = tf.boolean_mask(X,tf.squeeze(model(X)&gt;=<span class="number">0.5</span>),axis = <span class="number">0</span>)</span><br><span class="line">Xn_pred = tf.boolean_mask(X,tf.squeeze(model(X)&lt;<span class="number">0.5</span>),axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2.scatter(Xp_pred[:,<span class="number">0</span>].numpy(),Xp_pred[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">ax2.scatter(Xn_pred[:,<span class="number">0</span>].numpy(),Xn_pred[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">ax2.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax2.set_title(<span class="string">"y_pred"</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/3-2-04-%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<h2 id="gao-jie-api">高阶API</h2>
<p>使用TensorFlow的高阶API实现线性回归模型和DNN二分类模型。TensorFlow的高阶API主要为tf.keras.models提供的模型的类接口。</p>
<p>使用Keras接口有以下3种方式构建模型：</p>
<ol>
<li>使用Sequential按层顺序构建模型</li>
<li>使用函数式API构建任意结构模型</li>
<li>继承Model基类构建自定义模型。</li>
</ol>
<p>此处分别使用Sequential按层顺序构建模型以及继承Model基类构建自定义模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure>
<h3 id="xian-xing-hui-gui-mo-xing-2">线性回归模型</h3>
<p>使用Sequential按层顺序构建模型，并使用内置model.fit方法训练模型。</p>
<h4 id="zhun-bei-shu-ju-8">准备数据</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,losses,metrics,optimizers</span><br><span class="line"></span><br><span class="line"><span class="comment">#样本数量</span></span><br><span class="line">n = <span class="number">400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成测试用数据集</span></span><br><span class="line">X = tf.random.uniform([n,<span class="number">2</span>],minval=<span class="number">-10</span>,maxval=<span class="number">10</span>) </span><br><span class="line">w0 = tf.constant([[<span class="number">2.0</span>],[<span class="number">-3.0</span>]])</span><br><span class="line">b0 = tf.constant([[<span class="number">3.0</span>]])</span><br><span class="line">Y = X@w0 + b0 + tf.random.normal([n,<span class="number">1</span>],mean = <span class="number">0.0</span>,stddev= <span class="number">2.0</span>)  <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], c = <span class="string">"b"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], c = <span class="string">"g"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/3-3-01-%E5%9B%9E%E5%BD%92%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<h4 id="ding-yi-mo-xing-8">定义模型</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>,input_shape =(<span class="number">2</span>,)))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">dense (Dense)                (None, 1)                 3         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 3</span><br><span class="line">Trainable params: 3</span><br><span class="line">Non-trainable params: 0</span><br></pre></td></tr></table></figure>
<h4 id="xun-lian-mo-xing-7">训练模型</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">### 使用fit方法进行训练</span></span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">"adam"</span>,loss=<span class="string">"mse"</span>,metrics=[<span class="string">"mae"</span>])</span><br><span class="line">model.fit(X,Y,batch_size = <span class="number">10</span>,epochs = <span class="number">200</span>)  </span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"w = "</span>,model.layers[<span class="number">0</span>].kernel)</span><br><span class="line">tf.print(<span class="string">"b = "</span>,model.layers[<span class="number">0</span>].bias)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">Epoch <span class="number">197</span>/<span class="number">200</span></span><br><span class="line"><span class="number">400</span>/<span class="number">400</span> [==============================] - <span class="number">0</span>s <span class="number">190</span>us/sample - loss: <span class="number">4.3977</span> - mae: <span class="number">1.7129</span></span><br><span class="line">Epoch <span class="number">198</span>/<span class="number">200</span></span><br><span class="line"><span class="number">400</span>/<span class="number">400</span> [==============================] - <span class="number">0</span>s <span class="number">172</span>us/sample - loss: <span class="number">4.3918</span> - mae: <span class="number">1.7117</span></span><br><span class="line">Epoch <span class="number">199</span>/<span class="number">200</span></span><br><span class="line"><span class="number">400</span>/<span class="number">400</span> [==============================] - <span class="number">0</span>s <span class="number">134</span>us/sample - loss: <span class="number">4.3861</span> - mae: <span class="number">1.7106</span></span><br><span class="line">Epoch <span class="number">200</span>/<span class="number">200</span></span><br><span class="line"><span class="number">400</span>/<span class="number">400</span> [==============================] - <span class="number">0</span>s <span class="number">166</span>us/sample - loss: <span class="number">4.3786</span> - mae: <span class="number">1.7092</span></span><br><span class="line">w =  [[<span class="number">1.99339032</span>]</span><br><span class="line"> [<span class="number">-3.00866461</span>]]</span><br><span class="line">b =  [<span class="number">2.67018795</span>]</span><br></pre></td></tr></table></figure>
<h4 id="jie-guo-ke-shi-hua-2">结果可视化</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line">w,b = model.variables</span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], c = <span class="string">"b"</span>,label = <span class="string">"samples"</span>)</span><br><span class="line">ax1.plot(X[:,<span class="number">0</span>],w[<span class="number">0</span>]*X[:,<span class="number">0</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">"model"</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], c = <span class="string">"g"</span>,label = <span class="string">"samples"</span>)</span><br><span class="line">ax2.plot(X[:,<span class="number">1</span>],w[<span class="number">1</span>]*X[:,<span class="number">1</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">"model"</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/3-3-02-%E5%9B%9E%E5%BD%92%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<h3 id="er-dnn-er-fen-lei-mo-xing">二，DNN二分类模型</h3>
<p>此范例我们使用继承Model基类构建自定义模型，并构建自定义训练循环【面向专家】</p>
<h4 id="zhun-bei-shu-ju-9">准备数据</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,losses,metrics,optimizers</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#正负样本数量</span></span><br><span class="line">n_positive,n_negative = <span class="number">2000</span>,<span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成正样本, 小圆环分布</span></span><br><span class="line">r_p = <span class="number">5.0</span> + tf.random.truncated_normal([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_p = tf.random.uniform([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xp = tf.concat([r_p*tf.cos(theta_p),r_p*tf.sin(theta_p)],axis = <span class="number">1</span>)</span><br><span class="line">Yp = tf.ones_like(r_p)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成负样本, 大圆环分布</span></span><br><span class="line">r_n = <span class="number">8.0</span> + tf.random.truncated_normal([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_n = tf.random.uniform([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xn = tf.concat([r_n*tf.cos(theta_n),r_n*tf.sin(theta_n)],axis = <span class="number">1</span>)</span><br><span class="line">Yn = tf.zeros_like(r_n)</span><br><span class="line"></span><br><span class="line"><span class="comment">#汇总样本</span></span><br><span class="line">X = tf.concat([Xp,Xn],axis = <span class="number">0</span>)</span><br><span class="line">Y = tf.concat([Yp,Yn],axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#样本洗牌</span></span><br><span class="line">data = tf.concat([X,Y],axis = <span class="number">1</span>)</span><br><span class="line">data = tf.random.shuffle(data)</span><br><span class="line">X = data[:,:<span class="number">2</span>]</span><br><span class="line">Y = data[:,<span class="number">2</span>:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line">plt.figure(figsize = (<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">plt.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">plt.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/3-3-03-%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ds_train = tf.data.Dataset.from_tensor_slices((X[<span class="number">0</span>:n*<span class="number">3</span>//<span class="number">4</span>,:],Y[<span class="number">0</span>:n*<span class="number">3</span>//<span class="number">4</span>,:])) \</span><br><span class="line">     .shuffle(buffer_size = <span class="number">1000</span>).batch(<span class="number">20</span>) \</span><br><span class="line">     .prefetch(tf.data.experimental.AUTOTUNE) \</span><br><span class="line">     .cache()</span><br><span class="line"></span><br><span class="line">ds_valid = tf.data.Dataset.from_tensor_slices((X[n*<span class="number">3</span>//<span class="number">4</span>:,:],Y[n*<span class="number">3</span>//<span class="number">4</span>:,:])) \</span><br><span class="line">     .batch(<span class="number">20</span>) \</span><br><span class="line">     .prefetch(tf.data.experimental.AUTOTUNE) \</span><br><span class="line">     .cache()</span><br></pre></td></tr></table></figure>
<h4 id="ding-yi-mo-xing-9">定义模型</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DNNModel</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(DNNModel, self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self,input_shape)</span>:</span></span><br><span class="line">        self.dense1 = layers.Dense(<span class="number">4</span>,activation = <span class="string">"relu"</span>,name = <span class="string">"dense1"</span>) </span><br><span class="line">        self.dense2 = layers.Dense(<span class="number">8</span>,activation = <span class="string">"relu"</span>,name = <span class="string">"dense2"</span>)</span><br><span class="line">        self.dense3 = layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>,name = <span class="string">"dense3"</span>)</span><br><span class="line">        super(DNNModel,self).build(input_shape)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 正向传播</span></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [None,2], dtype = tf.float32)])  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x = self.dense1(x)</span><br><span class="line">        x = self.dense2(x)</span><br><span class="line">        y = self.dense3(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">model = DNNModel()</span><br><span class="line">model.build(input_shape =(<span class="literal">None</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">Model: "dnn_model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">dense1 (Dense)               multiple                  12        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense2 (Dense)               multiple                  40        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense3 (Dense)               multiple                  9         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 61</span><br><span class="line">Trainable params: 61</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<h4 id="xun-lian-mo-xing-8">训练模型</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">### 自定义训练循环</span></span><br><span class="line"></span><br><span class="line">optimizer = optimizers.Adam(learning_rate=<span class="number">0.01</span>)</span><br><span class="line">loss_func = tf.keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">train_loss = tf.keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_metric = tf.keras.metrics.BinaryAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">valid_loss = tf.keras.metrics.Mean(name=<span class="string">'valid_loss'</span>)</span><br><span class="line">valid_metric = tf.keras.metrics.BinaryAccuracy(name=<span class="string">'valid_accuracy'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = loss_func(labels, predictions)</span><br><span class="line">    grads = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(grads, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss.update_state(loss)</span><br><span class="line">    train_metric.update_state(labels, predictions)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">valid_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    predictions = model(features)</span><br><span class="line">    batch_loss = loss_func(labels, predictions)</span><br><span class="line">    valid_loss.update_state(batch_loss)</span><br><span class="line">    valid_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,ds_train,ds_valid,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_train:</span><br><span class="line">            train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_valid:</span><br><span class="line">            valid_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        logs = <span class="string">'Epoch=&#123;&#125;,Loss:&#123;&#125;,Accuracy:&#123;&#125;,Valid Loss:&#123;&#125;,Valid Accuracy:&#123;&#125;'</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>  epoch%<span class="number">100</span> ==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(tf.strings.format(logs,</span><br><span class="line">            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))</span><br><span class="line">        </span><br><span class="line">        train_loss.reset_states()</span><br><span class="line">        valid_loss.reset_states()</span><br><span class="line">        train_metric.reset_states()</span><br><span class="line">        valid_metric.reset_states()</span><br><span class="line"></span><br><span class="line">train_model(model,ds_train,ds_valid,<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">35</span>:<span class="number">02</span></span><br><span class="line">Epoch=<span class="number">100</span>,Loss:<span class="number">0.194088802</span>,Accuracy:<span class="number">0.923064</span>,Valid Loss:<span class="number">0.215538561</span>,Valid Accuracy:<span class="number">0.904368</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">35</span>:<span class="number">22</span></span><br><span class="line">Epoch=<span class="number">200</span>,Loss:<span class="number">0.151239693</span>,Accuracy:<span class="number">0.93768847</span>,Valid Loss:<span class="number">0.181166962</span>,Valid Accuracy:<span class="number">0.920664132</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">35</span>:<span class="number">43</span></span><br><span class="line">Epoch=<span class="number">300</span>,Loss:<span class="number">0.134556711</span>,Accuracy:<span class="number">0.944247484</span>,Valid Loss:<span class="number">0.171530813</span>,Valid Accuracy:<span class="number">0.926396072</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">36</span>:<span class="number">04</span></span><br><span class="line">Epoch=<span class="number">400</span>,Loss:<span class="number">0.125722557</span>,Accuracy:<span class="number">0.949172914</span>,Valid Loss:<span class="number">0.16731061</span>,Valid Accuracy:<span class="number">0.929318547</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">36</span>:<span class="number">24</span></span><br><span class="line">Epoch=<span class="number">500</span>,Loss:<span class="number">0.120216407</span>,Accuracy:<span class="number">0.952525079</span>,Valid Loss:<span class="number">0.164817035</span>,Valid Accuracy:<span class="number">0.931044817</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">36</span>:<span class="number">44</span></span><br><span class="line">Epoch=<span class="number">600</span>,Loss:<span class="number">0.116434008</span>,Accuracy:<span class="number">0.954830289</span>,Valid Loss:<span class="number">0.163089141</span>,Valid Accuracy:<span class="number">0.932202339</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">05</span></span><br><span class="line">Epoch=<span class="number">700</span>,Loss:<span class="number">0.113658346</span>,Accuracy:<span class="number">0.956433</span>,Valid Loss:<span class="number">0.161804497</span>,Valid Accuracy:<span class="number">0.933092058</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">25</span></span><br><span class="line">Epoch=<span class="number">800</span>,Loss:<span class="number">0.111522928</span>,Accuracy:<span class="number">0.957467675</span>,Valid Loss:<span class="number">0.160796657</span>,Valid Accuracy:<span class="number">0.93379426</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">46</span></span><br><span class="line">Epoch=<span class="number">900</span>,Loss:<span class="number">0.109816991</span>,Accuracy:<span class="number">0.958205402</span>,Valid Loss:<span class="number">0.159987748</span>,Valid Accuracy:<span class="number">0.934343576</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">38</span>:<span class="number">06</span></span><br><span class="line">Epoch=<span class="number">1000</span>,Loss:<span class="number">0.10841465</span>,Accuracy:<span class="number">0.958805501</span>,Valid Loss:<span class="number">0.159325734</span>,Valid Accuracy:<span class="number">0.934785843</span></span><br></pre></td></tr></table></figure>
<h4 id="jie-guo-ke-shi-hua-3">结果可视化</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig, (ax1,ax2) = plt.subplots(nrows=<span class="number">1</span>,ncols=<span class="number">2</span>,figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">ax1.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">ax1.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax1.set_title(<span class="string">"y_true"</span>);</span><br><span class="line"></span><br><span class="line">Xp_pred = tf.boolean_mask(X,tf.squeeze(model(X)&gt;=<span class="number">0.5</span>),axis = <span class="number">0</span>)</span><br><span class="line">Xn_pred = tf.boolean_mask(X,tf.squeeze(model(X)&lt;<span class="number">0.5</span>),axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2.scatter(Xp_pred[:,<span class="number">0</span>].numpy(),Xp_pred[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">ax2.scatter(Xn_pred[:,<span class="number">0</span>].numpy(),Xn_pred[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">ax2.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax2.set_title(<span class="string">"y_pred"</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/3-3-04-%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<h1 id="tensor-flow-de-di-jie-api">TensorFlow的低阶API</h1>
<p>TensorFlow的低阶API主要包括张量操作，计算图和自动微分。如果把模型比作一个房子，那么低阶API就是【模型之砖】。</p>
<p>在低阶API层次上，可以把TensorFlow当做一个增强版的numpy来使用。TensorFlow提供的方法比numpy更全面，运算速度更快，如果需要的话，还可以使用GPU进行加速。</p>
<p>前面几章对低阶API已经有了一个整体的认识，本章将重点详细介绍张量操作和Autograph计算图。</p>
<p>张量的操作主要包括张量的结构操作和张量的数学运算。</p>
<p>张量结构操作诸如：张量创建，索引切片，维度变换，合并分割。</p>
<p>张量数学运算主要有：标量运算，向量运算，矩阵运算，张量运算的广播机制。</p>
<p>Autograph计算图我们将介绍使用Autograph的规范建议，Autograph的机制原理，Autograph和tf.Module.</p>
<h2 id="zhang-liang-de-jie-gou-cao-zuo">张量的结构操作</h2>
<p>张量的操作主要包括张量的结构操作和张量的数学运算。</p>
<p>张量结构操作诸如：张量创建，索引切片，维度变换，合并分割。</p>
<p>张量数学运算主要有：标量运算，向量运算，矩阵运算。另外我们会介绍张量运算的广播机制。</p>
<p>本篇我们介绍张量的结构操作。</p>
<h3 id="chuang-jian-zhang-liang">创建张量</h3>
<p>张量创建的许多方法和numpy中创建array的方法很像。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],dtype = tf.float32)</span><br><span class="line">tf.print(a) <span class="comment"># [1 2 3]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b = tf.range(<span class="number">1</span>,<span class="number">10</span>,delta = <span class="number">2</span>)</span><br><span class="line">tf.print(b) <span class="comment"># [1 3 5 7 9]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c = tf.linspace(<span class="number">0.0</span>,<span class="number">2</span>*<span class="number">3.14</span>,<span class="number">100</span>)</span><br><span class="line">tf.print(c) <span class="comment"># [0 0.0634343475 0.126868695 ... 6.15313148 6.21656609 6.28]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = tf.zeros([<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line">tf.print(d)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.ones([<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line">b = tf.zeros_like(a,dtype= tf.float32)</span><br><span class="line">tf.print(a)</span><br><span class="line">tf.print(b)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]]</span><br><span class="line">[[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b = tf.fill([<span class="number">3</span>,<span class="number">2</span>],<span class="number">5</span>)</span><br><span class="line">tf.print(b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">5</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">5</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#均匀分布随机</span></span><br><span class="line">tf.random.set_seed(<span class="number">1.0</span>)</span><br><span class="line">a = tf.random.uniform([<span class="number">5</span>],minval=<span class="number">0</span>,maxval=<span class="number">10</span>)</span><br><span class="line">tf.print(a) <span class="comment"># [1.65130854 9.01481247 6.30974197 4.34546089 2.9193902]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#正态分布随机</span></span><br><span class="line">b = tf.random.normal([<span class="number">3</span>,<span class="number">3</span>],mean=<span class="number">0.0</span>,stddev=<span class="number">1.0</span>)</span><br><span class="line">tf.print(b)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">0.403087884</span> <span class="number">-1.0880208</span> <span class="number">-0.0630953535</span>]</span><br><span class="line"> [<span class="number">1.33655667</span> <span class="number">0.711760104</span> <span class="number">-0.489286453</span>]</span><br><span class="line"> [<span class="number">-0.764221311</span> <span class="number">-1.03724861</span> <span class="number">-1.25193381</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#正态分布随机，剔除2倍方差以外数据重新生成</span></span><br><span class="line">c = tf.random.truncated_normal((<span class="number">5</span>,<span class="number">5</span>), mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, dtype=tf.float32)</span><br><span class="line">tf.print(c)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">-0.457012236</span> <span class="number">-0.406867266</span> <span class="number">0.728577733</span> <span class="number">-0.892977774</span> <span class="number">-0.369404584</span>]</span><br><span class="line"> [<span class="number">0.323488563</span> <span class="number">1.19383323</span> <span class="number">0.888299048</span> <span class="number">1.25985599</span> <span class="number">-1.95951891</span>]</span><br><span class="line"> [<span class="number">-0.202244401</span> <span class="number">0.294496894</span> <span class="number">-0.468728036</span> <span class="number">1.29494202</span> <span class="number">1.48142183</span>]</span><br><span class="line"> [<span class="number">0.0810953453</span> <span class="number">1.63843894</span> <span class="number">0.556645</span> <span class="number">0.977199793</span> <span class="number">-1.17777884</span>]</span><br><span class="line"> [<span class="number">1.67368948</span> <span class="number">0.0647980496</span> <span class="number">-0.705142677</span> <span class="number">-0.281972528</span> <span class="number">0.126546144</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 特殊矩阵</span></span><br><span class="line">I = tf.eye(<span class="number">3</span>,<span class="number">3</span>) <span class="comment">#单位矩阵</span></span><br><span class="line">tf.print(I)</span><br><span class="line">tf.print(<span class="string">" "</span>)</span><br><span class="line">t = tf.linalg.diag([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) <span class="comment">#对角阵</span></span><br><span class="line">tf.print(t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">1</span>]]</span><br><span class="line"> </span><br><span class="line">[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">2</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">3</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="suo-yin-qie-pian">索引切片</h3>
<p>张量的索引切片方式和numpy几乎是一样的。切片时支持缺省参数和省略号。</p>
<p>对于tf.Variable,可以通过索引和切片对部分元素进行修改。</p>
<p>对于提取张量的连续子区域，也可以使用tf.slice.</p>
<p>此外，对于不规则的切片提取,可以使用tf.gather,tf.gather_nd,tf.boolean_mask。</p>
<p>tf.boolean_mask功能最为强大，它可以实现tf.gather,tf.gather_nd的功能，并且tf.boolean_mask还可以实现布尔索引。</p>
<p>如果要通过修改张量的某些元素得到新的张量，可以使用tf.where，tf.scatter_nd。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">3</span>)</span><br><span class="line">t = tf.random.uniform([<span class="number">5</span>,<span class="number">5</span>],minval=<span class="number">0</span>,maxval=<span class="number">10</span>,dtype=tf.int32)</span><br><span class="line">tf.print(t)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">4</span> <span class="number">7</span> <span class="number">4</span> <span class="number">2</span> <span class="number">9</span>]</span><br><span class="line"> [<span class="number">9</span> <span class="number">1</span> <span class="number">2</span> <span class="number">4</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">7</span> <span class="number">2</span> <span class="number">7</span> <span class="number">4</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">9</span> <span class="number">6</span> <span class="number">9</span> <span class="number">7</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">7</span> <span class="number">0</span> <span class="number">0</span> <span class="number">3</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#第0行</span></span><br><span class="line">tf.print(t[<span class="number">0</span>]) <span class="comment"># [4 7 4 2 9]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#倒数第一行</span></span><br><span class="line">tf.print(t[<span class="number">-1</span>]) <span class="comment"># [3 7 0 0 3]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#第1行第3列</span></span><br><span class="line">tf.print(t[<span class="number">1</span>,<span class="number">3</span>]) <span class="comment"># 4</span></span><br><span class="line">tf.print(t[<span class="number">1</span>][<span class="number">3</span>]) <span class="comment"># 4</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#第1行至第3行</span></span><br><span class="line">tf.print(t[<span class="number">1</span>:<span class="number">4</span>,:])</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">9</span> <span class="number">1</span> <span class="number">2</span> <span class="number">4</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">7</span> <span class="number">2</span> <span class="number">7</span> <span class="number">4</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">9</span> <span class="number">6</span> <span class="number">9</span> <span class="number">7</span> <span class="number">2</span>]]</span><br><span class="line"></span><br><span class="line">tf.print(tf.slice(t,[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">3</span>,<span class="number">5</span>])) <span class="comment">#tf.slice(input,begin_vector,size_vector)</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">9</span> <span class="number">1</span> <span class="number">2</span> <span class="number">4</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">7</span> <span class="number">2</span> <span class="number">7</span> <span class="number">4</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">9</span> <span class="number">6</span> <span class="number">9</span> <span class="number">7</span> <span class="number">2</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#第1行至最后一行，第0列到最后一列每隔两列取一列</span></span><br><span class="line">tf.print(t[<span class="number">1</span>:<span class="number">4</span>,:<span class="number">4</span>:<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">9</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">7</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">9</span> <span class="number">9</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#对变量来说，还可以使用索引和切片修改部分元素</span></span><br><span class="line">x = tf.Variable([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]],dtype = tf.float32)</span><br><span class="line">x[<span class="number">1</span>,:].assign(tf.constant([<span class="number">0.0</span>,<span class="number">0.0</span>]))</span><br><span class="line">tf.print(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.random.uniform([<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>],minval=<span class="number">0</span>,maxval=<span class="number">10</span>,dtype=tf.int32)</span><br><span class="line">tf.print(a)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[[<span class="number">7</span> <span class="number">3</span> <span class="number">9</span>]</span><br><span class="line">  [<span class="number">9</span> <span class="number">0</span> <span class="number">7</span>]</span><br><span class="line">  [<span class="number">9</span> <span class="number">6</span> <span class="number">7</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">3</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">8</span> <span class="number">1</span>]</span><br><span class="line">  [<span class="number">3</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">4</span> <span class="number">0</span> <span class="number">6</span>]</span><br><span class="line">  [<span class="number">6</span> <span class="number">2</span> <span class="number">2</span>]</span><br><span class="line">  [<span class="number">7</span> <span class="number">9</span> <span class="number">5</span>]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#省略号可以表示多个冒号</span></span><br><span class="line">tf.print(a[...,<span class="number">1</span>])</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">3</span> <span class="number">0</span> <span class="number">6</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">8</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">2</span> <span class="number">9</span>]]</span><br></pre></td></tr></table></figure>
<p>以上切片方式相对规则，对于不规则的切片提取,可以使用tf.gather,tf.gather_nd,tf.boolean_mask。</p>
<p>考虑班级成绩册的例子，有4个班级，每个班级10个学生，每个学生7门科目成绩。可以用一个4×10×7的张量来表示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scores = tf.random.uniform((<span class="number">4</span>,<span class="number">10</span>,<span class="number">7</span>),minval=<span class="number">0</span>,maxval=<span class="number">100</span>,dtype=tf.int32)</span><br><span class="line">tf.print(scores)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[[<span class="number">52</span> <span class="number">82</span> <span class="number">66</span> ... <span class="number">17</span> <span class="number">86</span> <span class="number">14</span>]</span><br><span class="line">  [<span class="number">8</span> <span class="number">36</span> <span class="number">94</span> ... <span class="number">13</span> <span class="number">78</span> <span class="number">41</span>]</span><br><span class="line">  [<span class="number">77</span> <span class="number">53</span> <span class="number">51</span> ... <span class="number">22</span> <span class="number">91</span> <span class="number">56</span>]</span><br><span class="line">  ...</span><br><span class="line">  [<span class="number">11</span> <span class="number">19</span> <span class="number">26</span> ... <span class="number">89</span> <span class="number">86</span> <span class="number">68</span>]</span><br><span class="line">  [<span class="number">60</span> <span class="number">72</span> <span class="number">0</span> ... <span class="number">11</span> <span class="number">26</span> <span class="number">15</span>]</span><br><span class="line">  [<span class="number">24</span> <span class="number">99</span> <span class="number">38</span> ... <span class="number">97</span> <span class="number">44</span> <span class="number">74</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">79</span> <span class="number">73</span> <span class="number">73</span> ... <span class="number">35</span> <span class="number">3</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">83</span> <span class="number">36</span> <span class="number">31</span> ... <span class="number">75</span> <span class="number">38</span> <span class="number">85</span>]</span><br><span class="line">  [<span class="number">54</span> <span class="number">26</span> <span class="number">67</span> ... <span class="number">60</span> <span class="number">68</span> <span class="number">98</span>]</span><br><span class="line">  ...</span><br><span class="line">  [<span class="number">20</span> <span class="number">5</span> <span class="number">18</span> ... <span class="number">32</span> <span class="number">45</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">72</span> <span class="number">52</span> <span class="number">81</span> ... <span class="number">88</span> <span class="number">41</span> <span class="number">20</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">21</span> <span class="number">89</span> ... <span class="number">53</span> <span class="number">10</span> <span class="number">90</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">52</span> <span class="number">80</span> <span class="number">22</span> ... <span class="number">29</span> <span class="number">25</span> <span class="number">60</span>]</span><br><span class="line">  [<span class="number">78</span> <span class="number">71</span> <span class="number">54</span> ... <span class="number">43</span> <span class="number">98</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">21</span> <span class="number">66</span> <span class="number">53</span> ... <span class="number">97</span> <span class="number">75</span> <span class="number">77</span>]</span><br><span class="line">  ...</span><br><span class="line">  [<span class="number">6</span> <span class="number">74</span> <span class="number">3</span> ... <span class="number">53</span> <span class="number">65</span> <span class="number">43</span>]</span><br><span class="line">  [<span class="number">98</span> <span class="number">36</span> <span class="number">72</span> ... <span class="number">33</span> <span class="number">36</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">61</span> <span class="number">78</span> <span class="number">70</span> ... <span class="number">7</span> <span class="number">59</span> <span class="number">21</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">56</span> <span class="number">57</span> <span class="number">45</span> ... <span class="number">23</span> <span class="number">15</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">35</span> <span class="number">8</span> <span class="number">82</span> ... <span class="number">11</span> <span class="number">59</span> <span class="number">97</span>]</span><br><span class="line">  [<span class="number">44</span> <span class="number">6</span> <span class="number">99</span> ... <span class="number">81</span> <span class="number">60</span> <span class="number">27</span>]</span><br><span class="line">  ...</span><br><span class="line">  [<span class="number">76</span> <span class="number">26</span> <span class="number">35</span> ... <span class="number">51</span> <span class="number">8</span> <span class="number">17</span>]</span><br><span class="line">  [<span class="number">33</span> <span class="number">52</span> <span class="number">53</span> ... <span class="number">78</span> <span class="number">37</span> <span class="number">31</span>]</span><br><span class="line">  [<span class="number">71</span> <span class="number">27</span> <span class="number">44</span> ... <span class="number">0</span> <span class="number">52</span> <span class="number">16</span>]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#抽取每个班级第0个学生，第5个学生，第9个学生的全部成绩</span></span><br><span class="line">p = tf.gather(scores,[<span class="number">0</span>,<span class="number">5</span>,<span class="number">9</span>],axis=<span class="number">1</span>)</span><br><span class="line">tf.print(p)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[[<span class="number">52</span> <span class="number">82</span> <span class="number">66</span> ... <span class="number">17</span> <span class="number">86</span> <span class="number">14</span>]</span><br><span class="line">  [<span class="number">24</span> <span class="number">80</span> <span class="number">70</span> ... <span class="number">72</span> <span class="number">63</span> <span class="number">96</span>]</span><br><span class="line">  [<span class="number">24</span> <span class="number">99</span> <span class="number">38</span> ... <span class="number">97</span> <span class="number">44</span> <span class="number">74</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">79</span> <span class="number">73</span> <span class="number">73</span> ... <span class="number">35</span> <span class="number">3</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">46</span> <span class="number">10</span> <span class="number">94</span> ... <span class="number">23</span> <span class="number">18</span> <span class="number">92</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">21</span> <span class="number">89</span> ... <span class="number">53</span> <span class="number">10</span> <span class="number">90</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">52</span> <span class="number">80</span> <span class="number">22</span> ... <span class="number">29</span> <span class="number">25</span> <span class="number">60</span>]</span><br><span class="line">  [<span class="number">19</span> <span class="number">12</span> <span class="number">23</span> ... <span class="number">87</span> <span class="number">86</span> <span class="number">25</span>]</span><br><span class="line">  [<span class="number">61</span> <span class="number">78</span> <span class="number">70</span> ... <span class="number">7</span> <span class="number">59</span> <span class="number">21</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">56</span> <span class="number">57</span> <span class="number">45</span> ... <span class="number">23</span> <span class="number">15</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">6</span> <span class="number">41</span> <span class="number">79</span> ... <span class="number">97</span> <span class="number">43</span> <span class="number">13</span>]</span><br><span class="line">  [<span class="number">71</span> <span class="number">27</span> <span class="number">44</span> ... <span class="number">0</span> <span class="number">52</span> <span class="number">16</span>]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#抽取每个班级第0个学生，第5个学生，第9个学生的第1门课程，第3门课程，第6门课程成绩</span></span><br><span class="line">q = tf.gather(tf.gather(scores,[<span class="number">0</span>,<span class="number">5</span>,<span class="number">9</span>],axis=<span class="number">1</span>),[<span class="number">1</span>,<span class="number">3</span>,<span class="number">6</span>],axis=<span class="number">2</span>)</span><br><span class="line">tf.print(q)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[[<span class="number">82</span> <span class="number">55</span> <span class="number">14</span>]</span><br><span class="line">  [<span class="number">80</span> <span class="number">46</span> <span class="number">96</span>]</span><br><span class="line">  [<span class="number">99</span> <span class="number">58</span> <span class="number">74</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">73</span> <span class="number">48</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">10</span> <span class="number">38</span> <span class="number">92</span>]</span><br><span class="line">  [<span class="number">21</span> <span class="number">86</span> <span class="number">90</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">80</span> <span class="number">57</span> <span class="number">60</span>]</span><br><span class="line">  [<span class="number">12</span> <span class="number">34</span> <span class="number">25</span>]</span><br><span class="line">  [<span class="number">78</span> <span class="number">71</span> <span class="number">21</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">57</span> <span class="number">75</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">41</span> <span class="number">47</span> <span class="number">13</span>]</span><br><span class="line">  [<span class="number">27</span> <span class="number">96</span> <span class="number">16</span>]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 抽取第0个班级第0个学生，第2个班级的第4个学生，第3个班级的第6个学生的全部成绩</span></span><br><span class="line"><span class="comment">#indices的长度为采样样本的个数，每个元素为采样位置的坐标</span></span><br><span class="line">s = tf.gather_nd(scores,indices = [(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">2</span>,<span class="number">4</span>),(<span class="number">3</span>,<span class="number">6</span>)])</span><br><span class="line">s</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">7</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">52</span>, <span class="number">82</span>, <span class="number">66</span>, <span class="number">55</span>, <span class="number">17</span>, <span class="number">86</span>, <span class="number">14</span>],</span><br><span class="line">       [<span class="number">99</span>, <span class="number">94</span>, <span class="number">46</span>, <span class="number">70</span>,  <span class="number">1</span>, <span class="number">63</span>, <span class="number">41</span>],</span><br><span class="line">       [<span class="number">46</span>, <span class="number">83</span>, <span class="number">70</span>, <span class="number">80</span>, <span class="number">90</span>, <span class="number">85</span>, <span class="number">17</span>]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<p>以上tf.gather和tf.gather_nd的功能也可以用tf.boolean_mask来实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#抽取每个班级第0个学生，第5个学生，第9个学生的全部成绩</span></span><br><span class="line">p = tf.boolean_mask(scores,[<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,</span><br><span class="line">                            <span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>],axis=<span class="number">1</span>)</span><br><span class="line">tf.print(p)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[[<span class="number">52</span> <span class="number">82</span> <span class="number">66</span> ... <span class="number">17</span> <span class="number">86</span> <span class="number">14</span>]</span><br><span class="line">  [<span class="number">24</span> <span class="number">80</span> <span class="number">70</span> ... <span class="number">72</span> <span class="number">63</span> <span class="number">96</span>]</span><br><span class="line">  [<span class="number">24</span> <span class="number">99</span> <span class="number">38</span> ... <span class="number">97</span> <span class="number">44</span> <span class="number">74</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">79</span> <span class="number">73</span> <span class="number">73</span> ... <span class="number">35</span> <span class="number">3</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">46</span> <span class="number">10</span> <span class="number">94</span> ... <span class="number">23</span> <span class="number">18</span> <span class="number">92</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">21</span> <span class="number">89</span> ... <span class="number">53</span> <span class="number">10</span> <span class="number">90</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">52</span> <span class="number">80</span> <span class="number">22</span> ... <span class="number">29</span> <span class="number">25</span> <span class="number">60</span>]</span><br><span class="line">  [<span class="number">19</span> <span class="number">12</span> <span class="number">23</span> ... <span class="number">87</span> <span class="number">86</span> <span class="number">25</span>]</span><br><span class="line">  [<span class="number">61</span> <span class="number">78</span> <span class="number">70</span> ... <span class="number">7</span> <span class="number">59</span> <span class="number">21</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">56</span> <span class="number">57</span> <span class="number">45</span> ... <span class="number">23</span> <span class="number">15</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">6</span> <span class="number">41</span> <span class="number">79</span> ... <span class="number">97</span> <span class="number">43</span> <span class="number">13</span>]</span><br><span class="line">  [<span class="number">71</span> <span class="number">27</span> <span class="number">44</span> ... <span class="number">0</span> <span class="number">52</span> <span class="number">16</span>]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#抽取第0个班级第0个学生，第2个班级的第4个学生，第3个班级的第6个学生的全部成绩</span></span><br><span class="line">s = tf.boolean_mask(scores,</span><br><span class="line">    [[<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>],</span><br><span class="line">     [<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>],</span><br><span class="line">     [<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>],</span><br><span class="line">     [<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>]])</span><br><span class="line">tf.print(s)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">52</span> <span class="number">82</span> <span class="number">66</span> ... <span class="number">17</span> <span class="number">86</span> <span class="number">14</span>]</span><br><span class="line"> [<span class="number">99</span> <span class="number">94</span> <span class="number">46</span> ... <span class="number">1</span> <span class="number">63</span> <span class="number">41</span>]</span><br><span class="line"> [<span class="number">46</span> <span class="number">83</span> <span class="number">70</span> ... <span class="number">90</span> <span class="number">85</span> <span class="number">17</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#利用tf.boolean_mask可以实现布尔索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#找到矩阵中小于0的元素</span></span><br><span class="line">c = tf.constant([[<span class="number">-1</span>,<span class="number">1</span>,<span class="number">-1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">-2</span>],[<span class="number">3</span>,<span class="number">-3</span>,<span class="number">3</span>]],dtype=tf.float32)</span><br><span class="line">tf.print(c,<span class="string">"\n"</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">-1</span> <span class="number">1</span> <span class="number">-1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">2</span> <span class="number">-2</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">-3</span> <span class="number">3</span>]] </span><br><span class="line"></span><br><span class="line">tf.print(tf.boolean_mask(c,c&lt;<span class="number">0</span>),<span class="string">"\n"</span>)  <span class="comment"># [-1 -1 -2 -3] </span></span><br><span class="line">tf.print(c[c&lt;<span class="number">0</span>]) <span class="comment">#布尔索引，为boolean_mask的语法糖形式  [-1 -1 -2 -3]</span></span><br></pre></td></tr></table></figure>
<p>以上这些方法仅能提取张量的部分元素值，但不能更改张量的部分元素值得到新的张量。如果要通过修改张量的部分元素值得到新的张量，可以使用tf.where和tf.scatter_nd。</p>
<p>tf.where可以理解为if的张量版本，此外它还可以用于找到满足条件的所有元素的位置坐标。</p>
<p>tf.scatter_nd的作用和tf.gather_nd有些相反，tf.gather_nd用于收集张量的给定位置的元素，而tf.scatter_nd可以将某些值插入到一个给定shape的全0的张量的指定位置处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#找到张量中小于0的元素,将其换成np.nan得到新的张量</span></span><br><span class="line"><span class="comment">#tf.where和np.where作用类似，可以理解为if的张量版本</span></span><br><span class="line"></span><br><span class="line">c = tf.constant([[<span class="number">-1</span>,<span class="number">1</span>,<span class="number">-1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">-2</span>],[<span class="number">3</span>,<span class="number">-3</span>,<span class="number">3</span>]],dtype=tf.float32)</span><br><span class="line">d = tf.where(c&lt;<span class="number">0</span>,tf.fill(c.shape,np.nan),c) </span><br><span class="line">d</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[nan,  <span class="number">1.</span>, nan],</span><br><span class="line">       [ <span class="number">2.</span>,  <span class="number">2.</span>, nan],</span><br><span class="line">       [ <span class="number">3.</span>, nan,  <span class="number">3.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#如果where只有一个参数，将返回所有满足条件的位置坐标</span></span><br><span class="line">indices = tf.where(c&lt;<span class="number">0</span>)</span><br><span class="line">indices</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: shape=(4, 2), dtype=int64, numpy=</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">1</span>]])&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#将张量的第[0,0]和[2,1]两个位置元素替换为0得到新的张量</span></span><br><span class="line">d = c - tf.scatter_nd([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">2</span>,<span class="number">1</span>]],[c[<span class="number">0</span>,<span class="number">0</span>],c[<span class="number">2</span>,<span class="number">1</span>]],c.shape)</span><br><span class="line">d</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">1.</span>, <span class="number">-1.</span>],</span><br><span class="line">       [ <span class="number">2.</span>,  <span class="number">2.</span>, <span class="number">-2.</span>],</span><br><span class="line">       [ <span class="number">3.</span>,  <span class="number">0.</span>,  <span class="number">3.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#scatter_nd的作用和gather_nd有些相反</span></span><br><span class="line"><span class="comment">#可以将某些值插入到一个给定shape的全0的张量的指定位置处。</span></span><br><span class="line">indices = tf.where(c&lt;<span class="number">0</span>)</span><br><span class="line">tf.scatter_nd(indices,tf.gather_nd(c,indices),c.shape)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=</span></span><br><span class="line">array([[<span class="number">-1.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-2.</span>],</span><br><span class="line">       [ <span class="number">0.</span>, <span class="number">-3.</span>,  <span class="number">0.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h3 id="wei-du-bian-huan">维度变换</h3>
<p>维度变换相关函数主要有 tf.reshape, tf.squeeze, tf.expand_dims, tf.transpose.</p>
<blockquote>
<p>tf.reshape 可以改变张量的形状。(tf.reshape可以改变张量的形状，但是其本质上不会改变张量元素的存储顺序，所以，该操作实际上非常迅速，并且是可逆的。)</p>
<p>tf.squeeze 可以减少维度。</p>
<p>tf.expand_dims 可以增加维度。</p>
<p>tf.transpose 可以交换维度。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.random.uniform(shape=[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">2</span>],minval=<span class="number">0</span>,maxval=<span class="number">255</span>,dtype=tf.int32)</span><br><span class="line">tf.print(a.shape) <span class="comment"># TensorShape([1, 3, 3, 2])</span></span><br><span class="line">tf.print(a)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[[[<span class="number">135</span> <span class="number">178</span>]</span><br><span class="line">   [<span class="number">26</span> <span class="number">116</span>]</span><br><span class="line">   [<span class="number">29</span> <span class="number">224</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">179</span> <span class="number">219</span>]</span><br><span class="line">   [<span class="number">153</span> <span class="number">209</span>]</span><br><span class="line">   [<span class="number">111</span> <span class="number">215</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">39</span> <span class="number">7</span>]</span><br><span class="line">   [<span class="number">138</span> <span class="number">129</span>]</span><br><span class="line">   [<span class="number">59</span> <span class="number">205</span>]]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 改成 （3,6）形状的张量</span></span><br><span class="line">b = tf.reshape(a,[<span class="number">3</span>,<span class="number">6</span>])</span><br><span class="line">tf.print(b.shape) <span class="comment"># TensorShape([3, 6])</span></span><br><span class="line">tf.print(b)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">135</span> <span class="number">178</span> <span class="number">26</span> <span class="number">116</span> <span class="number">29</span> <span class="number">224</span>]</span><br><span class="line"> [<span class="number">179</span> <span class="number">219</span> <span class="number">153</span> <span class="number">209</span> <span class="number">111</span> <span class="number">215</span>]</span><br><span class="line"> [<span class="number">39</span> <span class="number">7</span> <span class="number">138</span> <span class="number">129</span> <span class="number">59</span> <span class="number">205</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 改回成 [1,3,3,2] 形状的张量</span></span><br><span class="line">c = tf.reshape(b,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">2</span>])</span><br><span class="line">tf.print(c)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[[[<span class="number">135</span> <span class="number">178</span>]</span><br><span class="line">   [<span class="number">26</span> <span class="number">116</span>]</span><br><span class="line">   [<span class="number">29</span> <span class="number">224</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">179</span> <span class="number">219</span>]</span><br><span class="line">   [<span class="number">153</span> <span class="number">209</span>]</span><br><span class="line">   [<span class="number">111</span> <span class="number">215</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">39</span> <span class="number">7</span>]</span><br><span class="line">   [<span class="number">138</span> <span class="number">129</span>]</span><br><span class="line">   [<span class="number">59</span> <span class="number">205</span>]]]]</span><br></pre></td></tr></table></figure>
<p>如果张量在某个维度上只有一个元素，利用tf.squeeze可以消除这个维度。</p>
<p>和tf.reshape相似，它本质上不会改变张量元素的存储顺序。</p>
<p>张量的各个元素在内存中是线性存储的，其一般规律是，同一层级中的相邻元素的物理地址也相邻。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = tf.squeeze(a)</span><br><span class="line">tf.print(s.shape) <span class="comment"># TensorShape([3, 3, 2])</span></span><br><span class="line">tf.print(s)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[[<span class="number">135</span> <span class="number">178</span>]</span><br><span class="line">  [<span class="number">26</span> <span class="number">116</span>]</span><br><span class="line">  [<span class="number">29</span> <span class="number">224</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">179</span> <span class="number">219</span>]</span><br><span class="line">  [<span class="number">153</span> <span class="number">209</span>]</span><br><span class="line">  [<span class="number">111</span> <span class="number">215</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">39</span> <span class="number">7</span>]</span><br><span class="line">  [<span class="number">138</span> <span class="number">129</span>]</span><br><span class="line">  [<span class="number">59</span> <span class="number">205</span>]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = tf.expand_dims(s,axis=<span class="number">0</span>) <span class="comment">#在第0维插入长度为1的一个维度</span></span><br><span class="line">d</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[[[<span class="number">135</span>, <span class="number">178</span>],</span><br><span class="line">         [ <span class="number">26</span>, <span class="number">116</span>],</span><br><span class="line">         [ <span class="number">29</span>, <span class="number">224</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">179</span>, <span class="number">219</span>],</span><br><span class="line">         [<span class="number">153</span>, <span class="number">209</span>],</span><br><span class="line">         [<span class="number">111</span>, <span class="number">215</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">39</span>,   <span class="number">7</span>],</span><br><span class="line">         [<span class="number">138</span>, <span class="number">129</span>],</span><br><span class="line">         [ <span class="number">59</span>, <span class="number">205</span>]]]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<p>tf.transpose可以交换张量的维度，与tf.reshape不同，它会改变张量元素的存储顺序。tf.transpose常用于图片存储格式的变换上。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Batch,Height,Width,Channel</span></span><br><span class="line">a = tf.random.uniform(shape=[<span class="number">100</span>,<span class="number">600</span>,<span class="number">600</span>,<span class="number">4</span>],minval=<span class="number">0</span>,maxval=<span class="number">255</span>,dtype=tf.int32)</span><br><span class="line">tf.print(a.shape) <span class="comment"># TensorShape([100, 600, 600, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换成 Channel,Height,Width,Batch</span></span><br><span class="line">s= tf.transpose(a,perm=[<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>])</span><br><span class="line">tf.print(s.shape) <span class="comment"># TensorShape([4, 600, 600, 100])</span></span><br></pre></td></tr></table></figure>
<h3 id="he-bing-fen-ge">合并分割</h3>
<p>和numpy类似，可以用tf.concat和tf.stack方法对多个张量进行合并，可以用tf.split方法把一个张量分割成多个张量。</p>
<p>tf.concat和tf.stack有略微的区别，tf.concat是连接，不会增加维度，而tf.stack是堆叠，会增加维度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">5.0</span>,<span class="number">6.0</span>],[<span class="number">7.0</span>,<span class="number">8.0</span>]])</span><br><span class="line">c = tf.constant([[<span class="number">9.0</span>,<span class="number">10.0</span>],[<span class="number">11.0</span>,<span class="number">12.0</span>]])</span><br><span class="line"></span><br><span class="line">tf.concat([a,b,c],axis = <span class="number">0</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">6</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">       [ <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">       [ <span class="number">5.</span>,  <span class="number">6.</span>],</span><br><span class="line">       [ <span class="number">7.</span>,  <span class="number">8.</span>],</span><br><span class="line">       [ <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">       [<span class="number">11.</span>, <span class="number">12.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.concat([a,b,c],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">6</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">       [ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>, <span class="number">11.</span>, <span class="number">12.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.stack([a,b,c])</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[[ <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">3.</span>,  <span class="number">4.</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">5.</span>,  <span class="number">6.</span>],</span><br><span class="line">        [ <span class="number">7.</span>,  <span class="number">8.</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">        [<span class="number">11.</span>, <span class="number">12.</span>]]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.stack([a,b,c],axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[[ <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">5.</span>,  <span class="number">6.</span>],</span><br><span class="line">        [ <span class="number">9.</span>, <span class="number">10.</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">7.</span>,  <span class="number">8.</span>],</span><br><span class="line">        [<span class="number">11.</span>, <span class="number">12.</span>]]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">5.0</span>,<span class="number">6.0</span>],[<span class="number">7.0</span>,<span class="number">8.0</span>]])</span><br><span class="line">c = tf.constant([[<span class="number">9.0</span>,<span class="number">10.0</span>],[<span class="number">11.0</span>,<span class="number">12.0</span>]])</span><br><span class="line"></span><br><span class="line">c = tf.concat([a,b,c],axis = <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>tf.split是tf.concat的逆运算，可以指定分割份数平均分割，也可以通过指定每份的记录数量进行分割。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#tf.split(value,num_or_size_splits,axis)</span></span><br><span class="line">tf.split(c,<span class="number">3</span>,axis = <span class="number">0</span>)  <span class="comment">#指定分割份数，平均分割</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">4.</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">5.</span>, <span class="number">6.</span>],</span><br><span class="line">        [<span class="number">7.</span>, <span class="number">8.</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[ <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">        [<span class="number">11.</span>, <span class="number">12.</span>]], dtype=float32)&gt;]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.split(c,[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],axis = <span class="number">0</span>) <span class="comment">#指定每份的记录数量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">4.</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">5.</span>, <span class="number">6.</span>],</span><br><span class="line">        [<span class="number">7.</span>, <span class="number">8.</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[ <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">        [<span class="number">11.</span>, <span class="number">12.</span>]], dtype=float32)&gt;]</span><br></pre></td></tr></table></figure>
<h2 id="zhang-liang-de-shu-xue-yun-suan">张量的数学运算</h2>
<p>张量的操作主要包括张量的结构操作和张量的数学运算。</p>
<p>张量结构操作诸如：张量创建，索引切片，维度变换，合并分割。</p>
<p>张量数学运算主要有：标量运算，向量运算，矩阵运算。</p>
<p>张量运算的广播机制。</p>
<h3 id="biao-liang-yun-suan">标量运算</h3>
<p>张量的数学运算符可以分为标量运算符、向量运算符、以及矩阵运算符。</p>
<p>加减乘除乘方，以及三角函数，指数，对数等常见函数，逻辑比较运算符等都是标量运算符。</p>
<p>标量运算符的特点是对张量实施逐元素运算。</p>
<p>有些标量运算符对常用的数学运算符进行了重载。并且支持类似numpy的广播特性。</p>
<p>许多标量运算符都在 tf.math模块下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">-3</span>,<span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">5.0</span>,<span class="number">6</span>],[<span class="number">7.0</span>,<span class="number">8.0</span>]])</span><br><span class="line">a+b  <span class="comment">#运算符重载</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">6.</span>,  <span class="number">8.</span>],</span><br><span class="line">       [ <span class="number">4.</span>, <span class="number">12.</span>]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line">a-b </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">-4.</span>,  <span class="number">-4.</span>],</span><br><span class="line">       [<span class="number">-10.</span>,  <span class="number">-4.</span>]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line">a*b </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[  <span class="number">5.</span>,  <span class="number">12.</span>],</span><br><span class="line">       [<span class="number">-21.</span>,  <span class="number">32.</span>]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line">a/b</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">0.2</span>       ,  <span class="number">0.33333334</span>],</span><br><span class="line">       [<span class="number">-0.42857143</span>,  <span class="number">0.5</span>       ]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line">a**<span class="number">2</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">4.</span>],</span><br><span class="line">       [ <span class="number">9.</span>, <span class="number">16.</span>]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line">a**(<span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">1.</span>       , <span class="number">1.4142135</span>],</span><br><span class="line">       [      nan, <span class="number">2.</span>       ]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line">a%<span class="number">3</span> <span class="comment">#mod的运算符重载，等价于m = tf.math.mod(a,3)</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>], dtype=int32)&gt;</span><br><span class="line">    </span><br><span class="line">a//<span class="number">3</span>  <span class="comment">#地板除法</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">-1.</span>,  <span class="number">1.</span>]], dtype=float32)&gt;</span><br><span class="line">(a&gt;=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=bool, numpy=</span><br><span class="line">array([[<span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">       [<span class="literal">False</span>,  <span class="literal">True</span>]])&gt;</span><br><span class="line"></span><br><span class="line">(a&gt;=<span class="number">2</span>)&amp;(a&lt;=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=bool, numpy=</span><br><span class="line">array([[<span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">       [<span class="literal">False</span>, <span class="literal">False</span>]])&gt;</span><br><span class="line"></span><br><span class="line">(a&gt;=<span class="number">2</span>)|(a&lt;=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=bool, numpy=</span><br><span class="line">array([[ <span class="literal">True</span>,  <span class="literal">True</span>],</span><br><span class="line">       [ <span class="literal">True</span>,  <span class="literal">True</span>]])&gt;</span><br><span class="line"></span><br><span class="line">a==<span class="number">5</span> <span class="comment">#tf.equal(a,5)</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=bool, numpy=array([<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>])&gt;</span><br><span class="line">    </span><br><span class="line">tf.sqrt(a)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">1.</span>       , <span class="number">1.4142135</span>],</span><br><span class="line">       [      nan, <span class="number">2.</span>       ]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1.0</span>,<span class="number">8.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">5.0</span>,<span class="number">6.0</span>])</span><br><span class="line">c = tf.constant([<span class="number">6.0</span>,<span class="number">7.0</span>])</span><br><span class="line">tf.add_n([a,b,c]) <span class="comment"># &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([12., 21.], dtype=float32)&gt;</span></span><br><span class="line">tf.print(tf.maximum(a,b)) <span class="comment"># [5 8]</span></span><br><span class="line">tf.print(tf.minimum(a,b)) <span class="comment"># [1 6]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = tf.constant([<span class="number">2.6</span>,<span class="number">-2.7</span>])</span><br><span class="line"></span><br><span class="line">tf.print(tf.math.round(x)) <span class="comment">#保留整数部分，四舍五入 [3 -3]</span></span><br><span class="line">tf.print(tf.math.floor(x)) <span class="comment">#保留整数部分，向下归整 [2 -3]</span></span><br><span class="line">tf.print(tf.math.ceil(x))  <span class="comment">#保留整数部分，向上归整 [3 -2]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 幅值裁剪</span></span><br><span class="line">x = tf.constant([<span class="number">0.9</span>,<span class="number">-0.8</span>,<span class="number">100.0</span>,<span class="number">-20.0</span>,<span class="number">0.7</span>])</span><br><span class="line">y = tf.clip_by_value(x,clip_value_min=<span class="number">-1</span>,clip_value_max=<span class="number">1</span>)</span><br><span class="line">z = tf.clip_by_norm(x,clip_norm = <span class="number">3</span>)</span><br><span class="line">tf.print(y) <span class="comment"># [0.9 -0.8 1 -1 0.7]</span></span><br><span class="line">tf.print(z) <span class="comment"># [0.0264732055 -0.0235317405 2.94146752 -0.588293493 0.0205902718]</span></span><br></pre></td></tr></table></figure>
<h3 id="xiang-liang-yun-suan">向量运算</h3>
<p>向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。<br>
许多向量运算符都以reduce开头。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#向量reduce</span></span><br><span class="line">a = tf.range(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">tf.print(tf.reduce_sum(a)) <span class="comment"># 45</span></span><br><span class="line">tf.print(tf.reduce_mean(a)) <span class="comment"># 5</span></span><br><span class="line">tf.print(tf.reduce_max(a)) <span class="comment"># 9</span></span><br><span class="line">tf.print(tf.reduce_min(a)) <span class="comment"># 1</span></span><br><span class="line">tf.print(tf.reduce_prod(a)) <span class="comment"># 362880</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#张量指定维度进行reduce</span></span><br><span class="line">b = tf.reshape(a,(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">tf.print(tf.reduce_sum(b, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line">tf.print(tf.reduce_sum(b, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">6</span>]</span><br><span class="line"> [<span class="number">15</span>]</span><br><span class="line"> [<span class="number">24</span>]]</span><br><span class="line">[[<span class="number">12</span> <span class="number">15</span> <span class="number">18</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#bool类型的reduce</span></span><br><span class="line">p = tf.constant([<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>])</span><br><span class="line">q = tf.constant([<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>])</span><br><span class="line">tf.print(tf.reduce_all(p)) <span class="comment"># 0</span></span><br><span class="line">tf.print(tf.reduce_any(q)) <span class="comment"># 1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#利用tf.foldr实现tf.reduce_sum</span></span><br><span class="line">s = tf.foldr(<span class="keyword">lambda</span> a,b:a+b,tf.range(<span class="number">10</span>)) </span><br><span class="line">tf.print(s) <span class="comment"># 45</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#cum扫描累积</span></span><br><span class="line">a = tf.range(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">tf.print(tf.math.cumsum(a)) <span class="comment"># [1 3 6 ... 28 36 45]</span></span><br><span class="line">tf.print(tf.math.cumprod(a)) <span class="comment"># [1 2 6 ... 5040 40320 362880]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#arg最大最小值索引</span></span><br><span class="line">a = tf.range(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">tf.print(tf.argmax(a)) <span class="comment"># 8</span></span><br><span class="line">tf.print(tf.argmin(a)) <span class="comment"># 0</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#tf.math.top_k可以用于对张量排序</span></span><br><span class="line">a = tf.constant([<span class="number">1</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">values,indices = tf.math.top_k(a,<span class="number">3</span>,sorted=<span class="literal">True</span>)</span><br><span class="line">tf.print(values) <span class="comment"># [8 7 5]</span></span><br><span class="line">tf.print(indices) <span class="comment"># [5 2 3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#利用tf.math.top_k可以在TensorFlow中实现KNN算法</span></span><br></pre></td></tr></table></figure>
<h3 id="ju-zhen-yun-suan">矩阵运算</h3>
<p>矩阵必须是二维的。类似tf.constant([1,2,3])这样的不是矩阵。</p>
<p>矩阵运算包括：矩阵乘法，矩阵转置，矩阵逆，矩阵求迹，矩阵范数，矩阵行列式，矩阵求特征值，矩阵分解等运算。</p>
<p>除了一些常用的运算外，大部分和矩阵有关的运算都在tf.linalg子包中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#矩阵乘法</span></span><br><span class="line">a = tf.constant([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">2</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">2</span>]])</span><br><span class="line">a@b  <span class="comment">#等价于tf.matmul(a,b)</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">2</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">6</span>, <span class="number">8</span>]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#矩阵转置</span></span><br><span class="line">a = tf.constant([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">tf.transpose(a)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">4</span>]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#矩阵逆，必须为tf.float32或tf.double类型</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]],dtype = tf.float32)</span><br><span class="line">tf.linalg.inv(a)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">-2.0000002</span> ,  <span class="number">1.0000001</span> ],</span><br><span class="line">       [ <span class="number">1.5000001</span> , <span class="number">-0.50000006</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#矩阵求trace</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]],dtype = tf.float32)</span><br><span class="line">tf.linalg.trace(a) <span class="comment"># &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#矩阵求范数</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">tf.linalg.norm(a) <span class="comment"># &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.477226&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#矩阵行列式</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">tf.linalg.det(a) <span class="comment"># &lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#矩阵特征值</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">-5</span>,<span class="number">4</span>]])</span><br><span class="line">tf.linalg.eigvals(a) </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>,), dtype=complex64, numpy=array([<span class="number">2.4999995</span>+<span class="number">2.7838817j</span>, <span class="number">2.5</span>      <span class="number">-2.783882j</span> ], dtype=complex64)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#矩阵QR分解, 将一个方阵分解为一个正交矩阵q和上三角矩阵r</span></span><br><span class="line"><span class="comment">#QR分解实际上是对矩阵a实施Schmidt正交化得到q</span></span><br><span class="line"></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]],dtype = tf.float32)</span><br><span class="line">q,r = tf.linalg.qr(a)</span><br><span class="line">tf.print(q)</span><br><span class="line">tf.print(r)</span><br><span class="line">tf.print(q@r)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">-0.316227794</span> <span class="number">-0.948683321</span>]</span><br><span class="line"> [<span class="number">-0.948683321</span> <span class="number">0.316227734</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">-3.1622777</span> <span class="number">-4.4271884</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">-0.632455349</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1.00000012</span> <span class="number">1.99999976</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#矩阵svd分解</span></span><br><span class="line"><span class="comment">#svd分解可以将任意一个矩阵分解为一个正交矩阵u,一个对角阵s和一个正交矩阵v.t()的乘积</span></span><br><span class="line"><span class="comment">#svd常用于矩阵压缩和降维</span></span><br><span class="line"></span><br><span class="line">a  = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>],[<span class="number">5.0</span>,<span class="number">6.0</span>]], dtype = tf.float32)</span><br><span class="line">s,u,v = tf.linalg.svd(a)</span><br><span class="line">tf.print(u,<span class="string">"\n"</span>)</span><br><span class="line">tf.print(s,<span class="string">"\n"</span>)</span><br><span class="line">tf.print(v,<span class="string">"\n"</span>)</span><br><span class="line">tf.print(u@tf.linalg.diag(s)@tf.transpose(v))</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用svd分解可以在TensorFlow中实现主成分分析降维</span></span><br><span class="line">[[<span class="number">0.229847744</span> <span class="number">-0.88346082</span>]</span><br><span class="line"> [<span class="number">0.524744868</span> <span class="number">-0.240782902</span>]</span><br><span class="line"> [<span class="number">0.819642067</span> <span class="number">0.401896209</span>]] </span><br><span class="line"></span><br><span class="line">[<span class="number">9.52551842</span> <span class="number">0.51429987</span>] </span><br><span class="line"></span><br><span class="line">[[<span class="number">0.619629562</span> <span class="number">0.784894466</span>]</span><br><span class="line"> [<span class="number">0.784894466</span> <span class="number">-0.619629562</span>]] </span><br><span class="line"></span><br><span class="line">[[<span class="number">1.00000119</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">3.00000095</span> <span class="number">4.00000048</span>]</span><br><span class="line"> [<span class="number">5.00000143</span> <span class="number">6.00000095</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="yan-bo-ji-zhi">广播机制</h3>
<p>TensorFlow的广播规则和numpy是一样的:</p>
<ul>
<li>1、如果张量的维度不同，将维度较小的张量进行扩展，直到两个张量的维度都一样。</li>
<li>2、如果两个张量在某个维度上的长度是相同的，或者其中一个张量在该维度上的长度为1，那么我们就说这两个张量在该维度上是相容的。</li>
<li>3、如果两个张量在所有维度上都是相容的，它们就能使用广播。</li>
<li>4、广播之后，每个维度的长度将取两个张量在该维度长度的较大值。</li>
<li>5、在任何一个维度上，如果一个张量的长度为1，另一个张量长度大于1，那么在该维度上，就好像是对第一个张量进行了复制。</li>
</ul>
<p>tf.broadcast_to 以显式的方式按照广播机制扩展张量的维度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = tf.constant([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]])</span><br><span class="line">b + a  <span class="comment">#等价于 b + tf.broadcast_to(a,b.shape)</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.broadcast_to(a,b.shape)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#计算广播后计算结果的形状，静态形状，TensorShape类型参数</span></span><br><span class="line">tf.broadcast_static_shape(a.shape,b.shape) <span class="comment"># TensorShape([3, 3])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#计算广播后计算结果的形状，动态形状，Tensor类型参数</span></span><br><span class="line">c = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">d = tf.constant([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line">tf.broadcast_dynamic_shape(tf.shape(c),tf.shape(d)) <span class="comment"># &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 3], dtype=int32)&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#广播效果</span></span><br><span class="line">c+d <span class="comment">#等价于 tf.broadcast_to(c,[3,3]) + tf.broadcast_to(d,[3,3])</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="auto-graph-de-shi-yong-gui-fan">AutoGraph的使用规范</h2>
<p>有三种计算图的构建方式：</p>
<ol>
<li>静态计算图</li>
<li>动态计算图</li>
<li>Autograph</li>
</ol>
<p>TensorFlow 2.0主要使用的是动态计算图和Autograph。</p>
<p>动态计算图易于调试，编码效率较高，但执行效率偏低。</p>
<p>静态计算图执行效率很高，但较难调试。</p>
<p>而Autograph机制可以将动态图转换成静态计算图，兼收执行效率和编码效率之利。</p>
<p>当然Autograph机制能够转换的代码并不是没有任何约束的，有一些编码规范需要遵循，否则可能会转换失败或者不符合预期。</p>
<p>下面着重介绍Autograph的编码规范和Autograph转换成静态图的原理，并介绍使用tf.Module来更好地构建Autograph。</p>
<h3 id="autograph-bian-ma-gui-fan-zong-jie">Autograph编码规范总结</h3>
<ol>
<li>被@tf.function修饰的函数应尽可能使用TensorFlow中的函数而不是Python中的其他函数。例如使用tf.print而不是print，使用tf.range而不是range，使用tf.constant(True)而不是True.</li>
<li>避免在@tf.function修饰的函数内部定义tf.Variable.</li>
<li>被@tf.function修饰的函数不可修改该函数外部的Python列表或字典等数据结构变量。</li>
</ol>
<h3 id="autograph-bian-ma-gui-fan-jie-xi">Autograph编码规范解析</h3>
<h4 id="bei-tf-function-xiu-shi-de-han-shu-ying-jin-liang-shi-yong-tensor-flow-zhong-de-han-shu-er-bu-shi-python-zhong-de-qi-ta-han-shu">被@tf.function修饰的函数应尽量使用TensorFlow中的函数而不是Python中的其他函数。</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">np_random</span><span class="params">()</span>:</span></span><br><span class="line">    a = np.random.randn(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">    tf.print(a)</span><br><span class="line">    </span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf_random</span><span class="params">()</span>:</span></span><br><span class="line">    a = tf.random.normal((<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">    tf.print(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#np_random每次执行都是一样的结果。</span></span><br><span class="line">np_random()</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">array([[ <span class="number">0.22619201</span>, <span class="number">-0.4550123</span> , <span class="number">-0.42587565</span>],</span><br><span class="line">       [ <span class="number">0.05429906</span>,  <span class="number">0.2312667</span> , <span class="number">-1.44819738</span>],</span><br><span class="line">       [ <span class="number">0.36571796</span>,  <span class="number">1.45578986</span>, <span class="number">-1.05348983</span>]])</span><br><span class="line">np_random()</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">array([[ <span class="number">0.22619201</span>, <span class="number">-0.4550123</span> , <span class="number">-0.42587565</span>],</span><br><span class="line">       [ <span class="number">0.05429906</span>,  <span class="number">0.2312667</span> , <span class="number">-1.44819738</span>],</span><br><span class="line">       [ <span class="number">0.36571796</span>,  <span class="number">1.45578986</span>, <span class="number">-1.05348983</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#tf_random每次执行都会有重新生成随机数。</span></span><br><span class="line">tf_random()</span><br><span class="line"><span class="comment"># output：</span></span><br><span class="line">[[<span class="number">-1.38956189</span> <span class="number">-0.394843668</span> <span class="number">0.420657277</span>]</span><br><span class="line"> [<span class="number">2.87235498</span> <span class="number">-1.33740318</span> <span class="number">-0.533843279</span>]</span><br><span class="line"> [<span class="number">0.918233037</span> <span class="number">0.118598573</span> <span class="number">-0.399486482</span>]]</span><br><span class="line">tf_random()</span><br><span class="line"><span class="comment"># output：</span></span><br><span class="line">[[<span class="number">-0.858178258</span> <span class="number">1.67509317</span> <span class="number">0.511889517</span>]</span><br><span class="line"> [<span class="number">-0.545829177</span> <span class="number">-2.20118237</span> <span class="number">-0.968222201</span>]</span><br><span class="line"> [<span class="number">0.733958483</span> <span class="number">-0.61904633</span> <span class="number">0.77440238</span>]]</span><br></pre></td></tr></table></figure>
<h4 id="bi-mian-zai-tf-function-xiu-shi-de-han-shu-nei-bu-ding-yi-tf-variable">避免在@tf.function修饰的函数内部定义tf.Variable</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 避免在@tf.function修饰的函数内部定义tf.Variable.</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">1.0</span>,dtype=tf.float32)</span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outer_var</span><span class="params">()</span>:</span></span><br><span class="line">    x.assign_add(<span class="number">1.0</span>)</span><br><span class="line">    tf.print(x)</span><br><span class="line">    <span class="keyword">return</span>(x)</span><br><span class="line"></span><br><span class="line">outer_var() </span><br><span class="line">outer_var()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inner_var</span><span class="params">()</span>:</span></span><br><span class="line">    x = tf.Variable(<span class="number">1.0</span>,dtype = tf.float32)</span><br><span class="line">    x.assign_add(<span class="number">1.0</span>)</span><br><span class="line">    tf.print(x)</span><br><span class="line">    <span class="keyword">return</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行将报错</span></span><br><span class="line"><span class="comment">#inner_var()</span></span><br><span class="line"><span class="comment">#inner_var()</span></span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">ValueError                                Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-12</span>-c95a7c3c1ddd&gt; <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">      <span class="number">7</span> </span><br><span class="line">      <span class="number">8</span> <span class="comment">#执行将报错</span></span><br><span class="line">----&gt; 9 inner_var()</span><br><span class="line">     <span class="number">10</span> inner_var()</span><br><span class="line"></span><br><span class="line">~/anaconda3/lib/python3<span class="number">.7</span>/site-packages/tensorflow_core/python/eager/def_function.py <span class="keyword">in</span> __call__(self, *args, **kwds)</span><br><span class="line">    <span class="number">566</span>         xla_context.Exit()</span><br><span class="line">    <span class="number">567</span>     <span class="keyword">else</span>:</span><br><span class="line">--&gt; 568       result = self._call(*args, **kwds)</span><br><span class="line">    <span class="number">569</span> </span><br><span class="line">    <span class="number">570</span>     <span class="keyword">if</span> tracing_count == self._get_tracing_count():</span><br><span class="line">......</span><br><span class="line">ValueError: tf.function-decorated function tried to create variables on non-first call.</span><br></pre></td></tr></table></figure>
<h4 id="bei-tf-function-xiu-shi-de-han-shu-bu-ke-xiu-gai-gai-han-shu-wai-bu-de-python-lie-biao-huo-zi-dian-deng-jie-gou-lei-xing-bian-liang">被@tf.function修饰的函数不可修改该函数外部的Python列表或字典等结构类型变量。</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor_list = []</span><br><span class="line"></span><br><span class="line"><span class="comment">#@tf.function #加上这一行切换成Autograph结果将不符合预期！！！</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append_tensor</span><span class="params">(x)</span>:</span></span><br><span class="line">    tensor_list.append(x)</span><br><span class="line">    <span class="keyword">return</span> tensor_list</span><br><span class="line"></span><br><span class="line">append_tensor(tf.constant(<span class="number">5.0</span>))</span><br><span class="line">append_tensor(tf.constant(<span class="number">6.0</span>))</span><br><span class="line">print(tensor_list)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[&lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">5.0</span>&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">6.0</span>&gt;]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor_list = []</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function #加上这一行切换成Autograph结果将不符合预期！！！</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append_tensor</span><span class="params">(x)</span>:</span></span><br><span class="line">    tensor_list.append(x)</span><br><span class="line">    <span class="keyword">return</span> tensor_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">append_tensor(tf.constant(<span class="number">5.0</span>))</span><br><span class="line">append_tensor(tf.constant(<span class="number">6.0</span>))</span><br><span class="line">print(tensor_list)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[&lt;tf.Tensor <span class="string">'x:0'</span> shape=() dtype=float32&gt;]</span><br></pre></td></tr></table></figure>
<h2 id="auto-graph-de-ji-zhi-yuan-li">AutoGraph的机制原理</h2>
<h3 id="autograph-de-ji-zhi-yuan-li">Autograph的机制原理</h3>
<p><strong>当我们使用@tf.function装饰一个函数的时候，后面到底发生了什么呢？</strong></p>
<p>例如我们写下如下代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function(autograph=True)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myadd</span><span class="params">(a,b)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tf.range(<span class="number">3</span>):</span><br><span class="line">        tf.print(i)</span><br><span class="line">    c = a+b</span><br><span class="line">    print(<span class="string">"tracing"</span>)</span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure>
<p>后面什么都没有发生。仅仅是在Python堆栈中记录了这样一个函数的签名。</p>
<p><strong>当我们第一次调用这个被@tf.function装饰的函数时，后面到底发生了什么？</strong></p>
<p>例如我们写下如下代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myadd(tf.constant(<span class="string">"hello"</span>),tf.constant(<span class="string">"world"</span>))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tracing</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>发生了2件事情：</p>
<p>第一件事情是创建计算图，即创建一个静态计算图，跟踪执行一遍函数体中的Python代码，确定各个变量的Tensor类型，并根据执行顺序将算子添加到计算图中。在这个过程中，如果开启了autograph=True(默认开启),会将Python控制流转换成TensorFlow图内控制流。</p>
<p>主要是将if语句转换成 tf.cond算子表达，将while和for循环语句转换成tf.while_loop算子表达，并在必要的时候添加<br>
tf.control_dependencies指定执行顺序依赖关系。相当于在 tensorflow1.0执行了类似下面的语句：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    a = tf.placeholder(shape=[],dtype=tf.string)</span><br><span class="line">    b = tf.placeholder(shape=[],dtype=tf.string)</span><br><span class="line">    cond = <span class="keyword">lambda</span> i: i&lt;tf.constant(<span class="number">3</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(i)</span>:</span></span><br><span class="line">        tf.print(i)</span><br><span class="line">        <span class="keyword">return</span>(i+<span class="number">1</span>)</span><br><span class="line">    loop = tf.while_loop(cond,body,loop_vars=[<span class="number">0</span>])</span><br><span class="line">    loop</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies(loop):</span><br><span class="line">        c = tf.strings.join([a,b])</span><br><span class="line">    print(<span class="string">"tracing"</span>)</span><br></pre></td></tr></table></figure>
<p>第二件事情是执行计算图。相当于在 tensorflow1.0中执行了下面的语句：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session(graph=g) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(c,feed_dict=&#123;a:tf.constant(<span class="string">"hello"</span>),b:tf.constant(<span class="string">"world"</span>)&#125;)</span><br></pre></td></tr></table></figure>
<p>因此我们先看到的是第一个步骤的结果：即Python调用标准输出流打印&quot;tracing&quot;语句。然后看到第二个步骤的结果：TensorFlow调用标准输出流打印1,2,3。</p>
<p><strong>当我们再次用相同的输入参数类型调用这个被@tf.function装饰的函数时，后面到底发生了什么？</strong></p>
<p>例如我们写下如下代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myadd(tf.constant(<span class="string">"good"</span>),tf.constant(<span class="string">"morning"</span>))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>只会发生一件事情，那就是上面步骤的第二步，执行计算图。所以这一次我们没有看到打印&quot;tracing&quot;的结果。</p>
<p><strong>当我们再次用不同的的输入参数类型调用这个被@tf.function装饰的函数时，后面到底发生了什么？</strong></p>
<p>例如我们写下如下代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myadd(tf.constant(<span class="number">1</span>),tf.constant(<span class="number">2</span>))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tracing</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>由于输入参数的类型已经发生变化，已经创建的计算图不能够再次使用。需要重新做2件事情：创建新的计算图、执行计算图。</p>
<p>所以我们又会先看到的是第一个步骤的结果：即Python调用标准输出流打印&quot;tracing&quot;语句。然后再看到第二个步骤的结果：TensorFlow调用标准输出流打印1,2,3。</p>
<p><strong>需要注意的是，如果调用被@tf.function装饰的函数时输入的参数不是Tensor类型，则每次都会重新创建计算图。</strong></p>
<p>例如我们写下如下代码。两次都会重新创建计算图。因此，一般建议调用@tf.function时应传入Tensor类型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myadd(<span class="string">"hello"</span>,<span class="string">"world"</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tracing</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">myadd(<span class="string">"good"</span>,<span class="string">"morning"</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tracing</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>
<h3 id="zhong-xin-li-jie-autograph-de-bian-ma-gui-fan">重新理解Autograph的编码规范</h3>
<p>了解了以上Autograph的机制原理，我们也就能够理解Autograph编码规范的3条建议了。</p>
<ol>
<li>
<p>被@tf.function修饰的函数应尽量使用TensorFlow中的函数而不是Python中的其他函数。例如使用tf.print而不是print.</p>
<p>解释：Python中的函数仅仅会在跟踪执行函数以创建静态图的阶段使用，普通Python函数是无法嵌入到静态计算图中的，所以在计算图构建好之后再次调用的时候，这些Python函数并没有被计算，而TensorFlow中的函数则可以嵌入到计算图中。使用普通的Python函数会导致被@tf.function修饰前【eager执行】和被@tf.function修饰后【静态图执行】的输出不一致。</p>
</li>
<li>
<p>避免在@tf.function修饰的函数内部定义tf.Variable.</p>
<p>解释：如果函数内部定义了tf.Variable,那么在【eager执行】时，这种创建tf.Variable的行为在每次函数调用时候都会发生。但是在【静态图执行】时，这种创建tf.Variable的行为只会发生在第一步跟踪Python代码逻辑创建计算图时，这会导致被@tf.function修饰前【eager执行】和被@tf.function修饰后【静态图执行】的输出不一致。实际上，TensorFlow在这种情况下一般会报错。</p>
</li>
<li>
<p>被@tf.function修饰的函数不可修改该函数外部的Python列表或字典等数据结构变量。</p>
<p>解释：静态计算图是被编译成C++代码在TensorFlow内核中执行的。Python中的列表和字典等数据结构变量是无法嵌入到计算图中，它们仅仅能够在创建计算图时被读取，在执行计算图时是无法修改Python中的列表或字典这样的数据结构变量的。</p>
</li>
</ol>
<h2 id="auto-graph-he-tf-module">AutoGraph和tf.Module</h2>
<p>前面介绍了Autograph的编码规范和Autograph转换成静态图的原理。</p>
<p>本篇介绍使用tf.Module来更好地构建Autograph。</p>
<h3 id="autograph-he-tf-module-gai-shu">Autograph和tf.Module概述</h3>
<p>前面在介绍Autograph的编码规范时提到构建Autograph时应该避免在@tf.function修饰的函数内部定义tf.Variable.</p>
<p>但是如果在函数外部定义tf.Variable的话，又会显得这个函数有外部变量依赖，封装不够完美。</p>
<p>一种简单的思路是定义一个类，并将相关的tf.Variable创建放在类的初始化方法中。而将函数的逻辑放在其他方法中。</p>
<p>惊喜的是，TensorFlow提供了一个基类tf.Module，通过继承它构建子类，我们不仅可以获得以上的自然而然，而且可以非常方便地管理变量，还可以非常方便地管理它引用的其它Module，最重要的是，我们能够利用tf.saved_model保存模型并实现跨平台部署使用。</p>
<p>实际上，tf.keras.models.Model,tf.keras.layers.Layer 都是继承自tf.Module的，提供了方便的变量管理和所引用的子模块管理的功能。</p>
<p><strong>因此，利用tf.Module提供的封装，再结合TensoFlow丰富的低阶API，实际上我们能够基于TensorFlow开发任意机器学习模型(而非仅仅是神经网络模型)，并实现跨平台部署使用。</strong></p>
<h3 id="ying-yong-tf-module-feng-zhuang-autograph">应用tf.Module封装Autograph</h3>
<p>定义一个简单的function。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line">x = tf.Variable(<span class="number">1.0</span>,dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在tf.function中用input_signature限定输入张量的签名类型：shape和dtype</span></span><br><span class="line"><span class="meta">@tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])    </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_print</span><span class="params">(a)</span>:</span></span><br><span class="line">    x.assign_add(a)</span><br><span class="line">    tf.print(x)</span><br><span class="line">    <span class="keyword">return</span>(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_print(tf.constant(<span class="number">3.0</span>)) <span class="comment"># 4</span></span><br><span class="line"><span class="comment">#add_print(tf.constant(3)) #输入不符合张量签名的参数将报错</span></span><br></pre></td></tr></table></figure>
<p>下面利用tf.Module的子类化将其封装一下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DemoModule</span><span class="params">(tf.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,init_value = tf.constant<span class="params">(<span class="number">0.0</span>)</span>,name=None)</span>:</span></span><br><span class="line">        super(DemoModule, self).__init__(name=name)</span><br><span class="line">        <span class="keyword">with</span> self.name_scope:  <span class="comment">#相当于with tf.name_scope("demo_module")</span></span><br><span class="line">            self.x = tf.Variable(init_value,dtype = tf.float32,trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">     </span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addprint</span><span class="params">(self,a)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> self.name_scope:</span><br><span class="line">            self.x.assign_add(a)</span><br><span class="line">            tf.print(self.x)</span><br><span class="line">            <span class="keyword">return</span>(self.x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#执行</span></span><br><span class="line">demo = DemoModule(init_value = tf.constant(<span class="number">1.0</span>))</span><br><span class="line">result = demo.addprint(tf.constant(<span class="number">5.0</span>)) <span class="comment"># 6</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#查看模块中的全部变量和全部可训练变量</span></span><br><span class="line">print(demo.variables)</span><br><span class="line">print(demo.trainable_variables)</span><br></pre></td></tr></table></figure>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">(&lt;tf.Variable <span class="string">'demo_module/Variable:0'</span> shape=() <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=6.0&gt;,)</span><br><span class="line">(&lt;tf.Variable <span class="string">'demo_module/Variable:0'</span> shape=() <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=6.0&gt;,)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#查看模块中的全部子模块</span></span><br><span class="line">demo.submodules</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用tf.saved_model 保存模型，并指定需要跨平台部署的方法</span></span><br><span class="line">tf.saved_model.save(demo,<span class="string">"./data/demo/1"</span>,signatures = &#123;<span class="string">"serving_default"</span>:demo.addprint&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#加载模型</span></span><br><span class="line">demo2 = tf.saved_model.load(<span class="string">"./data/demo/1"</span>)</span><br><span class="line">demo2.addprint(tf.constant(<span class="number">5.0</span>)) <span class="comment"># 11</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看模型文件相关信息，红框标出来的输出信息在模型部署和跨平台使用时有可能会用到</span></span><br><span class="line">!saved_model_cli show --dir ./data/demo/<span class="number">1</span> --all</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/%E6%9F%A5%E7%9C%8B%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E4%BF%A1%E6%81%AF.jpg" alt></p>
<p>在tensorboard中查看计算图，模块会被添加模块名demo_module,方便层次化呈现计算图结构。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建日志</span></span><br><span class="line">stamp = datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">logdir = <span class="string">'./data/demomodule/%s'</span> % stamp</span><br><span class="line">writer = tf.summary.create_file_writer(logdir)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开启autograph跟踪</span></span><br><span class="line">tf.summary.trace_on(graph=<span class="literal">True</span>, profiler=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#执行autograph</span></span><br><span class="line">demo = DemoModule(init_value = tf.constant(<span class="number">0.0</span>))</span><br><span class="line">result = demo.addprint(tf.constant(<span class="number">5.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#将计算图信息写入日志</span></span><br><span class="line"><span class="keyword">with</span> writer.as_default():</span><br><span class="line">    tf.summary.trace_export(</span><br><span class="line">        name=<span class="string">"demomodule"</span>,</span><br><span class="line">        step=<span class="number">0</span>,</span><br><span class="line">        profiler_outdir=logdir)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#启动 tensorboard在jupyter中的魔法命令</span></span><br><span class="line">%reload_ext tensorboard</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorboard <span class="keyword">import</span> notebook</span><br><span class="line">notebook.list()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">notebook.start(<span class="string">"--logdir ./data/demomodule/"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/demomodule%E7%9A%84%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%BB%93%E6%9E%84.jpg" alt></p>
<p>除了利用tf.Module的子类化实现封装，我们也可以通过给tf.Module添加属性的方法进行封装。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mymodule = tf.Module()</span><br><span class="line">mymodule.x = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addprint</span><span class="params">(a)</span>:</span></span><br><span class="line">    mymodule.x.assign_add(a)</span><br><span class="line">    tf.print(mymodule.x)</span><br><span class="line">    <span class="keyword">return</span> (mymodule.x)</span><br><span class="line"></span><br><span class="line">mymodule.addprint = addprint</span><br><span class="line">mymodule.addprint(tf.constant(<span class="number">1.0</span>)).numpy() <span class="comment"># 1.0</span></span><br><span class="line">print(mymodule.variables) <span class="comment"># (&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0&gt;,)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用tf.saved_model 保存模型</span></span><br><span class="line">tf.saved_model.save(mymodule,<span class="string">"./data/mymodule"</span>,</span><br><span class="line">    signatures = &#123;<span class="string">"serving_default"</span>:mymodule.addprint&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载模型</span></span><br><span class="line">mymodule2 = tf.saved_model.load(<span class="string">"./data/mymodule"</span>) <span class="comment"># INFO:tensorflow:Assets written to: ./data/mymodule/assets</span></span><br><span class="line">mymodule2.addprint(tf.constant(<span class="number">5.0</span>)) <span class="comment"># 5</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-module-he-tf-keras-model-tf-keras-layers-layer">tf.Module和tf.keras.Model，tf.keras.layers.Layer</h3>
<p>tf.keras中的模型和层都是继承tf.Module实现的，也具有变量管理和子模块管理功能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,losses,metrics</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(issubclass(tf.keras.Model,tf.Module))</span><br><span class="line">print(issubclass(tf.keras.layers.Layer,tf.Module))</span><br><span class="line">print(issubclass(tf.keras.Model,tf.keras.layers.Layer))</span><br></pre></td></tr></table></figure>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session() </span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">4</span>,input_shape = (<span class="number">10</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">2</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">Model: <span class="string">"sequential"</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param <span class="comment">#   </span></span><br><span class="line">=================================================================</span><br><span class="line">dense (Dense)                (<span class="literal">None</span>, <span class="number">4</span>)                 <span class="number">44</span>        </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (<span class="literal">None</span>, <span class="number">2</span>)                 <span class="number">10</span>        </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_2 (Dense)              (<span class="literal">None</span>, <span class="number">1</span>)                 <span class="number">3</span>         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">57</span></span><br><span class="line">Trainable params: <span class="number">57</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.variables</span><br><span class="line"></span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">[&lt;tf.Variable <span class="string">'dense/kernel:0'</span> shape=(<span class="number">10</span>, <span class="number">4</span>) dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">-0.06741005</span>,  <span class="number">0.45534766</span>,  <span class="number">0.5190817</span> , <span class="number">-0.01806331</span>],</span><br><span class="line">        [<span class="number">-0.14258742</span>, <span class="number">-0.49711505</span>,  <span class="number">0.26030976</span>,  <span class="number">0.18607801</span>],</span><br><span class="line">        [<span class="number">-0.62806034</span>,  <span class="number">0.5327399</span> ,  <span class="number">0.42206633</span>,  <span class="number">0.29201728</span>],</span><br><span class="line">        [<span class="number">-0.16602087</span>, <span class="number">-0.18901917</span>,  <span class="number">0.55159235</span>, <span class="number">-0.01091868</span>],</span><br><span class="line">        [ <span class="number">0.04533798</span>,  <span class="number">0.326845</span>  , <span class="number">-0.582667</span>  ,  <span class="number">0.19431782</span>],</span><br><span class="line">        [ <span class="number">0.6494713</span> , <span class="number">-0.16174704</span>,  <span class="number">0.4062966</span> ,  <span class="number">0.48760796</span>],</span><br><span class="line">        [ <span class="number">0.58400524</span>, <span class="number">-0.6280886</span> , <span class="number">-0.11265379</span>, <span class="number">-0.6438277</span> ],</span><br><span class="line">        [ <span class="number">0.26642334</span>,  <span class="number">0.49275804</span>,  <span class="number">0.20793378</span>, <span class="number">-0.43889117</span>],</span><br><span class="line">        [ <span class="number">0.4092741</span> ,  <span class="number">0.09871006</span>, <span class="number">-0.2073121</span> ,  <span class="number">0.26047975</span>],</span><br><span class="line">        [ <span class="number">0.43910992</span>,  <span class="number">0.00199282</span>, <span class="number">-0.07711256</span>, <span class="number">-0.27966842</span>]],</span><br><span class="line">       dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable <span class="string">'dense/bias:0'</span> shape=(<span class="number">4</span>,) dtype=float32, numpy=array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable <span class="string">'dense_1/kernel:0'</span> shape=(<span class="number">4</span>, <span class="number">2</span>) dtype=float32, numpy=</span><br><span class="line"> array([[ <span class="number">0.5022683</span> , <span class="number">-0.0507431</span> ],</span><br><span class="line">        [<span class="number">-0.61540484</span>,  <span class="number">0.9369011</span> ],</span><br><span class="line">        [<span class="number">-0.14412141</span>, <span class="number">-0.54607415</span>],</span><br><span class="line">        [ <span class="number">0.2027781</span> , <span class="number">-0.4651153</span> ]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable <span class="string">'dense_1/bias:0'</span> shape=(<span class="number">2</span>,) dtype=float32, numpy=array([<span class="number">0.</span>, <span class="number">0.</span>], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable <span class="string">'dense_2/kernel:0'</span> shape=(<span class="number">2</span>, <span class="number">1</span>) dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">-0.244825</span> ],</span><br><span class="line">        [<span class="number">-1.2101456</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable <span class="string">'dense_2/bias:0'</span> shape=(<span class="number">1</span>,) dtype=float32, numpy=array([<span class="number">0.</span>], dtype=float32)&gt;]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.layers[<span class="number">0</span>].trainable = <span class="literal">False</span> <span class="comment">#冻结第0层的变量,使其不可训练</span></span><br><span class="line">model.trainable_variables</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[&lt;tf.Variable <span class="string">'dense_1/kernel:0'</span> shape=(<span class="number">4</span>, <span class="number">2</span>) dtype=float32, numpy=</span><br><span class="line"> array([[ <span class="number">0.5022683</span> , <span class="number">-0.0507431</span> ],</span><br><span class="line">        [<span class="number">-0.61540484</span>,  <span class="number">0.9369011</span> ],</span><br><span class="line">        [<span class="number">-0.14412141</span>, <span class="number">-0.54607415</span>],</span><br><span class="line">        [ <span class="number">0.2027781</span> , <span class="number">-0.4651153</span> ]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable <span class="string">'dense_1/bias:0'</span> shape=(<span class="number">2</span>,) dtype=float32, numpy=array([<span class="number">0.</span>, <span class="number">0.</span>], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable <span class="string">'dense_2/kernel:0'</span> shape=(<span class="number">2</span>, <span class="number">1</span>) dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">-0.244825</span> ],</span><br><span class="line">        [<span class="number">-1.2101456</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable <span class="string">'dense_2/bias:0'</span> shape=(<span class="number">1</span>,) dtype=float32, numpy=array([<span class="number">0.</span>], dtype=float32)&gt;]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.submodules</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">(&lt;tensorflow.python.keras.engine.input_layer.InputLayer at <span class="number">0x144d8c080</span>&gt;,</span><br><span class="line"> &lt;tensorflow.python.keras.layers.core.Dense at <span class="number">0x144daada0</span>&gt;,</span><br><span class="line"> &lt;tensorflow.python.keras.layers.core.Dense at <span class="number">0x144d8c5c0</span>&gt;,</span><br><span class="line"> &lt;tensorflow.python.keras.layers.core.Dense at <span class="number">0x144d7aa20</span>&gt;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.layers</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[&lt;tensorflow.python.keras.layers.core.Dense at <span class="number">0x144daada0</span>&gt;,</span><br><span class="line"> &lt;tensorflow.python.keras.layers.core.Dense at <span class="number">0x144d8c5c0</span>&gt;,</span><br><span class="line"> &lt;tensorflow.python.keras.layers.core.Dense at <span class="number">0x144d7aa20</span>&gt;]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(model.name) <span class="comment"># sequential</span></span><br><span class="line">print(model.name_scope()) <span class="comment"># sequential</span></span><br></pre></td></tr></table></figure>
<h1 id="tensor-flow-de-zhong-jie-api">TensorFlow的中阶API</h1>
<p>TensorFlow的中阶API主要包括:</p>
<ul>
<li>
<p>数据管道(tf.data)</p>
</li>
<li>
<p>特征列(tf.feature_column)</p>
</li>
<li>
<p>激活函数(tf.nn)</p>
</li>
<li>
<p>模型层(tf.keras.layers)</p>
</li>
<li>
<p>损失函数(tf.keras.losses)</p>
</li>
<li>
<p>评估函数(tf.keras.metrics)</p>
</li>
<li>
<p>优化器(tf.keras.optimizers)</p>
</li>
<li>
<p>回调函数(tf.keras.callbacks)</p>
</li>
</ul>
<p>如果把模型比作一个房子，那么中阶API就是【模型之墙】。</p>
<h2 id="shu-ju-guan-dao-dataset">数据管道Dataset</h2>
<p>如果需要训练的数据大小不大，例如不到1G，那么可以直接全部读入内存中进行训练，这样一般效率最高。</p>
<p>但如果需要训练的数据很大，例如超过10G，无法一次载入内存，那么通常需要在训练的过程中分批逐渐读入。</p>
<p>使用 tf.data API 可以构建数据输入管道，轻松处理大量的数据，不同的数据格式，以及不同的数据转换。</p>
<h3 id="gou-jian-shu-ju-guan-dao">构建数据管道</h3>
<p>可以从 Numpy array, Pandas DataFrame, Python generator, csv文件, 文本文件, 文件路径, tfrecords文件等方式构建数据管道。其中通过Numpy array, Pandas DataFrame, 文件路径构建数据管道是最常用的方法。</p>
<p>通过tfrecords文件方式构建数据管道较为复杂，需要对样本构建tf.Example后压缩成字符串写到tfrecords文件，读取后再解析成tf.Example。但tfrecords文件的优点是压缩后文件较小，便于网络传播，加载速度较快。</p>
<h4 id="cong-numpy-array-gou-jian-shu-ju-guan-dao">从Numpy array构建数据管道</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从Numpy array构建数据管道</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets </span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ds1 = tf.data.Dataset.from_tensor_slices((iris[<span class="string">"data"</span>],iris[<span class="string">"target"</span>]))</span><br><span class="line"><span class="keyword">for</span> features,label <span class="keyword">in</span> ds1.take(<span class="number">5</span>):</span><br><span class="line">    print(features,label)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor([<span class="number">5.1</span> <span class="number">3.5</span> <span class="number">1.4</span> <span class="number">0.2</span>], shape=(<span class="number">4</span>,), dtype=float64) tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">4.9</span> <span class="number">3.</span>  <span class="number">1.4</span> <span class="number">0.2</span>], shape=(<span class="number">4</span>,), dtype=float64) tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">4.7</span> <span class="number">3.2</span> <span class="number">1.3</span> <span class="number">0.2</span>], shape=(<span class="number">4</span>,), dtype=float64) tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">4.6</span> <span class="number">3.1</span> <span class="number">1.5</span> <span class="number">0.2</span>], shape=(<span class="number">4</span>,), dtype=float64) tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">5.</span>  <span class="number">3.6</span> <span class="number">1.4</span> <span class="number">0.2</span>], shape=(<span class="number">4</span>,), dtype=float64) tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br></pre></td></tr></table></figure>
<h4 id="cong-pandas-data-frame-gou-jian-shu-ju-guan-dao">从 Pandas DataFrame构建数据管道</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从 Pandas DataFrame构建数据管道</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">dfiris = pd.DataFrame(iris[<span class="string">"data"</span>],columns = iris.feature_names)</span><br><span class="line">ds2 = tf.data.Dataset.from_tensor_slices((dfiris.to_dict(<span class="string">"list"</span>),iris[<span class="string">"target"</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> features,label <span class="keyword">in</span> ds2.take(<span class="number">3</span>):</span><br><span class="line">    print(features,label)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">&#123;<span class="string">'sepal length (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">5.1</span>&gt;, <span class="string">'sepal width (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">3.5</span>&gt;, <span class="string">'petal length (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">1.4</span>&gt;, <span class="string">'petal width (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">0.2</span>&gt;&#125; tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">&#123;<span class="string">'sepal length (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">4.9</span>&gt;, <span class="string">'sepal width (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">3.0</span>&gt;, <span class="string">'petal length (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">1.4</span>&gt;, <span class="string">'petal width (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">0.2</span>&gt;&#125; tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">&#123;<span class="string">'sepal length (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">4.7</span>&gt;, <span class="string">'sepal width (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">3.2</span>&gt;, <span class="string">'petal length (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">1.3</span>&gt;, <span class="string">'petal width (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">0.2</span>&gt;&#125; tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br></pre></td></tr></table></figure>
<h4 id="cong-python-generator-gou-jian-shu-ju-guan-dao">从Python generator构建数据管道</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从Python generator构建数据管道</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个从文件中读取图片的generator</span></span><br><span class="line">image_generator = ImageDataGenerator(rescale=<span class="number">1.0</span>/<span class="number">255</span>).flow_from_directory(</span><br><span class="line">                    <span class="string">"./data/cifar2/test/"</span>,</span><br><span class="line">                    target_size=(<span class="number">32</span>, <span class="number">32</span>),</span><br><span class="line">                    batch_size=<span class="number">20</span>,</span><br><span class="line">                    class_mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">classdict = image_generator.class_indices</span><br><span class="line">print(classdict)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> features,label <span class="keyword">in</span> image_generator:</span><br><span class="line">        <span class="keyword">yield</span> (features,label)</span><br><span class="line"></span><br><span class="line">ds3 = tf.data.Dataset.from_generator(generator,output_types=(tf.float32,tf.int32))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">6</span>)) </span><br><span class="line"><span class="keyword">for</span> i,(img,label) <span class="keyword">in</span> enumerate(ds3.unbatch().take(<span class="number">9</span>)):</span><br><span class="line">    ax=plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    ax.imshow(img.numpy())</span><br><span class="line">    ax.set_title(<span class="string">"label = %d"</span>%label)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([]) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/5-1-cifar2%E9%A2%84%E8%A7%88.jpg" alt></p>
<h4 id="cong-csv-wen-jian-gou-jian-shu-ju-guan-dao">从csv文件构建数据管道</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从csv文件构建数据管道</span></span><br><span class="line">ds4 = tf.data.experimental.make_csv_dataset(</span><br><span class="line">      file_pattern = [<span class="string">"./data/titanic/train.csv"</span>,<span class="string">"./data/titanic/test.csv"</span>],</span><br><span class="line">      batch_size=<span class="number">3</span>, </span><br><span class="line">      label_name=<span class="string">"Survived"</span>,</span><br><span class="line">      na_value=<span class="string">""</span>,</span><br><span class="line">      num_epochs=<span class="number">1</span>,</span><br><span class="line">      ignore_errors=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data,label <span class="keyword">in</span> ds4.take(<span class="number">2</span>):</span><br><span class="line">    print(data,label)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">OrderedDict([(<span class="string">'PassengerId'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">540</span>,  <span class="number">58</span>, <span class="number">764</span>], dtype=int32)&gt;), (<span class="string">'Pclass'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>], dtype=int32)&gt;), (<span class="string">'Name'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=</span><br><span class="line">array([<span class="string">b'Frolicher, Miss. Hedwig Margaritha'</span>, <span class="string">b'Novel, Mr. Mansouer'</span>,</span><br><span class="line">       <span class="string">b'Carter, Mrs. William Ernest (Lucile Polk)'</span>], dtype=object)&gt;), (<span class="string">'Sex'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=array([<span class="string">b'female'</span>, <span class="string">b'male'</span>, <span class="string">b'female'</span>], dtype=object)&gt;), (<span class="string">'Age'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">22.</span> , <span class="number">28.5</span>, <span class="number">36.</span> ], dtype=float32)&gt;), (<span class="string">'SibSp'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], dtype=int32)&gt;), (<span class="string">'Parch'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>], dtype=int32)&gt;), (<span class="string">'Ticket'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=array([<span class="string">b'13568'</span>, <span class="string">b'2697'</span>, <span class="string">b'113760'</span>], dtype=object)&gt;), (<span class="string">'Fare'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=float32, numpy=array([ <span class="number">49.5</span>   ,   <span class="number">7.2292</span>, <span class="number">120.</span>    ], dtype=float32)&gt;), (<span class="string">'Cabin'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=array([<span class="string">b'B39'</span>, <span class="string">b''</span>, <span class="string">b'B96 B98'</span>], dtype=object)&gt;), (<span class="string">'Embarked'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=array([<span class="string">b'C'</span>, <span class="string">b'C'</span>, <span class="string">b'S'</span>], dtype=object)&gt;)]) tf.Tensor([<span class="number">1</span> <span class="number">0</span> <span class="number">1</span>], shape=(<span class="number">3</span>,), dtype=int32)</span><br><span class="line">OrderedDict([(<span class="string">'PassengerId'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">845</span>,  <span class="number">66</span>, <span class="number">390</span>], dtype=int32)&gt;), (<span class="string">'Pclass'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>], dtype=int32)&gt;), (<span class="string">'Name'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=</span><br><span class="line">array([<span class="string">b'Culumovic, Mr. Jeso'</span>, <span class="string">b'Moubarek, Master. Gerios'</span>,</span><br><span class="line">       <span class="string">b'Lehmann, Miss. Bertha'</span>], dtype=object)&gt;), (<span class="string">'Sex'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=array([<span class="string">b'male'</span>, <span class="string">b'male'</span>, <span class="string">b'female'</span>], dtype=object)&gt;), (<span class="string">'Age'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">17.</span>,  <span class="number">0.</span>, <span class="number">17.</span>], dtype=float32)&gt;), (<span class="string">'SibSp'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], dtype=int32)&gt;), (<span class="string">'Parch'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], dtype=int32)&gt;), (<span class="string">'Ticket'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=array([<span class="string">b'315090'</span>, <span class="string">b'2661'</span>, <span class="string">b'SC 1748'</span>], dtype=object)&gt;), (<span class="string">'Fare'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=float32, numpy=array([ <span class="number">8.6625</span>, <span class="number">15.2458</span>, <span class="number">12.</span>    ], dtype=float32)&gt;), (<span class="string">'Cabin'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=array([<span class="string">b''</span>, <span class="string">b''</span>, <span class="string">b''</span>], dtype=object)&gt;), (<span class="string">'Embarked'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=array([<span class="string">b'S'</span>, <span class="string">b'C'</span>, <span class="string">b'C'</span>], dtype=object)&gt;)]) tf.Tensor([<span class="number">0</span> <span class="number">1</span> <span class="number">1</span>], shape=(<span class="number">3</span>,), dtype=int32)</span><br></pre></td></tr></table></figure>
<h4 id="cong-wen-ben-wen-jian-gou-jian-shu-ju-guan-dao">从文本文件构建数据管道</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从文本文件构建数据管道</span></span><br><span class="line">ds5 = tf.data.TextLineDataset(</span><br><span class="line">    filenames = [<span class="string">"./data/titanic/train.csv"</span>,<span class="string">"./data/titanic/test.csv"</span>]</span><br><span class="line">    ).skip(<span class="number">1</span>) <span class="comment">#略去第一行header</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ds5.take(<span class="number">5</span>):</span><br><span class="line">    print(line)</span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">tf.Tensor(<span class="string">b'493,0,1,"Molson, Mr. Harry Markland",male,55.0,0,0,113787,30.5,C30,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'53,1,1,"Harper, Mrs. Henry Sleeper (Myna Haxtun)",female,49.0,1,0,PC 17572,76.7292,D33,C'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'388,1,2,"Buss, Miss. Kate",female,36.0,0,0,27849,13.0,,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'192,0,2,"Carbines, Mr. William",male,19.0,0,0,28424,13.0,,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'687,0,3,"Panula, Mr. Jaako Arnold",male,14.0,4,1,3101295,39.6875,,S'</span>, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<h4 id="cong-wen-jian-lu-jing-gou-jian-shu-ju-guan-dao">从文件路径构建数据管道</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ds6 = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/train/*/*.jpg"</span>)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> ds6.take(<span class="number">5</span>):</span><br><span class="line">    print(file)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="string">b'./data/cifar2/train/automobile/1263.jpg'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'./data/cifar2/train/airplane/2837.jpg'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'./data/cifar2/train/airplane/4264.jpg'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'./data/cifar2/train/automobile/4241.jpg'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'./data/cifar2/train/automobile/192.jpg'</span>, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(img_path,size = <span class="params">(<span class="number">32</span>,<span class="number">32</span>)</span>)</span>:</span></span><br><span class="line">    label = <span class="number">1</span> <span class="keyword">if</span> tf.strings.regex_full_match(img_path,<span class="string">".*/automobile/.*"</span>) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    img = tf.io.read_file(img_path)</span><br><span class="line">    img = tf.image.decode_jpeg(img) <span class="comment">#注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img,size)</span><br><span class="line">    <span class="keyword">return</span>(img,label)</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"><span class="keyword">for</span> i,(img,label) <span class="keyword">in</span> enumerate(ds6.map(load_image).take(<span class="number">2</span>)):</span><br><span class="line">    plt.figure(i)</span><br><span class="line">    plt.imshow((img/<span class="number">255.0</span>).numpy())</span><br><span class="line">    plt.title(<span class="string">"label = %d"</span>%label)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/5-1-car2.jpg" alt></p>
<h4 id="cong-tfrecords-wen-jian-gou-jian-shu-ju-guan-dao">从tfrecords文件构建数据管道</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># inpath：原始数据路径 outpath:TFRecord文件输出路径</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tfrecords</span><span class="params">(inpath,outpath)</span>:</span> </span><br><span class="line">    writer = tf.io.TFRecordWriter(outpath)</span><br><span class="line">    dirs = os.listdir(inpath)</span><br><span class="line">    <span class="keyword">for</span> index, name <span class="keyword">in</span> enumerate(dirs):</span><br><span class="line">        class_path = inpath +<span class="string">"/"</span>+ name+<span class="string">"/"</span></span><br><span class="line">        <span class="keyword">for</span> img_name <span class="keyword">in</span> os.listdir(class_path):</span><br><span class="line">            img_path = class_path + img_name</span><br><span class="line">            img = tf.io.read_file(img_path)</span><br><span class="line">            <span class="comment">#img = tf.image.decode_image(img)</span></span><br><span class="line">            <span class="comment">#img = tf.image.encode_jpeg(img) #统一成jpeg格式压缩</span></span><br><span class="line">            example = tf.train.Example(</span><br><span class="line">               features=tf.train.Features(feature=&#123;</span><br><span class="line">                    <span class="string">'label'</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),</span><br><span class="line">                    <span class="string">'img_raw'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.numpy()]))</span><br><span class="line">               &#125;))</span><br><span class="line">            writer.write(example.SerializeToString())</span><br><span class="line">    writer.close()</span><br><span class="line">    </span><br><span class="line">create_tfrecords(<span class="string">"./data/cifar2/test/"</span>,<span class="string">"./data/cifar2_test.tfrecords/"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_example</span><span class="params">(proto)</span>:</span></span><br><span class="line">    description =&#123; <span class="string">'img_raw'</span> : tf.io.FixedLenFeature([], tf.string),</span><br><span class="line">                   <span class="string">'label'</span>: tf.io.FixedLenFeature([], tf.int64)&#125; </span><br><span class="line">    example = tf.io.parse_single_example(proto, description)</span><br><span class="line">    img = tf.image.decode_jpeg(example[<span class="string">"img_raw"</span>])   <span class="comment">#注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img, (<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">    label = example[<span class="string">"label"</span>]</span><br><span class="line">    <span class="keyword">return</span>(img,label)</span><br><span class="line"></span><br><span class="line">ds7 = tf.data.TFRecordDataset(<span class="string">"./data/cifar2_test.tfrecords"</span>).map(parse_example).shuffle(<span class="number">3000</span>)</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">6</span>)) </span><br><span class="line"><span class="keyword">for</span> i,(img,label) <span class="keyword">in</span> enumerate(ds7.take(<span class="number">9</span>)):</span><br><span class="line">    ax=plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    ax.imshow((img/<span class="number">255.0</span>).numpy())</span><br><span class="line">    ax.set_title(<span class="string">"label = %d"</span>%label)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([]) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/5-1-car9.jpg" alt></p>
<h3 id="ying-yong-shu-ju-zhuan-huan">应用数据转换</h3>
<p>Dataset数据结构应用非常灵活，因为它本质上是一个Sequece序列，其每个元素可以是各种类型，例如可以是张量，列表，字典，也可以是Dataset。</p>
<p>Dataset包含了非常丰富的数据转换功能。</p>
<ul>
<li>map: 将转换函数映射到数据集每一个元素。</li>
<li>flat_map: 将转换函数映射到数据集的每一个元素，并将嵌套的Dataset压平。</li>
<li>interleave: 效果类似flat_map,但可以将不同来源的数据夹在一起。</li>
<li>filter: 过滤掉某些元素。</li>
<li>zip: 将两个长度相同的Dataset横向铰合。</li>
<li>concatenate: 将两个Dataset纵向连接。</li>
<li>reduce: 执行归并操作。</li>
<li>batch : 构建批次，每次放一个批次。比原始数据增加一个维度。 其逆操作为unbatch。</li>
<li>padded_batch: 构建批次，类似batch, 但可以填充到相同的形状。</li>
<li>window :构建滑动窗口，返回Dataset of Dataset.</li>
<li>shuffle: 数据顺序洗牌。</li>
<li>repeat: 重复数据若干次，不带参数时，重复无数次。</li>
<li>shard: 采样，从某个位置开始隔固定距离采样一个元素。</li>
<li>take: 采样，从开始位置取前几个元素。</li>
</ul>
<h4 id="map">map</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#map:将转换函数映射到数据集每一个元素</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="string">"hello world"</span>,<span class="string">"hello China"</span>,<span class="string">"hello Beijing"</span>])</span><br><span class="line">ds_map = ds.map(<span class="keyword">lambda</span> x:tf.strings.split(x,<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_map:</span><br><span class="line">    print(x)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">tf.Tensor([<span class="string">b'hello'</span> <span class="string">b'world'</span>], shape=(<span class="number">2</span>,), dtype=string)</span><br><span class="line">tf.Tensor([<span class="string">b'hello'</span> <span class="string">b'China'</span>], shape=(<span class="number">2</span>,), dtype=string)</span><br><span class="line">tf.Tensor([<span class="string">b'hello'</span> <span class="string">b'Beijing'</span>], shape=(<span class="number">2</span>,), dtype=string)</span><br></pre></td></tr></table></figure>
<h4 id="flat-map">flat_map</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#flat_map:将转换函数映射到数据集的每一个元素，并将嵌套的Dataset压平。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="string">"hello world"</span>,<span class="string">"hello China"</span>,<span class="string">"hello Beijing"</span>])</span><br><span class="line">ds_flatmap = ds.flat_map(<span class="keyword">lambda</span> x:tf.data.Dataset.from_tensor_slices(tf.strings.split(x,<span class="string">" "</span>)))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_flatmap:</span><br><span class="line">    print(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="string">b'hello'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'world'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'hello'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'China'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'hello'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'Beijing'</span>, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<h4 id="interleave">interleave</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># interleave: 效果类似flat_map,但可以将不同来源的数据夹在一起。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="string">"hello world"</span>,<span class="string">"hello China"</span>,<span class="string">"hello Beijing"</span>])</span><br><span class="line">ds_interleave = ds.interleave(<span class="keyword">lambda</span> x:tf.data.Dataset.from_tensor_slices(tf.strings.split(x,<span class="string">" "</span>)))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_interleave:</span><br><span class="line">    print(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">tf.Tensor(<span class="string">b'hello'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'hello'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'hello'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'world'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'China'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'Beijing'</span>, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<h4 id="filter">filter</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#filter:过滤掉某些元素。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="string">"hello world"</span>,<span class="string">"hello China"</span>,<span class="string">"hello Beijing"</span>])</span><br><span class="line"><span class="comment">#找出含有字母a或B的元素</span></span><br><span class="line">ds_filter = ds.filter(<span class="keyword">lambda</span> x: tf.strings.regex_full_match(x, <span class="string">".*[a|B].*"</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_filter:</span><br><span class="line">    print(x)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="string">b'hello China'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'hello Beijing'</span>, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<h4 id="zip">zip</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#zip:将两个长度相同的Dataset横向铰合。</span></span><br><span class="line"></span><br><span class="line">ds1 = tf.data.Dataset.range(<span class="number">0</span>,<span class="number">3</span>)</span><br><span class="line">ds2 = tf.data.Dataset.range(<span class="number">3</span>,<span class="number">6</span>)</span><br><span class="line">ds3 = tf.data.Dataset.range(<span class="number">6</span>,<span class="number">9</span>)</span><br><span class="line">ds_zip = tf.data.Dataset.zip((ds1,ds2,ds3))</span><br><span class="line"><span class="keyword">for</span> x,y,z <span class="keyword">in</span> ds_zip:</span><br><span class="line">    print(x.numpy(),y.numpy(),z.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="number">0</span> <span class="number">3</span> <span class="number">6</span></span><br><span class="line"><span class="number">1</span> <span class="number">4</span> <span class="number">7</span></span><br><span class="line"><span class="number">2</span> <span class="number">5</span> <span class="number">8</span></span><br></pre></td></tr></table></figure>
<h4 id="condatenate">condatenate</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#condatenate:将两个Dataset纵向连接。</span></span><br><span class="line"></span><br><span class="line">ds1 = tf.data.Dataset.range(<span class="number">0</span>,<span class="number">3</span>)</span><br><span class="line">ds2 = tf.data.Dataset.range(<span class="number">3</span>,<span class="number">6</span>)</span><br><span class="line">ds_concat = tf.data.Dataset.concatenate(ds1,ds2)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_concat:</span><br><span class="line">    print(x)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">1</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">2</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">3</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">4</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">5</span>, shape=(), dtype=int64)</span><br></pre></td></tr></table></figure>
<h4 id="reduce">reduce</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#reduce:执行归并操作。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5.0</span>])</span><br><span class="line">result = ds.reduce(<span class="number">0.0</span>,<span class="keyword">lambda</span> x,y:tf.add(x,y))</span><br><span class="line">result <span class="comment"># &lt;tf.Tensor: shape=(), dtype=float32, numpy=15.0&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="batch">batch</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#batch:构建批次，每次放一个批次。比原始数据增加一个维度。 其逆操作为unbatch。 </span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_batch = ds.batch(<span class="number">4</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_batch:</span><br><span class="line">    print(x)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor([<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span>], shape=(<span class="number">4</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span>], shape=(<span class="number">4</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([ <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span> <span class="number">11</span>], shape=(<span class="number">4</span>,), dtype=int64)</span><br></pre></td></tr></table></figure>
<h4 id="padden-batch">padden_batch</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#padded_batch:构建批次，类似batch, 但可以填充到相同的形状。</span></span><br><span class="line"></span><br><span class="line">elements = [[<span class="number">1</span>, <span class="number">2</span>],[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],[<span class="number">6</span>, <span class="number">7</span>],[<span class="number">8</span>]]</span><br><span class="line">ds = tf.data.Dataset.from_generator(<span class="keyword">lambda</span>: iter(elements), tf.int32)</span><br><span class="line"></span><br><span class="line">ds_padded_batch = ds.padded_batch(<span class="number">2</span>,padded_shapes = [<span class="number">4</span>,])</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_padded_batch:</span><br><span class="line">    print(x)    </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">0</span>]], shape=(<span class="number">2</span>, <span class="number">4</span>), dtype=int32)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">6</span> <span class="number">7</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">8</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]], shape=(<span class="number">2</span>, <span class="number">4</span>), dtype=int32)</span><br></pre></td></tr></table></figure>
<h4 id="window">window</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#window:构建滑动窗口，返回Dataset of Dataset.</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line"><span class="comment">#window返回的是Dataset of Dataset,可以用flat_map压平</span></span><br><span class="line">ds_window = ds.window(<span class="number">3</span>, shift=<span class="number">1</span>).flat_map(<span class="keyword">lambda</span> x: x.batch(<span class="number">3</span>,drop_remainder=<span class="literal">True</span>)) </span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_window:</span><br><span class="line">    print(x)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor([<span class="number">0</span> <span class="number">1</span> <span class="number">2</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">2</span> <span class="number">3</span> <span class="number">4</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">3</span> <span class="number">4</span> <span class="number">5</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">5</span> <span class="number">6</span> <span class="number">7</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">6</span> <span class="number">7</span> <span class="number">8</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">7</span> <span class="number">8</span> <span class="number">9</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([ <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([ <span class="number">9</span> <span class="number">10</span> <span class="number">11</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br></pre></td></tr></table></figure>
<h4 id="shuffle">shuffle</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#shuffle:数据顺序洗牌。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_shuffle = ds.shuffle(buffer_size = <span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_shuffle:</span><br><span class="line">    print(x)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="number">1</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">4</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">6</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">5</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">2</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">7</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">11</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">3</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">9</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">10</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">8</span>, shape=(), dtype=int64)</span><br></pre></td></tr></table></figure>
<h4 id="repeat">repeat</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#repeat:重复数据若干次，不带参数时，重复无数次。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">3</span>)</span><br><span class="line">ds_repeat = ds.repeat(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_repeat:</span><br><span class="line">    print(x)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">1</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">2</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">1</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">2</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">1</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">2</span>, shape=(), dtype=int64)</span><br></pre></td></tr></table></figure>
<h4 id="shard">shard</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#shard:采样，从某个位置开始隔固定距离采样一个元素。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_shard = ds.shard(<span class="number">3</span>,index = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_shard:</span><br><span class="line">    print(x)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="number">1</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">4</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">7</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">10</span>, shape=(), dtype=int64)</span><br></pre></td></tr></table></figure>
<h4 id="take">take</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#take:采样，从开始位置取前几个元素。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_take = ds.take(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">list(ds_take.as_numpy_iterator()) <span class="comment"># [0, 1, 2]</span></span><br></pre></td></tr></table></figure>
<h3 id="ti-sheng-guan-dao-xing-neng">提升管道性能</h3>
<p>训练深度学习模型常常会非常耗时。模型训练的耗时主要来自于两个部分，一部分来自<strong>数据准备</strong>，另一部分来自<strong>参数迭代</strong>。参数迭代过程的耗时通常依赖于GPU来提升。而数据准备过程的耗时则可以通过构建高效的数据管道进行提升。</p>
<p>以下是一些构建高效数据管道的建议:</p>
<ol>
<li>使用 prefetch 方法让数据准备和参数迭代两个过程相互并行。</li>
<li>使用 interleave 方法可以让数据读取过程多进程执行,并将不同来源数据夹在一起。</li>
<li>使用 map 时设置num_parallel_calls 让数据转换过程多进程执行。</li>
<li>使用 cache 方法让数据在第一个epoch后缓存到内存中，仅限于数据集不大情形。</li>
<li>使用 map转换时，先batch, 然后采用向量化的转换方法对每个batch进行转换。</li>
</ol>
<h4 id="prefetch-rang-shu-ju-zhun-bei-he-can-shu-die-dai-bing-xing">prefetch :让数据准备和参数迭代并行</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    ts = tf.timestamp()</span><br><span class="line">    today_ts = ts%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>,end = <span class="string">""</span>)</span><br><span class="line">    tf.print(timestring)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据准备和参数迭代两个过程默认情况下是串行的。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="comment">#假设每次准备数据需要2s</span></span><br><span class="line">        time.sleep(<span class="number">2</span>) </span><br><span class="line">        <span class="keyword">yield</span> i </span><br><span class="line">ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟参数迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#假设每一步训练需要1s</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练过程预计耗时 10*2+10*1 = 30s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training..."</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds:</span><br><span class="line">    train_step()  </span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用 prefetch 方法让数据准备和参数迭代两个过程相互并行。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程预计耗时 max(10*2,10*1) = 20s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training with prefetch..."</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.data.experimental.AUTOTUNE 可以让程序自动选择合适的参数</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds.prefetch(buffer_size = tf.data.experimental.AUTOTUNE):</span><br><span class="line">    train_step()  </span><br><span class="line">    </span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure>
<h4 id="interleave-shu-ju-du-qu-duo-jin-cheng-bing-jiang-bu-tong-lai-yuan-shu-ju-jia-zai-yi-qi">interleave :数据读取多进程,并将不同来源数据夹在一起</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ds_files = tf.data.Dataset.list_files(<span class="string">"./data/titanic/*.csv"</span>)</span><br><span class="line">ds = ds_files.flat_map(<span class="keyword">lambda</span> x:tf.data.TextLineDataset(x).skip(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ds.take(<span class="number">4</span>):</span><br><span class="line">    print(line)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="string">b'493,0,1,"Molson, Mr. Harry Markland",male,55.0,0,0,113787,30.5,C30,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'53,1,1,"Harper, Mrs. Henry Sleeper (Myna Haxtun)",female,49.0,1,0,PC 17572,76.7292,D33,C'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'388,1,2,"Buss, Miss. Kate",female,36.0,0,0,27849,13.0,,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'192,0,2,"Carbines, Mr. William",male,19.0,0,0,28424,13.0,,S'</span>, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ds_files = tf.data.Dataset.list_files(<span class="string">"./data/titanic/*.csv"</span>)</span><br><span class="line">ds = ds_files.interleave(<span class="keyword">lambda</span> x:tf.data.TextLineDataset(x).skip(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ds.take(<span class="number">8</span>):</span><br><span class="line">    print(line)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="string">b'181,0,3,"Sage, Miss. Constance Gladys",female,,8,2,CA. 2343,69.55,,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'493,0,1,"Molson, Mr. Harry Markland",male,55.0,0,0,113787,30.5,C30,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'405,0,3,"Oreskovic, Miss. Marija",female,20.0,0,0,315096,8.6625,,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'53,1,1,"Harper, Mrs. Henry Sleeper (Myna Haxtun)",female,49.0,1,0,PC 17572,76.7292,D33,C'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'635,0,3,"Skoog, Miss. Mabel",female,9.0,3,2,347088,27.9,,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'388,1,2,"Buss, Miss. Kate",female,36.0,0,0,27849,13.0,,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'701,1,1,"Astor, Mrs. John Jacob (Madeleine Talmadge Force)",female,18.0,1,0,PC 17757,227.525,C62 C64,C'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'192,0,2,"Carbines, Mr. William",male,19.0,0,0,28424,13.0,,S'</span>, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<h4 id="map-she-zhi-num-parallel-calls-rang-shu-ju-zhuan-huan-guo-cheng-duo-jin-xing-zhi-xing">map : 设置num_parallel_calls 让数据转换过程多进行执行</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ds = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/train/*/*.jpg"</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(img_path,size = <span class="params">(<span class="number">32</span>,<span class="number">32</span>)</span>)</span>:</span></span><br><span class="line">    label = <span class="number">1</span> <span class="keyword">if</span> tf.strings.regex_full_match(img_path,<span class="string">".*/automobile/.*"</span>) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    img = tf.io.read_file(img_path)</span><br><span class="line">    img = tf.image.decode_jpeg(img) <span class="comment">#注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img,size)</span><br><span class="line">    <span class="keyword">return</span>(img,label)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#单进程转换</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start transformation..."</span>))</span><br><span class="line"></span><br><span class="line">ds_map = ds.map(load_image)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> ds_map:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end transformation..."</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#多进程转换</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start parallel transformation..."</span>))</span><br><span class="line"></span><br><span class="line">ds_map_parallel = ds.map(load_image,num_parallel_calls = tf.data.experimental.AUTOTUNE)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> ds_map_parallel:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end parallel transformation..."</span>))</span><br></pre></td></tr></table></figure>
<h4 id="cache-rang-shu-ju-zai-di-yi-ge-epoch-hou-huan-cun-dao-nei-cun-zhong">cache : 让数据在第一个epoch后缓存到内存中</h4>
<p>仅限于数据集不大情形</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="comment">#假设每次准备数据需要2s</span></span><br><span class="line">        time.sleep(<span class="number">2</span>) </span><br><span class="line">        <span class="keyword">yield</span> i </span><br><span class="line">ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟参数迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#假设每一步训练需要0s</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程预计耗时 (5*2+5*0)*3 = 30s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training..."</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> ds:</span><br><span class="line">        train_step()  </span><br><span class="line">    printbar()</span><br><span class="line">    tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">" ended"</span>)</span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="comment">#假设每次准备数据需要2s</span></span><br><span class="line">        time.sleep(<span class="number">2</span>) </span><br><span class="line">        <span class="keyword">yield</span> i </span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 cache 方法让数据在第一个epoch后缓存到内存中，仅限于数据集不大情形。</span></span><br><span class="line">ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32)).cache()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟参数迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#假设每一步训练需要0s</span></span><br><span class="line">    time.sleep(<span class="number">0</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程预计耗时 (5*2+5*0)+(5*0+5*0)*2 = 10s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training..."</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> ds:</span><br><span class="line">        train_step()  </span><br><span class="line">    printbar()</span><br><span class="line">    tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">" ended"</span>)</span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure>
<h4 id="map-zhuan-huan-shi-xian-batch-ran-hou-cai-yong-xiang-liang-hua-de-zhuan-huan-fang-fa-dui-mei-ge-batch-jin-xing-zhuan-huan">map转换时，先batch, 然后采用向量化的转换方法对每个batch进行转换</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#先map后batch</span></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">100000</span>)</span><br><span class="line">ds_map_batch = ds.map(<span class="keyword">lambda</span> x:x**<span class="number">2</span>).batch(<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start scalar transformation..."</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_map_batch:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end scalar transformation..."</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#先batch后map</span></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">100000</span>)</span><br><span class="line">ds_batch_map = ds.batch(<span class="number">20</span>).map(<span class="keyword">lambda</span> x:x**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start vector transformation..."</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_batch_map:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end vector transformation..."</span>))</span><br></pre></td></tr></table></figure>
<h2 id="te-zheng-lie-feature-column">特征列feature_column</h2>
<p>特征列通常用于对结构化数据实施特征工程时候使用，图像或者文本数据一般不会用到特征列。</p>
<h3 id="te-zheng-lie-yong-fa-gai-shu">特征列用法概述</h3>
<p>使用特征列可以将类别特征转换为one-hot编码特征，将连续特征构建分桶特征，以及对多个特征生成交叉特征等等。</p>
<p>要创建特征列，请调用 tf.feature_column 模块的函数。该模块中常用的九个函数如下图所示，所有九个函数都会返回一个 Categorical-Column 或一个<br>
Dense-Column 对象，但却不会返回 bucketized_column，后者继承自这两个类。</p>
<blockquote>
<p>注意：所有的Catogorical Column类型最终都要通过indicator_column转换成Dense Column类型才能传入模型！</p>
</blockquote>
<p><img src="/2020/05/07/tensorflow2/%E7%89%B9%E5%BE%81%E5%88%979%E7%A7%8D.jpg" alt></p>
<ul>
<li>numeric_column 数值列，最常用。</li>
<li>bucketized_column 分桶列，由数值列生成，可以由一个数值列出多个特征，one-hot编码。</li>
<li>categorical_column_with_identity 分类标识列，one-hot编码，相当于分桶列每个桶为1个整数的情况。</li>
<li>categorical_column_with_vocabulary_list 分类词汇列，one-hot编码，由list指定词典。</li>
<li>categorical_column_with_vocabulary_file 分类词汇列，由文件file指定词典。</li>
<li>categorical_column_with_hash_bucket 哈希列，整数或词典较大时采用。</li>
<li>indicator_column 指标列，由Categorical Column生成，one-hot编码</li>
<li>embedding_column 嵌入列，由Categorical Column生成，嵌入矢量分布参数需要学习。嵌入矢量维数建议取类别数量的 4 次方根。</li>
<li>crossed_column 交叉列，可以由除categorical_column_with_hash_bucket的任意分类列构成。</li>
</ul>
<h3 id="te-zheng-lie-shi-yong-fan-li">特征列使用范例</h3>
<p>以下是一个使用特征列解决Titanic生存问题的完整范例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#打印日志</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printlog</span><span class="params">(info)</span>:</span></span><br><span class="line">    nowtime = datetime.datetime.now().strftime(<span class="string">'%Y-%m-%d %H:%M:%S'</span>)</span><br><span class="line">    print(<span class="string">"\n"</span>+<span class="string">"=========="</span>*<span class="number">8</span> + <span class="string">"%s"</span>%nowtime)</span><br><span class="line">    print(info+<span class="string">'...\n\n'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="gou-jian-shu-ju-guan-dao-1">构建数据管道</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line"><span class="comment"># 一，构建数据管道</span></span><br><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line">printlog(<span class="string">"step1: prepare dataset..."</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dftrain_raw = pd.read_csv(<span class="string">"./data/titanic/train.csv"</span>)</span><br><span class="line">dftest_raw = pd.read_csv(<span class="string">"./data/titanic/test.csv"</span>)</span><br><span class="line"></span><br><span class="line">dfraw = pd.concat([dftrain_raw,dftest_raw])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_dfdata</span><span class="params">(dfraw)</span>:</span></span><br><span class="line">    dfdata = dfraw.copy()</span><br><span class="line">    dfdata.columns = [x.lower() <span class="keyword">for</span> x <span class="keyword">in</span> dfdata.columns]</span><br><span class="line">    dfdata = dfdata.rename(columns=&#123;<span class="string">'survived'</span>:<span class="string">'label'</span>&#125;)</span><br><span class="line">    dfdata = dfdata.drop([<span class="string">'passengerid'</span>,<span class="string">'name'</span>],axis = <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> col,dtype <span class="keyword">in</span> dict(dfdata.dtypes).items():</span><br><span class="line">        <span class="comment"># 判断是否包含缺失值</span></span><br><span class="line">        <span class="keyword">if</span> dfdata[col].hasnans:</span><br><span class="line">            <span class="comment"># 添加标识是否缺失列</span></span><br><span class="line">            dfdata[col + <span class="string">'_nan'</span>] = pd.isna(dfdata[col]).astype(<span class="string">'int32'</span>)</span><br><span class="line">            <span class="comment"># 填充</span></span><br><span class="line">            <span class="keyword">if</span> dtype <span class="keyword">not</span> <span class="keyword">in</span> [np.object,np.str,np.unicode]:</span><br><span class="line">                dfdata[col].fillna(dfdata[col].mean(),inplace = <span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dfdata[col].fillna(<span class="string">''</span>,inplace = <span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span>(dfdata)</span><br><span class="line"></span><br><span class="line">dfdata = prepare_dfdata(dfraw)</span><br><span class="line">dftrain = dfdata.iloc[<span class="number">0</span>:len(dftrain_raw),:]</span><br><span class="line">dftest = dfdata.iloc[len(dftrain_raw):,:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从 dataframe 导入数据 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">df_to_dataset</span><span class="params">(df, shuffle=True, batch_size=<span class="number">32</span>)</span>:</span></span><br><span class="line">    dfdata = df.copy()</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'label'</span> <span class="keyword">not</span> <span class="keyword">in</span> dfdata.columns:</span><br><span class="line">        ds = tf.data.Dataset.from_tensor_slices(dfdata.to_dict(orient = <span class="string">'list'</span>))</span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        labels = dfdata.pop(<span class="string">'label'</span>)</span><br><span class="line">        ds = tf.data.Dataset.from_tensor_slices((dfdata.to_dict(orient = <span class="string">'list'</span>), labels))  </span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        ds = ds.shuffle(buffer_size=len(dfdata))</span><br><span class="line">    ds = ds.batch(batch_size)</span><br><span class="line">    <span class="keyword">return</span> ds</span><br><span class="line"></span><br><span class="line">ds_train = df_to_dataset(dftrain)</span><br><span class="line">ds_test = df_to_dataset(dftest)</span><br></pre></td></tr></table></figure>
<h4 id="ding-yi-te-zheng-lie">定义特征列</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line"><span class="comment"># 二，定义特征列</span></span><br><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line">printlog(<span class="string">"step2: make feature columns..."</span>)</span><br><span class="line"></span><br><span class="line">feature_columns = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数值列</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> [<span class="string">'age'</span>,<span class="string">'fare'</span>,<span class="string">'parch'</span>,<span class="string">'sibsp'</span>] + [</span><br><span class="line">    c <span class="keyword">for</span> c <span class="keyword">in</span> dfdata.columns <span class="keyword">if</span> c.endswith(<span class="string">'_nan'</span>)]:</span><br><span class="line">    feature_columns.append(tf.feature_column.numeric_column(col))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分桶列</span></span><br><span class="line">age = tf.feature_column.numeric_column(<span class="string">'age'</span>)</span><br><span class="line">age_buckets = tf.feature_column.bucketized_column(age, </span><br><span class="line">             boundaries=[<span class="number">18</span>, <span class="number">25</span>, <span class="number">30</span>, <span class="number">35</span>, <span class="number">40</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">55</span>, <span class="number">60</span>, <span class="number">65</span>])</span><br><span class="line">feature_columns.append(age_buckets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类别列</span></span><br><span class="line"><span class="comment"># 注意：所有的Catogorical Column类型最终都要通过indicator_column转换成Dense Column类型才能传入模型！！</span></span><br><span class="line">sex = tf.feature_column.indicator_column(</span><br><span class="line">      tf.feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">      key=<span class="string">'sex'</span>,vocabulary_list=[<span class="string">"male"</span>, <span class="string">"female"</span>]))</span><br><span class="line">feature_columns.append(sex)</span><br><span class="line"></span><br><span class="line">pclass = tf.feature_column.indicator_column(</span><br><span class="line">      tf.feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">      key=<span class="string">'pclass'</span>,vocabulary_list=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]))</span><br><span class="line">feature_columns.append(pclass)</span><br><span class="line"></span><br><span class="line">ticket = tf.feature_column.indicator_column(</span><br><span class="line">     tf.feature_column.categorical_column_with_hash_bucket(<span class="string">'ticket'</span>,<span class="number">3</span>))</span><br><span class="line">feature_columns.append(ticket)</span><br><span class="line"></span><br><span class="line">embarked = tf.feature_column.indicator_column(</span><br><span class="line">      tf.feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">      key=<span class="string">'embarked'</span>,vocabulary_list=[<span class="string">'S'</span>,<span class="string">'C'</span>,<span class="string">'B'</span>]))</span><br><span class="line">feature_columns.append(embarked)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 嵌入列</span></span><br><span class="line">cabin = tf.feature_column.embedding_column(</span><br><span class="line">    tf.feature_column.categorical_column_with_hash_bucket(<span class="string">'cabin'</span>,<span class="number">32</span>),<span class="number">2</span>)</span><br><span class="line">feature_columns.append(cabin)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉列</span></span><br><span class="line">pclass_cate = tf.feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">          key=<span class="string">'pclass'</span>,vocabulary_list=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">crossed_feature = tf.feature_column.indicator_column(</span><br><span class="line">    tf.feature_column.crossed_column([age_buckets, pclass_cate],hash_bucket_size=<span class="number">15</span>))</span><br><span class="line"></span><br><span class="line">feature_columns.append(crossed_feature)</span><br></pre></td></tr></table></figure>
<h4 id="ding-yi-mo-xing-10">定义模型</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line"><span class="comment"># 三，定义模型</span></span><br><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line">printlog(<span class="string">"step3: define model..."</span>)</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">  layers.DenseFeatures(feature_columns), <span class="comment">#将特征列放入到tf.keras.layers.DenseFeatures中!!!</span></span><br><span class="line">  layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">  layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">  layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h4 id="xun-lian-mo-xing-9">训练模型</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line"><span class="comment"># 四，训练模型</span></span><br><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line">printlog(<span class="string">"step4: train model..."</span>)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(ds_train,</span><br><span class="line">          validation_data=ds_test,</span><br><span class="line">          epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h4 id="ping-gu-mo-xing-4">评估模型</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line"><span class="comment"># 五，评估模型</span></span><br><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line">printlog(<span class="string">"step5: eval model..."</span>)</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">'val_'</span>+metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span>+ metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span>+metric, <span class="string">'val_'</span>+metric])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_metric(history,<span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">dense_features (DenseFeature multiple                  64        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                multiple                  3008      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_1 (Dense)              multiple                  4160      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_2 (Dense)              multiple                  65        </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 7,297</span><br><span class="line">Trainable params: 7,297</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/5-2-01-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0.jpg" alt></p>
<h2 id="ji-huo-han-shu-activation">激活函数activation</h2>
<p>激活函数在深度学习中扮演着非常重要的角色，它给网络赋予了非线性，从而使得神经网络能够拟合任意复杂的函数。</p>
<p>如果没有激活函数，无论多复杂的网络，都等价于单一的线性变换，无法对非线性函数进行拟合。</p>
<p>目前，深度学习中最流行的激活函数为 relu, 但也有些新推出的激活函数，如 swish、GELU 据称效果优于relu激活函数。</p>
<h3 id="chang-yong-ji-huo-han-shu">常用激活函数</h3>
<h4 id="tf-nn-sigmoid">tf.nn.sigmoid</h4>
<p>将实数压缩到0到1之间，一般只在二分类的最后输出层使用。主要缺陷为存在梯度消失问题，计算复杂度高，输出不以0为中心。</p>
<p><img src="/2020/05/07/tensorflow2/sigmoid.png" alt></p>
<h4 id="tf-nn-softmax">tf.nn.softmax</h4>
<p>sigmoid的多分类扩展，一般只在多分类问题的最后输出层使用。</p>
<p><img src="/2020/05/07/tensorflow2/softmax%E8%AF%B4%E6%98%8E.jpg" alt></p>
<h4 id="tf-nn-tanh">tf.nn.tanh</h4>
<p>将实数压缩到-1到1之间，输出期望为0。主要缺陷为存在梯度消失问题，计算复杂度高。</p>
<p><img src="/2020/05/07/tensorflow2/tanh.png" alt></p>
<h4 id="tf-nn-relu">tf.nn.relu</h4>
<p>修正线性单元，最流行的激活函数。一般隐藏层使用。主要缺陷是：输出不以0为中心，输入小于0时存在梯度消失问题(死亡relu)。</p>
<p><img src="/2020/05/07/tensorflow2/relu.png" alt></p>
<h4 id="tf-nn-leaky-relu">tf.nn.leaky_relu</h4>
<p>对修正线性单元的改进，解决了死亡relu问题。</p>
<p><img src="/2020/05/07/tensorflow2/leaky_relu.png" alt></p>
<h4 id="tf-nn-elu">tf.nn.elu</h4>
<p>指数线性单元。对relu的改进，能够缓解死亡relu问题。</p>
<p><img src="/2020/05/07/tensorflow2/elu.png" alt></p>
<h4 id="tf-nn-selu">tf.nn.selu</h4>
<p>扩展型指数线性单元。在权重用tf.keras.initializers.lecun_normal初始化前提下能够对神经网络进行自归一化。不可能出现梯度爆炸或者梯度消失问题。需要和Dropout的变种AlphaDropout一起使用。</p>
<p><img src="/2020/05/07/tensorflow2/selu.png" alt></p>
<h4 id="tf-nn-swish">tf.nn.swish</h4>
<p>自门控激活函数。谷歌出品，相关研究指出用swish替代relu将获得轻微效果提升。</p>
<p><img src="/2020/05/07/tensorflow2/swish.png" alt></p>
<h4 id="gelu">gelu</h4>
<p>高斯误差线性单元激活函数。在Transformer中表现最好。tf.nn模块尚没有实现该函数。</p>
<p><img src="/2020/05/07/tensorflow2/gelu.png" alt></p>
<h3 id="zai-mo-xing-zhong-shi-yong-ji-huo-han-shu">在模型中使用激活函数</h3>
<p>在keras模型中使用激活函数一般有两种方式：</p>
<ol>
<li>一种是作为某些层的activation参数指定</li>
<li>另一种是显式添加layers.Activation激活层</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">32</span>,input_shape = (<span class="literal">None</span>,<span class="number">16</span>),activation = tf.nn.relu)) <span class="comment">#通过activation参数指定</span></span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>))</span><br><span class="line">model.add(layers.Activation(tf.nn.softmax))  <span class="comment"># 显式添加layers.Activation激活层</span></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<h2 id="mo-xing-ceng-layers">模型层layers</h2>
<p>深度学习模型一般由各种模型层组合而成。</p>
<p>tf.keras.layers内置了非常丰富的各种功能的模型层。例如，</p>
<ul>
<li>layers.Dense,layers.Flatten</li>
<li>layers.Input</li>
<li>layers.DenseFeature</li>
<li>layers.Dropout</li>
<li>layers.Conv2D</li>
<li>layers.MaxPooling2D</li>
<li>layers.Conv1D</li>
<li>layers.Embedding</li>
<li>layers.GRU</li>
<li>layers.LSTM</li>
<li>layers.Bidirectional</li>
<li>…</li>
</ul>
<p>如果这些内置模型层不能够满足需求，我们也可以通过编写tf.keras.Lambda匿名模型层或继承tf.keras.layers.Layer基类构建自定义的模型层。其中tf.keras.Lambda匿名模型层只适用于构造没有学习参数的模型层。</p>
<h3 id="nei-zhi-mo-xing-ceng">内置模型层</h3>
<p>一些常用的内置模型层简单介绍如下。</p>
<h4 id="ji-chu-ceng">基础层</h4>
<ul>
<li>
<p>Dense：密集连接层。参数个数 = 输入层特征数× 输出层特征数(weight)＋ 输出层特征数(bias)</p>
</li>
<li>
<p>Activation：激活函数层。一般放在Dense层后面，等价于在Dense层中指定activation。</p>
</li>
<li>
<p>Dropout：随机置零层。训练期间以一定几率将输入置0，一种正则化手段。</p>
</li>
<li>
<p>BatchNormalization：批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。</p>
</li>
<li>
<p>SpatialDropout2D：空间随机置零层。训练期间以一定几率将整个特征图置0，一种正则化手段，有利于避免特征图之间过高的相关性。</p>
</li>
<li>
<p>Input：输入层。通常使用Functional API方式构建模型时作为第一层。</p>
</li>
<li>
<p>DenseFeature：特征列接入层，用于接收一个特征列列表并产生一个密集连接层。</p>
</li>
<li>
<p>Flatten：压平层，用于将多维张量压成一维。</p>
</li>
<li>
<p>Reshape：形状重塑层，改变输入张量的形状。</p>
</li>
<li>
<p>Concatenate：拼接层，将多个张量在某个维度上拼接。</p>
</li>
<li>
<p>Add：加法层。</p>
</li>
<li>
<p>Subtract： 减法层。</p>
</li>
<li>
<p>Maximum：取最大值层。</p>
</li>
<li>
<p>Minimum：取最小值层。</p>
</li>
</ul>
<h4 id="juan-ji-wang-luo-xiang-guan-ceng">卷积网络相关层</h4>
<ul>
<li>
<p>Conv1D：普通一维卷积，常用于文本。参数个数 = 输入通道数×卷积核尺寸(如3)×卷积核个数</p>
</li>
<li>
<p>Conv2D：普通二维卷积，常用于图像。参数个数 = 输入通道数×卷积核尺寸(如3乘3)×卷积核个数</p>
</li>
<li>
<p>Conv3D：普通三维卷积，常用于视频。参数个数 = 输入通道数×卷积核尺寸(如3乘3乘3)×卷积核个数</p>
</li>
<li>
<p>SeparableConv2D：二维深度可分离卷积层。不同于普通卷积同时对区域和通道操作，深度可分离卷积先操作区域，再操作通道。即先对每个通道做独立卷积操作区域，再用1乘1卷积跨通道组合操作通道。参数个数 = 输入通道数×卷积核尺寸 + 输入通道数×1×1×输出通道数。深度可分离卷积的参数数量一般远小于普通卷积，效果一般也更好。</p>
</li>
<li>
<p>DepthwiseConv2D：二维深度卷积层。仅有SeparableConv2D前半部分操作，即只操作区域，不操作通道，一般输出通道数和输入通道数相同，但也可以通过设置depth_multiplier让输出通道为输入通道的若干倍数。输出通道数 = 输入通道数 × depth_multiplier。参数个数 = 输入通道数×卷积核尺寸× depth_multiplier。</p>
</li>
<li>
<p>Conv2DTranspose：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。</p>
</li>
<li>
<p>LocallyConnected2D: 二维局部连接层。类似Conv2D，唯一的差别是没有空间上的权值共享，所以其参数个数远高于二维卷积。</p>
</li>
<li>
<p>MaxPool2D: 二维最大池化层。也称作下采样层。池化层无可训练参数，主要作用是降维。</p>
</li>
<li>
<p>AveragePooling2D: 二维平均池化层。</p>
</li>
<li>
<p>GlobalMaxPool2D: 全局最大池化层。每个通道仅保留一个值。一般从卷积层过渡到全连接层时使用，是Flatten的替代方案。</p>
</li>
<li>
<p>GlobalAvgPool2D: 全局平均池化层。每个通道仅保留一个值。</p>
</li>
</ul>
<h4 id="xun-huan-wang-luo-xiang-guan-ceng">循环网络相关层</h4>
<ul>
<li>
<p>Embedding：嵌入层。一种比Onehot更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。</p>
</li>
<li>
<p>LSTM：长短记忆循环网络层。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置return_sequences = True时可以返回各个中间步骤输出，否则只返回最终输出。</p>
</li>
<li>
<p>GRU：门控循环网络层。LSTM的低配版，不具有携带轨道，参数数量少于LSTM，训练速度更快。</p>
</li>
<li>
<p>SimpleRNN：简单循环网络层。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用。</p>
</li>
<li>
<p>ConvLSTM2D：卷积长短记忆循环网络层。结构上类似LSTM，但对输入的转换操作和对状态的转换操作都是卷积运算。</p>
</li>
<li>
<p>Bidirectional：双向循环网络包装器。可以将LSTM，GRU等层包装成双向循环网络。从而增强特征提取能力。</p>
</li>
<li>
<p>RNN：RNN基本层。接受一个循环网络单元或一个循环单元列表，通过调用tf.keras.backend.rnn函数在序列上进行迭代从而转换成循环网络层。</p>
</li>
<li>
<p>LSTMCell：LSTM单元。和LSTM在整个序列上迭代相比，它仅在序列上迭代一步。可以简单理解LSTM即RNN基本层包裹LSTMCell。</p>
</li>
<li>
<p>GRUCell：GRU单元。和GRU在整个序列上迭代相比，它仅在序列上迭代一步。</p>
</li>
<li>
<p>SimpleRNNCell：SimpleRNN单元。和SimpleRNN在整个序列上迭代相比，它仅在序列上迭代一步。</p>
</li>
<li>
<p>AbstractRNNCell：抽象RNN单元。通过对它的子类化用户可以自定义RNN单元，再通过RNN基本层的包裹实现用户自定义循环网络层。</p>
</li>
<li>
<p>Attention：Dot-product类型注意力机制层。可以用于构建注意力模型。</p>
</li>
<li>
<p>AdditiveAttention：Additive类型注意力机制层。可以用于构建注意力模型。</p>
</li>
<li>
<p>TimeDistributed：时间分布包装器。包装后可以将Dense、Conv2D等作用到每一个时间片段上。</p>
</li>
</ul>
<h3 id="zi-ding-yi-mo-xing-ceng">自定义模型层</h3>
<p>如果自定义模型层没有需要被训练的参数，一般推荐使用Lamda层实现。如果自定义模型层有需要被训练的参数，则可以通过对Layer基类子类化实现。</p>
<p>Lambda层由于没有需要被训练的参数，只需要定义正向传播逻辑即可，使用比Layer基类子类化更加简单。Lambda层的正向逻辑可以使用Python的lambda函数来表达，也可以用def关键字定义函数来表达。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models,regularizers</span><br><span class="line"></span><br><span class="line">mypower = layers.Lambda(<span class="keyword">lambda</span> x:tf.math.pow(x,<span class="number">2</span>))</span><br><span class="line">mypower(tf.range(<span class="number">5</span>)) <span class="comment"># &lt;tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 0,  1,  4,  9, 16], dtype=int32)&gt;</span></span><br></pre></td></tr></table></figure>
<p>Layer的子类化一般需要重新实现初始化方法，Build方法和Call方法。下面是一个简化的线性层的范例，类似Dense.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units=<span class="number">32</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Linear, self).__init__(**kwargs)</span><br><span class="line">        self.units = units</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#build方法一般定义Layer需要被训练的参数。    </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span> </span><br><span class="line">        self.w = self.add_weight(<span class="string">"w"</span>,shape=(input_shape[<span class="number">-1</span>], self.units),</span><br><span class="line">                                 initializer=<span class="string">'random_normal'</span>,</span><br><span class="line">                                 trainable=<span class="literal">True</span>) <span class="comment">#注意必须要有参数名称"w",否则会报错</span></span><br><span class="line">        self.b = self.add_weight(<span class="string">"b"</span>,shape=(self.units,),</span><br><span class="line">                                 initializer=<span class="string">'random_normal'</span>,</span><br><span class="line">                                 trainable=<span class="literal">True</span>)</span><br><span class="line">        super(Linear,self).build(input_shape) <span class="comment"># 相当于设置self.built = True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#call方法一般定义正向传播运算逻辑，__call__方法调用了它。  </span></span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span> </span><br><span class="line">        <span class="keyword">return</span> tf.matmul(inputs, self.w) + self.b</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#如果要让自定义的Layer通过Functional API 组合成模型时可以被保存成h5模型，需要自定义get_config方法。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        config = super(Linear, self).get_config()</span><br><span class="line">        config.update(&#123;<span class="string">'units'</span>: self.units&#125;)</span><br><span class="line">        <span class="keyword">return</span> config</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">linear = Linear(units = <span class="number">8</span>)</span><br><span class="line">print(linear.built) <span class="comment"># False</span></span><br><span class="line"><span class="comment">#指定input_shape，显式调用build方法，第0维代表样本数量，用None填充</span></span><br><span class="line">linear.build(input_shape = (<span class="literal">None</span>,<span class="number">16</span>)) </span><br><span class="line">print(linear.built) <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">linear = Linear(units = <span class="number">8</span>)</span><br><span class="line">print(linear.built) <span class="comment"># False</span></span><br><span class="line">linear.build(input_shape = (<span class="literal">None</span>,<span class="number">16</span>)) </span><br><span class="line">print(linear.compute_output_shape(input_shape = (<span class="literal">None</span>,<span class="number">16</span>))) <span class="comment"># (None, 8)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">linear = Linear(units = <span class="number">16</span>)</span><br><span class="line">print(linear.built) <span class="comment"># False</span></span><br><span class="line"><span class="comment">#如果built = False，调用__call__时会先调用build方法, 再调用call方法。</span></span><br><span class="line">linear(tf.random.uniform((<span class="number">100</span>,<span class="number">64</span>))) </span><br><span class="line">print(linear.built) <span class="comment"># True</span></span><br><span class="line">config = linear.get_config()</span><br><span class="line">print(config) <span class="comment"># &#123;'name': 'linear_3', 'trainable': True, 'dtype': 'float32', 'units': 16&#125;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line"><span class="comment">#注意该处的input_shape会被模型加工，无需使用None代表样本数量维</span></span><br><span class="line">model.add(Linear(units = <span class="number">1</span>,input_shape = (<span class="number">2</span>,)))  </span><br><span class="line">print(<span class="string">"model.input_shape: "</span>,model.input_shape)</span><br><span class="line">print(<span class="string">"model.output_shape: "</span>,model.output_shape)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">model.input_shape:  (None, 2)</span><br><span class="line">model.output_shape:  (None, 1)</span><br><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">linear (Linear)              (None, 1)                 3         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 3</span><br><span class="line">Trainable params: 3</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(optimizer = <span class="string">"sgd"</span>,loss = <span class="string">"mse"</span>,metrics=[<span class="string">"mae"</span>])</span><br><span class="line">print(model.predict(tf.constant([[<span class="number">3.0</span>,<span class="number">2.0</span>],[<span class="number">4.0</span>,<span class="number">5.0</span>]])))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">-0.04092304</span>]</span><br><span class="line"> [<span class="number">-0.06150477</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存成 h5模型</span></span><br><span class="line">model.save(<span class="string">"./data/linear_model.h5"</span>,save_format = <span class="string">"h5"</span>)</span><br><span class="line">model_loaded_keras = tf.keras.models.load_model(</span><br><span class="line">    <span class="string">"./data/linear_model.h5"</span>,custom_objects=&#123;<span class="string">"Linear"</span>:Linear&#125;)</span><br><span class="line">print(model_loaded_keras.predict(tf.constant([[<span class="number">3.0</span>,<span class="number">2.0</span>],[<span class="number">4.0</span>,<span class="number">5.0</span>]])))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">-0.04092304</span>]</span><br><span class="line"> [<span class="number">-0.06150477</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存成 tf模型</span></span><br><span class="line">model.save(<span class="string">"./data/linear_model"</span>,save_format = <span class="string">"tf"</span>)</span><br><span class="line">model_loaded_tf = tf.keras.models.load_model(<span class="string">"./data/linear_model"</span>)</span><br><span class="line">print(model_loaded_tf.predict(tf.constant([[<span class="number">3.0</span>,<span class="number">2.0</span>],[<span class="number">4.0</span>,<span class="number">5.0</span>]])))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">INFO:tensorflow:Assets written to: ./data/linear_model/assets</span><br><span class="line">[[<span class="number">-0.04092304</span>]</span><br><span class="line"> [<span class="number">-0.06150477</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="chu-shi-hua-mo-xing-can-shu">初始化模型参数</h3>
<p>模型的默认初始化方法：权重参数元素为[-0.07, 0.07]之间均匀分布的随机数，偏差参数则全为0。但我们经常需要使用其他方法来初始化权重。比如，将权重参数初始化成均值为0、标准差为0.01的正态分布随机数，并依然将偏差参数清零。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.d1 = tf.keras.layers.Dense(</span><br><span class="line">            units=<span class="number">10</span>,</span><br><span class="line">            activation=<span class="literal">None</span>,</span><br><span class="line">            kernel_initializer=tf.random_normal_initializer(mean=<span class="number">0</span>,stddev=<span class="number">0.01</span>),</span><br><span class="line">            bias_initializer=tf.zeros_initializer()</span><br><span class="line">        )</span><br><span class="line">        self.d2 = tf.keras.layers.Dense(</span><br><span class="line">            units=<span class="number">1</span>,</span><br><span class="line">            activation=<span class="literal">None</span>,</span><br><span class="line">            kernel_initializer=tf.ones_initializer(),</span><br><span class="line">            bias_initializer=tf.ones_initializer()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        output = self.d1(input)</span><br><span class="line">        output = self.d2(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"><span class="comment"># 可以使用`tf.keras.initializers`类中的方法实现自定义初始化。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_init</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.keras.initializers.Ones()</span><br><span class="line"></span><br><span class="line">model = tf.keras.models.Sequential()</span><br><span class="line">model.add(tf.keras.layers.Dense(<span class="number">64</span>, kernel_initializer=my_init()))</span><br></pre></td></tr></table></figure>
<h2 id="sun-shi-han-shu-losses">损失函数losses</h2>
<p>一般来说，监督学习的目标函数由损失函数和正则化项组成。（Objective = Loss + Regularization）</p>
<p>对于keras模型，目标函数中的正则化项一般在各层中指定，例如使用Dense的 kernel_regularizer 和 bias_regularizer等参数指定权重使用l1或者l2正则化项，此外还可以用kernel_constraint 和 bias_constraint等参数约束权重的取值范围，这也是一种正则化手段。</p>
<p>损失函数在模型编译时候指定。对于回归模型，通常使用的损失函数是均方损失函数 mean_squared_error。对于二分类模型，通常使用的是二元交叉熵损失函数 binary_crossentropy。</p>
<p>对于多分类模型，如果label是one-hot编码的，则使用类别交叉熵损失函数 categorical_crossentropy。如果label是类别序号编码的，则需要使用稀疏类别交叉熵损失函数 sparse_categorical_crossentropy。</p>
<p>如果有需要，也可以自定义损失函数，自定义损失函数需要接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为损失函数值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models,losses,regularizers,constraints</span><br></pre></td></tr></table></figure>
<h3 id="sun-shi-han-shu-he-zheng-ze-hua-xiang">损失函数和正则化项</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, input_dim=<span class="number">64</span>,</span><br><span class="line">                kernel_regularizer=regularizers.l2(<span class="number">0.01</span>), </span><br><span class="line">                activity_regularizer=regularizers.l1(<span class="number">0.01</span>),</span><br><span class="line">                kernel_constraint = constraints.MaxNorm(max_value=<span class="number">2</span>, axis=<span class="number">0</span>))) </span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>,</span><br><span class="line">        kernel_regularizer=regularizers.l1_l2(<span class="number">0.01</span>,<span class="number">0.01</span>),activation = <span class="string">"sigmoid"</span>))</span><br><span class="line">model.compile(optimizer = <span class="string">"rmsprop"</span>,</span><br><span class="line">        loss = <span class="string">"binary_crossentropy"</span>,metrics = [<span class="string">"AUC"</span>])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">dense (Dense)                (None, 64)                4160      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_1 (Dense)              (None, 10)                650       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 4,810</span><br><span class="line">Trainable params: 4,810</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<h3 id="nei-zhi-sun-shi-han-shu">内置损失函数</h3>
<p>内置的损失函数一般有两种形式:</p>
<ul>
<li>类的实现</li>
<li>函数的实现</li>
</ul>
<p>如：CategoricalCrossentropy 和 categorical_crossentropy 都是类别交叉熵损失函数，前者是类的实现形式，后者是函数的实现形式。</p>
<p>常用的一些内置损失函数说明如下。</p>
<ul>
<li>
<p>mean_squared_error（均方误差损失，用于回归，简写为 mse, 类与函数实现形式分别为 MeanSquaredError 和 MSE）</p>
</li>
<li>
<p>mean_absolute_error (平均绝对值误差损失，用于回归，简写为 mae, 类与函数实现形式分别为 MeanAbsoluteError 和 MAE)</p>
</li>
<li>
<p>mean_absolute_percentage_error (平均百分比误差损失，用于回归，简写为 mape, 类与函数实现形式分别为 MeanAbsolutePercentageError 和 MAPE)</p>
</li>
<li>
<p>Huber(Huber损失，只有类实现形式，用于回归，介于mse和mae之间，对异常值比较鲁棒，相对mse有一定的优势)</p>
</li>
<li>
<p>binary_crossentropy(二元交叉熵，用于二分类，类实现形式为 BinaryCrossentropy)</p>
</li>
<li>
<p>categorical_crossentropy(类别交叉熵，用于多分类，要求label为onehot编码，类实现形式为 CategoricalCrossentropy)</p>
</li>
<li>
<p>sparse_categorical_crossentropy(稀疏类别交叉熵，用于多分类，要求label为序号编码形式，类实现形式为 SparseCategoricalCrossentropy)</p>
</li>
<li>
<p>hinge(合页损失函数，用于二分类，最著名的应用是作为支持向量机SVM的损失函数，类实现形式为 Hinge)</p>
</li>
<li>
<p>kld(相对熵损失，也叫KL散度，常用于最大期望算法EM的损失函数，两个概率分布差异的一种信息度量。类与函数实现形式分别为 KLDivergence 或 KLD)</p>
</li>
<li>
<p>cosine_similarity(余弦相似度，可用于多分类，类实现形式为 CosineSimilarity)</p>
</li>
</ul>
<h3 id="zi-ding-yi-sun-shi-han-shu">自定义损失函数</h3>
<p>自定义损失函数接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为损失函数值。也可以对tf.keras.losses.Loss进行子类化，重写call方法实现损失的计算逻辑，从而得到损失函数的类的实现。</p>
<p>下面是一个Focal Loss的自定义实现示范。Focal Loss是一种对binary_crossentropy的改进损失函数形式。它在样本不均衡和存在较多易分类的样本时相比binary_crossentropy具有明显的优势。它有两个可调参数，alpha参数和gamma参数。其中alpha参数主要用于衰减负样本的权重，gamma参数主要用于衰减容易训练样本的权重。从而让模型更加聚焦在正样本和困难样本上。这就是为什么这个损失函数叫做Focal Loss。</p>
<p>\[focal\_loss(y,p) = \begin{cases}
-\alpha  (1-p)^{\gamma}\log(p) &amp;
\text{if y = 1}\\
-(1-\alpha) p^{\gamma}\log(1-p) &amp;
\text{if y = 0}
\end{cases} \]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">focal_loss</span><span class="params">(gamma=<span class="number">2.</span>, alpha=<span class="number">0.75</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">focal_loss_fixed</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">        bce = tf.losses.binary_crossentropy(y_true, y_pred)</span><br><span class="line">        p_t = (y_true * y_pred) + ((<span class="number">1</span> - y_true) * (<span class="number">1</span> - y_pred))</span><br><span class="line">        alpha_factor = y_true * alpha + (<span class="number">1</span> - y_true) * (<span class="number">1</span> - alpha)</span><br><span class="line">        modulating_factor = tf.pow(<span class="number">1.0</span> - p_t, gamma)</span><br><span class="line">        loss = tf.reduce_sum(alpha_factor * modulating_factor * bce,axis = <span class="number">-1</span> )</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    <span class="keyword">return</span> focal_loss_fixed</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span><span class="params">(tf.keras.losses.Loss)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,gamma=<span class="number">2.0</span>,alpha=<span class="number">0.75</span>,name = <span class="string">"focal_loss"</span>)</span>:</span></span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.alpha = alpha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self,y_true,y_pred)</span>:</span></span><br><span class="line">        bce = tf.losses.binary_crossentropy(y_true, y_pred)</span><br><span class="line">        p_t = (y_true * y_pred) + ((<span class="number">1</span> - y_true) * (<span class="number">1</span> - y_pred))</span><br><span class="line">        alpha_factor = y_true * self.alpha + (<span class="number">1</span> - y_true) * (<span class="number">1</span> - self.alpha)</span><br><span class="line">        modulating_factor = tf.pow(<span class="number">1.0</span> - p_t, self.gamma)</span><br><span class="line">        loss = tf.reduce_sum(alpha_factor * modulating_factor * bce,axis = <span class="number">-1</span> )</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h2 id="ping-gu-zhi-biao-metrics">评估指标metrics</h2>
<p>损失函数除了作为模型训练时候的优化目标，也能够作为模型好坏的一种评价指标。但通常人们还会从其它角度评估模型的好坏。这就是评估指标。通常损失函数都可以作为评估指标，如MAE,MSE,CategoricalCrossentropy等也是常用的评估指标。但评估指标不一定可以作为损失函数，例如AUC,Accuracy,Precision。因为评估指标不要求连续可导，而损失函数通常要求连续可导。</p>
<p>编译模型时，可以通过列表形式指定多个评估指标。如果有需要，也可以自定义评估指标。自定义评估指标需要接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为评估值。</p>
<p>也可以对tf.keras.metrics.Metric进行子类化，重写初始化方法, update_state方法, result方法实现评估指标的计算逻辑，从而得到评估指标的类的实现形式。</p>
<p>由于训练的过程通常是分批次训练的，而评估指标要跑完一个epoch才能够得到整体的指标结果。因此，类形式的评估指标更为常见。即需要编写初始化方法以创建与计算指标结果相关的一些中间变量，编写update_state方法在每个batch后更新相关中间变量的状态，编写result方法输出最终指标结果。</p>
<p>如果编写函数形式的评估指标，则只能取epoch中各个batch计算的评估指标结果的平均值作为整个epoch上的评估指标结果，这个结果通常会偏离整个epoch数据一次计算的结果。</p>
<h3 id="chang-yong-de-nei-zhi-ping-gu-zhi-biao">常用的内置评估指标</h3>
<ul>
<li>
<p>MeanSquaredError（均方误差，用于回归，可以简写为MSE，函数形式为mse）</p>
</li>
<li>
<p>MeanAbsoluteError (平均绝对值误差，用于回归，可以简写为MAE，函数形式为mae)</p>
</li>
<li>
<p>MeanAbsolutePercentageError (平均百分比误差，用于回归，可以简写为MAPE，函数形式为mape)</p>
</li>
<li>
<p>RootMeanSquaredError (均方根误差，用于回归)</p>
</li>
<li>
<p>Accuracy (准确率，用于分类，可以用字符串&quot;Accuracy&quot;表示，Accuracy=(TP+TN)/(TP+TN+FP+FN)，要求y_true和y_pred都为类别序号编码)</p>
</li>
<li>
<p>Precision (精确率，用于二分类，Precision = TP/(TP+FP))</p>
</li>
<li>
<p>Recall (召回率，用于二分类，Recall = TP/(TP+FN))</p>
</li>
<li>
<p>TruePositives (真正例，用于二分类)</p>
</li>
<li>
<p>TrueNegatives (真负例，用于二分类)</p>
</li>
<li>
<p>FalsePositives (假正例，用于二分类)</p>
</li>
<li>
<p>FalseNegatives (假负例，用于二分类)</p>
</li>
<li>
<p>AUC(ROC曲线(TPR vs FPR)下的面积，用于二分类，直观解释为随机抽取一个正样本和一个负样本，正样本的预测值大于负样本的概率)</p>
</li>
<li>
<p>CategoricalAccuracy（分类准确率，与Accuracy含义相同，要求y_true(label)为onehot编码形式）</p>
</li>
<li>
<p>SparseCategoricalAccuracy (稀疏分类准确率，与Accuracy含义相同，要求y_true(label)为序号编码形式)</p>
</li>
<li>
<p>MeanIoU (Intersection-Over-Union，常用于图像分割)</p>
</li>
<li>
<p>TopKCategoricalAccuracy (多分类TopK准确率，要求y_true(label)为onehot编码形式)</p>
</li>
<li>
<p>SparseTopKCategoricalAccuracy (稀疏多分类TopK准确率，要求y_true(label)为序号编码形式)</p>
</li>
<li>
<p>Mean (平均值)</p>
</li>
<li>
<p>Sum (求和)</p>
</li>
</ul>
<h3 id="zi-ding-yi-ping-gu-zhi-biao">自定义评估指标</h3>
<p>以金融风控领域常用的KS指标为例，示范自定义评估指标。KS指标适合二分类问题，其计算方式为 KS=max(TPR-FPR).</p>
<p>其中TPR=TP/(TP+FN) , FPR = FP/(FP+TN) TPR曲线实际上就是正样本的累积分布曲线(CDF)，FPR曲线实际上就是负样本的累积分布曲线(CDF)。KS指标就是正样本和负样本累积分布曲线差值的最大值。</p>
<p><img src="/2020/05/07/tensorflow2/KS_curve.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models,losses,metrics</span><br><span class="line"></span><br><span class="line"><span class="comment">#函数形式的自定义评估指标</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ks</span><span class="params">(y_true,y_pred)</span>:</span></span><br><span class="line">    y_true = tf.reshape(y_true,(<span class="number">-1</span>,))</span><br><span class="line">    y_pred = tf.reshape(y_pred,(<span class="number">-1</span>,))</span><br><span class="line">    length = tf.shape(y_true)[<span class="number">0</span>]</span><br><span class="line">    t = tf.math.top_k(y_pred,k = length,sorted = <span class="literal">False</span>)</span><br><span class="line">    y_pred_sorted = tf.gather(y_pred,t.indices)</span><br><span class="line">    y_true_sorted = tf.gather(y_true,t.indices)</span><br><span class="line">    cum_positive_ratio = tf.truediv(</span><br><span class="line">        tf.cumsum(y_true_sorted),tf.reduce_sum(y_true_sorted))</span><br><span class="line">    cum_negative_ratio = tf.truediv(</span><br><span class="line">        tf.cumsum(<span class="number">1</span> - y_true_sorted),tf.reduce_sum(<span class="number">1</span> - y_true_sorted))</span><br><span class="line">    ks_value = tf.reduce_max(tf.abs(cum_positive_ratio - cum_negative_ratio)) </span><br><span class="line">    <span class="keyword">return</span> ks_value</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_true = tf.constant([[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">0</span>]])</span><br><span class="line">y_pred = tf.constant([[<span class="number">0.6</span>],[<span class="number">0.1</span>],[<span class="number">0.4</span>],[<span class="number">0.5</span>],[<span class="number">0.7</span>],[<span class="number">0.7</span>],[<span class="number">0.7</span>],</span><br><span class="line">                      [<span class="number">0.4</span>],[<span class="number">0.4</span>],[<span class="number">0.5</span>],[<span class="number">0.8</span>],[<span class="number">0.3</span>],[<span class="number">0.5</span>],[<span class="number">0.3</span>]])</span><br><span class="line">tf.print(ks(y_true,y_pred)) <span class="comment"># 0.625</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#类形式的自定义评估指标</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KS</span><span class="params">(metrics.Metric)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name = <span class="string">"ks"</span>, **kwargs)</span>:</span></span><br><span class="line">        super(KS,self).__init__(name=name,**kwargs)</span><br><span class="line">        self.true_positives = self.add_weight(</span><br><span class="line">            name = <span class="string">"tp"</span>,shape = (<span class="number">101</span>,), initializer = <span class="string">"zeros"</span>)</span><br><span class="line">        self.false_positives = self.add_weight(</span><br><span class="line">            name = <span class="string">"fp"</span>,shape = (<span class="number">101</span>,), initializer = <span class="string">"zeros"</span>)</span><br><span class="line">   </span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_state</span><span class="params">(self,y_true,y_pred)</span>:</span></span><br><span class="line">        y_true = tf.cast(tf.reshape(y_true,(<span class="number">-1</span>,)),tf.bool)</span><br><span class="line">        y_pred = tf.cast(<span class="number">100</span>*tf.reshape(y_pred,(<span class="number">-1</span>,)),tf.int32)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tf.range(<span class="number">0</span>,tf.shape(y_true)[<span class="number">0</span>]):</span><br><span class="line">            <span class="keyword">if</span> y_true[i]:</span><br><span class="line">                self.true_positives[y_pred[i]].assign(</span><br><span class="line">                    self.true_positives[y_pred[i]]+<span class="number">1.0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.false_positives[y_pred[i]].assign(</span><br><span class="line">                    self.false_positives[y_pred[i]]+<span class="number">1.0</span>)</span><br><span class="line">        <span class="keyword">return</span> (self.true_positives,self.false_positives)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">result</span><span class="params">(self)</span>:</span></span><br><span class="line">        cum_positive_ratio = tf.truediv(</span><br><span class="line">            tf.cumsum(self.true_positives),tf.reduce_sum(self.true_positives))</span><br><span class="line">        cum_negative_ratio = tf.truediv(</span><br><span class="line">            tf.cumsum(self.false_positives),tf.reduce_sum(self.false_positives))</span><br><span class="line">        ks_value = tf.reduce_max(tf.abs(cum_positive_ratio - cum_negative_ratio)) </span><br><span class="line">        <span class="keyword">return</span> ks_value</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_true = tf.constant([[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">0</span>]])</span><br><span class="line">y_pred = tf.constant([[<span class="number">0.6</span>],[<span class="number">0.1</span>],[<span class="number">0.4</span>],[<span class="number">0.5</span>],[<span class="number">0.7</span>],[<span class="number">0.7</span>],</span><br><span class="line">                      [<span class="number">0.7</span>],[<span class="number">0.4</span>],[<span class="number">0.4</span>],[<span class="number">0.5</span>],[<span class="number">0.8</span>],[<span class="number">0.3</span>],[<span class="number">0.5</span>],[<span class="number">0.3</span>]])</span><br><span class="line"></span><br><span class="line">myks = KS()</span><br><span class="line">myks.update_state(y_true,y_pred)</span><br><span class="line">tf.print(myks.result()) <span class="comment"># 0.625</span></span><br></pre></td></tr></table></figure>
<h2 id="you-hua-qi-optimizers">优化器optimizers</h2>
<p>模型优化算法的选择直接关系到最终模型的性能。有时候效果不好，未必是特征的问题或者模型设计的问题，很可能就是优化算法的问题。深度学习优化算法大概经历了 SGD -&gt; SGDM -&gt; NAG -&gt;Adagrad -&gt; Adadelta(RMSprop) -&gt; Adam -&gt; Nadam 这样的发展历程。</p>
<p>对于一般新手炼丹师，优化器直接使用Adam，并使用其默认参数就OK了。</p>
<p>一些爱写论文的炼丹师由于追求评估指标效果，可能会偏爱前期使用Adam优化器快速下降，后期使用SGD并精调优化器参数得到更好的结果。此外目前也有一些前沿的优化算法，据称效果比Adam更好，例如LazyAdam, Look-ahead, RAdam, Ranger等.</p>
<h3 id="you-hua-qi-de-shi-yong">优化器的使用</h3>
<p>优化器主要使用apply_gradients方法传入变量和对应梯度从而来对给定变量进行迭代，或者直接使用minimize方法对目标函数进行迭代优化。</p>
<p>当然，更常见的使用是在编译时将优化器传入keras的Model,通过调用model.fit实现对Loss的的迭代优化。</p>
<p>初始化优化器时会创建一个变量optimier.iterations用于记录迭代的次数。因此优化器和tf.Variable一样，一般需要在@tf.function外创建。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    ts = tf.timestamp()</span><br><span class="line">    today_ts = ts%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>,end = <span class="string">""</span>)</span><br><span class="line">    tf.print(timestring)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用optimizer.apply_gradients</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minimizef</span><span class="params">()</span>:</span></span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> tf.constant(<span class="literal">True</span>): </span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">        dy_dx = tape.gradient(y,x)</span><br><span class="line">        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#迭代终止条件</span></span><br><span class="line">        <span class="keyword">if</span> tf.abs(dy_dx)&lt;tf.constant(<span class="number">0.00001</span>):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> tf.math.mod(optimizer.iterations,<span class="number">100</span>)==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"step = "</span>,optimizer.iterations)</span><br><span class="line">            tf.print(<span class="string">"x = "</span>, x)</span><br><span class="line">            tf.print(<span class="string">""</span>)</span><br><span class="line">                </span><br><span class="line">    y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"y ="</span>,minimizef())</span><br><span class="line">tf.print(<span class="string">"x ="</span>,x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用optimizer.minimize</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)   </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">()</span>:</span>   </span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    y = a*tf.pow(x,<span class="number">2</span>)+b*x+c</span><br><span class="line">    <span class="keyword">return</span>(y)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epoch = <span class="number">1000</span>)</span>:</span>  </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> tf.range(epoch):  </span><br><span class="line">        optimizer.minimize(f,[x])</span><br><span class="line">    tf.print(<span class="string">"epoch = "</span>,optimizer.iterations)</span><br><span class="line">    <span class="keyword">return</span>(f())</span><br><span class="line"></span><br><span class="line">train(<span class="number">1000</span>)</span><br><span class="line">tf.print(<span class="string">"y = "</span>,f())</span><br><span class="line">tf.print(<span class="string">"x = "</span>,x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"><span class="comment"># 使用model.fit</span></span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FakeModel</span><span class="params">(tf.keras.models.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,a,b,c)</span>:</span></span><br><span class="line">        super(FakeModel,self).__init__()</span><br><span class="line">        self.a = a</span><br><span class="line">        self.b = b</span><br><span class="line">        self.c = c</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>)</span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self,features)</span>:</span></span><br><span class="line">        loss  = self.a*(self.x)**<span class="number">2</span>+self.b*(self.x)+self.c</span><br><span class="line">        <span class="keyword">return</span>(tf.ones_like(features)*loss)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myloss</span><span class="params">(y_true,y_pred)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(y_pred)</span><br><span class="line"></span><br><span class="line">model = FakeModel(tf.constant(<span class="number">1.0</span>),tf.constant(<span class="number">-2.0</span>),tf.constant(<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line">model.build()</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.compile(optimizer = </span><br><span class="line">              tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>),loss = myloss)</span><br><span class="line">history = model.fit(tf.zeros((<span class="number">100</span>,<span class="number">2</span>)),</span><br><span class="line">                    tf.ones(<span class="number">100</span>),batch_size = <span class="number">1</span>,epochs = <span class="number">10</span>)  <span class="comment">#迭代1000次</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.print(<span class="string">"x="</span>,model.x)</span><br><span class="line">tf.print(<span class="string">"loss="</span>,model(tf.constant(<span class="number">0.0</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="nei-zhi-you-hua-qi">内置优化器</h3>
<p>深度学习优化算法大概经历了 SGD -&gt; SGDM -&gt; NAG -&gt;Adagrad -&gt; Adadelta(RMSprop) -&gt; Adam -&gt; Nadam 这样的发展历程。</p>
<p>在keras.optimizers子模块中，它们基本上都有对应的类的实现。</p>
<ul>
<li>
<p>SGD, 默认参数为纯SGD, 设置momentum参数不为0实际上变成SGDM, 考虑了一阶动量, 设置 nesterov为True后变成NAG，即 Nesterov Accelerated Gradient，在计算梯度时计算的是向前走一步所在位置的梯度。</p>
</li>
<li>
<p>Adagrad, 考虑了二阶动量，对于不同的参数有不同的学习率，即自适应学习率。缺点是学习率单调下降，可能后期学习速率过慢乃至提前停止学习。</p>
</li>
<li>
<p>RMSprop, 考虑了二阶动量，对于不同的参数有不同的学习率，即自适应学习率，对Adagrad进行了优化，通过指数平滑只考虑一定窗口内的二阶动量。</p>
</li>
<li>
<p>Adadelta, 考虑了二阶动量，与RMSprop类似，但是更加复杂一些，自适应性更强。</p>
</li>
<li>
<p>Adam, 同时考虑了一阶动量和二阶动量，可以看成RMSprop上进一步考虑了一阶动量。</p>
</li>
<li>
<p>Nadam, 在Adam基础上进一步考虑了 Nesterov Acceleration。</p>
</li>
</ul>
<h2 id="hui-diao-han-shu-callbacks">回调函数callbacks</h2>
<p>tf.keras的回调函数实际上是一个类，一般是在model.fit时作为参数指定，用于控制在训练过程开始或者在训练过程结束，在每个epoch训练开始或者训练结束，在每个batch训练开始或者训练结束时执行一些操作，例如收集一些日志信息，改变学习率等超参数，提前终止训练过程等等。</p>
<p>同样地，针对model.evaluate或者model.predict也可以指定callbacks参数，用于控制在评估或预测开始或者结束时，在每个batch开始或者结束时执行一些操作，但这种用法相对少见。</p>
<p>大部分时候，keras.callbacks子模块中定义的回调函数类已经足够使用了，如果有特定的需要，我们也可以通过对keras.callbacks.Callbacks实施子类化构造自定义的回调函数。所有回调函数都继承至 keras.callbacks.Callbacks基类，拥有params和model这两个属性。其中params 是一个dict，记录了训练相关参数 (例如 verbosity, batch size, number of epochs 等等)。model即当前关联的模型的引用。</p>
<p>此外，对于回调类中的一些方法如on_epoch_begin,on_batch_end，还会有一个输入参数logs, 提供有关当前epoch或者batch的一些信息，并能够记录计算结果，如果model.fit指定了多个回调函数类，这些logs变量将在这些回调函数类的同名函数间依顺序传递。</p>
<h3 id="nei-zhi-hui-diao-han-shu">内置回调函数</h3>
<ul>
<li>
<p>BaseLogger： 收集每个epoch上metrics在各个batch上的平均值，对stateful_metrics参数中的带中间状态的指标直接拿最终值无需对各个batch平均，指标均值结果将添加到logs变量中。该回调函数被所有模型默认添加，且是第一个被添加的。</p>
</li>
<li>
<p>History： 将BaseLogger计算的各个epoch的metrics结果记录到history这个dict变量中，并作为model.fit的返回值。该回调函数被所有模型默认添加，在BaseLogger之后被添加。</p>
</li>
<li>
<p>EarlyStopping： 当被监控指标在设定的若干个epoch后没有提升，则提前终止训练。</p>
</li>
<li>
<p>TensorBoard： 为Tensorboard可视化保存日志信息。支持评估指标，计算图，模型参数等的可视化。</p>
</li>
<li>
<p>ModelCheckpoint： 在每个epoch后保存模型。</p>
</li>
<li>
<p>ReduceLROnPlateau：如果监控指标在设定的若干个epoch后没有提升，则以一定的因子减少学习率。</p>
</li>
<li>
<p>TerminateOnNaN：如果遇到loss为NaN，提前终止训练。</p>
</li>
<li>
<p>LearningRateScheduler：学习率控制器。给定学习率lr和epoch的函数关系，根据该函数关系在每个epoch前调整学习率。</p>
</li>
<li>
<p>CSVLogger：将每个epoch后的logs结果记录到CSV文件中。</p>
</li>
<li>
<p>ProgbarLogger：将每个epoch后的logs结果打印到标准输出流中。</p>
</li>
</ul>
<h3 id="zi-ding-yi-hui-diao-han-shu">自定义回调函数</h3>
<p>可以使用callbacks.LambdaCallback编写较为简单的回调函数，也可以通过对callbacks.Callback子类化编写更加复杂的回调函数逻辑。如果需要深入学习tf.Keras中的回调函数，不要犹豫阅读内置回调函数的源代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models,losses,metrics,callbacks</span><br><span class="line"><span class="keyword">import</span> tensorflow.keras.backend <span class="keyword">as</span> K</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 示范使用LambdaCallback编写较为简单的回调函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">json_log = open(<span class="string">'./data/keras_log.json'</span>, mode=<span class="string">'wt'</span>, buffering=<span class="number">1</span>)</span><br><span class="line">json_logging_callback = callbacks.LambdaCallback(</span><br><span class="line">    on_epoch_end=<span class="keyword">lambda</span> epoch, logs: json_log.write(</span><br><span class="line">        json.dumps(dict(epoch = epoch,**logs)) + <span class="string">'\n'</span>),</span><br><span class="line">    on_train_end=<span class="keyword">lambda</span> logs: json_log.close()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 示范通过Callback子类化编写回调函数（LearningRateScheduler的源代码）</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LearningRateScheduler</span><span class="params">(callbacks.Callback)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, schedule, verbose=<span class="number">0</span>)</span>:</span></span><br><span class="line">        super(LearningRateScheduler, self).__init__()</span><br><span class="line">        self.schedule = schedule</span><br><span class="line">        self.verbose = verbose</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_begin</span><span class="params">(self, epoch, logs=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(self.model.optimizer, <span class="string">'lr'</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Optimizer must have a "lr" attribute.'</span>)</span><br><span class="line">        <span class="keyword">try</span>:  </span><br><span class="line">            lr = float(K.get_value(self.model.optimizer.lr))</span><br><span class="line">            lr = self.schedule(epoch, lr)</span><br><span class="line">        <span class="keyword">except</span> TypeError:  <span class="comment"># Support for old API for backward compatibility</span></span><br><span class="line">            lr = self.schedule(epoch)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(lr, (tf.Tensor, float, np.float32, np.float64)):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'The output of the "schedule" function '</span></span><br><span class="line">                             <span class="string">'should be float.'</span>)</span><br><span class="line">        <span class="keyword">if</span> isinstance(lr, ops.Tensor) <span class="keyword">and</span> <span class="keyword">not</span> lr.dtype.is_floating:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'The dtype of Tensor should be float'</span>)</span><br><span class="line">        K.set_value(self.model.optimizer.lr, K.get_value(lr))</span><br><span class="line">        <span class="keyword">if</span> self.verbose &gt; <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'\nEpoch %05d: LearningRateScheduler reducing learning '</span></span><br><span class="line">                 <span class="string">'rate to %s.'</span> % (epoch + <span class="number">1</span>, lr))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span><span class="params">(self, epoch, logs=None)</span>:</span></span><br><span class="line">        logs = logs <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">        logs[<span class="string">'lr'</span>] = K.get_value(self.model.optimizer.lr)</span><br></pre></td></tr></table></figure>
<h1 id="tensor-flow-de-gao-jie-api">TensorFlow的高阶API</h1>
<p>TensorFlow的高阶API主要是tensorflow.keras.models.</p>
<p>本章主要详细介绍tensorflow.keras.models相关的以下内容。</p>
<ul>
<li>
<p>模型的构建（Sequential、functional API、Model子类化）</p>
</li>
<li>
<p>模型的训练（内置fit方法、内置train_on_batch方法、自定义训练循环、单GPU训练模型、多GPU训练模型、TPU训练模型）</p>
</li>
<li>
<p>模型的部署（tensorflow serving部署模型、使用spark(scala)调用tensorflow模型）</p>
</li>
</ul>
<h2 id="gou-jian-mo-xing-de-3-chong-fang-fa">构建模型的3种方法</h2>
<p>可以使用以下3种方式构建模型：</p>
<ol>
<li>使用Sequential按层顺序构建模型</li>
<li>使用函数式API构建任意结构模型</li>
<li>继承Model基类构建自定义模型。</li>
</ol>
<p>对于顺序结构的模型，优先使用Sequential方法构建。</p>
<p>如果模型有多输入或者多输出，或者模型需要共享权重，或者模型具有残差连接等非顺序结构，推荐使用函数式API进行创建。</p>
<p>如果无特定必要，尽可能避免使用Model子类化的方式构建模型，这种方式提供了极大的灵活性，但也有更大的概率出错。</p>
<p>下面以IMDB电影评论的分类问题为例，演示3种创建模型的方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_token_path = <span class="string">"./data/imdb/train_token.csv"</span></span><br><span class="line">test_token_path = <span class="string">"./data/imdb/test_token.csv"</span></span><br><span class="line"></span><br><span class="line">MAX_WORDS = <span class="number">10000</span>  <span class="comment"># We will only consider the top 10,000 words in the dataset</span></span><br><span class="line">MAX_LEN = <span class="number">200</span>  <span class="comment"># We will cut reviews after 200 words</span></span><br><span class="line">BATCH_SIZE = <span class="number">20</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建管道</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_line</span><span class="params">(line)</span>:</span></span><br><span class="line">    t = tf.strings.split(line,<span class="string">"\t"</span>)</span><br><span class="line">    label = tf.reshape(tf.cast(tf.strings.to_number(t[<span class="number">0</span>]),tf.int32),(<span class="number">-1</span>,))</span><br><span class="line">    features = tf.cast(tf.strings.to_number(tf.strings.split(t[<span class="number">1</span>],<span class="string">" "</span>)),tf.int32)</span><br><span class="line">    <span class="keyword">return</span> (features,label)</span><br><span class="line"></span><br><span class="line">ds_train=  tf.data.TextLineDataset(filenames = [train_token_path]) \</span><br><span class="line">   .map(parse_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \</span><br><span class="line">   .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">   .prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">ds_test=  tf.data.TextLineDataset(filenames = [test_token_path]) \</span><br><span class="line">   .map(parse_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \</span><br><span class="line">   .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">   .prefetch(tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure>
<h3 id="sequential-an-ceng-shun-xu-chuang-jian-mo-xing">Sequential按层顺序创建模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line"></span><br><span class="line">model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'Nadam'</span>,</span><br><span class="line">            loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>,<span class="string">"AUC"</span>])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/Sequential%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">baselogger = callbacks.BaseLogger(stateful_metrics=[<span class="string">"AUC"</span>])</span><br><span class="line">logdir = <span class="string">"./data/keras_model/"</span> + datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=<span class="number">1</span>)</span><br><span class="line">history = model.fit(ds_train,validation_data = ds_test,</span><br><span class="line">        epochs = <span class="number">6</span>,callbacks=[baselogger,tensorboard_callback])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">'val_'</span>+metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span>+ metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span>+metric, <span class="string">'val_'</span>+metric])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"AUC"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/6-1-fit%E6%A8%A1%E5%9E%8B.jpg" alt></p>
<h3 id="han-shu-shi-api-chuang-jian-ren-yi-jie-gou-mo-xing">函数式API创建任意结构模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">inputs = layers.Input(shape=[MAX_LEN])</span><br><span class="line">x  = layers.Embedding(MAX_WORDS,<span class="number">7</span>)(inputs)</span><br><span class="line"></span><br><span class="line">branch1 = layers.SeparableConv1D(<span class="number">64</span>,<span class="number">3</span>,activation=<span class="string">"relu"</span>)(x)</span><br><span class="line">branch1 = layers.MaxPool1D(<span class="number">3</span>)(branch1)</span><br><span class="line">branch1 = layers.SeparableConv1D(<span class="number">32</span>,<span class="number">3</span>,activation=<span class="string">"relu"</span>)(branch1)</span><br><span class="line">branch1 = layers.GlobalMaxPool1D()(branch1)</span><br><span class="line"></span><br><span class="line">branch2 = layers.SeparableConv1D(<span class="number">64</span>,<span class="number">5</span>,activation=<span class="string">"relu"</span>)(x)</span><br><span class="line">branch2 = layers.MaxPool1D(<span class="number">5</span>)(branch2)</span><br><span class="line">branch2 = layers.SeparableConv1D(<span class="number">32</span>,<span class="number">5</span>,activation=<span class="string">"relu"</span>)(branch2)</span><br><span class="line">branch2 = layers.GlobalMaxPool1D()(branch2)</span><br><span class="line"></span><br><span class="line">branch3 = layers.SeparableConv1D(<span class="number">64</span>,<span class="number">7</span>,activation=<span class="string">"relu"</span>)(x)</span><br><span class="line">branch3 = layers.MaxPool1D(<span class="number">7</span>)(branch3)</span><br><span class="line">branch3 = layers.SeparableConv1D(<span class="number">32</span>,<span class="number">7</span>,activation=<span class="string">"relu"</span>)(branch3)</span><br><span class="line">branch3 = layers.GlobalMaxPool1D()(branch3)</span><br><span class="line"></span><br><span class="line">concat = layers.Concatenate()([branch1,branch2,branch3])</span><br><span class="line">outputs = layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>)(concat)</span><br><span class="line"></span><br><span class="line">model = models.Model(inputs = inputs,outputs = outputs)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'Nadam'</span>,</span><br><span class="line">            loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>,<span class="string">"AUC"</span>])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">Model: "model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">Layer (type)                    Output Shape         Param #     Connected to                     </span><br><span class="line">==================================================================================================</span><br><span class="line">input_1 (InputLayer)            [(None, 200)]        0                                            </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">embedding (Embedding)           (None, 200, 7)       70000       input_1[<span class="string">0</span>][<span class="symbol">0</span>]                    </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable_conv1d (SeparableConv (None, 198, 64)      533         embedding[<span class="string">0</span>][<span class="symbol">0</span>]                  </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable<span class="emphasis">_conv1d_</span>2 (SeparableCo (None, 196, 64)      547         embedding[<span class="string">0</span>][<span class="symbol">0</span>]                  </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable<span class="emphasis">_conv1d_</span>4 (SeparableCo (None, 194, 64)      561         embedding[<span class="string">0</span>][<span class="symbol">0</span>]                  </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">max<span class="emphasis">_pooling1d (MaxPooling1D)    (None, 66, 64)       0           separable_</span>conv1d[<span class="string">0</span>][<span class="symbol">0</span>]           </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">max<span class="emphasis">_pooling1d_</span>1 (MaxPooling1D)  (None, 39, 64)       0           separable<span class="emphasis">_conv1d_</span>2[<span class="string">0</span>][<span class="symbol">0</span>]         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">max<span class="emphasis">_pooling1d_</span>2 (MaxPooling1D)  (None, 27, 64)       0           separable<span class="emphasis">_conv1d_</span>4[<span class="string">0</span>][<span class="symbol">0</span>]         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable<span class="emphasis">_conv1d_</span>1 (SeparableCo (None, 64, 32)       2272        max_pooling1d[<span class="string">0</span>][<span class="symbol">0</span>]              </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable<span class="emphasis">_conv1d_</span>3 (SeparableCo (None, 35, 32)       2400        max<span class="emphasis">_pooling1d_</span>1[<span class="string">0</span>][<span class="symbol">0</span>]            </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable<span class="emphasis">_conv1d_</span>5 (SeparableCo (None, 21, 32)       2528        max<span class="emphasis">_pooling1d_</span>2[<span class="string">0</span>][<span class="symbol">0</span>]            </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">global<span class="emphasis">_max_</span>pooling1d (GlobalMax (None, 32)           0           separable<span class="emphasis">_conv1d_</span>1[<span class="string">0</span>][<span class="symbol">0</span>]         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">global<span class="emphasis">_max_</span>pooling1d<span class="emphasis">_1 (GlobalM (None, 32)           0           separable_</span>conv1d_3[<span class="string">0</span>][<span class="symbol">0</span>]         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">global<span class="emphasis">_max_</span>pooling1d<span class="emphasis">_2 (GlobalM (None, 32)           0           separable_</span>conv1d_5[<span class="string">0</span>][<span class="symbol">0</span>]         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">concatenate (Concatenate)       (None, 96)           0           global<span class="emphasis">_max_</span>pooling1d[<span class="string">0</span>][<span class="symbol">0</span>]       </span><br><span class="line"><span class="code">                                                                 global_max_pooling1d_1[0][0]     </span></span><br><span class="line"><span class="code">                                                                 global_max_pooling1d_2[0][0]     </span></span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">dense (Dense)                   (None, 1)            97          concatenate[<span class="string">0</span>][<span class="symbol">0</span>]                </span><br><span class="line">==================================================================================================</span><br><span class="line">Total params: 78,938</span><br><span class="line">Trainable params: 78,938</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/FunctionalAPI%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">logdir = <span class="string">"./data/keras_model/"</span> + datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=<span class="number">1</span>)</span><br><span class="line">history = model.fit(ds_train,validation_data = ds_test,epochs = <span class="number">6</span>,callbacks=[tensorboard_callback])</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">Epoch <span class="number">1</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">32</span>s <span class="number">32</span>ms/step - loss: <span class="number">0.5527</span> - accuracy: <span class="number">0.6758</span> - AUC: <span class="number">0.7731</span> - val_loss: <span class="number">0.3646</span> - val_accuracy: <span class="number">0.8426</span> - val_AUC: <span class="number">0.9192</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">24</span>s <span class="number">24</span>ms/step - loss: <span class="number">0.3024</span> - accuracy: <span class="number">0.8737</span> - AUC: <span class="number">0.9444</span> - val_loss: <span class="number">0.3281</span> - val_accuracy: <span class="number">0.8644</span> - val_AUC: <span class="number">0.9350</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">24</span>s <span class="number">24</span>ms/step - loss: <span class="number">0.2158</span> - accuracy: <span class="number">0.9159</span> - AUC: <span class="number">0.9715</span> - val_loss: <span class="number">0.3461</span> - val_accuracy: <span class="number">0.8666</span> - val_AUC: <span class="number">0.9363</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">24</span>s <span class="number">24</span>ms/step - loss: <span class="number">0.1492</span> - accuracy: <span class="number">0.9464</span> - AUC: <span class="number">0.9859</span> - val_loss: <span class="number">0.4017</span> - val_accuracy: <span class="number">0.8568</span> - val_AUC: <span class="number">0.9311</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">24</span>s <span class="number">24</span>ms/step - loss: <span class="number">0.0944</span> - accuracy: <span class="number">0.9696</span> - AUC: <span class="number">0.9939</span> - val_loss: <span class="number">0.4998</span> - val_accuracy: <span class="number">0.8550</span> - val_AUC: <span class="number">0.9233</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">26</span>s <span class="number">26</span>ms/step - loss: <span class="number">0.0526</span> - accuracy: <span class="number">0.9865</span> - AUC: <span class="number">0.9977</span> - val_loss: <span class="number">0.6463</span> - val_accuracy: <span class="number">0.8462</span> - val_AUC: <span class="number">0.9138</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"AUC"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/6-1-2-train.jpg" alt></p>
<h3 id="model-zi-lei-hua-chuang-jian-zi-ding-yi-mo-xing">Model子类化创建自定义模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先自定义一个残差模块，为自定义Layer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResBlock</span><span class="params">(layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kernel_size, **kwargs)</span>:</span></span><br><span class="line">        super(ResBlock, self).__init__(**kwargs)</span><br><span class="line">        self.kernel_size = kernel_size</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self,input_shape)</span>:</span></span><br><span class="line">        self.conv1 = layers.Conv1D(filters=<span class="number">64</span>,kernel_size=self.kernel_size,</span><br><span class="line">                                   activation = <span class="string">"relu"</span>,padding=<span class="string">"same"</span>)</span><br><span class="line">        self.conv2 = layers.Conv1D(filters=<span class="number">32</span>,kernel_size=self.kernel_size,</span><br><span class="line">                                   activation = <span class="string">"relu"</span>,padding=<span class="string">"same"</span>)</span><br><span class="line">        self.conv3 = layers.Conv1D(filters=input_shape[<span class="number">-1</span>],</span><br><span class="line">                                   kernel_size=self.kernel_size,activation = <span class="string">"relu"</span>,padding=<span class="string">"same"</span>)</span><br><span class="line">        self.maxpool = layers.MaxPool1D(<span class="number">2</span>)</span><br><span class="line">        super(ResBlock,self).build(input_shape) <span class="comment"># 相当于设置self.built = True</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = self.conv1(inputs)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = layers.Add()([inputs,x])</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#如果要让自定义的Layer通过Functional API 组合成模型时可以序列化，需要自定义get_config方法。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        config = super(ResBlock, self).get_config()</span><br><span class="line">        config.update(&#123;<span class="string">'kernel_size'</span>: self.kernel_size&#125;)</span><br><span class="line">        <span class="keyword">return</span> config</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试ResBlock</span></span><br><span class="line">resblock = ResBlock(kernel_size = <span class="number">3</span>)</span><br><span class="line">resblock.build(input_shape = (<span class="literal">None</span>,<span class="number">200</span>,<span class="number">7</span>))</span><br><span class="line">resblock.compute_output_shape(input_shape=(<span class="literal">None</span>,<span class="number">200</span>,<span class="number">7</span>))</span><br><span class="line"><span class="comment"># TensorShape([None, 100, 7])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义模型，实际上也可以使用Sequential或者FunctionalAPI</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImdbModel</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(ImdbModel, self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self,input_shape)</span>:</span></span><br><span class="line">        self.embedding = layers.Embedding(MAX_WORDS,<span class="number">7</span>)</span><br><span class="line">        self.block1 = ResBlock(<span class="number">7</span>)</span><br><span class="line">        self.block2 = ResBlock(<span class="number">5</span>)</span><br><span class="line">        self.dense = layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">        super(ImdbModel,self).build(input_shape)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x = self.block1(x)</span><br><span class="line">        x = self.block2(x)</span><br><span class="line">        x = layers.Flatten()(x)</span><br><span class="line">        x = self.dense(x)</span><br><span class="line">        <span class="keyword">return</span>(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = ImdbModel()</span><br><span class="line">model.build(input_shape =(<span class="literal">None</span>,<span class="number">200</span>))</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'Nadam'</span>,</span><br><span class="line">            loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>,<span class="string">"AUC"</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">Model: "imdb_model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        multiple                  70000     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">res_block (ResBlock)         multiple                  19143     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">res<span class="emphasis">_block_</span>1 (ResBlock)       multiple                  13703     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                multiple                  351       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 103,197</span><br><span class="line">Trainable params: 103,197</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/Model%E5%AD%90%E7%B1%BB%E5%8C%96%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line">logdir = <span class="string">"./tflogs/keras_model/"</span> + datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=<span class="number">1</span>)</span><br><span class="line">history = model.fit(ds_train,validation_data = ds_test,</span><br><span class="line">                    epochs = <span class="number">6</span>,callbacks=[tensorboard_callback])</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">Epoch <span class="number">1</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">47</span>s <span class="number">47</span>ms/step - loss: <span class="number">0.5629</span> - accuracy: <span class="number">0.6618</span> - AUC: <span class="number">0.7548</span> - val_loss: <span class="number">0.3422</span> - val_accuracy: <span class="number">0.8510</span> - val_AUC: <span class="number">0.9286</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">43</span>s <span class="number">43</span>ms/step - loss: <span class="number">0.2648</span> - accuracy: <span class="number">0.8903</span> - AUC: <span class="number">0.9576</span> - val_loss: <span class="number">0.3276</span> - val_accuracy: <span class="number">0.8650</span> - val_AUC: <span class="number">0.9410</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">42</span>s <span class="number">42</span>ms/step - loss: <span class="number">0.1573</span> - accuracy: <span class="number">0.9439</span> - AUC: <span class="number">0.9846</span> - val_loss: <span class="number">0.3861</span> - val_accuracy: <span class="number">0.8682</span> - val_AUC: <span class="number">0.9390</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">42</span>s <span class="number">42</span>ms/step - loss: <span class="number">0.0849</span> - accuracy: <span class="number">0.9706</span> - AUC: <span class="number">0.9950</span> - val_loss: <span class="number">0.5324</span> - val_accuracy: <span class="number">0.8616</span> - val_AUC: <span class="number">0.9292</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">43</span>s <span class="number">43</span>ms/step - loss: <span class="number">0.0393</span> - accuracy: <span class="number">0.9876</span> - AUC: <span class="number">0.9986</span> - val_loss: <span class="number">0.7693</span> - val_accuracy: <span class="number">0.8566</span> - val_AUC: <span class="number">0.9132</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">44</span>s <span class="number">44</span>ms/step - loss: <span class="number">0.0222</span> - accuracy: <span class="number">0.9926</span> - AUC: <span class="number">0.9994</span> - val_loss: <span class="number">0.9328</span> - val_accuracy: <span class="number">0.8584</span> - val_AUC: <span class="number">0.9052</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"AUC"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/6-1-3-fit%E6%A8%A1%E5%9E%8B.jpg" alt></p>
<h2 id="xun-lian-mo-xing-de-3-chong-fang-fa">训练模型的3种方法</h2>
<p>模型的训练主要有：</p>
<ol>
<li>内置fit方法</li>
<li>内置tran_on_batch方法</li>
<li>自定义训练循环。</li>
</ol>
<p>注：fit_generator方法在tf.keras中不推荐使用，其功能已经被fit包含。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> * </span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">MAX_LEN = <span class="number">300</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()</span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)</span><br><span class="line">x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)</span><br><span class="line"></span><br><span class="line">MAX_WORDS = x_train.max()+<span class="number">1</span></span><br><span class="line">CAT_NUM = y_train.max()+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">   </span><br><span class="line">ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<h3 id="nei-zhi-fit-fang-fa">内置fit方法</h3>
<p>该方法功能非常强大, 支持对numpy array, tf.data.Dataset以及 Python generator数据进行训练。</p>
<p>并且可以通过设置回调函数实现对训练过程的复杂控制逻辑。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    model = models.Sequential()</span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compile_model</span><span class="params">(model)</span>:</span></span><br><span class="line">    model.compile(optimizer=optimizers.Nadam(),</span><br><span class="line">                loss=losses.SparseCategoricalCrossentropy(),</span><br><span class="line">                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(<span class="number">5</span>)]) </span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"> </span><br><span class="line">model = create_model()</span><br><span class="line">model.summary()</span><br><span class="line">model = compile_model(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (None, 300, 7)            216874    </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d (Conv1D)              (None, 296, 64)           2304      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max_pooling1d (MaxPooling1D) (None, 148, 64)           0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d_1 (Conv1D)            (None, 146, 32)           6176      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max<span class="emphasis">_pooling1d_</span>1 (MaxPooling1 (None, 73, 32)            0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">flatten (Flatten)            (None, 2336)              0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 46)                107502    </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 332,856</span><br><span class="line">Trainable params: 332,856</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit(ds_train,validation_data = ds_test,epochs = <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">Train <span class="keyword">for</span> <span class="number">281</span> steps, validate <span class="keyword">for</span> <span class="number">71</span> steps</span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">11</span>s <span class="number">37</span>ms/step - loss: <span class="number">2.0231</span> - sparse_categorical_accuracy: <span class="number">0.4636</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7450</span> - val_loss: <span class="number">1.7346</span> - val_sparse_categorical_accuracy: <span class="number">0.5534</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7560</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">9</span>s <span class="number">31</span>ms/step - loss: <span class="number">1.5079</span> - sparse_categorical_accuracy: <span class="number">0.6091</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7901</span> - val_loss: <span class="number">1.5475</span> - val_sparse_categorical_accuracy: <span class="number">0.6109</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7792</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">9</span>s <span class="number">33</span>ms/step - loss: <span class="number">1.2204</span> - sparse_categorical_accuracy: <span class="number">0.6823</span> - sparse_top_k_categorical_accuracy: <span class="number">0.8448</span> - val_loss: <span class="number">1.5455</span> - val_sparse_categorical_accuracy: <span class="number">0.6367</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8001</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">9</span>s <span class="number">33</span>ms/step - loss: <span class="number">0.9382</span> - sparse_categorical_accuracy: <span class="number">0.7543</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9075</span> - val_loss: <span class="number">1.6780</span> - val_sparse_categorical_accuracy: <span class="number">0.6398</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8032</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">10</span>s <span class="number">34</span>ms/step - loss: <span class="number">0.6791</span> - sparse_categorical_accuracy: <span class="number">0.8255</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9513</span> - val_loss: <span class="number">1.9426</span> - val_sparse_categorical_accuracy: <span class="number">0.6376</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7956</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">9</span>s <span class="number">33</span>ms/step - loss: <span class="number">0.5063</span> - sparse_categorical_accuracy: <span class="number">0.8762</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9716</span> - val_loss: <span class="number">2.2141</span> - val_sparse_categorical_accuracy: <span class="number">0.6291</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7947</span></span><br><span class="line">Epoch <span class="number">7</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">10</span>s <span class="number">37</span>ms/step - loss: <span class="number">0.4031</span> - sparse_categorical_accuracy: <span class="number">0.9050</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9817</span> - val_loss: <span class="number">2.4126</span> - val_sparse_categorical_accuracy: <span class="number">0.6264</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7947</span></span><br><span class="line">Epoch <span class="number">8</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">10</span>s <span class="number">35</span>ms/step - loss: <span class="number">0.3380</span> - sparse_categorical_accuracy: <span class="number">0.9205</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9881</span> - val_loss: <span class="number">2.5366</span> - val_sparse_categorical_accuracy: <span class="number">0.6242</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7974</span></span><br><span class="line">Epoch <span class="number">9</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">10</span>s <span class="number">36</span>ms/step - loss: <span class="number">0.2921</span> - sparse_categorical_accuracy: <span class="number">0.9299</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9909</span> - val_loss: <span class="number">2.6564</span> - val_sparse_categorical_accuracy: <span class="number">0.6242</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7983</span></span><br><span class="line">Epoch <span class="number">10</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">9</span>s <span class="number">30</span>ms/step - loss: <span class="number">0.2613</span> - sparse_categorical_accuracy: <span class="number">0.9334</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9947</span> - val_loss: <span class="number">2.7365</span> - val_sparse_categorical_accuracy: <span class="number">0.6220</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8005</span></span><br></pre></td></tr></table></figure>
<h3 id="nei-zhi-train-on-batch-fang-fa">内置train_on_batch方法</h3>
<p>该内置方法相比较fit方法更加灵活，可以不通过回调函数而直接在批次层次上更加精细地控制训练的过程。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = models.Sequential()</span><br><span class="line"></span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compile_model</span><span class="params">(model)</span>:</span></span><br><span class="line">    model.compile(optimizer=optimizers.Nadam(),</span><br><span class="line">                loss=losses.SparseCategoricalCrossentropy(),</span><br><span class="line">                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(<span class="number">5</span>)]) </span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"> </span><br><span class="line">model = create_model()</span><br><span class="line">model.summary()</span><br><span class="line">model = compile_model(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (None, 300, 7)            216874    </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d (Conv1D)              (None, 296, 64)           2304      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max_pooling1d (MaxPooling1D) (None, 148, 64)           0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d_1 (Conv1D)            (None, 146, 32)           6176      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max<span class="emphasis">_pooling1d_</span>1 (MaxPooling1 (None, 73, 32)            0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">flatten (Flatten)            (None, 2336)              0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 46)                107502    </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 332,856</span><br><span class="line">Trainable params: 332,856</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,ds_train,ds_valid,epoches)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epoches+<span class="number">1</span>):</span><br><span class="line">        model.reset_metrics()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 在后期降低学习率</span></span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">5</span>:</span><br><span class="line">            model.optimizer.lr.assign(model.optimizer.lr/<span class="number">2.0</span>)</span><br><span class="line">            tf.print(<span class="string">"Lowering optimizer Learning Rate...\n\n"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> ds_train:</span><br><span class="line">            train_result = model.train_on_batch(x, y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> ds_valid:</span><br><span class="line">            valid_result = model.test_on_batch(x, y,reset_metrics=<span class="literal">False</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">1</span> ==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch = "</span>,epoch)</span><br><span class="line">            print(<span class="string">"train:"</span>,dict(zip(model.metrics_names,train_result)))</span><br><span class="line">            print(<span class="string">"valid:"</span>,dict(zip(model.metrics_names,valid_result)))</span><br><span class="line">            print(<span class="string">""</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_model(model,ds_train,ds_test,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">================================================================================13:09:19</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">1</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.82411176</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.77272725</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.8636364</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">1.9265995</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.5743544</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.75779164</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:09:27</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">2</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.6006621</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.90909094</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.95454544</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">1.844159</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6126447</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.7920748</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:09:35</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">3</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.36935613</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.90909094</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.95454544</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">2.163433</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.63312554</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.8045414</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:09:42</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">4</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.2304088</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.90909094</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">2.8911984</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6344613</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.7978629</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">Lowering</span> <span class="string">optimizer</span> <span class="string">Learning</span> <span class="string">Rate...</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:09:51</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">5</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.111194365</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.95454544</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">3.6431572</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6295637</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.7978629</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:09:59</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">6</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.07741702</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.95454544</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">4.074161</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6255565</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.794301</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:10:07</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">7</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.056113098</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">4.4461513</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6273375</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.79652715</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:10:17</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">8</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.043448802</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">4.7687583</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6224399</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.79741764</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:10:26</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">9</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.035002146</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">5.130505</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6175423</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.794301</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:10:34</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">10</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.028303564</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">5.4559293</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6148709</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.7947462</span><span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="zi-ding-yi-xun-lian-xun-huan">自定义训练循环</h3>
<p>自定义训练循环无需编译模型，直接利用优化器根据损失函数反向传播迭代参数，拥有最高的灵活性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    model = models.Sequential()</span><br><span class="line"></span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line">model = create_model()</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = optimizers.Nadam()</span><br><span class="line">loss_func = losses.SparseCategoricalCrossentropy()</span><br><span class="line"></span><br><span class="line">train_loss = metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_metric = metrics.SparseCategoricalAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">valid_loss = metrics.Mean(name=<span class="string">'valid_loss'</span>)</span><br><span class="line">valid_metric = metrics.SparseCategoricalAccuracy(name=<span class="string">'valid_accuracy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features,training = <span class="literal">True</span>)</span><br><span class="line">        loss = loss_func(labels, predictions)</span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss.update_state(loss)</span><br><span class="line">    train_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">valid_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    predictions = model(features)</span><br><span class="line">    batch_loss = loss_func(labels, predictions)</span><br><span class="line">    valid_loss.update_state(batch_loss)</span><br><span class="line">    valid_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,ds_train,ds_valid,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_train:</span><br><span class="line">            train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_valid:</span><br><span class="line">            valid_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        logs = <span class="string">'Epoch=&#123;&#125;,Loss:&#123;&#125;,Accuracy:&#123;&#125;,Valid Loss:&#123;&#125;,Valid Accuracy:&#123;&#125;'</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">1</span> ==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(tf.strings.format(logs,</span><br><span class="line">            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))</span><br><span class="line">            tf.print(<span class="string">""</span>)</span><br><span class="line">            </span><br><span class="line">        train_loss.reset_states()</span><br><span class="line">        valid_loss.reset_states()</span><br><span class="line">        train_metric.reset_states()</span><br><span class="line">        valid_metric.reset_states()</span><br><span class="line"></span><br><span class="line">train_model(model,ds_train,ds_test,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">03</span></span><br><span class="line">Epoch=<span class="number">1</span>,Loss:<span class="number">2.02051544</span>,Accuracy:<span class="number">0.460253835</span>,Valid Loss:<span class="number">1.75700927</span>,Valid Accuracy:<span class="number">0.536954582</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">09</span></span><br><span class="line">Epoch=<span class="number">2</span>,Loss:<span class="number">1.510795</span>,Accuracy:<span class="number">0.610665798</span>,Valid Loss:<span class="number">1.55349839</span>,Valid Accuracy:<span class="number">0.616206586</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">17</span></span><br><span class="line">Epoch=<span class="number">3</span>,Loss:<span class="number">1.19221532</span>,Accuracy:<span class="number">0.696170092</span>,Valid Loss:<span class="number">1.52315605</span>,Valid Accuracy:<span class="number">0.651380241</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">23</span></span><br><span class="line">Epoch=<span class="number">4</span>,Loss:<span class="number">0.90101546</span>,Accuracy:<span class="number">0.766310394</span>,Valid Loss:<span class="number">1.68327653</span>,Valid Accuracy:<span class="number">0.648263574</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">30</span></span><br><span class="line">Epoch=<span class="number">5</span>,Loss:<span class="number">0.655430496</span>,Accuracy:<span class="number">0.831329346</span>,Valid Loss:<span class="number">1.90872383</span>,Valid Accuracy:<span class="number">0.641139805</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">37</span></span><br><span class="line">Epoch=<span class="number">6</span>,Loss:<span class="number">0.492730737</span>,Accuracy:<span class="number">0.877866864</span>,Valid Loss:<span class="number">2.09966016</span>,Valid Accuracy:<span class="number">0.63223511</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">44</span></span><br><span class="line">Epoch=<span class="number">7</span>,Loss:<span class="number">0.391238362</span>,Accuracy:<span class="number">0.904030263</span>,Valid Loss:<span class="number">2.27431226</span>,Valid Accuracy:<span class="number">0.625111282</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">51</span></span><br><span class="line">Epoch=<span class="number">8</span>,Loss:<span class="number">0.327761739</span>,Accuracy:<span class="number">0.922066331</span>,Valid Loss:<span class="number">2.42568827</span>,Valid Accuracy:<span class="number">0.617542326</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">58</span></span><br><span class="line">Epoch=<span class="number">9</span>,Loss:<span class="number">0.285573095</span>,Accuracy:<span class="number">0.930527747</span>,Valid Loss:<span class="number">2.55942106</span>,Valid Accuracy:<span class="number">0.612644672</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">13</span>:<span class="number">05</span></span><br><span class="line">Epoch=<span class="number">10</span>,Loss:<span class="number">0.255482465</span>,Accuracy:<span class="number">0.936094403</span>,Valid Loss:<span class="number">2.67789412</span>,Valid Accuracy:<span class="number">0.612199485</span></span><br></pre></td></tr></table></figure>
<h2 id="shi-yong-dan-gpu-xun-lian-mo-xing">使用单GPU训练模型</h2>
<p>深度学习的训练过程常常非常耗时，一个模型训练几个小时是家常便饭，训练几天也是常有的事情，有时候甚至要训练几十天。训练过程的耗时主要来自于两个部分，一部分来自数据准备，另一部分来自参数迭代。</p>
<p>当数据准备过程还是模型训练时间的主要瓶颈时，我们可以使用更多进程来准备数据。当参数迭代过程成为训练时间的主要瓶颈时，我们通常的方法是应用GPU或者Google的TPU来进行加速。</p>
<p>无论是内置fit方法，还是自定义训练循环，从CPU切换成单GPU训练模型都是非常方便的，无需更改任何代码。当存在可用的GPU时，如果不特意指定device，tensorflow会自动优先选择使用GPU来创建张量和执行张量计算。</p>
<p>但如果是在公司或者学校实验室的服务器环境，存在多个GPU和多个使用者时，为了不让单个同学的任务占用全部GPU资源导致其他同学无法使用（tensorflow默认获取全部GPU的全部内存资源权限，但实际上只使用一个GPU的部分资源），通常会在开头增加以下几行代码以控制每个任务使用的GPU编号和显存大小，以便其他同学也能够同时训练模型。在Colab笔记本中：修改-&gt;笔记本设置-&gt;硬件加速器 中选择 GPU</p>
<p>可通过以下colab链接测试效果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%tensorflow_version <span class="number">2.</span>x</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.__version__)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> * </span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure>
<h3 id="gpu-she-zhi">GPU设置</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gpus = tf.config.list_physical_devices(<span class="string">"GPU"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">    gpu0 = gpus[<span class="number">0</span>] <span class="comment">#如果有多个GPU，仅使用第0个GPU</span></span><br><span class="line">    tf.config.experimental.set_memory_growth(gpu0, <span class="literal">True</span>) <span class="comment">#设置GPU显存用量按需使用</span></span><br><span class="line">    <span class="comment"># 或者也可以设置GPU显存为固定使用量(例如：4G)</span></span><br><span class="line">    <span class="comment">#tf.config.experimental.set_virtual_device_configuration(gpu0,</span></span><br><span class="line">    <span class="comment">#    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) </span></span><br><span class="line">    tf.config.set_visible_devices([gpu0],<span class="string">"GPU"</span>)</span><br></pre></td></tr></table></figure>
<p>比较GPU和CPU的计算速度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">printbar()</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">"/gpu:0"</span>):</span><br><span class="line">    tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line">    a = tf.random.uniform((<span class="number">10000</span>,<span class="number">100</span>),minval = <span class="number">0</span>,maxval = <span class="number">3.0</span>)</span><br><span class="line">    b = tf.random.uniform((<span class="number">100</span>,<span class="number">100000</span>),minval = <span class="number">0</span>,maxval = <span class="number">3.0</span>)</span><br><span class="line">    c = a@b</span><br><span class="line">    tf.print(tf.reduce_sum(tf.reduce_sum(c,axis = <span class="number">0</span>),axis=<span class="number">0</span>))</span><br><span class="line">printbar()</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">01</span></span><br><span class="line"><span class="number">2.24953778e+11</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">01</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">printbar()</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">"/cpu:0"</span>):</span><br><span class="line">    tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line">    a = tf.random.uniform((<span class="number">10000</span>,<span class="number">100</span>),minval = <span class="number">0</span>,maxval = <span class="number">3.0</span>)</span><br><span class="line">    b = tf.random.uniform((<span class="number">100</span>,<span class="number">100000</span>),minval = <span class="number">0</span>,maxval = <span class="number">3.0</span>)</span><br><span class="line">    c = a@b</span><br><span class="line">    tf.print(tf.reduce_sum(tf.reduce_sum(c,axis = <span class="number">0</span>),axis=<span class="number">0</span>))</span><br><span class="line">printbar()</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">34</span></span><br><span class="line"><span class="number">2.24953795e+11</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">40</span></span><br></pre></td></tr></table></figure>
<h3 id="zhun-bei-shu-ju-10">准备数据</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">MAX_LEN = <span class="number">300</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()</span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)</span><br><span class="line">x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)</span><br><span class="line"></span><br><span class="line">MAX_WORDS = x_train.max()+<span class="number">1</span></span><br><span class="line">CAT_NUM = y_train.max()+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">   </span><br><span class="line">ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing-11">定义模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    model = models.Sequential()</span><br><span class="line"></span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line">model = create_model()</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (None, 300, 7)            216874    </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d (Conv1D)              (None, 296, 64)           2304      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max_pooling1d (MaxPooling1D) (None, 148, 64)           0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d_1 (Conv1D)            (None, 146, 32)           6176      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max<span class="emphasis">_pooling1d_</span>1 (MaxPooling1 (None, 73, 32)            0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">flatten (Flatten)            (None, 2336)              0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 46)                107502    </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 332,856</span><br><span class="line">Trainable params: 332,856</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<h3 id="xun-lian-mo-xing-10">训练模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = optimizers.Nadam()</span><br><span class="line">loss_func = losses.SparseCategoricalCrossentropy()</span><br><span class="line"></span><br><span class="line">train_loss = metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_metric = metrics.SparseCategoricalAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">valid_loss = metrics.Mean(name=<span class="string">'valid_loss'</span>)</span><br><span class="line">valid_metric = metrics.SparseCategoricalAccuracy(name=<span class="string">'valid_accuracy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features,training = <span class="literal">True</span>)</span><br><span class="line">        loss = loss_func(labels, predictions)</span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss.update_state(loss)</span><br><span class="line">    train_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">valid_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    predictions = model(features)</span><br><span class="line">    batch_loss = loss_func(labels, predictions)</span><br><span class="line">    valid_loss.update_state(batch_loss)</span><br><span class="line">    valid_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,ds_train,ds_valid,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_train:</span><br><span class="line">            train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_valid:</span><br><span class="line">            valid_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        logs = <span class="string">'Epoch=&#123;&#125;,Loss:&#123;&#125;,Accuracy:&#123;&#125;,Valid Loss:&#123;&#125;,Valid Accuracy:&#123;&#125;'</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">1</span> ==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(tf.strings.format(logs,</span><br><span class="line">            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))</span><br><span class="line">            tf.print(<span class="string">""</span>)</span><br><span class="line">            </span><br><span class="line">        train_loss.reset_states()</span><br><span class="line">        valid_loss.reset_states()</span><br><span class="line">        train_metric.reset_states()</span><br><span class="line">        valid_metric.reset_states()</span><br><span class="line"></span><br><span class="line">train_model(model,ds_train,ds_test,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">26</span></span><br><span class="line">Epoch=<span class="number">1</span>,Loss:<span class="number">1.96735072</span>,Accuracy:<span class="number">0.489200622</span>,Valid Loss:<span class="number">1.64124215</span>,Valid Accuracy:<span class="number">0.582813919</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">28</span></span><br><span class="line">Epoch=<span class="number">2</span>,Loss:<span class="number">1.4640888</span>,Accuracy:<span class="number">0.624805152</span>,Valid Loss:<span class="number">1.5559175</span>,Valid Accuracy:<span class="number">0.607747078</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">30</span></span><br><span class="line">Epoch=<span class="number">3</span>,Loss:<span class="number">1.20681274</span>,Accuracy:<span class="number">0.68581605</span>,Valid Loss:<span class="number">1.58494771</span>,Valid Accuracy:<span class="number">0.622439921</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">31</span></span><br><span class="line">Epoch=<span class="number">4</span>,Loss:<span class="number">0.937500894</span>,Accuracy:<span class="number">0.75361836</span>,Valid Loss:<span class="number">1.77466083</span>,Valid Accuracy:<span class="number">0.621994674</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">33</span></span><br><span class="line">Epoch=<span class="number">5</span>,Loss:<span class="number">0.693960547</span>,Accuracy:<span class="number">0.822199941</span>,Valid Loss:<span class="number">2.00267363</span>,Valid Accuracy:<span class="number">0.6197685</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">35</span></span><br><span class="line">Epoch=<span class="number">6</span>,Loss:<span class="number">0.519614</span>,Accuracy:<span class="number">0.870296121</span>,Valid Loss:<span class="number">2.23463202</span>,Valid Accuracy:<span class="number">0.613980412</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">37</span></span><br><span class="line">Epoch=<span class="number">7</span>,Loss:<span class="number">0.408562034</span>,Accuracy:<span class="number">0.901246965</span>,Valid Loss:<span class="number">2.46969271</span>,Valid Accuracy:<span class="number">0.612199485</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">39</span></span><br><span class="line">Epoch=<span class="number">8</span>,Loss:<span class="number">0.339028627</span>,Accuracy:<span class="number">0.920062363</span>,Valid Loss:<span class="number">2.68585229</span>,Valid Accuracy:<span class="number">0.615316093</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">41</span></span><br><span class="line">Epoch=<span class="number">9</span>,Loss:<span class="number">0.293798745</span>,Accuracy:<span class="number">0.92930305</span>,Valid Loss:<span class="number">2.88995624</span>,Valid Accuracy:<span class="number">0.613535166</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">43</span></span><br><span class="line">Epoch=<span class="number">10</span>,Loss:<span class="number">0.263130337</span>,Accuracy:<span class="number">0.936651051</span>,Valid Loss:<span class="number">3.09705234</span>,Valid Accuracy:<span class="number">0.612644672</span></span><br></pre></td></tr></table></figure>
<h2 id="shi-yong-duo-gpu-xun-lian-mo-xing">使用多GPU训练模型</h2>
<p>如果使用多GPU训练模型，推荐使用内置fit方法，较为方便，仅需添加2行代码。</p>
<p>在Colab笔记本中：修改-&gt;笔记本设置-&gt;硬件加速器 中选择 GPU</p>
<p>MirroredStrategy过程简介：</p>
<ul>
<li>训练开始前，该策略在所有 N 个计算设备上均各复制一份完整的模型；</li>
<li>每次训练传入一个批次的数据时，将数据分成 N 份，分别传入 N 个计算设备（即数据并行）；</li>
<li>N 个计算设备使用本地变量（镜像变量）分别计算自己所获得的部分数据的梯度；</li>
<li>使用分布式计算的 All-reduce 操作，在计算设备间高效交换梯度数据并进行求和，使得最终每个设备都有了所有设备的梯度之和；</li>
<li>使用梯度求和的结果更新本地变量（镜像变量）；</li>
<li>当所有设备均更新本地变量后，进行下一轮训练（即该并行策略是同步的）。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%tensorflow_version <span class="number">2.</span>x</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.__version__)</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#此处在colab上使用1个GPU模拟出两个逻辑GPU进行多GPU训练</span></span><br><span class="line">gpus = tf.config.experimental.list_physical_devices(<span class="string">'GPU'</span>)</span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">    <span class="comment"># 设置两个逻辑GPU模拟多GPU训练</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        tf.config.experimental.set_virtual_device_configuration(gpus[<span class="number">0</span>],</span><br><span class="line">            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=<span class="number">1024</span>),</span><br><span class="line">             tf.config.experimental.VirtualDeviceConfiguration(memory_limit=<span class="number">1024</span>)])</span><br><span class="line">        logical_gpus = tf.config.experimental.list_logical_devices(<span class="string">'GPU'</span>)</span><br><span class="line">        print(len(gpus), <span class="string">"Physical GPU,"</span>, len(logical_gpus), <span class="string">"Logical GPUs"</span>)</span><br><span class="line">    <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br></pre></td></tr></table></figure>
<h3 id="zhun-bei-shu-ju-11">准备数据</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">MAX_LEN = <span class="number">300</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()</span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)</span><br><span class="line">x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)</span><br><span class="line"></span><br><span class="line">MAX_WORDS = x_train.max()+<span class="number">1</span></span><br><span class="line">CAT_NUM = y_train.max()+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">   </span><br><span class="line">ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing-12">定义模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    model = models.Sequential()</span><br><span class="line"></span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compile_model</span><span class="params">(model)</span>:</span></span><br><span class="line">    model.compile(optimizer=optimizers.Nadam(),</span><br><span class="line">                loss=losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(<span class="number">5</span>)]) </span><br><span class="line">    <span class="keyword">return</span>(model)</span><br></pre></td></tr></table></figure>
<h3 id="xun-lian-mo-xing-11">训练模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#增加以下两行代码</span></span><br><span class="line">strategy = tf.distribute.MirroredStrategy()  </span><br><span class="line"><span class="keyword">with</span> strategy.scope(): </span><br><span class="line">    model = create_model()</span><br><span class="line">    model.summary()</span><br><span class="line">    model = compile_model(model)</span><br><span class="line">    </span><br><span class="line">history = model.fit(ds_train,validation_data = ds_test,epochs = <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="built_in">WARNING</span>:tensorflow:NCCL <span class="keyword">is</span> <span class="keyword">not</span> supported <span class="keyword">when</span> <span class="keyword">using</span> virtual GPUs, fallingback <span class="keyword">to</span> reduction <span class="keyword">to</span> one device</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:<span class="keyword">Using</span> MirroredStrategy <span class="keyword">with</span> devices (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>)</span><br><span class="line">Model: "sequential"</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="keyword">type</span>)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (<span class="keyword">None</span>, <span class="number">300</span>, <span class="number">7</span>)            <span class="number">216874</span>    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv1d (Conv1D)              (<span class="keyword">None</span>, <span class="number">296</span>, <span class="number">64</span>)           <span class="number">2304</span>      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling1d (MaxPooling1D) (<span class="keyword">None</span>, <span class="number">148</span>, <span class="number">64</span>)           <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv1d_1 (Conv1D)            (<span class="keyword">None</span>, <span class="number">146</span>, <span class="number">32</span>)           <span class="number">6176</span>      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling1d_1 (MaxPooling1 (<span class="keyword">None</span>, <span class="number">73</span>, <span class="number">32</span>)            <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten (Flatten)            (<span class="keyword">None</span>, <span class="number">2336</span>)              <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                (<span class="keyword">None</span>, <span class="number">46</span>)                <span class="number">107502</span>    </span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">332</span>,<span class="number">856</span></span><br><span class="line">Trainable params: <span class="number">332</span>,<span class="number">856</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line">Train <span class="keyword">for</span> <span class="number">281</span> steps, <span class="keyword">validate</span> <span class="keyword">for</span> <span class="number">71</span> steps</span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">10</span></span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">15</span>s <span class="number">53</span>ms/step - loss: <span class="number">2.0270</span> - sparse_categorical_accuracy: <span class="number">0.4653</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7481</span> - val_loss: <span class="number">1.7517</span> - val_sparse_categorical_accuracy: <span class="number">0.5481</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7578</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">1.5206</span> - sparse_categorical_accuracy: <span class="number">0.6045</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7938</span> - val_loss: <span class="number">1.5715</span> - val_sparse_categorical_accuracy: <span class="number">0.5993</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7983</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">1.2178</span> - sparse_categorical_accuracy: <span class="number">0.6843</span> - sparse_top_k_categorical_accuracy: <span class="number">0.8547</span> - val_loss: <span class="number">1.5232</span> - val_sparse_categorical_accuracy: <span class="number">0.6327</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8112</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">13</span>ms/step - loss: <span class="number">0.9127</span> - sparse_categorical_accuracy: <span class="number">0.7648</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9113</span> - val_loss: <span class="number">1.6527</span> - val_sparse_categorical_accuracy: <span class="number">0.6296</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8201</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.6606</span> - sparse_categorical_accuracy: <span class="number">0.8321</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9525</span> - val_loss: <span class="number">1.8791</span> - val_sparse_categorical_accuracy: <span class="number">0.6158</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8219</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.4919</span> - sparse_categorical_accuracy: <span class="number">0.8799</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9725</span> - val_loss: <span class="number">2.1282</span> - val_sparse_categorical_accuracy: <span class="number">0.6037</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8112</span></span><br><span class="line">Epoch <span class="number">7</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.3947</span> - sparse_categorical_accuracy: <span class="number">0.9051</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9814</span> - val_loss: <span class="number">2.3033</span> - val_sparse_categorical_accuracy: <span class="number">0.6046</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8094</span></span><br><span class="line">Epoch <span class="number">8</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.3335</span> - sparse_categorical_accuracy: <span class="number">0.9207</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9863</span> - val_loss: <span class="number">2.4255</span> - val_sparse_categorical_accuracy: <span class="number">0.5993</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8099</span></span><br><span class="line">Epoch <span class="number">9</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.2919</span> - sparse_categorical_accuracy: <span class="number">0.9304</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9911</span> - val_loss: <span class="number">2.5571</span> - val_sparse_categorical_accuracy: <span class="number">0.6020</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8126</span></span><br><span class="line">Epoch <span class="number">10</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.2617</span> - sparse_categorical_accuracy: <span class="number">0.9342</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9937</span> - val_loss: <span class="number">2.6700</span> - val_sparse_categorical_accuracy: <span class="number">0.6077</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8148</span></span><br><span class="line">CPU times: <span class="keyword">user</span> <span class="number">1</span>min <span class="number">2</span>s, sys: <span class="number">8.59</span> s, total: <span class="number">1</span>min <span class="number">10</span>s</span><br><span class="line">Wall <span class="type">time</span>: <span class="number">58.5</span> s</span><br></pre></td></tr></table></figure>
<h2 id="shi-yong-tpu-xun-lian-mo-xing">使用TPU训练模型</h2>
<p>如果想尝试使用Google Colab上的TPU来训练模型，也是非常方便，仅需添加6行代码。在Colab笔记本中：修改-&gt;笔记本设置-&gt;硬件加速器中选择 TPU</p>
<p>可通过以下colab链接测试效果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%tensorflow_version <span class="number">2.</span>x</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.__version__)</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<h3 id="zhun-bei-shu-ju-12">准备数据</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">MAX_LEN = <span class="number">300</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()</span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)</span><br><span class="line">x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)</span><br><span class="line"></span><br><span class="line">MAX_WORDS = x_train.max()+<span class="number">1</span></span><br><span class="line">CAT_NUM = y_train.max()+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">   </span><br><span class="line">ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing-13">定义模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    model = models.Sequential()</span><br><span class="line"></span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compile_model</span><span class="params">(model)</span>:</span></span><br><span class="line">    model.compile(optimizer=optimizers.Nadam(),</span><br><span class="line">                loss=losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(<span class="number">5</span>)]) </span><br><span class="line">    <span class="keyword">return</span>(model)</span><br></pre></td></tr></table></figure>
<h3 id="xun-lian-mo-xing-12">训练模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#增加以下6行代码</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=<span class="string">'grpc://'</span> + os.environ[<span class="string">'COLAB_TPU_ADDR'</span>])</span><br><span class="line">tf.config.experimental_connect_to_cluster(resolver)</span><br><span class="line">tf.tpu.experimental.initialize_tpu_system(resolver)</span><br><span class="line">strategy = tf.distribute.experimental.TPUStrategy(resolver)</span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">    model = create_model()</span><br><span class="line">    model.summary()</span><br><span class="line">    model = compile_model(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">WARNING:tensorflow:TPU system <span class="number">10.26</span><span class="number">.134</span><span class="number">.242</span>:<span class="number">8470</span> has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.</span><br><span class="line">WARNING:tensorflow:TPU system <span class="number">10.26</span><span class="number">.134</span><span class="number">.242</span>:<span class="number">8470</span> has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.</span><br><span class="line">INFO:tensorflow:Initializing the TPU system: <span class="number">10.26</span><span class="number">.134</span><span class="number">.242</span>:<span class="number">8470</span></span><br><span class="line">INFO:tensorflow:Initializing the TPU system: <span class="number">10.26</span><span class="number">.134</span><span class="number">.242</span>:<span class="number">8470</span></span><br><span class="line">INFO:tensorflow:Clearing <span class="keyword">out</span> eager caches</span><br><span class="line">INFO:tensorflow:Clearing <span class="keyword">out</span> eager caches</span><br><span class="line">INFO:tensorflow:Finished initializing TPU system.</span><br><span class="line">INFO:tensorflow:Finished initializing TPU system.</span><br><span class="line">INFO:tensorflow:Found TPU system:</span><br><span class="line">INFO:tensorflow:Found TPU system:</span><br><span class="line">INFO:tensorflow:*** Num TPU Cores: <span class="number">8</span></span><br><span class="line">INFO:tensorflow:*** Num TPU Cores: <span class="number">8</span></span><br><span class="line">INFO:tensorflow:*** Num TPU Workers: <span class="number">1</span></span><br><span class="line">INFO:tensorflow:*** Num TPU Workers: <span class="number">1</span></span><br><span class="line">INFO:tensorflow:*** Num TPU Cores Per Worker: <span class="number">8</span></span><br><span class="line">INFO:tensorflow:*** Num TPU Cores Per Worker: <span class="number">8</span></span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span>, CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span>, CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:XLA_CPU:<span class="number">0</span>, XLA_CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:XLA_CPU:<span class="number">0</span>, XLA_CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span>, CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span>, CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">0</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">0</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">1</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">1</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">2</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">2</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">3</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">3</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">4</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">4</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">5</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">5</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">6</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">6</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">7</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">7</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU_SYSTEM:<span class="number">0</span>, TPU_SYSTEM, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU_SYSTEM:<span class="number">0</span>, TPU_SYSTEM, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:XLA_CPU:<span class="number">0</span>, XLA_CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:XLA_CPU:<span class="number">0</span>, XLA_CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">Model: <span class="string">"sequential"</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (None, <span class="number">300</span>, <span class="number">7</span>)            <span class="number">216874</span>    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv1d (Conv1D)              (None, <span class="number">296</span>, <span class="number">64</span>)           <span class="number">2304</span>      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling1d (MaxPooling1D) (None, <span class="number">148</span>, <span class="number">64</span>)           <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv1d_1 (Conv1D)            (None, <span class="number">146</span>, <span class="number">32</span>)           <span class="number">6176</span>      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling1d_1 (MaxPooling1 (None, <span class="number">73</span>, <span class="number">32</span>)            <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten (Flatten)            (None, <span class="number">2336</span>)              <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                (None, <span class="number">46</span>)                <span class="number">107502</span>    </span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">332</span>,<span class="number">856</span></span><br><span class="line">Trainable params: <span class="number">332</span>,<span class="number">856</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit(ds_train,validation_data = ds_test,epochs = <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">Train <span class="keyword">for</span> <span class="number">281</span> steps, validate <span class="keyword">for</span> <span class="number">71</span> steps</span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">12</span>s <span class="number">43</span>ms/step - loss: <span class="number">3.4466</span> - sparse_categorical_accuracy: <span class="number">0.4332</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7180</span> - val_loss: <span class="number">3.3179</span> - val_sparse_categorical_accuracy: <span class="number">0.5352</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7195</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">6</span>s <span class="number">20</span>ms/step - loss: <span class="number">3.3251</span> - sparse_categorical_accuracy: <span class="number">0.5405</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7302</span> - val_loss: <span class="number">3.3082</span> - val_sparse_categorical_accuracy: <span class="number">0.5463</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7235</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">6</span>s <span class="number">20</span>ms/step - loss: <span class="number">3.2961</span> - sparse_categorical_accuracy: <span class="number">0.5729</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7280</span> - val_loss: <span class="number">3.3026</span> - val_sparse_categorical_accuracy: <span class="number">0.5499</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7217</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">5</span>s <span class="number">19</span>ms/step - loss: <span class="number">3.2751</span> - sparse_categorical_accuracy: <span class="number">0.5924</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7276</span> - val_loss: <span class="number">3.2957</span> - val_sparse_categorical_accuracy: <span class="number">0.5543</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7217</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">5</span>s <span class="number">19</span>ms/step - loss: <span class="number">3.2655</span> - sparse_categorical_accuracy: <span class="number">0.6008</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7290</span> - val_loss: <span class="number">3.3022</span> - val_sparse_categorical_accuracy: <span class="number">0.5490</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7231</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">5</span>s <span class="number">19</span>ms/step - loss: <span class="number">3.2616</span> - sparse_categorical_accuracy: <span class="number">0.6041</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7295</span> - val_loss: <span class="number">3.3015</span> - val_sparse_categorical_accuracy: <span class="number">0.5503</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7235</span></span><br><span class="line">Epoch <span class="number">7</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">6</span>s <span class="number">21</span>ms/step - loss: <span class="number">3.2595</span> - sparse_categorical_accuracy: <span class="number">0.6059</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7322</span> - val_loss: <span class="number">3.3064</span> - val_sparse_categorical_accuracy: <span class="number">0.5454</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7266</span></span><br><span class="line">Epoch <span class="number">8</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">6</span>s <span class="number">21</span>ms/step - loss: <span class="number">3.2591</span> - sparse_categorical_accuracy: <span class="number">0.6063</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7327</span> - val_loss: <span class="number">3.3025</span> - val_sparse_categorical_accuracy: <span class="number">0.5481</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7231</span></span><br><span class="line">Epoch <span class="number">9</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">5</span>s <span class="number">19</span>ms/step - loss: <span class="number">3.2588</span> - sparse_categorical_accuracy: <span class="number">0.6062</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7332</span> - val_loss: <span class="number">3.2992</span> - val_sparse_categorical_accuracy: <span class="number">0.5521</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7257</span></span><br><span class="line">Epoch <span class="number">10</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">5</span>s <span class="number">18</span>ms/step - loss: <span class="number">3.2577</span> - sparse_categorical_accuracy: <span class="number">0.6073</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7363</span> - val_loss: <span class="number">3.2981</span> - val_sparse_categorical_accuracy: <span class="number">0.5516</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7306</span></span><br><span class="line">CPU times: user <span class="number">18.9</span> s, sys: <span class="number">3.86</span> s, total: <span class="number">22.7</span> s</span><br><span class="line">Wall time: <span class="number">1</span>min <span class="number">1</span>s</span><br></pre></td></tr></table></figure>
<h2 id="shi-yong-tensorflow-serving-bu-shu-mo-xing">使用tensorflow-serving部署模型</h2>
<p>TensorFlow训练好的模型以tensorflow原生方式保存成protobuf文件后可以用许多方式部署运行。</p>
<p>例如：通过 tensorflow-js 可以用javascrip脚本加载模型并在浏览器中运行模型。</p>
<p>通过 tensorflow-lite 可以在移动和嵌入式设备上加载并运行TensorFlow模型。</p>
<p>通过 tensorflow-serving 可以加载模型后提供网络接口API服务，通过任意编程语言发送网络请求都可以获取模型预测结果。</p>
<p>通过 tensorFlow for Java接口，可以在Java或者spark(scala)中调用tensorflow模型进行预测。</p>
<p>主要介绍tensorflow serving部署模型、使用spark(scala)调用tensorflow模型的方法。</p>
<h3 id="tensorflow-serving-mo-xing-bu-shu-gai-shu">tensorflow serving模型部署概述</h3>
<p>使用 tensorflow serving 部署模型要完成以下步骤。</p>
<ul>
<li>
<p>(1) 准备protobuf模型文件。</p>
</li>
<li>
<p>(2) 安装tensorflow serving。</p>
</li>
<li>
<p>(3) 启动tensorflow serving 服务。</p>
</li>
<li>
<p>(4) 向API服务发送请求，获取预测结果。</p>
</li>
</ul>
<p>可通过以下colab链接测试效果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%tensorflow_version <span class="number">2.</span>x</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.__version__)</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<h3 id="zhun-bei-protobuf-mo-xing-wen-jian">准备protobuf模型文件</h3>
<p>我们使用tf.keras 训练一个简单的线性回归模型，并保存成protobuf文件。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,optimizers</span><br><span class="line"></span><br><span class="line"><span class="comment">## 样本数量</span></span><br><span class="line">n = <span class="number">800</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 生成测试用数据集</span></span><br><span class="line">X = tf.random.uniform([n,<span class="number">2</span>],minval=<span class="number">-10</span>,maxval=<span class="number">10</span>) </span><br><span class="line">w0 = tf.constant([[<span class="number">2.0</span>],[<span class="number">-1.0</span>]])</span><br><span class="line">b0 = tf.constant(<span class="number">3.0</span>)</span><br><span class="line"></span><br><span class="line">Y = X@w0 + b0 + tf.random.normal([n,<span class="number">1</span>],</span><br><span class="line">    mean = <span class="number">0.0</span>,stddev= <span class="number">2.0</span>) <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 建立模型</span></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line">inputs = layers.Input(shape = (<span class="number">2</span>,),name =<span class="string">"inputs"</span>) <span class="comment">#设置输入名字为inputs</span></span><br><span class="line">outputs = layers.Dense(<span class="number">1</span>, name = <span class="string">"outputs"</span>)(inputs) <span class="comment">#设置输出名字为outputs</span></span><br><span class="line">linear = models.Model(inputs = inputs,outputs = outputs)</span><br><span class="line">linear.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 使用fit方法进行训练</span></span><br><span class="line">linear.compile(optimizer=<span class="string">"rmsprop"</span>,loss=<span class="string">"mse"</span>,metrics=[<span class="string">"mae"</span>])</span><br><span class="line">linear.fit(X,Y,batch_size = <span class="number">8</span>,epochs = <span class="number">100</span>)  </span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"w = "</span>,linear.layers[<span class="number">1</span>].kernel)</span><br><span class="line">tf.print(<span class="string">"b = "</span>,linear.layers[<span class="number">1</span>].bias)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 将模型保存成pb格式文件</span></span><br><span class="line">export_path = <span class="string">"./data/linear_model/"</span></span><br><span class="line">version = <span class="string">"1"</span>       <span class="comment">#后续可以通过版本号进行模型版本迭代与管理</span></span><br><span class="line">linear.save(export_path+version, save_format=<span class="string">"tf"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#查看保存的模型文件</span></span><br><span class="line">!ls &#123;export_path+version&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">assets</span>	<span class="selector-tag">saved_model</span><span class="selector-class">.pb</span>	<span class="selector-tag">variables</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看模型文件相关信息</span></span><br><span class="line">!saved_model_cli show --dir &#123;export_path+str(version)&#125; --all</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">MetaGraphDef <span class="keyword">with</span> tag-<span class="keyword">set</span>: <span class="string">'serve'</span> contains the <span class="keyword">following</span> SignatureDefs:</span><br><span class="line"></span><br><span class="line">signature_def[<span class="string">'__saved_model_init_op'</span>]:</span><br><span class="line">  The given SavedModel SignatureDef contains the <span class="keyword">following</span> <span class="keyword">input</span>(s):</span><br><span class="line">  The given SavedModel SignatureDef contains the <span class="keyword">following</span> <span class="keyword">output</span>(s):</span><br><span class="line">    outputs[<span class="string">'__saved_model_init_op'</span>] tensor_info:</span><br><span class="line">        dtype: DT_INVALID</span><br><span class="line">        shape: unknown_rank</span><br><span class="line">        <span class="keyword">name</span>: NoOp</span><br><span class="line">  Method <span class="keyword">name</span> <span class="keyword">is</span>: </span><br><span class="line"></span><br><span class="line">signature_def[<span class="string">'serving_default'</span>]:</span><br><span class="line">  The given SavedModel SignatureDef contains the <span class="keyword">following</span> <span class="keyword">input</span>(s):</span><br><span class="line">    inputs[<span class="string">'inputs'</span>] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">name</span>: serving_default_inputs:<span class="number">0</span></span><br><span class="line">  The given SavedModel SignatureDef contains the <span class="keyword">following</span> <span class="keyword">output</span>(s):</span><br><span class="line">    outputs[<span class="string">'outputs'</span>] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">name</span>: StatefulPartitionedCall:<span class="number">0</span></span><br><span class="line">  Method <span class="keyword">name</span> <span class="keyword">is</span>: tensorflow/serving/predict</span><br><span class="line"><span class="keyword">WARNING</span>:tensorflow:<span class="keyword">From</span> /tensorflow<span class="number">-2.1</span><span class="number">.0</span>/python3<span class="number">.6</span>/tensorflow_core/python/ops/resource_variable_ops.py:<span class="number">1786</span>: <span class="keyword">calling</span> BaseResourceVariable.__init__ (<span class="keyword">from</span> tensorflow.python.ops.resource_variable_ops) <span class="keyword">with</span> <span class="keyword">constraint</span> <span class="keyword">is</span> deprecated <span class="keyword">and</span> will be removed <span class="keyword">in</span> a future version.</span><br><span class="line">Instructions <span class="keyword">for</span> updating:</span><br><span class="line"><span class="keyword">If</span> <span class="keyword">using</span> Keras pass *_constraint arguments <span class="keyword">to</span> layers.</span><br><span class="line"></span><br><span class="line">Defined Functions:</span><br><span class="line">  <span class="keyword">Function</span> <span class="keyword">Name</span>: <span class="string">'__call__'</span></span><br><span class="line">    <span class="keyword">Option</span> <span class="comment">#1</span></span><br><span class="line">      Callable <span class="keyword">with</span>:</span><br><span class="line">        Argument <span class="comment">#1</span></span><br><span class="line">          inputs: TensorSpec(shape=(<span class="keyword">None</span>, <span class="number">2</span>), dtype=tf.float32, <span class="keyword">name</span>=<span class="string">'inputs'</span>)</span><br><span class="line">        Argument <span class="comment">#2</span></span><br><span class="line">          DType: <span class="built_in">bool</span></span><br><span class="line">          <span class="keyword">Value</span>: <span class="literal">False</span></span><br><span class="line">        Argument <span class="comment">#3</span></span><br><span class="line">          DType: NoneType</span><br><span class="line">          <span class="keyword">Value</span>: <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">Option</span> <span class="comment">#2</span></span><br><span class="line">      Callable <span class="keyword">with</span>:</span><br><span class="line">        Argument <span class="comment">#1</span></span><br><span class="line">          inputs: TensorSpec(shape=(<span class="keyword">None</span>, <span class="number">2</span>), dtype=tf.float32, <span class="keyword">name</span>=<span class="string">'inputs'</span>)</span><br><span class="line">        Argument <span class="comment">#2</span></span><br><span class="line">          DType: <span class="built_in">bool</span></span><br><span class="line">          <span class="keyword">Value</span>: <span class="literal">True</span></span><br><span class="line">        Argument <span class="comment">#3</span></span><br><span class="line">          DType: NoneType</span><br><span class="line">          <span class="keyword">Value</span>: <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">Function</span> <span class="keyword">Name</span>: <span class="string">'_default_save_signature'</span></span><br><span class="line">    <span class="keyword">Option</span> <span class="comment">#1</span></span><br><span class="line">      Callable <span class="keyword">with</span>:</span><br><span class="line">        Argument <span class="comment">#1</span></span><br><span class="line">          inputs: TensorSpec(shape=(<span class="keyword">None</span>, <span class="number">2</span>), dtype=tf.float32, <span class="keyword">name</span>=<span class="string">'inputs'</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">Function</span> <span class="keyword">Name</span>: <span class="string">'call_and_return_all_conditional_losses'</span></span><br><span class="line">    <span class="keyword">Option</span> <span class="comment">#1</span></span><br><span class="line">      Callable <span class="keyword">with</span>:</span><br><span class="line">        Argument <span class="comment">#1</span></span><br><span class="line">          inputs: TensorSpec(shape=(<span class="keyword">None</span>, <span class="number">2</span>), dtype=tf.float32, <span class="keyword">name</span>=<span class="string">'inputs'</span>)</span><br><span class="line">        Argument <span class="comment">#2</span></span><br><span class="line">          DType: <span class="built_in">bool</span></span><br><span class="line">          <span class="keyword">Value</span>: <span class="literal">True</span></span><br><span class="line">        Argument <span class="comment">#3</span></span><br><span class="line">          DType: NoneType</span><br><span class="line">          <span class="keyword">Value</span>: <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">Option</span> <span class="comment">#2</span></span><br><span class="line">      Callable <span class="keyword">with</span>:</span><br><span class="line">        Argument <span class="comment">#1</span></span><br><span class="line">          inputs: TensorSpec(shape=(<span class="keyword">None</span>, <span class="number">2</span>), dtype=tf.float32, <span class="keyword">name</span>=<span class="string">'inputs'</span>)</span><br><span class="line">        Argument <span class="comment">#2</span></span><br><span class="line">          DType: <span class="built_in">bool</span></span><br><span class="line">          <span class="keyword">Value</span>: <span class="literal">False</span></span><br><span class="line">        Argument <span class="comment">#3</span></span><br><span class="line">          DType: NoneType</span><br><span class="line">          <span class="keyword">Value</span>: <span class="keyword">None</span></span><br></pre></td></tr></table></figure>
<h3 id="an-zhuang-tensorflow-serving">安装 tensorflow serving</h3>
<p>安装 tensorflow serving 有2种主要方法：通过Docker镜像安装，通过apt安装。通过Docker镜像安装是最简单，最直接的方法，推荐采用。</p>
<p>Docker可以理解成一种容器，其上面可以给各种不同的程序提供独立的运行环境。一般业务中用到tensorflow的企业都会有运维同学通过Docker 搭建 tensorflow serving.无需算法工程师同学动手安装，以下安装过程仅供参考。</p>
<p>不同操作系统机器上安装Docker的方法可以参照以下链接。</p>
<p>Windows: <a href="https://www.runoob.com/docker/windows-docker-install.html" target="_blank" rel="noopener">https://www.runoob.com/docker/windows-docker-install.html</a></p>
<p>MacOs: <a href="https://www.runoob.com/docker/macos-docker-install.html" target="_blank" rel="noopener">https://www.runoob.com/docker/macos-docker-install.html</a></p>
<p>CentOS: <a href="https://www.runoob.com/docker/centos-docker-install.html" target="_blank" rel="noopener">https://www.runoob.com/docker/centos-docker-install.html</a></p>
<p>安装Docker成功后，使用如下命令加载 tensorflow/serving 镜像到Docker中</p>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="attribute">docker</span> pull tensorflow/serving</span><br></pre></td></tr></table></figure>
<h3 id="qi-dong-tensorflow-serving-fu-wu">启动 tensorflow serving 服务</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!docker run -t --rm -p <span class="number">8501</span>:<span class="number">8501</span> \</span><br><span class="line">    -v <span class="string">"/Users/.../data/linear_model/"</span> \</span><br><span class="line">    -e MODEL_NAME=linear_model \</span><br><span class="line">    tensorflow/serving &amp; &gt;server.log <span class="number">2</span>&gt;&amp;<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="xiang-api-fu-wu-fa-song-qing-qiu">向API服务发送请求</h3>
<p>可以使用任何编程语言的http功能发送请求，下面示范linux的 curl 命令发送请求，以及Python的requests库发送请求。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!curl -d <span class="string">'&#123;"instances": [[1.0, 2.0], [5.0,7.0]]&#125;'</span> \</span><br><span class="line">    -X POST http://localhost:<span class="number">8501</span>/v1/models/linear_model:predict</span><br></pre></td></tr></table></figure>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"predictions"</span>: [[<span class="number">3.06546211</span>], [<span class="number">6.02843142</span>]</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json,requests</span><br><span class="line"></span><br><span class="line">data = json.dumps(&#123;<span class="string">"signature_name"</span>: <span class="string">"serving_default"</span>, <span class="string">"instances"</span>: [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">5.0</span>,<span class="number">7.0</span>]]&#125;)</span><br><span class="line">headers = &#123;<span class="string">"content-type"</span>: <span class="string">"application/json"</span>&#125;</span><br><span class="line">json_response = requests.post(<span class="string">'http://localhost:8501/v1/models/linear_model:predict'</span>, </span><br><span class="line">        data=data, headers=headers)</span><br><span class="line">predictions = json.loads(json_response.text)[<span class="string">"predictions"</span>]</span><br><span class="line">print(predictions) <span class="comment"># [[3.06546211], [6.02843142]]</span></span><br></pre></td></tr></table></figure>
<h2 id="shi-yong-spark-scala-diao-yong-tensorflow-2-0-xun-lian-hao-de-mo-xing">使用spark-scala调用tensorflow2.0训练好的模型</h2>
<p>本篇文章介绍在spark中调用训练好的tensorflow模型进行预测的方法。</p>
<p>本文内容的学习需要一定的spark和scala基础。</p>
<p>如果使用pyspark的话会比较简单，只需要在每个executor上用Python加载模型分别预测就可以了。</p>
<p>但工程上为了性能考虑，通常使用的是scala版本的spark。</p>
<p>本篇文章我们通过TensorFlow for Java 在spark中调用训练好的tensorflow模型。</p>
<p>利用spark的分布式计算能力，从而可以让训练好的tensorflow模型在成百上千的机器上分布式并行执行模型推断。</p>
<h3 id="spark-scala-diao-yong-tensorflow-mo-xing-gai-shu">spark-scala调用tensorflow模型概述</h3>
<p>在spark(scala)中调用tensorflow模型进行预测需要完成以下几个步骤。</p>
<p>（1）准备protobuf模型文件</p>
<p>（2）创建spark(scala)项目，在项目中添加java版本的tensorflow对应的jar包依赖</p>
<p>（3）在spark(scala)项目中driver端加载tensorflow模型调试成功</p>
<p>（4）在spark(scala)项目中通过RDD在executor上加载tensorflow模型调试成功</p>
<p>（5） 在spark(scala)项目中通过DataFrame在executor上加载tensorflow模型调试成功</p>
<h3 id="zhun-bei-protobuf-mo-xing-wen-jian-1">准备protobuf模型文件</h3>
<p>我们使用tf.keras 训练一个简单的线性回归模型，并保存成protobuf文件。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,optimizers</span><br><span class="line"></span><br><span class="line"><span class="comment">## 样本数量</span></span><br><span class="line">n = <span class="number">800</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 生成测试用数据集</span></span><br><span class="line">X = tf.random.uniform([n,<span class="number">2</span>],minval=<span class="number">-10</span>,maxval=<span class="number">10</span>) </span><br><span class="line">w0 = tf.constant([[<span class="number">2.0</span>],[<span class="number">-1.0</span>]])</span><br><span class="line">b0 = tf.constant(<span class="number">3.0</span>)</span><br><span class="line"></span><br><span class="line">Y = X@w0 + b0 + tf.random.normal([n,<span class="number">1</span>],mean = <span class="number">0.0</span>,stddev= <span class="number">2.0</span>)  <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 建立模型</span></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line">inputs = layers.Input(shape = (<span class="number">2</span>,),name =<span class="string">"inputs"</span>) <span class="comment">#设置输入名字为inputs</span></span><br><span class="line">outputs = layers.Dense(<span class="number">1</span>, name = <span class="string">"outputs"</span>)(inputs) <span class="comment">#设置输出名字为outputs</span></span><br><span class="line">linear = models.Model(inputs = inputs,outputs = outputs)</span><br><span class="line">linear.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 使用fit方法进行训练</span></span><br><span class="line">linear.compile(optimizer=<span class="string">"rmsprop"</span>,loss=<span class="string">"mse"</span>,metrics=[<span class="string">"mae"</span>])</span><br><span class="line">linear.fit(X,Y,batch_size = <span class="number">8</span>,epochs = <span class="number">100</span>)  </span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"w = "</span>,linear.layers[<span class="number">1</span>].kernel)</span><br><span class="line">tf.print(<span class="string">"b = "</span>,linear.layers[<span class="number">1</span>].bias)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 将模型保存成pb格式文件</span></span><br><span class="line">export_path = <span class="string">"./data/linear_model/"</span></span><br><span class="line">version = <span class="string">"1"</span>       <span class="comment">#后续可以通过版本号进行模型版本迭代与管理</span></span><br><span class="line">linear.save(export_path+version, save_format=<span class="string">"tf"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!ls &#123;export_path+version&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看模型文件相关信息</span></span><br><span class="line">!saved_model_cli show --dir &#123;export_path+str(version)&#125; --all</span><br></pre></td></tr></table></figure>
<p>模型文件信息中这些标红的部分都是后面有可能会用到的。</p>
<p><img src="/2020/05/07/tensorflow2/%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E4%BF%A1%E6%81%AF.png" alt></p>
<h3 id="chuang-jian-spark-scala-xiang-mu-zai-xiang-mu-zhong-tian-jia-java-ban-ben-de-tensorflow-dui-ying-de-jar-bao-yi-lai">创建spark(scala)项目，在项目中添加java版本的tensorflow对应的jar包依赖</h3>
<p>如果使用maven管理项目，需要添加如下 jar包依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.tensorflow/tensorflow --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.tensorflow<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>tensorflow<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>也可以从下面网址中直接下载 org.tensorflow.tensorflow的jar包</p>
<p>以及其依赖的org.tensorflow.libtensorflow 和 org.tensorflowlibtensorflow_jni的jar包 放到项目中。</p>
<p><a href="https://mvnrepository.com/artifact/org.tensorflow/tensorflow/1.15.0" target="_blank" rel="noopener">https://mvnrepository.com/artifact/org.tensorflow/tensorflow/1.15.0</a></p>
<h3 id="zai-spark-scala-xiang-mu-zhong-driver-duan-jia-zai-tensorflow-mo-xing-diao-shi-cheng-gong">在spark(scala)项目中driver端加载tensorflow模型调试成功</h3>
<p>我们的示范代码在jupyter notebook中进行演示，需要安装toree以支持spark(scala)。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">WrappedArray</span></span><br><span class="line"><span class="keyword">import</span> org.&#123;tensorflow=&gt;tf&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//注：load函数的第二个参数一般都是“serve”，可以从模型文件相关信息中找到</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> bundle = tf.<span class="type">SavedModelBundle</span> </span><br><span class="line">   .load(<span class="string">"/Users/liangyun/CodeFiles/eat_tensorflow2_in_30_days/data/linear_model/1"</span>,<span class="string">"serve"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//注：在java版本的tensorflow中还是类似tensorflow1.0中静态计算图的模式，需要建立Session, 指定feed的数据和fetch的结果, 然后 run.</span></span><br><span class="line"><span class="comment">//注：如果有多个数据需要喂入，可以连续使用多个feed方法</span></span><br><span class="line"><span class="comment">//注：输入必须是float类型</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sess = bundle.session()</span><br><span class="line"><span class="keyword">val</span> x = tf.<span class="type">Tensor</span>.create(<span class="type">Array</span>(<span class="type">Array</span>(<span class="number">1.0</span>f,<span class="number">2.0</span>f),<span class="type">Array</span>(<span class="number">2.0</span>f,<span class="number">3.0</span>f)))</span><br><span class="line"><span class="keyword">val</span> y =  sess.runner().feed(<span class="string">"serving_default_inputs:0"</span>, x)</span><br><span class="line">         .fetch(<span class="string">"StatefulPartitionedCall:0"</span>).run().get(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> result = <span class="type">Array</span>.ofDim[<span class="type">Float</span>](y.shape()(<span class="number">0</span>).toInt,y.shape()(<span class="number">1</span>).toInt)</span><br><span class="line">y.copyTo(result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(x != <span class="literal">null</span>) x.close()</span><br><span class="line"><span class="keyword">if</span>(y != <span class="literal">null</span>) y.close()</span><br><span class="line"><span class="keyword">if</span>(sess != <span class="literal">null</span>) sess.close()</span><br><span class="line"><span class="keyword">if</span>(bundle != <span class="literal">null</span>) bundle.close()  </span><br><span class="line"></span><br><span class="line">result</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">Array(<span class="name">Array</span>(<span class="number">3.019596</span>), Array(<span class="number">3.9878292</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/TfDriver.png" alt></p>
<h3 id="zai-spark-scala-xiang-mu-zhong-tong-guo-rdd-zai-executor-shang-jia-zai-tensorflow-mo-xing-diao-shi-cheng-gong">在spark(scala)项目中通过RDD在executor上加载tensorflow模型调试成功</h3>
<p>下面我们通过广播机制将Driver端加载的TensorFlow模型传递到各个executor上，并在executor上分布式地调用模型进行推断。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">WrappedArray</span></span><br><span class="line"><span class="keyword">import</span> org.&#123;tensorflow=&gt;tf&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">    .builder()</span><br><span class="line">    .appName(<span class="string">"TfRDD"</span>)</span><br><span class="line">    .enableHiveSupport()</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment">//在Driver端加载模型</span></span><br><span class="line"><span class="keyword">val</span> bundle = tf.<span class="type">SavedModelBundle</span> </span><br><span class="line">   .load(<span class="string">"/Users/liangyun/CodeFiles/master_tensorflow2_in_20_hours/data/linear_model/1"</span>,<span class="string">"serve"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//利用广播将模型发送到executor上</span></span><br><span class="line"><span class="keyword">val</span> broads = sc.broadcast(bundle)</span><br><span class="line"></span><br><span class="line"><span class="comment">//构造数据集</span></span><br><span class="line"><span class="keyword">val</span> rdd_data = sc.makeRDD(<span class="type">List</span>(<span class="type">Array</span>(<span class="number">1.0</span>f,<span class="number">2.0</span>f),<span class="type">Array</span>(<span class="number">3.0</span>f,<span class="number">5.0</span>f),<span class="type">Array</span>(<span class="number">6.0</span>f,<span class="number">7.0</span>f),<span class="type">Array</span>(<span class="number">8.0</span>f,<span class="number">3.0</span>f)))</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过mapPartitions调用模型进行批量推断</span></span><br><span class="line"><span class="keyword">val</span> rdd_result = rdd_data.mapPartitions(iter =&gt; &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> arr = iter.toArray</span><br><span class="line">    <span class="keyword">val</span> model = broads.value</span><br><span class="line">    <span class="keyword">val</span> sess = model.session()</span><br><span class="line">    <span class="keyword">val</span> x = tf.<span class="type">Tensor</span>.create(arr)</span><br><span class="line">    <span class="keyword">val</span> y =  sess.runner().feed(<span class="string">"serving_default_inputs:0"</span>, x)</span><br><span class="line">             .fetch(<span class="string">"StatefulPartitionedCall:0"</span>).run().get(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将预测结果拷贝到相同shape的Float类型的Array中</span></span><br><span class="line">    <span class="keyword">val</span> result = <span class="type">Array</span>.ofDim[<span class="type">Float</span>](y.shape()(<span class="number">0</span>).toInt,y.shape()(<span class="number">1</span>).toInt)</span><br><span class="line">    y.copyTo(result)</span><br><span class="line">    result.iterator</span><br><span class="line">    </span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rdd_result.take(<span class="number">5</span>)</span><br><span class="line">bundle.close</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">Array(<span class="name">Array</span>(<span class="number">3.019596</span>), Array(<span class="number">3.9264367</span>), Array(<span class="number">7.8607616</span>), Array(<span class="number">15.974984</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow2/TfRDD.png" alt></p>
<h3 id="zai-spark-scala-xiang-mu-zhong-tong-guo-data-frame-zai-executor-shang-jia-zai-tensorflow-mo-xing-diao-shi-cheng-gong">在spark(scala)项目中通过DataFrame在executor上加载tensorflow模型调试成功</h3>
<p>除了可以在Spark的RDD数据上调用tensorflow模型进行分布式推断，</p>
<p>我们也可以在DataFrame数据上调用tensorflow模型进行分布式推断。</p>
<p>主要思路是将推断方法注册成为一个sparkSQL函数。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">WrappedArray</span></span><br><span class="line"><span class="keyword">import</span> org.&#123;tensorflow=&gt;tf&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TfDataFrame</span> <span class="keyword">extends</span> <span class="title">Serializable</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]):<span class="type">Unit</span> = &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">        .builder()</span><br><span class="line">        .appName(<span class="string">"TfDataFrame"</span>)</span><br><span class="line">        .enableHiveSupport()</span><br><span class="line">        .getOrCreate()</span><br><span class="line">        <span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> bundle = tf.<span class="type">SavedModelBundle</span> </span><br><span class="line">           .load(<span class="string">"/Users/liangyun/CodeFiles/master_tensorflow2_in_20_hours/data/linear_model/1"</span>,<span class="string">"serve"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> broads = sc.broadcast(bundle)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//构造预测函数，并将其注册成sparkSQL的udf</span></span><br><span class="line">        <span class="keyword">val</span> tfpredict = (features:<span class="type">WrappedArray</span>[<span class="type">Float</span>])  =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> bund = broads.value</span><br><span class="line">            <span class="keyword">val</span> sess = bund.session()</span><br><span class="line">            <span class="keyword">val</span> x = tf.<span class="type">Tensor</span>.create(<span class="type">Array</span>(features.toArray))</span><br><span class="line">            <span class="keyword">val</span> y =  sess.runner().feed(<span class="string">"serving_default_inputs:0"</span>, x)</span><br><span class="line">                     .fetch(<span class="string">"StatefulPartitionedCall:0"</span>).run().get(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">val</span> result = <span class="type">Array</span>.ofDim[<span class="type">Float</span>](y.shape()(<span class="number">0</span>).toInt,y.shape()(<span class="number">1</span>).toInt)</span><br><span class="line">            y.copyTo(result)</span><br><span class="line">            <span class="keyword">val</span> y_pred = result(<span class="number">0</span>)(<span class="number">0</span>)</span><br><span class="line">            y_pred</span><br><span class="line">        &#125;</span><br><span class="line">        spark.udf.register(<span class="string">"tfpredict"</span>,tfpredict)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//构造DataFrame数据集，将features放到一列中</span></span><br><span class="line">        <span class="keyword">val</span> dfdata = sc.parallelize(<span class="type">List</span>(<span class="type">Array</span>(<span class="number">1.0</span>f,<span class="number">2.0</span>f),<span class="type">Array</span>(<span class="number">3.0</span>f,<span class="number">5.0</span>f),<span class="type">Array</span>(<span class="number">7.0</span>f,<span class="number">8.0</span>f))).toDF(<span class="string">"features"</span>)</span><br><span class="line">        dfdata.show </span><br><span class="line">        </span><br><span class="line">        <span class="comment">//调用sparkSQL预测函数，增加一个新的列作为y_preds</span></span><br><span class="line">        <span class="keyword">val</span> dfresult = dfdata.selectExpr(<span class="string">"features"</span>,<span class="string">"tfpredict(features) as y_preds"</span>)</span><br><span class="line">        dfresult.show </span><br><span class="line">        bundle.close</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">TfDataFrame</span>.main(<span class="type">Array</span>())</span><br></pre></td></tr></table></figure>
<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line"><span class="code">+----------+</span></span><br><span class="line">|  features|</span><br><span class="line"><span class="code">+----------+</span></span><br><span class="line">|[1.0, 2.0]|</span><br><span class="line">|[3.0, 5.0]|</span><br><span class="line">|[7.0, 8.0]|</span><br><span class="line"><span class="code">+----------+</span></span><br><span class="line"></span><br><span class="line"><span class="code">+----------+</span>---------+</span><br><span class="line">|  features|  y<span class="emphasis">_preds|</span></span><br><span class="line"><span class="emphasis">+----------+---------+</span></span><br><span class="line"><span class="emphasis">|[1.0, 2.0]| 3.019596|</span></span><br><span class="line"><span class="emphasis">|[3.0, 5.0]|3.9264367|</span></span><br><span class="line"><span class="emphasis">|[7.0, 8.0]| 8.828995|</span></span><br><span class="line"><span class="emphasis">+----------+---------+</span></span><br></pre></td></tr></table></figure>
<p>以上我们分别在spark 的RDD数据结构和DataFrame数据结构上实现了调用一个tf.keras实现的线性回归模型进行分布式模型推断。</p>
<p>在本例基础上稍作修改则可以用spark调用训练好的各种复杂的神经网络模型进行分布式模型推断。</p>
<p>但实际上tensorflow并不仅仅适合实现神经网络，其底层的计算图语言可以表达各种数值计算过程。</p>
<p>利用其丰富的低阶API，我们可以在tensorflow2.0上实现任意机器学习模型，</p>
<p>结合tf.Module提供的便捷的封装功能，我们可以将训练好的任意机器学习模型导出成模型文件并在spark上分布式调用执行。</p>
<p>这无疑为我们的工程应用提供了巨大的想象空间。</p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://github.com/lyhue1991/eat_tensorflow2_in_30_days" target="_blank" rel="noopener">eat_tensorflow2_in_30_days</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/98472075" target="_blank" rel="noopener">《一文概览深度学习中的激活函数》</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/98863801" target="_blank" rel="noopener">《从ReLU到GELU,一文概览神经网络中的激活函数》</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/80594704" target="_blank" rel="noopener">《5分钟理解Focal Loss与GHM——解决样本不平衡利器》</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/68509398" target="_blank" rel="noopener">《用GPU加速Keras模型——Colab免费GPU使用攻略》</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32230623" target="_blank" rel="noopener">详见《一个框架看懂优化算法之异同 SGD/AdaGrad/Adam》</a></li>
<li><a href="https://colab.research.google.com/drive/1r5dLoeJq5z01sU72BX2M5UiNSkuxsEFe" target="_blank" rel="noopener">《tf_单GPU》</a></li>
<li><a href="https://colab.research.google.com/drive/1j2kp_t0S_cofExSN7IyJ4QtMscbVlXU-" target="_blank" rel="noopener">《tf_多GPU》</a></li>
<li><a href="https://colab.research.google.com/drive/1XCIhATyE1R7lq6uwFlYlRsUr5d9_-r1s" target="_blank" rel="noopener">《tf_TPU》</a></li>
<li><a href="https://colab.research.google.com/drive/1vS5LAYJTEn-H0GDb1irzIuyRB8E3eWc8" target="_blank" rel="noopener">《tf_serving》</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/TensorFlow2</category>
      </categories>
      <tags>
        <tag>建模流程</tag>
      </tags>
  </entry>
  <entry>
    <title>时间序列预测</title>
    <url>/2020/04/29/machine_learning/time_feature/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/04/29/machine_learning/time_feature/image-20200729223943395.png" alt></p>
<a id="more"></a>
<h1 id="te-zheng">特征</h1>
<h2 id="chang-gui-te-zheng">常规特征</h2>
<p>滑动窗口法。寻找100个时间相邻的样本的某一个特征的某一个统计特性，比如：</p>
<ul>
<li>均值<code>mean()</code></li>
<li>中位数<code>median()</code></li>
<li>最大值<code>max()</code></li>
<li>最小值<code>min()</code></li>
<li>标准差<code>std()</code></li>
<li>四分位点<code>quantile(q=0.25)/quantile(q=0.75)</code></li>
<li>数据的偏度<code>df.skew()</code></li>
<li>数据的峰度<code>df.kurt()</code></li>
<li>数据的绝对离差<code>df.max()</code>,数据与其均值的差的绝对值的平均值</li>
<li>绝对值的最大值</li>
<li>绝对值的最小值</li>
<li>梯度的绝对值</li>
<li>最大值与最小值的差值</li>
<li>最大值与最小值的倍数</li>
<li>使用时间序列的shift偏移</li>
</ul>
<h2 id="fei-chang-gui-te-zheng">非常规特征</h2>
<ol>
<li>
<p>节假日：节假日都是一个需要注意的特征，因为<strong>放假了就会消耗更多的能源，产生更多的消费购物…</strong>。</p>
<blockquote>
<p>不过需要注意的是：欧美国家的一些节假日和中国不一样，所以需要看一下欧美的法定节假日日期以及周末的放假日期。</p>
</blockquote>
</li>
<li>
<p><strong>分离时间戳</strong>产生更多特征:一般时间戳timestamp可能是这样的格式：13:21 20/3/1997，这样的话，可以把这个<code>timestamp</code>分离成5个特征:<code>hour,minute,day,month,year</code>。</p>
<blockquote>
<p>进一步的，可以对<strong>小时</strong>数据进行数据探索分析，看看是否存在<strong>白天黑夜</strong>的不同导致的对预测结果的影响,对<strong>day和month</strong>进行分析，看看是否存在<strong>节假日</strong>对预测结果的影响。</p>
</blockquote>
</li>
<li>
<p>日期数据的<strong>循环性</strong>:通常对于上面的hour特征是[0,23]的，但是其实0点和23点并不像数字本身看起来差别那么巨大，所以可以<strong>使用cos或者sin来将时间变成一个循环</strong>。相似的是星期一与星期日的差别。</p>
</li>
<li>
<p>人造节日:中国的6.18，5.20，11.11，12.12等</p>
</li>
<li>
<p>季节导致的偏移:可能同样的情况春天发生和秋天发生并不相同</p>
</li>
</ol>
<h2 id="zhou-qi-xing-te-zheng">周期性特征</h2>
<p>部分时间特征是具有周期性的。可能是一个月是一个周期，也可能一个季度，一年等等。为了寻找这个周期性，可以使用自相关系数来寻找。简单的说，就是通过平移特征工程，然后绘制自相关系数随着平移距离的函数图像。</p>
<p>有了这个周期性之后，就可以构建更多的特征。比方说同比（去年同月）或者环比（相邻月份）。环比和同比一般都是比率，我们可以再构建这个环比的梯度，类似二阶差分。</p>
<h2 id="qia-er-man-lu-bo-qi">卡尔曼滤波器</h2>
<p><strong>使用kalman滤波器可以作为时间序列的一种特征工程。</strong></p>
<p>在处理一些微观数据的时候，有的时候观测仪器会存在测量误差，这个时候我们可以使用卡尔曼滤波器来进行一定的矫正。对于每一个时间点的数据，获取的方法有两个：</p>
<ul>
<li>第一个就是观测，但是测量的结果不一定准确，可能受限于测量仪器的精度。</li>
<li>第二个就是用这个时间点之前的所有数据，预测这个时间点的数据，当然，这个预测值也是不准的。</li>
</ul>
<p>Kalman Filter卡尔曼滤波器利用这两种方法，相互促进，预测的值更准，或者说让观测到的值更接近本质。</p>
<h3 id="yuan-li">原理</h3>
<p>每一个观测数据，严谨的说都应该会有一个偏差值。比方说，现在温度计测量是26度，偏差值是0.5度，那么真实的问题应该是在\((25.5,26.5)\)之间，或者写成\(26 \pm 0.5\)。</p>
<p>这样<strong>预测的值</strong>，和<strong>观测的值</strong>，再加上这<strong>两个各自的偏差</strong>，总共四个已知信息，来推测真实的、更本质的数据。</p>
<blockquote>
<ul>
<li>预测的值：可以通过事先设定的公式，上一个时刻的真实的值算出来；</li>
<li>观测的值：直接读取测量仪器的值。</li>
<li>观测的值的偏差：这个也是可以直接得到的；</li>
<li>预测的值的偏差：这个是从上一个时间点的预测的值的偏差经过给定公式计算出来的。<br>
下面的公式中，脚标k表示时间点，k-1是上一个时间点。大写字母<code>A,B,C</code>表示常数，事先设定的；大写字母<code>H</code>，是一个需要计算的。</li>
</ul>
</blockquote>
<ul>
<li>预测的值：\(x_{k}^{\text {预测 }}=A * x_{k-1}^{\text {真实 }}+B * u_{k-1}\)</li>
<li>观测的值：\(x_{k}^{\text {观测 }}\)</li>
<li>观测的值的偏差：\(p_{k}^{\text {观测 }}\)</li>
<li>预测的值的偏差：\(p_{k}^{\text {预测 }}=\sqrt{(1-H)*(p_{k-1}^{\text{预测}})^2}\)</li>
<li>kalman增益H：\(H_{k}=\frac{\left(p_{k}^{\text {预测 }}\right)^{2}}{\left(p_{k}^{\text {预测 }}\right)^{2}+\left(p_{k}^{\text {观测 }}\right)^{2}}\)</li>
<li>真实的值： \(x_{k}^{\text {真实 }}=H_{k} * x_{k}^{\text {观測 }}+\left(1-H_{k}\right) * x_{k}^{\text {称测 }}\)</li>
</ul>
<p>可以看出，这个kalman增益就是一个加权平均的权重，是观测值更重要还是预测值更重要；两者的重要性就由两者的偏差大小决定，偏差小的更重要。</p>
<p>其中表示上一个时间点的控制信号，比方说一个机器人，机器人的状态去控制机器人自身的行为，但是很多情况这个控制信号是不用考虑的。比方对股市的时间序列做kalman滤波，那么并没有什么控制信号去控制，只是任由其自由发展。</p>
<h3 id="dai-ma">代码</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pykalman <span class="keyword">import</span> KalmanFilter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Kalman1D</span><span class="params">(observations,damping=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="comment"># To return the smoothed time series data</span></span><br><span class="line">    observation_covariance = damping</span><br><span class="line">    initial_value_guess = observations[<span class="number">0</span>]</span><br><span class="line">    transition_matrix = <span class="number">1</span></span><br><span class="line">    transition_covariance = <span class="number">0.1</span></span><br><span class="line">    initial_value_guess</span><br><span class="line">    kf = KalmanFilter(</span><br><span class="line">            initial_state_mean=initial_value_guess,</span><br><span class="line">            initial_state_covariance=observation_covariance,</span><br><span class="line">            observation_covariance=observation_covariance,</span><br><span class="line">            transition_covariance=transition_covariance,</span><br><span class="line">            transition_matrices=transition_matrix</span><br><span class="line">        )</span><br><span class="line">    pred_state, state_cov = kf.smooth(observations)</span><br><span class="line">    <span class="keyword">return</span> pred_state</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>initial_state_mean和initial_state_covariance：在上面的公式中，一开始的初始值，就是第一个观测值，但是在这个方法中，初始值并不是第一个观测值，而是由一个正态分布中随机采样出来的一个值，这个正态分布就是以initial_state_mean为均值，以initial_state_covariance为方差的；</li>
<li>observation_covariance这个可以相当于观测偏差；</li>
<li>transition_covariance这个就是预测偏差；</li>
<li>transition_matrices就是上面公式中的大写字母A，为1</li>
</ul>
</blockquote>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://mp.weixin.qq.com/s/buFe2q3bOEQmyRTb0ddNJw" target="_blank" rel="noopener">项目总结 | 对 “时间” 构建的特征工程</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>word_seg</title>
    <url>/2020/04/28/word_seg/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/04/28/word_seg/2.png" alt></p>
<a id="more"></a>
<h3 id="fen-ci-jian-jie">分词简介</h3>
<p>中文word_seg是指将一个汉字序列切分成一个一个单独的词，与英文以空格作为天然的分隔符不同，中文字符在语义识别时，需要把数个字符组合成词，才能表达出真正的含义。word_seg通常应用于自然语言处理、搜索引擎、智能推荐等领域。</p>
<p>word_seg根据其核心思想主要分为两种，第一种是基于词典的分词，先把句子按照字典切分成词，再寻找词的最佳组合方式；第二种是基于字的分词，即由字构词，先把句子分成一个个字，再将字组合成词，寻找最优的切分策略，同时也可以转化成序列标注问题。</p>
<p>其分类大致可分为：</p>
<ol>
<li>
<p>基于匹配规则的方法</p>
<ul>
<li>正向最大匹配法(forward maximum matching method, FMM)</li>
<li>逆向最大匹配法(backward maximum matching method, BMM)</li>
<li>最短路径word_seg</li>
</ul>
</li>
<li>
<p>基于统计以及机器学习的分词方法</p>
<ul>
<li>基于N-gram语言模型的分词方法</li>
<li>基于HMM的分词方法</li>
<li>基于CRF的分词方法</li>
<li>基于词感知机的分词方法</li>
<li>基于深度学习的端到端的分词方法</li>
</ul>
</li>
</ol>
<p>基于规则匹配的分词通常会加入一些启发式规则，比如“正向/反向最大匹配”，“长词优先”等。</p>
<p>基于统计以及机器学习的分词方法，它们基于人工标注的词性和统计特征，对中文进行建模，即根据观测到的数据(标注好的语料)对模型参数进行训练，在分词阶段再通过模型计算各种分词出现的概率，将概率最大的分词结果作为最终结果。这类word_seg能很好处理歧义和未登录词问题，效果比基于规则匹配的方法效果好，但是需要大量的人工标注数据，以及较慢的分词速度。</p>
<p><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码及数据</a></p>
<h4 id="zhong-wen-fen-ci-de-ying-yong">中文分词的应用</h4>
<p>目前在自然语言处理技术中，中文处理技术比西文处理技术要落后很大一段距离，许多西文的处理方法中文不能直接采用，就是因为中文必需有分词这道工序。中文分词是其他中文信息处理的基础，搜索引擎只是中文分词的一个应用。其他的比如机器翻译（MT）、语音合成、自动分类、自动摘要、自动校对等等，都需要用到分词。因为中文需要分词，可能会影响一些研究，但同时也为一些企业带来机会，因为国外的计算机处理技术要想进入中国市场，首先也是要解决中文分词问题。在中文研究方面，相比外国人来说，中国人有十分明显的优势。</p>
<p>分词准确性对搜索引擎来说十分重要，但如果分词速度太慢，即使准确性再高，对于搜索引擎来说也是不可用的，因为搜索引擎需要处理数以亿计的网页，如果分词耗用的时间过长，会严重影响搜索引擎内容更新的速度。因此对于搜索引擎来说，分词的准确性和速度，二者都需要达到很高的要求。目前研究中文分词的大多是科研院校，清华、北大、中科院、北京语言学院、东北大学、IBM研究院、微软中国研究院等都有自己的研究队伍，而真正专业研究中文分词的商业公司除了海量科技以外，几乎没有。</p>
<h3 id="ji-yu-pi-pei-gui-ze-de-fang-fa">基于匹配规则的方法</h3>
<p>主要是人工建立词库也叫做词典，通过词典匹配的方式对句子进行划分。其实现简单高效，但是对未登陆词很难进行处理。主要有前向最大匹配法，后向最大匹配法以及双向最大匹配法。</p>
<h4 id="qian-xiang-zui-da-pi-pei-suan-fa">前向最大匹配算法</h4>
<p>前向最大匹配算法，是从待分词句子的左边向右边搜索，寻找词的最大匹配。规定一个词的最大长度，每次扫描的时候寻找当前开始的这个长度的词来和字典中的词匹配，如果没有找到，就缩短长度继续寻找，直到找到字典中的词或者成为单字。</p>
<h5 id="suan-fa-liu-cheng">算法流程</h5>
<ol>
<li>从前向后扫描字典，测试读入的子串是否在字典中</li>
<li>如果存在，则从输入中删除掉该子串，重新按照规则取子串，重复1</li>
<li>如果不存在于字典中，则从右向左减少子串长度，重复1</li>
</ol>
<h5 id="fen-ci-shi-li">分词实例：</h5>
<p>比如说输入 “北京大学生前来应聘”，假设词典中最长的单词为 5 个（MAX_LENGTH）。</p>
<ol>
<li>第一轮：取子串 “北京大学生”，正向取词，如果匹配失败，<strong>每次去掉匹配字段最后面的一个字</strong><br>
“北京大学生”，扫描 5 字词典，没有匹配，子串长度减 1 变为“北京大学”<br>
“北京大学”，扫描 4 字词典，有匹配，输出“北京大学”，输入变为“生前来应聘”</li>
<li>第二轮：取子串“生前来应聘”<br>
“生前来应聘”，扫描 5 字词典，没有匹配，子串长度减 1 变为“生前来应”<br>
“生前来应”，扫描 4 字词典，没有匹配，子串长度减 1 变为“生前来”<br>
“生前来”，扫描 3 字词典，没有匹配，子串长度减 1 变为“生前”<br>
“生前”，扫描 2 字词典，有匹配，输出“生前”，输入变为“来应聘””</li>
<li>第三轮：取子串“来应聘”<br>
“来应聘”，扫描 3 字词典，没有匹配，子串长度减 1 变为“来应”<br>
“来应”，扫描 2 字词典，没有匹配，子串长度减 1 变为“来”<br>
颗粒度最小为 1，直接输出“来”，输入变为“应聘”</li>
<li>第四轮：取子串“应聘”<br>
“应聘”，扫描 2 字词典，有匹配，输出“应聘”，输入变为“”<br>
输入长度为0，扫描终止</li>
</ol>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>数据准备：词库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_max_match</span><span class="params">(sentence, window_size, word_dict)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    前向最大匹配算法：</span></span><br><span class="line"><span class="string">    （1）扫描字典，测试读入的子串是否在字典中</span></span><br><span class="line"><span class="string">    （2）如果存在，则从输入中删除掉该子串，重新按照规则取子串，重复（1）</span></span><br><span class="line"><span class="string">    （3）如果不存在于字典中，则从右向左减少子串长度，重复（1）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param sentence: 待分词的句子</span></span><br><span class="line"><span class="string">    :param window_size: 词的最大长度</span></span><br><span class="line"><span class="string">    :param word_dict: 词典</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    seg_words = []  <span class="comment"># 存放分词结果</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> sentence:</span><br><span class="line">        <span class="keyword">for</span> word_len <span class="keyword">in</span> range(window_size, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> sentence[:word_len] <span class="keyword">in</span> word_dict:</span><br><span class="line">                <span class="comment"># 如果该词再字典中，将该词保存到分词结果中,并将其从句中切出来</span></span><br><span class="line">                seg_words.append(sentence[:word_len])</span><br><span class="line">                sentence = sentence[word_len:]</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 如果窗口中的词不在字典中，将第一个字切分出来</span></span><br><span class="line">            seg_words.append(sentence[:word_len])</span><br><span class="line">            sentence = sentence[word_len:]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'/'</span>.join(seg_words)</span><br></pre></td></tr></table></figure>
<h4 id="hou-xiang-zui-da-pi-pei-suan-fa">后向最大匹配算法</h4>
<p>在词典中从句尾向句首进行扫描，尽可能地选择与词典中最长单词匹配的词作为目标分词，然后进行下一次匹配。</p>
<p>在实践中，逆向最大匹配算法性能优于正向最大匹配算法。</p>
<h5 id="suan-fa-liu-cheng-1">算法流程</h5>
<ol>
<li>从后向前扫描字典，测试读入的子串是否在字典中</li>
<li>如果存在，则从输入中删除掉该子串，重新按照规则取子串，重复1</li>
<li>如果不存在于字典中，则从左向右减少子串长度，重复1</li>
</ol>
<h5 id="fen-ci-shi-li-1">分词实例</h5>
<p>输入 “北京大学生前来应聘”，假设词典中最长的单词为 5 个（MAX_LENGTH）。</p>
<ol>
<li>第一轮：取子串 “生前来应聘”，逆向取词，如果匹配失败，<strong>每次去掉匹配字段最前面的一个字</strong><br>
“生前来应聘”，扫描 5 字词典，没有匹配，字串长度减 1 变为“前来应聘”<br>
“前来应聘”，扫描 4 字词典，没有匹配，字串长度减 1 变为“来应聘”<br>
“来应聘”，扫描 3 字词典，没有匹配，字串长度减 1 变为“应聘”<br>
“应聘”，扫描 2 字词典，有匹配，输出“应聘”，输入变为“大学生前来”</li>
<li>第二轮：取子串“大学生前来”<br>
“大学生前来”，扫描 5 字词典，没有匹配，字串长度减 1 变为“学生前来”<br>
“学生前来”，扫描 4 字词典，没有匹配，字串长度减 1 变为“生前来”<br>
“生前来”，扫描 3 字词典，没有匹配，字串长度减 1 变为“前来”<br>
“前来”，扫描 2 字词典，有匹配，输出“前来”，输入变为“北京大学生”</li>
<li>第三轮：取子串“北京大学生”<br>
“北京大学生”，扫描 5 字词典，没有匹配，字串长度减 1 变为“京大学生”<br>
“京大学生”，扫描 4 字词典，没有匹配，字串长度减 1 变为“大学生”<br>
“大学生”，扫描 3 字词典，有匹配，输出“大学生”，输入变为“北京”</li>
<li>第四轮：取子串“北京”<br>
“北京”，扫描 2 字词典，有匹配，输出“北京”，输入变为“”<br>
输入长度为0，扫描终止</li>
</ol>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-1"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>数据准备：词库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_max_match</span><span class="params">(sentence, window_size, word_dict)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    后向最大匹配算法：</span></span><br><span class="line"><span class="string">    （1）从后向前扫描字典，测试读入的子串是否在字典中</span></span><br><span class="line"><span class="string">    （2）如果存在，则从输入中删除掉该子串，重新按照规则取子串，重复（1）</span></span><br><span class="line"><span class="string">    （3）如果不存在于字典中，则从右向左减少子串长度，重复（1）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param sentence: 待分词的句子</span></span><br><span class="line"><span class="string">    :param window_size: 词的最大长度</span></span><br><span class="line"><span class="string">    :param word_dict: 词典</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    seg_words = []</span><br><span class="line">    <span class="keyword">while</span> sentence:</span><br><span class="line">        <span class="keyword">for</span> word_len <span class="keyword">in</span> range(window_size, <span class="number">0</span>, <span class="number">-1</span>):  <span class="comment"># 每次去掉匹配字段最前面的一个字</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> sentence[len(sentence) - word_len:] <span class="keyword">in</span> word_dict:</span><br><span class="line">                <span class="comment"># 如果该词再字典中，将该词保存到分词结果中,并将其从句中切出来</span></span><br><span class="line">                seg_words.append(sentence[len(sentence) - word_len:])</span><br><span class="line">                sentence = sentence[:len(sentence) - word_len]</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 如果窗口中的词不在字典中，将最后一个字切分出来</span></span><br><span class="line">            seg_words.append(sentence[<span class="number">-1</span>:])</span><br><span class="line">            sentence = sentence[:<span class="number">-1</span>]</span><br><span class="line">    seg_words.reverse()</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'/'</span>.join(seg_words)</span><br></pre></td></tr></table></figure>
<h4 id="shuang-xiang-zui-da-pi-pei-fa">双向最大匹配法</h4>
<p>因为同一个句子，在机械分词中经常会出现多种分词的组合，因此需要进行歧义消除，来得到最优的分词结果。</p>
<p>以很常见的MMSEG机械word_seg为例，MMSEG在搜索引擎Solr中经常使用到，是一种非常可靠高效的word_seg。MMSEG消除歧义的规则有四个，它在使用中依次用这四个规则进行过滤，直到只有一种结果或者第四个规则使用完毕。这个四个规则分别是：</p>
<ol>
<li>
<p>最大匹配。选择“词组长度最大的”那个词组，然后选择这个词组的第一个词，作为切分出的第一个词，如对于“中国人民万岁”，匹配结果分别为：</p>
<ul>
<li>中/国/人</li>
<li>中国/人/民</li>
<li>中国/人民/万岁</li>
<li>中国人/民/万岁</li>
</ul>
<p>在这个例子“词组长度最长的”词组为后两个，因此选择了“中国人/民/万岁”中的“中国人”，或者“中国/人民/万岁”中的“中国”。</p>
</li>
<li>
<p>最大平均词语长度。经过规则1过滤后，如果剩余的词组超过1个，那就选择平均词语长度最大的那个(平均词长=词组总字数/词语数量)。比如“生活水平”，可能得到如下词组：</p>
<ul>
<li>生/活水/平 (4/3=1.33)</li>
<li>生活/水/平 (4/3=1.33)</li>
<li>生活/水平 (4/2=2)</li>
</ul>
<p>根据此规则，就可以确定选择“生活/水平”这个词组</p>
</li>
<li>
<p>词语长度的最小变化率。这个变化率一般可以由标准差来决定。比如对于“中国人民万岁”这个短语，可以计算：</p>
<ul>
<li>中国/人民/万岁(标准差=sqrt(((2-2)<sup>2+(2-2)</sup>2+(2-2^2))/3)=0)</li>
<li>中国人/民/万岁(标准差=sqrt(((2-3)<sup>2+(2-1)</sup>2+(2-2)^2)/3)=0.8165)</li>
</ul>
<p>于是选择“中国/人民/万岁”这个词组。</p>
</li>
<li>
<p>计算词组中的所有单字词词频的自然对数，然后将得到的值相加，取总和最大的词组。比如：</p>
<ul>
<li>设施/和服/务</li>
<li>设施/和/服务</li>
</ul>
<p>这两个词组中分别有“务”和“和”这两个单字词，假设“务”作为单字词时候的频率是5，“和”作为单字词时候的频率是10，对5和10取自然对数，然后取最大值者，所以取“和”字所在的词组，即“设施/和/服务”。</p>
</li>
</ol>
<p>在实际中根据需求，选择或制定相应的规则来改善分词的质量。</p>
<h5 id="suan-fa-liu-cheng-2">算法流程</h5>
<ol>
<li>比较正向最大匹配和逆向最大匹配结果</li>
<li>如果分词数量结果不同，那么取分词数量较少的那个</li>
<li>如果分词数量结果相同
<ul>
<li>分词结果相同，可以返回任何一个</li>
<li>分词结果不同，返回单字数比较少的那个</li>
</ul>
</li>
</ol>
<p>这种规则的出发点来自语言学上的启发：汉语中单字词的数量要远小于非单字词。因此，算法应当尽量减少结果中的单字，保留更多的完整词语。</p>
<h5 id="fen-ci-shi-li-2">分词实例</h5>
<p>正向匹配最终切分结果为：北京大学 / 生前 / 来 / 应聘，分词数量为 4，单字数为 1<br>
逆向匹配最终切分结果为：”北京/ 大学生/ 前来 / 应聘，分词数量为 4，单字数为 0<br>
逆向匹配单字数少，因此返回逆向匹配的结果。</p>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-2"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>数据准备：词库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fb_max_match</span><span class="params">(sentence,window_size,word_dict)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    1. 比较正向最大匹配和逆向最大匹配结果</span></span><br><span class="line"><span class="string">    2. 如果分词数量结果不同，那么取分词数量较少的那个</span></span><br><span class="line"><span class="string">    3. 如果分词数量结果相同</span></span><br><span class="line"><span class="string">       * 分词结果相同，可以返回任何一个</span></span><br><span class="line"><span class="string">       * 分词结果不同，返回单字数比较少的那个</span></span><br><span class="line"><span class="string">    :param sentence: 待分词的句子</span></span><br><span class="line"><span class="string">    :param window_size: 词的最大长度</span></span><br><span class="line"><span class="string">    :param word_dict: 词典</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    forward_seg = forward_max_match(sentence,window_size,word_dict)</span><br><span class="line">    backward_seg = backward_max_match(sentence,window_size,word_dict)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果分词结果不同，返回词数较小的分词结果</span></span><br><span class="line">    <span class="keyword">if</span> len(forward_seg) != len(backward_seg):</span><br><span class="line">        <span class="keyword">return</span> forward_seg <span class="keyword">if</span> len(forward_seg) &lt; len(backward_seg) <span class="keyword">else</span> backward_seg</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果分词结果词数相同，优先考虑返回包含单个字符最少的分词结果</span></span><br><span class="line">        forward_single_word_count = len([filter(<span class="keyword">lambda</span> x:len(x) == <span class="number">1</span>,forward_seg)])</span><br><span class="line">        backward_single_word_count = len([filter(<span class="keyword">lambda</span> x:len(x) == <span class="number">1</span>,backward_seg)])</span><br><span class="line">        <span class="keyword">if</span> forward_single_word_count != backward_single_word_count:</span><br><span class="line">            <span class="keyword">return</span> forward_seg <span class="keyword">if</span> forward_single_word_count &lt; backward_single_word_count <span class="keyword">else</span> backward_seg</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 否则，返回任意结果</span></span><br><span class="line">            <span class="keyword">return</span> forward_seg</span><br></pre></td></tr></table></figure>
<h5 id="que-dian">缺点：</h5>
<p>对歧义问题的解决有点弱</p>
<h4 id="n-zui-duan-lu-jing-word-seg">(N-)最短路径word_seg</h4>
<h5 id="ji-ben-si-xiang">基本思想</h5>
<p>首先基于词典对文本进行全切分(改编版最大匹配)；然后基于词语的临接关系构建一个有向图；使用模型(比如一阶马尔科夫模型)对图里的边加权；最后使用最短路径算法求，从句首到句末质量最高(比如概率最大)的路径，就得到了分词结果。</p>
<h5 id="ju-li">举例</h5>
<p>最短路径word_seg首先将一句话中的所有词匹配出来，构成词图（有向无环图DAG），之后寻找从起始点到终点的最短路径作为最佳组合方式，以“他说的确实在理”为例，给出对这句话的3-最短路：<br>
<img src="/2020/04/28/word_seg/%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84word_seg.png" alt="avatar"></p>
<ol>
<li>
<p>构建有向图：根据一个已有词典构造出的有向无环图（全切分，根据临接关系构建有向图）。它将字串分为单个的字，每个字用图中相邻的两个结点表示，故对于长度为n的字串，需要n+1个结点。两节点间若有边，则表示两节点间所包含的所有结点构成的词，如图中结点2、3、4构成词“的确”。</p>
</li>
<li>
<p>计算权重：本例子中权重为1（为了简单），实际应用中，可以使用一阶马尔科夫模型进行权重计算。</p>
<ul>
<li>
<p>p(实，在)=p(实)*p(在|实)。概率取值越大，说明一个边出现的概率越大，这条边会提升所在分句结果的概率，由于最后要计算最短路径，需要构造一个连接权重与分局结果质量成反比的指标，因此对概率取了倒：weight=1/p(实,在)</p>
</li>
<li>
<p>这个概率可能非常小，得到的权重取值非常大。而我们后面在计算路径的长度时，会将若干个边的权重加起来，这时候有上溢出的可能性。避免上溢出的常用策略是取对数。：weight=log(1/p(实，在))。</p>
</li>
<li>
<p>概率的估算：概率就用频率来估计，p(实) = （“实”字在语料中出现的次数）/（语料的总词数），</p>
<p>p(在|实) = p(实在)/p(实)=(“实在”在语料中出现的次数)/(“实”在语料中出现的次数)</p>
</li>
</ul>
</li>
</ol>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-wei-te-bi-a"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码（维特比）</a></h5>
<p>数据准备：1、词库，2、词语之间的条件概率（用来计算路径权重）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用维特比算法求词图的最短路径</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(self, word_graph)</span>:</span></span><br><span class="line">    path_length_map = &#123;&#125;  <span class="comment"># 用于存储所有的路径，后面的邻接词语所在位置，以及对应的长度</span></span><br><span class="line">    word_graph = [[[<span class="string">"&lt;start&gt;"</span>, <span class="number">1</span>]]] + word_graph + [[[<span class="string">"&lt;end&gt;"</span>, <span class="number">-1</span>]]]</span><br><span class="line">    <span class="comment"># 这是一种比较简单的数据结构</span></span><br><span class="line">    path_length_map[(<span class="string">"&lt;start&gt;"</span>,)] = [<span class="number">1</span>, <span class="number">0</span>]  <span class="comment"># start处，后面的临接词语在列表的1处，路径长度是0,。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(word_graph)):</span><br><span class="line">        distance_from_start2current = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> len(word_graph[i]) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> former_path <span class="keyword">in</span> list(path_length_map.keys()):  <span class="comment"># path_length_map内容一直在变，需要深拷贝key,也就是已经积累的所有路径</span></span><br><span class="line">            <span class="comment"># 取出已经积累的路径，后面的临接词语位置，以及路径的长度。</span></span><br><span class="line">            [next_index_4_former_path, former_distance] = path_length_map[former_path]</span><br><span class="line">            former_word = former_path[<span class="number">-1</span>]</span><br><span class="line">            later_path = list(former_path)</span><br><span class="line">            <span class="keyword">if</span> next_index_4_former_path == i:  <span class="comment"># 如果这条路径的临接词语的位置就是当前索引</span></span><br><span class="line">                <span class="keyword">for</span> current_word <span class="keyword">in</span> word_graph[i]:  <span class="comment"># 遍历词图数据中，这个位置上的所有换选词语，然后与former_path拼接新路径</span></span><br><span class="line">                    current_word, next_index = current_word</span><br><span class="line">                    new_path = tuple(later_path + [current_word])  <span class="comment"># 只有int, string, tuple这种不可修改的数据类型可以hash，</span></span><br><span class="line">                    <span class="comment"># 也就是成为dict的key</span></span><br><span class="line">                    <span class="comment"># 计算新路径的长度</span></span><br><span class="line">                    new_path_len = former_distance + self.word_distance.get((former_word, current_word), <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">                    path_length_map[new_path] = [next_index, new_path_len]  <span class="comment"># 存储新路径后面的临接词语，以及路径长度</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 维特比的部分。选择到达当前节点的路径中，最短的那一条</span></span><br><span class="line">                    <span class="keyword">if</span> current_word <span class="keyword">in</span> distance_from_start2current:  <span class="comment"># 如果已经有到达当前词语的路径，需要择优</span></span><br><span class="line">                        <span class="keyword">if</span> distance_from_start2current[current_word][<span class="number">1</span>] &gt; new_path_len:  <span class="comment"># 如果当前新路径比已有的更短</span></span><br><span class="line">                            distance_from_start2current[current_word] = [new_path, new_path_len]  <span class="comment"># 用更短的路径数据覆盖原来的</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        distance_from_start2current[current_word] = [new_path, new_path_len]  <span class="comment"># 如果还没有这条路径，就记录它</span></span><br><span class="line">    shortest_path = distance_from_start2current[<span class="string">"&lt;end&gt;"</span>][<span class="number">0</span>]</span><br><span class="line">    shortest_path = shortest_path[<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> shortest_path</span><br></pre></td></tr></table></figure>
<h4 id="que-dian-1">缺点</h4>
<ol>
<li>对歧义和新词的处理不是很好，对词典中未出现的词没法进行处理。</li>
<li>分词效果取决于词典的质量。</li>
</ol>
<h3 id="ji-yu-tong-ji-yi-ji-ji-qi-xue-xi-de-fen-ci-fang-fa">基于统计以及机器学习的分词方法</h3>
<h4 id="ji-yu-n-gram-yu-yan-mo-xing-de-fen-ci-fang-fa">基于N-gram语言模型的分词方法</h4>
<h5 id="jian-jie">简介</h5>
<p>n-gram模型，称为N元模型，可用于定义字符串中的距离，也可用于中文的分词；该模型假设第n个词的出现只与前面n-1个词相关，与其他词都不相关，整个语句的概率就是各个词出现概率的乘积；而这些概率，利用语料，统计同时出现相关词的概率次数计算得到；常用的模型是Bi-gram和Tri-gram模型。<br>
假设一个字符串s由m个词组成，因此我们需要计算出\(p(w_1,w_2,\ldots,w_m)\)的概率，根据概率论中的链式法则得到如下：<br>
\[
p(w_1,w_2,\ldots,w_m) = p(w_1)*p(w_2|w_1)*p(w_3|w_2,w_1)\ldots p(w_m|w_{m-1},\ldots,w_2,w_1)=\prod_{i}{p(w_i|w_1,w_2,\ldots,w_{i-1})}
\]<br>
那么下面的问题是如何计算上面每一个概率，比如\(p(w_1,w_2,w_3,w_4,w_5)\)，一种比较直观的计算就是计数然后用除法：<br>
\[
p(w_5|w_1,w_2,w_3,w_4) = \frac{Count(w_1,w_2,w_3,w_4,w_5)}{Count(w_1,w_2,w_3,w_4)}
\]</p>
<p>直接计算这个概率的难度有点大：</p>
<ol>
<li>
<p>直接这样计算会导致参数空间过大。</p>
<p>一个语言模型的参数就是所有的这些条件概率，试想按上面方式计算\(p(w_5|w_1,w_2,w_3,w_4)\),这里\(w_5\)有词典大小取值的可能，记词典大小：\(|V|\)，则该模型的参数个数是\(|V|^5\)，而且这还不包含\(P(w_4|w_1,w_2,w_3)\)的个数，可以看到这样去计算条件概率会使语言模型参数个数过多而无法使用。</p>
</li>
<li>
<p>数据稀疏严重。我的理解是像上面那样计数计算，比如计数分子\(w_1,w_2,w_3,w_4,w_5\),在我们所能见的文本中出现的次数是很小的，这样计算的结果是过多的条件概率会等于0，因为我们根本没有看到足够的文本来统计！假设一个语料库中单词的数量为\(|V|\)个，一个句子由\(n\)个词组成，那么每个单词都可能有\(|V|\)个取值，那么由这些词组成的\(n\)元组合的数目为\(|V|^n\)种，也就是说，组合数会随着\(n\)的增加而呈现指数级别的增长，随着\(n\)的增加，语料数据库能够提供的数据是非常有限的，除非有海量的各种类型的语料数据，否则还有大量的\(n\)元组合都没有在语料库中出现过（即由\(n\)个单词所组成的一些句子根本就没出现过，可以理解为很多的\(n\)元组所组成的句子不能被人很好地理解）也就是说依据最大似然估计得到的概率将会是0，模型可能仅仅能够计算寥寥几个句子。怎么解决呢？</p>
</li>
</ol>
<p>解决參数空间过大的问题。引入了马尔科夫假设：**随意一个词出现的概率只与它前面出现的有限的一个或者几个词有关。**假设第\(w_i\)个词语只与它前面的\(n\)个词语相关，这样我们就得到前面的条件概率计算简化如下<br>
\[
p(w_i|w_{i-1},\ldots,w_2,w_1) \approx p(w_i|w_{i-1},\ldots,w_{i-n})\\
p(w_1,w_2,\ldots,w_m) \approx \prod_{i} p(w_i|w_{i-1},\ldots,w_{i-n})
\]</p>
<p>当n=1，即一元模型（Uni-gram）,即\(w_i\)与它前面的0个词相关，即\(w_i\)不与任何词相关，每一个词都是相互独立的：<br>
\[
P\left(w_{1}, w_{2}, \cdots, w_{m}\right)=\prod_{i=1}^{m} P\left(w_{i}\right)
\]<br>
当n=2，即二元模型（Bi-gram）,此时\(w_i\)与它前面1个词相关：<br>
\[
P\left(w_{1}, w_{2}, \cdots, w_{m}\right)=\prod_{i=1}^{m} P\left(w_{i} | w_{i-1}\right)
\]<br>
当n=3时，即三元模型（Tri-gram）,此时\(w_i\)与它前面2个词相关：<br>
\[
P\left(w_{1}, w_{2}, \cdots, w_{m}\right)=\prod_{i=1}^{m} P\left(w_{i} | w_{i-2} w_{i-1}\right)
\]<br>
一般来说，N元模型就是假设当前词的出现概率只与它前面的N-1个词有关。而这些概率参数都是可以通过大规模语料库来计算。<strong>在实践中用的最多的就是bigram和trigram了，高于四元的用的非常少，由于训练它须要更庞大的语料，并且数据稀疏严重，时间复杂度高，精度却提高的不多。</strong></p>
<h5 id="can-shu-gu-ji">参数估计</h5>
<p>要计算出模型中的条件概率，这些条件概率也称为模型的参数，得到这些参数的过程称为训练。用最大似然性估计计算下面的条件概率：<br>
\[
P\left(w_{i} | w_{i-1}\right)=\frac{c\left(w_{i-1}, w_{i}\right)}{c\left(w_{i-1}\right)}
\]<br>
一元语言模型中：句子概率定义为：\(P\left(s\right)=\prod_{i=1}^{m} P\left(w_{i}\right)\),这个式子成立的条件是有一个假设，就是条件无关假设，我们认为每个词都是条件无关的。这里的参数种类是一种 \(P(w_n)\),但是参数实例有\(|V|\)个(V是词典大小),我们应该如何得到每个参数实例的值。用的是极大似然估计法。比如训练语料是:&quot;星期五早晨，我特意起了个大早，为的就是看看早晨的天空。&quot;那么我们的字典为：星 期 五 早 晨 ，我 特 意 起 了 个 大 早 为 的 就 是 看 天 空 。 22个不同词，每个词语的概率直接用极大似然估计法估计得到。如：p(星) = 1/27，p(期) = 1/27。于是需要存储学习得到的模型参数，一个向量，22维，每个维度保存着每个单词的概率值。当需要计算：p(我看看早晨的天空)=p(我)p(看)p(看)p(早)p(晨)p(的)p(天)p(空)=\(\frac{1}{27}*\frac{1}{27}*\frac{1}{27}\ldots *\frac{1}{27}\)，可以直接计算出来。于是只要将每句话拆开为每个单词然后用累积形式运算，这样就能算出每句话的概率。缺点是：不包含语序信息。</p>
<p>二元语言模型中：为了计算对应的二元模型的参数，即\(P(w_i | w_{i-1})\)，要先计数即\(c(w_{i-1},w_i)\)，然后计数\(c(w_{i-1})\),再用除法可得到这些条件概率.可以看到对于\(c(w_{i-1},w_i)\)来说，\(w_{i-1}\)有语料库词典大小（记作\(|V|\)）的可能取值，\(w_i\)也是，所以\(c(w_{i-1},w_i)\)要计算的个数有\(|V|^2\)。</p>
<p>\(c(w_{i-1},w_i)\)计数结果如下:</p>
<table>
<thead>
<tr>
<th></th>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>english</th>
</tr>
</thead>
<tbody>
<tr>
<td>i</td>
<td>5</td>
<td>827</td>
<td>0</td>
<td>9</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>want</td>
<td>2</td>
<td>0</td>
<td>608</td>
<td>1</td>
<td>6</td>
<td>6</td>
<td>5</td>
<td>1</td>
</tr>
<tr>
<td>to</td>
<td>2</td>
<td>0</td>
<td>4</td>
<td>686</td>
<td>2</td>
<td>0</td>
<td>6</td>
<td>211</td>
</tr>
<tr>
<td>eat</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>16</td>
<td>2</td>
<td>42</td>
<td>0</td>
</tr>
<tr>
<td>chinese</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>82</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>food</td>
<td>15</td>
<td>0</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>4</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>lunch</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>spend</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>\(c(w_{i-1})\)的计数如下：</p>
<table>
<thead>
<tr>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>english</th>
</tr>
</thead>
<tbody>
<tr>
<td>2533</td>
<td>927</td>
<td>2417</td>
<td>746</td>
<td>158</td>
<td>1093</td>
<td>341</td>
<td>278</td>
</tr>
</tbody>
</table>
<p>那么二元模型的参数计算结果如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>english</th>
</tr>
</thead>
<tbody>
<tr>
<td>i</td>
<td>0.002</td>
<td>0.33</td>
<td>0</td>
<td>0.0036</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.00079</td>
</tr>
<tr>
<td>want</td>
<td>0.0022</td>
<td>0</td>
<td>0.66</td>
<td>0.0011</td>
<td>0.0065</td>
<td>0.0065</td>
<td>0.0054</td>
<td>0.0011</td>
</tr>
<tr>
<td>to</td>
<td>0.00083</td>
<td>0</td>
<td>0.0017</td>
<td>0.28</td>
<td>0.00083</td>
<td>0</td>
<td>0.0025</td>
<td>0.087</td>
</tr>
<tr>
<td>eat</td>
<td>0</td>
<td>0</td>
<td>0.0027</td>
<td>0</td>
<td>0.021</td>
<td>0.0027</td>
<td>0.056</td>
<td>0</td>
</tr>
<tr>
<td>chinese</td>
<td>0.0063</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.52</td>
<td>0.0063</td>
<td>0</td>
</tr>
<tr>
<td>food</td>
<td>0.014</td>
<td>0</td>
<td>0.0014</td>
<td>0</td>
<td>0.00092</td>
<td>0.0037</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>lunch</td>
<td>0.0059</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.0029</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>spend</td>
<td>0.0036</td>
<td>0</td>
<td>0.0036</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>比如计算其中的P(want | i) = 0.33如:\(p(want|i) = \frac{c(i,want)}{c(i)}=\frac{827}{2533} \approx0.33\),针对这个语料库的二元模型建立好了后，可以计算目标，即一个句子的概率了，一个例子如下：</p>
<p>\(p(s)=p(i want english food) = p(i|&lt;start>)*p(want|i)*p(english|want)*p(food|english)*p(&lt;end>|food) \approx 0.000031\)</p>
<p>该二元模型所捕捉到的一些实际信息:</p>
<ul>
<li>\(p(english|want) = 0.0011,p(chinese|want)=0.0065\),want chinese 的概率更高，这和真实世界情况相对应，因为chinese food 比 English food 更受欢迎。</li>
<li>\(p(to|want)=0.66\), want to 的概率很高，反映了语法特性</li>
<li>\(p(food|to)=0\) ，to food 概率为0，因为这种搭配不常见</li>
<li>\(p(want|spend)=0\), spend want 概率为0，因为这样违反了语法。</li>
</ul>
<p>常常在对数空间里面计算概率，原因有两个：</p>
<ol>
<li>防止溢出，如果计算的句子很长，最后得到的结果将非常小，甚至会溢出，比如计算得到的概率是0.001，那么假设以10为底取对数的结果就是-3，这样就不会溢出。</li>
<li>对数空间里面加法可以代替乘法，因为log(p1p2) = logp1 + logp2，而在计算机内部，显然加法比乘法执行更快！</li>
</ol>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-3"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>数据准备：unigram:(word, freq),bigram:(word1,word2,freq) 语料数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="comment"># global parameter</span></span><br><span class="line">DELIMITER = <span class="string">" "</span>  <span class="comment"># 分词之后的分隔符</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DNASegment</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.word1_dict_prob = &#123;&#125;  <span class="comment"># 记录概率,1-gram</span></span><br><span class="line">        self.word1_dict_count = &#123;&#125;  <span class="comment"># 记录词频,1-gram</span></span><br><span class="line">        self.word1_dict_count[<span class="string">"&lt;S&gt;"</span>] = <span class="number">8310575403</span>  <span class="comment"># 开始的&lt;S&gt;的个数</span></span><br><span class="line"></span><br><span class="line">        self.word2_dict_prob = &#123;&#125;  <span class="comment"># 记录概率,2-gram</span></span><br><span class="line">        self.word2_dict_count = &#123;&#125;  <span class="comment"># 记录词频,2-gram</span></span><br><span class="line"></span><br><span class="line">        self.gmax_word_length = <span class="number">0</span></span><br><span class="line">        self.all_freq = <span class="number">0</span>  <span class="comment"># 所有词的词频总和,1-gram的</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 估算未出现的词的概率,根据beautiful data里面的方法估算</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_unkonw_word_prob</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> math.log(<span class="number">10.</span> / (self.all_freq * <span class="number">10</span> ** len(word)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得片段的概率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_prob</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> self.word1_dict_prob:  <span class="comment"># 如果字典包含这个词</span></span><br><span class="line">            prob = self.word1_dict_prob[word]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            prob = self.get_unkonw_word_prob(word)</span><br><span class="line">        <span class="keyword">return</span> prob</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得两个词的转移概率(bigram)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_trans_prob</span><span class="params">(self, first_word, second_word)</span>:</span></span><br><span class="line">        trans_word = first_word + <span class="string">" "</span> + second_word</span><br><span class="line">        <span class="comment"># print trans_word</span></span><br><span class="line">        <span class="keyword">if</span> trans_word <span class="keyword">in</span> self.word2_dict_count:</span><br><span class="line">            trans_prob = math.log(self.word2_dict_count[trans_word] / self.word1_dict_count[first_word])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            trans_prob = self.get_word_prob(second_word)</span><br><span class="line">        <span class="keyword">return</span> trans_prob</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 寻找node的最佳前驱节点</span></span><br><span class="line">    <span class="comment"># 方法为寻找所有可能的前驱片段</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_best_pre_node</span><span class="params">(self, sequence, node, node_state_list)</span>:</span></span><br><span class="line">        <span class="comment"># 如果node比最大词长小，取的片段长度以node的长度为限</span></span><br><span class="line">        max_seg_length = min([node, self.gmax_word_length])</span><br><span class="line">        pre_node_list = []  <span class="comment"># 前驱节点列表</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得所有的前驱片段，并记录累加概率</span></span><br><span class="line">        <span class="keyword">for</span> segment_length <span class="keyword">in</span> range(<span class="number">1</span>, max_seg_length + <span class="number">1</span>):</span><br><span class="line">            segment_start_node = node - segment_length</span><br><span class="line">            segment = sequence[segment_start_node:node]  <span class="comment"># 获取片段</span></span><br><span class="line"></span><br><span class="line">            pre_node = segment_start_node  <span class="comment"># 取该片段，则记录对应的前驱节点</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> pre_node == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 如果前驱片段开始节点是序列的开始节点，</span></span><br><span class="line">                <span class="comment"># 则概率为&lt;S&gt;转移到当前词的概率</span></span><br><span class="line">                <span class="comment"># segment_prob = self.get_word_prob(segment)</span></span><br><span class="line">                segment_prob = self.get_word_trans_prob(<span class="string">"&lt;S&gt;"</span>, segment)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 如果不是序列开始节点，按照二元概率计算</span></span><br><span class="line">                <span class="comment"># 获得前驱片段的前一个词</span></span><br><span class="line">                pre_pre_node = node_state_list[pre_node][<span class="string">"pre_node"</span>]</span><br><span class="line">                pre_pre_word = sequence[pre_pre_node:pre_node]</span><br><span class="line">                segment_prob = self.get_word_trans_prob(pre_pre_word, segment)</span><br><span class="line"></span><br><span class="line">            pre_node_prob_sum = node_state_list[pre_node][<span class="string">"prob_sum"</span>]  <span class="comment"># 前驱节点的概率的累加值</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 当前node一个候选的累加概率值</span></span><br><span class="line">            candidate_prob_sum = pre_node_prob_sum + segment_prob</span><br><span class="line"></span><br><span class="line">            pre_node_list.append((pre_node, candidate_prob_sum))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 找到最大的候选概率值</span></span><br><span class="line">        (best_pre_node, best_prob_sum) = max(pre_node_list, key=<span class="keyword">lambda</span> d: d[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> (best_pre_node, best_prob_sum)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最大概率分词</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mp_seg</span><span class="params">(self, sequence)</span>:</span></span><br><span class="line">        sequence = sequence.strip()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化</span></span><br><span class="line">        node_state_list = []  <span class="comment"># 记录节点的最佳前驱，index就是位置信息</span></span><br><span class="line">        <span class="comment"># 初始节点，也就是0节点信息</span></span><br><span class="line">        ini_state = &#123;&#125;</span><br><span class="line">        ini_state[<span class="string">"pre_node"</span>] = <span class="number">-1</span>  <span class="comment"># 前一个节点</span></span><br><span class="line">        ini_state[<span class="string">"prob_sum"</span>] = <span class="number">0</span>  <span class="comment"># 当前的概率总和</span></span><br><span class="line">        node_state_list.append(ini_state)</span><br><span class="line">        <span class="comment"># 字符串概率为2元概率</span></span><br><span class="line">        <span class="comment"># P(a b c) = P(a|&lt;S&gt;)P(b|a)P(c|b)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 逐个节点寻找最佳前驱节点</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> range(<span class="number">1</span>, len(sequence) + <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 寻找最佳前驱，并记录当前最大的概率累加值</span></span><br><span class="line">            (best_pre_node, best_prob_sum) = self.get_best_pre_node(sequence, node, node_state_list)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 添加到队列</span></span><br><span class="line">            cur_node = &#123;&#125;</span><br><span class="line">            cur_node[<span class="string">"pre_node"</span>] = best_pre_node</span><br><span class="line">            cur_node[<span class="string">"prob_sum"</span>] = best_prob_sum</span><br><span class="line">            node_state_list.append(cur_node)</span><br><span class="line">            <span class="comment"># print "cur node list",node_state_list</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># step 2, 获得最优路径,从后到前</span></span><br><span class="line">        best_path = []</span><br><span class="line">        node = len(sequence)  <span class="comment"># 最后一个点</span></span><br><span class="line">        best_path.append(node)</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            pre_node = node_state_list[node][<span class="string">"pre_node"</span>]</span><br><span class="line">            <span class="keyword">if</span> pre_node == <span class="number">-1</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            node = pre_node</span><br><span class="line">            best_path.append(node)</span><br><span class="line">        best_path.reverse()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># step 3, 构建切分</span></span><br><span class="line">        word_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(best_path) - <span class="number">1</span>):</span><br><span class="line">            left = best_path[i]</span><br><span class="line">            right = best_path[i + <span class="number">1</span>]</span><br><span class="line">            word = sequence[left:right]</span><br><span class="line">            word_list.append(word)</span><br><span class="line"></span><br><span class="line">        seg_sequence = DELIMITER.join(word_list)</span><br><span class="line">        <span class="keyword">return</span> seg_sequence</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载词典，为词\t词频的格式</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initial_dict</span><span class="params">(self, gram1_file, gram2_file)</span>:</span></span><br><span class="line">        <span class="comment"># 读取unigram文件</span></span><br><span class="line">        dict_file = open(gram1_file, <span class="string">"r"</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> dict_file:</span><br><span class="line">            sequence = line.strip()</span><br><span class="line">            key = sequence.split(<span class="string">'\t'</span>)[<span class="number">0</span>]</span><br><span class="line">            value = float(sequence.split(<span class="string">'\t'</span>)[<span class="number">1</span>])</span><br><span class="line">            self.word1_dict_count[key] = value</span><br><span class="line">        <span class="comment"># 计算频率</span></span><br><span class="line">        self.all_freq = sum(self.word1_dict_count.values())  <span class="comment"># 所有词的词频</span></span><br><span class="line">        self.gmax_word_length = max(len(key) <span class="keyword">for</span> key <span class="keyword">in</span> self.word1_dict_count.keys())</span><br><span class="line">        self.gmax_word_length = <span class="number">20</span></span><br><span class="line">        self.all_freq = <span class="number">1024908267229.0</span></span><br><span class="line">        <span class="comment"># 计算1gram词的概率</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> self.word1_dict_count:</span><br><span class="line">            self.word1_dict_prob[key] = math.log(self.word1_dict_count[key] / self.all_freq)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取2_gram_file，同时计算转移概率</span></span><br><span class="line">        dict_file = open(gram2_file, <span class="string">"r"</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> dict_file:</span><br><span class="line">            sequence = line.strip()</span><br><span class="line">            key = sequence.split(<span class="string">'\t'</span>)[<span class="number">0</span>]</span><br><span class="line">            value = float(sequence.split(<span class="string">'\t'</span>)[<span class="number">1</span>])</span><br><span class="line">            first_word = key.split(<span class="string">" "</span>)[<span class="number">0</span>]</span><br><span class="line">            second_word = key.split(<span class="string">" "</span>)[<span class="number">1</span>]</span><br><span class="line">            self.word2_dict_count[key] = float(value)</span><br><span class="line">            <span class="keyword">if</span> first_word <span class="keyword">in</span> self.word1_dict_count:</span><br><span class="line">                <span class="comment"># 取自然对数</span></span><br><span class="line">                self.word2_dict_prob[key] = math.log(value / self.word1_dict_count[first_word])  </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.word2_dict_prob[key] = self.word1_dict_prob[second_word]</span><br></pre></td></tr></table></figure>
<h5 id="zong-jie">总结</h5>
<p><strong>列举出所有可能的分词方式，再分别计算句子概率，选择概率最大的作为最终分词结果。穷举法，速度慢。</strong></p>
<p>缺点：</p>
<ol>
<li>N-grams有一些不足，因为语言存在一个长距离依赖关系，比如：“The computer which I had just put into the machine room on the fifth floor crashed.”假如要预测最后一个词语crashed出现的概率，如果采用二元模型，那么crashed与floor实际关联可能性应该非常小，相反的，这句子的主语computer与crashed的相关性很大，但是n-grams并没有捕捉到这个信息。</li>
<li>一个词是由前一个或者几个词决定的，这样可以去除一部分歧义问题，但是n-gram模型还是基于马尔科夫模型的，其基本原理就是无后效性，就是后续的节点的状态不影响前面的状态，就是先前的分词形式一旦确定，无论后续跟的是什么词，都不会再有变化，这在现实中显然是不成立的。。因此就有一些可以考虑到后续词的算法，如crf等方法。</li>
</ol>
<h4 id="ji-yu-hmm-de-word-seg">基于HMM的word_seg</h4>
<h5 id="yin-ma-er-ke-fu-mo-xing-hmm">隐马尔科夫模型(HMM)</h5>
<p>隐马尔可夫模型是关于<strong>时序</strong>的概率模型,描述由一个隐藏的马尔可夫链随机生成不可观测的<strong>状态序列</strong>,再由各个状态生成一个观测而产生<strong>观测随机序列</strong>的过程.HMM是一种生成式模型，它的理论基础是朴素贝叶斯，本质上就类似于我们将朴素贝叶斯在单样本分类问题上的应用推广到序列样本分类问题上。</p>
<h6 id="mo-xing-biao-shi">模型表示</h6>
<p>设Q是所有可能的状态的集合\(Q=\left\{q_{1}, q_{2}, \cdots, q_{N}\right\}\),V是所有可能的观测的集合\(V=\left\{v_{1}, v_{2}, \cdots, v_{M}\right\}\),I是长度为T的状态序列\(I=\left(i_{1}, i_{2}, \cdots, i_{T}\right)\), O是对应的观测序列\(O=\left(o_{1}, o_{2}, \cdots, o_{T}\right)\),</p>
<ol>
<li>A是<strong>状态转移概率矩阵</strong>\(A=\left[a_{i j}\right]_{N \times N}\),\(a_{ij}\)表示在时刻t处于状态\(q_i\)的条件下在时刻t+1转移到状态\(q_j\)的概率.</li>
<li>B是<strong>观测概率矩阵</strong> \(B=\left[b_{j}(k)\right]_{N \times M}\),\(b_{ij}\)是在时刻t处于状态\(q_j\)的条件下生成观测\(v_k\)的概率.</li>
<li>\(\pi\)是<strong>初始状态概率向量</strong>\(\pi=\pi(x)\),\(\pi_i\)表示时刻t=1处于状态qi的概率.</li>
</ol>
<p>隐马尔可夫模型由初始状态概率向量\(pi\),状态转移概率矩阵A以及观测概率矩阵B确定.\(\pi\)和A决定即隐藏的<strong>马尔可夫链</strong>,生成不可观测的<strong>状态序列</strong>.B决定如何从状态生成观测,与状态序列综合确定了<strong>观测序列</strong>.因此,隐马尔可夫模型可以用<strong>三元符号</strong>表示\(\lambda = (A,B,\pi)\)</p>
<h6 id="liang-ge-ji-ben-jia-she">两个基本假设</h6>
<ol>
<li><strong>齐次马尔可夫性假设</strong>:假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态.</li>
<li><strong>观测独立性假设</strong>:假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态.</li>
</ol>
<h6 id="san-ge-ji-ben-wen-ti">三个基本问题</h6>
<p><strong>1. 概率计算问题</strong></p>
<p>给定模型\(\lambda = (A,B,\pi)\)和观测序列,\(O=\left(o_{1}, o_{2}, \cdots, o_{T}\right)\)计算在模型\(\lambda\)下观测序列O出现的概率\(P(O|λ)\).</p>
<ol>
<li>直接计算：列举所有可能长度为T的状态序列,求各个状态序列I与观测序列O的联合概率,但计算量太大,实际操作不可行.</li>
<li><strong>前向算法</strong>：定义到时刻t部分观测序列为\(o_1\)~\(o_t\)且状态为\(q_i\)的概率为<strong>前向概率</strong>,记作\(\alpha_{t}(i)=P\left(o_{1}, o_{2}, \cdots, o_{t}, i_{t}=q_{i} | \lambda\right)\).初始化前向概率\(\alpha_{1}(i)=\pi_{i} b_{i}\left(o_{1}\right), \quad i=1,2, \cdots, N\)，递推，对\(t=1\) ~ \(T-1\),\(\alpha_{t+1}(i)=\left[\sum_{j=1}^{N} \alpha_{t}(j) a_{j i}\right] b_{i}\left(o_{t+1}\right), \quad i=1,2, \cdots, N\),得到\(P(O | \lambda)=\sum_{i=1}^{N} \alpha_{T}(i)\)减少计算量的原因在于每一次计算直接引用前一个时刻的计算结果,避免重复计算.</li>
<li><strong>后向算法</strong>:定义在时刻t状态为\(q_i\)的条件下,从t+1到T的部分观测序列为\(o_{i+1}\)~\(o_T\)的概率为<strong>后向概率</strong>,记作\(\beta_{t}(i)=P\left(o_{t+1}, o_{t+2}, \cdots, o_{r} | i_{t}=q_{i}, \lambda\right)\).初始化后向概率\(\beta_{r}(i)=1, \quad i=1,2, \cdots, N\),递推,对\(t=T-1\)~\(1\)\(\beta_{t}(i)=\sum_{j=1}^{N} a_{i j} b_{j}\left(o_{i+1}\right) \beta_{i+1}(j), \quad i=1,2, \cdots, N\),得到\(P(O | \lambda)=\sum_{i=1}^{N} \pi_{i} b_{i}\left(o_{1}\right) \beta_{1}(i)\)</li>
</ol>
<p><strong>2. 学习算法</strong></p>
<p>已知观测序列\(O=(o_1,o_2, \cdots,o_r)\),估计模型\(\lambda = (A,B,\pi)\),的参数,使得在该模型下观测序列概率\(p(O|\lambda)\)最大.根据训练数据是否包括观察序列对应的状态序列分别由监督学习与非监督学习实现.</p>
<ol>
<li>
<p>监督学习：估计转移概率\(\hat{a}_{i j}=\frac{A_{i j}}{\sum_{j=1}^{N} A_{i j}}, \quad i=1,2, \cdots, N ; \quad j=1,2, \cdots, N\) 和观测概率\(\hat{b}_{j}(k)=\frac{B_{j k}}{\sum_{k=1}^{M} B_{j k}}, \quad j=1,2, \cdots, N, k=1,2, \cdots, M\).初始状态概率\(\pi_i\)的估计为S个样本中初始状态为\(q_i\)的频率.</p>
</li>
<li>
<p><strong>非监督学习(Baum-Welch算法)</strong>:将观测序列数据看作观测数据O,状态序列数据看作不可观测的<strong>隐数据</strong>I.首先确定完全数据的对数似然函数\(log p(O,I|\lambda)\),求Q函数</p>
</li>
</ol>
<p>\[
   \begin{aligned}
   Q(\lambda, \bar{\lambda})=&amp; \sum_{t} \log \pi_{i_1} P(O, I | \bar{\lambda}) \\
   &amp;+\sum_{I}\left(\sum_{i=1}^{I-1} \log a_{i_t,i_{t+1}}\right) P(O, I | \bar{\lambda}) \\
   &amp;+\sum_{I}\left(\sum_{i=1}^{T} \log b_{i_t}\left(o_{t}\right)\right) P(O, I | \bar{\lambda})
   \end{aligned}
\]</p>
<p>,用拉格朗日乘子法极大化Q函数求模型参数\(\pi_{i}=\frac{P\left(O, i_{1}=i | \bar{\lambda}\right)}{P(O | \bar{\lambda})}\),\(a_{i j}=\frac{\sum_{i=1}^{T-1} P\left(O, i_{t}=i, i_{t+1}=j | \bar{\lambda}\right)}{\sum_{t=1}^{T-1} P\left(O, i_{t}=i | \bar{\lambda}\right)}\),\(b_{j}(k)=\frac{\sum_{i=1}^{T} P\left(O, i_{t}=j | \bar{\lambda}\right) I\left(o_{i}=v_{k}\right)}{\sum_{i=1}^{T} P\left(O, i_{t}=j | \bar{\lambda}\right)}\),</p>
<p><strong>3. 预测问题</strong></p>
<p>也称为解码问题.已知模型\(\lambda = (A,B,\pi)\)和观测序列\(O=(O_1,O_2,\cdots,O_T)\),求对给定观测序列条件概率\(P(I|O)\)最大的状态序列\(I=(i_1,i_2,\cdots,i_T)\)</p>
<ol>
<li>
<p><strong>近似算法</strong>: 在每个时刻t选择在该时刻最有可能出现的状态\(i_t^*\),从而得到一个状态序列作为预测的结果.优点是<strong>计算简单</strong>,缺点是不能保证状态序列整体是最有可能的状态序列</p>
</li>
<li>
<p><strong>维特比算法</strong>:用<strong>动态规划</strong>求概率最大路径,这一条路径对应着一个状态序列.从t=1开始,递推地计算在时刻t状态为i的各条部分路径的最大概率,直至得到时刻t=T状态为i的各条路径的最大概率.时刻t=T的最大概率即为<strong>最优路径</strong>的概率\(P^\star\),最优路径的<strong>终结点</strong>\(i_t^\star\)也同时得到,之后从终结点开始由后向前逐步求得<strong>结点</strong>,得到最优路径.</p>
</li>
</ol>
<h5 id="jian-jie-1">简介</h5>
<p>在word_seg中，分词对应着三大问题中的预测问题（解码问题），隐马尔可夫经常用作能够发现新词的算法，通过海量的数据学习，能够将人名、地名、互联网上的新词等一一识别出来，具有广泛的应用场景。</p>
<p>其分词过程:</p>
<p><img src="/2020/04/28/word_seg/hmm_seg.png" alt="avatar"></p>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-4"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>数据准备：分好词的词库</p>
<p>step 1:通过统计语料库中词的频次，计算三个概率：初始状态概率start，状态转移概率矩阵trans，发射概率emit。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HmmTrain</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.line_index = <span class="number">-1</span></span><br><span class="line">        self.char_set = set()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init</span><span class="params">(self)</span>:</span>  <span class="comment"># 初始化字典</span></span><br><span class="line">        trans_dict = &#123;&#125;  <span class="comment"># 存储状态转移概率</span></span><br><span class="line">        emit_dict = &#123;&#125;  <span class="comment"># 发射概率(状态-&gt;词语的条件概率)</span></span><br><span class="line">        Count_dict = &#123;&#125;  <span class="comment"># 存储所有状态序列 ，用于归一化分母</span></span><br><span class="line">        start_dict = &#123;&#125;  <span class="comment"># 存储状态的初始概率</span></span><br><span class="line">        state_list = [<span class="string">'B'</span>, <span class="string">'M'</span>, <span class="string">'E'</span>, <span class="string">'S'</span>]  <span class="comment"># 状态序列</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> state <span class="keyword">in</span> state_list:</span><br><span class="line">            trans_dict[state] = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> state1 <span class="keyword">in</span> state_list:</span><br><span class="line">                trans_dict[state][state1] = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> state <span class="keyword">in</span> state_list:</span><br><span class="line">            start_dict[state] = <span class="number">0.0</span></span><br><span class="line">            emit_dict[state] = &#123;&#125;</span><br><span class="line">            Count_dict[state] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(trans_dict) #&#123;'B': &#123;'B': 0.0, 'S': 0.0, 'M': 0.0, 'E': 0.0&#125;, 'S': &#123;'B': 0.0, 'S': 0.0, 'M': 0.0, 'E': 0.0&#125;,。。。&#125;</span></span><br><span class="line">        <span class="comment"># print(emit_dict) # &#123;'B': &#123;&#125;, 'S': &#123;&#125;, 'M': &#123;&#125;, 'E': &#123;&#125;&#125;</span></span><br><span class="line">        <span class="comment"># print(start_dict) # &#123;'B': 0.0, 'S': 0.0, 'M': 0.0, 'E': 0.0&#125;</span></span><br><span class="line">        <span class="comment"># print(Count_dict) # &#123;'B': 0, 'S': 0, 'M': 0, 'E': 0&#125;</span></span><br><span class="line">        <span class="keyword">return</span> trans_dict, emit_dict, start_dict, Count_dict</span><br><span class="line"></span><br><span class="line">    <span class="string">'''保存模型'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_model</span><span class="params">(self, word_dict, model_path)</span>:</span></span><br><span class="line">        f = open(model_path, <span class="string">'w'</span>)</span><br><span class="line">        f.write(str(word_dict))</span><br><span class="line">        f.close()</span><br><span class="line"></span><br><span class="line">    <span class="string">'''词语状态转换'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_status</span><span class="params">(self, word)</span>:</span>  <span class="comment"># 根据词语，输出词语对应的SBME状态</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        S:单字词</span></span><br><span class="line"><span class="string">        B:词的开头</span></span><br><span class="line"><span class="string">        M:词的中间</span></span><br><span class="line"><span class="string">        E:词的末尾</span></span><br><span class="line"><span class="string">        能 ['S']</span></span><br><span class="line"><span class="string">        前往 ['B', 'E']</span></span><br><span class="line"><span class="string">        科威特 ['B', 'M', 'E']</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        word_status = []</span><br><span class="line">        <span class="keyword">if</span> len(word) == <span class="number">1</span>:</span><br><span class="line">            word_status.append(<span class="string">'S'</span>)</span><br><span class="line">        <span class="keyword">elif</span> len(word) == <span class="number">2</span>:</span><br><span class="line">            word_status = [<span class="string">'B'</span>, <span class="string">'E'</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            M_num = len(word) - <span class="number">2</span></span><br><span class="line">            M_list = [<span class="string">'M'</span>] * M_num</span><br><span class="line">            word_status.append(<span class="string">'B'</span>)</span><br><span class="line">            word_status.extend(M_list)</span><br><span class="line">            word_status.append(<span class="string">'E'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> word_status</span><br><span class="line"></span><br><span class="line">    <span class="string">'''基于人工标注语料库，训练发射概率，初始状态， 转移概率'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, train_filepath, trans_path, emit_path, start_path)</span>:</span></span><br><span class="line">        trans_dict, emit_dict, start_dict, Count_dict = self.init()</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> open(train_filepath):</span><br><span class="line">            self.line_index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            line = line.strip()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            char_list = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(line)):</span><br><span class="line">                <span class="keyword">if</span> line[i] == <span class="string">" "</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                char_list.append(line[i])</span><br><span class="line"></span><br><span class="line">            self.char_set = set(char_list)  <span class="comment"># 训练预料库中所有字的集合</span></span><br><span class="line"></span><br><span class="line">            word_list = line.split(<span class="string">" "</span>)</span><br><span class="line">            line_status = []  <span class="comment"># 统计状态序列</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> word_list:</span><br><span class="line">                line_status.extend(self.get_word_status(word))  <span class="comment"># 一句话对应一行连续的状态</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> len(char_list) == len(line_status):</span><br><span class="line">                <span class="comment"># print(word_list) # ['但', '从', '生物学', '眼光', '看', '就', '并非', '如此', '了', '。']</span></span><br><span class="line">                <span class="comment"># print(line_status) # ['S', 'S', 'B', 'M', 'E', 'B', 'E', 'S', 'S', 'B', 'E', 'B', 'E', 'S', 'S']</span></span><br><span class="line">                <span class="comment"># print('******')</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(len(line_status)):</span><br><span class="line">                    <span class="keyword">if</span> i == <span class="number">0</span>:  <span class="comment"># 如果只有一个词，则直接算作是初始概率</span></span><br><span class="line">                        start_dict[line_status[<span class="number">0</span>]] += <span class="number">1</span>  <span class="comment"># start_dict记录句子第一个字的状态，用于计算初始状态概率</span></span><br><span class="line">                        Count_dict[line_status[<span class="number">0</span>]] += <span class="number">1</span>  <span class="comment"># 记录每一个状态的出现次数</span></span><br><span class="line">                    <span class="keyword">else</span>:  <span class="comment"># 统计上一个状态到下一个状态，两个状态之间的转移概率</span></span><br><span class="line">                        trans_dict[line_status[i - <span class="number">1</span>]][line_status[i]] += <span class="number">1</span>  <span class="comment"># 用于计算转移概率</span></span><br><span class="line">                        Count_dict[line_status[i]] += <span class="number">1</span></span><br><span class="line">                        <span class="comment"># 统计发射概率</span></span><br><span class="line">                        <span class="keyword">if</span> char_list[i] <span class="keyword">not</span> <span class="keyword">in</span> emit_dict[line_status[i]]:</span><br><span class="line">                            emit_dict[line_status[i]][char_list[i]] = <span class="number">0.0</span></span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            emit_dict[line_status[i]][char_list[i]] += <span class="number">1</span>  <span class="comment"># 用于计算发射概率</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(emit_dict)#&#123;'S': &#123;'否': 10.0, '昔': 25.0, '直': 238.0, '六': 1004.0, '殖': 17.0, '仗': 36.0, '挪': 15.0, '朗': 151.0</span></span><br><span class="line">        <span class="comment"># print(trans_dict)#&#123;'S': &#123;'S': 747969.0, 'E': 0.0, 'M': 0.0, 'B': 563988.0&#125;, 'E': &#123;'S': 737404.0, 'E': 0.0, 'M': 0.0, 'B': 651128.0&#125;,</span></span><br><span class="line">        <span class="comment"># print(start_dict) #&#123;'S': 124543.0, 'E': 0.0, 'M': 0.0, 'B': 173416.0&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 进行归一化</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> start_dict:  <span class="comment"># 状态的初始概率</span></span><br><span class="line">            start_dict[key] = start_dict[key] * <span class="number">1.0</span> / self.line_index</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> trans_dict:  <span class="comment"># 状态转移概率</span></span><br><span class="line">            <span class="keyword">for</span> key1 <span class="keyword">in</span> trans_dict[key]:</span><br><span class="line">                trans_dict[key][key1] = trans_dict[key][key1] / Count_dict[key]</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> emit_dict:  <span class="comment"># 发射概率(状态-&gt;词语的条件概率)</span></span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> emit_dict[key]:</span><br><span class="line">                emit_dict[key][word] = emit_dict[key][word] / Count_dict[key]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(emit_dict)#&#123;'S': &#123;'否': 6.211504202703743e-06, '昔': 1.5528760506759358e-05, '直': 0.0001478338000243491,</span></span><br><span class="line">        <span class="comment"># print(trans_dict)#&#123;'S': &#123;'S': 0.46460125869921165, 'E': 0.0, 'M': 0.0, 'B': 0.3503213832274479&#125;,</span></span><br><span class="line">        <span class="comment"># print(start_dict)#&#123;'S': 0.41798844132394497, 'E': 0.0, 'M': 0.0, 'B': 0.5820149148537713&#125;</span></span><br><span class="line">        self.save_model(trans_dict, trans_path)</span><br><span class="line">        self.save_model(emit_dict, emit_path)</span><br><span class="line">        self.save_model(start_dict, start_path)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> trans_dict, emit_dict, start_dict</span><br></pre></td></tr></table></figure>
<p>Step 2: 使用维特比算法，求解概率最大路径</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HmmCut</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,start_model_path,trans_model_path,emit_model_path)</span>:</span></span><br><span class="line">        self.prob_trans = self.load_model(trans_model_path)</span><br><span class="line">        self.prob_emit = self.load_model(emit_model_path)</span><br><span class="line">        self.prob_start = self.load_model(start_model_path)</span><br><span class="line"></span><br><span class="line">    <span class="string">'''加载模型'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_model</span><span class="params">(self, model_path)</span>:</span></span><br><span class="line">        f = open(model_path, <span class="string">'r'</span>)</span><br><span class="line">        a = f.read()</span><br><span class="line">        word_dict = eval(a)</span><br><span class="line">        f.close()</span><br><span class="line">        <span class="keyword">return</span> word_dict</span><br><span class="line"></span><br><span class="line">    <span class="string">'''verterbi算法求解'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(self, obs, states, start_p, trans_p, emit_p)</span>:</span>  <span class="comment"># 维特比算法（一种递归算法）</span></span><br><span class="line">        <span class="comment"># 算法的局限在于训练语料要足够大，需要给每个词一个发射概率,.get(obs[0], 0)的用法是如果dict中不存在这个key,则返回0值</span></span><br><span class="line">        V = [&#123;&#125;]</span><br><span class="line">        path = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">            V[<span class="number">0</span>][y] = start_p[y] * emit_p[y].get(obs[<span class="number">0</span>], <span class="number">0</span>)  <span class="comment"># 在位置0，以y状态为末尾的状态序列的最大概率</span></span><br><span class="line">            path[y] = [y]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, len(obs)):</span><br><span class="line">            V.append(&#123;&#125;)</span><br><span class="line">            newpath = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">                state_path = ([(V[t - <span class="number">1</span>][y0] * trans_p[y0].get(y, <span class="number">0</span>) * emit_p[y].get(obs[t], <span class="number">0</span>), y0) <span class="keyword">for</span> y0 <span class="keyword">in</span> states <span class="keyword">if</span> V[t - <span class="number">1</span>][y0] &gt; <span class="number">0</span>])</span><br><span class="line">                <span class="keyword">if</span> state_path == []:</span><br><span class="line">                    (prob, state) = (<span class="number">0.0</span>, <span class="string">'S'</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    (prob, state) = max(state_path)</span><br><span class="line">                V[t][y] = prob</span><br><span class="line">                newpath[y] = path[state] + [y]</span><br><span class="line"></span><br><span class="line">            path = newpath  <span class="comment"># 记录状态序列</span></span><br><span class="line">        (prob, state) = max([(V[len(obs) - <span class="number">1</span>][y], y) <span class="keyword">for</span> y <span class="keyword">in</span> states])  <span class="comment"># 在最后一个位置，以y状态为末尾的状态序列的最大概率</span></span><br><span class="line">        <span class="keyword">return</span> (prob, path[state])  <span class="comment"># 返回概率和状态序列</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分词主控函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cut</span><span class="params">(self, sent)</span>:</span></span><br><span class="line">        prob, pos_list = self.viterbi(sent, (<span class="string">'B'</span>, <span class="string">'M'</span>, <span class="string">'E'</span>, <span class="string">'S'</span>), self.prob_start, self.prob_trans, self.prob_emit)</span><br><span class="line">        seglist = list()</span><br><span class="line">        word = list()</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(len(pos_list)):</span><br><span class="line">            <span class="keyword">if</span> pos_list[index] == <span class="string">'S'</span>:</span><br><span class="line">                word.append(sent[index])</span><br><span class="line">                seglist.append(word)</span><br><span class="line">                word = []</span><br><span class="line">            <span class="keyword">elif</span> pos_list[index] <span class="keyword">in</span> [<span class="string">'B'</span>, <span class="string">'M'</span>]:</span><br><span class="line">                word.append(sent[index])</span><br><span class="line">            <span class="keyword">elif</span> pos_list[index] == <span class="string">'E'</span>:</span><br><span class="line">                word.append(sent[index])</span><br><span class="line">                seglist.append(word)</span><br><span class="line">                word = []</span><br><span class="line">        seglist = [<span class="string">''</span>.join(tmp) <span class="keyword">for</span> tmp <span class="keyword">in</span> seglist]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> seglist</span><br></pre></td></tr></table></figure>
<p>如果画出状态转移图像，发现状态只能在层间点转移，转移路径上的分数为概率，求一条概率最大的路径。这可以用维特比算法计算，本质上就是一个动态规划</p>
<p><img src="/2020/04/28/word_seg/hmm_viterbi.png" alt="avatar"></p>
<h5 id="zong-jie-1">总结</h5>
<p><strong>从三方面比较HMM和N-gram。</strong></p>
<ol>
<li>HMM模型是一个生成模型，区别于N-gram语言模型，HMM没有直接对给定观测值后状态的分布 \(P(S|O)\)（<em>O</em> 代表观测序列）进行建模，而是对状态序列本身的分布\(P(S)\)和给定状态后观测值的分布 \(p(O|S)\)建模 ；</li>
<li>学习过程与N-gram相同，HMM在有监督学习的情况下，使用极大似然估计参数；</li>
<li>预测时，HMM采用维特比算法。</li>
</ol>
<h4 id="crf-fen-ci">CRF分词</h4>
<h5 id="tiao-jian-sui-ji-chang-crf">条件随机场CRF</h5>
<p>条件随机场基于概率无向图模型，利用最大团理论，对随机变量的联合分布\(P(Y)\)进行建模。</p>
<p>在序列标注任务中的条件随机场，往往是指<strong>线性链条件随机场</strong>，这时，在条件概率模型\(P(Y|X)\)中，\(Y\)是输出变量，表示标记序列（或状态序列），\(X\)是输入变量，表示需要标注的观测序列。学习时，利用训练数据集通过极大似然估计或正则化的极大似然估计得到条件概率模型\(p(Y|X)\)；预测时，对于给定的输入序列x，求出条件概率\(p(y|x)\)最大的输出序列y.</p>
<h6 id="tiao-jian-sui-ji-chang-de-can-shu-hua-xing-shi">条件随机场的参数化形式</h6>
<p>\[
P(y|x)=\frac{1}{Z(x)} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, j} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right) 
\\
Z(x)=\sum_{y} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, j} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right)
\]</p>
<h5 id="tiao-jian-sui-ji-chang-fen-ci-fang-fa">条件随机场分词方法</h5>
<p>条件随机场和隐马尔可夫一样，也是使用BMES四个状态位来进行分词。以如下句子为例：</p>
<p>中 国 是 泱 泱 大 国<br>
B  B  B  B  B  B  B<br>
M M M M M M M<br>
E  E  E  E  E  E  E<br>
S  S  S  S  S  S  S</p>
<p>条件随机场解码就是在以上由标记组成的数组中搜索一条最优的路径。</p>
<p>要把每一个字(即观察变量)对应的每一个状态BMES(即标记变量)的概率都求出来。例如对于观察变量“国”，当前标记变量为E，前一个观察变量为“中”，前一个标记变量为B，则：t(B, E, ‘国’) 对应到条件随机场里相邻标记变量\((y_{i-1},y_i)\)的势函数。s(E, ‘国’) 对应到条件随机场里单个标记变量\(y_i\)对应的势函数\(s_l(y_i,x,i)\)。t(B, E, ‘国’), s(E, ‘国’)相应的权值\(λ_k,\mu_l\)， 都是由条件随机场用大量的标注语料训练出来。因此分词的标记识别就是求对于各个观察变量，它们的标记变量(BMES)状态序列的概率最大值，即求：的概率组合最大值。这个解法与隐马尔可夫类似，可以用viterbi算法求解。</p>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-5"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>使用crf<ins>实现对模型的训练，crf</ins> 安装、数据格式及模板参数请参考：<a href="https://jeffery0628.github.io/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/" target="_blank" rel="noopener">序列标注</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> CRFPP</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRFModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model=<span class="string">'model_name'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 类初始化</span></span><br><span class="line"><span class="string">        :param model: 模型名称</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_tagger</span><span class="params">(self, tag_data)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 添加语料</span></span><br><span class="line"><span class="string">        :param tag_data: 数据</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        word_str = tag_data.strip()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(self.model):</span><br><span class="line">            print(<span class="string">'模型不存在,请确认模型路径是否正确!'</span>)</span><br><span class="line">            exit()</span><br><span class="line">        tagger = CRFPP.Tagger(<span class="string">"-m &#123;&#125; -v 3 -n2"</span>.format(self.model))</span><br><span class="line">        tagger.clear()</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> word_str:</span><br><span class="line">            tagger.add(word)</span><br><span class="line">        tagger.parse()</span><br><span class="line">        <span class="keyword">return</span> tagger</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">text_mark</span><span class="params">(self, tag_data, begin=<span class="string">'B'</span>, middle=<span class="string">'I'</span>, end=<span class="string">'E'</span>, single=<span class="string">'S'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        文本标记</span></span><br><span class="line"><span class="string">        :param tag_data: 数据</span></span><br><span class="line"><span class="string">        :param begin: 开始标记</span></span><br><span class="line"><span class="string">        :param middle: 中间标记</span></span><br><span class="line"><span class="string">        :param end: 结束标记</span></span><br><span class="line"><span class="string">        :param single: 单字结束标记</span></span><br><span class="line"><span class="string">        :return result: 标记列表</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        tagger = self.add_tagger(tag_data)</span><br><span class="line">        size = tagger.size()</span><br><span class="line">        tag_text = <span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, size):</span><br><span class="line">            word, tag = tagger.x(i, <span class="number">0</span>), tagger.y2(i)</span><br><span class="line">            <span class="keyword">if</span> tag <span class="keyword">in</span> [begin, middle]:</span><br><span class="line">                tag_text += word</span><br><span class="line">            <span class="keyword">elif</span> tag <span class="keyword">in</span> [end, single]:</span><br><span class="line">                tag_text += word + <span class="string">"*&amp;*"</span></span><br><span class="line">        result = tag_text.split(<span class="string">'*&amp;*'</span>)</span><br><span class="line">        result.pop()</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crf_test</span><span class="params">(self, tag_data, separator=<span class="string">'/'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: crf测试</span></span><br><span class="line"><span class="string">        :param tag_data:</span></span><br><span class="line"><span class="string">        :param separator:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        result = self.text_mark(tag_data)</span><br><span class="line">        data = separator.join(result)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crf_learn</span><span class="params">(self, filename)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 训练模型</span></span><br><span class="line"><span class="string">        :param filename: 已标注数据源</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        crf_bash = <span class="string">"crf_learn -f 3 -c 4.0 data/template.txt &#123;&#125; &#123;&#125;"</span>.format(filename, self.model)</span><br><span class="line">        process = subprocess.Popen(crf_bash.split(), stdout=subprocess.PIPE)</span><br><span class="line">        output = process.communicate()[<span class="number">0</span>]</span><br><span class="line">        print(output.decode(encoding=<span class="string">'utf-8'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    crf_model = CRFModel(model=<span class="string">'data/model_crf'</span>)</span><br><span class="line">    crf_model.crf_learn(filename=<span class="string">'data/train_file_crf.txt'</span>)</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    print(crf_model.crf_test(tag_data=<span class="string">'我们一定要战胜敌人，我们认为它们都是纸老虎。'</span>))</span><br></pre></td></tr></table></figure>
<h5 id="tiao-jian-sui-ji-chang-fen-ci-de-you-que-dian">条件随机场分词的优缺点</h5>
<p>条件随机场分词是一种精度很高的分词方法，它比隐马尔可夫的精度要高，是因为隐马尔可夫假设观察变量\(x_i\)只与当前状态\(y_i\)有关，而与其它状态\(y_{i-1}\)，\(y_{i+1}\)无关;而条件随机场假设了当前观察变量\(x_i\)与上下文相关，如 ，就是考虑到上一个字标记状态为B时，当前标记状态为E并且输出“国”字的概率。因此通过上下文的分析，条件随机场分词会提升到更高的精度。但因为复杂度比较高，条件随机场一般训练代价都比较大。</p>
<h4 id="jie-gou-hua-gan-zhi-ji-word-seg">结构化感知机word_seg</h4>
<p>利用隐马尔科夫模型实现基于序列标注的中文分词器，效果并不理想。事实上，隐马尔可夫模型假设人们说的话仅仅取决于一个隐藏的BMES序列，这个假设太单纯了，不符合语言规律。语言不是由这么简单的标签序列生成，语言含有更多特征，而隐马尔科夫模型没有捕捉到。<strong>隐马弥可夫模型能捕捉的特征仅限于两种: 其一，前一个标签是什么；其二，当前字符是什么</strong>。为了利用更多的特征，线性模型( linear model )应运而生。线性模型由两部分构成: 一系列用来提取特征的特征函数\(\phi\)，以及相应的权重向量\(w\)。</p>
<h5 id="gan-zhi-ji-suan-fa">感知机算法</h5>
<p>感知机算法是一种迭代式的算法：在训练集上运行多个迭代，每次读入一个样本，执行预测，将预测结果与正确答案进行对比，计算误差，根据误差更新模型参数，再次进行训练，直到误差最小为止。</p>
<ul>
<li><strong>损失函数</strong>: 从数值优化的角度来讲，迭代式机器学习算法都在优化(减小)一个损失函数。损失函数 J(w) 用来衡量模型在训练集上的错误程度，自变量是模型参数 \(w\)，因变量是一个标量，表示模型在训练集上的损失的大小。</li>
<li><strong>梯度下降</strong>: 给定样本，其特征向量 \(x\) 只是常数，对 \(J(w)\) 求导，得到一个梯度向量 \(\Delta w\)，它的反方向一定是当前位置损失函数减小速度最快的方向。如果参数点 \(w\) 反方向移动就会使损失函数减小，叫梯度下降。</li>
<li><strong>学习率</strong>: 梯度下降的步长叫做学习率。</li>
<li><strong>随机梯度下降</strong>(SGD): 如果算法每次迭代随机选取部分样本计算损失函数的梯度，则称为随机梯度下降。</li>
</ul>
<p>假如数据本身线性不可分，感知机损失函数不会收敛，每次迭代分离超平面都会剧烈振荡。这时可以对感知机算法打补丁，使用投票感知机或平均感知机。</p>
<ol>
<li>
<p><strong>投票感知机</strong>：每次迭代的模型都保留，准确率也保留，预测时，每个模型都给出自己的结果，乘以它的准确率加权平均值作为最终结果。</p>
</li>
<li>
<p>投票感知机要求存储多个模型及加权，计算开销较大，更实际的做法是取多个模型的权重的平均，这就是<strong>平均感知机</strong>。</p>
</li>
</ol>
<h5 id="jie-gou-hua-yu-ce-wen-ti">结构化预测问题</h5>
<p>自然语言处理问题大致可分为两类，一种是分类问题，另一种就是结构化预测问题，序列标注只是结构化预测的一个特例，对感知机稍作拓展，分类器就能支持结构化预测。<br>
信息的层次结构特点称作结构化。<strong>那么结构化预测</strong>(structhre，prediction)则是预测对象结构的一类监督学习问题。相应的模型训练过程称作<strong>结构化学习</strong>(stutured laming )。分类问题的预测结果是一个决策边界， 回归问题的预测结果是一个实数标量，而结构化预测的结果则是一个完整的结构。<br>
自然语言处理中有许多任务是结构化预测，比如序列标注预测结构是一整个序列，句法分析预测结构是一棵句法树，机器翻译预测结构是一段完整的译文。这些结构由许多部分构成，最小的部分虽然也是分类问题(比如中文分词时每个字符分类为{B,M,E,S} ),但必须考虑结构整体的合理程度。</p>
<h6 id="jie-gou-hua-yu-ce-yu-xue-xi-liu-cheng">结构化预测与学习流程</h6>
<p>结构化预测的过程就是给定一个模型 λ 及打分函数 score，利用打分函数给一些备选结构打分，选择分数最高的结构作为预测输出，公式如下:<br>
\[
\hat{y}=\arg \max _{y \in Y} \operatorname{score}({\lambda}(x, y))
\]<br>
其中，Y 是备选结构的集合。既然结构化预测就是搜索得分最高的结构 y，那么结构化学习的目标就是想方设法让正确答案 y 的得分最高。不同的模型有不同的算法，对于线性模型，训练算法为结构化感知机。</p>
<h5 id="jie-gou-hua-gan-zhi-ji-suan-fa">结构化感知机算法</h5>
<p>要让线性模型支持结构化预测，必须先设计打分函数。打分函数的输入有两个缺一不可的参数: 特征 \(x\) 和结构\(y\)。但之前的线性模型的“打分函数”只接受一个自变量 \(x\)。做法是定义新的特征函数\(\phi (x,y)\)，把结构 \(y\) 也作为一种特征，输出新的“结构化特征向量”。新特征向量与权重向量做点积后，就得到一个标量，将其作为分数:<br>
\[
\operatorname{score}(x, y)=w \cdot \phi(x, y)
\]<br>
打分函数有了，取分值最大的结构作为预测结果，得到结构化预测函数:<br>
\[
\hat{y}=\arg \max _{y \in Y}(w \cdot \phi(x, y))
\]<br>
预测函数与线性分类器的决策函数很像，都是权重向量点积特征向量。那么感知机算法也可以拓展复用，得到线性模型的结构化学习算法:</p>
<ol>
<li>读入样本 \((x,y)\)，进行结构化预测\(\hat{y}=\arg \max _{y \in Y}(w \cdot \phi(x, y))\)</li>
<li>与正确答案相比，若不相等，则更新参数: 奖励正确答案触发的特征函数的权重，否则进行惩罚:\(w \leftarrow w+\phi\left(x^{(i)}, y\right)-\phi\left(x^{(i)}, \hat{y}\right)\)</li>
<li>调整学习率:\(\boldsymbol{w} \leftarrow \boldsymbol{w}+\alpha\left(\phi\left(\boldsymbol{x}^{(i)}, \boldsymbol{y}\right)-\phi\left(\boldsymbol{x}^{(i)}, \hat{\boldsymbol{y}}\right)\right)\)</li>
</ol>
<h6 id="jie-gou-hua-gan-zhi-ji-yu-gan-zhi-ji-suan-fa-bi-jiao">结构化感知机与感知机算法比较</h6>
<ul>
<li>结构化感知机修改了特征向量。</li>
<li>结构化感知机的参数更新赏罚分明。</li>
</ul>
<h6 id="jie-gou-hua-gan-zhi-ji-yu-xu-lie-biao-zhu">结构化感知机与序列标注</h6>
<p>序列标注最大的结构特点就是标签相互之间的依赖性，这种依赖性利用初始状态概率想俩狗和状态转移概率矩阵体系那，那么对于结构化感知机，就可以使用<strong>转移特征</strong>来表示:<br>
\[
\phi_{k}\left(y_{t-1}, y_{t}\right)=\left\{\begin{array}{ll}
1, &amp; y_{t-1}=s_{i}, and , y_{t}=s_{j} \\
0
\end{array} \quad i=0, \cdots, N ; j=1, \cdots, N\right.
\]<br>
其中，\(y_t\)为序列第 t 个标签，\(s_i\)为标注集第 i 种标签，N 为标注集大小。</p>
<p><strong>状态特征</strong>，类似于隐马尔可夫模型的发射概率矩阵，状态特征只与当前的状态有关，与之前的状态无关:<br>
\[
\phi_{i}\left(x_{i}, y_{i}\right)=\left\{\begin{array}{l}
1 \\
0
\end{array}\right.
\]<br>
于是，结构化感知机的特征函数就是转移特征和状态特征的合集:<br>
\[
\phi=\left[\phi_{k} ; \phi_{l}\right] \quad k=1, \cdots, N^{2}+N ; l=N^{2}+N+1, \cdots
\]<br>
基于以上公式，统一用打分函数来表示:<br>
\[
\operatorname{score}(\boldsymbol{x}, \boldsymbol{y})=\sum_{t=1}^{T} \boldsymbol{w} \cdot \phi\left(y_{t-1}, y_{t}, \boldsymbol{x}_{t}\right)
\]<br>
有了打分公式，就可以利用维特比算法求解得分最高的序列。</p>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-6"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CPTTrain</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, segment, train)</span>:</span></span><br><span class="line">        self.__char_type = &#123;&#125;</span><br><span class="line">        data_path = <span class="string">"data"</span></span><br><span class="line">        <span class="keyword">for</span> ind, name <span class="keyword">in</span> enumerate([<span class="string">"punc"</span>, <span class="string">"alph"</span>, <span class="string">"date"</span>, <span class="string">"num"</span>]):</span><br><span class="line">            fn = data_path + <span class="string">"/"</span> + name</span><br><span class="line">            <span class="keyword">if</span> os.path.isfile(fn):</span><br><span class="line">                <span class="keyword">for</span> line <span class="keyword">in</span> open(fn, <span class="string">"r"</span>):</span><br><span class="line">                    self.__char_type[line.strip()] = ind</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">"can't open"</span>, fn)</span><br><span class="line">                exit()</span><br><span class="line"></span><br><span class="line">        self.__train_insts = <span class="literal">None</span>  <span class="comment"># all instances for training.</span></span><br><span class="line">        self.__feats_weight = <span class="literal">None</span>  <span class="comment"># ["b", "m", "e", "s"][all the features] --&gt; weight.</span></span><br><span class="line">        self.__words_num = <span class="literal">None</span>  <span class="comment"># total words num in all the instances.</span></span><br><span class="line">        self.__insts_num = <span class="literal">None</span>  <span class="comment"># namley the sentences' num.</span></span><br><span class="line">        self.__cur_ite_ID = <span class="literal">None</span>  <span class="comment"># current iteration index.</span></span><br><span class="line">        self.__cur_inst_ID = <span class="literal">None</span>  <span class="comment"># current index_th instance.</span></span><br><span class="line">        self.__real_inst_ID = <span class="literal">None</span>  <span class="comment"># the accurate index in training instances after randimizing.</span></span><br><span class="line">        self.__last_update = <span class="literal">None</span>  <span class="comment"># ["b".."s"][feature] --&gt; [last_update_ite_ID, last_update_inst_ID]</span></span><br><span class="line">        self.__feats_weight_sum = <span class="literal">None</span>  <span class="comment"># sum of ["b".."s"][feature] from begin to end.</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> segment <span class="keyword">and</span> train <span class="keyword">or</span> <span class="keyword">not</span> segment <span class="keyword">and</span> <span class="keyword">not</span> train:</span><br><span class="line">            print(<span class="string">"there is only a True and False in segment and train"</span>)</span><br><span class="line">            exit()</span><br><span class="line">        <span class="keyword">elif</span> train:</span><br><span class="line">            self.Train = self.__Train</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.__LoadModel()</span><br><span class="line">            self.Segment = self.__Segment</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__LoadModel</span><span class="params">(self)</span>:</span></span><br><span class="line">        model = <span class="string">"data/avgmodel"</span></span><br><span class="line">        print(<span class="string">"load"</span>, model, <span class="string">"..."</span>)</span><br><span class="line">        self.__feats_weight = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> os.path.isfile(model):</span><br><span class="line">            start = time.clock()</span><br><span class="line">            self.__feats_weight = pickle.load(open(model, <span class="string">"rb"</span>))</span><br><span class="line">            end = time.clock()</span><br><span class="line">            print(<span class="string">"It takes %d seconds"</span> % (end - start))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"can't open"</span>, model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Train</span><span class="params">(self, corp_file_name, max_train_num, max_ite_num)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.__LoadCorp(corp_file_name, max_train_num):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        starttime = time.clock()</span><br><span class="line"></span><br><span class="line">        self.__feats_weight = &#123;&#125;</span><br><span class="line">        self.__last_update = &#123;&#125;</span><br><span class="line">        self.__feats_weight_sum = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> self.__cur_ite_ID <span class="keyword">in</span> range(max_ite_num):</span><br><span class="line">            <span class="keyword">if</span> self.__Iterate():</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        self.__SaveModel()</span><br><span class="line">        endtime = time.clock()</span><br><span class="line">        print(<span class="string">"total iteration times is %d seconds"</span> % (endtime - starttime))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__GenerateFeats</span><span class="params">(self, inst)</span>:</span></span><br><span class="line">        inst_feat = []</span><br><span class="line">        <span class="keyword">for</span> ind, [c, tag, t] <span class="keyword">in</span> enumerate(inst):</span><br><span class="line">            inst_feat.append([])</span><br><span class="line">            <span class="keyword">if</span> t == <span class="number">-1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># Cn</span></span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">-2</span>, <span class="number">3</span>):</span><br><span class="line">                inst_feat[<span class="number">-1</span>].append(<span class="string">"C%d==%s"</span> % (n, inst[ind + n][<span class="number">0</span>]))</span><br><span class="line">            <span class="comment"># CnCn+1</span></span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">-2</span>, <span class="number">2</span>):</span><br><span class="line">                inst_feat[<span class="number">-1</span>].append(<span class="string">"C%dC%d==%s%s"</span> % (n, n + <span class="number">1</span>, inst[ind + n][<span class="number">0</span>], inst[ind + n + <span class="number">1</span>][<span class="number">0</span>]))</span><br><span class="line">            <span class="comment"># C-1C1</span></span><br><span class="line">            inst_feat[<span class="number">-1</span>].append(<span class="string">"C-1C1==%s%s"</span> % (inst[ind - <span class="number">1</span>][<span class="number">0</span>], inst[ind + <span class="number">1</span>][<span class="number">0</span>]))</span><br><span class="line">            <span class="comment"># Pu(C0)</span></span><br><span class="line">            inst_feat[<span class="number">-1</span>].append(<span class="string">"Pu(%s)==%d"</span> % (c, int(t == <span class="number">0</span>)))</span><br><span class="line">            <span class="comment"># T(C-2)T(C-1)T(C0)T(C1)T(C2)</span></span><br><span class="line">            inst_feat[<span class="number">-1</span>].append(<span class="string">"T-2...2=%d%d%d%d%d"</span> % (</span><br><span class="line">            inst[ind - <span class="number">2</span>][<span class="number">2</span>], inst[ind - <span class="number">1</span>][<span class="number">2</span>], inst[ind][<span class="number">2</span>], inst[ind + <span class="number">1</span>][<span class="number">2</span>], inst[ind + <span class="number">2</span>][<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> inst_feat</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__SaveModel</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># the last time to sum all the features.</span></span><br><span class="line">        norm = float(self.__cur_ite_ID + <span class="number">1</span>) * self.__insts_num</span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> self.__feats_weight_sum:</span><br><span class="line">            last_ite_ID = self.__last_update[feat][<span class="number">0</span>]</span><br><span class="line">            last_inst_ID = self.__last_update[feat][<span class="number">1</span>]</span><br><span class="line">            c = (self.__cur_ite_ID - last_ite_ID) * self.__insts_num + self.__cur_inst_ID - last_inst_ID</span><br><span class="line">            self.__feats_weight_sum[feat] += self.__feats_weight[feat] * c</span><br><span class="line">            self.__feats_weight_sum[feat] = self.__feats_weight_sum[feat] / norm</span><br><span class="line"></span><br><span class="line">        pickle.dump(self.__feats_weight_sum, open(<span class="string">"data/avgmodel"</span>, <span class="string">"wb"</span>))</span><br><span class="line">        self.__train_insts = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__LoadCorp</span><span class="params">(self, corp_file_name, max_train_num)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(corp_file_name):</span><br><span class="line">            print(<span class="string">"can't open"</span>, corp_file_name)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        self.__train_insts = []</span><br><span class="line">        self.__words_num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> ind, line <span class="keyword">in</span> enumerate(open(corp_file_name, <span class="string">"r"</span>)):</span><br><span class="line">            <span class="keyword">if</span> max_train_num &gt; <span class="number">0</span> <span class="keyword">and</span> ind &gt;= max_train_num:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            self.__train_insts.append(self.__PreProcess(line.strip()))</span><br><span class="line">            self.__words_num += len(self.__train_insts[<span class="number">-1</span>]) - <span class="number">4</span></span><br><span class="line">        self.__insts_num = len(self.__train_insts)</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"number of total insts is"</span>, self.__insts_num)</span><br><span class="line">        print(<span class="string">"number of total characters is"</span>, self.__words_num)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__PreProcess</span><span class="params">(self, sent)</span>:</span></span><br><span class="line">        inst = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            inst.append([<span class="string">"&lt;s&gt;"</span>, <span class="string">"s"</span>, <span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sent.split():</span><br><span class="line">            rt = word.rpartition(<span class="string">"/"</span>)</span><br><span class="line">            t = self.__char_type.get(rt[<span class="number">0</span>], <span class="number">4</span>)</span><br><span class="line">            inst.append([rt[<span class="number">0</span>], rt[<span class="number">2</span>], t])  <span class="comment"># [c, tag, t]</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            inst.append([<span class="string">"&lt;s&gt;"</span>, <span class="string">"s"</span>, <span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> inst</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Segment</span><span class="params">(self, src)</span>:</span></span><br><span class="line">        <span class="string">"""suppose there is one sentence once."""</span></span><br><span class="line">        inst = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            inst.append([<span class="string">"&lt;s&gt;"</span>, <span class="string">"s"</span>, <span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> src:</span><br><span class="line">            inst.append([c, <span class="string">""</span>, self.__char_type.get(c, <span class="number">4</span>)])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            inst.append([<span class="string">"&lt;s&gt;"</span>, <span class="string">"s"</span>, <span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        feats = self.__GenerateFeats(inst)</span><br><span class="line">        tags = self.__DPSegment(inst, feats)</span><br><span class="line"></span><br><span class="line">        rst = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(tags) - <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">if</span> tags[i] <span class="keyword">in</span> [<span class="string">"s"</span>, <span class="string">"b"</span>]:</span><br><span class="line">                rst.append(inst[i][<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                rst[<span class="number">-1</span>] += inst[i][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">" "</span>.join(rst)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Iterate</span><span class="params">(self)</span>:</span></span><br><span class="line">        start = time.clock()</span><br><span class="line">        print(<span class="string">"%d th iteration"</span> % self.__cur_ite_ID)</span><br><span class="line"></span><br><span class="line">        train_list = random.sample(range(self.__insts_num), self.__insts_num)</span><br><span class="line">        error_sents_num = <span class="number">0</span></span><br><span class="line">        error_words_num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> self.__cur_inst_ID, self.__real_inst_ID <span class="keyword">in</span> enumerate(train_list):</span><br><span class="line">            num = self.__TrainInstance()</span><br><span class="line">            error_sents_num += <span class="number">1</span> <span class="keyword">if</span> num &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            error_words_num += num</span><br><span class="line"></span><br><span class="line">        st = <span class="number">1</span> - float(error_sents_num) / self.__insts_num</span><br><span class="line">        wt = <span class="number">1</span> - float(error_words_num) / self.__words_num</span><br><span class="line"></span><br><span class="line">        end = time.clock()</span><br><span class="line">        print(<span class="string">"sents accuracy = %f%%, words accuracy = %f%%, it takes %d seconds"</span> % (st * <span class="number">100</span>, wt * <span class="number">100</span>, end - start))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> error_sents_num == <span class="number">0</span> <span class="keyword">and</span> error_words_num == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__TrainInstance</span><span class="params">(self)</span>:</span></span><br><span class="line">        cur_inst = self.__train_insts[self.__real_inst_ID]</span><br><span class="line">        feats = self.__GenerateFeats(cur_inst)</span><br><span class="line"></span><br><span class="line">        seg = self.__DPSegment(cur_inst, feats)</span><br><span class="line">        <span class="keyword">return</span> self.__Correct(seg, feats)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__DPSegment</span><span class="params">(self, inst, feats)</span>:</span></span><br><span class="line">        num = len(inst)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get all position's score.</span></span><br><span class="line">        value = [&#123;&#125; <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, num - <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> [<span class="string">"b"</span>, <span class="string">"m"</span>, <span class="string">"e"</span>, <span class="string">"s"</span>]:</span><br><span class="line">                value[i][t] = self.__GetScore(i, t, feats)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># find optimal path.</span></span><br><span class="line">        tags = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]</span><br><span class="line">        best = [<span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]  <span class="comment"># best[i]: [i, i + length(i)) is optimal segment.</span></span><br><span class="line">        length = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num - <span class="number">2</span> - <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">for</span> dis <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">                <span class="keyword">if</span> i + dis &gt; num - <span class="number">2</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                cur_score = best[i + dis]</span><br><span class="line">                self.__Tag(i, i + dis, tags)</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(i, i + dis):</span><br><span class="line">                    cur_score += value[k][tags[k]]</span><br><span class="line">                <span class="keyword">if</span> length[i] <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> cur_score &gt; best[i]:</span><br><span class="line">                    best[i] = cur_score</span><br><span class="line">                    length[i] = dis</span><br><span class="line"></span><br><span class="line">        i = <span class="number">2</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; num - <span class="number">2</span>:</span><br><span class="line">            self.__Tag(i, i + length[i], tags)</span><br><span class="line">            i += length[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tags</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__GetScore</span><span class="params">(self, pos, t, feats)</span>:</span></span><br><span class="line">        pos_feats = feats[pos]</span><br><span class="line">        score = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> pos_feats:</span><br><span class="line">            score += self.__feats_weight.get(feat + <span class="string">"=&gt;"</span> + t, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Tag</span><span class="params">(self, f, t, tags)</span>:</span></span><br><span class="line">        <span class="string">"""tag the sequence tags in the xrange of [f, t)"""</span></span><br><span class="line">        <span class="keyword">if</span> t - f == <span class="number">1</span>:</span><br><span class="line">            tags[f] = <span class="string">"s"</span></span><br><span class="line">        <span class="keyword">elif</span> t - f &gt;= <span class="number">2</span>:</span><br><span class="line">            tags[f], tags[t - <span class="number">1</span>] = <span class="string">"b"</span>, <span class="string">"e"</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(f + <span class="number">1</span>, t - <span class="number">1</span>):</span><br><span class="line">                tags[i] = <span class="string">"m"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Correct</span><span class="params">(self, tags, feats)</span>:</span></span><br><span class="line">        updates = &#123;&#125;</span><br><span class="line">        cur_inst = self.__train_insts[self.__real_inst_ID]</span><br><span class="line">        error_words_num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(cur_inst) - <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">if</span> tags[i] == cur_inst[i][<span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            error_words_num += <span class="number">1</span></span><br><span class="line">            pos_feats = feats[i]</span><br><span class="line">            target = cur_inst[i][<span class="number">1</span>]</span><br><span class="line">            mine = tags[i]</span><br><span class="line">            <span class="keyword">for</span> feat <span class="keyword">in</span> pos_feats:</span><br><span class="line">                updates[feat + <span class="string">"=&gt;"</span> + target] = updates.get(feat + <span class="string">"=&gt;"</span> + target, <span class="number">0.0</span>) + <span class="number">1</span></span><br><span class="line">                updates[feat + <span class="string">"=&gt;"</span> + mine] = updates.get(feat + <span class="string">"=&gt;"</span> + mine, <span class="number">0.0</span>) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        self.__Update(updates)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> error_words_num</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Update</span><span class="params">(self, updates)</span>:</span></span><br><span class="line">        <span class="comment"># update the features weight.</span></span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> updates:</span><br><span class="line">            pair = self.__last_update.get(feat, [<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">            last_ite_ID = pair[<span class="number">0</span>]</span><br><span class="line">            last_inst_ID = pair[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            c = (self.__cur_ite_ID - last_ite_ID) * self.__insts_num + self.__cur_inst_ID - last_inst_ID</span><br><span class="line">            self.__feats_weight_sum[feat] = self.__feats_weight_sum.get(feat, <span class="number">0</span>) + c * self.__feats_weight.get(feat, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            self.__feats_weight[feat] = self.__feats_weight.get(feat, <span class="number">0</span>) + updates[feat]</span><br><span class="line">            self.__last_update[feat] = [self.__cur_ite_ID, self.__cur_inst_ID]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"></span><br><span class="line">    train = CPTTrain(train=<span class="literal">True</span>, segment=<span class="literal">False</span>)</span><br><span class="line">    train.Train(<span class="string">"data/msr_train.txt"</span>, max_train_num=<span class="number">1000000</span>, max_ite_num=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    srcs = [<span class="string">"夏天的清晨"</span>,</span><br><span class="line">            <span class="string">"“人们常说生活是一部教科书，而血与火的战争更是不可多得的教科书，她确实是名副其实的‘我的大学’。"</span>,</span><br><span class="line">            <span class="string">"夏天的清晨夏天看见猪八戒和嫦娥了。"</span>,</span><br><span class="line">            <span class="string">"海运业雄踞全球之首，按吨位计占世界总数的１７％。"</span>]</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"avg"</span>)</span><br><span class="line">    seg = CPTTrain(train=<span class="literal">False</span>, segment=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> src <span class="keyword">in</span> srcs:</span><br><span class="line">        print(seg.Segment(src))</span><br></pre></td></tr></table></figure>
<h4 id="ji-yu-shen-du-xue-xi-de-duan-dao-duan-de-fen-ci-fang-fa">基于深度学习的端到端的分词方法</h4>
<p>在中文分词上，基于神经网络的方法，往往使用「字向量 + BiLSTM + CRF」模型，利用神经网络来学习特征，将传统 CRF 中的人工特征工程量降到最低</p>
<p>BiLSTM、BiLSTM+CRF、BiLSTM+CNN+CRF、BERT、BERT+CRF、BERT+BiLSTM、BERT+BiLSTM+CRF，由于以上模型均可对序列标注任务进行建模求解，所以均可拿来做中文分词。以比较典型的「字向量 + BiLSTM （+CNN）+ CRF」模型为例：</p>
<p>BiLSTM融合两组学习方向相反（一个按句子顺序，一个按句子逆序）的LSTM层，能够在理论上实现当前词即包含历史信息、又包含未来信息，更有利于对当前词进行标注。虽然依赖于神经网络强大的非线性拟合能力，理论上我们已经能够学习出不错的模型。但是，BiLSTM只考虑了标签上的上下文信息。对于序列标注任务来说，当前位置的标签\(y_t\)与前一个位置\(y_{t-1}\)、后一个位置\(y_{t+1}\)都有潜在的关系。例如，“东南大学欢迎您”被标注为“东/S 南/M 大/M 学/E 欢/B 迎/E 您/S”，由分词的标注规则可知，B标签后只能接M和E，BiLSTM没有利用这种标签之间的上下文信息。因此，就有人提出了在模型后接一层CRF层，用于在整个序列上学习最优的标签序列：</p>
<p><img src="/2020/04/28/word_seg/bilstm_crf.jpeg" alt="avatar"></p>
<p>BiLSTM+CNN+CRF：对于分词任务，当前词的标签基本上只与前几个和和几个词有关联。BiLSTM在学习较长句子时，可能因为模型容量问题丢弃一些重要信息，因此在模型中加了一个CNN层，用于提取当前词的局部特征。（不知效果怎样？？！！）</p>
<p><img src="/2020/04/28/word_seg/bilstm_cnn_crf.png" alt="avatar"></p>
<h5 id="mo-xing-bi-jiao">模型比较</h5>
<p>Trained and tested with pku dataset</p>
<p>CRF</p>
<table>
<thead>
<tr>
<th style="text-align:left">Template</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">crf_template</td>
<td style="text-align:center">0.938</td>
<td style="text-align:center">0.923</td>
<td style="text-align:center">0.931</td>
</tr>
</tbody>
</table>
<p>Bi-LSTM</p>
<table>
<thead>
<tr>
<th style="text-align:left">Structure</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">emb256_hid256_l3</td>
<td style="text-align:center">0.9252</td>
<td style="text-align:center">0.9237</td>
<td style="text-align:center">0.9243</td>
</tr>
</tbody>
</table>
<p>Bi-LSTM + CRF</p>
<table>
<thead>
<tr>
<th style="text-align:left">Structure</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">emb256_hid256_l3</td>
<td style="text-align:center">0.9343</td>
<td style="text-align:center">0.9336</td>
<td style="text-align:center">0.9339</td>
</tr>
</tbody>
</table>
<p>BERT + Bi-LSTM</p>
<table>
<thead>
<tr>
<th style="text-align:left">Structure</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">emb768_hid512_l2</td>
<td style="text-align:center">0.9698</td>
<td style="text-align:center">0.9650</td>
<td style="text-align:center">0.9646</td>
</tr>
</tbody>
</table>
<h5 id="dai-ma">代码</h5>
<p>以BERT+RNN+CRF为例，可根据需求进行改动。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> TorchCRF <span class="keyword">import</span> CRF </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertRNNCRF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_tags, rnn_type, bert_path, bert_train, seg_vocab_size,hidden_dim, n_layers, bidirectional, batch_first,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout, restrain)</span>:</span></span><br><span class="line">        super(BertRNNCRF, self).__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="comment"># 对bert进行训练</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.bert.named_parameters():</span><br><span class="line">            param.requires_grad = bert_train</span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                               hidden_size=hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        self.crf = CRF(num_tags, batch_first=<span class="literal">True</span>, restrain_matrix=restrain, loss_side=<span class="number">2.5</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            self.fc_tags = nn.Linear(hidden_dim*<span class="number">2</span>, num_tags)</span><br><span class="line">            <span class="comment"># self.fc_tags = nn.Linear(768, num_tags)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.fc_tags = nn.Linear(hidden_dim, num_tags)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, context, seq_len,max_seq_len, mask_bert)</span>:</span></span><br><span class="line">        <span class="comment"># context    输入的句子</span></span><br><span class="line">        <span class="comment"># mask_bert  对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        bert_sentence, bert_cls = self.bert(context, attention_mask=mask_bert)</span><br><span class="line">        <span class="comment"># sentence_len = bert_sentence.shape[1]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># bert_cls = bert_cls.unsqueeze(dim=1).repeat(1, sentence_len, 1)</span></span><br><span class="line">        <span class="comment"># bert_sentence = bert_sentence + bert_cls</span></span><br><span class="line">        encoder_out, sorted_seq_lengths, desorted_indices = self.prepare_pack_padded_sequence(bert_sentence, seq_len)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(encoder_out, sorted_seq_lengths,</span><br><span class="line">                                                            batch_first=self.batch_first)</span><br><span class="line">        <span class="comment"># self.rnn.flatten_parameters()</span></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        <span class="comment"># output = [ batch size,sent len, hidden_dim * bidirectional]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first,total_length=max_seq_len)</span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        out = self.fc_tags(self.dropout(output.contiguous()))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out[:, <span class="number">1</span>:, :]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_pack_padded_sequence</span><span class="params">(self, inputs_words, seq_lengths, descending=True)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param device:</span></span><br><span class="line"><span class="string">        :param inputs_words:</span></span><br><span class="line"><span class="string">        :param seq_lengths:</span></span><br><span class="line"><span class="string">        :param descending:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        sorted_seq_lengths, indices = torch.sort(seq_lengths, descending=descending)</span><br><span class="line">        _, desorted_indices = torch.sort(indices, descending=<span class="literal">False</span>)</span><br><span class="line">        sorted_inputs_words = inputs_words[indices]</span><br><span class="line">        <span class="keyword">return</span> sorted_inputs_words, sorted_seq_lengths, desorted_indices</span><br></pre></td></tr></table></figure>
<h3 id="fen-ci-zhong-de-nan-ti">分词中的难题</h3>
<p>中文是一种十分复杂的语言，让计算机理解中文语言更是困难。在中文分词过程中，有两大难题一直没有完全突破。</p>
<h4 id="1-qi-yi-qie-fen-wen-ti">1、歧义切分问题</h4>
<p>歧义是指同样的一句话，可能有两种或者更多的切分方法。</p>
<ul>
<li>交集歧义：“结合成”，结合/成，结/合成</li>
<li>组合歧义：“起身”，他站起/身/来。 他明天/起身/去北京。</li>
<li>混合型歧义：同时具备交集歧义和组合歧义的特点。1，这篇文章写得太平淡了。2，这墙抹的太平了。3，即使太平时期也不应该放松警惕。“太平淡”是交集型，“太平”是组合型。</li>
</ul>
<p>交集歧义相对组合歧义来说是还算比较容易处理，组合歧义需要根据整个句子来判断。（特征：上下文语义分析，韵律分析，语气，重音，停顿）</p>
<h4 id="2-wei-deng-lu-ci">2、未登录词</h4>
<p>未登录词（生词，新词）：1.已有的词表中没有收录的词。2.已有的训练语料中未曾出现的词（集外词OOV）。</p>
<p>类型：</p>
<ol>
<li>新出现的普通词汇：博客，超女，给力。</li>
<li>专有名词：人名、地名、组织名、时间、数字表达、</li>
<li>专业名词和研究领域名词：苏丹红，禽流感</li>
<li>其他专用名词：新出现的产品名，电影，书记。</li>
</ol>
<p>对于真实数据来说，未登录词对分词精度的影响远远超过了歧义切分。未登录词是很难处理的问题，而且这些又正好是人们经常使用的词，因此对于搜索引擎来说，分词系统中的未登录词识别十分重要。目前未登录词识别准确率已经成为评价一个分词系统好坏的重要标志之一。</p>
<h3 id="can-kao">参考</h3>
<ol>
<li>
<p><a href="https://blog.csdn.net/selinda001/article/details/79345072" target="_blank" rel="noopener">中文分词引擎 java 实现 — 正向最大、逆向最大、双向最大匹配法</a></p>
</li>
<li>
<p><a href="https://www.cnblogs.com/sxron/articles/6391926.html" target="_blank" rel="noopener">中文word_seg总结</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/Zh823275484/article/details/87878512" target="_blank" rel="noopener">基于n-gram模型的中文分词</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/qq_27825451/article/details/102457058" target="_blank" rel="noopener">详解语言模型NGram及困惑度Perplexity</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/wangliang_f/article/details/17532633" target="_blank" rel="noopener">分词学习(3)，基于ngram语言模型的n元分词</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/33261835" target="_blank" rel="noopener">中文word_seg简介</a></p>
</li>
<li>
<p><a href="https://www.jianshu.com/p/715fa597c6bc" target="_blank" rel="noopener">NLP：word_seg综述</a></p>
</li>
<li>
<p><a href="https://github.com/NLP-LOVE/Introduction-NLP/blob/master/chapter/5.%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%88%86%E7%B1%BB%E4%B8%8E%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8.md" target="_blank" rel="noopener">感知机分类与序列标注</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
      <tags>
        <tag>word_seg</tag>
      </tags>
  </entry>
  <entry>
    <title>文本纠错</title>
    <url>/2020/04/24/text_correction/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/04/24/text_correction/v2-cd4b3605ecff9b1018015673f2f03fb3_1200x500.jpg" alt></p>
<a id="more"></a>
<h1 id="jian-jie">简介</h1>
<h2 id="chang-jian-cuo-wu-lei-xing">常见错误类型</h2>
<ol>
<li>谐音字词，如 配副眼睛-配副眼镜</li>
<li>混淆音字词，如 流浪织女-牛郎织女</li>
<li>字词顺序颠倒，如 伍迪艾伦-艾伦伍迪</li>
<li>字词补全，如 爱有天意-假如爱有天意</li>
<li>形似字错误，如 高梁-高粱</li>
<li>中文拼音全拼，如 xingfu-幸福</li>
<li>中文拼音缩写，如 sz-深圳</li>
<li>语法错误，如 想象难以-难以想象</li>
</ol>
<p>针对不同业务场景，这些问题并不一定全部存在，比如输入法中需要处理前四种，搜索引擎需要处理所有类型，语音识别后文本纠错只需要处理前两种， 其中’形似字错误’主要针对五笔或者笔画手写输入等。</p>
<h2 id="jiu-cuo-fang-fa">纠错方法</h2>
<p>纠错算法分为两个方向：基于规则、深度模型</p>
<h3 id="strong-gui-ze-de-jie-jue-si-lu-strong"><strong>规则的解决思路</strong></h3>
<p>中文纠错分为两步走，第一步是错误检测，第二步是错误纠正；</p>
<p>错误检测部分先通过结巴中文分词器切词，由于句子中含有错别字，所以切词结果往往会有切分错误的情况，这样从字粒度和词粒度两方面检测错误， 整合这两种粒度的疑似错误结果，形成疑似错误位置候选集；</p>
<p>错误纠正部分，是遍历所有的疑似错误位置，并使用音似、形似词典替换错误位置的词，然后通过语言模型计算句子困惑度，对所有候选集结果比较并排序，得到最优纠正词。</p>
<h3 id="strong-shen-du-mo-xing-de-jie-jue-si-lu-strong"><strong>深度模型的解决思路</strong></h3>
<p>端到端的深度模型可以避免人工提取特征，减少人工工作量，RNN序列模型对文本任务拟合能力强，rnn_attention在英文文本纠错比赛中取得第一名成绩，证明应用效果不错；</p>
<p>CRF会计算全局最优输出节点的条件概率，对句子中特定错误类型的检测，会根据整句话判定该错误，阿里参赛2016中文语法纠错任务并取得第一名，证明应用效果不错；</p>
<p>seq2seq模型是使用encoder-decoder结构解决序列转换问题，目前在序列转换任务中（如机器翻译、对话生成、文本摘要、图像描述）使用最广泛、效果最好的模型之一。</p>
<h1 id="bian-ji-ju-chi">编辑距离</h1>
<h2 id="jian-jie-1">简介</h2>
<p>编辑距离的经典应用就是用于拼写检错，如果用户输入的词语不在词典中，自动从词典中找出编辑距离小于某个数n的单词，让用户选择正确的那一个，n通常取到2或者3。</p>
<p>字符串A到B的<strong>编辑距离</strong>是指，只用插入、删除和替换三种操作，最少需要多少步可以把A变成B。例如，从FAME到GATE需要两步（两次替换），从GAME到ACM则需要三步（删除G和E再添加C）。</p>
<h2 id="dai-ma">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minDistance_recursive</span><span class="params">(self, word1, word2)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        递归解法</span></span><br><span class="line"><span class="string">        :type word1: str</span></span><br><span class="line"><span class="string">        :type word2: str</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> word1 <span class="keyword">or</span> <span class="keyword">not</span> word2:</span><br><span class="line">            <span class="comment"># 递归出口：其中一个字符串为空，返回非空字符串的长度</span></span><br><span class="line">            <span class="keyword">return</span> (len(word1) <span class="keyword">or</span> len(word2))</span><br><span class="line">        <span class="keyword">if</span> word1[<span class="number">-1</span>] == word2[<span class="number">-1</span>]:</span><br><span class="line">            <span class="comment"># 如果两个字符串最后一个字符相同</span></span><br><span class="line">            <span class="keyword">return</span> self.minDistance(word1[:<span class="number">-1</span>], word2[:<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> + min(self.minDistance(word1[:<span class="number">-1</span>], word2), self.minDistance(word1, word2[:<span class="number">-1</span>]),</span><br><span class="line">                       self.minDistance(word1[:<span class="number">-1</span>], word2[:<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minDistance_dp</span><span class="params">(self,word1,word2)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        动态规划解法</span></span><br><span class="line"><span class="string">        :param word1:</span></span><br><span class="line"><span class="string">        :param word2:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> word1 <span class="keyword">or</span> <span class="keyword">not</span> word2:</span><br><span class="line">            <span class="keyword">return</span> (len(word1) <span class="keyword">or</span> len(word2))</span><br><span class="line"></span><br><span class="line">        dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(word2)+<span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(word1)+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dp 初始化</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word1)+<span class="number">1</span>):</span><br><span class="line">            dp[i][<span class="number">0</span>] = i</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word2)+<span class="number">1</span>):</span><br><span class="line">            dp[<span class="number">0</span>][i] = i</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(word1)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,len(word2)+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> word1[i<span class="number">-1</span>] == word2[j<span class="number">-1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="number">1</span> + min(dp[i<span class="number">-1</span>][j],dp[i][j<span class="number">-1</span>],dp[i<span class="number">-1</span>][j<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[len(word1)][len(word2)]</span><br></pre></td></tr></table></figure>
<p><a href="https://leetcode-cn.com/problems/edit-distance/solution/bian-ji-ju-chi-by-leetcode-solution/" target="_blank" rel="noopener">leetcode题解</a></p>
<h1 id="pin-xie-zi-dong-jiu-cuo-xi-tong">拼写自动纠错系统</h1>
<p>给定一待纠错词w,需要从一系列候选词中选出一最可能的词c。也就是：\(argmax(p(c|w))\), c in 候选词表。根据贝叶斯原理，\(p(c|w) = p(w|c) * p(c) / p(w)\). 又有对任意可能的c,p(w)一样，故也就是求使\(argmax(p(w|c) * p(c))\)成立的c.</p>
<h2 id="candidate-model">Candidate Model:</h2>
<p>利用编辑距离小于1或者编辑距离小于2，来构造初步候选集，然后利用语料库，过滤初步候选集，得到符合规范的候选词。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits1</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="string">"All edits that are one edit away from `word`."</span></span><br><span class="line">    letters    = <span class="string">'abcdefghijklmnopqrstuvwxyz'</span></span><br><span class="line">    splits     = [(word[:i], word[i:])    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word) + <span class="number">1</span>)]</span><br><span class="line">    deletes    = [L + R[<span class="number">1</span>:]               <span class="keyword">for</span> L, R <span class="keyword">in</span> splits <span class="keyword">if</span> R]</span><br><span class="line">    transposes = [L + R[<span class="number">1</span>] + R[<span class="number">0</span>] + R[<span class="number">2</span>:] <span class="keyword">for</span> L, R <span class="keyword">in</span> splits <span class="keyword">if</span> len(R)&gt;<span class="number">1</span>]</span><br><span class="line">    replaces   = [L + c + R[<span class="number">1</span>:]           <span class="keyword">for</span> L, R <span class="keyword">in</span> splits <span class="keyword">if</span> R <span class="keyword">for</span> c <span class="keyword">in</span> letters]</span><br><span class="line">    inserts    = [L + c + R               <span class="keyword">for</span> L, R <span class="keyword">in</span> splits <span class="keyword">for</span> c <span class="keyword">in</span> letters]</span><br><span class="line">    <span class="keyword">return</span> set(deletes + transposes + replaces + inserts)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edits2</span><span class="params">(word)</span>:</span> </span><br><span class="line">  <span class="string">"编辑距离小于2的候选词"</span></span><br><span class="line">  <span class="keyword">return</span> (e2 <span class="keyword">for</span> e1 <span class="keyword">in</span> edits1(word) <span class="keyword">for</span> e2 <span class="keyword">in</span> edits1(e1))</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">known</span><span class="params">(words)</span>:</span> </span><br><span class="line">  <span class="string">"在语料中过滤出复合拼写规范的（正确）的词"</span></span><br><span class="line">  <span class="keyword">return</span> set(w <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> w <span class="keyword">in</span> corpus)</span><br></pre></td></tr></table></figure>
<h2 id="language-model">Language Model</h2>
<p>候选集合中的词再语料库中的出现概率。比如：the 在英文文本中出现的概率率为7%，则有：p(the)=0.07</p>
<p>,可以通过统计语料库中\(p(c)\) = 词c出现的频次/所有词的出现次数之和。（unigram，bigram）</p>
<h2 id="error-model">Error Model</h2>
<p>当想要输入the却意外拼写成teh的概率:p(teh|the)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">candidates</span><span class="params">(word)</span>:</span> </span><br><span class="line">    <span class="keyword">return</span> known([word]) <span class="keyword">or</span> known(edits1(word)) <span class="keyword">or</span> known(edits2(word)) <span class="keyword">or</span> [word]</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">correction</span><span class="params">(word)</span>:</span> </span><br><span class="line">    <span class="keyword">return</span> max(candidates(word), key=P)</span><br></pre></td></tr></table></figure>
<h2 id="selection-mechanism">Selection Mechanism</h2>
<p>从正确的拼写候选集中选择最有可能的正确结果，argmax</p>
<h2 id="zong-jie">总结</h2>
<ol>
<li>利用编辑距离构造正确结果候选集</li>
<li>利用language model 和error model 来计算概率。</li>
<li>从正确结果候选集中选择概率最大的正确拼写。</li>
</ol>
<h3 id="can-kao">参考</h3>
<ol>
<li>
<p><a href="https://cloud.tencent.com/developer/article/1435917" target="_blank" rel="noopener">中文文本纠错算法走到多远了？</a></p>
</li>
<li>
<p><a href="https://github.com/shibing624/pycorrector" target="_blank" rel="noopener">pycorrect</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>文本纠错</tag>
      </tags>
  </entry>
  <entry>
    <title>关键词抽取</title>
    <url>/2020/04/21/keyword_extraction/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/04/21/keyword_extraction/gjc01.png" alt></p>
<a id="more"></a>
<h1 id="tf-idf-ti-qu-guan-jian-ci">TF-IDF 提取关键词</h1>
<h2 id="jian-jie">简介</h2>
<p>TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率).TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一份文件在一个语料库中的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。换句话说就是：<strong>一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章。</strong></p>
<blockquote>
<p>TF-IDF 算法主要适用于英文，中文首先要分词，分词后要解决多词一义，以及一词多义问题，这两个问题通过简单的tf-idf方法不能很好的解决。于是就有了后来的词嵌入方法，用向量来表征一个词。</p>
</blockquote>
<h2 id="tf-idf">TF-IDF</h2>
<h3 id="tf">TF</h3>
<p>表示词条（关键字）在文本中出现的次数（frequency）(一般不用)。TF 背后的隐含的假设是，查询关键字中的单词应该相对于其他单词更加重要，而文档的重要程度，也就是相关度，与单词在文档中出现的次数成正比。比如，“Car” 这个单词在文档 A 里出现了 5 次，而在文档 B 里出现了 20 次，那么 TF 计算就认为文档 B 可能更相关。</p>
<h4 id="bian-chong-yi-tong-guo-dui-shu-han-shu-bi-mian-tf-xian-xing-zeng-chang">变种一:通过对数函数避免 TF 线性增长</h4>
<p>理由：虽然我们一般认为一个文档包含查询关键词多次相对来说表达了某种相关度，但这样的关系很难说是线性的。以 “Car Insurance” 为例，文档 A 可能包含 “Car” 这个词 100 次，而文档 B 可能包含 200 次，是不是说文档 B 的相关度就是文档 A 的 2 倍呢？其实，当这种频次超过了某个阈值之后，这个 TF 也就没那么有区分度了。</p>
<p><strong>用 Log，也就是对数函数，对 TF 进行变换，就是一个不让 TF 线性增长的技巧</strong>。具体来说，人们常常用 1+Log(TF) 这个值来代替原来的 TF 取值。在这样新的计算下，假设 “Car” 出现一次，新的值是 1，出现 100 次，新的值是 \(log 100=5.6\)，而出现 200 次，新的值是\(log 200 = 6.3\)。很明显，这样的计算保持了一个平衡，既有区分度，但也不至于完全线性增长。</p>
<h4 id="bian-chong-er-biao-zhun-hua-jie-jue-chang-wen-dang-duan-wen-dang-wen-ti">变种二：标准化解决长文档、短文档问题</h4>
<p>经典的计算并没有考虑 “长文档” 和“短文档”的区别。一个文档 A 有 3,000 个单词，一个文档 B 有 250 个单词，很明显，即便 “Car” 在这两个文档中都同样出现过 20 次，也不能说这两个文档都同等相关。<strong>对 TF 进行 “标准化”（Normalization），特别是根据文档的最大 TF 值进行的标准化，成了另外一个比较常用的技巧</strong>。<br>
\[
tf_{ij} = \frac{n_{i,j}}{\sum_k n_{k,j}}
\]<br>
其中\(n_{i,j}\)是词\(w_i\)在文档\(d_j\)中出现的次数，分母是文档\(d_j\)中所有词汇出现的次数总和。</p>
<h3 id="idf">IDF</h3>
<p>仅有 TF 不能比较完整地描述文档的相关度。因为语言的因素，有一些单词可能会比较自然地在很多文档中反复出现，比如英语中的 “The”、“An”、“But” 等等。这些词大多起到了链接语句的作用，是保持语言连贯不可或缺的部分。然而，如果我们要搜索 “How to Build A Car” 这个关键词，其中的 “How”、“To” 以及 “A” 都极可能在绝大多数的文档中出现，这个时候 TF 就无法帮助我们区分文档的相关度了。</p>
<p>逆文档频率的思路很简单，就是我们需要去 “惩罚”（Penalize）那些出现在太多文档中的单词。</p>
<h4 id="bian-chong-san-dui-shu-han-shu-chu-li-idf">变种三：对数函数处理 IDF</h4>
<p>某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。<br>
\[
idf_w = log{\frac{|D|}{|j:t_i \in d_j|}}
\]</p>
<p>其中，\(|D|\)表示语料库中文档总数，分母为包含词w的文档数+1，分母加1是为了避免出现分母为零的情况。</p>
<p>样做的好处就是，第一，使用了文档总数来做标准化，很类似上面提到的标准化的思路；第二，利用对数来达到非线性增长的目的。</p>
<h4 id="bian-chong-si-cha-xun-ci-ji-wen-dang-xiang-liang-biao-zhun-hua">变种四：查询词及文档向量标准化</h4>
<p>对查询关键字向量，以及文档向量进行标准化，使得这些向量能够不受向量里有效元素多少的影响，也就是不同的文档可能有不同的长度。在线性代数里，可以把向量都标准化为一个单位向量的长度。这个时候再进行点积运算，就相当于在原来的向量上进行余弦相似度的运算。所以，另外一个角度利用这个规则就是直接在多数时候进行余弦相似度运算，以代替点积运算。</p>
<h3 id="tf-idf-1">TF-IDF</h3>
<p>某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语(关键词)。<br>
\[
TF-IDF = TF * IDF
\]</p>
<h2 id="tf-idf-de-ying-yong">TF-IDF 的应用</h2>
<ol>
<li>搜索引擎</li>
<li>关键词提取</li>
<li>文本相似性</li>
<li>文本摘要</li>
</ol>
<h2 id="dai-ma">代码</h2>
<figure class="highlight python"><figcaption><span>tf-idf</span><a href="https://github.com/jeffery0628/keyword_extraction" target="_blank" rel="noopener">github</a></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf_idf</span><span class="params">(document, corpus)</span>:</span>  <span class="comment"># 计算TF-IDF,并返回字典</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param document: 计算document里面每个词的tfidf值，document为文本分词后的形式，</span></span><br><span class="line"><span class="string">    如:[6 月 19 日 2012 年度 中国 爱心 城市 公益活动 新闻 发布会 在京举行]</span></span><br><span class="line"><span class="string">    如果是对一篇文档进行关键词提取，则需要对文档进行分句，把每句话看成一个document，corpus则存放的是整篇文档分词后的所有句子（句子为分词后的结果）。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param corpus:  corpus为所有问当分词后的列表：[document1,document2,document3,...]</span></span><br><span class="line"><span class="string">    :return:dict类型，按照tfidf值从大到小排序： orderdict[word] = tfidf_value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    word_tfidf = &#123;&#125;</span><br><span class="line">    <span class="comment"># 计算词频</span></span><br><span class="line">    freq_words = Counter(document)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> freq_words:</span><br><span class="line">        <span class="comment"># 计算TF：某个词在文章中出现的次数/文章总词数</span></span><br><span class="line">        tf = freq_words[word] / len(document)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算IDF：log(语料库的文档总数/(包含该词的文档数+1))</span></span><br><span class="line">        idf = math.log(len(corpus) / (wordinfilecount(word, corpus) + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算每个词的TFIDF值</span></span><br><span class="line">        tfidf = tf * idf  <span class="comment"># 计算TF-IDF</span></span><br><span class="line">        word_tfidf[word] = tfidf</span><br><span class="line"></span><br><span class="line">    orderdic = sorted(word_tfidf.items(), key=<span class="keyword">lambda</span> item: item[<span class="number">1</span>], reverse=<span class="literal">True</span>)  <span class="comment"># 给字典排序</span></span><br><span class="line">    <span class="keyword">return</span> orderdic</span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    stop_words_path = <span class="string">r'stop_words.txt'</span>  <span class="comment"># 停用词表路径</span></span><br><span class="line">    stop_words = get_stopwords(stop_words_path)  <span class="comment"># 获取停用词表列表</span></span><br><span class="line"></span><br><span class="line">    documents_dir = <span class="string">'data'</span></span><br><span class="line">    filelist = get_documents(documents_dir)  <span class="comment"># 获取文件列表</span></span><br><span class="line"></span><br><span class="line">    corpus = get_corpus(filelist, stop_words)  <span class="comment"># 建立语料库</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx,document <span class="keyword">in</span> enumerate(corpus):</span><br><span class="line">        word_tfidf = tf_idf(document, corpus)  <span class="comment"># 计算TF-IDF</span></span><br><span class="line">        <span class="comment"># 输出前十关键词</span></span><br><span class="line">        print(<span class="string">'document &#123;&#125;,top 10 key words&#123;&#125;'</span>.format(idx+<span class="number">1</span>,word_tfidf[:<span class="number">10</span>]))</span><br></pre></td></tr></table></figure>
<p>使用scikit-learn 计算tfidf</p>
<figure class="highlight python"><figcaption><span>tf-idf</span><a href="https://github.com/jeffery0628/keyword_extraction" target="_blank" rel="noopener">github</a></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer, TfidfVectorizer</span><br><span class="line"><span class="comment"># 对语料进行稍微处理</span></span><br><span class="line">corpus = [*map(<span class="keyword">lambda</span> x:<span class="string">" "</span>.join(x), corpus)]</span><br><span class="line">tfidf_model = TfidfVectorizer()</span><br><span class="line">tfidf_matrix = tfidf_model.fit_transform(corpus)  <span class="comment"># 计算每个词的tfidf值</span></span><br><span class="line">words = tfidf_model.get_feature_names()<span class="comment"># 所有词的集合</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(corpus)):</span><br><span class="line">  	word_tfidf = &#123;&#125;</span><br><span class="line">  	<span class="keyword">for</span> j <span class="keyword">in</span> range(len(words)):</span><br><span class="line">        word_tfidf[words[j]] = tfidf_matrix[i, j]</span><br><span class="line">        word_tfidf = sorted(word_tfidf.items(),key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>],reverse=<span class="literal">True</span>)</span><br><span class="line">        print(<span class="string">'document &#123;&#125;,top 10 key words&#123;&#125;'</span>.format(i+<span class="number">1</span>, word_tfidf[:<span class="number">10</span>]))</span><br></pre></td></tr></table></figure>
<h1 id="text-rank-ti-qu-guan-jian-ci">TextRank 提取关键词</h1>
<h2 id="jian-jie-1">简介</h2>
<p>TF-IDF 算法对有多段文本的关键词提取非常有效，但是对于单篇或者文档分割较少的文本表现得不是特别好。如果需要提取关键词的语句只有一句话，那么基于TF-IDF可以知道，所有关键词的重要度都为0(因为IDF值为0)，这种情况下使用TextRank是比较好的选择。</p>
<p>TextRank是一种基于图排序的算法，基本思想是(来源于PageRank)：通过把文本分割成若干组成单元(单词、句子)并建立图模型，利用投票机制对文本中的重要成分进行排序，<strong>仅利用单篇文档本身的信息就可以实现关键词提取、文本摘要</strong>。和 LDA、HMM 等模型不同, TextRank不需要事先对多篇文档进行学习训练, 因其简洁有效而得到广泛应用。</p>
<h2 id="page-rank">PageRank</h2>
<h3 id="page-rank-de-jian-hua-mo-xing">PageRank 的简化模型</h3>
<p>假设一共有 4 个网页 A、B、C、D。它们之间的链接信息如图所示：</p>
<p><img src="/2020/04/21/keyword_extraction/pagerank.png" alt="avatar"></p>
<p>出链指的是链接出去的链接。入链指的是链接进来的链接。比如图中 A 有 2 个入链，3 个出链。简单来说，一个网页的影响力 = 所有入链集合的页面的加权影响力之和，用公式表示为：</p>
<p>\[
P R(u)=\sum_{v \in B_{u}} \frac{P R(v)}{L(v)}
\]<br>
u 为待评估的页面， \(B_{u}\) 为页面 \(u\) 的入链集合。针对入链集合中的任意页面 \(v\)，它能给 \(u\) 带来的影响力是其自身的影响力 \(PR(v)\) 除以 \(v\) 页面的出链数量，即页面 \(v\) 把影响力 \(PR(v)\) 平均分配给了它的出链，这样统计所有能给 \(u\) 带来链接的页面 \(v\)，得到的总和就是网页 \(u\) 的影响力，即为 \(PR(u)\)。所以出链会给被链接的页面赋予影响力，当我们统计了一个网页链出去的数量，也就是统计了这个网页的跳转概率。</p>
<p>在例子中，A 有三个出链分别链接到了 B、C、D 上。那么当用户访问 A 的时候，就有跳转到 B、C 或者 D 的可能性，跳转概率均为 1/3。B 有两个出链，链接到了 A 和 D 上，跳转概率为 1/2。这样，我们可以得到 A、B、C、D 这四个网页的转移矩阵 M：</p>
<p>\[
M=\left[\begin{array}{cccc}
0 &amp; 1 / 2 &amp; 1 &amp; 0 \\
1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\
1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\
1 / 3 &amp; 1 / 2 &amp; 0 &amp; 0
\end{array}\right]
\]<br>
假设 A、B、C、D 四个页面的初始影响力都是相同的，即：<br>
\[
w_{0}=\left[\begin{array}{l}
1 / 4 \\
1 / 4 \\
1 / 4 \\
1 / 4
\end{array}\right]
\]<br>
当进行第一次转移之后，各页面的影响力 \(w_{1}\) 变为：<br>
\[
w_1=Mw_0=\left[\begin{array}{cccc}
0 &amp; 1 / 2 &amp; 1 &amp; 0 \\
1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\
1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\
1 / 3 &amp; 1 / 2 &amp; 0 &amp; 0
\end{array}\right]\left[\begin{array}{c}
1 / 4 \\
1 / 4 \\
1 / 4 \\
1 / 4
\end{array}\right]=\left[\begin{array}{c}
9 / 24 \\
5 / 24 \\
5 / 24 \\
5 / 24
\end{array}\right]
\]<br>
然后再用转移矩阵乘以 \(w_{1}\) 得到 \(w_{2}\) 结果，直到第 \(n\) 次迭代后 \(w_{n}\) 影响力不再发生变化，可以收敛到 \((0.3333,0.2222,0.2222,0.2222\)，也就是对应着 A、B、C、D 四个页面最终平衡状态下的影响力。能看出 A 页面相比于其他页面来说权重更大，也就是 PR 值更高。而 B、C、D 页面的 PR 值相等。</p>
<h3 id="deng-ji-xie-lu-rank-leak">等级泄露（Rank Leak）</h3>
<p>如果一个网页没有出链，就像是一个黑洞一样，吸收了其他网页的影响力而不释放，最终会导致其他网页的 PR 值为 0。</p>
<p><img src="/2020/04/21/keyword_extraction/rank_leak.png" alt="avatar"></p>
<h3 id="deng-ji-chen-mei-rank-sink">等级沉没（Rank Sink）</h3>
<p>如果一个网页只有出链，没有入链，计算的过程迭代下来，会导致<strong>这个网页</strong>的 PR 值为 0（也就是不存在公式中的 V）。</p>
<p><img src="/2020/04/21/keyword_extraction/rank_sink.png" alt="avatar"></p>
<h3 id="jie-jue-fang-an-sui-ji-liu-lan">解决方案：随机浏览</h3>
<p>为了解决简化模型中存在的等级泄露和等级沉没的问题，拉里·佩奇提出了 PageRank 的随机浏览模型。他假设了这样一个场景：用户并不都是按照跳转链接的方式来上网，还有一种可能是不论当前处于哪个页面，都有概率访问到其他任意的页面，比如说用户就是要直接输入网址访问其他页面，虽然这个概率比较小。</p>
<p>所以他定义了阻尼因子 d，这个因子代表了用户按照跳转链接来上网的概率，通常可以取一个固定值 0.85，而 1-d=0.15 则代表了用户不是通过跳转链接的方式来访问网页的，比如直接输入网址。<br>
\[
P R(u)=\frac{1-d}{N}+d \sum_{v \in B_{u}} \frac{P R(v)}{L(v)}
\]</p>
<p>其中 \(N\) 为网页总数，这样我们又可以重新迭代网页的权重计算了，加入了阻尼因子 \(d\)，一定程度上解决了等级泄露和等级沉没的问题。通过数学定理也可以证明，最终 PageRank 随机浏览模型是可以收敛的，也就是可以得到一个稳定正常的 PR 值。</p>
<h2 id="text-rank">TextRank</h2>
<p>TextRank通过词之间的相邻关系构建网络，然后用PageRank迭代计算每个节点的rank值，排序rank值即可得到关键词。PageRank迭代计算公式如下：</p>
<p>\[
P R\left(V_{i}\right)=(1-d)+d * \sum_{j \in \operatorname{In}\left(V_{i}\right)} \frac{1}{\left|O u t\left(V_{j}\right)\right|} P R\left(V_{j}\right)
\]<br>
其中，d:表示阻尼系数，一般为0.85。\(V_i\)：表示图中任一节点。\(In(V_i)\):表示指向顶点\(V_i\)的所有顶点的集合。\(|Out(V_j)|\)：表示由顶点\(V_j\)连接出去的所有顶点集合个数。\(PR(V_i)\)：表示顶点\(V_i\)的最终排序权重。(与pagerank公式基本一致。)</p>
<p>网页之间的链接关系可以用图表示，那么怎么把一个句子（可以看作词的序列）构建成图呢？TextRank将某一个词与其前面的N个词、以及后面的N个词均具有图相邻关系（类似于N-gram语法模型）。具体实现：设置一个长度为N的滑动窗口，所有在这个窗口之内的词都视作词结点的相邻结点；则TextRank构建的词图为无向图。下图给出了由一个文档构建的词图（去掉了停用词并按词性做了筛选）：</p>
<p><img src="/2020/04/21/keyword_extraction/textrank.png" alt="avatar"></p>
<p>考虑到不同词对可能有不同的共现（co-occurrence），TextRank将共现作为无向图边的权值。那么，TextRank的迭代计算公式如下：<br>
\[
W S\left(V_{i}\right)=(1-d)+d * \sum_{j \in \operatorname{In}\left(V_{i}\right)} \frac{w_{ij}}{\left|O u t\left(V_{j}\right)\right|} W S\left(V_{j}\right)
\]<br>
该公式仅仅比PageRank多了一个权重项\(w_{ij}\)，用来表示两个节点之间的边连接有不同的重要程度。</p>
<h3 id="text-rank-guan-jian-ci-duan-yu-ti-qu-suan-fa">TextRank 关键词(短语)提取算法</h3>
<ol>
<li>把给定的文本\(T\)按照完整句子进行分割，即\(T = [S_1,S_2,\ldots,S_n]\).</li>
<li>文本\(T\)中每个句子\(S_i\)，进行分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词，如名词、动词、形容词，即\(S_i = [t_{i,1},t_{i,2},\ldots,t_{i,\pi}]\),其中\(t_{ij}\)是保留后的候选关键词。</li>
<li>构建候选关键词图\(G = (V,E)\)，其中\(V\)为节点集，由步骤2生成的候选关键词组成，然后采用共现关系（co-occurrence）构造任两点之间的边，两个节点之间存在边仅当它们对应的词汇在长度为K的窗口中共现，K表示窗口大小，即最多共现K个单词。</li>
<li>根据上面公式，迭代传播各节点的权重，直至收敛。</li>
<li>对节点权重进行倒序排序，从而得到最重要的T个单词，作为候选关键词。</li>
<li>由步骤5得到最重要的T个单词，在原始文本中进行标记，若形成相邻词组，则组合成多词关键词。</li>
</ol>
<h3 id="dai-ma-1">代码</h3>
<figure class="highlight python"><figcaption><span>textrank</span><a href="https://github.com/jeffery0628/keyword_extraction" target="_blank" rel="noopener">github</a></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">textrank_graph</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.graph = defaultdict(list) <span class="comment"># key:[(),(),(),...] 如：是 [('是', '全国', 1), ('是', '调查', 1), ('是', '失业率', 1), ('是', '城镇', 1)]</span></span><br><span class="line">        self.d = <span class="number">0.85</span>  <span class="comment"># d是阻尼系数，一般设置为0.85</span></span><br><span class="line">        self.min_diff = <span class="number">1e-5</span>  <span class="comment"># 设定收敛阈值</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加节点之间的边</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addEdge</span><span class="params">(self, start, end, weight)</span>:</span></span><br><span class="line">        self.graph[start].append((start, end, weight))</span><br><span class="line">        self.graph[end].append((end, start, weight))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 节点排序</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rank</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 一共有14个节点</span></span><br><span class="line">        print(len(self.graph))</span><br><span class="line">        <span class="comment"># 默认初始化权重</span></span><br><span class="line">        weight_deault = <span class="number">1.0</span> / (len(self.graph) <span class="keyword">or</span> <span class="number">1.0</span>)</span><br><span class="line">        <span class="comment"># nodeweight_dict, 存储节点的权重</span></span><br><span class="line">        nodeweight_dict = defaultdict(float)</span><br><span class="line">        <span class="comment"># outsum，存储节点的出度权重</span></span><br><span class="line">        outsum_node_dict = defaultdict(float)</span><br><span class="line">        <span class="comment"># 根据图中的边，更新节点权重</span></span><br><span class="line">        <span class="keyword">for</span> node, out_edge <span class="keyword">in</span> self.graph.items():</span><br><span class="line">            <span class="comment"># 是 [('是', '全国', 1), ('是', '调查', 1), ('是', '失业率', 1), ('是', '城镇', 1)]</span></span><br><span class="line">            nodeweight_dict[node] = weight_deault <span class="comment"># 初始化节点权重</span></span><br><span class="line">            outsum_node_dict[node] = sum((edge[<span class="number">2</span>] <span class="keyword">for</span> edge <span class="keyword">in</span> out_edge), <span class="number">0.0</span>) <span class="comment"># 统计node节点的出度</span></span><br><span class="line">        <span class="comment"># 初始状态下的textrank重要性权重</span></span><br><span class="line">        sorted_keys = sorted(self.graph.keys())</span><br><span class="line">        <span class="comment"># 设定迭代次数，</span></span><br><span class="line">        step_dict = [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">1000</span>):</span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> sorted_keys:</span><br><span class="line">                s = <span class="number">0</span></span><br><span class="line">                <span class="comment"># 计算公式：(edge_weight/outsum_node_dict[edge_node])*node_weight[edge_node]</span></span><br><span class="line">                <span class="keyword">for</span> e <span class="keyword">in</span> self.graph[node]:</span><br><span class="line">                    s += e[<span class="number">2</span>] / outsum_node_dict[e[<span class="number">1</span>]] * nodeweight_dict[e[<span class="number">1</span>]]</span><br><span class="line">                <span class="comment"># 计算公式：(1-d) + d*s</span></span><br><span class="line">                nodeweight_dict[node] = (<span class="number">1</span> - self.d) + self.d * s</span><br><span class="line">            step_dict.append(sum(nodeweight_dict.values()))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> abs(step_dict[step] - step_dict[step - <span class="number">1</span>]) &lt;= self.min_diff:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 利用Z-score进行权重归一化，也称为离差标准化，是对原始数据的线性变换，使结果值映射到[0 - 1]之间。</span></span><br><span class="line">        <span class="comment"># 先设定最大值与最小值均为系统存储的最大值和最小值</span></span><br><span class="line">        (min_rank, max_rank) = (sys.float_info[<span class="number">0</span>], sys.float_info[<span class="number">3</span>])</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> nodeweight_dict.values():</span><br><span class="line">            <span class="keyword">if</span> w &lt; min_rank:</span><br><span class="line">                min_rank = w</span><br><span class="line">            <span class="keyword">if</span> w &gt; max_rank:</span><br><span class="line">                max_rank = w</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> n, w <span class="keyword">in</span> nodeweight_dict.items(): <span class="comment"># 归一化</span></span><br><span class="line">            nodeweight_dict[n] = (w - min_rank / <span class="number">10.0</span>) / (max_rank - min_rank / <span class="number">10.0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nodeweight_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextRank</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.candi_pos = [<span class="string">'n'</span>, <span class="string">'v'</span>, <span class="string">'a'</span>] <span class="comment"># 关键词的词性：名词，动词，形容词</span></span><br><span class="line">        self.span = <span class="number">5</span> <span class="comment"># 窗口大小</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract_keywords</span><span class="params">(self, text, num_keywords)</span>:</span></span><br><span class="line">        g = textrank_graph()</span><br><span class="line">        cm = defaultdict(int)</span><br><span class="line">        word_list = [[word.word, word.flag] <span class="keyword">for</span> word <span class="keyword">in</span> pseg.cut(text)] <span class="comment"># 使用jieba分词并且对词性进行标注</span></span><br><span class="line">        <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(word_list): <span class="comment"># 该循环用于统计在窗口范围内，词的共现次数</span></span><br><span class="line">            <span class="keyword">if</span> word[<span class="number">1</span>][<span class="number">0</span>] <span class="keyword">in</span> self.candi_pos <span class="keyword">and</span> len(word[<span class="number">0</span>]) &gt; <span class="number">1</span>: <span class="comment">#</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, i + self.span):</span><br><span class="line">                    <span class="keyword">if</span> j &gt;= len(word_list):<span class="comment"># 防止下标越界</span></span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    <span class="keyword">if</span> word_list[j][<span class="number">1</span>][<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> self.candi_pos <span class="keyword">or</span> len(word_list[j][<span class="number">0</span>]) &lt; <span class="number">2</span>: <span class="comment"># 排除词性不在关键词词性列表中的词或者词长度小于2的词</span></span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    pair = tuple((word[<span class="number">0</span>], word_list[j][<span class="number">0</span>]))</span><br><span class="line">                    cm[(pair)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> terms, w <span class="keyword">in</span> cm.items():</span><br><span class="line">            g.addEdge(terms[<span class="number">0</span>], terms[<span class="number">1</span>], w)</span><br><span class="line">        nodes_rank = g.rank()</span><br><span class="line">        nodes_rank = sorted(nodes_rank.items(), key=<span class="keyword">lambda</span> asd:asd[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nodes_rank[:num_keywords]</span><br></pre></td></tr></table></figure>
<h1 id="lda-latent-dirichlet-allocation-ti-qu-guan-jian-ci">LDA (Latent Dirichlet Allocation) 提取关键词</h1>
<h2 id="jian-jie-2">简介</h2>
<p>TF-IDF 和 TextRank 两种算法更多反映的是文本的统计信息，对于文本之间的语义关系考虑得比较少。LDA是一种能够体文本语义关系的关键词提取方法。</p>
<blockquote>
<p>二项分布（Binomial Distribution），即重复n次的伯努利试验（Bernoulli Experiment），用\(\xi\)表示随机试验的结果。如果事件发生的概率是P,则不发生的概率\(q=1-p\)，\(N\)次独立重复试验中发生K次的概率是\(P(\xi=K)= C(n,k) * p^k * (1-p)^{n-k}\)，其中\(C(n, k) =\frac{n!}{(k!(n-k)!)}\). 期望：\(E(ξ)=np\),方差：\(D(ξ)=npq\)其中\(q=1-p\)</p>
<p>多项分布（Multinomial Distribution）：多项式分布是二项式分布的推广。二项分布的典型例子是扔硬币，硬币正面朝上概率为p, 重复扔n次硬币，k次为正面的概率即为一个二项分布概率。把二项分布公式推广至多种状态，就得到了多项分布。<br>
某随机实验如果有k个可能结局\(A_1,A_2,\ldots,A_k\)，分别将他们的出现次数记为随机变量\(X_1,X_2,\ldots,X_k\)，它们的概率分布分别是\(p_1,p_2,\ldots,p_k\)那么在\(n\)次采样的总结果中，\(A_1\)出现\(n_1\)次、\(A_2\)出现\(n_2\)次,\(\dots\),\(A_k\)出现\(n_k\)次的这种事件的出现概率\(P\)有下面公式：<br>
\[
P\left(X_{1}=n_{1}, \cdots, X_{k}=n_{k}\right)=\left\{\begin{array}{ll}
\frac{n !}{n_{1} ! \cdots n_{k} !} p_{1}^{n_{1}} \cdots p_{k}^{n_{k}} &amp; , \sum_{i=1}^{k} n_{i}=n \\
0 &amp; , \text { otherwise }
\end{array}\right.
\]<br>
Beta分布与Dirichlet分布的定义域均为[0,1]，在实际使用中，通常将两者作为概率的分布，Beta分布描述的是单变量分布，Dirichlet分布描述的是多变量分布，因此，Beta分布可作为二项分布的先验概率，Dirichlet分布可作为多项分布的先验概率。</p>
</blockquote>
<p>在主题模型中，主题表示一个概念，表现为一系列相关的单词，是这些单词的条件概率。形象来说，主题就是一个桶，里面装了出现概率较高的单词，这些单词与这个主题有很强的相关性。</p>
<h2 id="li-lun">理论</h2>
<p>怎样才能生成主题？对文章的主题应该怎么分析？这是主题模型要解决的问题。</p>
<p>首先，可以用生成模型来看文档和主题这两件事。所谓生成模型，就是说，我们认为**一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”**这样一个过程得到的。那么，如果我们要生成一篇文档，它里面的每个词语出现的概率为：<br>
\[
p(W|D) = \sum_T P(W|T)P(T|D)
\]<br>
其中，\(W\)表示词，\(T\)表示主题，\(D\)表示文档。</p>
<p><img src="/2020/04/21/keyword_extraction/lda.png" alt="avatar"></p>
<p>其中”文档-词语”矩阵表示每个单词在每个文档中的词频，即出现的概率；”主题-词语”矩阵表示每个主题中每个单词的出现概率；”文档-主题”矩阵表示每个主题在每个文档中出现的概率。</p>
<p>单词对于主题的概率和主题对于文档的概率，可以通过Gibbs采样法（？？？）来进行概率的计算。</p>
<p>主题\(T_k\)下各个词\(W_i\)的权重计算公式：<br>
\[
P(W_i|T_k) = \frac{C_{ik}+\beta}{\sum_{i=1}^N{C_{ik}+N*\beta}} = \phi_i^{t=k} 
\]<br>
其中，\(w_i\)：表示单词集合中的任一单词。\(T_k\):表示主题集合中任一主题。\(P(w_i|T_k)\):表示在主题为\(k\)时，单词\(i\)出现的概率，其简记为\(\phi_i^{t=k}\)，\(C_{ik}\):表示语料库中单词\(i\)被赋予主题\(k\)的次数。\(N\):表示词汇表的大小。\(\beta\)：表示超参数。</p>
<p>文档\(D_m\)下各个词\(T_k\)的权重计算公式：<br>
\[
P(T_k|D_m) = \frac{C_{km}+\alpha}{\sum^K_{k=1}C_{km}+K*\alpha}=\theta^m_{t=k}
\]<br>
其中，\(D_m\):表示文档集合中任一文档。\(T_k\):表示主题集合中任一主题。\(P(T_k|D_m)\):表示语料库中文档m中单词被赋予主题\(k\)的次数。\(K\)：表示主题的数量。\(\alpha\)表示超参数。</p>
<p>得到了指定文档下某主题出现的概率，以及指定主题下、某单词出现的概率。那么由联合概率分布可以知道，对于指定文档某单词出现的概率：<br>
\[
P(W_i|D_m) = \sum_{k=1}^K{\phi_i^{t=k}*\theta_{t=k}^m}
\]<br>
基于上述公式，可以计算出单词\(i\)对于文档\(m\)的主题重要性。</p>
<p>但是由于在LDA主题概率模型中，所有的词汇都会以一定的概率出现在每个主题，所以这样会导致最终计算的单词对于文档的主题重要性区分度受影响。为了避免这种情况，一般会将单词相对于主题概率小于一定阈值的概率置为0.</p>
<h2 id="dai-ma-2">代码</h2>
<figure class="highlight python"><figcaption><span>LDA</span><a href="https://github.com/jeffery0628/keyword_extraction" target="_blank" rel="noopener">github</a></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 主题模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TopicModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 三个传入参数：处理后的数据集，关键词数量，具体模型（LSI、LDA），主题数量</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, doc_list, keyword_num, model=<span class="string">'LDA'</span>, num_topics=<span class="number">4</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 使用gensim的接口，将文本转为向量化表示</span></span><br><span class="line">        <span class="comment"># 先构建词空间</span></span><br><span class="line">        self.dictionary = corpora.Dictionary(doc_list)</span><br><span class="line">        <span class="comment"># 使用BOW模型向量化 (token_id,freq)</span></span><br><span class="line">        corpus = [self.dictionary.doc2bow(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> doc_list]  <span class="comment"># (token_id,freq)</span></span><br><span class="line">        <span class="comment"># 对每个词，根据tf-idf进行加权，得到加权后的向量表示</span></span><br><span class="line">        self.tfidf_model = models.TfidfModel(corpus)</span><br><span class="line">        self.tfidf_corpus = self.tfidf_model[corpus]</span><br><span class="line"></span><br><span class="line">        self.keyword_num = keyword_num</span><br><span class="line">        self.num_topics = num_topics</span><br><span class="line">        <span class="comment"># 选择加载胡模型</span></span><br><span class="line">        <span class="keyword">if</span> model == <span class="string">'LSI'</span>:</span><br><span class="line">            self.model = self.train_lsi()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.model = self.train_lda()</span><br><span class="line">        <span class="comment"># 得到数据集的主题-词分布</span></span><br><span class="line">        word_dic = self.word_dictionary(doc_list)</span><br><span class="line">        self.wordtopic_dic = self.get_wordtopic(word_dic)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 向量化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">doc2bowvec</span><span class="params">(self, word_list)</span>:</span></span><br><span class="line">        vec_list = [<span class="number">1</span> <span class="keyword">if</span> word <span class="keyword">in</span> word_list <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> word <span class="keyword">in</span> self.dictionary]</span><br><span class="line">        print(<span class="string">"vec_list"</span>, vec_list)</span><br><span class="line">        <span class="keyword">return</span> vec_list</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 词空间构建方法和向量化方法，在没有gensim接口时的一般处理方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word_dictionary</span><span class="params">(self, doc_list)</span>:</span></span><br><span class="line">        dictionary = []</span><br><span class="line">        <span class="comment"># 2及变1及结构</span></span><br><span class="line">        <span class="keyword">for</span> doc <span class="keyword">in</span> doc_list:</span><br><span class="line">            <span class="comment"># extend he append 方法有何异同 容易出错</span></span><br><span class="line">            dictionary.extend(doc)</span><br><span class="line"></span><br><span class="line">        dictionary = list(set(dictionary))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dictionary</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到数据集的主题 - 词分布</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_wordtopic</span><span class="params">(self, word_dic)</span>:</span></span><br><span class="line">        wordtopic_dic = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> word_dic:</span><br><span class="line">            singlist = [word]</span><br><span class="line">            <span class="comment"># 计算每个词胡加权向量</span></span><br><span class="line">            word_corpus = self.tfidf_model[self.dictionary.doc2bow(singlist)]</span><br><span class="line">            <span class="comment"># 计算每个词de主题向量</span></span><br><span class="line">            word_topic = self.model[word_corpus]</span><br><span class="line">            wordtopic_dic[word] = word_topic</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> wordtopic_dic</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_lsi</span><span class="params">(self)</span>:</span></span><br><span class="line">        lsi = models.LsiModel(self.tfidf_corpus, id2word=self.dictionary, num_topics=self.num_topics)</span><br><span class="line">        <span class="keyword">return</span> lsi</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_lda</span><span class="params">(self)</span>:</span></span><br><span class="line">        lda = models.LdaModel(self.tfidf_corpus, id2word=self.dictionary, num_topics=self.num_topics)</span><br><span class="line">        <span class="keyword">return</span> lda</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算词的分布和文档的分布的相似度，取相似度最高的keyword_num个词作为关键词</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_simword</span><span class="params">(self, word_list)</span>:</span></span><br><span class="line">        <span class="comment"># 文档的加权向量</span></span><br><span class="line">        sentcorpus = self.tfidf_model[self.dictionary.doc2bow(word_list)]</span><br><span class="line">        <span class="comment"># 文档主题 向量</span></span><br><span class="line">        senttopic = self.model[sentcorpus]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># senttopic [(0, 0.03457821), (1, 0.034260772), (2, 0.8970413), (3, 0.034119748)]</span></span><br><span class="line">        <span class="comment"># 余弦相似度计算</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">calsim</span><span class="params">(l1, l2)</span>:</span></span><br><span class="line">            a, b, c = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">            <span class="keyword">for</span> t1, t2 <span class="keyword">in</span> zip(l1, l2):</span><br><span class="line">                x1 = t1[<span class="number">1</span>]</span><br><span class="line">                x2 = t2[<span class="number">1</span>]</span><br><span class="line">                a += x1 * x1</span><br><span class="line">                b += x1 * x1</span><br><span class="line">                c += x2 * x2</span><br><span class="line">            sim = a / math.sqrt(b * c) <span class="keyword">if</span> <span class="keyword">not</span> (b * c) == <span class="number">0.0</span> <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line">            <span class="keyword">return</span> sim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算输入文本和每个词的主题分布相似度</span></span><br><span class="line">        sim_dic = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> self.wordtopic_dic.items():</span><br><span class="line">            <span class="comment"># 还是计算每个再本文档中的词  和文档的相识度</span></span><br><span class="line">            <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> word_list:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment">#</span></span><br><span class="line">            sim = calsim(v, senttopic)</span><br><span class="line">            sim_dic[k] = sim</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> sorted(sim_dic.items(), key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:self.keyword_num]:</span><br><span class="line">            print(k, v)</span><br><span class="line">        print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topic_extract</span><span class="params">(word_list, model, pos=False, keyword_num=<span class="number">10</span>)</span>:</span></span><br><span class="line">    doc_list = load_data(pos)</span><br><span class="line">    topic_model = TopicModel(doc_list, keyword_num, model=model)</span><br><span class="line">    topic_model.get_simword(word_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textrank_extract</span><span class="params">(text, pos=False, keyword_num=<span class="number">10</span>)</span>:</span></span><br><span class="line">    textrank = analyse.textrank</span><br><span class="line">    keywords = textrank(text, keyword_num)</span><br><span class="line">    <span class="comment"># 输出抽取出的关键词</span></span><br><span class="line">    <span class="keyword">for</span> keyword <span class="keyword">in</span> keywords:</span><br><span class="line">        print(keyword + <span class="string">"/ "</span>, end=<span class="string">''</span>)</span><br><span class="line">    print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># test</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    text = <span class="string">'6月19日,《2012年度“中国爱心城市”公益活动新闻发布会》在京举行。'</span> + \</span><br><span class="line">           <span class="string">'中华社会救助基金会理事长许嘉璐到会讲话。基金会高级顾问朱发忠,全国老龄'</span> + \</span><br><span class="line">           <span class="string">'办副主任朱勇,民政部社会救助司助理巡视员周萍,中华社会救助基金会副理事长耿志远,'</span> + \</span><br><span class="line">           <span class="string">'重庆市民政局巡视员谭明政。晋江市人大常委会主任陈健倩,以及10余个省、市、自治区民政局'</span> + \</span><br><span class="line">           <span class="string">'领导及四十多家媒体参加了发布会。中华社会救助基金会秘书长时正新介绍本年度“中国爱心城'</span> + \</span><br><span class="line">           <span class="string">'市”公益活动将以“爱心城市宣传、孤老关爱救助项目及第二届中国爱心城市大会”为主要内容,重庆市'</span> + \</span><br><span class="line">           <span class="string">'、呼和浩特市、长沙市、太原市、蚌埠市、南昌市、汕头市、沧州市、晋江市及遵化市将会积极参加'</span> + \</span><br><span class="line">           <span class="string">'这一公益活动。中国雅虎副总编张银生和凤凰网城市频道总监赵耀分别以各自媒体优势介绍了活动'</span> + \</span><br><span class="line">           <span class="string">'的宣传方案。会上,中华社会救助基金会与“第二届中国爱心城市大会”承办方晋江市签约,许嘉璐理'</span> + \</span><br><span class="line">           <span class="string">'事长接受晋江市参与“百万孤老关爱行动”向国家重点扶贫地区捐赠的价值400万元的款物。晋江市人大'</span> + \</span><br><span class="line">           <span class="string">'常委会主任陈健倩介绍了大会的筹备情况。'</span></span><br><span class="line"></span><br><span class="line">    pos = <span class="literal">False</span></span><br><span class="line">    seg_list = seg_to_list(text, pos)</span><br><span class="line">    filter_list = word_filter(seg_list, pos)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'LDA模型结果：'</span>)</span><br><span class="line">    topic_extract(filter_list, <span class="string">'LDA'</span>, pos)</span><br></pre></td></tr></table></figure>
<h3 id="lda-bu-zou-zong-jie">LDA 步骤总结</h3>
<p>数据集处理</p>
<ol>
<li>先构建词空间  Dictionary(4064 unique tokens: [‘上将’, ‘专门’, ‘乘客’, ‘仪式’, ‘体验’]…)</li>
<li>使用BOW模型向量化   corpus [[(0, 1), (1, 1), (2, 2), (3, 1),。。</li>
<li>对每个词，根据tf-idf进行加权，得到加权后的向量表示</li>
</ol>
<p>根据数据集获得模型</p>
<ol start="4">
<li>得到数据集的主题-词分布  model (得到每个词的向量）（文档转列表 再转集合去重，再转列表）{‘白血病’: [(0, 0.1273009), (1, 0.6181468), (2, 0.12732704), (3, 0.12722531)], ‘婴儿’: [。。。</li>
<li>求文档的分布:词》向量》tf/idf加权》同第4步得到文档的分布向量 [(0, 0.033984687), (1, 0.033736005), (2, 0.8978361), (3, 0.03444325)]</li>
<li>计算余弦距离得到结果</li>
</ol>
<h2 id="zong-jie">总结</h2>
<ol>
<li>
<p>为什么要用浅层语义分析？</p>
<p>我觉着一方面是考虑文本的语义信息。还有就是用词袋模型所表示的单词-文本矩阵一方面存在数据稀疏的问题，另一方面就是词本身一词多义和多词一义现象在进行文本相似度计算的时候未必能够准确的表达两个文本的语义相似度。</p>
</li>
<li>
<p>潜在语义分析算法</p>
<ol>
<li>矩阵奇异值分解算法</li>
<li>非负矩阵分解算法</li>
</ol>
</li>
</ol>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://www.cnblogs.com/jpcflyer/p/11180263.html" target="_blank" rel="noopener">机器学习经典算法之PageRank</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/41091116" target="_blank" rel="noopener">通俗易懂理解——TF-IDF与TextRank</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>key word extraction</tag>
      </tags>
  </entry>
  <entry>
    <title>循环神经网络</title>
    <url>/2020/04/18/deeplearning/rnn/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/04/18/deeplearning/rnn/6.8_lstm_3.svg" alt></p>
<a id="more"></a>
<h1 id="yu-yan-mo-xing">语言模型</h1>
<p>语言模型（language model）是自然语言处理的重要技术。自然语言处理中最常见的数据是文本数据。我们可以把一段自然语言文本看作一段离散的时间序列。假设一段长度为\(T\)的文本中的词依次为\(w_1, w_2, \ldots, w_T\)，那么在离散的时间序列中，\(w_t\)（\(1 \leq t \leq T\)）可看作在时间步\(t\)的输出或标签。给定一个长度为\(T\)的词的序列\(w_1, w_2, \ldots, w_T\)，语言模型将计算该序列的概率：</p>
<p>\[
P(w_1, w_2, \ldots, w_T)
\]</p>
<p>语言模型可用于提升语音识别和机器翻译的性能。例如，在语音识别中，给定一段“厨房里食油用完了”的语音，有可能会输出“厨房里食油用完了”和“厨房里石油用完了”这两个读音完全一样的文本序列。如果语言模型判断出前者的概率大于后者的概率，我们就可以根据相同读音的语音输出“厨房里食油用完了”的文本序列。在机器翻译中，如果对英文“you go first”逐词翻译成中文的话，可能得到“你走先”“你先走”等排列方式的文本序列。如果语言模型判断出“你先走”的概率大于其他排列方式的文本序列的概率，我们就可以把“you go first”翻译成“你先走”。</p>
<h2 id="yu-yan-mo-xing-de-ji-suan">语言模型的计算</h2>
<p>既然语言模型很有用，那该如何计算它呢？假设序列\(w_1, w_2, \ldots, w_T\)中的每个词是依次生成的，有</p>
<p>\[
P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^T P(w_t \mid w_1, \ldots, w_{t-1})
\]</p>
<p>例如，一段含有4个词的文本序列的概率</p>
<p>\[
P(w_1, w_2, w_3, w_4) =  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_1, w_2, w_3)
\]</p>
<p>为了计算语言模型，我们需要计算词的概率，以及一个词在给定前几个词的情况下的条件概率，即语言模型参数。设训练数据集为一个大型文本语料库，如维基百科的所有条目。词的概率可以通过该词在训练数据集中的相对词频来计算。例如，\(P(w_1)\)可以计算为\(w_1\)在训练数据集中的词频（词出现的次数）与训练数据集的总词数之比。因此，根据条件概率定义，一个词在给定前几个词的情况下的条件概率也可以通过训练数据集中的相对词频计算。例如，\(P(w_2 \mid w_1)\)可以计算为\(w_1, w_2\)两词相邻的频率与\(w_1\)词频的比值，因为该比值即\(P(w_1, w_2)\)与\(P(w_1)\)之比；而\(P(w_3 \mid w_1, w_2)\)同理可以计算为\(w_1\)、\(w_2\)和\(w_3\)三词相邻的频率与\(w_1\)和\(w_2\)两词相邻的频率的比值。以此类推。</p>
<h2 id="n-yuan-yu-fa">n元语法</h2>
<p>当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。\(n\)元语法通过马尔可夫假设（虽然并不一定成立）简化了语言模型的计算。这里的马尔可夫假设是指一个词的出现只与前面\(n\)个词相关，即\(n\)阶马尔可夫链（Markov chain of order \(n\)）。如果\(n=1\)，那么有\(P(w_3 \mid w_1, w_2) = P(w_3 \mid w_2)\)。如果基于\(n-1\)阶马尔可夫链，我们可以将语言模型改写为</p>
<p>\[
P(w_1, w_2, \ldots, w_T) \approx \prod_{t=1}^T P(w_t \mid w_{t-(n-1)}, \ldots, w_{t-1})
\]</p>
<p>以上也叫\(n\)元语法（\(n\)-grams）。它是基于\(n - 1\)阶马尔可夫链的概率语言模型。当\(n\)分别为1、2和3时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法（trigram）。例如，长度为4的序列\(w_1, w_2, w_3, w_4\)在一元语法、二元语法和三元语法中的概率分别为</p>
<p>\[
\begin{aligned}
P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2) P(w_3) P(w_4) ,\\
P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_2) P(w_4 \mid w_3) ,\\
P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_2, w_3) .
\end{aligned}
\]</p>
<p>当\(n\)较小时，\(n\)元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。然而，当\(n\)较大时，\(n\)元语法需要计算并存储大量的词频和多词相邻频率。</p>
<h1 id="xun-huan-shen-jing-wang-luo">循环神经网络</h1>
<p>在\(n\)元语法中，时间步\(t\)的词\(w_t\)基于前面所有词的条件概率只考虑了最近时间步的\(n-1\)个词。如果要考虑比\(t-(n-1)\)更早时间步的词对\(w_t\)的可能影响，需要增大\(n\)。但这样模型参数的数量将随之呈指数级增长。</p>
<p>循环神经网络并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。首先回忆一下多层感知机，然后描述如何添加隐藏状态来将它变成循环神经网络。</p>
<h2 id="bu-han-yin-cang-zhuang-tai-de-shen-jing-wang-luo">不含隐藏状态的神经网络</h2>
<p>考虑一个含单隐藏层的多层感知机。给定样本数为\(n\)、输入个数（特征数或特征向量维度）为\(d\)的小批量数据样本\(\boldsymbol{X} \in \mathbb{R}^{n \times d}\)。设隐藏层的激活函数为\(\phi\)，那么隐藏层的输出\(\boldsymbol{H} \in \mathbb{R}^{n \times h}\)计算为</p>
<p>\[
\boldsymbol{H} = \phi(\boldsymbol{X} \boldsymbol{W}_{xh} + \boldsymbol{b}_h)
\]</p>
<p>其中隐藏层权重参数\(\boldsymbol{W}_{xh} \in \mathbb{R}^{d \times h}\)，隐藏层偏差参数 \(\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}\)，\(h\)为隐藏单元个数。上式相加的两项形状不同，因此将按照广播机制相加。把隐藏变量\(\boldsymbol{H}\)作为输出层的输入，且设输出个数为\(q\)（如分类问题中的类别数），输出层的输出为</p>
<p>\[
\boldsymbol{O} = \boldsymbol{H} \boldsymbol{W}_{hq} + \boldsymbol{b}_q
\]</p>
<p>其中输出变量\(\boldsymbol{O} \in \mathbb{R}^{n \times q}\), 输出层权重参数\(\boldsymbol{W}_{hq} \in \mathbb{R}^{h \times q}\), 输出层偏差参数\(\boldsymbol{b}_q \in \mathbb{R}^{1 \times q}\)。如果是分类问题，可以使用\(\text{softmax}(\boldsymbol{O})\)来计算输出类别的概率分布。</p>
<h2 id="han-yin-cang-zhuang-tai-de-xun-huan-shen-jing-wang-luo">含隐藏状态的循环神经网络</h2>
<p>考虑输入数据存在时间相关性的情况。假设\(\boldsymbol{X}_t \in \mathbb{R}^{n \times d}\)是序列中时间步\(t\)的小批量输入，\(\boldsymbol{H}_t  \in \mathbb{R}^{n \times h}\)是该时间步的隐藏变量。与多层感知机不同的是，这里保存上一时间步的隐藏变量\(\boldsymbol{H}_{t-1}\)，并引入一个新的权重参数\(\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}\)，该参数用来描述在当前时间步如何使用上一时间步的隐藏变量。具体来说，时间步\(t\)的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：</p>
<p>\[
\boldsymbol{H}_t = \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}  + \boldsymbol{b}_h)
\]</p>
<p>与多层感知机相比，在这里添加了\(\boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}\)一项。由上式中相邻时间步的隐藏变量\(\boldsymbol{H}_t\)和\(\boldsymbol{H}_{t-1}\)之间的关系可知，这里的隐藏变量能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。因此，该隐藏变量也称为隐藏状态。由于隐藏状态在当前时间步的定义使用了上一时间步的隐藏状态，上式的计算是循环的。使用循环计算的网络即循环神经网络。</p>
<p>循环神经网络有很多种不同的构造方法。含上式所定义的隐藏状态的循环神经网络是极为常见的一种。在时间步\(t\)，输出层的输出和多层感知机中的计算类似：</p>
<p>\[
\boldsymbol{O}_t = \boldsymbol{H}_t \boldsymbol{W}_{hq} + \boldsymbol{b}_q
\]</p>
<p>循环神经网络的参数包括隐藏层的权重\(\boldsymbol{W}_{xh} \in \mathbb{R}^{d \times h}\)、\(\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}\)和偏差 \(\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}\)，以及输出层的权重\(\boldsymbol{W}_{hq} \in \mathbb{R}^{h \times q}\)和偏差\(\boldsymbol{b}_q \in \mathbb{R}^{1 \times q}\)。值得一提的是，即便在不同时间步，循环神经网络也始终使用这些模型参数。因此，循环神经网络模型参数的数量不随时间步的增加而增长。</p>
<p>下图展示了循环神经网络在3个相邻时间步的计算逻辑。在时间步\(t\)，隐藏状态的计算可以看成是将输入\(\boldsymbol{X}_t\)和前一时间步隐藏状态\(\boldsymbol{H}_{t-1}\)连结后输入一个激活函数为\(\phi\)的全连接层。该全连接层的输出就是当前时间步的隐藏状态\(\boldsymbol{H}_t\)，且模型参数为\(\boldsymbol{W}_{xh}\)与\(\boldsymbol{W}_{hh}\)的连结，偏差为\(\boldsymbol{b}_h\)。当前时间步\(t\)的隐藏状态\(\boldsymbol{H}_t\)将参与下一个时间步\(t+1\)的隐藏状态\(\boldsymbol{H}_{t+1}\)的计算，并输入到当前时间步的全连接输出层。</p>
<p><img src="/2020/04/18/deeplearning/rnn/6.2_rnn.svg" alt="含隐藏状态的循环神经网络"></p>
<p>刚刚提到，隐藏状态中\(\boldsymbol{X}_t \boldsymbol{W}_{xh} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}\)的计算等价于\(\boldsymbol{X}_t\)与\(\boldsymbol{H}_{t-1}\)连结后的矩阵乘以\(\boldsymbol{W}_{xh}\)与\(\boldsymbol{W}_{hh}\)连结后的矩阵。接下来，用一个具体的例子来验证这一点。首先，构造矩阵\(X\)、\(W_{xh}\)、\(H\)和\(W_{hh}\)，它们的形状分别为(3, 1)、(1, 4)、(3, 4)和(4, 4)。将\(X\)与\(W_xh\)、\(H\)与\(W_hh\)分别相乘，再把两个乘法运算的结果相加，得到形状为(3, 4)的矩阵。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X, W_xh = tf.random.normal(shape=(<span class="number">3</span>, <span class="number">1</span>)), tf.random.normal(shape=(<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">H, W_hh = tf.random.normal(shape=(<span class="number">3</span>, <span class="number">4</span>)), tf.random.normal(shape=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">tf.matmul(X, W_xh) + tf.matmul(H, W_hh)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: id=<span class="number">219</span>, shape=(<span class="number">3</span>, <span class="number">4</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[<span class="number">-0.5083256</span> , <span class="number">-0.8676372</span> , <span class="number">-0.04281294</span>, <span class="number">-1.9311084</span> ],</span><br><span class="line">       [ <span class="number">1.3117619</span> , <span class="number">-0.08776671</span>,  <span class="number">0.5579821</span> ,  <span class="number">1.4077363</span> ],</span><br><span class="line">       [ <span class="number">0.18361318</span>,  <span class="number">0.94494146</span>, <span class="number">-0.31174743</span>,  <span class="number">1.3439546</span> ]],</span><br><span class="line">      dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure>
<p>将矩阵<code>X</code>和<code>H</code>按列（维度1）连结，连结后的矩阵形状为(3, 5)。可见，连结后矩阵在维度1的长度为矩阵<code>X</code>和<code>H</code>在维度1的长度之和（\(1+4\)）。然后，将矩阵<code>W_xh</code>和<code>W_hh</code>按行（维度0）连结，连结后的矩阵形状为(5, 4)。最后将两个连结后的矩阵相乘，得到与上面代码输出相同的形状为(3, 4)的矩阵。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.matmul(tf.concat([X,H],axis=<span class="number">1</span>),tf.concat([W_xh,W_hh],axis=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: id=<span class="number">226</span>, shape=(<span class="number">3</span>, <span class="number">4</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[<span class="number">-0.5083256</span> , <span class="number">-0.8676373</span> , <span class="number">-0.04281296</span>, <span class="number">-1.9311084</span> ],</span><br><span class="line">       [ <span class="number">1.3117617</span> , <span class="number">-0.08776676</span>,  <span class="number">0.5579822</span> ,  <span class="number">1.4077362</span> ],</span><br><span class="line">       [ <span class="number">0.18361317</span>,  <span class="number">0.94494146</span>, <span class="number">-0.3117473</span> ,  <span class="number">1.3439546</span> ]],</span><br><span class="line">      dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="ying-yong-ji-yu-zi-fu-ji-xun-huan-shen-jing-wang-luo-de-yu-yan-mo-xing">应用：基于字符级循环神经网络的语言模型</h2>
<p>如何应用循环神经网络来构建一个语言模型?设小批量中样本数为1，文本序列为“想”“要”“有”“直”“升”“机”。下图演示了如何使用循环神经网络基于当前和过去的字符来预测下一个字符。在训练时，我们对每个时间步的输出层输出使用softmax运算，然后使用交叉熵损失函数来计算它与标签的误差。在下图中，由于隐藏层中隐藏状态的循环计算，时间步3的输出\(\boldsymbol{O}_3\)取决于文本序列“想”“要”“有”。 由于训练数据中该序列的下一个词为“直”，时间步3的损失将取决于该时间步基于序列“想”“要”“有”生成下一个词的概率分布与该时间步的标签“直”。</p>
<p><img src="/2020/04/18/deeplearning/rnn/6.2_rnn-train.svg" alt="基于字符级循环神经网络的语言模型"></p>
<p>因为每个输入词是一个字符，因此这个模型被称为字符级循环神经网络。因为不同字符的个数远小于不同词的个数（对于英文尤其如此），所以字符级循环神经网络的计算通常更加简单。</p>
<h2 id="dai-ma-shi-xian">代码实现</h2>
<h3 id="chu-shi-hua-mo-xing-can-shu">初始化模型参数</h3>
<p>接下来，初始化模型参数。隐藏单元个数 <code>num_hiddens</code>是一个超参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.Variable(tf.random.normal(shape=shape,stddev=<span class="number">0.01</span>,mean=<span class="number">0</span>,dtype=tf.float32))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    W_xh = _one((num_inputs, num_hiddens))</span><br><span class="line">    W_hh = _one((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = tf.Variable(tf.zeros(num_hiddens), dtype=tf.float32)</span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = tf.Variable(tf.zeros(num_outputs), dtype=tf.float32)</span><br><span class="line">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing">定义模型</h3>
<p>根据循环神经网络的计算表达式实现该模型。首先定义<code>init_rnn_state</code>函数来返回初始化的隐藏状态。它返回由一个形状为(批量大小, 隐藏单元个数)的值为0的<code>NDArray</code>组成的元组。使用元组是为了更便于处理隐藏状态含有多个<code>NDArray</code>的情况。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rnn_state</span><span class="params">(batch_size, num_hiddens)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (tf.zeros(shape=(batch_size, num_hiddens)), )</span><br></pre></td></tr></table></figure>
<p>下面的<code>rnn</code>函数定义了在一个时间步里如何计算隐藏状态和输出。这里的激活函数使用了tanh函数。当元素在实数域上均匀分布时，tanh函数值的均值为0。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn</span><span class="params">(inputs, state, params)</span>:</span></span><br><span class="line">    <span class="comment"># inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        X=tf.reshape(X,[<span class="number">-1</span>,W_xh.shape[<span class="number">0</span>]])</span><br><span class="line">        H = tf.tanh(tf.matmul(X, W_xh) + tf.matmul(H, W_hh) + b_h)</span><br><span class="line">        Y = tf.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H,)</span><br></pre></td></tr></table></figure>
<h2 id="tensorflow-shi-xian">tensorflow实现</h2>
<h3 id="ding-yi-mo-xing-1">定义模型</h3>
<p>Keras的<code>Rnn</code>模块提供了循环神经网络的实现。下面构造一个含单隐藏层、隐藏单元个数为256的循环神经网络层<code>rnn_layer</code>，并对权重做初始化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line">cell=keras.layers.SimpleRNNCell(num_hiddens,kernel_initializer=<span class="string">'glorot_uniform'</span>)</span><br><span class="line">rnn_layer = keras.layers.RNN(cell,time_major=<span class="literal">True</span>,return_sequences=<span class="literal">True</span>,return_state=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>这里<code>rnn_layer</code>的输入形状为(时间步数, 批量大小, 输入个数)。<code>rnn_layer</code>作为<code>nn.RNN</code>实例，在前向计算后会分别返回输出和隐藏状态h，其中输出指的是隐藏层在<strong>各个时间步</strong>上计算并输出的隐藏状态，它们通常作为后续输出层的输入。需要强调的是，该“输出”本身并不涉及输出层计算，形状为(时间步数, 批量大小, 隐藏单元个数)。而<code>nn.RNN</code>实例在前向计算返回的隐藏状态指的是隐藏层在<strong>最后时间步</strong>的隐藏状态：当隐藏层有多层时，每一层的隐藏状态都会记录在该变量中。</p>
<p>接下来继承<code>Module</code>类来定义一个完整的循环神经网络。它首先将输入数据使用one-hot向量表示后输入到<code>rnn_layer</code>中，然后使用全连接输出层得到输出。输出个数等于词典大小<code>vocab_size</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 本类已保存在d2lzh包中方便以后使用</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span><span class="params">(keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_layer, vocab_size, **kwargs)</span>:</span></span><br><span class="line">        super(RNNModel, self).__init__(**kwargs)</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.dense = keras.layers.Dense(vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, state)</span>:</span></span><br><span class="line">        <span class="comment"># 将输入转置成(num_steps, batch_size)后获取one-hot向量表示</span></span><br><span class="line">        X = tf.one_hot(tf.transpose(inputs), self.vocab_size)</span><br><span class="line">        Y,state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># 全连接层会首先将Y的形状变成(num_steps * batch_size, num_hiddens)，它的输出</span></span><br><span class="line">        <span class="comment"># 形状为(num_steps * batch_size, vocab_size)</span></span><br><span class="line">        output = self.dense(tf.reshape(Y,(<span class="number">-1</span>, Y.shape[<span class="number">-1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_initial_state</span><span class="params">(self, *args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.rnn.cell.get_initial_state(*args, **kwargs)</span><br></pre></td></tr></table></figure>
<h2 id="cai-jian-ti-du">裁剪梯度</h2>
<p>循环神经网络中较容易出现梯度衰减或梯度爆炸。为了应对梯度爆炸，可以裁剪梯度。假设把所有模型参数梯度的元素拼接成一个向量 \(\boldsymbol{g}\)，并设裁剪的阈值是\(\theta\)。裁剪后的梯度</p>
<p>\[
\min\left(\frac{\theta}{\|\boldsymbol{g}\|}, 1\right)\boldsymbol{g}
\]</p>
<p>的\(L_2\)范数不超过\(\theta\)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算裁剪后的梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span><span class="params">(grads,theta)</span>:</span></span><br><span class="line">    norm = np.array([<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(grads)):</span><br><span class="line">        norm+=tf.math.reduce_sum(grads[i] ** <span class="number">2</span>)</span><br><span class="line">    <span class="comment">#print("norm",norm)</span></span><br><span class="line">    norm = np.sqrt(norm).item()</span><br><span class="line">    new_gradient=[]</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> grad <span class="keyword">in</span> grads:</span><br><span class="line">            new_gradient.append(grad * theta / norm)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> grad <span class="keyword">in</span> grads:</span><br><span class="line">            new_gradient.append(grad)  </span><br><span class="line">    <span class="comment">#print("new_gradient",new_gradient)</span></span><br><span class="line">    <span class="keyword">return</span> new_gradient</span><br></pre></td></tr></table></figure>
<p>然而由于在优化时（训练RNN时），梯度无法准确合理的传到很靠前的时间点，因此RNN实际上只能记住并不长的序列信息因此RNN相比前馈网络，可以记住的历史信息要长一些，但是无法记住长距离的信息。</p>
<h2 id="kun-huo-du">困惑度</h2>
<p>困惑度是交叉熵的指数形式。</p>
<p>将一个sentence看做一个随机变量，\(W=\left\{w_{0}, w_{1}, \ldots, w_{n}\right\}\),这里假定是有限长度n，那么它对应的熵为：<br>
\[
H\left(w_{1}, w_{2}, \ldots, w_{n}\right)=-\log _{w_{1}^{n} \in L} p\left(w_{1}^{n}\right) \log p\left(w_{1}^{n}\right)
\]<br>
对应的per-word 熵，也就是entroy rate为：<br>
\[
\frac{1}{n} H\left(w_{1}, w_{2}, \ldots, w_{n}\right)=-\frac{1}{n} \log _{w_{1}^{n} \in L} p\left(w_{1}^{n}\right) \log p\left(w_{1}^{n}\right)
\]<br>
引入交叉熵，真实分布<code>p</code>，通过语言模型生成的<code>sentence</code>，可以看作预测得到分布为<code>m</code>，那么交叉熵：<br>
\[
H(p, m)=-\frac{1}{n} \sum_{W \in L} p\left(w_{1}, w_{2}, \ldots, w_{n}\right) \log m\left(w_{1}, w_{2}, \ldots, w_{n}\right)
\]<br>
交叉熵的值越小，我们预测得到的分布也就越接近真实分布。因为训练样本已经给出，也就是p是定值，故 \(H(p,m)\) 可写成：<br>
\[
H(W)=-\frac{1}{N} \log P\left(w_{1} w_{2} \cdots w_{N}\right)
\]<br>
基于语言模型 \(M=P\left(w_{i} \mid w_{i-N+1} \cdots w_{i-1}\right)\)的困惑度perplexity可定义为交叉熵的指数形式：<br>
\[
\begin{aligned}
\operatorname{Perplexity}(W) &amp;=2^{H(W)} \\
&amp;=P\left(w_{1} w_{2} \ldots w_{N}\right)^{-\frac{1}{N}} \\
&amp;=\sqrt[N]{\frac{1}{P\left(w_{1} w_{2} \ldots w_{N}\right)}} \\
&amp;=\sqrt[N]{\prod_{i=1}^{N} \frac{1}{P\left(w_{i} \mid w_{1} \ldots w_{i-1}\right)}}
\end{aligned}
\]</p>
<p>由公式可知，<strong>句子概率越大，语言模型越好，困惑度越小。</strong></p>
<p>通常使用困惑度来评价语言模型的好坏。特别地，</p>
<ul>
<li>最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；</li>
<li>最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；</li>
<li>基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。</li>
</ul>
<p>显然，任何一个有效模型的困惑度必须小于类别个数。</p>
<h1 id="tong-guo-shi-jian-fan-xiang-chuan-bo">通过时间反向传播</h1>
<p>对于rnn来说，如果不裁剪梯度，模型将无法正常训练。为了深刻理解这一现象，下面介绍循环神经网络中梯度的计算和存储方法，即通过时间反向传播（back-propagation through time）。</p>
<p>正向传播和反向传播相互依赖，正向传播在循环神经网络中比较直观，而通过时间反向传播其实是反向传播在循环神经网络中的具体应用。需要将循环神经网络按时间步展开，从而得到模型变量和参数之间的依赖关系，并依据链式法则应用反向传播计算并存储梯度。</p>
<h2 id="ding-yi-mo-xing-2">定义模型</h2>
<p>简单起见，考虑一个无偏差项的循环神经网络，且激活函数为恒等映射（\(\phi(x)=x\)）。设时间步 \(t\) 的输入为单样本 \(\boldsymbol{x}_t \in \mathbb{R}^d\)，标签为 \(y_t\)，那么隐藏状态 \(\boldsymbol{h}_t \in \mathbb{R}^h\)的计算表达式为</p>
<p>\[
\boldsymbol{h}_t = \boldsymbol{W}_{hx} \boldsymbol{x}_t + \boldsymbol{W}_{hh} \boldsymbol{h}_{t-1},
\]</p>
<p>其中\(\boldsymbol{W}_{hx} \in \mathbb{R}^{h \times d}\)和\(\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}\)是隐藏层权重参数。设输出层权重参数\(\boldsymbol{W}_{qh} \in \mathbb{R}^{q \times h}\)，时间步\(t\)的输出层变量\(\boldsymbol{o}_t \in \mathbb{R}^q\)计算为</p>
<p>\[
\boldsymbol{o}_t = \boldsymbol{W}_{qh} \boldsymbol{h}_{t}.
\]</p>
<p>设时间步\(t\)的损失为\(\ell(\boldsymbol{o}_t, y_t)\)。时间步数为\(T\)的损失函数\(L\)定义为</p>
<p>\[
L = \frac{1}{T} \sum_{t=1}^T \ell (\boldsymbol{o}_t, y_t).
\]</p>
<p>将\(L\)称为有关给定时间步的数据样本的目标函数。</p>
<h2 id="mo-xing-ji-suan-tu">模型计算图</h2>
<p>为了可视化循环神经网络中模型变量和参数在计算中的依赖关系，可以绘制模型计算图，如图所示。例如，时间步3的隐藏状态\(\boldsymbol{h}_3\)的计算依赖模型参数\(\boldsymbol{W}_{hx}\)、\(\boldsymbol{W}_{hh}\)、上一时间步隐藏状态\(\boldsymbol{h}_2\)以及当前时间步输入\(\boldsymbol{x}_3\)。</p>
<p><img src="/2020/04/18/deeplearning/rnn/6.6_rnn-bptt.svg" alt="时间步数为3的循环神经网络模型计算中的依赖关系。方框代表变量（无阴影）或参数（有阴影），圆圈代表运算符"></p>
<h2 id="fang-fa">方法</h2>
<p>刚刚提到，图中的模型的参数是 \(\boldsymbol{W}_{hx}\), \(\boldsymbol{W}_{hh}\) 和 \(\boldsymbol{W}_{qh}\)，训练模型通常需要模型参数的梯度\(\partial L/\partial \boldsymbol{W}_{hx}\)、\(\partial L/\partial \boldsymbol{W}_{hh}\)和\(\partial L/\partial \boldsymbol{W}_{qh}\)。<br>
根据上图中的依赖关系，可以按照其中箭头所指的反方向依次计算并存储梯度。为了表述方便，采用链式法则的运算符prod。</p>
<p>首先，目标函数有关各时间步输出层变量的梯度\(\partial L/\partial \boldsymbol{o}_t \in \mathbb{R}^q\)很容易计算：</p>
<p>\[
\frac{\partial L}{\partial \boldsymbol{o}_t} =  \frac{\partial \ell (\boldsymbol{o}_t, y_t)}{T \cdot \partial \boldsymbol{o}_t}
\]</p>
<p>下面，可以计算目标函数有关模型参数\(\boldsymbol{W}_{qh}\)的梯度\(\partial L/\partial \boldsymbol{W}_{qh} \in \mathbb{R}^{q \times h}\)。根据图6.3，\(L\)通过\(\boldsymbol{o}_1, \ldots, \boldsymbol{o}_T\)依赖\(\boldsymbol{W}_{qh}\)。依据链式法则，</p>
<p>\[
\frac{\partial L}{\partial \boldsymbol{W}_{qh}} 
= \sum_{t=1}^T \text{prod}\left(\frac{\partial L}{\partial \boldsymbol{o}_t}, \frac{\partial \boldsymbol{o}_t}{\partial \boldsymbol{W}_{qh}}\right) 
= \sum_{t=1}^T \frac{\partial L}{\partial \boldsymbol{o}_t} \boldsymbol{h}_t^\top.
\]</p>
<p>其次，注意到隐藏状态之间也存在依赖关系。\(L\)只通过\(\boldsymbol{o}_T\)依赖最终时间步\(T\)的隐藏状态\(\boldsymbol{h}_T\)。因此，先计算目标函数有关最终时间步隐藏状态的梯度\(\partial L/\partial \boldsymbol{h}_T \in \mathbb{R}^h\)。依据链式法则，得到<br>
\[
\frac{\partial L}{\partial \boldsymbol{h}_T} = \text{prod}\left(\frac{\partial L}{\partial \boldsymbol{o}_T}, \frac{\partial \boldsymbol{o}_T}{\partial \boldsymbol{h}_T} \right) = \boldsymbol{W}_{qh}^\top \frac{\partial L}{\partial \boldsymbol{o}_T}.
\]</p>
<p>接下来对于时间步\(t &lt; T\), 在上图中，\(L\)通过\(\boldsymbol{h}_{t+1}\)和\(\boldsymbol{o}_t\)依赖\(\boldsymbol{h}_t\)。依据链式法则，<br>
目标函数有关时间步\(t &lt; T\)的隐藏状态的梯度\(\partial L/\partial \boldsymbol{h}_t \in \mathbb{R}^h\)需要按照时间步从大到小依次计算：<br>
\[
\frac{\partial L}{\partial \boldsymbol{h}_t} 
= \text{prod} (\frac{\partial L}{\partial \boldsymbol{h}_{t+1}}, \frac{\partial \boldsymbol{h}_{t+1}}{\partial \boldsymbol{h}_t}) + \text{prod} (\frac{\partial L}{\partial \boldsymbol{o}_t}, \frac{\partial \boldsymbol{o}_t}{\partial \boldsymbol{h}_t} ) = \boldsymbol{W}_{hh}^\top \frac{\partial L}{\partial \boldsymbol{h}_{t+1}} + \boldsymbol{W}_{qh}^\top \frac{\partial L}{\partial \boldsymbol{o}_t}
\]</p>
<p>将上面的递归公式展开，对任意时间步\(1 \leq t \leq T\)，可以得到目标函数有关隐藏状态梯度的通项公式</p>
<p>\[
\frac{\partial L}{\partial \boldsymbol{h}_t} 
= \sum_{i=t}^T {\left(\boldsymbol{W}_{hh}^\top\right)}^{T-i} \boldsymbol{W}_{qh}^\top \frac{\partial L}{\partial \boldsymbol{o}_{T+t-i}}.
\]</p>
<p>由上式中的指数项可见，当时间步数 \(T\) 较大或者时间步 \(t\) 较小时，目标函数有关隐藏状态的梯度较容易出现衰减和爆炸。这也会影响其他包含\(\partial L / \partial \boldsymbol{h}_t\)项的梯度，例如隐藏层中模型参数的梯度\(\partial L / \partial \boldsymbol{W}_{hx} \in \mathbb{R}^{h \times d}\)和\(\partial L / \partial \boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}\)。<br>
在中，\(L\)通过\(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T\)依赖这些模型参数。<br>
依据链式法则，我们有<br>
\[
\begin{aligned}
\frac{\partial L}{\partial \boldsymbol{W}_{hx}} 
&amp;= \sum_{t=1}^T \text{prod}\left(\frac{\partial L}{\partial \boldsymbol{h}_t}, \frac{\partial \boldsymbol{h}_t}{\partial \boldsymbol{W}_{hx}}\right) 
= \sum_{t=1}^T \frac{\partial L}{\partial \boldsymbol{h}_t} \boldsymbol{x}_t^\top,\\
\frac{\partial L}{\partial \boldsymbol{W}_{hh}} 
&amp;= \sum_{t=1}^T \text{prod}\left(\frac{\partial L}{\partial \boldsymbol{h}_t}, \frac{\partial \boldsymbol{h}_t}{\partial \boldsymbol{W}_{hh}}\right) 
= \sum_{t=1}^T \frac{\partial L}{\partial \boldsymbol{h}_t} \boldsymbol{h}_{t-1}^\top.
\end{aligned}
\]</p>
<p>每次迭代中，在依次计算完以上各个梯度后，会将它们存储起来，从而避免重复计算。例如，由于隐藏状态梯度\(\partial L/\partial \boldsymbol{h}_t\)被计算和存储，之后的模型参数梯度\(\partial L/\partial  \boldsymbol{W}_{hx}\)和\(\partial L/\partial \boldsymbol{W}_{hh}\)的计算可以直接读取\(\partial L/\partial \boldsymbol{h}_t\)的值，而无须重复计算它们。此外，反向传播中的梯度计算可能会依赖变量的当前值。它们正是通过正向传播计算出来的。<br>
举例来说，参数梯度\(\partial L/\partial \boldsymbol{W}_{hh}\)的计算需要依赖隐藏状态在时间步\(t = 0, \ldots, T-1\)的当前值\(\boldsymbol{h}_t\)（\(\boldsymbol{h}_0\)是初始化得到的）。这些值是通过从输入层到输出层的正向传播计算并存储得到的。</p>
<h1 id="men-kong-xun-huan-dan-yuan-gru">门控循环单元（GRU）</h1>
<p>当时间步数较大或者时间步较小时，循环神经网络的梯度较容易出现衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但无法解决梯度衰减的问题。通常由于这个原因，循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系。</p>
<p>门控循环神经网络的提出，正是为了更好地捕捉时间序列中时间步距离较大的依赖关系。它通过可以学习的门来控制信息的流动。其中，门控循环单元（GRU）是一种常用的门控循环神经网络。</p>
<h2 id="men-kong-xun-huan-dan-yuan">门控循环单元</h2>
<p>下面将介绍门控循环单元的设计。它引入了重置门和更新门的概念，从而修改了循环神经网络中隐藏状态的计算方式。</p>
<h3 id="zhong-zhi-men-he-geng-xin-men">重置门和更新门</h3>
<p>如图所示，门控循环单元中的重置门和更新门的输入均为当前时间步输入\(\boldsymbol{X}_t\)与上一时间步隐藏状态\(\boldsymbol{H}_{t-1}\)，输出由激活函数为sigmoid函数的全连接层计算得到。</p>
<p><img src="/2020/04/18/deeplearning/rnn/6.7_gru_1.svg" alt="门控循环单元中重置门和更新门的计算"></p>
<p>具体来说，假设隐藏单元个数为\(h\)，给定时间步\(t\)的小批量输入\(\boldsymbol{X}_t \in \mathbb{R}^{n \times d}\)（样本数为\(n\)，输入个数为\(d\)）和上一时间步隐藏状态\(\boldsymbol{H}_{t-1} \in \mathbb{R}^{n \times h}\)。重置门\(\boldsymbol{R}_t \in \mathbb{R}^{n \times h}\)和更新门\(\boldsymbol{Z}_t \in \mathbb{R}^{n \times h}\)的计算如下：</p>
<p>\[
\begin{aligned}
\boldsymbol{R}_t = \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xr} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hr} + \boldsymbol{b}_r),\\
\boldsymbol{Z}_t = \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xz} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hz} + \boldsymbol{b}_z),
\end{aligned}
\]</p>
<p>其中\(\boldsymbol{W}_{xr}, \boldsymbol{W}_{xz} \in \mathbb{R}^{d \times h}\)和\(\boldsymbol{W}_{hr}, \boldsymbol{W}_{hz} \in \mathbb{R}^{h \times h}\)是权重参数，\(\boldsymbol{b}_r, \boldsymbol{b}_z \in \mathbb{R}^{1 \times h}\)是偏差参数。sigmoid函数可以将元素的值变换到0和1之间。因此，重置门\(\boldsymbol{R}_t\)和更新门\(\boldsymbol{Z}_t\)中每个元素的值域都是\([0, 1]\)。</p>
<h3 id="hou-xuan-yin-cang-zhuang-tai">候选隐藏状态</h3>
<p>接下来，门控循环单元将计算候选隐藏状态来辅助稍后的隐藏状态计算。如下图所示，将当前时间步重置门的输出与上一时间步隐藏状态做按元素乘法（符号为\(\odot\)）。如果重置门中元素值接近0，那么意味着重置对应隐藏状态元素为0，即丢弃上一时间步的隐藏状态。如果元素值接近1，那么表示保留上一时间步的隐藏状态。然后，将按元素乘法的结果与当前时间步的输入连结，再通过含激活函数tanh的全连接层计算出候选隐藏状态，其所有元素的值域为\([-1, 1]\)。</p>
<p><img src="/2020/04/18/deeplearning/rnn/6.7_gru_2.svg" alt="门控循环单元中候选隐藏状态的计算"></p>
<p>具体来说，时间步\(t\)的候选隐藏状态\(\tilde{\boldsymbol{H}}_t \in \mathbb{R}^{n \times h}\)的计算为</p>
<p>\[
\tilde{\boldsymbol{H}}_t = \text{tanh}(\boldsymbol{X}_t \boldsymbol{W}_{xh} + \left(\boldsymbol{R}_t \odot \boldsymbol{H}_{t-1}\right) \boldsymbol{W}_{hh} + \boldsymbol{b}_h)
\]</p>
<p>其中\(\boldsymbol{W}_{xh} \in \mathbb{R}^{d \times h}\)和\(\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}\)是权重参数，\(\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}\)是偏差参数。从上面这个公式可以看出，重置门控制了上一时间步的隐藏状态如何流入当前时间步的候选隐藏状态。而上一时间步的隐藏状态可能包含了时间序列截至上一时间步的全部历史信息。因此，重置门可以用来丢弃与预测无关的历史信息。</p>
<h3 id="yin-cang-zhuang-tai">隐藏状态</h3>
<p>最后，时间步\(t\)的隐藏状态\(\boldsymbol{H}_t \in \mathbb{R}^{n \times h}\)的计算使用当前时间步的更新门\(\boldsymbol{Z}_t\)来对上一时间步的隐藏状态\(\boldsymbol{H}_{t-1}\)和当前时间步的候选隐藏状态\(\tilde{\boldsymbol{H}}_t\)做组合：</p>
<p>\[
\boldsymbol{H}_t = \boldsymbol{Z}_t \odot \boldsymbol{H}_{t-1}  + (1 - \boldsymbol{Z}_t) \odot \tilde{\boldsymbol{H}}_t
\]</p>
<p><img src="/2020/04/18/deeplearning/rnn/6.7_gru_3.svg" alt="门控循环单元中隐藏状态的计算"></p>
<p>值得注意的是，更新门可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新，如上图所示。假设更新门在时间步\(t'\)到\(t\)（\(t' &lt; t\)）之间一直近似1。那么，在时间步\(t'\)到\(t\)之间的输入信息几乎没有流入时间步\(t\)的隐藏状态\(\boldsymbol{H}_t\)。实际上，这可以看作是较早时刻的隐藏状态\(\boldsymbol{H}_{t'-1}\)一直通过时间保存并传递至当前时间步\(t\)。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。</p>
<p>对门控循环单元的设计稍作总结：</p>
<ul>
<li>重置门有助于捕捉时间序列里短期的依赖关系；</li>
<li>更新门有助于捕捉时间序列里长期的依赖关系。</li>
</ul>
<h2 id="dai-ma-shi-xian-1">代码实现</h2>
<h3 id="chu-shi-hua-mo-xing-can-shu-1">初始化模型参数</h3>
<p>下面的代码对模型参数进行初始化。超参数<code>num_hiddens</code>定义了隐藏单元的个数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.Variable(tf.random.normal(shape=shape,stddev=<span class="number">0.01</span>,mean=<span class="number">0</span>,dtype=tf.float32))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (_one((num_inputs, num_hiddens)),</span><br><span class="line">                _one((num_hiddens, num_hiddens)),</span><br><span class="line">                tf.Variable(tf.zeros(num_hiddens), dtype=tf.float32))</span><br><span class="line"></span><br><span class="line">    W_xz, W_hz, b_z = _three()  <span class="comment"># 更新门参数</span></span><br><span class="line">    W_xr, W_hr, b_r = _three()  <span class="comment"># 重置门参数</span></span><br><span class="line">    W_xh, W_hh, b_h = _three()  <span class="comment"># 候选隐藏状态参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = tf.Variable(tf.zeros(num_outputs), dtype=tf.float32)</span><br><span class="line">    <span class="comment"># 附上梯度</span></span><br><span class="line">    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing-3">定义模型</h3>
<p>下面的代码定义隐藏状态初始化函数<code>init_gru_state</code>。同6.4节（循环神经网络的从零开始实现）中定义的<code>init_rnn_state</code>函数一样，它返回由一个形状为(批量大小, 隐藏单元个数)的值为0的<code>Tensor</code>组成的元组。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_gru_state</span><span class="params">(batch_size, num_hiddens)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (tf.zeros(shape=(batch_size, num_hiddens)), )</span><br></pre></td></tr></table></figure>
<p>下面根据门控循环单元的计算表达式定义模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gru</span><span class="params">(inputs, state, params)</span>:</span></span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        X=tf.reshape(X,[<span class="number">-1</span>,W_xh.shape[<span class="number">0</span>]])</span><br><span class="line">        Z = tf.sigmoid(tf.matmul(X, W_xz) + tf.matmul(H, W_hz) + b_z)</span><br><span class="line">        R = tf.sigmoid(tf.matmul(X, W_xr) + tf.matmul(H, W_hr) + b_r)</span><br><span class="line">        H_tilda = tf.tanh(tf.matmul(X, W_xh) + tf.matmul(R * H, W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = tf.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H,)</span><br></pre></td></tr></table></figure>
<ol>
<li>门控循环神经网络可以更好地捕捉时间序列中时间步距离较大的依赖关系。</li>
<li>门控循环单元引入了门的概念，从而修改了循环神经网络中隐藏状态的计算方式。它包括重置门、更新门、候选隐藏状态和隐藏状态。</li>
<li>重置门有助于捕捉时间序列里短期的依赖关系。</li>
<li>更新门有助于捕捉时间序列里长期的依赖关系。</li>
</ol>
<h1 id="chang-duan-qi-ji-yi-lstm">长短期记忆（LSTM）</h1>
<p>长短期记忆（long short-term memory，LSTM）。它比门控循环单元的结构稍微复杂一点。</p>
<h2 id="chang-duan-qi-ji-yi">长短期记忆</h2>
<p>LSTM 中引入了3个门，即输入门（input gate）、遗忘门（forget gate）和输出门（output gate），以及与隐藏状态形状相同的记忆细胞，从而记录额外的信息。</p>
<h3 id="shu-ru-men-yi-wang-men-he-shu-chu-men">输入门、遗忘门和输出门</h3>
<p>与门控循环单元中的重置门和更新门一样，如图所示，长短期记忆的门的输入均为当前时间步输入\(\boldsymbol{X}_t\)与上一时间步隐藏状态\(\boldsymbol{H}_{t-1}\)，输出由激活函数为sigmoid函数的全连接层计算得到。如此一来，这3个门元素的值域均为\([0,1]\)。</p>
<p><img src="/2020/04/18/deeplearning/rnn/6.8_lstm_0.svg" alt="长短期记忆中输入门、遗忘门和输出门的计算"></p>
<p>具体来说，假设隐藏单元个数为\(h\)，给定时间步\(t\)的小批量输入\(\boldsymbol{X}_t \in \mathbb{R}^{n \times d}\)（样本数为\(n\)，输入个数为\(d\)）和上一时间步隐藏状态\(\boldsymbol{H}_{t-1} \in \mathbb{R}^{n \times h}\)。<br>
时间步\(t\)的输入门\(\boldsymbol{I}_t \in \mathbb{R}^{n \times h}\)、遗忘门\(\boldsymbol{F}_t \in \mathbb{R}^{n \times h}\)和输出门\(\boldsymbol{O}_t \in \mathbb{R}^{n \times h}\)分别计算如下：<br>
\[
\begin{aligned}
\boldsymbol{I}_t &amp;= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xi} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hi} + \boldsymbol{b}_i),\\
\boldsymbol{F}_t &amp;= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xf} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hf} + \boldsymbol{b}_f),\\
\boldsymbol{O}_t &amp;= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xo} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{ho} + \boldsymbol{b}_o),
\end{aligned}
\]</p>
<p>其中的\(\boldsymbol{W}_{xi}, \boldsymbol{W}_{xf}, \boldsymbol{W}_{xo} \in \mathbb{R}^{d \times h}\)和\(\boldsymbol{W}_{hi}, \boldsymbol{W}_{hf}, \boldsymbol{W}_{ho} \in \mathbb{R}^{h \times h}\)是权重参数，\(\boldsymbol{b}_i, \boldsymbol{b}_f, \boldsymbol{b}_o \in \mathbb{R}^{1 \times h}\)是偏差参数。</p>
<h3 id="hou-xuan-ji-yi-xi-bao">候选记忆细胞</h3>
<p>接下来，长短期记忆需要计算候选记忆细胞\(\tilde{\boldsymbol{C}}_t\)。它的计算与上面介绍的3个门类似，但使用了值域在\([-1, 1]\)的tanh函数作为激活函数，如图6.8所示。</p>
<p><img src="/2020/04/18/deeplearning/rnn/6.8_lstm_1.svg" alt="长短期记忆中候选记忆细胞的计算"></p>
<p>具体来说，时间步\(t\)的候选记忆细胞\(\tilde{\boldsymbol{C}}_t \in \mathbb{R}^{n \times h}\)的计算为<br>
\[
\tilde{\boldsymbol{C}}_t = \text{tanh}(\boldsymbol{X}_t \boldsymbol{W}_{xc} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hc} + \boldsymbol{b}_c),
\]</p>
<p>其中\(\boldsymbol{W}_{xc} \in \mathbb{R}^{d \times h}\)和\(\boldsymbol{W}_{hc} \in \mathbb{R}^{h \times h}\)是权重参数，\(\boldsymbol{b}_c \in \mathbb{R}^{1 \times h}\)是偏差参数。</p>
<h3 id="ji-yi-xi-bao">记忆细胞</h3>
<p>可以通过元素值域在\([0, 1]\)的输入门、遗忘门和输出门来控制隐藏状态中信息的流动，这一般也是通过使用按元素乘法（符号为\(\odot\)）来实现的。当前时间步记忆细胞\(\boldsymbol{C}_t \in \mathbb{R}^{n \times h}\)的计算组合了上一时间步记忆细胞和当前时间步候选记忆细胞的信息，并通过遗忘门和输入门来控制信息的流动：</p>
<p>\[\boldsymbol{C}_t = \boldsymbol{F}_t \odot \boldsymbol{C}_{t-1} + \boldsymbol{I}_t \odot \tilde{\boldsymbol{C}}_t.\]</p>
<p>如下图所示，遗忘门控制上一时间步的记忆细胞\(\boldsymbol{C}_{t-1}\)中的信息是否传递到当前时间步，而输入门则控制当前时间步的输入\(\boldsymbol{X}_t\)通过候选记忆细胞\(\tilde{\boldsymbol{C}}_t\)如何流入当前时间步的记忆细胞。如果遗忘门一直近似1且输入门一直近似0，过去的记忆细胞将一直通过时间保存并传递至当前时间步。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。</p>
<p><img src="/2020/04/18/deeplearning/rnn/6.8_lstm_2.svg" alt="长短期记忆中记忆细胞的计算"></p>
<h3 id="yin-cang-zhuang-tai-1">隐藏状态</h3>
<p>有了记忆细胞以后，接下来还可以通过输出门来控制从记忆细胞到隐藏状态\(\boldsymbol{H}_t \in \mathbb{R}^{n \times h}\)的信息的流动：</p>
<p>\[
\boldsymbol{H}_t = \boldsymbol{O}_t \odot \text{tanh}(\boldsymbol{C}_t)
\]</p>
<p>这里的tanh函数确保隐藏状态元素值在-1到1之间。需要注意的是，当输出门近似1时，记忆细胞信息将传递到隐藏状态供输出层使用；当输出门近似0时，记忆细胞信息只自己保留。下图展示了长短期记忆中隐藏状态的计算。</p>
<p><img src="/2020/04/18/deeplearning/rnn/6.8_lstm_3.svg" alt="长短期记忆中隐藏状态的计算"></p>
<h2 id="dai-ma-shi-xian-2">代码实现</h2>
<h3 id="chu-shi-hua-mo-xing-can-shu-2">初始化模型参数</h3>
<p>下面的代码对模型参数进行初始化。超参数<code>num_hiddens</code>定义了隐藏单元的个数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.Variable(tf.random.normal(shape=shape,stddev=<span class="number">0.01</span>,mean=<span class="number">0</span>,dtype=tf.float32))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (_one((num_inputs, num_hiddens)),</span><br><span class="line">                _one((num_hiddens, num_hiddens)),</span><br><span class="line">                tf.Variable(tf.zeros(num_hiddens), dtype=tf.float32))</span><br><span class="line">    W_xi, W_hi, b_i = _three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = _three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = _three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = _three()  <span class="comment"># 候选记忆细胞参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = tf.Variable(tf.zeros(num_outputs), dtype=tf.float32)</span><br><span class="line">    <span class="keyword">return</span> [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q]</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing-4">定义模型</h3>
<p>在初始化函数中，长短期记忆的隐藏状态需要返回额外的形状为(批量大小, 隐藏单元个数)的值为0的记忆细胞。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_lstm_state</span><span class="params">(batch_size, num_hiddens)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (tf.zeros(shape=(batch_size, num_hiddens)), </span><br><span class="line">            tf.zeros(shape=(batch_size, num_hiddens)))</span><br></pre></td></tr></table></figure>
<p>下面根据长短期记忆的计算表达式定义模型。需要注意的是，只有隐藏状态会传递到输出层，而记忆细胞不参与输出层的计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm</span><span class="params">(inputs, state, params)</span>:</span></span><br><span class="line">    W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q = params</span><br><span class="line">    (H, C) = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        X=tf.reshape(X,[<span class="number">-1</span>,W_xi.shape[<span class="number">0</span>]])</span><br><span class="line">        I = tf.sigmoid(tf.matmul(X, W_xi) + tf.matmul(H, W_hi) + b_i)</span><br><span class="line">        F = tf.sigmoid(tf.matmul(X, W_xf) + tf.matmul(H, W_hf) + b_f)</span><br><span class="line">        O = tf.sigmoid(tf.matmul(X, W_xo) + tf.matmul(H, W_ho) + b_o)</span><br><span class="line">        C_tilda = tf.tanh(tf.matmul(X, W_xc) + tf.matmul(H, W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * tf.tanh(C)</span><br><span class="line">        Y = tf.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H, C)</span><br></pre></td></tr></table></figure>
<ol>
<li>长短期记忆的隐藏层输出包括隐藏状态和记忆细胞。只有隐藏状态会传递到输出层。</li>
<li>长短期记忆的输入门、遗忘门和输出门可以控制信息的流动。</li>
<li>长短期记忆可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。</li>
</ol>
<h1 id="shen-du-xun-huan-shen-jing-wang-luo">深度循环神经网络</h1>
<p>在深度学习应用里，通常会用到含有多个隐藏层的循环神经网络，也称作深度循环神经网络。下图演示了一个有\(L\)个隐藏层的深度循环神经网络，每个隐藏状态不断传递至当前层的下一时间步和当前时间步的下一层。</p>
<p><img src="/2020/04/18/deeplearning/rnn/6.9_deep-rnn.svg" alt="深度循环神经网络的架构"></p>
<p>具体来说，在时间步\(t\)里，设小批量输入\(\boldsymbol{X}_t \in \mathbb{R}^{n \times d}\)（样本数为\(n\)，输入个数为\(d\)），第\(\ell\)隐藏层（\(\ell=1,\ldots,L\)）的隐藏状态为\(\boldsymbol{H}_t^{(\ell)}  \in \mathbb{R}^{n \times h}\)（隐藏单元个数为\(h\)），输出层变量为\(\boldsymbol{O}_t \in \mathbb{R}^{n \times q}\)（输出个数为\(q\)），且隐藏层的激活函数为\(\phi\)。第1隐藏层的隐藏状态和之前的计算一样：</p>
<p>\[
\boldsymbol{H}_t^{(1)} = \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh}^{(1)} + \boldsymbol{H}_{t-1}^{(1)} \boldsymbol{W}_{hh}^{(1)}  + \boldsymbol{b}_h^{(1)})
\]</p>
<p>其中权重\(\boldsymbol{W}_{xh}^{(1)} \in \mathbb{R}^{d \times h}\)、\(\boldsymbol{W}_{hh}^{(1)} \in \mathbb{R}^{h \times h}\)和偏差 \(\boldsymbol{b}_h^{(1)} \in \mathbb{R}^{1 \times h}\)分别为第1隐藏层的模型参数。</p>
<p>当\(1 &lt; \ell \leq L\)时，第\(\ell\)隐藏层的隐藏状态的表达式为</p>
<p>\[
\boldsymbol{H}_t^{(\ell)} = \phi(\boldsymbol{H}_t^{(\ell-1)} \boldsymbol{W}_{xh}^{(\ell)} + \boldsymbol{H}_{t-1}^{(\ell)} \boldsymbol{W}_{hh}^{(\ell)}  + \boldsymbol{b}_h^{(\ell)})
\]</p>
<p>其中权重\(\boldsymbol{W}_{xh}^{(\ell)} \in \mathbb{R}^{h \times h}\)、\(\boldsymbol{W}_{hh}^{(\ell)} \in \mathbb{R}^{h \times h}\)和偏差 \(\boldsymbol{b}_h^{(\ell)} \in \mathbb{R}^{1 \times h}\)分别为第\(\ell\)隐藏层的模型参数。</p>
<p>最终，输出层的输出只需基于第\(L\)隐藏层的隐藏状态：</p>
<p>\[
\boldsymbol{O}_t = \boldsymbol{H}_t^{(L)} \boldsymbol{W}_{hq} + \boldsymbol{b}_q
\]</p>
<p>其中权重\(\boldsymbol{W}_{hq} \in \mathbb{R}^{h \times q}\)和偏差\(\boldsymbol{b}_q \in \mathbb{R}^{1 \times q}\)为输出层的模型参数。</p>
<p>同多层感知机一样，隐藏层个数\(L\)和隐藏单元个数\(h\)都是超参数。此外，如果将隐藏状态的计算换成门控循环单元或者长短期记忆的计算，我们可以得到深度门控循环神经网络。</p>
<h1 id="shuang-xiang-xun-huan-shen-jing-wang-luo">双向循环神经网络</h1>
<p>前面介绍的循环神经网络模型都是假设当前时间步是由前面的较早时间步的序列决定的，因此它们都将信息通过隐藏状态从前往后传递。有时候，当前时间步也可能由后面时间步决定。例如，当我们写下一个句子时，可能会根据句子后面的词来修改句子前面的用词。双向循环神经网络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息。下图演示了一个含单隐藏层的双向循环神经网络的架构。</p>
<p><img src="/2020/04/18/deeplearning/rnn/6.10_birnn.svg" alt="双向循环神经网络的架构"></p>
<p>给定时间步\(t\)的小批量输入\(\boldsymbol{X}_t \in \mathbb{R}^{n \times d}\)（样本数为\(n\)，输入个数为\(d\)）和隐藏层激活函数为\(\phi\)。在双向循环神经网络的架构中，<br>
设该时间步正向隐藏状态为\(\overrightarrow{\boldsymbol{H}}_t  \in \mathbb{R}^{n \times h}\)（正向隐藏单元个数为\(h\)），反向隐藏状态为\(\overleftarrow{\boldsymbol{H}}_t  \in \mathbb{R}^{n \times h}\)（反向隐藏单元个数为\(h\)）。我们可以分别计算正向隐藏状态和反向隐藏状态：<br>
\[
\begin{aligned}
\overrightarrow{\boldsymbol{H}}_t &amp;= \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh}^{(f)} + \overrightarrow{\boldsymbol{H}}_{t-1} \boldsymbol{W}_{hh}^{(f)}  + \boldsymbol{b}_h^{(f)}),\\
\overleftarrow{\boldsymbol{H}}_t &amp;= \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh}^{(b)} + \overleftarrow{\boldsymbol{H}}_{t+1} \boldsymbol{W}_{hh}^{(b)}  + \boldsymbol{b}_h^{(b)}),
\end{aligned}
\]</p>
<p>其中权重\(\boldsymbol{W}_{xh}^{(f)} \in \mathbb{R}^{d \times h}\)、\(\boldsymbol{W}_{hh}^{(f)} \in \mathbb{R}^{h \times h}\)、\(\boldsymbol{W}_{xh}^{(b)} \in \mathbb{R}^{d \times h}\)、\(\boldsymbol{W}_{hh}^{(b)} \in \mathbb{R}^{h \times h}\)和偏差 \(\boldsymbol{b}_h^{(f)} \in \mathbb{R}^{1 \times h}\)、\(\boldsymbol{b}_h^{(b)} \in \mathbb{R}^{1 \times h}\)均为模型参数。</p>
<p>然后我们连结两个方向的隐藏状态\(\overrightarrow{\boldsymbol{H}}_t\)和\(\overleftarrow{\boldsymbol{H}}_t\)来得到隐藏状态\(\boldsymbol{H}_t \in \mathbb{R}^{n \times 2h}\)，并将其输入到输出层。输出层计算输出\(\boldsymbol{O}_t \in \mathbb{R}^{n \times q}\)（输出个数为\(q\)）：</p>
<p>\[
\boldsymbol{O}_t = \boldsymbol{H}_t \boldsymbol{W}_{hq} + \boldsymbol{b}_q
\]</p>
<p>其中权重\(\boldsymbol{W}_{hq} \in \mathbb{R}^{2h \times q}\)和偏差\(\boldsymbol{b}_q \in \mathbb{R}^{1 \times q}\)为输出层的模型参数。不同方向上的隐藏单元个数也可以不同。</p>
]]></content>
      <categories>
        <category>技术/深度学习</category>
      </categories>
      <tags>
        <tag>rnn</tag>
      </tags>
  </entry>
  <entry>
    <title>bert系列</title>
    <url>/2020/04/13/research/bert%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/Sesame_japan_cast_white_bg.jpg" alt></p>
<a id="more"></a>
<h2 id="bert-ji-qi-hou-xu-mo-xing">BERT 及其后续模型</h2>
<p>了解这些后续的模型，方便在后续的研究和应用中来更好的选择模型。</p>
<h3 id="bert">BERT</h3>
<p>BERT模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/bert.jpg" alt></p>
<p>对比OpenAI GPT(Generative pre-trained transformer)，BERT是双向的Transformer block连接；就像单向rnn和双向rnn的区别，直觉上来讲效果会好一些。</p>
<p>对比ELMo，虽然都是“双向”，但目标函数其实是不同的。ELMo是分别以 \(P(w_i|w_1,\cdots,w_{i-1})\) 和\(P(w_i|w_{i+1},\cdots,w_n)\)作为目标函数，独立训练处两个representation然后拼接，而BERT则是以\(P(w_i|w_1,\cdots,w_{i-1},w_{i+1}\cdots,w_n)\)  作为目标函数训练LM。</p>
<h4 id="task-1-mlm">Task 1: MLM</h4>
<p>由于BERT需要通过上下文信息，来预测中心词的信息，同时又不希望模型提前看见中心词的信息，因此提出了一种 Masked Language Model 的预训练方式，即随机从输入预料上 mask 掉一些单词，然后通过的上下文预测该单词，类似于一个完形填空任务。</p>
<p>在预训练任务中，15%的 Word Piece 会被mask，这15%的 Word Piece 中，80%的时候会直接替换为 [Mask] ，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token</p>
<ul>
<li>没有100%mask的原因
<ul>
<li>如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词</li>
</ul>
</li>
<li>加入10%随机token的原因
<ul>
<li>Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘</li>
<li>另外编码器不知道哪些词需要预测的，哪些词是错误的，因此被迫需要学习每一个token的表示向量</li>
</ul>
</li>
<li>另外，每个batchsize只有15%的单词被mask的原因，是因为性能开销的问题，双向编码器比单项编码器训练要更慢</li>
</ul>
<h4 id="task-2-nsp">Task 2: NSP</h4>
<p>仅仅一个MLM任务是不足以让 BERT 解决阅读理解等句子关系判断任务的，因此添加了额外的一个预训练任务，即 Next Sequence Prediction。</p>
<p>具体任务即为一个句子关系判断任务，即判断句子B是否是句子A的下文，如果是的话输出’IsNext‘，否则输出’NotNext‘。</p>
<p>训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在图4中的[CLS]符号中.</p>
<h4 id="shu-ru">输入</h4>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/bert_input.jpg" alt></p>
<ul>
<li>Token Embeddings：即传统的词向量层，每个输入样本的首字符需要设置为[CLS]，可以用于之后的分类任务，若有两个不同的句子，需要用[SEP]分隔，且最后一个字符需要用[SEP]表示终止</li>
<li>Segment Embeddings：为[0,1]序列，用来在NSP任务中区别两个句子，便于做句子关系判断任务</li>
<li>Position Embeddings：与Transformer中的位置向量不同，BERT中的位置向量是直接训练出来的</li>
</ul>
<h4 id="fine-tunninng">Fine-tunninng</h4>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/bert_finetune.jpg" alt></p>
<p>对于不同的下游任务，我们仅需要对BERT不同位置的输出进行处理即可，或者直接将BERT不同位置的输出直接输入到下游模型当中。具体的如下所示：</p>
<ul>
<li>对于情感分析等单句分类任务，可以直接输入单个句子（不需要[SEP]分隔双句），将[CLS]的输出直接输入到分类器进行分类</li>
<li>对于句子对任务（句子关系判断任务），需要用[SEP]分隔两个句子输入到模型中，然后同样仅须将[CLS]的输出送到分类器进行分类</li>
<li>对于问答任务，将问题与答案拼接输入到BERT模型中，然后将答案位置的输出向量进行二分类并在句子方向上进行softmax（只需预测开始和结束位置即可）</li>
<li>对于命名实体识别任务，对每个位置的输出进行分类即可，如果将每个位置的输出作为特征输入到CRF将取得更好的效果。</li>
</ul>
<h4 id="you-dian">优点</h4>
<ul>
<li>考虑双向信息（上下文信息）</li>
<li>不用考虑很长的时序问题（梯度弥散，梯度爆炸）：long-term dependency</li>
</ul>
<h4 id="que-dian">缺点</h4>
<ul>
<li>BERT的预训练任务MLM使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务(训练数据缺乏mask)</li>
<li>缺乏生成能力</li>
<li>另外，BERT没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计。（朴素贝叶斯，naive）</li>
<li>由于最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）</li>
<li>适合处理自然语义理解类任务(NLU)，而不适合自然语言生成类任务(NLG)</li>
</ul>
<h4 id="torch-shi-yong">torch使用</h4>
<ol>
<li>导入相关库</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> transformers <span class="keyword">as</span> ppb <span class="comment"># pytorch transformers</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>导入预训练好的 DistilBERT 模型与分词器</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, <span class="string">'distilbert-base-uncased'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Want BERT instead of distilBERT? Uncomment the following line:</span></span><br><span class="line"><span class="comment">#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')# </span></span><br><span class="line"></span><br><span class="line">Load pretrained model/tokenizertokenizer = tokenizer_class.from_pretrained(pretrained_weights)</span><br><span class="line">model = model_class.from_pretrained(pretrained_weights)</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>tokenize：分词+[cls]+[sep]<br>
<img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/bert_tokenize.png" alt="avatar"></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.encode(<span class="string">'a visually stunning rumination on love'</span>, add_special_tokens=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>padding</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_len = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tokenized.values:</span><br><span class="line"> <span class="keyword">if</span> len(i) &gt; max_len:</span><br><span class="line">     max_len = len(i)</span><br><span class="line">padded = np.array([i + [<span class="number">0</span>]*(max_len-len(i)) <span class="keyword">for</span> i <span class="keyword">in</span> tokenized.values])</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>masking</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">attention_mask = np.where(padded != <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ol start="6">
<li>使用bert进行编码</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># last_hidden_states = [batch_size, max_sentence_len, 768]</span></span><br><span class="line">    last_hidden_states = model(input_ids, attention_mask=attention_mask)</span><br></pre></td></tr></table></figure>
<p><a href="http://fancyerii.github.io/2019/03/09/bert-codes/#dataprocessor" target="_blank" rel="noopener">bert代码阅读</a></p>
<hr>
<p>谷歌对BERT进行了改版，下面将对改版后的bert进行学习，对比改版前后主要的相似点和不同点，以便可以选择在研究或应用中使用哪一种。提出的几种方法改进BERT的预测指标或计算速度，但是始终达不到两者兼顾。<strong>XLNet和RoBERTa改善了性能，而DistilBERT提高了推理速度</strong>。</p>
<h3 id="strong-bert-wwm-strong"><strong>BERT-WWM</strong></h3>
<p><a href>论文</a></p>
<h4 id="jian-jie">简介</h4>
<p>wwm 即 Whole Word Masking（对全词进行Mask）。相比于bert的改进是用Mask标签替换一个完整的词而不是字，中文和英文不同，英文最小的token是一个单词，而中文中最小的token却是字，词是由一个或多个字组成，且每个词之间没有明显的分割，包含更多信息的是词，对全词mask就是对整个词都通过mask进行掩码。<br>
<img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/bert_wwm_exam.png" alt></p>
<h4 id="strong-bert-wwm-ext-strong"><strong>Bert-wwm-ext</strong></h4>
<p>它是BERT-wwm的一个升级版，相比于BERT-wwm的改进是增加了训练数据集同时也增加了训练步数。<br>
BERT-wwm-ext主要是有两点改进：<br>
1）预训练数据集做了增加，达到5.4B；<br>
2）训练步数增大，训练第一阶段1M步，训练第二阶段400K步。</p>
<p><a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">模型下载</a></p>
<h3 id="xl-net">XLNet</h3>
<p>XLNet作为bert的升级模型，主要在以下三个方面进行了优化:</p>
<ul>
<li>采用AR模型替代AE模型，解决mask带来的负面影响</li>
<li>双流注意力机制</li>
<li>引入transformer-xl</li>
</ul>
<h4 id="ar-yu-ae-yu-yan-mo-xing">AR与AE语言模型</h4>
<p>目前主流的nlp预训练模型包括两类 Auto Regressive (<strong>AR</strong>) Language Model 与Auto Encoding (AE) Language Model。</p>
<h5 id="ar-mo-xing">AR模型</h5>
<p>AR模型的主要任务在于评估语料的概率分布，例如，给定一个序列 \(X=(x_1, \cdots ,x_T)\) ，AR模型就是在计算其极大似然估计\(p(X)=\prod_{t=1}^Tp(x_t∣x_{&lt;t})\)即已知\(x_t\)之前的序列，预测\(x_t\) 的值，当然也可以反着来\(p(X)=\prod_{t=1}^Tp(x_t∣x_{>t})\)即已知\(x_t\)之后的序列，预测\(x_t\)的值。AR模型一个很明显的缺点就是：模型是单向的，我们更希望的是根据上下文来预测目标，而不单是上文或者下文，之前open AI提出的GPT就是采用的AR模式，包括GPT2.0也是该模式。</p>
<p>优点：</p>
<ol>
<li>具备生成能力</li>
<li>考虑了相关性</li>
<li>无监督</li>
<li>严格的数学表达</li>
</ol>
<p>缺点：</p>
<ol>
<li>单向的</li>
<li>（难考虑 long term dependency）</li>
</ol>
<h5 id="ae-mo-xing">AE模型</h5>
<p>AE模型采用的就是以上下文的方式，最典型的成功案例就是bert。简单回顾下bert的预训练阶段，预训练包括了两个任务，Masked Language Model与Next Sentence Prediction，Next Sentence Prediction即判断两个序列的推断关系，Masked Language Model采用了一个标志位[MASK]来随机替换一些词，再用[MASK]的上下文来预测[MASK]的真实值，bert的最大问题也是处在这个MASK的点，因为在微调阶段，没有MASK这就导致预训练和微调数据的不统一，从而引入了一些人为误差。</p>
<p>在xlnet中，采用了<strong>AR模型</strong>，但是怎么解决这个上下文的问题呢？</p>
<h4 id="pai-lie-yu-yan-mo-xing-wei-liao-kao-lu-shang-xia-wen-xin-xi">排列语言模型(为了考虑上下文信息)</h4>
<p>为了解决上文提到的问题，作者提出了排列语言模型，该模型不再对传统的AR模型的序列的值按顺序进行建模，而是最大化所有可能的序列的因式分解顺序的期望对数似然，这句话可能有点不好理解，举个栗子，假如我们有一个序列\([1,2,3,4]\)，如果我们的预测目标是3，对于传统的AR模型来说，结果是\(p(3)=\prod^3_{t=1}p(3|x_{&lt;t})\)，如果采用本文的方法，先对该序列进行因式分解，最终会有24种排列方式，下图是其中可能的四种情况，对于第一种情况因为3的左边没有其他的值，所以该情况无需做对应的计算，第二种情况3的左边还包括了2与4，所以得到的结果是\(p(3)=p(3|2)p(3|2,4)\)，后续的情况类似，这样处理过后不但保留了序列的上下文信息，也<strong>避免了采用mask标记位，巧妙的改进了bert与传统AR模型的缺点</strong>。<br>
<img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/xlnet.png" alt></p>
<h5 id="ru-he-xun-lian">如何训练？</h5>
<ol>
<li>假如句子长度为20，则会产生\(20\)!种排列，在这\(20\)!种排列中随机抽样，得到训练样本。</li>
<li>为获得丰富的上下文语义信息，对抽取的样本最后的几个单词进行预测。</li>
</ol>
<h4 id="ji-yu-mu-biao-gan-zhi-biao-zheng-de-strong-shuang-liu-zi-zhu-yi-li-strong-wei-liao-jie-jue-wei-zhi-xin-xi">基于目标感知表征的<strong>双流自注意力</strong>(为了解决位置信息)</h4>
<p>虽然排列语言模型能满足目前的目标，但是对于普通的transformer结构来说是存在一定的问题的，为什么这么说呢，假设我们要求这样的一个对数似然，\(p_\theta(X_{z_t}|x_{z_{&lt;t}})\)，如果采用标准的softmax的话，那么<br>
\[
p_{\theta}\left(X_{z_t} | x_{z_{&lt;t}}\right)=\frac{\exp \left(e(x)^{T} h_{\theta}\left(x_{z_{&lt;t}}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{T} h_{\theta}\left(x_{z_{&lt;t}}\right)\right)}
\]<br>
其中\(h_\theta (x_{z_{&lt;t}})\)表示的是添加了mask后的transformer的输出值，可以发现\(h_\theta (x_{z_{&lt;t}})\)并不依赖于其要预测的内容的位置信息，因为无论预测目标的位置在哪里，因式分解后得到的所有情况都是一样的，并且transformer的权重对于不同的情况是一样的，因此无论目标位置怎么变都能得到相同的分布结果，如下图所示，假如我们的序列index表示为[1,2,3]，对于目标2与3来说，其因式分解后的结果是一样的，那么经过transformer之后得到的结果肯定也是一样的。<br>
<img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/123.png" alt></p>
<p>这就导致模型没法得到正确的表述，为了解决这个问题，论文中提出来新的分布计算方法，来实现目标位置感知<br>
\[
p_{\theta}\left(X_{zt}=x | x_{z&lt;t}\right)=\frac{\exp \left(e(x)^{T} g_{\theta}\left(x_{z&lt;t}, z_{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{T} g_{\theta}\left(x_{z&lt;t}, z_{t}\right)\right)}
\]<br>
其中\(g_{\theta}\left(x_{z_{&lt;t}}, z_{t}\right)\)是新的表示形式，并且把位置信息\(z_t\)作为了其输入。<br>
这个新的表示形式，论文把该方法称为Two-Stream Self-Attention，双流自注意力，该机制需要解决了两个问题:</p>
<ul>
<li>如果目标是预测\(x_{z_t}\)，\(g_\theta (x_{z&lt;t},z_t)\)那么只能有其位置信息\(z_t\) 而不能包含内容信息\(x_{z_t}\)</li>
<li>如果目标是预测其他tokens即\(x_{z_j}\)那么应该包含\(x_{z_t}\)的内容信息这样才有完整的上下文信息</li>
</ul>
<p>传统的transformer并不满足这样的需求，因此作者采用了两种表述来代替原来的表述，这也是为什么称为<strong>双流</strong>的原因:</p>
<ul>
<li>
<p>content representation内容表述，即\(h_\theta (x_{z_{\leq t}})\)下文用\(h_{z_t}\)表示，该表述和传统的transformer一样，同时编码了上下文和\(x_{z_t}\)自身<br>
\[
h_{z_{t}}^{(m)}=\text {Attention}\left(Q=h_{z_{t}}^{(m-1)}, K V=h_{z \leq t}^{(m-1)} ; \theta\right)
\]<br>
<img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/content_representation.png" alt></p>
</li>
<li>
<p>query representation查询表述，即\(g_\theta (x_{z_{&lt;t}})\)，下文用\(g_{z_t}\) 表示，该表述包含上下文的内容信息\(x_{z_{&lt;t}}\) 和目标的位置信息\(z_t\)，但是不包括目标的内容信息\(x_{z_{t}}\) ，从图中可以看到，K与V的计算并没有包括Q，自然也就无法获取到目标的内容信息，但是目标的位置信息在计算Q的时候保留了下来，<br>
\[
g_{z_{t}}^{(m)}=\text {Attention}\left(Q=g_{z_{t}}^{(m-1)}, K V=h_{z&lt;t}^{(m-1)} ; \theta\right)
\]</p>
</li>
</ul>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/query_representation.png" alt></p>
<p>总的计算过程，首先，第一层的查询流是随机初始化了一个向量即\(g_i^{0}=w\)，内容流是采用的词向量即\(h_i^{0}=e(x_i)\)，self-attention的计算过程中两个流的网络权重是共享的，最后在微调阶段，只需要简单的把query stream移除，只采用content stream即可。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/two_stream.png" alt></p>
<h4 id="yin-ru-transformer-xl">引入transformer XL</h4>
<p>作者还将transformer-xl的两个最重要的技术点应用了进来，即片段循环机制与相对位置编码。</p>
<h5 id="pian-duan-xun-huan-ji-zhi">片段循环机制</h5>
<p>ransformer-xl的提出主要是为了解决超长序列的依赖问题，对于普通的transformer由于有一个最长序列的超参数控制其长度（如果不对文本控制，则会极大消耗内存，因为在计算score矩阵的时候，会开辟\(n^2\)大小的矩阵，对于一篇文本，长度为\(10^5\),则需要开辟\(10^5 * 10^5 = 10^10\)大小的score矩阵，极其消耗内存。），对于特别长的序列就会导致丢失一些信息，transformer-xl就能解决这个问题。我们看个例子，假设我们有一个长度为1000的序列，如果我们设置transformer的最大序列长度是100，那么这个1000长度的序列需要计算十次，并且每一次的计算都没法考虑到每一个段之间的关系，如果采用transformer-xl，首先取第一个段进行计算，然后把得到的结果的隐藏层的值进行缓存，第二个段计算的过程中，把缓存的值拼接起来再进行计算（RNN）。该机制不但能保留长依赖关系还能加快训练，因为每一个前置片段都保留了下来，不需要再重新计算，在transformer-xl的论文中，经过试验其速度比transformer快了1800倍。<br>
在xlnet中引入片段循环机制其实也很简单，只需要在计算KV的时候做简单的修改，其中\(\tilde{h}^{m-1}\)表示的是缓存值。<br>
\[
h_{z_{t}}^{(m)}=\text {Attention}\left(Q=h_{z_{t}}^{(m-1)}, K V=\left[\tilde{h}^{(m-1)}, h_{z \leq t}^{(m-1)}\right] ; \theta\right)
\]</p>
<h5 id="xiang-dui-wei-zhi-bian-ma">相对位置编码</h5>
<p>bert的position embedding采用的是绝对位置编码，但是绝对位置编码在transformer-xl中有一个致命的问题，因为没法区分到底是哪一个片段里的，这就导致了一些位置信息的损失，这里被替换为了transformer-xl中的相对位置编码。假设给定一对位置\(i\)与\(j\)，如果\(i\)与\(j\)是同一个片段里的那么我们令这个片段编码\(s_{ij}=s_+\)，如果不在一个片段里则令这个片段编码为\(s_{ij}=s_-\)，这个值是在训练的过程中得到的，也是用来计算attention weight时候用到的，在传统的transformer中\(\text {attention weight}= softmax(\frac{Q⋅K}{d}V)\)，在引入相对位置编码后，首先要计算出\(a_{ij}=(q_i+b)^T_{s_{sj}}\)，其中\(b\)也是一个需要训练得到的偏执量，最后把得到的\(a_ij\)与传统的transformer的weight相加从而得到最终的attention weight。</p>
<p><a href="https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_xlnet.py" target="_blank" rel="noopener">源码</a></p>
<h4 id="zong-jie">总结</h4>
<p>在实验中发现，xlnet更多还是在长文本的阅读理解类的任务上提升更明显一些，这也很符合上文中介绍的这些优化点，在实际工业场景中，机器翻译、本文摘要类的任务应该会有更好的效果，当然在其他的文本分类、自然语言推理等任务上xlnet也有一定的效果提升。nlp领域的模型目前已经完全采用了pretrain+fine tuning的模式，GPT2.0的单向模式在增大训练语料的情况下效果就已经超越了bert，可想而知该领域的研究还有很大的上升空间</p>
<h3 id="ro-ber-ta">RoBERTa</h3>
<h4 id="jian-jie-1">简介</h4>
<p>在XLNet全面超越Bert后没多久，Facebook提出了RoBERTa（a Robustly Optimized BERT Pretraining Approach）。再度在多个任务上达到SOTA。</p>
<p>它在模型层面没有改变Google的Bert，<strong>改变的只是预训练的方法</strong>。</p>
<h4 id="dong-ji">动机</h4>
<p>Yinhan Liu等人[6]认为超参数的选择对最终结果有重大影响，为此他们提出了BERT预训练的重复研究，其中包括对超参数调整和训练集大小的影响的仔细评估。<strong>最终，他们发现了BERT的训练不足</strong>，并提出了一种改进的模型来训练BERT模型。</p>
<h4 id="gai-jin">改进</h4>
<h5 id="jing-tai-masking-vs-dong-tai-masking">静态Masking vs 动态Masking</h5>
<p>原来Bert对每一个序列随机选择15%的Tokens替换成[MASK]，为了消除与下游任务的不匹配，还对这15%的Tokens进行（1）80%的概率替换成[MASK]；（2）10%的概率不变；（3）10%的概率替换成其他词。但整个训练过程，这15%的Tokens一旦被选择就不再改变，也就是说从一开始随机选择了这15%的Tokens，之后的N个epoch里都不再改变了。这就叫做静态Masking。</p>
<p>而RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。这就叫做动态Masking。作者在只将静态Masking改成动态Masking，其他参数不变的情况下做了实验，动态Masking确实能提高性能。</p>
<h5 id="with-nsp-vs-without-nsp">with NSP vs without NSP</h5>
<p>原本的Bert为了捕捉句子之间的关系，使用了NSP任务进行预训练，就是输入一对句子A和B，判断这两个句子是否是连续的。在训练的数据中，50%的B是A的下一个句子，50%的B是随机抽取的。</p>
<p>而RoBERTa去除了NSP，而是每次输入连续的多个句子，直到最大长度512（可以跨文章）。这种训练方式叫做（FULL - SENTENCES），而原来的Bert每次只输入两个句子。实验表明在MNLI这种推断句子关系的任务上RoBERTa也能有更好性能。（？？？？）</p>
<h5 id="geng-da-de-mini-batch">更大的mini-batch</h5>
<p>原本的BERTbase 的batch size是256，训练1M个steps。RoBERTa的batch size为8k。为什么要用更大的batch size呢？作者借鉴了在机器翻译中，用更大的batch size配合更大学习率能提升模型优化速率和模型性能的现象，并且也用实验证明了确实Bert还能用更大的batch size。</p>
<h5 id="geng-duo-de-shu-ju-geng-chang-de-xun-lian-shi-jian">更多的数据，更长的训练时间</h5>
<p>借鉴XLNet用了比Bert多10倍的数据，RoBERTa也用了更多的数据。性能确实再次彪升。当然，也需要配合更长时间的训练。</p>
<h5 id="byte-pair-encoding-bpe-zi-fu-bian-ma">Byte-Pair Encoding (BPE)字符编码</h5>
<p>使用Sennrich[8]等人提出的Byte-Pair Encoding (BPE)字符编码，它是字符级和单词级表示之间的混合体，可以处理自然语言语料库中常见的大词汇，避免训练数据出现更多的“[UNK]”标志符号，从而影响预训练模型的性能。其中，“[UNK]”标记符表示当在BERT自带字典vocab.txt找不到某个字或者英文单词时，则用“[UNK]”表示。</p>
<h3 id="span-bert">SpanBERT</h3>
<p><a href="https://arxiv.org/pdf/1907.10529.pdf" target="_blank" rel="noopener">论文</a></p>
<h4 id="jian-jie-2">简介</h4>
<p>在许多 NLP 任务中都涉及对多个文本分词间关系的推理。例如，在抽取式问答任务中，在回答问题“Which NFL team won Super Bown 50?”时，判断“Denver Broncos” 是否属于“NFL team”是非常重要的步骤。相比于在已知“Broncos”预测“Denver”的情况，直接预测“Denver Broncos”难度更大，这意味着这类分词对自监督任务提出了更多的挑战。</p>
<p>SpanBert对 BERT 模型进行了如下改进：</p>
<ol>
<li>没有segment embedding，只有一个长的句子，类似RoBERTa。</li>
<li><strong>Span Masking</strong>:对随机的邻接分词（span）而非随机的单个词语（token）添加mask；</li>
<li>MLM+SBO:通过使用分词边界的表示来预测被添加mask的分词的内容，不再依赖分词内单个 token 的表示。</li>
</ol>
<p>SpanBERT 能够对分词进行更好地表示和预测。该模型和 BERT 在mask机制和训练目标上存在差别。首先，SpanBERT 不再对随机的单个 token 添加mask，而是对随机对邻接分词添加mask。其次，SpanBert提出了一个新的训练目标 span-boundary objective (SBO) 进行模型训练。通过对分词添加mask，作者能够使模型依据其所在语境预测整个分词。另外，SBO 能使模型在边界词中存储其分词级别的信息，使得模型的调优更佳容易。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/spanbert.png" alt></p>
<p>模型使用边界词 was和 to来预测分词中的每个单词。</p>
<p>为了搭建 SpanBERT ，作者首先构建了一个 BERT 模型的并进行了微调，SpanBERT的表现优于原始 BERT 模型。在搭建baseline的时候，作者发现对单个部分进行预训练的效果，比使用 next sentence prediction (NSP) 目标对两个长度为一半的部分进行训练的效果更优，在下游任务中表现尤其明显。因此，作者在经过调优的 BERT 模型的顶端对模型进行了改进。</p>
<h4 id="span-masking">Span Masking(???)</h4>
<p>对于每一个单词序列 \(X = (x_1, \ldots , x_n )\)，作者通过迭代地采样文本的分词选择单词，直到达到掩膜要求的大小（例如 X 的 15%），并形成 X 的子集 Y。在每次迭代中，作者首先从几何分布 \(l \sim Geo(p)\)中采样得到分词的长度，该几何分布是偏态分布，偏向于较短的分词。之后，作者随机（均匀地）选择分词的起点。</p>
<p>根据几何分布，先随机选择一段（span）的长度，之后再根据均匀分布随机选择这一段的起始位置，最后按照长度遮盖。作者设定几何分布取 p=0.2，并裁剪最大长度只能是 10（不应当是长度 10 以上修剪，而应当为丢弃），利用此方案获得平均采样长度分布。因此分词的平均长度为 3.8 。作者还测量了词语（word）中的分词程度，使得添加掩膜的分词更长。下图展示了分词掩膜长度的分布情况。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/spanbert_fenci.png" alt></p>
<p>和在 BERT 中一样，作者将 Y 的规模设定为 X 的15%，其中 80% 使用 [MASK] 进行替换，10% 使用随机单词替换，10%保持不变。与之不同的是，作者是在分词级别进行的这一替换，而非将每个单词单独替换。</p>
<h4 id="fen-ci-bian-jie-mu-biao-sbo">分词边界目标(SBO)???</h4>
<p>分词选择模型一般使用其边界词创建一个固定长度的分词表示。为了于该模型相适应，作者希望结尾分词的表示的总和与中间分词的内容尽量相同。为此，作者引入了 SBO ，其仅使用观测到的边界词来预测带掩膜的分词的内容。</p>
<p>具体做法是，在训练时取 Span 前后边界的两个词，值得指出，这两个词不在 Span 内，然后<strong>用这两个词向量加上 Span 中被遮盖掉词的位置向量，来预测原词</strong>。<br>
\[
\mathbf{y}_{i}=f\left(\mathbf{x}_{s-1}, \mathbf{x}_{e+1}, \mathbf{p}_{i}\right)
\]<br>
详细做法是将词向量和位置向量拼接起来，作者使用一个两层的前馈神经网络作为表示函数，该网络使用 GeLu 激活函数，并使用层正则化：</p>
<p>\[
\begin{aligned}
\mathbf{h} &amp;=\text { LayerNorm }\left(\operatorname{GeLU}\left(W_{1} \cdot\left[\mathbf{x}_{s-1} ; \mathbf{x}_{e+1} ; \mathbf{p}_{i}\right]\right)\right) \\
f(\cdot) &amp;=\text { LayerNorm }\left(\operatorname{GeLU}\left(W_{2} \cdot \mathbf{h}\right)\right)
\end{aligned}
\]<br>
作者使用向量表示\(y_i\)来预测\(x_i\)，并和 MLM 一样使用交叉熵作为损失函数，就是 SBO 目标的损失，之后将这个损失和 BERT 的 **Mased Language Model （MLM）**的损失加起来，一起用于训练模型。<br>
\[
\mathcal{L}(\text { football })=\mathcal{L}_{\mathrm{MLM}}\left(\mathbf{x}_{7}\right)+\mathcal{L}_{\mathrm{SBO}}\left(\mathbf{x}_{4}, \mathbf{x}_{9}, \mathbf{p}_{7}\right)
\]</p>
<h4 id="dan-xu-lie-xun-lian">单序列训练</h4>
<p>SpanBERT 没用 Next Sentence Prediction (NSP) 任务，而是直接用 Single-Sequence Training，也就是不加入 NSP 任务来判断是否两句是上下句，直接用一句来训练，片段长度最多为512个单词。</p>
<h4 id="xi-jie">细节</h4>
<ol>
<li>训练时用了 <strong>Dynamic Masking</strong> 而不是像 BERT 在预处理时做 静态Mask。</li>
<li><strong>取消 BERT 中随机采样短句的策略</strong></li>
<li>对Adam优化器中的一些参数做了改变。</li>
</ol>
<h5 id="mask-ji-zhi">mask机制</h5>
<p>作者在子单词、完整词语、命名实体、名词短语和随机分词方面进行了比较：发现使用随机分词掩膜机制效果更优。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/spanbert_token.png" alt></p>
<h5 id="fu-zhu-mu-biao">辅助目标</h5>
<p>使用 SBO 替换 NSP 并使用单序列进行预测的效果更优。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/spanbert_a_o.png" alt></p>
<h4 id="zong-jie-1">总结</h4>
<p>SpanBERT是基于分词的预训练模型，在多个评测任务中的得分都超越了 BERT 且在分词选择类任务中的提升尤其明显。</p>
<h3 id="ernie">ERNIE</h3>
<p><a href="https://arxiv.org/pdf/1905.07129" target="_blank" rel="noopener">论文</a></p>
<h4 id="jian-jie-3">简介</h4>
<p>无论是Elmo、GPT, 还是能力更强的 BERT 模型，其建模对象主要聚焦在原始语言信号上，较少利用语义知识单元建模。这个问题在中文方面尤为明显，例如，BERT 在处理中文语言时，通过预测汉字进行建模，模型很难学出更大语义单元的完整语义表示。例如，对于乒 [mask] 球，清明上 [mask] 图，[mask] 颜六色这些词，BERT 模型通过字的搭配，很容易推测出掩码的字信息，但没有显式地对语义概念单元 (如乒乓球、清明上河图) 以及其对应的语义关系进行建模。如果能够让模型学习到海量文本中蕴含的潜在知识，势必会进一步提升各个 NLP 任务效果。</p>
<p>ERNIE 模型通过建模海量数据中的实体概念等先验语义知识，学习真实世界的语义关系。具体来说，ERNIE 模型通过对词、实体等语义单元的掩码，使得模型学习完整概念的语义表示。相较于 BERT 学习原始语言信号，ERNIE 直接对先验语义知识单元进行建模，增强了模型语义表示能力。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/ernie.png" alt></p>
<p>在 BERT 模型中，通过『哈』与『滨』的局部共现，即可判断出『尔』字，模型没有学习与『哈尔滨』相关的知识。而 ERNIE 通过学习词与实体的表达，使模型能够建模出『哈尔滨』与『黑龙江』的关系，学到『哈尔滨』是『黑龙江』的省会以及『哈尔滨』是个冰雪城市。</p>
<h4 id="mo-xing">模型</h4>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/ernie_ar.png" alt></p>
<p>模型上主要的改进是在bert的后段加入了实体向量和经过bert编码后的向量拼接，另外在输出时多加了实体自编码的任务，从而帮助模型注入实体知识信息。</p>
<h5 id="t-encoder">T-Encoder</h5>
<p>这部分就是纯粹的bert结构，在该部分模型中主要负责对输入句子（token embedding, segment embedding和positional embedding）进行编码.</p>
<h5 id="k-encoder">K-Encoder</h5>
<p>引入实体信息，使用了TransE训练实体向量，再通过多头Attention进行编码，然后通过实体对齐技术，将实体向量加入到对应实体的第一个token编码后的向量上。（例如姚明是一个实体，在输入时会被分割成姚、明，最后在这部分引入的姚明这个实体的向量会被加入到“姚”这个字经过bert之后的向量上去）<br>
\[
\begin{aligned}
\left\{\tilde{\boldsymbol{w}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{w}}_{n}^{(i)}\right\} &amp;=\mathrm{MH}-\mathrm{ATT}\left(\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}\right) \\
\left\{\tilde{\boldsymbol{e}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{e}}_{m}^{(i)}\right\} &amp;=\mathrm{MH}-\mathrm{ATT}\left(\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}\right)
\end{aligned}
\]<br>
由于直接拼接后向量维度会与其他未拼接的向量维度不同，所以加入information fusion layer，另外考虑到后面的实体自编码任务，所以这里在融合信息之后，有实体向量加入的部分需要另外多输出一个实体向量。</p>
<p>加入实体信息之后的融合输出过程：<br>
\[
\begin{aligned}
\boldsymbol{h}_{j} &amp;=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{W}}_{e}^{(i)} \tilde{\boldsymbol{e}}_{k}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\
\boldsymbol{w}_{j}^{(i)} &amp;=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \\
\boldsymbol{e}_{k}^{(i)} &amp;=\sigma\left(\boldsymbol{W}_{e}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{e}^{(i)}\right)
\end{aligned}
\]<br>
未加入实体信息之后的融合输出过程：<br>
\[
\begin{aligned}
\boldsymbol{h}_{j} &amp;=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\
\boldsymbol{w}_{j}^{(i)} &amp;=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right)
\end{aligned}
\]</p>
<h5 id="shi-ti-zi-bian-ma">实体自编码???</h5>
<p>为了更好地使用实体信息，作者在这里多加入了一个预训练任务 entity auto-encoder(dEA)。<br>
\[
p\left(e_{j} | w_{i}\right)=\frac{\exp \left(\operatorname{linear}\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{j}\right)}{\sum_{k=1}^{m} \exp \left(\operatorname{linear}\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{k}\right)}
\]<br>
该预训练任务和bert相似，按以下方式进行训练：</p>
<ol>
<li>15% 的情况下：用 [MASK] 替换被选择的单词，例如，my dog is hairy → my dog is [MASK]</li>
<li>5% 的情况下：用一个随机单词替换被选择的单词，例如，my dog is hairy → my dog is apple</li>
<li>80% 的情况下：保持被选择的单词不变，例如，my dog is hairy → my dog is hairy</li>
</ol>
<h5 id="extra-task">Extra-task</h5>
<p>除了正常的任务之外，ERNIE引入了两个新任务Entity Typing和Relation Classification</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/ernie_extra_task.png" alt></p>
<h5 id="zong-jie-2">总结</h5>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/ernie_experiment.png" alt></p>
<p>ERNIE在通用任务其实相比bert优势不大，尽管文章提到了ERNIE更鲁棒以及GLUE的评估对ERNIE不是很友好，在增加复杂度的同时，并没有取得期待的效果。ERNIE提供了一种很好的实体信息引入思路，并且其新提出的预训练方法也给希望将bert这一模型引入关系抽取领域提供了很好的例子。</p>
<h3 id="mt-dnn-yu-xun-lian-duo-ren-wu">MT-DNN（预训练+多任务）</h3>
<p><a href="https://arxiv.org/pdf/1901.11504.pdf" target="_blank" rel="noopener">论文</a>,<a href="https://github.com/namisan/mt-dnn" target="_blank" rel="noopener">源码</a></p>
<h4 id="jian-jie-4">简介</h4>
<p>学习文本的向量空间表示，是许多自然语言理解(NLU)任务的基础。两种流行的方法是<strong>多任务学习</strong>和<strong>语言模型的预训练</strong>。MT-DNN通过提出一种新的多任务深度神经网络（MT-DNN）来综合两种方法的优点。</p>
<p><strong>预训练</strong>，如BERT、GPT等，就不多说了。</p>
<p><strong>多任务</strong>学习( MTL )的灵感来自于人的学习活动。在人类的学习活动中，人们经常应用从以前的任务中学到的知识来帮助学习新的任务。例如，在学习滑冰这件事情上，一个知道如何滑雪的人比什么都不知道的人容易。同样，**联合学习多个(相关)**任务也很有用，这样在一个任务中学习到的知识有益于其他任务。对于MTL（Multi-task Learning，多任务学习）来说，其优点有两个：1）弥补了有些任务的数据不足问题；2）有正则的作用，防止模型过拟合。</p>
<p><strong>两者结合</strong>（强强联手！）：MT-DNN认为，MTL和pretrain有很好的互补作用，那么是不是可以结合一下，发挥两者的作用。更具体的就是，先用BERT进行pretrain，然后用MTL进行finetune，这就形成了MT-DNN。可见，与BERT的不同在于finetune的过程，这里用MTL作为目标。</p>
<h4 id="mo-xing-1">模型</h4>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/mtdnn.png" alt></p>
<p>表征学习 MT-DNN 模型的架构。下面的网络层在所有任务中都共享，上面的两层是针对特定任务。输入\(X\)(一句话或句子对)首先表征为一个序列的嵌入向量，在\(l_1\)  中每个词对应一个向量。然后 Transformer 编码器捕捉每个单词的语境信息并在\(l_2\)中生成共享的语境嵌入向量。最后，针对每个任务，特定任务层生成特定任务的表征，而后是分类、相似性打分、关联排序等必需的操作。</p>
<h5 id="duo-ren-wu">多任务</h5>
<p>MT-DNN是结合了4种类型的NLU任务：单句分类、句子对分类、文本相似度打分和相关度排序。</p>
<ul>
<li>单句分类：CoLA任务是预测英语句子是否合乎语法，SST-2任务预测电影评论是正向还是负向。</li>
<li>文本相似度：这是一个回归任务。对于给定的句子对，模型计算二者之间的相似度。在GLUE中只有STS-B这个任务是处理文本相似度。</li>
<li>成对文本分类（文本蕴含）：对于给定的句子对，推理两个句子之间的关系。RET和MNLI是语言推理任务，推理句子之间是否存在蕴含关系、矛盾的关系或者中立关系。QQP和MRPC是预测句子是否语义等价。</li>
<li>相关性排序：给定一个问题和一系列候选答案，模型根据问题对所有候选答案进行排序。QNLI是斯坦福问答数据集的一个版本，任务是预测候选答案中是否包含对问题的正确答案。尽管这是一个二分类任务，但我们依旧把它当作排序任务，因为模型重排了候选答案，将正确答案排在更前。</li>
</ul>
<h6 id="dan-ju-fen-lei">单句分类</h6>
<p>用[CLS]的表征作为特征，设为\(x\)，则对于单句的分类任务，直接在后面接入一个分类层即可.<br>
\[
P_{r}(c | X)=\operatorname{softmax}\left(W^{T} \cdot x\right)
\]<br>
loss是交叉熵损失：<br>
\[
-\sum_{c} I(X, c) \log \left(P_{r}(c | X)\right)
\]</p>
<h6 id="ju-zi-xiang-si-du">句子相似度</h6>
<p>将两句话pack后送进去，得到的[CLS]的表征，可拿出来计算分数：<br>
\[
\operatorname{sim}\left(X_{1}, X_{2}\right)=\operatorname{sigmoid}\left(w^{T} \cdot x\right)
\]<br>
loss是MSE：<br>
\[
\left(y-\operatorname{Sim}\left(X_{1}, X_{2}\right)\right)^{2}
\]</p>
<h6 id="ju-zi-dui-fen-lei">句子对分类</h6>
<p>对这个任务不熟悉。。。</p>
<h6 id="xiang-guan-xing-pai-xu">相关性排序</h6>
<p>先计算两个句子之间的相似度，输入两个句子pack，采用[CLS]的输出作为表征。<br>
\[
\operatorname{Rel}(Q, A)=g\left(w^{T} \cdot x\right)
\]<br>
loss采用排序损失：<br>
\[
\begin{aligned}
&amp;-\sum_{Q, A^{+}} P_{r}\left(A^{+} | Q\right) \\
P_{r}\left(A^{+} | Q\right) &amp;=\frac{\exp \left(\gamma \operatorname{Rel}\left(Q, A^{+}\right)\right)}{\sum_{A^{\prime} \in A} \exp \left(\gamma \operatorname{Rel}\left(Q, A^{\prime}\right)\right)}
\end{aligned}
\]</p>
<h5 id="xun-lian-guo-cheng">训练过程</h5>
<p>MT-DNN 的训练程序包含两个阶段：预训练和多任务微调。</p>
<ol>
<li>预训练阶段遵循 BERT 模型的方式。lexicon encoder和Transformer encoder参数的学习是通过两个无监督预测任务：掩码语言建模(masked language modeling)和下一句预测(next sentence pre-<br>
diction)。</li>
<li>在多任务微调阶段，使用基于minibatch的随机梯度下降（SGD）来学习模型参数（也就是，所有共享层和任务特定层的参数），算法流程如下图所示。每个epoch，选择一个mini-batch \(b_t\)(在9个GLUE任务中)，再对特定任务\(k\)进行模型参数的更新。这近似地优化所有多任务的和。</li>
</ol>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/mtdnn_al.png" alt></p>
<h3 id="distil-bert">DistilBERT</h3>
<p><a href="https://arxiv.org/pdf/1910.01108" target="_blank" rel="noopener">论文</a></p>
<h4 id="dong-ji-1">动机</h4>
<p>预训练的语言模型正变得日益庞大，这些庞大的模型尽管能够带来准确率的提升，但是在这些预训练模型投入使用的过程中，往往需要对模型进行微调，而这需要大量的资源，而且模型投入使用的时候，由于计算量巨大，模型处理数据的时延过长。一种比较好的解决方案是对模型进行压缩。</p>
<h4 id="mo-xing-ya-suo-de-fang-fa">模型压缩的方法</h4>
<ol>
<li>
<p>量化</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/quantization.jpg" alt></p>
</li>
</ol>
<p>量化方法意味着降低模型权重的精度。比如K-means 量化方法：对模型权重矩阵W进行分组，分成n簇，然后把权重矩阵里的值转化成\(1,\cdots,n\)的正数。通过这种方式把矩阵中的32位的float型转化成只有8位（或者1位，binarizing matrix）的整数。</p>
<h5 id="cai-jian-pruning">裁剪（Pruning）</h5>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/Pruning.png" alt></p>
<h6 id="weight-pruning">weight pruning</h6>
<p>把权重矩阵中数值较大的值设置为0，从而构造稀疏矩阵。（理论上稀疏矩阵乘法速度快于normal(danse)矩阵乘法。）</p>
<h6 id="removing-neurons">Removing neurons</h6>
<p>直接删除权重矩阵中不重要的的某一行或某一列。（删除和fine-tune交替进行，这样剩下的神经元在一定程度上可以弥补被删除的神经元）</p>
<h6 id="removing-weight-matrices">Removing weight matrices</h6>
<p>[10]从big transformers中直接删除对accuracy贡献不大的整个attention head。但是这种方式不能保证能够并行加速，因为相邻的权重矩阵的size变的不同了。</p>
<h5 id="zhi-shi-zheng-liu">知识蒸馏</h5>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/Knowledge-distillation.jpg" alt></p>
<p>DistilBert使用模型蒸馏技术：这是一种能够将大型模型（被称为「老师」）压缩为较小模型（即「学生」）的技术。</p>
<p><a href="https://blog.csdn.net/nature553863/article/details/80568658" target="_blank" rel="noopener">知识蒸馏方法</a></p>
<h6 id="zhi-shi-zheng-liu-qian-yi-fan-hua-neng-li">知识蒸馏：迁移泛化能力</h6>
<p>知识蒸馏是一种压缩技术，要求对小型模型进行训练，以使其拥有类似于大型模型的行为特征。在监督学习领域，我们在训练分类模型时往往会利用对数似然信号实现概率最大化（logits 的 softmax），进而预测出正确类。在大多数情况下，性能良好的模型能够利用具有高概率的正确类预测输出分布，同时其它类的发生概率则接近于零。</p>
<p><strong>但是，某些“接近于零”的概率要比其它概率更大，这在一定程度上反映出模型的泛化能力</strong>。例如，把普通椅子误认为扶手椅虽然属于错误，但这种错误远比将其误认为蘑菇来得轻微。这种不确定性，有时被称为“暗知识”。也可以从另一个角度来理解蒸馏——用于防止模型对预测结果太过确定（类似于标签平滑）。</p>
<p>在语言建模当中，可以通过查看词汇表中的分布轻松观察到这种不确定性。下图为 BERT 对《卡萨布兰卡》电影当中经典台词下一句用词的猜测：</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/next_word.png" alt></p>
<p>BERT 提出的 20 大高概率用词猜测结果。语言模型确定了两个可能性最高的选项（day 与 life），接下来的词汇相比之下概率要低得多。</p>
<h6 id="ru-he-fu-zhi-zhe-xie-an-zhi-shi">如何复制这些“暗知识”</h6>
<p>在训练当中，通过训练学生网络，来模拟老师网络的全部输出分布（也就是知识），也就是通过匹配输出分布的方式训练学生网络，从而实现与老师网络相同的泛化方式，即通过软目标（老师概率）将交叉熵从老师处传递给学生。<br>
\[
L=-\sum_{i} t_{i} * \log \left(s_{i}\right)
\]<br>
其中 \(t_i\)为来自老师的 \(logit\)，\(s_i\) 为学生的 \(logit\)。以老师的行为\(t_i\)作为标签，让学生去学习老师的行为。**这个损失函数属于更丰富的训练信号，因为单一示例要比单一硬目标拥有更高的强制约束效果。**为了进一步揭示分类结果的质量，Hinton 等人提出了 softmax 温度的概念：<br>
\[
p_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}
\]<br>
T 为温度参数。当\(T \rightarrow 0\)时，分布变为 Kronecker（相当于独热目标矢量）；当 \(T \rightarrow +\infty\)时，则变为均匀分布。在训练过程中，将相同的温度参数应用于学生与老师网络，即可进一步为每个训练示例揭示更多信号。T 被设置为 1 即为标准 Softmax。</p>
<p>在蒸馏方面，使用 Kullback-Leibler 损失函数，因为其拥有相同的优化效果：<br>
\[
K L(p \| q)=\mathbb{E}_{p}\left(\log \left(\frac{p}{q}\right)\right)=\sum_{i} p_{i} * \log \left(p_{i}\right)-\sum_{i} p_{i} * \log \left(q_{i}\right)
\]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line">KD_loss = nn.KLDivLoss(reduction=<span class="string">'batchmean'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kd_step</span><span class="params">(teacher: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">            student: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">            temperature: float,</span></span></span><br><span class="line"><span class="function"><span class="params">            inputs: torch.tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            optimizer: Optimizer)</span>:</span></span><br><span class="line">    teacher.eval()</span><br><span class="line">    student.train()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        logits_t = teacher(inputs=inputs) <span class="comment"># 老师的输出</span></span><br><span class="line">    logits_s = student(inputs=inputs) <span class="comment"># 学生的输出</span></span><br><span class="line">    <span class="comment"># 计算kd_loss</span></span><br><span class="line">    loss = KD_loss(input=F.log_softmax(logits_s/temperature, dim=<span class="number">-1</span>),</span><br><span class="line">                   target=F.softmax(logits_t/temperature, dim=<span class="number">-1</span>))</span><br><span class="line">    </span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<p>利用老师信号，能够训练出一套较小的语言模型 DistilBERT，属于 BERT 的监督产物。</p>
<h4 id="mo-xing-jie-gou">模型结构</h4>
<p>DistilBERT想较于BERT删除了 token-type 嵌入与 pooler（用于下一句分类任务），但其余部分架构保持不变，而层数也减少至原本的二分之一。总体而言，蒸馏模型 DistilBERT 在总体参数数量上约为 BERT 的一半，但在 GLUE 语言理解基准测试中能够保留 95% 的 BERT 性能表现。</p>
<ol>
<li><strong>为什么不降低隐藏层的大小？</strong><br>
将 768 减至 512 ，意味着总参数量约下降至原本的二分之一。在现代框架当中，大多数运算都经过高度优化，而且张量的最终维度（隐藏维度）的变化会对 Transformer 架构（线性分层与层规范化）中的大部分运算产生小幅影响。实验中发现，层数对于推理时间的影响要远高于隐藏层的大小。因此，更小并不代表着一定更快。</li>
</ol>
<p>训练子网络的核心不只是建立架构，还要求为子网络找到正确的初始化方式以实现收敛。 DistilBERT 使用bert的参数进行初始化，将层数削减一半，并采用相同的隐藏大小。DistilBERT还用到了最近 RoBERTa 论文当中提到的一些训练技巧，这也再次证明 BERT 模型的训练方式对其最终表现有着至关重要的影响。与 RoBERTa 类似，对 DIstilBERT 进行大批次训练，使用梯度累积（每批最多 4000 个例子）、配合动态遮挡并删除了下一句预测目标。</p>
<h4 id="zong-jie-3">总结</h4>
<p>DistilBERT 的表现：保留了BERT  95% 以上的性能，同时将参数减少了 40%。推理时间方面，DistilBERT 比 BERT 快 60%，体积比 BERT 小 60%。</p>
<h3 id="albert">ALBERT</h3>
<p><a href="%5BPDF%5D(https://arxiv.org/pdf/1909.11942)">论文</a></p>
<h4 id="jian-jie-5">简介</h4>
<p>自BERT的成功以来，预训练模型都采用了很大的参数量以取得更好的模型表现。但是模型参数量越来越大也带来了很多问题，比如对算力要求越来越高、模型需要更长的时间去训练、甚至有些情况下参数量更大的模型表现却更差。为了解决目前预训练模型参数量过大的问题，albert提出了两种能够大幅减少预训练模型参数量的方法，此外还提出用Sentence-order prediction（SOP）任务代替BERT中的Next-sentence prediction（NSP）任务。</p>
<h4 id="mo-xing-2">模型</h4>
<h5 id="qian-ru-xiang-liang-yin-shi-fen-jie-factorized-embedding-parameterization">嵌入向量因式分解（Factorized embedding parameterization）</h5>
<p>在BERT、XLNet、RoBERTa等模型中，由于模型结构的限制，WordePiece embedding的大小\(E\) 总是与隐层大小\(H\)相同，即 \(E \equiv H\) 。从建模的角度考虑，词嵌入学习的是单词与上下文无关的表示，而隐层则是学习与上下文相关的表示。显然后者更加复杂，需要更多的参数，也就是说模型应当增大隐层大小\(H\)，或者说满足 \(H \gg E\)。但实际上词汇表的大小\(V\)通常非常大，如果\(E=H\)的话，增加隐层大小\(H\)后将会使embedding matrix的维度\(V \times E\)非常巨大。</p>
<p>因此ALBERT想要打破\(E\)与\(H\) 之间的绑定关系，从而减小模型的参数量，同时提升模型表现。具体做法是将embedding matrix分解为两个大小分别为\(V \times  E\) 和\(E \times H\) 矩阵，也就是说先将单词投影到一个低维的embedding空间 \(E\) ，再将其投影到高维的隐藏空间\(H\)  。这使得embedding matrix的维度从\(O(V \times H)\)  减小到\(O(v \times E + E \times H)\) 。当 \(H \gg E\)时，参数量减少非常明显。在实现时，随机初始化\(V\times E\)和\(E\times H\)的矩阵，计算某个单词的表示需用一个单词的one-hot向量乘以\(V\times E\)维的矩阵（也就是lookup），再用得到的结果乘\(E\times H\)维的矩阵即可。两个矩阵的参数通过模型学习。</p>
<p>从下图实验结果可见，对于不共享参数的情况，\(E\)几乎是与大越好；而共享参数之后， \(E\)太大反而会使模型表现变差，\(E=128\)模型表现最好，因此ALBERT的默认参数设置中\(E=128\).</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/albert_embedding.png" alt></p>
<p>考虑到ALBERT-base的 \(h=768\)，那么 \(E=768\)时，模型应该可以看作没有减少embedding参数量的情况。而不共享参数的实验结果表明此时模型表现更好，那么似乎说明了Factorized embedding在一定程度上降低了模型的表现。</p>
<h5 id="kua-ceng-can-shu-gong-xiang-can-shu-liang-jian-shao-zhu-yao-gong-xiang">跨层参数共享（参数量减少主要共享）</h5>
<p>另一个减少参数量的方法就是层之间的参数共享，即多个层使用相同的参数。参数共享有三种方式：只共享feed-forward network的参数、只共享attention的参数、共享全部参数。ALBERT默认是共享全部参数的。实验表明加入参数共享之后，每一层的输入embedding和输出embedding的L2距离和余弦相似度都比BERT稳定了很多。这证明参数共享能够使模型参数更加稳定。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/cross.png" alt></p>
<p>如下图所示，可以看出参数共享几乎也是对模型结果有负面影响的。但是考虑到其能够大幅削减参数量，并且对结果影响不是特别大，因此权衡之下选择了参数共享。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/albert_crosslayer.png" alt></p>
<h5 id="ju-jian-lian-guan-xing">句间连贯性</h5>
<ul>
<li><strong>NSP</strong>：下一句预测， 正样本=上下相邻的2个句子，负样本=随机2个句子</li>
<li><strong>SOP</strong>：句子顺序预测，正样本=正常顺序的2个相邻句子，负样本=调换顺序的2个相邻句子</li>
</ul>
<p>NSP任务过于简单，只要模型发现两个句子的主题不一样就行了，所以SOP预测任务能够让模型学习到更多的信息。</p>
<p>除了减少模型参数外，还对BERT的预训练任务Next-sentence prediction (NSP)进行了改进。在BERT中，NSP任务的正例是文章中连续的两个句子，而负例则是从两篇文档中各选一个句子构造而成。在先前的研究中，已经证明NSP是并不是一个合适的预训练任务。作者推测其原因是模型在判断两个句子的关系时不仅考虑了两个句子之间的连贯性（coherence），还会考虑到两个句子的话题（topic）。而两篇文档的话题通常不同，模型会更多的通过话题去分析两个句子的关系，而不是句子间的连贯性，这使得NSP任务变成了一个相对简单的任务。</p>
<p>因此本文提出了Sentence-order prediction (SOP)来取代NSP。具体来说，其正例与NSP相同，但负例是通过选择一篇文档中的两个连续的句子并将它们的顺序交换构造的。这样两个句子就会有相同的话题，模型学习到的就更多是句子间的连贯性。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/albert_sop.png" alt></p>
<p>如上实验结果所示，如果不使用NSP或SOP作为预训练任务的话，模型在NSP和SOP两个任务上表现都很差；如果使用NSP作为预训练任务的话，模型确实能很好的解决NSP问题，但是在SOP问题上表现却很差，几乎相当于随机猜，因此说明NSP任务确实很难学到句子间的连贯性；而如果用SOP作为预训练任务，则模型也可以较好的解决NSP问题，同时模型在下游任务上表现也更好。说明SOP确实是更好的预训练任务。</p>
<h4 id="qi-ta">其他</h4>
<h5 id="e-wai-de-xun-lian-shu-ju-he-dropout">额外的训练数据和dropout</h5>
<p>ALBERT训练时还加入了XLNet和RoBERTa训练时用的额外数据，实验表明加入额外数据（W additional data）确实会提升模型表现。此外，作者还观察到模型似乎一直没有过拟合数据，因此去除了Dropout，从对比试验可以看出，去除Dropout（W/O Dropout）后模型表现确实更好。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/albert_dropout.png" alt></p>
<h4 id="zong-jie-4">总结</h4>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/albert_bert.png" alt></p>
<p>ALBERT的训练速度明显比BERT快，ALBERT-xxlarge的表现更是全方面超过了BERT。</p>
<p>本文有两个小细节可以学习，一个是在模型不会过拟合的情况下不使用dropout；另一个则是warm-start，即在训练深层网络（例如12层）时，可以先训练浅层网络（例如6层），再在其基础上做fine-tune，这样可以加快深层模型的收敛。</p>
<h3 id="tiny-bert">TinyBERT</h3>
<p><a href="https://arxiv.org/abs/1909.10351" target="_blank" rel="noopener">论文</a></p>
<h4 id="jian-jie-6">简介</h4>
<p>在 NLP 领域，BERT 由于模型过于庞大，单个样本计算一次的开销动辄上百毫秒，很难应用到实际生产中。中科技大学、华为诺亚方舟实验室的研究者提出了 TinyBERT，这是一种为基于 transformer 的模型专门设计的知识蒸馏方法，模型大小为 BERT 的 13.3%，推理速度是 BERT 的 9.4 倍，而且性能没有出现明显下降。</p>
<p>目前主流的几种蒸馏方法大概分成 1. 利用 transformer 结构蒸馏, 2. 利用其它简单的结构比如 BiLSTM 等蒸馏。由于 BiLSTM 等结构简单，且一般是用 BERT 最后一层的输出结果进行蒸馏，不能学到 transformer 中间层的信息，对于复杂的语义匹配任务，效果有点不尽人意。</p>
<p>基于 transformer 结构的蒸馏方法目前比较出名的有微软的 BERT-PKD (Patient Knowledge Distillation for BERT)，huggingface 的 DistilBERT，以及华为的 TinyBERT。他们的基本思路都是减少 transformer encoding 的层数和 hidden size 大小，实现细节上各有不同，主要差异体现在 loss 的设计上。</p>
<h4 id="mo-xing-3">模型</h4>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/tinyBert.jpg" alt></p>
<p>整个 TinyBERT 的 loss 设计分为三部分：</p>
<h5 id="embedding-layer-distillation">Embedding-layer Distillation</h5>
<p>\[
\mathcal{L}_{\mathrm{embd}}=\operatorname{MSE}\left(\boldsymbol{E}^{S} \boldsymbol{W}_{e}, \boldsymbol{E}^{T}\right)
\]<br>
其中，\(E^s \in R^{l \times ds}\),\(E^T \ in R^{l \times dt}\)分别表示 student 网络的 embedding 和 teacher 网络的 embedding. 其中 l 代表 sequence length, ds 代表 student embedding 维度， dt 代表 teacher embedding 维度。由于 student 网络的 embedding 层通常较 teacher 会变小以获得更小的模型和加速，所以 \(W_e\) 是一个 \(d_s \times d_t\)维的可训练的线性变换矩阵，把 student 的 embedding 投影到 teacher embedding 所在的空间。最后再算 MSE，得到 embedding loss.</p>
<h5 id="transformer-layer-distillation">Transformer-layer Distillation</h5>
<p>TinyBERT 的 transformer 蒸馏采用隔 k 层蒸馏的方式。举个例子，teacher BERT 一共有 12 层，若是设置 student BERT 为 4 层，就是每隔 3 层计算一个 transformer loss. 映射函数为 g(m) = 3 * m(???), m 为 student encoder 层数。具体对应为 student 第 1 层 transformer 对应 teacher 第 3 层，第 2 层对应第 6 层，第 3 层对应第 9 层，第 4 层对应第 12 层。每一层的 transformer loss 又分为两部分组成，attention based distillation 和 hidden states based distillation.</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/transormers_distillation.jpg" alt></p>
<h6 id="attention-based-loss">Attention based loss</h6>
<p>\[
\mathcal{L}_{\mathrm{attn}}=\frac{1}{h} \sum_{i=1}^{h} \operatorname{MSE}\left(\boldsymbol{A}_{i}^{S}, \boldsymbol{A}_{i}^{T}\right)
\]</p>
<p>其中，\(A_i \in R^{l \times l}\) \(h\)代表attention的头数，\(l\)代表输入长度，\(A_i^S\)代表student网络第i个attention头的attention score矩阵。\(A_i^T\)代表teacher网络的第i个attention头的attention score 矩阵。为什么要做这个损失？在What Does BERT Look At? An Analysis of BERT’s Attention [12]中研究了attention 权重到底学到了什么，实验发现与语义还有语法相关的词比如第一个动词宾语，第一个介词宾语，以及[CLS], [SEP], 逗号等 token，有很高的注意力权重。为了确保这部分信息能被 student 网络学到，TinyBERT 在 loss 设计中加上了 student 和 teacher 的 attention matrix 的 MSE。这样语言知识可以很好的从 teacher BERT 转移到 student BERT.</p>
<h6 id="hidden-states-based-distillation">hidden states based distillation</h6>
<p>\[
\mathcal{L}_{\mathrm{hidn}}=\mathrm{MSE}\left(\boldsymbol{H}^{S} \boldsymbol{W}_{h}, \boldsymbol{H}^{T}\right)
\]</p>
<p>其中，\(H^S \in R^{l \times ds}\),\(H^T \in R^{l \times dt}\)分别是 student transformer 和 teacher transformer 的隐层输出。和 embedding loss 同理，\(W_h\)把\(H^S\)投影到\(H^T\)所在的空间。</p>
<h6 id="prediction-layer-distillation">Prediction-Layer Distillation</h6>
<p>\[
\mathcal{L}_{\text {pred }}=-\operatorname{softmax}\left(\boldsymbol{z}^{T}\right) \cdot \log \_ \left(\boldsymbol{z}^{S} / t\right)
\]</p>
<p>其中 t 是 temperature value，暂时设为 1.除了模仿中间层的行为外，这一层用来模拟 teacher 网络在 predict 层的表现。具体来说，这一层计算了 teacher 输出的概率分布和 student 输出的概率分布的 softmax 交叉熵。这一层的实现和具体任务相关.</p>
<p>prediction loss 有很多变化。</p>
<ol>
<li>在 TinyBERT 中，这个 loss 是 teacher BERT 预测的概率和 student BERT 预测概率的 softmax 交叉熵.</li>
<li>在 BERT-PKD 模型中，这个 loss 是 teacher BERT 和 student BERT 的交叉熵和 student BERT 和 hard target( one-hot)的交叉熵的加权平均。</li>
</ol>
<p>直接用 hard target loss，效果比使用 teacher student softmax 交叉熵下降 5-6 个点。因为 softmax 比 one-hot 编码了更多概率分布的信息。并且softmax cross-entropy loss 容易发生不收敛的情况，把 softmax 交叉熵改成 MSE, 收敛效果变好，但泛化效果变差。这是因为使用 softmax cross-entropy 需要学到整个概率分布，更难收敛，因为拟合了 teacher BERT 的概率分布，有更强的泛化性。MSE 对极值敏感，收敛的更快，但泛化效果不如前者。</p>
<p>所以总的loss：<br>
\[
\mathcal{L}_{\text {model }}=\sum_{m=0}^{M+1} \lambda_{m} \mathcal{L}_{\text {layer }}\left(S_{m}, T_{g(m)}\right)
\]<br>
其中<br>
\[
\mathcal{L}_{\text {layer }}\left(S_{m}, T_{g(m)}\right)=\left\{\begin{array}{ll}
\mathcal{L}_{\text {embd }}\left(S_{0}, T_{0}\right), &amp; m=0 \\
\mathcal{L}_{\text {hidn }}\left(S_{m}, T_{g(m)}\right)+\mathcal{L}_{\text {attn }}\left(S_{m}, T_{g(m)}\right), &amp; M \geq m>0 \\
\mathcal{L}_{\text {pred }}\left(S_{M+1}, T_{N+1}\right), &amp; m=M+1
\end{array}\right.
\]</p>
<h4 id="liang-jie-duan-xue-xi-kuang-jia">两阶段学习框架</h4>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/two_step.jpg" alt></p>
<p>类似于原生的 BERT 先 pre-train, 根据具体任务再 fine-tine。TinyBERT 先在 general domain 数据集上用未经微调的 BERT 充当教师蒸馏出一个 base 模型，在此基础上，具体任务通过数据增强，利用微调后的 BERT 再进行重新执行蒸馏。这种两阶段的方法给 TinyBERT 提供了像 BERT 一样的泛化能力。</p>
<h4 id="zong-jie-5">总结</h4>
<p>TinyBERT 作为一种蒸馏方法，能有效的提取 BERT transformer 结构中丰富的语意信息，在不牺牲性能的情况下，速度能获得 8 到 9 倍的提升。</p>
<h3 id="fast-bert">FastBert</h3>
<h4 id="jian-jie-7">简介</h4>
<p>FastBERT的创新点很容易理解，就是在每层Transformer后都去预测样本标签，如果某样本预测结果的置信度很高，就不用继续计算了。论文把这个逻辑称为样本自适应机制（Sample-wise adaptive mechanism），就是自适应调整每个样本的计算量，容易的样本通过一两层就可以预测出来，较难的样本则需要走完全程。</p>
<p>举个例子，比如 text_a = ‘北京鲜花快递’ text_b = ‘北京鲜花速递’ 这种case可能浅层的bert就已经能够很好的算出其相关性打分，所以算完两层bert的打分和12层的结果基本一致，而text_a = ‘北京鲜花快递’ text_b = ‘有没有哪个地方卖花卉，而且包送，位置北京’ 这种case可能需要深层的bert去抽取两边的语义特征，并计算其是否匹配，所以在inference阶段就需要算完12层。</p>
<p><img src="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/fastbert.jpg" alt></p>
<p>作者将原BERT模型称为主干（Backbone），每个分类器称为分支（Branch）。这里的分支Classifier都是最后一层的分类器蒸馏来的，作者将这称为自蒸馏（Self-distillation）。就是在预训练和精调阶段都只更新主干参数，精调完后<strong>freeze主干参数，用分支分类器（图中的student）蒸馏主干分类器（图中的teacher）的概率分布</strong>。之所以叫自蒸馏，是因为之前的蒸馏都是用两个模型去做，一个模型学习另一个模型的知识，而FastBERT是自己（分支）蒸馏自己（主干）的知识。值得注意的是，蒸馏时需要freeze主干部分，保证pretrain和finetune阶段学习的知识不被影响，仅用brach 来尽可能的拟合teacher的分布。同时，使用自蒸馏还有一点重要的好处，就是<strong>不再依赖于标注数据</strong>。蒸馏的效果可以通过源源不断的无标签数据来提升。</p>
<h4 id="zong-jie-6">总结</h4>
<p>FastBERT通过提前输出简单样本的预测结果，减少模型的计算负担，从而提高推理速度。虽然每层都多了一个分类器，但分类器的计算量也比Transformer小了两个数量级，对速度影响较小。后续的分支自蒸馏也设计的比较巧妙，可以利用无监督数据不断提升分支分类器的效果。</p>
<h3 id="zong-jie-7">总结</h3>
<p>性能：</p>
<ol>
<li>BERT-WWM</li>
<li>XLNet（长文本表现更好一些）</li>
<li>RoBERTa（动态mask、without NSP、更大batch，更多数据，更长训练时间）</li>
<li>SpanBERT(基于分词的预训练模型)</li>
<li>MT-DNN(预训练+多任务)</li>
<li>ERNIR（引入实体信息）</li>
</ol>
<p>效率：</p>
<ol>
<li>DistilBert（知识蒸馏）</li>
<li>ALBert（因式分解、跨层权重共享）</li>
<li>TinyBert（知识蒸馏）</li>
</ol>
<h3 id="can-kao">参考</h3>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/46652512" target="_blank" rel="noopener">【NLP】Google BERT详解</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/81157740" target="_blank" rel="noopener">带你读论文丨8篇论文梳理BERT相关模型进展与反思</a></li>
<li><a href="https://blog.csdn.net/u012526436/article/details/93196139" target="_blank" rel="noopener">最通俗易懂的XLNET详解</a></li>
<li><a href="https://www.jianshu.com/p/eddf04ba8545" target="_blank" rel="noopener">改进版的RoBERTa到底改进了什么？</a></li>
<li><a href="https://www.bilibili.com/video/av73657563/" target="_blank" rel="noopener">【AI模型】最通俗易懂的XLNet详解</a></li>
<li>Liu Y, Ott M, Goyal N, et al. Roberta: A robustly optimized bert pretraining approach[J]. arXiv preprint arXiv:1907.11692, 2019</li>
<li>Sennrich R, Haddow B, Birch A. Neural machine translation of rare words with subword units[J]. arXiv preprint arXiv:1508.07909, 2015.</li>
<li><a href="http://fancyerii.github.io/2019/03/09/bert-codes/#dataprocessor" target="_blank" rel="noopener">bert代码阅读</a></li>
<li><a href="https://www.infoq.cn/article/STabowUeFupgc4gRqRQj" target="_blank" rel="noopener">更小、更快、更便宜、更轻量：开源 DistilBERT，BERT 的精简版本</a></li>
<li>Michel, P., Levy, O., &amp; Neubig, G. (2019). Are Sixteen Heads Really Better than One? Retrieved from <a href="https://arxiv.org/abs/1905.10650" target="_blank" rel="noopener">https://arxiv.org/abs/1905.10650</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/94359189" target="_blank" rel="noopener">比 Bert 体积更小速度更快的 TinyBERT</a></li>
<li><a href="https://arxiv.org/abs/1906.04341" target="_blank" rel="noopener">https://arxiv.org/abs/1906.04341</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/87562926" target="_blank" rel="noopener">【论文阅读】ALBERT</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/59436589" target="_blank" rel="noopener">中文任务全面超越BERT：百度正式发布NLP预训练模型ERNIE</a></li>
<li><a href="https://www.jianshu.com/p/5e12e6edbd59" target="_blank" rel="noopener">BERT泛读系列（三）—— ERNIE: Enhanced Language Representation with Informative Entities论文笔记</a></li>
<li><a href="https://blog.csdn.net/ljp1919/article/details/90269059" target="_blank" rel="noopener">文献阅读：MT-DNN模型</a></li>
<li><a href="https://blog.csdn.net/Magical_Bubble/article/details/89517709" target="_blank" rel="noopener">MT-DNN解读(论文 + PyTorch源码)</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>BERT-WWM</tag>
        <tag>XLNet</tag>
        <tag>RoBERTa</tag>
        <tag>SpanBERT</tag>
        <tag>ernie</tag>
        <tag>MT-DNN</tag>
        <tag>DistillBERT</tag>
        <tag>ALBERT</tag>
        <tag>TinyBERT</tag>
      </tags>
  </entry>
  <entry>
    <title>文本多标签分类</title>
    <url>/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>对于事件抽取中单句多事件的问题，打算用文本多标签分类来进行解决。</p>
<h3 id="nan-dian">难点</h3>
<ol>
<li>类标数量不确定，有些样本可能只有一个类标，有些样本的类标可能高达几十甚至上百个。</li>
<li>类标之间相互依赖，例如包含蓝天类标的样本很大概率上包含白云，如何解决类标之间的依赖性问题也是一大难点。</li>
<li>多标签的训练集比较难以获取。</li>
</ol>
<a id="more"></a>
<h3 id="fang-fa">方法</h3>
<p>前有很多关于多标签的学习算法，依据解决问题的角度，这些算法可以分为两大类:一是基于问题转化的方法，二是基于算法适用的方法。基于问题转化的方法是转化问题数据，使之使用现有算法；基于算法适用的方法是指针对某一特定的算法进行扩展，从而能够处理多标记数据，改进算法，适用数据。</p>
<h4 id="strong-ji-yu-wen-ti-zhuan-hua-de-fang-fa-strong"><strong>基于问题转化的方法</strong></h4>
<p>基于问题转化的方法中有的考虑标签之间的关联性，有的不考虑标签的关联性。最简单的不考虑关联性的算法将多标签中的每一个标签当成是单标签，对每一个标签实施常见的分类算法。具体而言，在传统机器学习的模型中对每一类标签做二分类，可以使用SVM、DT、Naive Bayes、DT、Xgboost等算法；在深度学习中，对每一类训练一个文本分类模型(如：textCNN、textRNN等)。考虑多标签的相关性时候可以将上一个输出的标签当成是下一个标签分类器的输入。在传统机器学习模型中可以使用<strong>分类器链</strong>，在这种情况下，第一个分类器只在输入数据上进行训练，然后每个分类器都在输入空间和链上的所有之前的分类器上进行训练。让我们试着通过一个例子来理解这个问题。在下面给出的数据集里，我们将X作为输入空间，而Y作为标签。在分类器链中，这个问题将被转换成4个不同的标签问题，就像下面所示。黄色部分是输入空间，白色部分代表目标变量。</p>
<p><img src="/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/1.png" alt="avatar"></p>
<p>在深度学习中，于输出层加上一个时序模型，将每一时刻输入的数据序列中加入上一时刻输出的结果值。在获得文章的整体语义(Text feature vector)后，将Text feature vector输入到一个RNN的序列中作为初始值，每一时刻输入是上一时刻的输出。从某种程度上来说，下图所示的模型将多标签任务当成了序列生成任务来处理。</p>
<p><img src="/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/cnn_rnn.png" alt="avatar"></p>
<p>除了将标签分开看之外，还有将标签统一来看(Label Powerset)。在这方面，我们将问题转化为一个多类问题，一个多类分类器在训练数据中发现的所有唯一的标签组合上被训练。让我们通过一个例子来理解它。</p>
<p><img src="/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/multi_one.png" alt="avatar"></p>
<p>在这一点上，我们发现x1和x4有相同的标签。同样的，x3和x6有相同的标签。因此，标签powerset将这个问题转换为一个单一的多类问题，如下所示。</p>
<p><img src="/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/multi_two.png" alt="avatar"></p>
<p>因此，标签powerset给训练集中的每一个可能的标签组合提供了一个独特的类。转化为单标签后就可以使用SVM、textCNN、textRNN等分类算法训练模型了。</p>
<p>感觉Label Powerset只适合标签数少的数据，一旦标签数目太多(假设有n个)，使用Label Powerset后可能的数据集将分布在[0,\(2^{n-1}\)]空间内，数据会很稀疏。</p>
<h4 id="strong-ji-yu-suan-fa-gua-yong-de-fang-fa-strong"><strong>基于算法适用的方法</strong></h4>
<p>改变算法来直接执行多标签分类，而不是将问题转化为不同的问题子集。在传统机器学习模型中适用于的多标签分类模型有:kNN多标签版本MLkNN，SVM的多标签版本Rank-SVM等。在深度学习中常常是修改多分类模型的输出层，使其适用于多标签的分类，如：在输出层对每一个标签的输出值使用sigmod函数进行2分类(标签之间无关联信息)。或者使用一般的CNN提取句子的语义信息，但考虑到句子表示的不同部分在句子分类的过程中会起不同的作用，故在进行分类时候使用了Attention机制(见图)，使得在预测每一类时候句子的不同部分的表示起不同的作用。</p>
<p><img src="/2020/04/11/%E6%96%87%E6%9C%AC%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/3.png" alt="avatar"></p>
<h3 id="zi-yuan">资源</h3>
<ol>
<li><a href="https://ai.baidu.com/tech/nlp/doctagger" target="_blank" rel="noopener">百度AI开放平台(文章标签)</a></li>
<li><a href="https://github.com/inspirehep/magpie" target="_blank" rel="noopener">Magpie(多标签分类工具)</a></li>
<li><a href="https://biendata.com/competition/zhihu/" target="_blank" rel="noopener">2017知乎“看山杯”比赛</a></li>
<li>[英文新闻多 标签分类数据集](<a href="https://drive.google.com/file/d/18-JOCIj9v5bZCrn9CIsk23W4wyhroCp_/view?usp=sharing" target="_blank" rel="noopener">https://drive.google.com/file/d/18-JOCIj9v5bZCrn9CIsk23W4wyhroCp_/view?usp=sharing</a></li>
<li>[网上可采集的、质量较高的多标签分类数据集(<a href="https://movie.douban.com/" target="_blank" rel="noopener">豆瓣电影评论</a>)</li>
<li>GitHub上比较完善的<a href="https://github.com/brightmart/text_classification" target="_blank" rel="noopener">文本多标签分类项目</a></li>
<li>NLPCC2018，task6和tesk8均可以当成是多标签的分类任务；</li>
</ol>
<h3 id="can-kao">参考</h3>
<ol>
<li><a href="https://www.cnblogs.com/cxf-zzj/p/10049613.html" target="_blank" rel="noopener">多标签分类(multi-label classification)综述</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>序列标注</title>
    <url>/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>后面需要补充序列标注在分词、词性标注、命名实体识别等相关任务的代码实践(走过的坑。。)及相关理论知识。</p>
<h3 id="tiao-jian-sui-ji-chang-crf">条件随机场(CRF)</h3>
<h4 id="crf-yong-yu-xu-lie-biao-zhu-wen-ti">crf 用于序列标注问题</h4>
<p>在 CRF 的序列标注问题中，要计算的是条件概率：<br>
\[
P\left(y_{1}, \ldots, y_{n} | x_{1}, \ldots, x_{n}\right)=P\left(y_{1}, \ldots, y_{n} | \boldsymbol{x}\right), \quad \boldsymbol{x}=\left(x_{1}, \ldots, x_{n}\right) \tag{1}
\]</p>
<a id="more"></a>
<p>为了得到这个概率的估计，CRF 做了两个假设：</p>
<h5 id="strong-jia-she-yi-gai-fen-bu-shi-zhi-shu-zu-fen-bu-strong"><strong>假设一：该分布是指数族分布。</strong></h5>
<p>这个假设意味着存在函数 \(f(y_1,…,y_n;x)\)，使得：<br>
\[
P\left(y_{1}, \ldots, y_{n} | \boldsymbol{x}\right)=\frac{1}{Z(\boldsymbol{x})} \exp \left(f\left(y_{1}, \ldots, y_{n} ; \boldsymbol{x}\right)\right) \tag{2}
\]<br>
其中 Z(x) 是归一化因子，因为这个是条件分布，所以归一化因子跟 x 有关。这个 f 函数可以视为一个打分函数，打分函数取指数并归一化后就得到概率分布。</p>
<h5 id="strong-jia-she-er-shu-chu-zhi-jian-de-guan-lian-jin-fa-sheng-zai-xiang-lin-wei-zhi-bing-qie-guan-lian-shi-zhi-shu-jia-xing-de-strong"><strong>假设二：输出之间的关联仅发生在相邻位置，并且关联是指数加性的。</strong></h5>
<p>这个假设意味着 f(y1,…,yn;x) 可以更进一步简化为：<br>
\[
\begin{aligned}
f\left(y_{1}, \ldots, y_{n} ; \boldsymbol{x}\right)=h\left(y_{1} ; \boldsymbol{x}\right) &amp;+g\left(y_{1}, y_{2} ; \boldsymbol{x}\right)+h\left(y_{2} ; \boldsymbol{x}\right)+\ldots \\
&amp;+g\left(y_{n-1}, y_{n} ; \boldsymbol{x}\right)+h\left(y_{n} ; \boldsymbol{x}\right)
\end{aligned} \tag{3}
\]<br>
这也就是说，现在我们只需要对每一个标签和每一个相邻标签对分别打分，然后将所有打分结果求和得到总分。</p>
<h5 id="xian-xing-lian-crf">线性链crf</h5>
<p>尽管已经做了大量简化，但一般来说，(3)式所表示的概率模型还是过于复杂，难以求解。<strong>于是考虑到当前深度学习模型中，RNN 或者层叠 CNN 等模型已经能够比较充分捕捉各个 y 与输出 x 的联系</strong>，因此，不妨考虑函数 g 跟 x 无关，那么：<br>
\[
\begin{aligned}
f\left(y_{1}, \ldots, y_{n} ; \boldsymbol{x}\right)=h\left(y_{1} ; \boldsymbol{x}\right) &amp;+g\left(y_{1}, y_{2}\right)+h\left(y_{2} ; \boldsymbol{x}\right)+\ldots \\
&amp;+g\left(y_{n-1}, y_{n}\right)+h\left(y_{n} ; \boldsymbol{x}\right)
\end{aligned}
\]<br>
这时候 g 实际上就是一个有限的、待训练的参数矩阵而已，而单标签的打分函数 \(h(y_i;x\)) 可以通过 RNN 或者 CNN 来建模。因此，该模型是可以建立的，其中概率分布变为：<br>
\[
P\left(y_{1}, \ldots, y_{n} | x\right)=\frac{1}{Z(x)} \exp \left(h\left(y_{1} ; x\right)+\sum_{k=1}^{n-1} g\left(y_{k}, y_{k+1}\right)+h\left(y_{k+1} ; x\right)\right)
\]</p>
<h5 id="gui-yi-hua-yin-zi">归一化因子</h5>
<p>为了训练 CRF 模型，用最大似然方法，也就是用：\(-\log P\left(y_{1}, \ldots, y_{n} | x\right)\)，作为损失函数，可以算出它等于：<br>
\[
-\left(h\left(y_{1} ; \boldsymbol{x}\right)+\sum_{k=1}^{n-1} g\left(y_{k}, y_{k+1}\right)+h\left(y_{k+1} ; \boldsymbol{x}\right)\right)+\log Z(\boldsymbol{x})
\]<br>
其中第一项是原来概率式的<strong>分子</strong>的对数，它是对目标的序列的打分，虽然它看上去挺迂回的，但是并不难计算。真正的难度在于<strong>分母</strong>的对数 \(logZ(x)\) 这一项。</p>
<p>归一化因子，在物理上也叫配分函数，在这里它需要我们对所有可能的路径的打分进行指数求和，而我们前面已经说到，这样的路径数是指数量级的（k^n），因此直接来算几乎是不可能的。</p>
<p>事实上，<strong>归一化因子难算，几乎是所有概率图模型的公共难题</strong>。幸运的是，在 CRF 模型中，由于我们只考虑了临近标签的联系（马尔可夫假设），因此我们可以递归地算出归一化因子，这使得原来是指数级的计算量降低为线性级别。</p>
<p>具体来说，我们将计算到时刻 \(t\) 的归一化因子记为\(Z_t\)，并将它分为 \(k\) 个部分：<br>
\[
Z_{t}=Z_{t}^{(1)}+Z_{t}^{(2)}+\cdots+Z_{t}^{(k)}
\]<br>
分别是截止到当前时刻 \(t\) 中、以标签 \(1,…,k\) 为终点的所有路径的得分指数和。那么，我们可以递归地计算：</p>
<p>\[
\begin{aligned}
Z_{t+1}^{(1)} &amp;=\left(Z_{t}^{(1)} G_{11}+Z_{t}^{(2)} G_{21}+\cdots+Z_{t}^{(k)} G_{k 1}\right) h_{t+1}(1 | x) \\
Z_{t+1}^{(2)} &amp;=\left(Z_{t}^{(1)} G_{12}+Z_{t}^{(2)} G_{22}+\cdots+Z_{t}^{(k)} G_{k 2}\right) h_{t+1}(2 | x) \\
&amp; \vdots \\
Z_{t+1}^{(k)} &amp;=\left(Z_{t}^{(1)} G_{1 k}+Z_{t}^{(2)} G_{2 k}+\cdots+Z_{t}^{(k)} G_{k k}\right) h_{t+1}(k | x)
\end{aligned}
\]<br>
它可以简单写为矩阵形式：<br>
\[
\mathbf{Z}_{t+1}=\mathbf{Z}_{t} \boldsymbol{G} \otimes H\left(y_{t+1} | \boldsymbol{x}\right) 
\]<br>
其中,\(Z_{t}=\left[Z_{t}^{(1)}, \ldots, Z_{t}^{(k)}\right]\),而 G 是对 g(yi,yj) (状态转移)各个元素取指数后的矩阵，即\(G=e^{g(y i, y j)}\),而\(H(y_{t+1}|x)\),是编码模型\(h(y_{t+1}|x)\),（RNN、CNN等）对位置 t+1 的各个标签的打分的指数，即\(H\left(y_{t+1} | x\right)=e^{h\left(y_{t+1} | x\right)}\)，也是一个向量。\(Z_tG\) 这一步是矩阵乘法，得到一个向量，而 \(⊗\) 是两个向量的逐位对应相乘。</p>
<p><img src="/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/decode-crf.png" alt="avatar"></p>
<p>从t到t+1时刻的计算，包括转移概率和j+1节点本身的概率</p>
<h5 id="dong-tai-gui-hua">动态规划</h5>
<p>写出损失函数 \(-logP(y_1,…,y_n|x)\) 后，就可以完成模型的训练了，因为目前的深度学习框架都已经带有自动求导的功能，只要我们能写出可导的 loss，就可以帮我们完成优化过程了。</p>
<p>那么剩下的最后一步，就是模型训练完成后，如何根据输入找出最优路径来。跟前面一样，这也是一个从 \(k^n\) 条路径中选最优的问题，而同样地，因为马尔可夫假设的存在，它可以转化为一个动态规划问题，用 viterbi 算法解决，计算量正比于 n。</p>
<p>动态规划在本博客已经出现了多次了，<strong>它的递归思想就是：一条最优路径切成两段，那么每一段都是一条（局部）最优路径</strong>。</p>
<h5 id="dai-ma-shi-xian">代码实现</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List, Optional</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Conditional random field.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This module implements a conditional random field [LMP01]_. The forward computation</span></span><br><span class="line"><span class="string">    of this class computes the log likelihood of the given sequence of tags and</span></span><br><span class="line"><span class="string">    emission score tensor. This class also has `~CRF.decode` method which finds</span></span><br><span class="line"><span class="string">    the best tag sequence given an emission score tensor using `Viterbi algorithm`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_tags: Number of tags.</span></span><br><span class="line"><span class="string">        batch_first: Whether the first dimension corresponds to the size of a minibatch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        start_transitions (`~torch.nn.Parameter`): Start transition score tensor of size</span></span><br><span class="line"><span class="string">            ``(num_tags,)``.</span></span><br><span class="line"><span class="string">        end_transitions (`~torch.nn.Parameter`): End transition score tensor of size</span></span><br><span class="line"><span class="string">            ``(num_tags,)``.</span></span><br><span class="line"><span class="string">        transitions (`~torch.nn.Parameter`): Transition score tensor of size</span></span><br><span class="line"><span class="string">            ``(num_tags, num_tags)``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. [LMP01] Lafferty, J., McCallum, A., Pereira, F. (2001).</span></span><br><span class="line"><span class="string">       "Conditional random fields: Probabilistic models for segmenting and</span></span><br><span class="line"><span class="string">       labeling sequence data". *Proc. 18th International Conf. on Machine</span></span><br><span class="line"><span class="string">       Learning*. Morgan Kaufmann. pp. 282–289.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_tags: int, batch_first: bool = False)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="keyword">if</span> num_tags &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f'invalid number of tags: <span class="subst">&#123;num_tags&#125;</span>'</span>)</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.num_tags = num_tags</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.start_transitions = nn.Parameter(torch.empty(num_tags))</span><br><span class="line">        self.end_transitions = nn.Parameter(torch.empty(num_tags))</span><br><span class="line">        self.transitions = nn.Parameter(torch.empty(num_tags, num_tags))</span><br><span class="line"></span><br><span class="line">        self.reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span><span class="params">(self)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""Initialize the transition parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The parameters will be initialized randomly from a uniform distribution</span></span><br><span class="line"><span class="string">        between -0.1 and 0.1.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        nn.init.uniform_(self.start_transitions, <span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">        nn.init.uniform_(self.end_transitions, <span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">        nn.init.uniform_(self.transitions, <span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span> -&gt; str:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f'<span class="subst">&#123;self.__class__.__name__&#125;</span>(num_tags=<span class="subst">&#123;self.num_tags&#125;</span>)'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            self,</span></span></span><br><span class="line"><span class="function"><span class="params">            emissions: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            tags: torch.LongTensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            mask: Optional[torch.ByteTensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">            reduction: str = <span class="string">'sum'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">"""Compute the conditional log likelihood of a sequence of tags given emission scores.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            emissions (`~torch.Tensor`): Emission score tensor of size</span></span><br><span class="line"><span class="string">                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,</span></span><br><span class="line"><span class="string">                ``(batch_size, seq_length, num_tags)`` otherwise.</span></span><br><span class="line"><span class="string">            tags (`~torch.LongTensor`): Sequence of tags tensor of size</span></span><br><span class="line"><span class="string">                ``(seq_length, batch_size)`` if ``batch_first`` is ``False``,</span></span><br><span class="line"><span class="string">                ``(batch_size, seq_length)`` otherwise.</span></span><br><span class="line"><span class="string">            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``</span></span><br><span class="line"><span class="string">                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.</span></span><br><span class="line"><span class="string">            reduction: Specifies  the reduction to apply to the output:</span></span><br><span class="line"><span class="string">                ``none|sum|mean|token_mean``. ``none``: no reduction will be applied.</span></span><br><span class="line"><span class="string">                ``sum``: the output will be summed over batches. ``mean``: the output will be</span></span><br><span class="line"><span class="string">                averaged over batches. ``token_mean``: the output will be averaged over tokens.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            `~torch.Tensor`: The log likelihood. This will have size ``(batch_size,)`` if</span></span><br><span class="line"><span class="string">            reduction is ``none``, ``()`` otherwise.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self._validate(emissions, tags=tags, mask=mask)</span><br><span class="line">        <span class="keyword">if</span> reduction <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">'none'</span>, <span class="string">'sum'</span>, <span class="string">'mean'</span>, <span class="string">'token_mean'</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f'invalid reduction: <span class="subst">&#123;reduction&#125;</span>'</span>)</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            mask = torch.ones_like(tags, dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            emissions = emissions.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            tags = tags.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            mask = mask.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        numerator = self._compute_score(emissions, tags, mask)</span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        denominator = self._compute_normalizer(emissions, mask)</span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        llh = numerator - denominator</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> reduction == <span class="string">'none'</span>:</span><br><span class="line">            <span class="keyword">return</span> llh</span><br><span class="line">        <span class="keyword">if</span> reduction == <span class="string">'sum'</span>:</span><br><span class="line">            <span class="keyword">return</span> llh.sum()</span><br><span class="line">        <span class="keyword">if</span> reduction == <span class="string">'mean'</span>:</span><br><span class="line">            <span class="keyword">return</span> llh.mean()</span><br><span class="line">        <span class="keyword">assert</span> reduction == <span class="string">'token_mean'</span></span><br><span class="line">        <span class="keyword">return</span> llh.sum() / mask.type_as(emissions).sum()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, emissions: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">               mask: Optional[torch.ByteTensor] = None)</span> -&gt; List[List[int]]:</span></span><br><span class="line">        <span class="string">"""Find the most likely tag sequence using Viterbi algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            emissions (`~torch.Tensor`): Emission score tensor of size</span></span><br><span class="line"><span class="string">                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,</span></span><br><span class="line"><span class="string">                ``(batch_size, seq_length, num_tags)`` otherwise.</span></span><br><span class="line"><span class="string">            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``</span></span><br><span class="line"><span class="string">                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            List of list containing the best tag sequence for each batch.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self._validate(emissions, mask=mask)</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            mask = emissions.new_ones(emissions.shape[:<span class="number">2</span>], dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            emissions = emissions.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            mask = mask.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self._viterbi_decode(emissions, mask)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_validate</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            self,</span></span></span><br><span class="line"><span class="function"><span class="params">            emissions: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            tags: Optional[torch.LongTensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">            mask: Optional[torch.ByteTensor] = None)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="keyword">if</span> emissions.dim() != <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f'emissions must have dimension of 3, got <span class="subst">&#123;emissions.dim()&#125;</span>'</span>)</span><br><span class="line">        <span class="keyword">if</span> emissions.size(<span class="number">2</span>) != self.num_tags:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f'expected last dimension of emissions is <span class="subst">&#123;self.num_tags&#125;</span>, '</span></span><br><span class="line">                <span class="string">f'got <span class="subst">&#123;emissions.size(<span class="number">2</span>)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> tags <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> emissions.shape[:<span class="number">2</span>] != tags.shape:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">'the first two dimensions of emissions and tags must match, '</span></span><br><span class="line">                    <span class="string">f'got <span class="subst">&#123;tuple(emissions.shape[:<span class="number">2</span>])&#125;</span> and <span class="subst">&#123;tuple(tags.shape)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> emissions.shape[:<span class="number">2</span>] != mask.shape:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">'the first two dimensions of emissions and mask must match, '</span></span><br><span class="line">                    <span class="string">f'got <span class="subst">&#123;tuple(emissions.shape[:<span class="number">2</span>])&#125;</span> and <span class="subst">&#123;tuple(mask.shape)&#125;</span>'</span>)</span><br><span class="line">            no_empty_seq = <span class="keyword">not</span> self.batch_first <span class="keyword">and</span> mask[<span class="number">0</span>].all()</span><br><span class="line">            no_empty_seq_bf = self.batch_first <span class="keyword">and</span> mask[:, <span class="number">0</span>].all()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> no_empty_seq <span class="keyword">and</span> <span class="keyword">not</span> no_empty_seq_bf:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">'mask of the first timestep must all be on'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_compute_score</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            self, emissions: torch.Tensor, tags: torch.LongTensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            mask: torch.ByteTensor)</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="comment"># emissions: (seq_length, batch_size, num_tags)</span></span><br><span class="line">        <span class="comment"># tags: (seq_length, batch_size)</span></span><br><span class="line">        <span class="comment"># mask: (seq_length, batch_size)</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.dim() == <span class="number">3</span> <span class="keyword">and</span> tags.dim() == <span class="number">2</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.shape[:<span class="number">2</span>] == tags.shape</span><br><span class="line">        <span class="keyword">assert</span> emissions.size(<span class="number">2</span>) == self.num_tags</span><br><span class="line">        <span class="keyword">assert</span> mask.shape == tags.shape</span><br><span class="line">        <span class="keyword">assert</span> mask[<span class="number">0</span>].all()</span><br><span class="line"></span><br><span class="line">        seq_length, batch_size = tags.shape</span><br><span class="line">        mask = mask.type_as(emissions)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Start transition score and first emission</span></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        score = self.start_transitions[tags[<span class="number">0</span>]]</span><br><span class="line">        score += emissions[<span class="number">0</span>, torch.arange(batch_size), tags[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, seq_length):</span><br><span class="line">            <span class="comment"># Transition score to next tag, only added if next timestep is valid (mask == 1)</span></span><br><span class="line">            <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">            score += self.transitions[tags[i - <span class="number">1</span>], tags[i]] * mask[i]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Emission score for next tag, only added if next timestep is valid (mask == 1)</span></span><br><span class="line">            <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># End transition score</span></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        seq_ends = mask.long().sum(dim=<span class="number">0</span>) - <span class="number">1</span></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        last_tags = tags[seq_ends, torch.arange(batch_size)]</span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        score += self.end_transitions[last_tags]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_compute_normalizer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            self, emissions: torch.Tensor, mask: torch.ByteTensor)</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="comment"># emissions: (seq_length, batch_size, num_tags)</span></span><br><span class="line">        <span class="comment"># mask: (seq_length, batch_size)</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.dim() == <span class="number">3</span> <span class="keyword">and</span> mask.dim() == <span class="number">2</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.shape[:<span class="number">2</span>] == mask.shape</span><br><span class="line">        <span class="keyword">assert</span> emissions.size(<span class="number">2</span>) == self.num_tags</span><br><span class="line">        <span class="keyword">assert</span> mask[<span class="number">0</span>].all()</span><br><span class="line"></span><br><span class="line">        seq_length = emissions.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Start transition score and first emission; score has size of</span></span><br><span class="line">        <span class="comment"># (batch_size, num_tags) where for each batch, the j-th column stores</span></span><br><span class="line">        <span class="comment"># the score that the first timestep has tag j</span></span><br><span class="line">        <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">        score = self.start_transitions + emissions[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, seq_length):</span><br><span class="line">            <span class="comment"># Broadcast score for every possible next tag</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags, 1)</span></span><br><span class="line">            broadcast_score = score.unsqueeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Broadcast emission score for every possible current tag</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, 1, num_tags)</span></span><br><span class="line">            broadcast_emissions = emissions[i].unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute the score tensor of size (batch_size, num_tags, num_tags) where</span></span><br><span class="line">            <span class="comment"># for each sample, entry at row i and column j stores the sum of scores of all</span></span><br><span class="line">            <span class="comment"># possible tag sequences so far that end with transitioning from tag i to tag j</span></span><br><span class="line">            <span class="comment"># and emitting</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags, num_tags)</span></span><br><span class="line">            next_score = broadcast_score + self.transitions + broadcast_emissions</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Sum over all possible current tags, but we're in score space, so a sum</span></span><br><span class="line">            <span class="comment"># becomes a log-sum-exp: for each sample, entry i stores the sum of scores of</span></span><br><span class="line">            <span class="comment"># all possible tag sequences so far, that end in tag i</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">            next_score = torch.logsumexp(next_score, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Set score to the next score if this timestep is valid (mask == 1)</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">            score = torch.where(mask[i].unsqueeze(<span class="number">1</span>), next_score, score)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># End transition score</span></span><br><span class="line">        <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">        score += self.end_transitions</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Sum (log-sum-exp) over all possible tags</span></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        <span class="keyword">return</span> torch.logsumexp(score, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_viterbi_decode</span><span class="params">(self, emissions: torch.FloatTensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                        mask: torch.ByteTensor)</span> -&gt; List[List[int]]:</span></span><br><span class="line">        <span class="comment"># emissions: (seq_length, batch_size, num_tags)</span></span><br><span class="line">        <span class="comment"># mask: (seq_length, batch_size)</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.dim() == <span class="number">3</span> <span class="keyword">and</span> mask.dim() == <span class="number">2</span></span><br><span class="line">        <span class="keyword">assert</span> emissions.shape[:<span class="number">2</span>] == mask.shape</span><br><span class="line">        <span class="keyword">assert</span> emissions.size(<span class="number">2</span>) == self.num_tags</span><br><span class="line">        <span class="keyword">assert</span> mask[<span class="number">0</span>].all()</span><br><span class="line"></span><br><span class="line">        seq_length, batch_size = mask.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Start transition and first emission</span></span><br><span class="line">        <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">        score = self.start_transitions + emissions[<span class="number">0</span>]</span><br><span class="line">        history = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># score is a tensor of size (batch_size, num_tags) where for every batch,</span></span><br><span class="line">        <span class="comment"># value at column j stores the score of the best tag sequence so far that ends</span></span><br><span class="line">        <span class="comment"># with tag j</span></span><br><span class="line">        <span class="comment"># history saves where the best tags candidate transitioned from; this is used</span></span><br><span class="line">        <span class="comment"># when we trace back the best tag sequence</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Viterbi algorithm recursive case: we compute the score of the best tag sequence</span></span><br><span class="line">        <span class="comment"># for every possible next tag</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, seq_length):</span><br><span class="line">            <span class="comment"># Broadcast viterbi score for every possible next tag</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags, 1)</span></span><br><span class="line">            broadcast_score = score.unsqueeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Broadcast emission score for every possible current tag</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, 1, num_tags)</span></span><br><span class="line">            broadcast_emission = emissions[i].unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute the score tensor of size (batch_size, num_tags, num_tags) where</span></span><br><span class="line">            <span class="comment"># for each sample, entry at row i and column j stores the score of the best</span></span><br><span class="line">            <span class="comment"># tag sequence so far that ends with transitioning from tag i to tag j and emitting</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags, num_tags)</span></span><br><span class="line">            next_score = broadcast_score + self.transitions + broadcast_emission</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Find the maximum score over all possible current tag</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">            next_score, indices = next_score.max(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Set score to the next score if this timestep is valid (mask == 1)</span></span><br><span class="line">            <span class="comment"># and save the index that produces the next score</span></span><br><span class="line">            <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">            score = torch.where(mask[i].unsqueeze(<span class="number">1</span>), next_score, score)</span><br><span class="line">            history.append(indices)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># End transition score</span></span><br><span class="line">        <span class="comment"># shape: (batch_size, num_tags)</span></span><br><span class="line">        score += self.end_transitions</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Now, compute the best path for each sample</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># shape: (batch_size,)</span></span><br><span class="line">        seq_ends = mask.long().sum(dim=<span class="number">0</span>) - <span class="number">1</span></span><br><span class="line">        best_tags_list = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(batch_size):</span><br><span class="line">            <span class="comment"># Find the tag which maximizes the score at the last timestep; this is our best tag</span></span><br><span class="line">            <span class="comment"># for the last timestep</span></span><br><span class="line">            _, best_last_tag = score[idx].max(dim=<span class="number">0</span>)</span><br><span class="line">            best_tags = [best_last_tag.item()]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># We trace back where the best last tag comes from, append that to our best tag</span></span><br><span class="line">            <span class="comment"># sequence, and trace it back again, and so on</span></span><br><span class="line">            <span class="keyword">for</span> hist <span class="keyword">in</span> reversed(history[:seq_ends[idx]]):</span><br><span class="line">                best_last_tag = hist[idx][best_tags[<span class="number">-1</span>]]</span><br><span class="line">                best_tags.append(best_last_tag.item())</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Reverse the order because we start from the last timestep</span></span><br><span class="line">            best_tags.reverse()</span><br><span class="line">            best_tags_list.append(best_tags)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> best_tags_list</span><br></pre></td></tr></table></figure>
<h4 id="zhu-zheng-softmax-he-crf-de-yi-tong">逐帧 softmax 和 CRF 的异同?</h4>
<p><strong>逐帧softmax</strong></p>
<p>CRF 主要用于序列标注问题，可以简单理解为是<strong>给序列中的每一帧都进行分类</strong>，既然是分类，很自然想到将这个序列用 CNN 或者 RNN 进行编码后，接一个全连接层用 softmax 激活，如下图所示：</p>
<p><img src="/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/encoder.png" alt="avatar"></p>
<p>然而，<strong>逐帧softmax并没有直接考虑输出的上下文关联</strong>。比如：当我们设计标签时，比如用 s、b、m、e 的 4 个标签来做字标注法的分词，目标输出序列本身会带有一些上下文关联，如 s 后面就不能接 m 和 e，等等。逐标签 softmax 并没有考虑这种输出层面的上下文关联，所以它意味着把这些关联放到了编码层面，希望模型能自己学到这些内容，但有时候会“强模型所难”。</p>
<p>而 CRF 则更直接一点，它<strong>将输出层面的关联分离了出来</strong>，这使得模型在学习上更为“从容”,CRF在输出端显式地考虑了上下文关联.</p>
<p><img src="/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/encoder-crf.png" alt="avatar"></p>
<p>如果仅仅是引入输出的关联，还不仅仅是 CRF 的全部，CRF 的真正精巧的地方，是它以路径为单位，考虑的是路径的概率。</p>
<p>假如一个输入有 n 帧，每一帧的标签有 k 中可能性，那么理论上就有\(k^n\)中不同的输入。我们将它用如下的网络图进行简单的可视化。在下图中，每个点代表一个标签的可能性，点之间的连线表示标签之间的关联，而每一种标注结果，都对应着图上的一条完整的路径。</p>
<p><img src="/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/crf-v.png" alt="avatar"></p>
<p>而在序列标注任务中，我们的正确答案是一般是唯一的。比如“今天天气不错”，如果对应的分词结果是“今天/天气/不/错”，那么目标输出序列就是 bebess，除此之外别的路径都不符合要求。</p>
<p>换言之，在序列标注任务中，我们的研究的基本单位应该是路径，我们要做的事情，是从 \(k^n\) 条路径选出正确的一条，那就意味着，如果将它视为一个分类问题，那么将是 \(k^n\) 类中选一类的分类问题。</p>
<p>这就是逐帧 softmax 和 CRF 的根本不同了：<strong>前者将序列标注看成是 n 个 k 分类问题，后者将序列标注看成是 1 个$ k^n$ 分类问题</strong>。</p>
<h4 id="crf">CRF++</h4>
<h5 id="jian-jie">简介</h5>
<p>CRF++ 是C++ 实现的CRF 工具。</p>
<p><a href="https://taku910.github.io/crfpp/" target="_blank" rel="noopener">文档</a></p>
<h5 id="an-zhuang">安装</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/taku910/crfpp.git</span><br><span class="line">cd crfpp</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>如果make的时候发生找不到winmain.h的错误: 可以下面这种方式修复：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sed -i <span class="string">'/#include "winmain.h"/d'</span> crf_test.cpp</span><br><span class="line">sed -i <span class="string">'/#include "winmain.h"/d'</span> crf_learn.cpp</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>后面需要用到Python 使用训练好的模型所以也一起安装CRFPP, cd python 目录下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cd python</span><br><span class="line">python setup.py build</span><br><span class="line">sudo python setup.py install</span><br></pre></td></tr></table></figure>
<p>然后在Python 或者Ipython 里输入 <code>import CRFPP</code> 如果发生如下错误</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ImportError: libcrfpp.so<span class="number">.0</span>: cannot open shared object file: No such file <span class="keyword">or</span> directory</span><br></pre></td></tr></table></figure>
<p>可用下面的方法解决</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/ld.so.conf</span><br><span class="line"><span class="comment"># 添加</span></span><br><span class="line">include /usr/local/lib</span><br><span class="line"><span class="comment"># 保存后加载一下</span></span><br><span class="line">sudo /sbin/ldconfig -v</span><br></pre></td></tr></table></figure>
<h5 id="xun-lian-shu-ju-ge-shi">训练数据格式</h5>
<p>对于训练数据，首先需要多列，但不能不一致，既在一个文件里有的行是两列，有的行是三列（这种格式是错误的）；其次第一列代表的是需要标注的<strong>字或词</strong>，最后一列是输出位标记tag，\如果有额外的特征，例如词性什么的，可以加到中间列里，所以训练集或者测试集的文件最少要有两列。以分词举例，标签为BMES</p>
<p>东	N	B<br>
南	N	M<br>
大	N	M<br>
学	N	E<br>
欢	V	B<br>
迎	V	E<br>
您	N	S</p>
<p>其中第3列是标签，也是测试文件中需要预测的结果，有BMES 4种状态。第二列是词性，不是必须的。</p>
<h5 id="te-zheng-mo-ban">特征模板</h5>
<p>每一行模板生成一组状态特征函数，数量是\(L*N\) 个，\(L\)是标签状态数是4个。\(N\)是此行模板在训练集上展开后的唯一样本数，在这个例子中，第一列的唯一字数是7个，所以有\(L*N = 4*7=28\)。例如：U01:%x[0,0]，生成如下28个函数：</p>
<figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line">func1 = <span class="keyword">if</span> (output = B and feature=U01:<span class="string">"东"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func2 = <span class="keyword">if</span> (output = M and feature=U01:<span class="string">"东"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func3 = <span class="keyword">if</span> (output = E and feature=U01:<span class="string">"东"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func4 = <span class="keyword">if</span> (output = S and feature=U01:<span class="string">"东"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func5 = <span class="keyword">if</span> (output = B and feature=U01:<span class="string">"南"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">...</span><br><span class="line">func25 = <span class="keyword">if</span> (output = B and feature=U01:<span class="string">"你"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func26 = <span class="keyword">if</span> (output = M and feature=U01:<span class="string">"你"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func27 = <span class="keyword">if</span> (output = E and feature=U01:<span class="string">"你"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">func28 = <span class="keyword">if</span> (output = S and feature=U01:<span class="string">"你"</span>) <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>这些函数经过训练后，其权值表示函数内文字对应该标签的概率（形象说法，概率和可大于1）。</p>
<h6 id="unigram">unigram</h6>
<p>U00:%x[-2,0]<br>
U01:%x[-1,0]<br>
U02:%x[0,0]<br>
U03:%x[1,0]<br>
U04:%x[2,0]<br>
U05:%x[-2,0]/%x[-1,0]/%x[0,0]<br>
U06:%x[-1,0]/%x[0,0]/%x[1,0]<br>
U07:%x[0,0]/%x[1,0]/%x[2,0]<br>
U08:%x[-1,0]/%x[0,0]<br>
U09:%x[0,0]/%x[1,0]</p>
<p>这是CRF<ins>例子中给出的模板，一共有９个模板，先看第一个模板，表示当前词和前面的第二个词组成的特征，以‘小明今天穿了一件红色上衣’为例，符合CRF</ins>处理格式的这句话应该变成如下形式：<br>
小　Ｂ<br>
明　Ｉ<br>
今　Ｂ   &lt;-- current word<br>
天　Ｉ<br>
穿　Ｓ<br>
了　Ｓ<br>
一　Ｂ<br>
件　Ｉ<br>
红　Ｂ<br>
色　Ｉ<br>
上　Ｂ<br>
衣　Ｉ<br>
假设我们有三个标记tag，Ｂ（表示一个词的开头那个字），Ｉ（表示一个词的结尾那个字），Ｓ（表示单个字的词），先看第一个模板Ｕ00:%x[-2,0],第一个模板产生的特征如下： 如果当前词是‘今’，那-2位置对应的字就是‘小’， 每个特征对应的字如下：</p>
<table>
<thead>
<tr>
<th>特征模板</th>
<th>意义</th>
<th>代表特征</th>
</tr>
</thead>
<tbody>
<tr>
<td>U00:%x[-2,0]</td>
<td>-2行，0列</td>
<td>小</td>
</tr>
<tr>
<td>U01:%x[-1,0]</td>
<td>-1行，0列</td>
<td>明</td>
</tr>
<tr>
<td>U02:%x[0,0]</td>
<td>0行，0列</td>
<td>今</td>
</tr>
<tr>
<td>U03:%x[1,0]</td>
<td>1行，0列</td>
<td>天</td>
</tr>
<tr>
<td>U04:%x[2,0]</td>
<td>2行，0列</td>
<td>穿</td>
</tr>
<tr>
<td>U05:%x[-2,0]/%x[-1,0]/%x[0,0]</td>
<td>-2行0列、-1行0列与0行0列的组合</td>
<td>小/明/今</td>
</tr>
<tr>
<td>U06:%x[-1,0]/%x[0,0]/%x[1,0]</td>
<td>-1行0列、0行0列与1行0列的组合</td>
<td>明/今/天</td>
</tr>
<tr>
<td>U07:%x[0,0]/%x[1,0]/%x[2,0]</td>
<td>0行0列、1行0列与2行0列的组合</td>
<td>今/天/穿</td>
</tr>
<tr>
<td>U08:%x[-1,0]/%x[0,0]</td>
<td>-1行0列与0行0列的组合</td>
<td>明/今</td>
</tr>
<tr>
<td>U09:%x[0,0]/%x[1,0]</td>
<td>0行0列与1行0列的组合</td>
<td>今/天</td>
</tr>
</tbody>
</table>
<p>根据第一个模板U00:%x[-2,0]能得到的转移特征函数如下：<br>
<code>func1=if(output=B and feature='U00:小' )　return 1 else return 0 </code><br>
其中output=B 指的是当前词（字）的预测标记，也就是’今‘的预测标记，每个模板会把所有可能的标记输出都列一遍，然后通过训练确定每种标记的权重，合理的标记在训练样本中出现的次数多，对应的权重就高，不合理的标记在训练样本中出现的少，对应的权重就少，但是在利用模板生成转移特征函数是会把所有可能的特征函数都列出来，由模型通过训练决定每个特征的重要程度。<br>
<code>func2=if(output=M and feature=’U00:小’) return 1 else return 0 </code><br>
<code>func3=if(output=E and feature=’U00:小’) return 1 else return 0 </code><br>
<code>func4=if(output=S and feature=’U00:小) return 1 else return 0 </code><br>
得到4个特征函数之后当前这个字’今‘的特征函数利用第一个模板就得到全部了，然后扫描下一个字‘天‘，以’天‘字作为当前字预测这个字的标记tag,同样会得到4个特征函数：<br>
<code>func5=if(output=B and feature=’U00:明’) return 1 else return 0 </code><br>
<code>func6=if(output=M and feature=’U00:明’) return 1 else return 0 </code><br>
<code>func7=if(output=E and feature=’U00:明’) return 1 else return 0 </code><br>
<code>func7=if(output=S and feature=’U00:明’) return 1 else return 0 </code><br>
特征函数中的feature指的是当前词的-2位置对应的词或对应的词的特征，因为在这里没有其他特征了，所以用字本身做特征，有的会有词性，那feature就会是’明‘这个字对应的词性而不是字本身了。 这个feature的作用就是确定模板所确定的当前词和临近词</p>
<h6 id="bigram">bigram</h6>
<p>与Unigram不同的是，Bigram类型模板生成的函数会多一个参数：上个节点的标签 \(y_{i-1}\)。生成函B01:%x[0,0] (当前词为“东南大学欢迎您” 的“您”)：<code>func1 = if (prev_output = E and output = S and feature=B01:&quot;您&quot;) return 1 else return 0</code></p>
<p>这样，每行模板则会生成 \(L*L*N\) 个特征函数。经过训练后，这些函数的权值反映了上一个节点的标签对当前节点的影响。(L表示隐状态的个数，如BMES为四个隐状态，N表示训练语料的大小。)</p>
<p>trigam理论上也是可行的，考虑前\(y_{i-2},y_{i-1}\)对\(y_i\)的影响，不过crf++并没有实现，这实际和二阶HMM模型类似的，有论文详细描述过其求解，比较复杂，但是其效果提升并未多大，所以一般不使用。</p>
<h6 id="chan-sheng-te-zheng-han-shu">产生特征函数</h6>
<p>训练阶段将遍历数据逐一产生特征函数，写成伪码过程为：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">m：样本个数</span><br><span class="line">k：模板个数</span><br><span class="line">templates：一组特征模板</span><br><span class="line">genFeatureFunc：根据模板和序列x，y和位置i产生特征函数</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">    <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; k; j++)&#123;</span><br><span class="line">        genFeatureFunc(templates[j],x,y,i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="te-zheng-han-shu">特征函数</h6>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span> B</span><br><span class="line"><span class="number">16</span> U00:-</span><br><span class="line"><span class="number">20</span> U00:<span class="number">0</span></span><br><span class="line"><span class="number">24</span> U00:<span class="number">1</span></span><br><span class="line"><span class="number">28</span> U00:<span class="number">2</span></span><br><span class="line"><span class="number">32</span> U00:<span class="number">3</span></span><br><span class="line"><span class="number">36</span> U00:<span class="number">4</span></span><br><span class="line"><span class="number">40</span> U00:<span class="number">5</span></span><br><span class="line"><span class="number">44</span> U00:<span class="number">6</span></span><br><span class="line"><span class="number">48</span> U00:<span class="number">7</span></span><br><span class="line"><span class="number">52</span> U00:<span class="number">8</span></span><br><span class="line"><span class="number">56</span> U00:<span class="number">9</span></span><br><span class="line"><span class="number">60</span> U00:_B<span class="number">-1</span></span><br><span class="line"><span class="number">64</span> U00:_B<span class="number">-2</span></span><br><span class="line">……</span><br><span class="line"><span class="number">17404</span> U01:厨</span><br><span class="line"><span class="number">17408</span> U01:去</span><br><span class="line"><span class="number">17412</span> U01:县</span><br><span class="line"><span class="number">17416</span> U01:参</span><br><span class="line"><span class="number">17420</span> U01:又</span><br><span class="line"><span class="number">17424</span> U01:叉</span><br><span class="line"><span class="number">17428</span> U01:及</span><br><span class="line"><span class="number">17432</span> U01:友</span><br><span class="line"><span class="number">17436</span> U01:双</span><br><span class="line"><span class="number">17440</span> U01:反</span><br><span class="line"><span class="number">17444</span> U01:发</span><br><span class="line"><span class="number">17448</span> U01:叔</span><br><span class="line"><span class="number">17452</span> U01:取</span><br><span class="line"><span class="number">17456</span> U01:受</span><br><span class="line">……</span><br><span class="line"><span class="number">77800</span> U05:_B<span class="number">-1</span>/一/个</span><br><span class="line"><span class="number">107540</span> U05:一/方/面</span><br><span class="line"><span class="number">107544</span> U05:一/无/所</span><br><span class="line"><span class="number">107548</span> U05:一/日/三</span><br><span class="line"><span class="number">107552</span> U05:一/日/为</span><br><span class="line"><span class="number">107556</span> U05:一/日/之</span><br><span class="line">……</span><br><span class="line"><span class="number">566536</span> U06:万/吨/_B+<span class="number">1</span></span><br><span class="line">……</span><br><span class="line"><span class="number">2159864</span> U09:ｖ/ｅ</span><br></pre></td></tr></table></figure>
<p>按照[id] [参数o]的格式排列，注意到id不是连续的，而是隔了四个（状态s隐藏起来了），这表示这四个标签（BMES）和公共的参数o组合成了四个特征函数。特别的，0-15为BEMS转移到BEMS的转移函数，也就是f(s’, s, o=null)。_B-1表示句子第一个单词前面的一个单词，_B+1表示末尾后面的一个单词.</p>
<h6 id="te-zheng-han-shu-quan-zhi">特征函数权值</h6>
<p>后面的小数依id顺序对应每个特征函数的权值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">9.0491453814148901</span></span><br><span class="line"><span class="number">7.0388286231971700</span></span><br><span class="line"><span class="number">-7.2545558164093009</span></span><br><span class="line"><span class="number">5.2799470769112835</span></span><br><span class="line"><span class="number">-8.5333633546653758</span></span><br><span class="line"><span class="number">-5.3549190735606933</span></span><br><span class="line"><span class="number">5.2575182675282477</span></span><br><span class="line"><span class="number">-5.4259109736696054</span></span><br></pre></td></tr></table></figure>
<p>比如说有一个句子“商品和服务”，对于每个字都按照上述模板生成一系列U特征函数的参数代入，得到一些类似010101的函数返回值，乘上这些函数的权值求和，就得到了各个标签的分数，由大到小代表输出这些标签的可能性。至于B特征函数（这里特指简单的f(s’, s)），在Viterbi后向解码的时候，前一个标签确定了后就可以代入当前的B特征函数，计算出每个输出标签的分数，再次求和排序即可。</p>
<h5 id="xun-lian-ji-yu-ce">训练及预测</h5>
<p>CRF++的训练命令一般格式如下：<br>
<code>crf_learn  -f 3 -c 4.0 template train.data model -t</code><br>
其中，template为模板文件，train.data为训练语料，-t表示可以得到一个model文件和一个model.txt文件，其他可选参数说明如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">-f, –freq=INT使用属性的出现次数不少于INT(默认为<span class="number">1</span>)</span><br><span class="line">-m, –maxiter=INT设置INT为LBFGS的最大迭代次数 (默认<span class="number">10</span>k)</span><br><span class="line">-c, –cost=FLOAT    设置FLOAT为代价参数，过大会过度拟合 (默认<span class="number">1.0</span>)</span><br><span class="line">-e, –eta=FLOAT设置终止标准FLOAT(默认<span class="number">0.0001</span>)</span><br><span class="line">-C, –convert将文本模式转为二进制模式</span><br><span class="line">-t, –textmodel为调试建立文本模型文件</span><br><span class="line">-a, –algorithm=(CRF|MIRA)    选择训练算法，默认为CRF-L2</span><br><span class="line">-p, –thread=INT线程数(默认<span class="number">1</span>)，利用多个CPU减少训练时间</span><br><span class="line">-H, –shrinking-size=INT    设置INT为最适宜的跌代变量次数 (默认<span class="number">20</span>)</span><br><span class="line">-v, –version显示版本号并退出</span><br><span class="line">-h, –help显示帮助并退出</span><br></pre></td></tr></table></figure>
<p>在训练过程中，会输出一些信息，其意义如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iter：迭代次数。当迭代次数达到maxiter时，迭代终止</span><br><span class="line">terr：标记错误率</span><br><span class="line">serr：句子错误率</span><br><span class="line">obj：当前对象的值。当这个值收敛到一个确定值的时候，训练完成</span><br><span class="line">diff：与上一个对象值之间的相对差。当此值低于eta时，训练完成</span><br></pre></td></tr></table></figure>
<p>在训练完模型后，我们可以使用训练好的模型对新数据进行预测，预测命令格式如下：<br>
<code>crf_test -m model seg_word_predict.data &gt; predict.txt</code></p>
<p><code>-m model</code>表示使用我们刚刚训练好的model模型，预测的数据文件为seg_word_predict.data, <code>&gt; predict.txt</code>表示将预测后的数据写入到predict.txt中。</p>
<h6 id="dai-ma">代码</h6>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> CRFPP</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRFModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model=<span class="string">'model_name'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 类初始化</span></span><br><span class="line"><span class="string">        :param model: 模型名称</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_tagger</span><span class="params">(self, tag_data)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 添加语料</span></span><br><span class="line"><span class="string">        :param tag_data: 数据</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        word_str = tag_data.strip()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(self.model):</span><br><span class="line">            print(<span class="string">'模型不存在,请确认模型路径是否正确!'</span>)</span><br><span class="line">            exit()</span><br><span class="line">        tagger = CRFPP.Tagger(<span class="string">"-m &#123;&#125; -v 3 -n2"</span>.format(self.model))</span><br><span class="line">        tagger.clear()</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> word_str:</span><br><span class="line">            tagger.add(word)</span><br><span class="line">        tagger.parse()</span><br><span class="line">        <span class="keyword">return</span> tagger</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">text_mark</span><span class="params">(self, tag_data, begin=<span class="string">'B'</span>, middle=<span class="string">'I'</span>, end=<span class="string">'E'</span>, single=<span class="string">'S'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        文本标记</span></span><br><span class="line"><span class="string">        :param tag_data: 数据</span></span><br><span class="line"><span class="string">        :param begin: 开始标记</span></span><br><span class="line"><span class="string">        :param middle: 中间标记</span></span><br><span class="line"><span class="string">        :param end: 结束标记</span></span><br><span class="line"><span class="string">        :param single: 单字结束标记</span></span><br><span class="line"><span class="string">        :return result: 标记列表</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        tagger = self.add_tagger(tag_data)</span><br><span class="line">        size = tagger.size()</span><br><span class="line">        tag_text = <span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, size):</span><br><span class="line">            word, tag = tagger.x(i, <span class="number">0</span>), tagger.y2(i)</span><br><span class="line">            <span class="keyword">if</span> tag <span class="keyword">in</span> [begin, middle]:</span><br><span class="line">                tag_text += word</span><br><span class="line">            <span class="keyword">elif</span> tag <span class="keyword">in</span> [end, single]:</span><br><span class="line">                tag_text += word + <span class="string">"*&amp;*"</span></span><br><span class="line">        result = tag_text.split(<span class="string">'*&amp;*'</span>)</span><br><span class="line">        result.pop()</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crf_test</span><span class="params">(self, tag_data, separator=<span class="string">'/'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: crf测试</span></span><br><span class="line"><span class="string">        :param tag_data:</span></span><br><span class="line"><span class="string">        :param separator:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        result = self.text_mark(tag_data)</span><br><span class="line">        data = separator.join(result)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crf_learn</span><span class="params">(self, filename)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 训练模型</span></span><br><span class="line"><span class="string">        :param filename: 已标注数据源</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        crf_bash = <span class="string">"crf_learn -f 3 -c 4.0 api/template.txt &#123;&#125; &#123;&#125;"</span>.format(filename, self.model)</span><br><span class="line">        process = subprocess.Popen(crf_bash.split(), stdout=subprocess.PIPE)</span><br><span class="line">        output = process.communicate()[<span class="number">0</span>]</span><br><span class="line">        print(output.decode(encoding=<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<h3 id="can-kao">参考</h3>
<ol>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/37163081" target="_blank" rel="noopener">简明条件随机场CRF介绍 | 附带纯Keras实现</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/qq_37667364/article/details/82919560" target="_blank" rel="noopener">CRF++/CRF/条件随机场的特征函数模板</a></p>
</li>
<li>
<p><a href="https://www.hankcs.com/nlp/the-crf-model-format-description.html" target="_blank" rel="noopener">CRF++模型格式说明</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>技术/深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>序列标注</tag>
        <tag>条件随机场</tag>
      </tags>
  </entry>
  <entry>
    <title>模型融合</title>
    <url>/2020/04/03/machine_learning/model_fusion/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/04/03/machine_learning/model_fusion/model_fusion.png" alt></p>
<a id="more"></a>
<p><a href="https://share.weiyun.com/EDpohPCk" target="_blank" rel="noopener">数据集下载</a></p>
<p>一般来说，通过融合多个不同模型的结果，可以提升最终的成绩，所以这以方法在各种数据竞赛中应用非常广泛。模型融合又可以从<strong>模型结果</strong>、<strong>模型自身</strong>、<strong>样本集</strong>等不同的角度进行融合。</p>
<h1 id="jian-dan-jia-quan-rong-he">简单加权融合</h1>
<h2 id="hui-gui-ren-wu-zhong-de-jia-quan-rong-he">回归任务中的加权融合</h2>
<p>对于回归问题，对各种模型的预测结果进行平均，所得到的结果能够减少过拟合，并使得边界更加平滑，(单个模型的边界可能很粗糙)。</p>
<p><img src="/2020/04/03/machine_learning/model_fusion/1739510-20190711144542394-704578871.png" alt></p>
<p><strong>加权融合根据各个模型的最终预测表现分配不同的权重，以改变其队最终结果影响的大小</strong>。例如，对于正确率低的模型给予较小的权重，而正确率高的模型给予更高的权重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成一些简单的样本，test_predi代表第i个模型的预测值</span></span><br><span class="line">test_pred1 = [<span class="number">1.2</span>, <span class="number">3.2</span>, <span class="number">2.1</span>, <span class="number">6.2</span>]</span><br><span class="line">test_pred2 = [<span class="number">0.9</span>, <span class="number">3.1</span>, <span class="number">2.0</span>, <span class="number">5.9</span>]</span><br><span class="line">test_pred3 = [<span class="number">1.1</span>, <span class="number">2.9</span>, <span class="number">2.2</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_test_true 代表模型的真实值</span></span><br><span class="line">y_test_true = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以先看一下各个模型的预测结果</span></span><br><span class="line">print(<span class="string">'Pred1 MAE:'</span>,mean_absolute_error(y_test_true, test_pred1)) </span><br><span class="line">print(<span class="string">'Pred2 MAE:'</span>,mean_absolute_error(y_test_true, test_pred2)) </span><br><span class="line">print(<span class="string">'Pred3 MAE:'</span>,mean_absolute_error(y_test_true, test_pred3))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 结果：</span></span><br><span class="line">Pred1 MAE: <span class="number">0.175</span></span><br><span class="line">Pred2 MAE: <span class="number">0.075</span></span><br><span class="line">Pred3 MAE: <span class="number">0.1</span></span><br></pre></td></tr></table></figure>
<p>可以发现，第 2 个模型的误差更小，准确率更高，所以应该给第二个模型的预测值赋予更高的权重</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加权融合，权重的默认值是(1/n)，n为模型个数，相当于默认使用平均加权融合</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weighted_method</span><span class="params">(test_pred1, test_pred2, test_pred3, w = [<span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>/<span class="number">3</span>])</span>:</span></span><br><span class="line">    weighted_result = w[<span class="number">0</span>] * pd.Series(test_pred1) + w[<span class="number">1</span>] * pd.Series(test_pred2) + w[<span class="number">2</span>] * pd.Series(test_pred3)</span><br><span class="line">    <span class="keyword">return</span> weighted</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据上面的MAE，计算每个模型的权重（MAE越小，权重越大）</span></span><br><span class="line">w = [<span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.3</span>] <span class="comment"># 这个权重是自定义的，也可以使用一些其它方法，例如softmax</span></span><br><span class="line">weighed_pred = weighted_method(test_pred1, test_pred2, test_pred3, w)</span><br><span class="line">print(<span class="string">'Weighted_pred MAE:'</span>,mean_absolute_error(y_test_true, Weighted_pre))   <span class="comment"># 融合之后效果提高了一些</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">Weighted_pre MAE: <span class="number">0.0575</span></span><br></pre></td></tr></table></figure>
<p>上述加权融合的技术是从模型结果的层面进行的，就是让每个模型跑一遍结果，然后对所有的结果进行融合，当然融合的方式不只有加权平均，还有例如平均、取中位数等：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义结果的平均函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Mean_method</span><span class="params">(test_pre1,test_pre2,test_pre3)</span>:</span></span><br><span class="line">    Mean_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=<span class="number">1</span>).mean(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> Mean_result</span><br><span class="line"></span><br><span class="line"><span class="comment">## 定义结果的中位数平均函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Median_method</span><span class="params">(test_pre1,test_pre2,test_pre3)</span>:</span></span><br><span class="line">    Median_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=<span class="number">1</span>).median(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> Median_result</span><br></pre></td></tr></table></figure>
<h2 id="fen-lei-ren-wu-zhong-de-voting">分类任务中的 Voting</h2>
<p>投票（Voting）是集成学习里面针对分类问题的一种结果融合策略。其基本思想是<strong>选择所有模型输出结果中，最多的那个类，即少数服从多数</strong>。</p>
<p>在不改变模型的情况下，直接对不同模型的预测结果进行投票或者平均，是一种简单却行之有效的融合方式。比如分类问题，假设有三个相互独立的模型，每个模型的正确率都是 70%，采用少数服从多数的方式进行投票，那么最终的正确率将是：<br>
\[
0.7 * 0.7 * 0.7+0.7 * 0.7 * 0.3 * 3=0.343+0.441=0.784
\]<br>
融合后预测正确的情况有两种，一种是<strong>三个模型都预测对了</strong>，另一种是<strong>其中两个模型预测对了，有一个模型预测错了</strong>。对于这两种情况，由于<strong>少数服从多数</strong>的机制存在，会使得最终结果都对。3 个 0.7 相乘对应的就是第一种情况，\(0.7∗0.7∗0.3∗3\) 对应的就是第二种情况。</p>
<p>经过简单的投票后，正确率提升了 8%。这是一个简单的概率问题——如果进行投票的模型越多，显然其结果将会更好，但前提条件是<strong>模型之间相互独立，结果之间没有相关性。越相近的模型进行融合，融合效果也会越差</strong>。</p>
<p>比如对于一个正确输出全为1的测试，我们有三个很相近的的预测结果，分别为：<br>
\[
\begin{array}{l}
1111111100=80 \% \text { accuracy } \\
11111111100=80 \% \text { accuracy } \\
1011111100=70 \% \text { accuracy }
\end{array}
\]</p>
<p>进行投票其结果为：<br>
\[
\begin{array}{l}
11111111100=80 \% \text { accuracy }
\end{array}
\]<br>
而假如各个预测结果之间有很大差异：<br>
\[
\begin{array}{l}
1111111100=80 \% \text { accuracy } \\
0111011101=70 \% \text { accuracy } \\
1000101111=60 \% \text { accuracy }
\end{array}
\]<br>
其投票结果将为：<br>
\[
\begin{array}{l}
1000101111=90 \% \text { accuracy }
\end{array}
\]<br>
可见模型之间差异越大(<strong>不是指正确率的差异，而是指模型之间相关性的差异</strong>)，融合所得的结果将会更好。</p>
<p><code>sklearn</code> 中的 <code>VotingClassifier</code> 实现了投票法。投票法的输出有两种类型：一种是直接输出类别标签，另一种是输出类别概率。前者叫做硬投票（Marjority/Hard voting），后者叫做软投票（Soft voting）。硬投票就是少数服从多数的原则，但有时候少数服从多数并不适用，更加合理的投票方式应该是有权值的投票。比如在唱歌比赛中，专业评审一人可以投 10 票，而观众一人只能投一票。</p>
<blockquote>
<ul>
<li>
<p><strong>硬投票</strong>选择各个模型输出最多的标签，如果标签数量相同，那么按照升序的次序进行选择:</p>
<p><img src="/2020/04/03/machine_learning/model_fusion/GwharR.png" alt></p>
</li>
</ul>
<p>hard voting 的少数服从多数原则在上面这种情况似乎不太合理，虽然只有模型 1 和模型 4 结果为 A，但它们俩的概率的高于 90%，也就是说很确定结果为 A，其它三个模型结果为 B，但从概率来看，并不是很确定。</p>
<ul>
<li>
<p><strong>软投票</strong>是根据各个模型输出的类别概率来进行类别的预测。如果给定权重，则会得到每个类别概率的加权平均值；否则就是普通的算术平均值。</p>
<p><img src="/2020/04/03/machine_learning/model_fusion/Gw4mFK.png" alt></p>
</li>
</ul>
</blockquote>
<p>以鸢尾花数据集测试对比投票法和单个模型的效果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line">x = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">clf1 = XGBClassifier(learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">150</span>, max_depth=<span class="number">3</span>, min_child_weight=<span class="number">2</span>, subsample=<span class="number">0.7</span>,</span><br><span class="line">                     colsample_bytree=<span class="number">0.6</span>, objective=<span class="string">'binary:logistic'</span>)</span><br><span class="line">clf2 = RandomForestClassifier(n_estimators=<span class="number">50</span>, max_depth=<span class="number">1</span>, min_samples_split=<span class="number">4</span>,</span><br><span class="line">                              min_samples_leaf=<span class="number">63</span>,oob_score=<span class="literal">True</span>)</span><br><span class="line">clf3 = SVC(C=<span class="number">0.1</span>, probability=<span class="literal">True</span>)  <span class="comment"># 软投票的时候，probability必须指定且为true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 硬投票</span></span><br><span class="line">eclf = VotingClassifier(estimators=[(<span class="string">'xgb'</span>, clf1), (<span class="string">'rf'</span>, clf2), (<span class="string">'svc'</span>, clf3)], voting=<span class="string">'hard'</span>)</span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> zip([clf1, clf2, clf3, eclf], [<span class="string">'XGBBoosting'</span>, <span class="string">'Random Forest'</span>, <span class="string">'SVM'</span>, <span class="string">'Voting'</span>]):</span><br><span class="line">    scores = cross_val_score(clf, x, y, cv=<span class="number">5</span>, scoring=<span class="string">'accuracy'</span>)</span><br><span class="line">    print(<span class="string">"Accuracy: %0.2f (+/- %0.2f) [%s]"</span> % (scores.mean(), scores.std(), label))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line">Accuracy: <span class="number">0.96</span> (+/- <span class="number">0.02</span>) [XGBBoosting]</span><br><span class="line">Accuracy: <span class="number">0.33</span> (+/- <span class="number">0.00</span>) [Random Forest]</span><br><span class="line">Accuracy: <span class="number">0.95</span> (+/- <span class="number">0.03</span>) [SVM]</span><br><span class="line">Accuracy: <span class="number">0.94</span> (+/- <span class="number">0.04</span>) [Voting]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 软投票只需要设置voting='soft'即可，这样最后的Voting正确率会成为0.96</span></span><br></pre></td></tr></table></figure>
<p>投票法非常简单，但是如果融合的模型中有些结果并不是很好，就会把整体的结果往下拉。</p>
<p>回归任务一般将多个模型的结果进行加权融合，分类任务一般采用投票法获取最终的结果。</p>
<h1 id="boosting-bagging">Boosting/Bagging</h1>
<p>Boosting/Bagging 都是从样本集的角度考虑把多个若模型集成起来的一种方式，只不过两者在集成的时候有些区别。xgb、lgb 属于 Boosting，而随机森林是 Bagging 的方式。</p>
<h2 id="boosting">Boosting</h2>
<p>Boosting 是将各种弱分类器串联起来的集成学习方式，每一个分类器的训练都依赖于前一个分类器的结果。串联（顺序运行）的方式导致了运行速度比较慢。和所有融合方式一样，它不会考虑各个弱分类器的内部结构，只是对训练数据（样本集）和连接方式进行操纵，以获得更小的误差。其基本思想是一种迭代的方法，<strong>每次训练的时候都更关心分类错误的样例，给这些分类错误的样例增加更大的权重，下一次迭代的目标就是更容易辨别出上一轮分类错误的样例</strong>。最终将这些弱分类器进行加权相加。</p>
<p><img src="/2020/04/03/machine_learning/model_fusion/GwOnHg.png" alt></p>
<p><img src="/2020/04/03/machine_learning/model_fusion/GwO33q.png" alt></p>
<p>Boosting 可以这么理解，比如用很多模型 M1,M2,…,Mn 去预测二手车的价格，但是这些模型的具体工作是这样安排的。首先让 M1 先训练然后预测价格，等 M1 预测完了之后，M2 的训练是对 M1 训练的改进和提升，即优化 M1 没有做好的事情。同样，M3 会基于 M2 的结果再次进行优化，这样一直到 Mn。这就是所谓的串联，即在训练过程中这 K 个模型之间是有依赖关系的，当引入第 i 个模型的时候，实际上是对前 i-1 个模型进行优化。最终的预测结果是对这 k 个模型结果的一个大组合。</p>
<p>Boosting 家族的代表有 adaboost、GBDT、xgboost、lightbgm 等，但是这些模型之间还是有区别的，可以分成 AdaBoost 流派和 GBDT 流派。比如 AdaBoost，在引入 M2 的时候，其实它关注的是 M1 预测不好的那些样本，这些样本在 M2 训练的时候，<strong>会加大权重</strong>。后面的模型引入也都是这个道理， 即关注前面模型预测不好的那些样本；而 GBDT，包括后面的 xgboost 这些，它们则是更加<strong>聚焦于残差</strong>，即 M2 引入的时候，它关注的是 M1 的所有预测结果与真实结果之间的差距，它想减少这个差距，后面的模型引入也是这个道理，即关注前面模型预测结果与真实结果之间的差距，然后一步一步的进行缩小。</p>
<h2 id="bagging">Bagging</h2>
<p>Bagging 是 Bootstrap Aggregating 的缩写。这种方法不对模型本身进行操作，而是作用于样本集上。采用的是随机有放回的选择性训练数据，然后构造分类器，最后进行组合。<strong>与 Boosting 方法中每个分类器之间相互依赖和串行运行不同，Bagging 方法中的学习器之间不存在强依赖关系，而是同时生成并运行</strong></p>
<p>其基本思路为：</p>
<ul>
<li>在样本集中进行 K 轮有放回的抽样，每次抽取 n 个样本，得到 K 个训练集；</li>
<li>分别用 K 个训练集训练得到 K 个模型</li>
<li>对得到的 K 个模型预测结果用投票或平均的方式进行融合</li>
</ul>
<p>在这里，训练集的选取可能不会包含所有样本集，未被包含的数据将成为<strong>包外数据</strong>，用来进行包外误差的泛化估计。每个模型的训练过程中，每次训练集可以取全部的特征进行训练，也可以随机取部分特征进行训练。极具代表性的随机森林算法就是每次随机选取部分特征。</p>
<p>下面仅从思想层面介绍随机森林算法：</p>
<ul>
<li>在样本集中进行 K 轮有放回的抽样，每次抽取 n 个样本，得到 K 个训练集，其中 n 一般远小于总样本数量</li>
<li>选取训练集，在整体特征集 M 中选取部分特征集 m 构建决策树，其中 <em>m</em>&lt;&lt;<em>M</em></li>
<li>在构造每颗决策树的过程中，按照选取最小的基尼指数进行分裂节点的选取，构建决策树。决策树的其它节点都采取相同的分裂规则进行构建，直到该节点的所有训练样例都属于同一类或达到树的最大深度</li>
<li>重复上述步骤，得到随机森林</li>
<li>多颗决策树同时进行预测，对结果进行投票或平均得到最终的分类结果</li>
</ul>
<p>多次随机选择的过程，使得随机森林不容易过拟合且有很好的抗干扰能力</p>
<h2 id="boosting-he-bagging-de-bi-jiao">Boosting和Bagging的比较</h2>
<h3 id="you-hua-fang-shi">优化方式</h3>
<p>在机器学习中，训练一个模型的过程通常是将 Loss 最小化的过程。但是单单最小化 Loss 并不能保证模型在解决一般化的问题时能够最优，甚至不能保证模型可用，也就是模型泛化能力不够。训练数据集的 Loss 与一般化数据集的 Loss 之间的差异被称为 generalization error：<br>
\[
\text {error}=\text {Bias}+\text {Variance}
\]<br>
<code>Variance</code>过大会导致模型过拟合，而<code>Bias</code>过大会导致模型欠拟合。</p>
<p><strong>Bagging 方法主要通过降低 <code>Variance</code> 来降低 <code>error</code>，Boosting 方法主要通过降低 <code>Bias</code> 来降低 <code>error</code></strong></p>
<blockquote>
<p>Bagging 方法采用多个不完全相同的训练集训练多个模型，最后结果取平均，由于\(E\left[\frac{\sum X_{i}}{n}\right]=E\left[X_{i}\right]\)，所以最终结果的 <code>Bias</code> 于单个模型的 <code>Bias</code> 很相近，一般不会显著降低 <code>Bias</code>。</p>
<p>对于 Variance</p>
<ol>
<li>子模型相互独立时，有：\(\operatorname{Var}\left[\frac{\sum X_{i}}{n}\right]=\frac{\operatorname{Var}\left[X_{i}\right]}{n}\)</li>
<li>子模型完全相同时，有：\(\operatorname{Var}\left[\frac{\sum X_{i}}{n}\right]=\operatorname{Var}\left[X_{i}\right]\)。</li>
</ol>
<p>Bagging 的多个子模型由不完全相同的数据集训练而成，子模型间有一定的相关性但又不完全独立，所以其结果在上述两式的中间状态，因此可以在一定程度上降低 Variance，从而使得总 error 减小。</p>
</blockquote>
<blockquote>
<p>Boosting 方法从优化角度来说， 是用 forward-stagewise 这种贪心法去最小化损失函数\(L\left(y, \sum a_{i} f_{i}(x)\right)\)。forward-stagewise 就是在迭代的第 n 步，求解新的子模型 \(f(x)\)及步长 <em>a</em> 来最小化\(L\left(y, f_{n-1}(x)+a f(x)\right)\),这里的\(f_{n-1}(x)\)是前 n 步得到的子模型的和。因此 Boosting 在最小化损失函数，Bias 自然逐步下降，而由于模型之间的强相关性，所以并不能显著降低 Variance。</p>
</blockquote>
<h3 id="yang-ben-xuan-ze">样本选择</h3>
<p>Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。</p>
<p>Boosting：每一轮的训练集不变，只是训练集中每个样本在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整的。</p>
<h3 id="yang-ben-quan-zhong">样本权重</h3>
<p>Bagging：使用均匀取样，每个样本的权重相等</p>
<p>Boosting：根据错误率不断调整样本的权重，错误率越大则权重越大</p>
<h3 id="yu-ce-han-shu">预测函数</h3>
<p>Bagging：所有预测函数的权重相等</p>
<p>Boosting：每个弱分类器都有相应的权重，误差小的分类器权重更大</p>
<h3 id="bing-xing-ji-suan">并行计算</h3>
<p>Bagging：各个预测函数可以并行生成</p>
<p>Boosting：理论上各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果</p>
<h1 id="stacking-blending">Stacking/Blending</h1>
<h2 id="stacking">Stacking</h2>
<p>Stacking 的本质是一种分层的结构，用了大量的基分类器，将其预测的结果作为下一层输入的特征，这样的结构使得它比相互独立训练的模型能够获得更多的特征。</p>
<p>下面以一种易于理解但不会实际使用的两层 stacking 方法为例，简要说明其结构和工作原理：</p>
<p><img src="/2020/04/03/machine_learning/model_fusion/G0PSzQ.png" alt></p>
<p>假设有三个基模型 <code>M1</code>，<code>M2</code>，<code>M3</code> 和一个元模型 <code>M4</code>，有训练集 train 和测试集 test，则：</p>
<ol>
<li>用训练集 train 训练基模型 M1（<code>M1.fit(train)</code>），然后分别在 train 和 test 上做预测，得到 P1（<code>M1.predict(train)</code>）和 T1（<code>M1.predict(test)</code>）</li>
<li>用训练集 train 训练基模型 M2（<code>M2.fit(train)</code>），然后分别在 train 和 test 上做预测，得到 P2（<code>M2.predict(train)</code>）和 T2（<code>M2.predict(test)</code>）</li>
<li>用训练集 train 训练基模型 M3（<code>M3.fit(train)</code>），然后分别在 train 和 test 上做预测，得到 P3（<code>M3.predict(train)</code>）和 T3（<code>M3.predict(test)</code>）</li>
</ol>
<p>这样第一层的模型就训练结束了，接下来</p>
<ol>
<li>把 P1，P2，P3 进行合并组成新的训练集 train2，把 T1，T2，T3 进行合并组成新的测试集 test2</li>
<li>用新的训练集 train2 训练元模型 M4（<code>M4.fit(train2)</code>），然后在 test2 上进行预测得到最终的预测结果 Y_pred（<code>M4.predict(test2)</code>）</li>
</ol>
<p>这样第二层训练预测就得到了最终的预测结果。这就是两层堆叠的一种基本的原始思路。Stacking 本质上就是这么直接的思路，但是直接这样做，对于训练集和测试集分布不那么一致的情况下是有问题的，<strong>其问题在于用训练集训练原始模型，又接着用训练的模型去预测训练集，会严重过拟合</strong>,因此，问题变成如何降低再训练的过拟合问题。一般有两种解决方法:</p>
<ul>
<li>次级模型尽量选择简单的线性模型</li>
<li>第一层训练模型使用交叉验证的方式</li>
</ul>
<p>第一种方法很容易理解，重点是看第二种方法到底是怎么做的:</p>
<p><img src="/2020/04/03/machine_learning/model_fusion/G0i24K.png" alt></p>
<p>以 5 折交叉验证为例</p>
<ol>
<li>
<p>首先将训练集分成 5 份。</p>
</li>
<li>
<p>对于每一个基模型 \(i\) 来说，用其中 4 份进行训练，然后用另一份训练集作验证集进行预测，得到 \(P_i\) 的一部分，然后再用测试集进行预测得到 \(T_i\) 的一部分，这样 5 轮下来之后，验证集的预测值就会拼接成一个完整的 P，测试集的 label 值取个平均就会得到一个完整的 T。</p>
</li>
<li>
<p>所有的 \(P_i\) 合并就得到了下一层的训练集 train2，所有的 \(T_i\) 合并就得到了下一层的测试集 test2。</p>
</li>
<li>
<p>利用 train2 训练第二层的模型，然后在 test2 上得到预测结果，就是最终的结果。</p>
<p><img src="/2020/04/03/machine_learning/model_fusion/G0knSS.png" alt></p>
</li>
</ol>
<p>Stacking 的过程可以用下面的图表示：</p>
<p><img src="/2020/04/03/machine_learning/model_fusion/G0kay4.png" alt></p>
<h3 id="hui-gui-zhong-de-stacking">回归中的stacking</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成一些简单的样本数据， test_predi代表第i个模型的预测值</span></span><br><span class="line">train_reg1 = [<span class="number">3.2</span>, <span class="number">8.2</span>, <span class="number">9.1</span>, <span class="number">5.2</span>]</span><br><span class="line">train_reg2 = [<span class="number">2.9</span>, <span class="number">8.1</span>, <span class="number">9.0</span>, <span class="number">4.9</span>]</span><br><span class="line">train_reg3 = [<span class="number">3.1</span>, <span class="number">7.9</span>, <span class="number">9.2</span>, <span class="number">5.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_test_true代表模型的真实值</span></span><br><span class="line">y_train_true = [<span class="number">3</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">test_pred1 = [<span class="number">1.2</span>, <span class="number">3.2</span>, <span class="number">2.1</span>, <span class="number">6.2</span>]</span><br><span class="line">test_pred2 = [<span class="number">0.9</span>, <span class="number">3.1</span>, <span class="number">2.0</span>, <span class="number">5.9</span>]</span><br><span class="line">test_pred3 = [<span class="number">1.1</span>, <span class="number">2.9</span>, <span class="number">2.2</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line">y_test_true = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Stacking_method</span><span class="params">(train_reg1, train_reg2, train_reg3, y_train_true, test_pred1, test_pred2, test_pred3, model_L2 = LinearRegression<span class="params">()</span>)</span>:</span></span><br><span class="line">    model_L2.fit(pd.concat([pd.Series(train_reg1), pd.Series(train_reg2), pd.Series(train_reg3)], axis=<span class="number">1</span>).values, y_train_true)</span><br><span class="line">    Stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1), pd.Series(test_pre2), pd.Series(test_pre3)], axis=<span class="number">1</span>).values)</span><br><span class="line">    <span class="keyword">return</span> Stacking_result</span><br><span class="line"> </span><br><span class="line">model_L2 = LinearRegression()</span><br><span class="line">Stacking_pre = Stacking_method(train_reg1, train_reg2, train_reg3, y_train_true,</span><br><span class="line">                               test_pre1, test_pre2, test_pre3, model_L2)</span><br><span class="line">print(<span class="string">'Stacking_pre MAE:'</span>, mean_absolute_error(y_test_true, Stacking_pre))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 结果：</span></span><br><span class="line">Stacking_pre MAE: <span class="number">0.04213</span></span><br></pre></td></tr></table></figure>
<p>这里的逻辑就是把第一层模型在训练集上的预测值，当作第二层训练集的特征，第一层模型在测试集上的预测值，当作第二层测试集的特征，然后在第二层建立一个简单的线性模型进行训练。</p>
<p>可以发现最终误差相对于之前进一步提升了，需要注意的是，<strong>第二层的模型不宜选的过于复杂</strong>，否则会导致模型过拟合。</p>
<p>接下来介绍一款强大的 stacking 工具 StackingCVRegressor，这是一种继承学习的元回归器，首先导入</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mxltend.regressor <span class="keyword">import</span> StackingCVRegressor</span><br></pre></td></tr></table></figure>
<p>在标准 stacking 过程中，拟合一级回归器的时候，如果使用了第二级回归器输入的相同训练集，就会导致过拟合。但是，StackingCVRegressor 使用了 “非折叠预测” 的概念：数据被分成 K 折，并且在 K 个连续的循环中，使用 K-1 折来拟合第一级回归器（即 K 折交叉验证的 StackingRegressor）。在每一轮中（一共 K 轮），一级回归器先后被应用于在每次迭代中还未用过的 1 个子集，然后将得到的预测叠加起来作为输入数据提供给二级回归器。在 StackingCVRegressor 训练完之后，一级回归器拟合整个数据集以获得最佳预测。这就是前面介绍的原理。</p>
<p>具体 API 及参数如下：</p>
<blockquote>
<p>StackingCVRegressor(regressors，meta_regressor，cv = 5，shuffle = True，use_features_in_secondary = False)</p>
<ul>
<li>regressors：基回归器，列表的形式，第一层模型。例如我打算第一层用 xgb 和 lgb，第二层用线性模型，那么这里就应该写 [xgb,lgb]</li>
<li>meta_regressor：元回归器，可以理解为第二层的模型，一般不能太复杂，例如使用一个普通的线性模型 lr</li>
<li>cv：交叉验证策略，默认是 5 折交叉验证</li>
<li>use_features_in_secondary：默认是 False，表示第二层的回归器只接受第一层回归器的结果进行训练和预测。如果设置为 True，表示第二层的回归器不仅接收第一层回归器的结果，还接收原始的数据集一块进行训练</li>
<li>shuffle：是否打乱样本的顺序</li>
<li>训练依然是用<code>.fit(x, y)</code>，但这里的 x 和 y 要求是数组，所以如果是 DataFrame，需要<code>np.array()</code>一下，并且 x 的 shape 应该是 (n_samples，n_features)，y 的 shape 应该是（n_samples）</li>
<li>预测依然是用<code>.predict(x_test)</code>，只不过 x_test 也是数组，形状和上面的一样</li>
</ul>
</blockquote>
<h3 id="fen-lei-zhong-de-stacking">分类中的stacking</h3>
<p>以鸢尾花数据集为例，首先手写实现 stacking 加深理解，然后使用<code>mlxtend.classifier.StackingClassifier</code>实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier, GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>, :]</span><br><span class="line"></span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型融合中用到的单个模型</span></span><br><span class="line">clfs = [LogisticRegression(solver=<span class="string">'lbfgs'</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'entropy'</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分一部分数据作为训练集</span></span><br><span class="line">X, X_predict, y, y_predict = train_test_split(data, target, test_size=<span class="number">0.3</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">dataset_blend_train = np.zeros((X.shape[<span class="number">0</span>], len(clfs)))   <span class="comment"># 每个模型的预测作为第二层的特征</span></span><br><span class="line">dataset_blend_test = np.zeros((X_predict.shape[<span class="number">0</span>], len(clfs)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5折stacking</span></span><br><span class="line">n_splits = <span class="number">5</span></span><br><span class="line">skf = StratifiedKFold(n_splits)</span><br><span class="line">skf = skf.split(X, y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> enumerate(clfs):</span><br><span class="line">    <span class="comment"># 依次训练各个单模型</span></span><br><span class="line">    dataset_blend_test_j = np.zeros((X_predict.shape[<span class="number">0</span>], <span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> i, (train, test) <span class="keyword">in</span> enumerate(skf):</span><br><span class="line">        <span class="comment"># 5—fold交叉训练，使用第i个部分作为预测， 剩余的部分来训练模型， 获得其预测的输出作为第i部分的新特征。</span></span><br><span class="line">        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]</span><br><span class="line">        clf.fit(X_train, y_train)</span><br><span class="line">        y_submission = clf.predict_proba(X_test)[:,<span class="number">1</span>]</span><br><span class="line">        dataset_blend_train[test, j] = y_submission</span><br><span class="line">        dataset_blend_test_j[:, j] = clf.predict_proba(X_predict)[:, <span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 对于测试集， 直接用这k个模型的预测值均值作为新的特征</span></span><br><span class="line">    dataset_blend_test[:, j] = dataset_blend_test_j.mean(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">"val auc Score: %f"</span> % roc_auc_score(y_predict, dataset_blend_test[:, j]))</span><br><span class="line"></span><br><span class="line">clf = LogisticRegression(solver=<span class="string">'lbfgs'</span>)</span><br><span class="line">clf.fit(dataset_blend_train, y)</span><br><span class="line">y_submission = clf.predict_proba(dataset_blend_test)[:,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Val auc Score of Stacking: %f"</span> % (roc_auc_score(y_predict, y_submission)))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 结果如下：</span></span><br><span class="line">val auc Score: <span class="number">1.000000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">val auc Score: <span class="number">0.500000</span></span><br><span class="line">Val auc Score of Stacking: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure>
<p>StackingClassifier 的 API 及参数如下：</p>
<blockquote>
<p>StackingClassifier(classifiers, meta_classifier, use_probas=False, average_probas=False, verbose=0, use_features_in_secondary=False)， 这里的参数和上面的 StackingCVRegressor 基本上差不多</p>
<ul>
<li>classifiers：基分类器， 数组形式 [clf1, clf2, clf3], 每个基分类器的属性被存储在类属性 <code>self.clfs_</code>中</li>
<li>meta_classifier：目标分类器，即第二层的分类器</li>
<li>use_probas：bool (default: False) 。如果设置为 True， 那么目标分类器的输入就是前面分类输出的类别概率值而不是类别标签</li>
<li>average_probas：bool (default: False)。用来设置上一个参数当使用概率值输出的时候是否使用平均值</li>
<li>verbose：int, optional (default=0)。用来控制使用过程中的日志输出，当 <code>verbose = 0</code>时，什么也不输出；<code>verbose = 1</code>时，输出回归器的序号和名字；<code>verbose = 2</code>时，输出详细的参数信息</li>
<li>use_features_in_secondary：bool (default: False)。如果设置为 True，那么最终的目标分类器就被基分类器产生的数据和最初的数据集同时训练。如果设置为 False，最终的分类器只会使用基分类器产生的数据训练。</li>
<li>常用方法： <code>.fit()</code>，<code>.predict()</code></li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mlxtend.classifier <span class="keyword">import</span> StackingCVClassifier</span><br><span class="line"><span class="comment"># 上面的这个操作，如果换成StackingClassifier， 是这样的形式：</span></span><br><span class="line">clf1 = RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>)</span><br><span class="line">clf2 = ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>)</span><br><span class="line">clf3 = ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'entropy'</span>)</span><br><span class="line">clf4 = GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)</span><br><span class="line">clf5 = LogisticRegression(solver=<span class="string">'lbfgs'</span>)</span><br><span class="line"></span><br><span class="line">sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3, clf4], meta_classifier=clf, cv=<span class="number">3</span>)</span><br><span class="line">sclf.fit(X, y)</span><br><span class="line"><span class="comment"># 5这交叉验证</span></span><br><span class="line"><span class="comment">#scores = cross_val_score(sclf, X, y, cv=3, scoring='accuracy')</span></span><br><span class="line"></span><br><span class="line">y_submission = sclf.predict(X_predict)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Val auc Score of Stacking: %f"</span> % (roc_auc_score(y_predict, y_submission)))</span><br></pre></td></tr></table></figure>
<h2 id="blending">Blending</h2>
<p>Blending 是一种和 Stacking 很相像的模型融合方式，它与 Stacking 的区别在于训练集不是通过 K-Fold 的策略来获得预测值，而是先建立一个 Holdout（留出集）。</p>
<h3 id="blending-dan-chun-holdout">Blending（单纯Holdout）</h3>
<p>单纯的 Holdout 就是直接把训练集分成两部分，70% 作为新的训练集，30% 作为验证集，然后用这 70% 的训练集分别训练第一层的模型，然后在 30% 的验证集上进行预测，把预测的结果作为第二层模型的训练集特征，这是训练部分。预测部分就是把<strong>真正的测试集</strong>先用第一层的模型预测，把预测结果作为第二层测试集的特征进行第二层的预测。 过程图如下：</p>
<p><img src="/2020/04/03/machine_learning/model_fusion/G0nNNj.png" alt></p>
<p>这种方法实现起来也比较容易，基本和 stacking 的代码差不多，只不过少了内层的循环，毕竟每个模型不用交叉验证了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#创建训练的数据集</span></span><br><span class="line"><span class="comment">#创建训练的数据集</span></span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:<span class="number">100</span>,:]</span><br><span class="line"></span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:<span class="number">100</span>]</span><br><span class="line"> </span><br><span class="line"><span class="comment">#模型融合中使用到的各个单模型</span></span><br><span class="line">clfs = [LogisticRegression(solver=<span class="string">'lbfgs'</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'entropy'</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>),</span><br><span class="line">        <span class="comment">#ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),</span></span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"> </span><br><span class="line"><span class="comment">#切分一部分数据作为测试集</span></span><br><span class="line">X, X_predict, y, y_predict = train_test_split(data, target, test_size=<span class="number">0.3</span>, random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#切分训练数据集为d1,d2两部分</span></span><br><span class="line">X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=<span class="number">0.5</span>, random_state=<span class="number">2020</span>)</span><br><span class="line">dataset_d1 = np.zeros((X_d2.shape[<span class="number">0</span>], len(clfs)))</span><br><span class="line">dataset_d2 = np.zeros((X_predict.shape[<span class="number">0</span>], len(clfs)))</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> enumerate(clfs):</span><br><span class="line">    <span class="comment">#依次训练各个单模型</span></span><br><span class="line">    clf.fit(X_d1, y_d1)</span><br><span class="line">    y_submission = clf.predict_proba(X_d2)[:, <span class="number">1</span>]</span><br><span class="line">    dataset_d1[:, j] = y_submission</span><br><span class="line">    <span class="comment">#对于测试集，直接用这k个模型的预测值作为新的特征。</span></span><br><span class="line">    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="comment">#print("val auc Score: %f" % roc_auc_score(y_predict, dataset_d2[:, j]))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#融合使用的模型</span></span><br><span class="line">clf = GradientBoostingClassifier(learning_rate=<span class="number">0.02</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">30</span>)</span><br><span class="line">clf.fit(dataset_d1, y_d2)</span><br><span class="line">y_submission = clf.predict_proba(dataset_d2)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">"Val auc Score of Blending: %f"</span> % (roc_auc_score(y_predict, y_submission)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果：</span></span><br><span class="line">Val auc Score of Blending: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure>
<h3 id="blending-holdout-jiao-cha">Blending(Holdout交叉)</h3>
<p>第二种引入了交叉验证的思想，也就是每个模型看到的 Holdout 集合并不一样。说白了，就是把 Stacking 流程中的 K-Fold CV 改成 HoldOut CV。第二阶段的 stacker 模型就基于第一阶段模型对这 30% 训练数据的预测值进行拟合</p>
<ol>
<li>在第一层中， 用 70% 的训练集训练多个模型， 然后去预测那 30% 的数据得到预测值 \(P_i\)， 同时也预测 test 集得到预测值 \(T_i\)。这里注意，那 30% 的数据每个模型并不是一样，也是类似于交叉验证的那种划分方式，只不过 stacking 那里是每个模型都会经历 K 折交叉验证，也就是有多少模型，就会有多少次 K 折交叉验证，而 blending 这里是所有模型合起来只经历了一次 K 折交叉验证（看下图就容易懂了）</li>
<li>第二层直接对 <em>P**i</em> 进行合并，作为新的训练集 train2，test 集的预测值 <em>T**i</em> 合并作为新的测试集 test2，然后训练第二层的模型</li>
</ol>
<p>Blending 的过程训练和预测过程可以使用下图来表示：</p>
<p><img src="/2020/04/03/machine_learning/model_fusion/G0ufoQ.png" alt></p>
<blockquote>
<p>Blending 的优势在于：</p>
<ul>
<li>Blending 比较简单，而 Stacking 相对比较复杂；</li>
<li>能够防止信息泄露：generalizers 和 stackers 使用不同的数据；</li>
</ul>
<p>Blending 缺点在于：</p>
<ul>
<li>只用了整体数据的一部分；</li>
<li>最终模型可能对留出集（holdout set）过拟合；</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型融合中用到的单个模型</span></span><br><span class="line">clfs = [LogisticRegression(solver=<span class="string">'lbfgs'</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'gini'</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, criterion=<span class="string">'entropy'</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分一部分数据作为训练集</span></span><br><span class="line">X, X_predict, y, y_predict = train_test_split(data, target, test_size=<span class="number">0.3</span>, random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line">dataset_blend_train = np.zeros((int(X.shape[<span class="number">0</span>]/n_splits), len(clfs)))   <span class="comment"># 每个模型的预测作为第二层的特征</span></span><br><span class="line">dataset_blend_test = np.zeros((X_predict.shape[<span class="number">0</span>], len(clfs)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5折stacking</span></span><br><span class="line">n_splits = <span class="number">5</span></span><br><span class="line">skf = StratifiedKFold(n_splits)</span><br><span class="line">skf = skf.split(X, y)</span><br><span class="line"></span><br><span class="line">fold = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i, (train, test) <span class="keyword">in</span> enumerate(skf):</span><br><span class="line">    fold[i] = (X[train], y[train], X[test], y[test])</span><br><span class="line">Y_blend = []</span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> enumerate(clfs):</span><br><span class="line">    <span class="comment"># 依次训练各个单模型</span></span><br><span class="line">    dataset_blend_test_j = np.zeros((X_predict.shape[<span class="number">0</span>], <span class="number">5</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 5—fold交叉训练，使用第i个部分作为预测， 剩余的部分来训练模型， 获得其预测的输出作为第i部分的新特征。</span></span><br><span class="line">    X_train, y_train, X_test, y_test = fold[j]</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    dataset_blend_train[:, j] =  clf.predict(X_test)</span><br><span class="line">    Y_blend.extend(y_test)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对于测试集，直接用这k个模型的预测值作为新的特征</span></span><br><span class="line">    dataset_blend_test[:, j] = clf.predict(X_predict)</span><br><span class="line">        </span><br><span class="line">    print(<span class="string">"val auc Score: %f"</span> % roc_auc_score(y_predict, dataset_blend_test[:, j]))</span><br><span class="line"></span><br><span class="line">dataset_blend_train = dataset_blend_train.T.reshape(<span class="number">70</span>, <span class="number">-1</span>)</span><br><span class="line">dataset_blend_test = np.mean(dataset_blend_test, axis=<span class="number">1</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">Y_blend = np.array(Y_blend).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">clf = LogisticRegression(solver=<span class="string">'lbfgs'</span>)</span><br><span class="line">clf.fit(dataset_blend_train, Y_blend)</span><br><span class="line">y_submission = clf.predict(dataset_blend_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Val auc Score of Stacking: %f"</span> % (roc_auc_score(y_predict, y_submission)))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 结果：</span></span><br><span class="line">Val auc Score of Stacking: <span class="number">1.000000</span></span><br></pre></td></tr></table></figure>
<h1 id="zong-jie">总结</h1>
<p>这篇文章是基于已经调参好的模型去研究如何发挥出模型更大的性能。从模型的结果、样本集的集成和模型自身融合三个方面去整理。</p>
<ol>
<li><strong>模型的结果方面</strong>，对于回归问题，可以对模型的结果进行加权融合等方式；对于分类问题，我们可以使用 Voting 的方式去得到最终的结果。</li>
<li><strong>样本集的集成技术方面</strong>，有 Boosting 和 Bagging 方式，都是把多个弱分类器进行集成的技术，但是两者是不同的。</li>
<li><strong>模型自身的融合方面</strong>， Stacking 和 Blending 的原理及具体实现方法，介绍了 mlxtend 库里面的模型融合工具</li>
</ol>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://wmathor.com/index.php/archives/1428/" target="_blank" rel="noopener">模型融合</a></li>
<li><a href="https://www.cnblogs.com/libin47/p/11169994.html" target="_blank" rel="noopener">模型融合方法学习总结</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>模型融合</tag>
      </tags>
  </entry>
  <entry>
    <title>建模调参</title>
    <url>/2020/04/02/machine_learning/modeling_modify_parameters/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/04/02/machine_learning/modeling_modify_parameters/%E5%BB%BA%E6%A8%A1%E8%B0%83%E5%8F%82.png" alt></p>
<a id="more"></a>
<p><code><a class="btn" href="建模调参.xmind">
            <i class="fa fa-download"></i>xmind文件下载
          </a></code></p>
<p><a href="https://share.weiyun.com/EDpohPCk" target="_blank" rel="noopener">数据集下载</a></p>
<p>本篇文章将会从简单的线性模型开始，了解如何建立一个模型以及建立完模型之后要分析什么东西，然后学习交叉验证的思想和技术，并且会构建一个线下测试集，之后我们会尝试建立更多的模型去解决这个问题，并对比它们的效果，当把模型选择出来之后，我们还得掌握一些调参的技术发挥模型最大的性能，模型选择出来之后，也调完参数，但是模型真的就没有问题了吗？我们还需要绘制学习率曲线看模型是否存在过拟合或者欠拟合的问题并给出相应的解决方法。</p>
<h1 id="cong-jian-dan-de-xian-xing-mo-xing-kai-shi">从简单的线性模型开始</h1>
<p><a href="https://tianchi.aliyun.com/competition/entrance/231784/information" target="_blank" rel="noopener">二手车交易价格预测</a>比赛是一个回归问题，所以需要选择一些回归模型来解决，线性模型就是一个比较简单的回归模型了，所以就从这个模型开始，看看针对这个模型，会得到什么结果以及这些结果究竟是什么含义。</p>
<p>线性回归 (Linear Regression) 是利用最小平方损失函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。简单的说，假设预测的二手车价格用\(Y\)来表示，而构造的特征用\(x_i\)，之后就可以建立如下的等式来描述它们的关系<br>
\[
Y=w_{1} x_{1}+w_{2} x_{2}+\ldots+w_{n} x_{n}+b
\]<br>
训练模型其实就是根据训练集的\(\left(x_{1}, x_{2}, \ldots, x_{n}, Y\right)\)样本求出合适权重\(\left(w_{1}, w_{2}, \dots, w_{n}\right)\)的过程。</p>
<p>首先导入特征工程处理完毕后保存的数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入之前处理好的数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">'./pre_data/pre_data.csv'</span>)</span><br><span class="line">data.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后训练集和测试集分开</span></span><br><span class="line">train = data[:train_data.shape[<span class="number">0</span>]]</span><br><span class="line">test = data[train_data.shape[<span class="number">0</span>]:]    <span class="comment"># 这个先不用</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择那些数值型的数据特征</span></span><br><span class="line">continue_fea = [<span class="string">'power'</span>, <span class="string">'kilometer'</span>, <span class="string">'v_2'</span>, <span class="string">'v_3'</span>, <span class="string">'v_4'</span>, <span class="string">'v_5'</span>, <span class="string">'v_6'</span>, <span class="string">'v_10'</span>, <span class="string">'v_11'</span>, <span class="string">'v_12'</span>, <span class="string">'v_14'</span>,</span><br><span class="line">                <span class="string">'v_std'</span>, <span class="string">'fuelType_price_average'</span>, <span class="string">'gearbox_std'</span>, <span class="string">'bodyType_price_average'</span>, <span class="string">'brand_price_average'</span>,</span><br><span class="line">                <span class="string">'used_time'</span>, <span class="string">'estivalue_price_average'</span>, <span class="string">'estivalueprice_std'</span>, <span class="string">'estivalue_price_min'</span>]</span><br><span class="line">train_x = train[continue_fea]</span><br><span class="line">train_y = train_data[<span class="string">'price'</span>]</span><br></pre></td></tr></table></figure>
<p>然后建立线性模型，直接使用 sklearn 库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">model = LinearRegression(normalize=<span class="literal">True</span>)</span><br><span class="line">model.fit(train_x, train_y)</span><br></pre></td></tr></table></figure>
<p>通过上面两行代码，其实就已经建立并且训练完了一个线性模型，接下来可以查看一下模型的一些参数\(w,b\)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""查看训练的线性回归模型的截距(intercept)与权重(coef)"""</span></span><br><span class="line">print(<span class="string">'intercept: '</span> + str(model.intercept_))</span><br><span class="line">sorted(dict(zip(continue_fea, model.coef_)).items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 结果：</span></span><br><span class="line">intercept: <span class="number">-178881.74591832393</span></span><br><span class="line">[(<span class="string">'v_6'</span>, <span class="number">482008.29891714785</span>),</span><br><span class="line"> (<span class="string">'v_std'</span>, <span class="number">23713.66414841167</span>),</span><br><span class="line"> (<span class="string">'v_10'</span>, <span class="number">7035.056136559963</span>),</span><br><span class="line"> (<span class="string">'v_14'</span>, <span class="number">1418.4037751433352</span>),</span><br><span class="line"> (<span class="string">'used_time'</span>, <span class="number">186.48306334062053</span>),</span><br><span class="line"> (<span class="string">'power'</span>, <span class="number">12.19202369791551</span>),</span><br><span class="line"> (<span class="string">'estivalue_price_average'</span>, <span class="number">0.4082359327905722</span>),</span><br><span class="line"> (<span class="string">'brand_price_average'</span>, <span class="number">0.38196351334425965</span>),</span><br><span class="line"> (<span class="string">'gearbox_std'</span>, <span class="number">0.1716754674248321</span>),</span><br><span class="line"> (<span class="string">'fuelType_price_average'</span>, <span class="number">0.023785798378739224</span>),</span><br><span class="line"> (<span class="string">'estivalueprice_std'</span>, <span class="number">-0.016868767797045624</span>),</span><br><span class="line"> (<span class="string">'bodyType_price_average'</span>, <span class="number">-0.21364358471329278</span>),</span><br><span class="line"> (<span class="string">'kilometer'</span>, <span class="number">-155.11999534761347</span>),</span><br><span class="line"> (<span class="string">'estivalue_price_min'</span>, <span class="number">-574.6952072539285</span>),</span><br><span class="line"> (<span class="string">'v_11'</span>, <span class="number">-1164.0263997737668</span>),</span><br><span class="line"> (<span class="string">'v_12'</span>, <span class="number">-1953.0558048250668</span>),</span><br><span class="line"> (<span class="string">'v_4'</span>, <span class="number">-2198.03802357537</span>),</span><br><span class="line"> (<span class="string">'v_3'</span>, <span class="number">-3811.7514971187525</span>),</span><br><span class="line"> (<span class="string">'v_2'</span>, <span class="number">-5116.825271420712</span>),</span><br><span class="line"> (<span class="string">'v_5'</span>, <span class="number">-447495.6394686485</span>)]</span><br></pre></td></tr></table></figure>
<p>上面的这些就是等式中每个\(x_i\)前面的系数 ， intercept 代表\(b\)。如果已经有了一系列\[\left(x_{1}, x_{2}, \ldots, x_{n}\right)\] 的样本，要预测\(y\)，只需要:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_pred = model.predict(x_test)</span><br></pre></td></tr></table></figure>
<p>虽然线性模型非常简单，但是关于线性模型还有些重要的东西我们得了解一下，比如从这些权重中如何看出哪个特征对线性模型来说更加重要些？这个其实我们看的是权重的绝对值，因为正相关和负相关都是相关，越大的说明那个特征对线性模型影响就越大。</p>
<p>其次，我们还可以看一下线性回归的训练效果，绘制一下 v_6 这个特征和标签的散点图：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">subsample_index = np.random.randint(low=<span class="number">0</span>, high=len(train_y), size=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(train_x[<span class="string">'v_6'</span>][subsample_index], train_y[subsample_index], color=<span class="string">'black'</span>)</span><br><span class="line">plt.scatter(train_x[<span class="string">'v_6'</span>][subsample_index], model.predict(train_x.loc[subsample_index]), color=<span class="string">'blue'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'v_6'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'price'</span>)</span><br><span class="line">plt.legend([<span class="string">'True Price'</span>,<span class="string">'Predicted Price'</span>],loc=<span class="string">'upper right'</span>)</span><br><span class="line">print(<span class="string">'The predicted price is obvious different from true price'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/modeling_modify_parameters/G8UgOO.png" alt></p>
<p>从上图中我们可以发现发现模型的预测结果（蓝色点）与真实标签（黑色点）的分布差异较大，且部分预测值出现了小于 0 的情况，说明我们的模型存在一些问题。 这个还是需要会看的，从这里我们也可以看出或许 price 这个需要处理一下。</p>
<p>price 的分布图如下：</p>
<p><img src="/2020/04/02/machine_learning/modeling_modify_parameters/G8UH6f.png" alt></p>
<p>通过这张图我们发现 price 呈长尾分布，不利于我们的建模预测。<strong>原因是很多模型都假设数据误差项符合正态分布，而长尾分布的数据违背了这一假设</strong>。</p>
<p><code><a class="btn" href="分布假设.pdf">
            <i class="fa fa-download"></i>回归分析的五个基本假设
          </a></code></p>
<p>所以可以先尝试取个对数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_y_ln = np.log1p(train_y)</span><br><span class="line">print(<span class="string">'The transformed price seems like normal distribution'</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">sns.distplot(train_y_ln)</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">sns.distplot(train_y_ln[train_y_ln &lt; np.quantile(train_y_ln, <span class="number">0.9</span>)])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/modeling_modify_parameters/G8Uzhn.png" alt></p>
<p>这样效果就好多了，然后重新训练一下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = model.fit(train_x, train_y_ln)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'intercept:'</span>+ str(model.intercept_))</span><br><span class="line">sorted(dict(zip(continue_fea, model.coef_)).items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>画出 v_6 和 price 的散点图看一下：</p>
<p><img src="/2020/04/02/machine_learning/modeling_modify_parameters/G8a3He.png" alt></p>
<h2 id="jiao-cha-yan-zheng">交叉验证</h2>
<p>在使用数据集对参数进行训练的时候，经常会发现人们通常会将整个训练集分为三个部分：训练集、验证集和测试集。这其实是为了保证训练效果而特意设置的。测试集很好理解，就是完全不参与训练的过程，仅仅用来观测测试效果的数据。而训练集和验证集则牵涉到下面的知识。</p>
<p>因为在实际的训练中，训练的结果对于训练集的拟合程度通常还是挺好的（初始条件敏感），但是对于训练集之外的数据的拟合程度通常就不那么令人满意了。因此我们通常并不会把所有的数据集都拿来训练，而是分出一部分来（这一部分不参加训练）对训练集生成的模型进行测试，相对客观的判断这个模型对训练集之外的数据的符合程度。在验证中，比较常用的就是 K 折交叉验证了，它可以有效的避免过拟合，最后得到的结果也比较具有说服性</p>
<p>K 折交叉验证是将原始数据分成 K 组，将每个子集数据分别做一次验证集，其余的 K-1 组子集数据作为训练集，这样会得到 K 个模型，用这 K 个模型最终的验证集分类准确率的平均数，作为此 K 折交叉验证下分类器的性能指标。以下图为例：</p>
<p><img src="/2020/04/02/machine_learning/modeling_modify_parameters/G8dwGR.png" alt></p>
<p>交叉验证，sklearn 中提供了一个函数，叫做<code>cross_val_score</code>，用这个函数实现交叉验证，函数具体的作用可以去查一下 sklearn 的官方文档。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error, make_scorer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_transfer</span><span class="params">(func)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(y, yhat)</span>:</span></span><br><span class="line">        result = func(np.log(y), np.nan_to_num(np.log(yhat)))   <span class="comment"># 这个是为了解决不合法的值的</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面是交叉验证</span></span><br><span class="line">scores = cross_val_score(model, X=train_x, y=train_y, verbose=<span class="number">1</span>, cv=<span class="number">5</span>, scoring=make_scorer(log_transfer(mean_absolute_error)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用线性回归模型，对未处理标签的特征数据进行五折交叉验证（Error 1.36）</span></span><br><span class="line">print(<span class="string">'AVG:'</span>, np.mean(scores))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对处理的标签交叉验证</span></span><br><span class="line">scores = cross_val_score(model, X=train_x, y=train_y_ln, verbose=<span class="number">1</span>, cv = <span class="number">5</span>, scoring=make_scorer(mean_absolute_error))</span><br><span class="line">print(<span class="string">'AVG:'</span>, np.mean(scores))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出五次的验证结果：</span></span><br><span class="line">scores = pd.DataFrame(scores.reshape(<span class="number">1</span>,<span class="number">-1</span>))</span><br><span class="line">scores.columns = [<span class="string">'cv'</span> + str(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>)]</span><br><span class="line">scores.index = [<span class="string">'MAE'</span>]</span><br><span class="line">scores</span><br></pre></td></tr></table></figure>
<p>得到的结果如下：</p>
<table>
<thead>
<tr>
<th>cv1</th>
<th>cv2</th>
<th>cv3</th>
<th>cv4</th>
<th>cv5</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.194979</td>
<td>0.195399</td>
<td>0.19679</td>
<td>0.19257</td>
<td>0.197563</td>
</tr>
</tbody>
</table>
<p>k 折交叉验证，并不适合处理时间序列数据，因为时间序列是有先后关系的。就拿这次比赛来说，通过 2018 年的二手车价格预测 2017 年的二手车价格，显然是不合理的，因此可以采用时间顺序对数据集进行分隔。在本例中，我们选用靠前时间的 4/5 样本当作训练集，靠后时间的 1/5 当作验证集，最终结果与五折交叉验证差距不大。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">split_point = len(train_x) // <span class="number">5</span> * <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">xtrain = train_x[:split_point]</span><br><span class="line">ytrain = train_y[:split_point]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">xval = train_x[split_point:]</span><br><span class="line">yval = train_y[split_point:]</span><br><span class="line">ytrain_ln = np.log1p(ytrain)</span><br><span class="line">yval_ln = np.log1p(yval)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">model.fit(xtrain, ytrain_ln)</span><br><span class="line">mean_absolute_error(yval_ln, model.predict(xval))</span><br></pre></td></tr></table></figure>
<h2 id="gou-jian-yi-ge-xian-xia-ce-shi-ji">构建一个线下测试集</h2>
<p>因为有时候我们发现在本地上训练数据集得到的结果很好，但是放到线上进行测试的时候往往不是那么理想，这就意味着我们线下的训练有些过拟合了，而我们一般并不能发现这种情况，毕竟对于线上的测试，我们没有真实的标签对比不，所以我们可以先构建一个线下的测试集。这个实操起来也很简单，就是我们有 150000 个样本，可以用 100000 个样本来做训练集，后面的 50000 做测试集，因为我们已经知道这 50000 个样本的真实标签，这样训练出来的模型我们就可以直接先测试一下泛化能力，对于后面的调参或者是模型的评估等感觉还是挺好用的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">'./pre_data/pre_data.csv'</span>)</span><br><span class="line"></span><br><span class="line">train = data[:train_data.shape[<span class="number">0</span>]]</span><br><span class="line">test = data[train_data.shape[<span class="number">0</span>]:]    <span class="comment"># 这个先不用</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选数据</span></span><br><span class="line">X = train[:<span class="number">100000</span>]</span><br><span class="line">Y= train_data[<span class="string">'price'</span>][:<span class="number">100000</span>]</span><br><span class="line">Y_ln = np.log1p(Y)</span><br><span class="line"></span><br><span class="line">XTest = train[<span class="number">100000</span>:]   <span class="comment"># 模拟一个线下测试集， 看看模型的泛化能力</span></span><br><span class="line">Ytrue = train_data[<span class="string">'price'</span>][<span class="number">100000</span>:]</span><br></pre></td></tr></table></figure>
<h1 id="ping-gu-mo-xing-de-kuang-jia">评估模型的框架</h1>
<p>模型选择的时候，可以根据数据的特征和优化目标先选出很多个模型作为备选，因为我们分析完数据不能立刻得出哪个算法对需要解决的问题更有效</p>
<p>就拿这个比赛来说，我们直观上认为由于问题是预测价格，所以这是一个回归问题，肯定使用回归模型（Regressor 系列），但是回归模型太多，但我们又知道部分数据呈线性分布，线性回归和正则化的回归算法可能对解决问题比较有效。而由于数据的离散化，通过决策树算法及相应的集成算法也一般会表现出色，所以我们可以锁定几个模型都尝试一下</p>
<p>一般先建立一个字典，把这些模型放到字典里面，然后分别进行交叉验证，可视化结果来判断哪个模型针对当前问题表现比较好，这样从这里面选出 3-4 个进行下面的环节，也就是模型的调参工作。这里给出一个评估算法模型的一个框架。首先采用 10 交叉验证来分离数据，通过绝对值误差来比较算法的准确度，误差值越小，准确度越高。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_folds = <span class="number">10</span></span><br><span class="line">seed = <span class="number">7</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把所有模型写到一个字典中</span></span><br><span class="line">models = &#123;&#125;</span><br><span class="line">models[<span class="string">'LR'</span>] = LinearRegression()</span><br><span class="line">models[<span class="string">'Ridge'</span>] = Ridge()</span><br><span class="line">models[<span class="string">'LASSO'</span>] = Lasso()</span><br><span class="line">models[<span class="string">'DecisionTree'</span>] = DecisionTreeRegressor()</span><br><span class="line">models[<span class="string">'RandomForest'</span>] = RandomForestRegressor()</span><br><span class="line">models[<span class="string">'GradientBoosting'</span>] = GradientBoostingRegressor()</span><br><span class="line">models[<span class="string">'XGB'</span>] = XGBRegressor(n_estimators = <span class="number">100</span>, objective=<span class="string">'reg:squarederror'</span>)</span><br><span class="line">models[<span class="string">'LGB'</span>] = LGBMRegressor(n_estimators=<span class="number">100</span>)</span><br><span class="line"><span class="comment">#models['SVR'] = SVR()   # 支持向量机运行不出来</span></span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> models:</span><br><span class="line">    kfold = KFold(n_splits=num_folds, random_state=seed)</span><br><span class="line">    cv_result = cross_val_score(models[key], X, Y_ln, cv=kfold, scoring=make_scorer(mean_absolute_error))</span><br><span class="line">    results.append(cv_result)</span><br><span class="line">    print(<span class="string">'%s: %f (%f)'</span> % (key, cv_result.mean(), cv_result.std()))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 评估算法 --- 箱线图</span></span><br><span class="line">fig1 = plt.figure(figsize=(<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line">fig1.suptitle(<span class="string">'Algorithm Comparison'</span>)</span><br><span class="line">ax = fig1.add_subplot(<span class="number">111</span>)</span><br><span class="line">plt.boxplot(results)</span><br><span class="line">ax.set_xticklabels(models.keys())</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 结果：</span></span><br><span class="line">LR: <span class="number">0.192890</span> (<span class="number">0.001501</span>)</span><br><span class="line">Ridge: <span class="number">0.196279</span> (<span class="number">0.001616</span>)</span><br><span class="line">LASSO: <span class="number">0.515573</span> (<span class="number">0.003923</span>)</span><br><span class="line">DecisionTree: <span class="number">0.190959</span> (<span class="number">0.002524</span>)</span><br><span class="line">RandomForest: <span class="number">0.142333</span> (<span class="number">0.001489</span>)</span><br><span class="line">GradientBoosting: <span class="number">0.178403</span> (<span class="number">0.001903</span>)</span><br><span class="line">XGB: <span class="number">0.178492</span> (<span class="number">0.001441</span>)</span><br><span class="line">LGB: <span class="number">0.147875</span> (<span class="number">0.001397</span>)</span><br></pre></td></tr></table></figure>
<p>看一下箱线图的结果：</p>
<p><img src="/2020/04/02/machine_learning/modeling_modify_parameters/G80L2n.png" alt></p>
<p>这样，各个模型的效果就一目了然了，从上图可以看出，随机森林和 LGB 的效果还是好一些的，后面可以基于这两个进行调参，当然 xgboost 的效果可能由于参数的原因表现不是那么理想，这里也作为了调参备选。</p>
<p>那么调参究竟有没有影响呢？这里做了一个实验，可以先看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model2 = LGBMRegressor(n_estimators=<span class="number">100</span>)</span><br><span class="line">model2.fit(X, Y_ln)</span><br><span class="line">pred2 = model2.predict(XTest)</span><br><span class="line">print(<span class="string">"mae: "</span>, mean_absolute_error(Ytrue, np.expm1(pred2)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果：</span></span><br><span class="line">mae:  <span class="number">713.9408513079144</span></span><br></pre></td></tr></table></figure>
<p>上面这个是没有调参的 LGB，下面再看一下调参的 LGB：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bulid_modl_lgb</span><span class="params">(x_train, y_train)</span>:</span></span><br><span class="line">    estimator = LGBMRegressor(num_leaves=<span class="number">127</span>, n_estimators=<span class="number">150</span>)</span><br><span class="line">    param_grid = &#123;<span class="string">'learning_rage'</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.2</span>]&#125;</span><br><span class="line">    gbm = GridSearchCV(estimator, param_grid)</span><br><span class="line">    gbm.fit(x_train, y_train)</span><br><span class="line">    <span class="keyword">return</span> gbm</span><br><span class="line"> </span><br><span class="line">model_lgb = bulid_modl_lgb(X, Y_ln)</span><br><span class="line">val_lgb = model_lgb.predict(XTest)</span><br><span class="line">MAE_lgb = mean_absolute_error(Ytrue, np.expm1(val_lgb))</span><br><span class="line">print(MAE_lgb)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 结果：</span></span><br><span class="line"><span class="number">591.4221480289154</span></span><br></pre></td></tr></table></figure>
<p>同样的 LGB，调参误差能降到 591，不调参 713，所以调参还是很重要的。但是在调参之前，先给出一个正态化模板：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">pipelines = &#123;&#125;</span><br><span class="line">pipelines[<span class="string">'ScalerLR'</span>] = Pipeline([(<span class="string">'Scaler'</span>, StandardScaler()), (<span class="string">'LR'</span>, LinearRegression())])</span><br><span class="line">pipelines[<span class="string">'ScalerRidge'</span>] = Pipeline([(<span class="string">'Scaler'</span>, StandardScaler()), (<span class="string">'Ridge'</span>, Ridge())])</span><br><span class="line">pipelines[<span class="string">'ScalerLasso'</span>] = Pipeline([(<span class="string">'Scaler'</span>, StandardScaler()), (<span class="string">'Lasso'</span>, Lasso())])</span><br><span class="line">pipelines[<span class="string">'ScalerTree'</span>] = Pipeline([(<span class="string">'Scaler'</span>, StandardScaler()), (<span class="string">'Tree'</span>, DecisionTreeRegressor())])</span><br><span class="line">pipelines[<span class="string">'ScalerForest'</span>] = Pipeline([(<span class="string">'Scaler'</span>, StandardScaler()), (<span class="string">'Forest'</span>, RandomForestRegressor())])</span><br><span class="line">pipelines[<span class="string">'ScalerGBDT'</span>] = Pipeline([(<span class="string">'Scaler'</span>, StandardScaler()), (<span class="string">'GBDT'</span>, GradientBoostingRegressor())])</span><br><span class="line">pipelines[<span class="string">'ScalerXGB'</span>] = Pipeline([(<span class="string">'Scaler'</span>, StandardScaler()), (<span class="string">'XGB'</span>, XGBRegressor(n_estimators = <span class="number">100</span>, objective=<span class="string">'reg:squarederror'</span>))])</span><br><span class="line">pipelines[<span class="string">'ScalerLGB'</span>] = Pipeline([(<span class="string">'Scaler'</span>, StandardScaler()), (<span class="string">'LGB'</span>, LGBMRegressor(n_estimators=<span class="number">100</span>))])</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> pipelines:</span><br><span class="line">    kfold = KFold(n_splits=num_folds, random_state=seed)</span><br><span class="line">    cv_result = cross_val_score(pipelines[key], X, Y_ln, cv=kfold, scoring=make_scorer(mean_absolute_error))</span><br><span class="line">    results.append(cv_result)</span><br><span class="line">    print(<span class="string">'%s: %f (%f)'</span> % (key, cv_result.mean(), cv_result.std()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估算法 --- 箱线图</span></span><br><span class="line">fig2 = plt.figure(figsize=(<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line">fig2.suptitle(<span class="string">'Algorithm Comparison'</span>)</span><br><span class="line">ax = fig2.add_subplot(<span class="number">111</span>)</span><br><span class="line">plt.boxplot(results)</span><br><span class="line">ax.set_xticklabels(models.keys())</span><br></pre></td></tr></table></figure>
<p>这里不用正态化，因为试验了一下，效果不如之前的好。</p>
<h1 id="mo-xing-diao-can">模型调参</h1>
<p>同特征工程一样，模型参数调节也是一项非常繁琐但又非常重要的工作。</p>
<p>根据模型复杂程度的不同，需要调节的参数数量也不尽相同。简单如逻辑回归，需要调节的通常只有正则项系数 \(C\)；复杂如随机森林，需要调节的变量会多出不少，最核心的如树的数量 n_estimators，树的深度 max_depth 等等。参数越多，调参的难度自然也越来越大，因为参数间排列组合的可能性越来越多。在训练样本比较少的情况下，<code>sklearn</code> 的 <code>GridSearchCV</code> 是个不错的选择，可以帮助我们自动寻找指定范围内的最佳参数组合。但实际情况是，<code>GridSearch</code> 通常需要的运行时间过长，长到我们不太能够忍受的程度。所以更多的时候需要我们自己手动先排除掉一部分数值，然后使用 GridSearch 自动调参。</p>
<p>模型调参有三种方式：</p>
<ul>
<li>贪心调参</li>
<li>网格搜索调参</li>
<li>贝叶斯调参</li>
</ul>
<p>这里给出一个模型可调参数及范围选取的参考：</p>
<p><img src="/2020/04/02/machine_learning/modeling_modify_parameters/G8sgdf.png" alt></p>
<p>以 LGB 为例，其他的模型也都是这个思路，为了减少篇幅，只对 LGB 做实验：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">objective = [<span class="string">'regression'</span>, <span class="string">'regression_l1'</span>, <span class="string">'mape'</span>, <span class="string">'huber'</span>, <span class="string">'fair'</span>]</span><br><span class="line">num_leaves = [<span class="number">10</span>, <span class="number">55</span>, <span class="number">70</span>, <span class="number">100</span>, <span class="number">200</span>]</span><br><span class="line">max_depth = [ <span class="number">10</span>, <span class="number">55</span>, <span class="number">70</span>, <span class="number">100</span>, <span class="number">200</span>]</span><br><span class="line">n_estimators = [<span class="number">200</span>, <span class="number">400</span>, <span class="number">800</span>, <span class="number">1000</span>]</span><br><span class="line">learning_rate =  [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.2</span>]</span><br></pre></td></tr></table></figure>
<h2 id="tan-xin-diao-can">贪心调参</h2>
<p>拿当前对模型影响最大的参数调优，直到最优化；再拿下一个影响最大的参数调优，如此下去，直到所有的参数调整完毕。这个方法的<strong>缺点就是可能会调到局部最优而不是全局最优，但是省时间省力</strong>，巨大的优势面前，可以一试。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先建立一个参数字典</span></span><br><span class="line">best_obj = dict()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调objective</span></span><br><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> objective:</span><br><span class="line">    model = LGBMRegressor(objective=obj)</span><br><span class="line">    score = np.mean(cross_val_score(model, X, Y_ln, verbose=<span class="number">0</span>, cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_obj[obj] = score</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 上面调好之后，用上面的参数调num_leaves</span></span><br><span class="line">best_leaves = dict()</span><br><span class="line"><span class="keyword">for</span> leaves <span class="keyword">in</span> num_leaves:</span><br><span class="line">    model = LGBMRegressor(objective=min(best_obj.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>], num_leaves=leaves)</span><br><span class="line">    score = np.mean(cross_val_score(model, X, Y_ln, verbose=<span class="number">0</span>, cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_leaves[leaves] = score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用上面两个最优参数调max_depth</span></span><br><span class="line">best_depth = dict()</span><br><span class="line"><span class="keyword">for</span> depth <span class="keyword">in</span> max_depth:</span><br><span class="line">    model = LGBMRegressor(objective=min(best_obj.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          num_leaves=min(best_leaves.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          max_depth=depth)</span><br><span class="line">    score = np.mean(cross_val_score(model, X, Y_ln, verbose=<span class="number">0</span>, cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_depth[depth] = score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调n_estimators</span></span><br><span class="line">best_nstimators = dict()</span><br><span class="line"><span class="keyword">for</span> nstimator <span class="keyword">in</span> n_estimators:</span><br><span class="line">    model = LGBMRegressor(objective=min(best_obj.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          num_leaves=min(best_leaves.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          max_depth=min(best_depth.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          n_estimators=nstimator)</span><br><span class="line">    </span><br><span class="line">    score = np.mean(cross_val_score(model, X, Y_ln, verbose=<span class="number">0</span>, cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_nstimators[nstimator] = score</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 调learning_rate</span></span><br><span class="line">best_lr = dict()</span><br><span class="line"><span class="keyword">for</span> lr <span class="keyword">in</span> learning_rate:</span><br><span class="line">    model = LGBMRegressor(objective=min(best_obj.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          num_leaves=min(best_leaves.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          max_depth=min(best_depth.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          n_estimators=min(best_nstimators.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          learning_rate=lr)</span><br><span class="line">    score = np.mean(cross_val_score(model, X, Y_ln, verbose=<span class="number">0</span>, cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_lr[lr] = score</span><br></pre></td></tr></table></figure>
<p>上面的过程建议放在不同的 cell 里面运行，之后可视化这个过程的结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.lineplot(x=[<span class="string">'0_initial'</span>,<span class="string">'1_turning_obj'</span>,<span class="string">'2_turning_leaves'</span>,</span><br><span class="line">               <span class="string">'3_turning_depth'</span>,<span class="string">'4_turning_estimators'</span>, <span class="string">'5_turning_lr'</span>],</span><br><span class="line">            y=[<span class="number">0.143</span> ,min(best_obj.values()), min(best_leaves.values()), min(best_depth.values()),</span><br><span class="line">              min(best_nstimators.values()), min(best_lr.values())])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/modeling_modify_parameters/G8yDtU.png" alt></p>
<p>贪心的调参策略还是不错的，可以打印最后调参的结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"best_obj:"</span>, min(best_obj.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>]))</span><br><span class="line">print(<span class="string">"best_leaves:"</span>, min(best_leaves.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>]) )</span><br><span class="line">print(<span class="string">'best_depth:'</span>, min(best_depth.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>]))</span><br><span class="line">print(<span class="string">'best_nstimators: '</span>, min(best_nstimators.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>]))</span><br><span class="line">print(<span class="string">'best_lr:'</span>, min(best_lr.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 结果如下：</span></span><br><span class="line">best_obj: (<span class="string">'regression_l1'</span>, <span class="number">0.1457016215267976</span>)</span><br><span class="line">best_leaves: (<span class="number">100</span>, <span class="number">0.132929241004274</span>)</span><br><span class="line">best_depth: (<span class="number">20</span>, <span class="number">0.13275966837758682</span>)</span><br><span class="line">best_nstimators:  (<span class="number">1000</span>, <span class="number">0.11861541074643345</span>)</span><br><span class="line">best_lr: (<span class="number">0.05</span>, <span class="number">0.11728267187328578</span>)</span><br></pre></td></tr></table></figure>
<h2 id="grid-search-cv-diao-can">GridSearchCV 调参</h2>
<p>GridSearchCV，它存在的意义就是自动调参，只要把参数输进去，就能给出最优化的结果和参数。但是这个方法适合于小数据集，一旦数据的量级上去了，很难得出结果。这个在这里面优势不大， 因为数据集很大，不太能跑出结果，但是有时候还是很好用的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个我这边电脑运行时间太长，先不跑了</span></span><br><span class="line">parameters = &#123;<span class="string">'objective'</span>:objective, <span class="string">'num_leaves'</span>:num_leaves, <span class="string">'max_depth'</span>:max_depth,</span><br><span class="line">             <span class="string">'n_estimators'</span>: n_estimators, <span class="string">'learning_rate'</span>:learning_rate&#125;</span><br><span class="line"></span><br><span class="line">model = LGBMRegressor()</span><br><span class="line">clf = GridSearchCV(model, parameters, cv=<span class="number">5</span>)</span><br><span class="line">clf = clf.fit(X, Y_ln)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出最优参数</span></span><br><span class="line">clf.best_params_</span><br></pre></td></tr></table></figure>
<h2 id="bei-xie-si-diao-can">贝叶斯调参</h2>
<p>首先需要安装包<code>pip install bayesian-optimization</code>。</p>
<p>贝叶斯优化用于机器学习调参，主要思想是，给定优化的目标函数 (广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布 (高斯过程, 直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。</p>
<p>它与常规的网格搜索或者随机搜索的区别是：</p>
<ul>
<li>贝叶斯调参采用高斯过程，考虑之前的参数信息，不断地更新先验；网格搜索未考虑之前的参数信息</li>
<li>贝叶斯调参迭代次数少，速度快；网格搜索速度慢，参数多时易导致维度爆炸</li>
<li>贝叶斯调参针对非凸问题依然稳健；网格搜索针对非凸问题易得到局部最优</li>
</ul>
<p>使用方法：</p>
<ul>
<li>定义优化函数 (rf_cv，在里面把优化的参数传入，然后建立模型，返回要优化的分数指标)</li>
<li>定义优化参数</li>
<li>开始优化（最大化分数还是最小化分数等）</li>
<li>得到优化结果</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span>  bayes_opt <span class="keyword">import</span> BayesianOptimization</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rf_cv</span><span class="params">(num_leaves, max_depth, subsample, min_child_samples)</span>:</span></span><br><span class="line">    model = LGBMRegressor(objective=<span class="string">'regression_l1'</span>, num_leaves=int(num_leaves),</span><br><span class="line">                         max_depth=int(max_depth), subsample=subsample,</span><br><span class="line">                         min_child_samples = int(min_child_samples))</span><br><span class="line">    val = cross_val_score(model, X, Y_ln, verbose=<span class="number">0</span>, cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)).mean()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>-val</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化参数</span></span><br><span class="line">rf_bo = BayesianOptimization(</span><br><span class="line">    rf_cv, </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">'num_leaves'</span>:(<span class="number">2</span>, <span class="number">100</span>),</span><br><span class="line">        <span class="string">'max_depth'</span>:(<span class="number">2</span>, <span class="number">100</span>),</span><br><span class="line">        <span class="string">'subsample'</span>:(<span class="number">0.1</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="string">'min_child_samples'</span>:(<span class="number">2</span>, <span class="number">100</span>)</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始优化</span></span><br><span class="line">num_iter = <span class="number">25</span></span><br><span class="line">init_points = <span class="number">5</span></span><br><span class="line">rf_bo.maximize(init_points=init_points,n_iter=num_iter)</span><br><span class="line"></span><br><span class="line"><span class="comment">#显示优化结果</span></span><br><span class="line">rf_bo.res[<span class="string">"max"</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#附近搜索（已经有不错的参数值的时候）</span></span><br><span class="line">rf_bo.explore(</span><br><span class="line">     &#123;<span class="string">'n_estimators'</span>: [<span class="number">10</span>, <span class="number">100</span>, <span class="number">200</span>],</span><br><span class="line">      <span class="string">'min_samples_split'</span>: [<span class="number">2</span>, <span class="number">10</span>, <span class="number">20</span>],</span><br><span class="line">      <span class="string">'max_features'</span>: [<span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">0.9</span>],</span><br><span class="line">      <span class="string">'max_depth'</span>: [<span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>]</span><br><span class="line">     &#125;)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/modeling_modify_parameters/G86agH.png" alt></p>
<p>基于上面的思路，也可以对随机森林进行调参：</p>
<blockquote>
<p>对 Random Forest 来说，增加 “子模型数”（n_estimators）可以明显降低整体模型的方差，且不会对子模型的偏差和方差有任何影响。模型的准确度会随着“子模型数” 的增加而提高。由于减少的是整体模型方差公式的第二项，故准确度的提高有一个上限。在不同的场景下，“分裂条件”（criterion）对模型的准确度的影响也不一样，该参数需要在实际运用时灵活调整。调整“最大叶节点数”（max_leaf_nodes）以及“最大树深度”（max_depth）之一，可以粗粒度地调整树的结构：叶节点越多或者树越深，意味着子模型的偏差越低，方差越高；同时，调整“分裂所需最小样本数”（min_samples_split）、“叶节点最小样本数”（min_samples_leaf）及“叶节点最小权重总值”（min_weight_fraction_leaf），可以更细粒度地调整树的结构：分裂所需样本数越少或者叶节点所需样本越少，也意味着子模型越复杂。一般来说，我们总采用 bootstrap 对样本进行子采样来降低子模型之间的关联度，从而降低整体模型的方差。适当地减少“分裂时考虑的最大特征数”（max_features），给子模型注入了另外的随机性，同样也达到了降低子模型之间关联度的效果。详细的可以参考：</p>
<ul>
<li><a href="https://blog.csdn.net/geduo_feng/article/details/79558572" target="_blank" rel="noopener">随机森林 sklearn FandomForest，及其调参</a></li>
<li><a href="https://www.zhihu.com/question/34470160/answer/114305935" target="_blank" rel="noopener">机器学习各种算法怎么调参？</a></li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义优化函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rf_cv</span><span class="params">(n_estimators,  max_depth)</span>:</span></span><br><span class="line">    model = RandomForestRegressor(n_estimators=int(n_estimators), </span><br><span class="line">                         max_depth=int(max_depth))</span><br><span class="line">    val = cross_val_score(model, X, Y_ln, verbose=<span class="number">0</span>, cv=<span class="number">5</span>, scoring=make_scorer(mean_absolute_error)).mean()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>-val</span><br><span class="line"></span><br><span class="line">rf_bo = BayesianOptimization(</span><br><span class="line">    rf_cv, </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">'n_estimators'</span>:(<span class="number">100</span>, <span class="number">200</span>),</span><br><span class="line">        <span class="string">'max_depth'</span>:(<span class="number">2</span>, <span class="number">100</span>)</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">rf_bo.maximize()</span><br></pre></td></tr></table></figure>
<h1 id="hui-zhi-xun-lian-ji-qu-xian-yu-yan-zheng-ji-qu-xian">绘制训练集曲线与验证集曲线</h1>
<p>从上面的步骤中，我们通过算法模型的评估框架选择出了合适的几个模型，又通过模型的调参步骤确定了模型的合适参数，这样我们基本上就得到了一个我们认为的比较好的模型了，但是这个模型真的就是好的模型了吗？ 我们还不能确定是否存在过拟合或者欠拟合问题，在实际中究竟应该怎么判断？ 学习曲线的绘制就是一个非常好的方式，可以帮助我们看一下我们调试好的模型还有没有过拟合或者欠拟合的问题。</p>
<p>关于学习曲线：</p>
<ul>
<li>学习曲线是不同训练集大小，模型在训练集和验证集上的得分变化曲线</li>
<li>学习曲线图的横坐标是 x_train 的数据量，纵坐标是对应的 train_score，test_score。随着训练样本的逐渐增加，算法练出的模型的表现能力；</li>
</ul>
<p>绘制学习曲线非常简单</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_sizes，train_scores，test_score = learning_curve(estimator, X, y, groups=<span class="literal">None</span>, train_sizes=array([<span class="number">0.1</span>, <span class="number">0.33</span>, <span class="number">0.55</span>, <span class="number">0.78</span>, <span class="number">1.</span> ]), cv=’warn’, scoring=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>主要的参数说明如下：</p>
<blockquote>
<p>通过 cv 设置交叉验证，取几次 (组) 数据，train_sizes 设置每一次取值，在不同训练集大小上计算得分</p>
<ul>
<li>estimator：估计器，用什么模型进行学习；</li>
<li>cv：交叉验证生成器，确定交叉验证拆分策略；</li>
</ul>
<p>画训练集的曲线时，横轴为 train_sizes, 纵轴为 train_scores_mean; train_scores 为二维数组, 行代表 train_sizes 不同时的得分，列表示取 cv 组数据。</p>
<p>画测试集的曲线时：横轴为 train_sizes, 纵轴为 test_scores_mean; test_scores 为二维数组</p>
<p>learning_curve 为什么运行时间那么长：模型要进行 train_sizes * cv 次运行</p>
</blockquote>
<p>基于一个训练好的模型，画一下学习曲线，看看这个学习曲线究竟怎么观察：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve, validation_curve</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curve</span><span class="params">(estimator, title, X, y, ylim=None, cv=None, n_jobs=<span class="number">1</span>, train_size=np.linspace<span class="params">(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span>)</span>)</span>:</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.title(title)</span><br><span class="line">    <span class="keyword">if</span> ylim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.ylim(*ylim)</span><br><span class="line">    plt.xlabel(<span class="string">'Training example'</span>)  </span><br><span class="line">    plt.ylabel(<span class="string">'score'</span>)  </span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_size, scoring = make_scorer(mean_absolute_error))  </span><br><span class="line">    train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)  </span><br><span class="line">    train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)  </span><br><span class="line">    test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)  </span><br><span class="line">    test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)  </span><br><span class="line">    plt.grid()<span class="comment">#区域  </span></span><br><span class="line">    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,  </span><br><span class="line">                     train_scores_mean + train_scores_std, alpha=<span class="number">0.1</span>,  </span><br><span class="line">                     color=<span class="string">"r"</span>)  </span><br><span class="line">    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,  </span><br><span class="line">                     test_scores_mean + test_scores_std, alpha=<span class="number">0.1</span>,  </span><br><span class="line">                     color=<span class="string">"g"</span>)  </span><br><span class="line">    plt.plot(train_sizes, train_scores_mean, <span class="string">'o-'</span>, color=<span class="string">'r'</span>,  </span><br><span class="line">             label=<span class="string">"Training score"</span>)  </span><br><span class="line">    plt.plot(train_sizes, test_scores_mean,<span class="string">'o-'</span>,color=<span class="string">"g"</span>,  </span><br><span class="line">             label=<span class="string">"Cross-validation score"</span>)  </span><br><span class="line">    plt.legend(loc=<span class="string">"best"</span>)  </span><br><span class="line">    <span class="keyword">return</span> plt  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设已经调好了LGB的参数，我们可以绘制一下曲线看看这个模型有没有什么问题</span></span><br><span class="line">model = LGBMRegressor(n_estimators=<span class="number">1000</span>, leaves=<span class="number">200</span>, learning_rate=<span class="number">0.05</span>, objective=<span class="string">'regression_l1'</span>)</span><br><span class="line">model.fit(X, Y_ln)</span><br><span class="line">pred2 = model.predict(XTest)</span><br><span class="line">print(<span class="string">"mae: "</span>, mean_absolute_error(Ytrue, np.expm1(pred2)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出学习曲线</span></span><br><span class="line">plot_learning_curve(model, <span class="string">'LGB'</span>, X[:<span class="number">10000</span>], Y_ln[:<span class="number">10000</span>], ylim=(<span class="number">0.0</span>, <span class="number">1</span>), cv=<span class="number">5</span>, n_jobs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/modeling_modify_parameters/G8g3tK.png" alt></p>
<p>learning_curve 里面有个 scoring 参数可以设置你想求的值，分类可以设置<code>accuracy</code>，回归问题可以设置<code>neg_mean_squared_error</code>，总体来说，值都是越大越好，但是注意如果模型设置的是<code>mae erro</code>，那就是越低越好。</p>
<p>高偏差和高方差应该怎么看呢？</p>
<p><img src="/2020/04/02/machine_learning/modeling_modify_parameters/G82PjH.png" alt></p>
<p><strong>什么情况欠拟合</strong>：模型在训练集和验证集上准确率相差不大，却都很差，说明模型对已知数据和未知数据都不能准确预测，属于高偏差。左上角那个图</p>
<p><strong>什么情况过拟合</strong>：模型在训练集和验证集上的准确率差距很大，说明模型能够很好的拟合已知数据，但是泛化能力很差，属于高方差。右上角那个图</p>
<p>右下角那个图是比较合适的。所以上面 lgb 的那个模型效果还是不错的</p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://wmathor.com/index.php/archives/1427/" target="_blank" rel="noopener">模型建立与调参</a></li>
<li><a href="https://blog.csdn.net/Noob_daniel/article/details/76087829" target="_blank" rel="noopener">回归分析的五个基本假设</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>建模调参</tag>
      </tags>
  </entry>
  <entry>
    <title>特征工程</title>
    <url>/2020/04/02/machine_learning/feature_engineering/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/04/02/machine_learning/feature_engineering/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B.png" alt></p>
<a id="more"></a>
<p><code><a class="btn" href="特征工程.xmind">
            <i class="fa fa-download"></i>xmind文件下载
          </a></code></p>
<p><a href="https://share.weiyun.com/EDpohPCk" target="_blank" rel="noopener">数据集下载</a></p>
<p>特征工程和数据清洗转换是至关重要的一块，因为<strong>数据和特征决定了机器学习的上限，而算法和模型只是逼近这个上限而已</strong>，所以特征工程的好坏往往决定着最后的结果。</p>
<p>特征工程一般包括特征构造，特征选择，降维等步骤，但是它一般是和数据清洗转换放在一块，也有的把这两块统称为特征工程，因为两者联系实在是密切（你中有我，我中有你的景象）。</p>
<p>通过数据清洗和转换，我们能够更好地表示出潜在问题的特征，使得数据的表达清晰一些，比如处理异常值清除噪声，填充缺失值可以加入先验知识等。而特征工程又进一步增强数据的表达能力，通过构造新特征，我们可以挖掘出数据的更多信息，使得数据的表达能力进一步放大，当然如果特征过多，又往往会造成冗余，这时候我们又得根据相关性等进行特征的选择和降维操作，所以这就是特征工程的逻辑。</p>
<h1 id="te-zheng-gou-zao">特征构造</h1>
<p>特征工程这块，在特征构造的时候，需要借助一些<strong>背景知识</strong>，遵循的一般原则就是我们需要发挥想象力，尽可能多的创造特征，不用先考虑哪些特征可能好，可能不好，先弥补这个广度，而特征构造的时候数值特征，类别特征，时间特征又得分开处理：</p>
<ul>
<li>对于数值特征，我们一般会尝试一些它们之间的加减组合（当然不要乱来，根据特征表达的含义）或者提取一些统计特征。</li>
<li>对于类别特征，我们一般会尝试之间的交叉组合，embedding 也是一种思路。</li>
<li>对于时间特征，这一块又可以作为一个大专题来学习，在时间序列的预测中这一块非常重要，也会非常复杂，需要就尽可能多的挖掘时间信息，会有不同的方式技巧。</li>
</ul>
<h2 id="shu-zhi-te-zheng-gou-zao">数值特征构造</h2>
<p>数值特征这块，由于大部分都是匿名特征，处理起来不是太好处理，只能尝试一些加减组合和统计特征。</p>
<h3 id="fen-xiang">分箱</h3>
<p>一部车有效寿命 30 万公里，将其分为 5 段，每段 6 万公里，每段价值依序为新车价的 5/15、4/15、3/15、2/15、1/15。假设新车价 12 万元，已行驶 7.5 万公里（5 年左右），那么该车估值为 $12×(3+3+2+1)÷15=7.2 $万元.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分成三段</span></span><br><span class="line">bins = [<span class="number">0</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>]</span><br><span class="line">num_data[<span class="string">'kil_bin'</span>] = pd.cut(num_data[<span class="string">'kilometer'</span>], bins, labels=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h3 id="tong-ji-te-zheng">统计特征</h3>
<p>平均值， 总和和标准差</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v_features = [<span class="string">'v_'</span> + str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">15</span>)]</span><br><span class="line">num_data[<span class="string">'v_sum'</span>] = num_data[v_features].apply(<span class="keyword">lambda</span> x: x.sum(), axis=<span class="number">1</span>)</span><br><span class="line">num_data[<span class="string">'v_mean'</span>] = num_data[v_features].apply(<span class="keyword">lambda</span> x: x.mean(), axis=<span class="number">1</span>)</span><br><span class="line">num_data[<span class="string">'v_std'</span>] = num_data[v_features].apply(<span class="keyword">lambda</span> x: x.std(), axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GADWZT.png" alt></p>
<h2 id="lei-bie-te-zheng">类别特征</h2>
<p>经过分析，可以构造的类别特征如下：</p>
<ol>
<li>从邮编中提取城市信息，因为是德国的数据，所以参考德国的邮编，加入先验知识，但是感觉这个没有用，可以先试一下</li>
<li>最好是从 regioncode 中提取出是不是华东地区，因为华东地区是二手车交易的主要地区</li>
<li>私用车和商用车分开（bodyType 提取）</li>
<li>微型车单独处理</li>
<li>新能源车和燃油车分开（在 fuelType 中提取，然后进行 One-Hot）</li>
<li>地区编码还是有影响的， 不同的地区汽车的保率不同</li>
</ol>
<blockquote>
<p>注意，One-Hot 不要太早，否则有些特征就没法提取潜在信息了</p>
</blockquote>
<h3 id="you-bian-te-zheng">邮编特征</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从邮编中提取城市信息</span></span><br><span class="line">cat_data[<span class="string">'city'</span>] = cat_data[<span class="string">'regionCode'</span>].apply(<span class="keyword">lambda</span> x: str(x)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<h3 id="si-yong-che-he-shang-wu-che-fen-kai">私用车和商务车分开</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">com_car = [<span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">6.0</span>]  <span class="comment"># 商用车</span></span><br><span class="line">GL_car = [<span class="number">0.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>]   <span class="comment"># 豪华系列  </span></span><br><span class="line">self_car = [<span class="number">1.0</span>, <span class="number">7.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">class_bodyType</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x <span class="keyword">in</span> GL_car:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">elif</span> x <span class="keyword">in</span> com_car:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line">cat_data[<span class="string">'car_class'</span>] = cat_data[<span class="string">'bodyType'</span>].apply(<span class="keyword">lambda</span> x : class_bodyType(x))</span><br></pre></td></tr></table></figure>
<h3 id="xin-neng-yuan-che-he-ran-you-che-fen-kai">新能源车和燃油车分开</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 是否是新能源</span></span><br><span class="line">is_fuel = [<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">cat_data[<span class="string">'is_fuel'</span>] = cat_data[<span class="string">'fuelType'</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x <span class="keyword">in</span> is_fuel <span class="keyword">else</span> <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="gou-zao-tong-ji-te-zheng">构造统计特征</h3>
<p>可以根据 brand，燃油类型，gearbox 类型，车型等，这里只拿一个举例，其他的类似：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data_gearbox = train_data.copy()   <span class="comment"># 不要动train_data</span></span><br><span class="line">train_data_gearbox[<span class="string">'gearbox'</span>] = cat_data[<span class="string">'gearbox'</span>][:train_data.shape[<span class="number">0</span>]]</span><br><span class="line">train_data_gearbox[<span class="string">'price'</span>] = train_target</span><br><span class="line"></span><br><span class="line">train_gb = train_data_gearbox.groupby(<span class="string">'gearbox'</span>)</span><br><span class="line">all_info = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> kind, kind_data <span class="keyword">in</span> train_gb:</span><br><span class="line">    info = &#123;&#125;</span><br><span class="line">    kind_data = kind_data[kind_data[<span class="string">'price'</span>] &gt; <span class="number">0</span>]</span><br><span class="line">    info[<span class="string">'gearbox_count'</span>] = len(kind_data)</span><br><span class="line">    info[<span class="string">'gearbox_price_max'</span>] = kind_data.price.max()</span><br><span class="line">    info[<span class="string">'gearbox_price_median'</span>] = kind_data.price.median()</span><br><span class="line">    info[<span class="string">'gearbox_price_min'</span>] = kind_data.price.min()</span><br><span class="line">    info[<span class="string">'gearbox_price_sum'</span>] = kind_data.price.sum()</span><br><span class="line">    info[<span class="string">'gearbox_std'</span>] = kind_data.price.std()</span><br><span class="line">    info[<span class="string">'gearbox_price_average'</span>] = round(kind_data.price.sum() / (len(kind_data) + <span class="number">1</span>), <span class="number">2</span>)</span><br><span class="line">    all_info[kind] = info</span><br><span class="line"></span><br><span class="line">gearbox_fe = pd.DataFrame(all_info).T.reset_index().rename(columns=&#123;<span class="string">"index"</span>: <span class="string">"gearbox"</span>&#125;)</span><br><span class="line"></span><br><span class="line">cat_data = cat_data.merge(gearbox_fe, how=<span class="string">'left'</span>, on=<span class="string">'gearbox'</span>)</span><br></pre></td></tr></table></figure>
<p>下面就可以把 bodyType 和 fuelType 删除，因为该提取的信息也提取完了，该独热的独热：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 删掉bodyType和fuelType，然后把gearbox，car_classis_fuel独热一下，这个不能太早，构造完了统计特征之后再独热</span></span><br><span class="line"><span class="keyword">del</span> cat_data[<span class="string">'bodyType'</span>]</span><br><span class="line"><span class="keyword">del</span> cat_data[<span class="string">'fuelType'</span>]</span><br><span class="line"></span><br><span class="line">cat_data = pd.get_dummies(cat_data, columns=[<span class="string">'gearbox'</span>, <span class="string">'car_class'</span>, <span class="string">'is_fuel'</span>, <span class="string">'notRepairedDamage'</span>])</span><br></pre></td></tr></table></figure>
<p>这样，类别特征就构造完毕了。最终结果如下：</p>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GADF54.png" alt></p>
<h2 id="shi-jian-te-zheng">时间特征</h2>
<p>根据上面的分析，可以构造的时间特征如下：</p>
<ol>
<li>汽车的上线日期与汽车的注册日期之差就是汽车的使用时间，一般来说与价格成反比</li>
<li>对汽车的使用时间进行分箱，使用了 3 年以下，3-7 年，7-10 年和 10 年以上，分为四个等级，10 年之后就是报废车了，应该会影响价格</li>
<li>淡旺季也会影响价格，所以可以从汽车的上线日期上提取一下淡旺季信息</li>
</ol>
<h3 id="qi-che-de-shi-yong-shi-jian">汽车的使用时间</h3>
<p>createDate-regDate，反应汽车使用时间，一般来说与价格成反比，但是要注意这一块中的问题就是时间格式， regDateFalse 这个字段有些是 0 月，如果忽略错误计算的话，使用时间有一些会是空值，当然可以考虑删除这些空值，但是因为训练集和测试集合并了，那么就不轻易删除了，采取的办法，把错误字段都给他加 1 个月，然后计算出天数之后在加上 30 天（这个有不同的处理方式，但是一般不删除或者置为空，因为删除和空值都有潜在的副作用）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这里是为了标记一下哪些字段有错误</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regDateFalse</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> str(x)[<span class="number">4</span>:<span class="number">6</span>] == <span class="string">'00'</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">time_data[<span class="string">'regDateFalse'</span>] = time_data[<span class="string">'regDate'</span>].apply(<span class="keyword">lambda</span> x: regDateFalse(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里是改正错误字段</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">changeFalse</span><span class="params">(x)</span>:</span></span><br><span class="line">    x = str(x)</span><br><span class="line">    <span class="keyword">if</span> x[<span class="number">4</span>:<span class="number">6</span>] == <span class="string">'00'</span>:</span><br><span class="line">        x = x[<span class="number">0</span>:<span class="number">4</span>] + <span class="string">'01'</span> + x[<span class="number">6</span>:]</span><br><span class="line">        x = int(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line">time_data[<span class="string">'regDate'</span>] = time_data[<span class="string">'regDate'</span>].apply(<span class="keyword">lambda</span> x: changeFalse(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用时间：data['creatDate'] - data['regDate']，反应汽车使用时间，一般来说价格与使用时间成反比</span></span><br><span class="line"><span class="comment"># 不过要注意，数据里有时间出错的格式，所以我们需要 errors='coerce'</span></span><br><span class="line">time_data[<span class="string">'used_time'</span>] = (pd.to_datetime(time_data[<span class="string">'creatDate'</span>], format=<span class="string">'%Y%m%d'</span>) - </span><br><span class="line">                            pd.to_datetime(time_data[<span class="string">'regDate'</span>], format=<span class="string">'%Y%m%d'</span>)).dt.days</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改错误</span></span><br><span class="line"><span class="comment"># 但是需要加上那一个月</span></span><br><span class="line">time_data.loc[time_data.regDateFalse==<span class="number">1</span>, <span class="string">'used_time'</span>] += <span class="number">30</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除标记列</span></span><br><span class="line"><span class="keyword">del</span> time_data[<span class="string">'regDateFalse'</span>]</span><br></pre></td></tr></table></figure>
<p>这样，一个特征构造完毕，used_time 字段，表示汽车的使用时间。</p>
<h3 id="qi-che-shi-fou-bao-fei">汽车是否报废</h3>
<p>时间特征还可以继续提取，我们假设用了 10 年的车作为报废车的话，那么我们可以根据使用天数计算出年数，然后根据年数构造出一个特征是不是报废</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用时间换成年来表示</span></span><br><span class="line">time_data[<span class="string">'used_time'</span>] = time_data[<span class="string">'used_time'</span>] / <span class="number">365.0</span></span><br><span class="line">time_data[<span class="string">'Is_scrap'</span>] = time_data[<span class="string">'used_time'</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x&gt;=<span class="number">10</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>我们还可以对 used_time 进行分箱，这个是根据背景估价的方法可以发现，汽车的使用时间 3 年，3-7 年，10 年以上的估价会有不同，所以分一下箱：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bins = [<span class="number">0</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>]</span><br><span class="line">time_data[<span class="string">'estivalue'</span>] = pd.cut(time_data[<span class="string">'used_time'</span>], bins, labels=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h3 id="dan-wang-ji">淡旺季</h3>
<p>根据汽车的上线售卖时间看，每年的 2，3 月份及 6,7,8 月份是整个汽车行业的低谷，年初和年末及 9 月份是二手车销售的黄金时期，所以根据上线时间选出淡旺季：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 选出淡旺季</span></span><br><span class="line">low_seasons = [<span class="string">'3'</span>, <span class="string">'6'</span>, <span class="string">'7'</span>, <span class="string">'8'</span>]</span><br><span class="line">time_data[<span class="string">'is_low_seasons'</span>] = time_data[<span class="string">'creatDate'</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> str(x)[<span class="number">5</span>] <span class="keyword">in</span> low_seasons <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 独热一下</span></span><br><span class="line">time_data = pd.get_dummies(time_data, columns=[<span class="string">'is_low_seasons'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这样时间特征构造完毕，删除日期了</span></span><br><span class="line"><span class="keyword">del</span> time_data[<span class="string">'regDate'</span>]</span><br><span class="line"><span class="keyword">del</span> time_data[<span class="string">'creatDate'</span>]</span><br></pre></td></tr></table></figure>
<p>看一下最后的构造结果，报废特征没有构造，因为发现了一个特点就是这里的数据 10 年以上的车会偏斜，所以感觉这个用 10 年作为分界线不太合适，只是提供一种思路</p>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GA0d6P.png" alt></p>
<h3 id="gen-ju-qi-che-de-shi-yong-shi-jian-huo-zhe-dan-wang-ji-fen-tong-jin-xing-tong-ji-te-zheng-de-gou-zao">根据汽车的使用时间或者淡旺季分桶进行统计特征的构造</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造统计特征的话需要在训练集上先计算</span></span><br><span class="line">train_data_timestats = train_data.copy()   <span class="comment"># 不要动train_data</span></span><br><span class="line"></span><br><span class="line">train_data_timestats[<span class="string">'estivalue'</span>] = time_data[<span class="string">'estivalue'</span>][:train_data.shape[<span class="number">0</span>]]</span><br><span class="line">train_data_timestats[<span class="string">'price'</span>] = train_target</span><br><span class="line"></span><br><span class="line">train_gt = train_data_timestats.groupby(<span class="string">'estivalue'</span>)</span><br><span class="line">all_info = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> kind, kind_data <span class="keyword">in</span> train_gt:</span><br><span class="line">    info = &#123;&#125;</span><br><span class="line">    kind_data = kind_data[kind_data[<span class="string">'price'</span>] &gt; <span class="number">0</span>]</span><br><span class="line">    info[<span class="string">'estivalue_count'</span>] = len(kind_data)</span><br><span class="line">    info[<span class="string">'estivalue_price_max'</span>] = kind_data.price.max()</span><br><span class="line">    info[<span class="string">'estivalue_price_median'</span>] = kind_data.price.median()</span><br><span class="line">    info[<span class="string">'estivalue_price_min'</span>] = kind_data.price.min()</span><br><span class="line">    info[<span class="string">'estivalue_price_sum'</span>] = kind_data.price.sum()</span><br><span class="line">    info[<span class="string">'estivalueprice_std'</span>] = kind_data.price.std()</span><br><span class="line">    info[<span class="string">'estivalue_price_average'</span>] = round(kind_data.price.sum() / (len(kind_data) + <span class="number">1</span>), <span class="number">2</span>)</span><br><span class="line">    all_info[kind] = info</span><br><span class="line"></span><br><span class="line">estivalue_fe = pd.DataFrame(all_info).T.reset_index().rename(columns=&#123;<span class="string">"index"</span>: <span class="string">"estivalue"</span>&#125;)</span><br><span class="line">time_data = time_data.merge(estivalue_fe, how=<span class="string">'left'</span>, on=<span class="string">'estivalue'</span>)</span><br></pre></td></tr></table></figure>
<p>这样，时间特征就基本构造完毕，最后的结果如下：</p>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GA0fXV.png" alt></p>
<p>这样，时间特征这块就构造了 10 个特征出来，当然还可以更多，由于篇幅原因，其他的可以自行尝试。</p>
<h2 id="he-bing-shu-ju">合并数据</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">final_data = pd.concat([num_data, cat_data, time_data], axis=<span class="number">1</span>)</span><br><span class="line">final_data.shape  <span class="comment"># (200000, 74)</span></span><br><span class="line">final_data.head()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GArCQI.png" alt></p>
<h1 id="te-zheng-xuan-ze">特征选择</h1>
<p>好的特征选择能够提升模型的性能，更能帮助我们理解数据的特点、底层结构，这对进一步改善模型、算法都有着重要作用。但是拿到数据集，一个特征选择方法，往往很难达到目的。通常情况下，我们经常不管三七二十一，选择一种自己最熟悉或者最方便的特征选择方法（往往目的是降维，而忽略了对特征和数据理解的目的）</p>
<p>特征选择主要有两个功能：</p>
<ul>
<li>减少特征数量、降维，使模型泛化能力更强，减少过拟合</li>
<li>增强对特征和特征值之间的理解</li>
</ul>
<p>通常来说，从两个方面考虑来选择特征：</p>
<ul>
<li>特征是否发散：如果一个特征不发散，例如方差接近于 0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用</li>
<li>特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择</li>
</ul>
<p>根据特征选择的形式又可以将特征选择方法分为 3 种：</p>
<ol>
<li>Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征</li>
<li>Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征</li>
<li>Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于 Filter 方法，但是是通过训练来确定特征的优劣</li>
</ol>
<h2 id="guo-lu-shi">过滤式</h2>
<p><strong>主要思想</strong>：对每一维特征 “打分”，即给每一维的特征赋予权重，这样的权重就代表着该特征的重要性，然后依据权重排序。先进行特征选择，然后去训练学习器，所以特征选择的过程与学习器无关。相当于先对特征进行过滤操作，然后用特征子集来训练分类器</p>
<p><strong>主要方法</strong>：</p>
<ol>
<li>移除低方差的特征</li>
<li>相关系数排序，分别计算每个特征与输出值之间的相关系数，设定一个阈值，选择相关系数大于阈值的部分特征</li>
<li>利用假设检验得到特征与输出值之间的相关性，方法有比如卡方检验、t 检验、F 检验等</li>
<li>互信息，利用互信息从信息熵的角度分析相关性</li>
</ol>
<p>**Trick1：**对于数值型特征，方差很小的特征可以不要，因为太小没有什么区分度，提供不了太多的信息，对于分类特征，也是同理，取值个数高度偏斜的那种可以先去掉</p>
<p>**Trick2：**根据与目标的相关性等选出比较相关的特征（当然有时候根据字段含义也可以选）</p>
<p>**Trick3：**方检验一般是检查离散变量与离散变量的相关性，当然离散变量的相关性信息增益和信息增益比也是不错的选择（可以通过决策树模型来评估来看），person 系数一般是查看连续变量与连续变量的线性相关关系</p>
<h3 id="qu-diao-qu-zhi-bian-hua-xiao-de-te-zheng">去掉取值变化小的特征</h3>
<p>这应该是最简单的特征选择方法了：假设某特征的特征值只有 0 和 1，并且在所有输入样本中，95% 的实例的该特征取值都是 1，那就可以认为这个特征作用不大。如果 100% 都是 1，那这个特征就没意义了。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用，而且实际当中，一般不太会有 95% 以上都取某个值的特征存在，所以这种方法虽然简单但是不太好用。可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。例如 seller 和 offerType</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对方差的大小排序</span></span><br><span class="line">select_data.std().sort_values()    <span class="comment"># select_data是final_data去掉了独热的那些特征</span></span><br></pre></td></tr></table></figure>
<p>根据这个，可以把方差非常小的特征作为备选的删除特征（备选，先盲目删除）</p>
<h3 id="dan-bian-liang-te-zheng-xuan-ze">单变量特征选择</h3>
<p>单变量特征选择能够对每一个特征进行测试，衡量该特征和响应变量之间的关系，根据得分扔掉不好的特征。对于回归和分类问题可以采用卡方检验等方式对特征进行测试</p>
<p>下面重点介绍一下 pearson 相关系数，皮尔森相关系数是一种最简单的，比较常用的方式。能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为 [-1，1]，-1 表示完全的负相关(这个变量下降，那个就会上升)，+1 表示完全的正相关，0 表示没有线性相关。Pearson Correlation 速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的) 之后第一时间就执行。Scipy 的 pearsonr 方法能够同时计算相关系数和 p-value，当然 pandas 的 corr 也可以计算：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">corr = select_data.corr(<span class="string">'pearson'</span>)    <span class="comment"># .corr('spearman')</span></span><br><span class="line">plt.figure(figsize=(<span class="number">25</span>, <span class="number">15</span>))</span><br><span class="line">corr[<span class="string">'price'</span>].sort_values(ascending=<span class="literal">False</span>)[<span class="number">1</span>:].plot(kind=<span class="string">'bar'</span>)</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GAsi9J.png" alt></p>
<p>当然，这个数据用 pearson 系数可能不是那么合理，可以使用 spearman 系数，这个被认为是排列后的变量的 pearson 的相关系数。</p>
<blockquote>
<p>皮尔逊相关系数:下面是皮尔逊相关系数的计算公式，只需要将（X和Y的协方差）/（X的标准差*Y的标准差）<br>
\[
\rho_{X, Y}=\frac{\operatorname{cov}(X, Y)}{\sigma_{X} \sigma_{Y}}=\frac{E\left(\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right)}{\sigma_{X} \sigma_{Y}}
\]</p>
<p>spearman相关系数:Spearman秩相关系数通常被认为是排列后的变量之间的Pearson线性相关系数.</p>
<p>那么对于这两个系数，怎样的值才是好的呢，遵循下面的关系:</p>
<ul>
<li><strong>0.8-1.0</strong>：极强相关</li>
<li><strong>0.6-0.8</strong>：强相关</li>
<li><strong>0.4-0.6</strong>：中等强度相关</li>
<li><strong>0.2-0.4</strong>：弱相关</li>
<li><strong>0.0-0.2</strong>：极弱或者无相关</li>
</ul>
<p>区别:</p>
<ul>
<li>连续数据，正态分布，线性关系，用 pearson 相关系数是最恰当，当然用 spearman 相关系数也可以，效率没有 pearson 相关系数高</li>
<li>上述任一条件不满足，就用 spearman 相关系数</li>
<li>两个定序测量数据（顺序变量）之间也用 spearman 相关系数，不能用 pearson 相关系数</li>
<li>Pearson 相关系数的一个<strong>明显缺陷</strong>是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson 相关性也可能会接近 0</li>
</ul>
<p>代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = loans_2007[[<span class="string">"funded_amnt"</span>, <span class="string">"funded_amnt_inv"</span>]]</span><br><span class="line"><span class="comment">#计算皮尔逊系数</span></span><br><span class="line">print(data.corr())</span><br><span class="line"><span class="comment">#计算spearman系数</span></span><br><span class="line">print(data.corr(<span class="string">'spearman'</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                 funded_amnt  funded_amnt_inv</span><br><span class="line">funded_amnt         <span class="number">1.000000</span>         <span class="number">0.947525</span></span><br><span class="line">funded_amnt_inv     <span class="number">0.947525</span>         <span class="number">1.000000</span></span><br><span class="line">                 funded_amnt  funded_amnt_inv</span><br><span class="line">funded_amnt          <span class="number">1.00000</span>          <span class="number">0.92876</span></span><br><span class="line">funded_amnt_inv      <span class="number">0.92876</span>          <span class="number">1.00000</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>当然还可以画出热力图来，这个的目的是可以看变量之间的关系，相关性大的，可以考虑保留其中一个：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f, ax = plt.subplots(figsize=(<span class="number">50</span>, <span class="number">30</span>))</span><br><span class="line">sns.heatmap(corr, annot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GAsJDP.png" alt></p>
<p>经过上面两个步骤就可以发现一些结论：</p>
<ul>
<li>根据与 price 的线性相关关系来看的话，可以考虑正负相关 0.6 以上的特征， v_std, v_12, v_0, v_8, estivalue_price_average, estivalue_price_median, estivalue_price_std， kil_bin, kilmoeter, estivalue_count, used_time, estivalue, v_3</li>
<li>某些变量之间有很强的的关联性，比如 v_mean 和 v_sum，这俩的相关性是 1，所以可以删掉其中一个</li>
</ul>
<p>当然，依然是备选删除选项和备选保留选项（这些都先别做），因为我们有时候不能盲目，就比如上面的相关性，我们明明知道 pearson 的缺陷是无法捕捉非线性相关，所以得出的这个结论也是片面的结论，所以这些都是备选，先做个心中有数，后面再用一些别的方式看看再说（如果现在就删除了，后面的方法就不好判断了）</p>
<h2 id="bao-guo-shi">包裹式</h2>
<p>单变量特征选择方法独立的衡量每个特征与响应变量之间的关系，另一种主流的特征选择方法是基于机器学习模型的方法。有些机器学习方法本身就具有对特征进行打分的机制，或者很容易将其运用到特征选择任务中，例如回归模型，SVM，决策树，随机森林等等。</p>
<p><strong>主要思想</strong>：包裹式从初始特征集合中不断的选择特征子集，训练学习器，根据学习器的性能来对子集进行评价，直到选择出最佳的子集。包裹式特征选择直接针对给定学习器进行优化。</p>
<p><strong>主要方法</strong>：递归特征消除算法，基于机器学习模型的特征排序。</p>
<p><strong>优缺点</strong>：</p>
<ul>
<li>优点：从最终学习器的性能来看，包裹式比过滤式更好</li>
<li>缺点：由于特征选择过程中需要多次训练学习器，因此包裹式特征选择的计算开销通常比过滤式特征选择要大得多</li>
</ul>
<p>基于学习模型的特征排序方法，这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。其实 Pearson 相关系数等价于线性回归里的标准化回归系数。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。</p>
<p>用随机森林来跑一下，看看随机森林比较喜欢特征：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score, ShuffleSplit</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"></span><br><span class="line">X = select_data.iloc[:, :<span class="number">-1</span>]</span><br><span class="line">Y = select_data[<span class="string">'price'</span>]</span><br><span class="line">names = select_data.columns</span><br><span class="line"></span><br><span class="line">rf = RandomForestRegressor(n_estimators=<span class="number">20</span>, max_depth=<span class="number">4</span>)</span><br><span class="line">kfold = KFold(n_splits=<span class="number">5</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">7</span>)</span><br><span class="line">scores = []</span><br><span class="line"><span class="keyword">for</span> column <span class="keyword">in</span> X.columns:</span><br><span class="line">    print(column)</span><br><span class="line">    tempx = X[column].values.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    score = cross_val_score(rf, tempx, Y, scoring=<span class="string">"r2"</span>,</span><br><span class="line">                              cv=kfold)</span><br><span class="line">    scores.append((round(np.mean(score), <span class="number">3</span>), column))</span><br><span class="line">print(sorted(scores, reverse=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<p>这里对喜欢的特征排序并打分，结果如下：</p>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GAyQaT.png" alt></p>
<p>这里就可以看出随机森林有用的特征排序，如果我们后面选择随机森林作为模型，就可以根据这个特征重要度选择特征。当然，如果是 xgboost，xgboost 里面有个画特征重要性的函数，可以这样做：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用xgboost跑一下</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> plot_importance</span><br><span class="line"></span><br><span class="line">xgb = XGBRegressor()</span><br><span class="line">xgb.fit(X, Y)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">plot_importance(xgb)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GAyrJe.png" alt></p>
<p>最后，把上面的这两种方式封装起来，还可以画出边际效应：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mlxtend.feature_selection <span class="keyword">import</span> SequentialFeatureSelector <span class="keyword">as</span> SFS</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># sfs = SFS(LinearRegression(), k_features=20, forward=True, floating=False, scoring='r2', cv=0)</span></span><br><span class="line">sfs = SFS(RandomForestRegressor(n_estimators=<span class="number">10</span>, max_depth=<span class="number">4</span>), k_features=<span class="number">20</span>, forward=<span class="literal">True</span>, floating=<span class="literal">False</span>, scoring=<span class="string">'r2'</span>, cv=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">X = select_data.iloc[:, :<span class="number">-1</span>]</span><br><span class="line">Y = select_data[<span class="string">'price'</span>]</span><br><span class="line"></span><br><span class="line">sfs.fit(X, Y)</span><br><span class="line">sfs.k_feature_names_    <span class="comment"># 随机森林放这里跑太慢了，所以中断了</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GAyIJg.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 画出边际效应</span></span><br><span class="line"><span class="keyword">from</span> mlxtend.plotting <span class="keyword">import</span> plot_sequential_feature_selection <span class="keyword">as</span> plot_sfs</span><br><span class="line"></span><br><span class="line">fig1 = plot_sfs(sfs.get_metric_dict(), kind=<span class="string">'std_dev'</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GA69SJ.png" alt></p>
<p>综合上面的这几种方式，就可以把保留和删除的特征给选出来了。</p>
<p>如果真的这样尝试一下，就会发现保留的特征里面， v_std, v_3, used_time, power, kilometer, estivalue 等这些特征都在，虽然不知道 v 系列特征的含义，但是汽车使用时间，发动机功率，行驶公里， 汽车使用时间的分箱特征其实对 price 的影响都是比较大的。</p>
<p>下面在介绍一种嵌入式的方式，当然这里我没用，因为我不打算后面的模型用线性模型来做。但这种思路得知道。</p>
<h2 id="qian-ru-shi">嵌入式</h2>
<p>在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。而嵌入式特征选择在学习器训练过程中自动地进行特征选择。嵌入式选择最常用的是 L1 正则化与 L2 正则化。在对线性回归模型加入两种正则化方法后，他们分别变成了岭回归与 Lasso 回归。</p>
<p><strong>主要思想</strong>：在模型既定的情况下学习出对提高模型准确性最好的特征。也就是在确定模型的过程中，挑选出那些对模型的训练有重要意义的特征。</p>
<p><strong>主要方法</strong>：简单易学的机器学习算法–岭回归（Ridge Regression），就是线性回归过程加入了 L2 正则项</p>
<ul>
<li>L2 正则化在拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。</li>
<li>L1 正则化有助于生成一个稀疏权值矩阵，进而可以用于特征选择。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression, Ridge,Lasso</span><br><span class="line"></span><br><span class="line">models = [LinearRegression(), Ridge(), Lasso()]</span><br><span class="line">result = dict()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">    model_name = str(model).split(<span class="string">'('</span>)[<span class="number">0</span>]</span><br><span class="line">    scores = cross_val_score(model, X=train_X, y=train_y, verbose=<span class="number">0</span>, cv=<span class="number">5</span>, scoring=<span class="string">'r2'</span>)</span><br><span class="line">    result[model_name] = scores</span><br></pre></td></tr></table></figure>
<h1 id="pca-jiang-wei">PCA 降维</h1>
<p>通过上面的特征选择部分，可以选出更好的分析特征，但是<strong>如果这些特征维度仍然很高</strong>怎么办？</p>
<p>如果数据特征维度太高，首先计算很麻烦，其次增加了问题的复杂程度，分析起来也不方便。这时候我们就会想是不是再去掉一些特征就好了呢？但是这个特征也不是凭自己的意愿去掉的，因为盲目减少数据的特征会损失掉数据包含的关键信息，容易产生错误的结论，对分析不利。所以我们想找到一个合理的方式，既可以减少我们需要分析的指标，而且尽可能多的保持原来数据的信息，PCA 就是这个合理的方式之一。</p>
<p>这里只整理如何用，但要注意一点，特征选择是从已存在的特征中选取携带信息最多的，选完之后的特征依然具有可解释性，而 PCA，将已存在的特征压缩，降维完毕后不是原来特征的任何一个，也就是 PCA 降维之后的特征我们根本不知道什么含义了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后使用</span></span><br><span class="line">pca = PCA(n_components=<span class="number">10</span>)</span><br><span class="line">X_new = pca.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="string">"""查看PCA的一些属性"""</span></span><br><span class="line">print(X_new.shape)   <span class="comment"># （200000， 10）</span></span><br><span class="line">print(pca.explained_variance_)    <span class="comment"># 属性可以查看降维后的每个特征向量上所带的信息量大小（可解释性方差的大小）</span></span><br><span class="line">print(pca.explained_variance_ratio_)  <span class="comment"># 查看降维后的每个新特征的信息量占原始数据总信息量的百分比</span></span><br><span class="line">print(pca.explained_variance_ratio_.sum())    <span class="comment"># 降维后信息保留量</span></span><br></pre></td></tr></table></figure>
<p>假设我保留了 10 个特征，然后运行代码，一下子就成了 10 维的矩阵，我们可以看一下 X_new：</p>
<p><img src="https://s1.ax1x.com/2020/03/28/GAcZBq.png#shadow" alt="img"></p>
<p>这些数已经只能说尽可能的保留原有的数据信息，但是是什么含义，我们也不知道了。</p>
<h1 id="can-kao">参考</h1>
<ol>
<li>
<p><a href="https://blog.csdn.net/wuzhongqiang/article/details/105146150" target="_blank" rel="noopener">特征选择，我们真的学会了吗？</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/wuzhongqiang/article/details/104607832" target="_blank" rel="noopener">白话机器学习算法理论 + 实战之 PCA 降维</a></p>
</li>
<li>
<p><a href="https://wmathor.com/index.php/archives/1426/" target="_blank" rel="noopener">特征工程</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>数据探索性分析(EDA)</title>
    <url>/2020/04/01/machine_learning/exploratory_data_analysis/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/eda.png" alt></p>
<a id="more"></a>
<p><code><a class="btn" href="数据探索性分析(EDA).xmind">
            <i class="fa fa-download"></i>xmind文件下载
          </a></code></p>
<p><a href="https://share.weiyun.com/EDpohPCk" target="_blank" rel="noopener">数据集下载</a></p>
<p>导入一些包</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> missingno <span class="keyword">as</span> msno <span class="comment"># 缺失值的可视化处理</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""导入数据集"""</span></span><br><span class="line"><span class="comment"># data = pd.read_csv('test.csv'，sep ='|' ，header = 0，skiprows = 10，nrows = 10)</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'./dataset/used_car_train_20200313.csv'</span>, sep=<span class="string">' '</span>) <span class="comment"># 指定分隔符为空格</span></span><br><span class="line">test_data = pd.read_csv(<span class="string">'./dataset/used_car_testA_20200313.csv'</span>, sep=<span class="string">' '</span>)</span><br></pre></td></tr></table></figure>
<h1 id="shu-ju-chu-shi">数据初识</h1>
<p>主要是对读取的数据有一个大致的了解，包括简单了解数据的行列信息，数据的统计特征等。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看数据的形状（行数和列数）</span></span><br><span class="line">print(<span class="string">'train_data shape :'</span>, train_data.shape) <span class="comment"># (150000, 31)</span></span><br><span class="line">print(<span class="string">'test_data shape :'</span>, test_data.shape) <span class="comment"># (50000, 30)</span></span><br><span class="line"><span class="comment"># 从形状可以看出训练集共 150000 个样本，30 个特征，1 列价格；测试集共 50000 个样本，30 个特征</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据简要概览</span></span><br><span class="line">train_data.head().append(train_data.tail()) <span class="comment"># 将开头5行和结尾5行拼接起来展示，head()和tail()默认值是5</span></span><br><span class="line">test_data.head().append(test_data.tail())</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8LEk6S.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据信息的查看 .info()可以看到每列的type，以及NAN缺失值的信息</span></span><br><span class="line">train_data.info()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8LZ5wQ.png" alt></p>
<p>通过<code>info()</code>可以发现几点信息，首先就是字段的类型，有一个 object（后面需要单独处理）。其次有一些字段有空值，清洗的时候需要处理。</p>
<p>通过<code>info()</code>了解数据每列的 type，还有助于了解是否存在除了 nan 以外的特殊符号异常</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 通过 .columns 查看列名</span></span><br><span class="line">train_data.columns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据的统计信息概览</span></span><br><span class="line">train_data.describe()</span><br></pre></td></tr></table></figure>
<p><code>describe()</code>中有每列的统计值，包括：个数 count、平均值 mean、方差 std、最小值 min、下四分位数 25%、中位数 50%、上四分位数 75%、以及最大值 max。查看这些信息可以瞬间掌握数据的大概范围<strong>以及异常值判断，例如 999999，-1 这些值其实是 nan 的另一种表达方式</strong>，有时候需要注意下。</p>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8LmXa4.png" alt></p>
<p>注意看左下角，提示有 30 列，但是刚才输出<code>shape</code>的时候明明提示有 31 列。注意，<code>describe()</code>是不包括 object 类型字段的统计信息的，毕竟不是数值类型。当然也可以单独用<code>describe()</code>看看</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data[<span class="string">'notRepairedDamage'</span>].describe()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果：</span></span><br><span class="line">count     <span class="number">150000</span></span><br><span class="line">unique         <span class="number">3</span></span><br><span class="line">top          <span class="number">0.0</span></span><br><span class="line">freq      <span class="number">111361</span></span><br><span class="line">Name: notRepairedDamage, dtype: object</span><br></pre></td></tr></table></figure>
<h1 id="shu-ju-gan-zhi">数据感知</h1>
<p>数据感知是在数据初识的基础上，进一步挖掘数据的信息，主要包括数据的确实值和异常值等</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看每列存在nan的情况</span></span><br><span class="line">train_data.isnull().sum()</span><br><span class="line"><span class="comment"># test_data.isnull().sum()</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8LuGnK.png" alt></p>
<p>可以看出，<code>model</code>、<code>bodyType</code>、<code>fuelType</code>、<code>gearbox</code>有缺失值。还可以对 nan 进行可视化，看的更加明显。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">missing = train_data.isnull().sum()</span><br><span class="line">missing = missing[missing &gt; <span class="number">0</span>]</span><br><span class="line">missing.sort_values(inplace=<span class="literal">True</span>)</span><br><span class="line">missing.plot.bar()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8LMJFe.png" alt></p>
<p>可视化 nan 的个数主要目的在于，查看 nan 存在的个数是否真的很大，如果很小一般选择填充，如果使用 lgb 等树模型可以直接让树自己去优化，但如果 nan 存在的过多，可以考虑删掉。</p>
<p>下面是可视化缺失值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可视化缺失值</span></span><br><span class="line">msno.matrix(train_data.sample(<span class="number">250</span>)) <span class="comment"># sample(250)表示抽取250个样本</span></span><br><span class="line"><span class="comment"># msno.matrix(test_data.sample(250))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># msno.bar(train_data.sample(1000))</span></span><br><span class="line"><span class="comment"># msno.bar(test_data.sample(1000))</span></span><br></pre></td></tr></table></figure>
<p>下图是代码运行后得到的结果，白线越多，代表缺失值越多（fuleType 缺失的最多）。</p>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8LQpkD.png" alt></p>
<p><strong>对数据持着怀疑的角度审视，尤其是 object 字段</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 看看object这个字段的取值情况</span></span><br><span class="line">train_data[<span class="string">'notRepairedDamage'</span>].value_counts()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.0</span>    <span class="number">111361</span></span><br><span class="line">-       <span class="number">24324</span></span><br><span class="line"><span class="number">1.0</span>     <span class="number">14315</span></span><br><span class="line">Name: notRepairedDamage, dtype: int64</span><br></pre></td></tr></table></figure>
<p>这个字段里面居然有个<code>-</code>值，如果单看比赛给的字段描述：0 代表有未修复的损害，1 代表没有，如果不持着怀疑的态度，很难发现这里还有个<code>-</code>，这个也代表缺失，因为很多模型可以对 nan 直接处理，所以这里我们先将<code>-</code>替换为 nan。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data[<span class="string">'noRepairedDamage'</span>].replace(<span class="string">'-'</span>, np.nan, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># test_data['noRepairedDamage'].replace('-', np.nan, inplace=True)</span></span><br></pre></td></tr></table></figure>
<h1 id="shu-ju-bu-huo">数据不惑</h1>
<p>通过初识和感知，不仅认识了数据，还发现了一些异常和缺失，下面进一步挖掘数据信息，主要包括查看预测值的分布以及将字段分成数值型和类别型，后面分开查看和处理</p>
<h2 id="liao-jie-shu-ju-de-fen-bu">了解数据的分布</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""查看预测值的频数"""</span></span><br><span class="line">train_data[<span class="string">'price'</span>].value_counts()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直方图可视化 自动划分10（默认值）个价格区间 统计每个区间的频数</span></span><br><span class="line">plt.hist(train_data[<span class="string">'price'</span>], orientation=<span class="string">'vertical'</span>, histtype=<span class="string">'bar'</span>, color=<span class="string">'red'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8L36H0.png" alt></p>
<p>查看频数，发现价格大于 20000 的值极少，其实这里也可以把这些值当作特殊值（或异常值）直接删掉，不过直接删掉不太好，毕竟这是个回归问题。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""总体分布概况（无界约翰逊分布等）"""</span></span><br><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> st</span><br><span class="line">y = train_data[<span class="string">'price'</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">'Johnson SU'</span>)</span><br><span class="line">sns.distplot(y, kde=<span class="literal">False</span>, fit=st.johnsonsu)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">'normal'</span>)</span><br><span class="line">sns.distplot(y, kde=<span class="literal">False</span>, fit=st.norm)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">plt.title(<span class="string">'Log Normal'</span>)</span><br><span class="line">sns.distplot(y, kde=<span class="literal">False</span>, fit=st.lognorm)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8LGmWD.png" alt></p>
<p>可以发现，价格不服从正态分布，所以在进行回归之前，必须将它进行转换，最佳拟合的是无界约翰逊分布。对预测标签做 log 转换，使其更加服从正态分布</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># log变换之后的分布会变得比较均匀，可以进行log变换进行预测，这也是预测问题常用的trick</span></span><br><span class="line">plt.hist(np.log(train_data[<span class="string">'price'</span>]), orientation=<span class="string">'vertical'</span>, histtype=<span class="string">'bar'</span>, color=<span class="string">'red'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8LGW6J.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""查看偏度和峰度"""</span></span><br><span class="line">sns.distplot(train_data[<span class="string">'price'</span>])</span><br><span class="line">print(<span class="string">'Skewness : %f'</span> % train_data[<span class="string">'price'</span>].skew()) <span class="comment"># 偏度</span></span><br><span class="line">print(<span class="string">'Kurtosis : %f'</span> % train_data[<span class="string">'price'</span>].kurt()) <span class="comment"># 峰度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">Skewness : <span class="number">3.346487</span></span><br><span class="line">Kurtosis : <span class="number">18.995183</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8LJDjH.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># train_data.skew(), train_data.kurt()</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">sns.distplot(train_data.skew(), color=<span class="string">'blue'</span>, axlabel=<span class="string">'Skewness'</span>)</span><br><span class="line">plot.subplot(<span class="number">122</span>)</span><br><span class="line">sns.distplot(train_data.kurt(), color=<span class="string">'orange'</span>, axlabel=<span class="string">'Kurtness'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8LYJxg.png" alt></p>
<p>峰度 Kurt 代表数据分布的尖锐程度，偏度简单来说就是数据的不对称程度。</p>
<blockquote>
<p>我们一般会拿偏度和峰度来看数据的分布形态，而且一般会跟正态分布做比较，我们把正态分布的偏度和峰度都看做零。如果我们在实操中，算到偏度峰度不为0，即表明变量存在左偏右偏，或者是高顶平顶这么一说。</p>
<p><strong>偏度（Skewness）</strong>: 是描述数据分布形态的统计量，其描述的是某总体取值分布的<strong>对称性</strong>，简单来说就是数据的不对称程度。\(Skewness=E[\frac{x-E(x)}{\sqrt{D(x)}^3}]\)</p>
<ol>
<li>
<p>Skewness = 0 ，分布形态与正态分布偏度相同。</p>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/histogram_symmetrical_nonskewed_normal.png" alt></p>
</li>
<li>
<p>Skewness &gt; 0 ，正偏差数值较大，为正偏或右偏。长尾巴拖在右边，数据右端有较多的极端值。</p>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/histogram_right_skewness_with_arrow.png" alt></p>
</li>
<li>
<p>Skewness &lt; 0 ，负偏差数值较大，为负偏或左偏。长尾巴拖在左边，数据左端有较多的极端值。</p>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/histogram_left_skewness_with_arrow.png" alt></p>
</li>
<li>
<p>数值的绝对值越大，表明数据分布越不对称，偏斜程度大。</p>
</li>
</ol>
<p><strong>峰度（Kurtosis）</strong>:是描述某变量所有取值分布形态陡缓程度的统计量，简单来说就是数据分布顶的<strong>尖锐程度</strong>。</p>
<ol>
<li>
<p>Kurtosis=0 与正态分布的陡缓程度相同。</p>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/distribution_plot_normal_dist_for_kurtosis.png" alt="img"></p>
</li>
<li>
<p>Kurtosis&gt;0 比正态分布的高峰更加陡峭——尖顶峰:具有正峰度值的分布表明，相比于正态分布，该分布有更重的尾部。实线表示正态分布，虚线表示具有正峰度值的分布。</p>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/distribution_plot_positive_kurtosis.png" alt></p>
</li>
<li>
<p>Kurtosis&lt;0 比正态分布的高峰来得平台——平顶峰:具有负峰度值的分布表明，相比于正态分布，该分布有更轻的尾部。实线表示正态分布，虚线表示具有负峰度值的分布。</p>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/distribution_plot_negative_kurtosis.png" alt></p>
</li>
</ol>
</blockquote>
<h2 id="ba-zi-duan-fen-wei-shu-zhi-zi-duan-he-lei-bie-zi-duan">把字段分为数值字段和类别字段</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""先分离出label值"""</span></span><br><span class="line">y_train = train_data[<span class="string">'price'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数值特征</span></span><br><span class="line"><span class="comment"># numeric_features = train_data.select_dtypes(include=[np.number])</span></span><br><span class="line"><span class="comment"># numeric_features.columns</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 类别特征</span></span><br><span class="line"><span class="comment"># categorical_features = train_data.select_dtypes(include=[np.object])</span></span><br><span class="line"><span class="comment"># categorical_features.columns</span></span><br></pre></td></tr></table></figure>
<p>上面是自动选取的方式，也可以人为设定</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""人为设定"""</span></span><br><span class="line">numeric_features = [<span class="string">'power'</span>, <span class="string">'kilometer'</span>].extend([<span class="string">'v_'</span>+str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">15</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里我感觉这个name和预测值没有关系，所以虽然是类别，可以先去掉看看, 日期的也去掉</span></span><br><span class="line">categorical_features = [<span class="string">'model'</span>, <span class="string">'brand'</span>, <span class="string">'bodyType'</span>, <span class="string">'fuelType'</span>, <span class="string">'gearbox'</span>, </span><br><span class="line">                        <span class="string">'notRepairedDamage'</span>,<span class="string">'regionCode'</span>, <span class="string">'seller'</span>, <span class="string">'offerType'</span>]</span><br></pre></td></tr></table></figure>
<h1 id="shu-ju-dong-xuan">数据洞玄</h1>
<p>前面的工作已经分析了预测值的分布，从分布中可以看到，如果把预测值进行对数变化一下，效果可能更好。然后把特征字段拆分为数值型和类别型。接下来我们主要对数值特征和类别特征进一步挖掘信息，包括类别偏斜，类别分布可视化，数值可视化等。</p>
<h2 id="lei-bie-te-zheng-de-tan-suo">类别特征的探索</h2>
<p>类别特征主要是看一下每个类别字段的取值和分布，会用到箱型图、小提琴图、柱状图等各种可视化技巧。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""类别偏斜处理"""</span></span><br><span class="line"><span class="keyword">for</span> cate_fea <span class="keyword">in</span> category_features:</span><br><span class="line">    print(cate_fea + <span class="string">'特征分布如下：'</span>)</span><br><span class="line">    print(<span class="string">'&#123;&#125;特征有&#123;&#125;不同的值'</span>.format(cate_fea, len(train_data[cate_fea].unique())))</span><br><span class="line">    print(train_data[cate_fea].value_counts())</span><br><span class="line">    print()</span><br></pre></td></tr></table></figure>
<p>这里主要是重点查看一下类别特征有没有数量严重偏斜的情况（由于太多，不在这里显示），这样的情况一般对预测没有什么帮助</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data[<span class="string">'seller'</span>].value_counts()</span><br><span class="line"></span><br><span class="line"><span class="number">0</span>    <span class="number">149999</span></span><br><span class="line"><span class="number">1</span>         <span class="number">1</span></span><br><span class="line">Name: seller, dtype: int64</span><br><span class="line"></span><br><span class="line">train_data[<span class="string">'offerType'</span>].value_counts()</span><br><span class="line"></span><br><span class="line"><span class="number">0</span>    <span class="number">150000</span></span><br><span class="line">Name: offerType, dtype: int64</span><br></pre></td></tr></table></figure>
<p>像<code>seller</code>、<code>offerType</code>字段偏斜就比较严重，直接删除这些字段:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">del</span> train_data[<span class="string">'seller'</span>]</span><br><span class="line"><span class="keyword">del</span> train_data[<span class="string">'offerType'</span>]</span><br><span class="line"><span class="keyword">del</span> test_data[<span class="string">'seller'</span>]</span><br><span class="line"><span class="keyword">del</span> test_data[<span class="string">'offerType'</span>]</span><br><span class="line"></span><br><span class="line">categorical_features.remove(<span class="string">'seller'</span>)</span><br><span class="line">categorical_features.remove(<span class="string">'offerType'</span>)</span><br></pre></td></tr></table></figure>
<p>下面看一下每个字段，有多少类（unique）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""类别的unique分布"""</span></span><br><span class="line"><span class="keyword">for</span> cat <span class="keyword">in</span> categorical_features:</span><br><span class="line">    print(len(train_data[cat].unique()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">249</span></span><br><span class="line"><span class="number">40</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line"><span class="number">8</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">7905</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为regionCode的类别太稀疏了，所以先去掉，因为后面要可视化，不画稀疏的</span></span><br><span class="line">categorical_features.remove(<span class="string">'regionCode'</span>)</span><br></pre></td></tr></table></figure>
<p>下面使用各种可视化方式，可视化类别特征:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""类别特征箱型图可视化"""</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> categorical_features:</span><br><span class="line">    train_data[c] = train_data[c].astype(<span class="string">'category'</span>)</span><br><span class="line">    <span class="keyword">if</span> train_data[c].isnull().any():</span><br><span class="line">        train_data[c] = train_data[c].cat.add_categories([<span class="string">'MISSING'</span>])</span><br><span class="line">        train_data[c] = train_data[c].fillna(<span class="string">'MISSING'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">boxplot</span><span class="params">(x, y, **kwargs)</span>:</span></span><br><span class="line">    sns.boxenplot(x=x, y=y)</span><br><span class="line">    x = plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line"></span><br><span class="line">f = pd.melt(train_data, id_vars=[<span class="string">'price'</span>], value_vars=categorical_features)</span><br><span class="line">g = sns.FacetGrid(f, col=<span class="string">"variable"</span>,  col_wrap=<span class="number">3</span>, sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>, size=<span class="number">5</span>)</span><br><span class="line">g = g.map(boxplot, <span class="string">"value"</span>, <span class="string">"price"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8LaCQO.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""类别特征的小提琴图可视化， 小提琴图类似箱型图，比后者高级点，图好看些"""</span></span><br><span class="line">catg_list = categorical_features</span><br><span class="line">target = <span class="string">'price'</span></span><br><span class="line"><span class="keyword">for</span> catg <span class="keyword">in</span> catg_list :</span><br><span class="line">    sns.violinplot(x=catg, y=target, data=train_data)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>小提琴的不在这展示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""类别特征的柱形图可视化"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bar_plot</span><span class="params">(x, y, **kwargs)</span>:</span></span><br><span class="line">    sns.barplot(x=x, y=y)</span><br><span class="line">    x=plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line"></span><br><span class="line">f = pd.melt(train_data, id_vars=[<span class="string">'price'</span>], value_vars=categorical_features)</span><br><span class="line">g = sns.FacetGrid(f, col=<span class="string">"variable"</span>,  col_wrap=<span class="number">3</span>, sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>, size=<span class="number">5</span>)</span><br><span class="line">g = g.map(bar_plot, <span class="string">"value"</span>, <span class="string">"price"</span>)</span><br></pre></td></tr></table></figure>
<p>看一下柱形图的结果</p>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8La50H.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""类别特征的每个类别频数可视化(count_plot)"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_plot</span><span class="params">(x,  **kwargs)</span>:</span></span><br><span class="line">    sns.countplot(x=x)</span><br><span class="line">    x=plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line"></span><br><span class="line">f = pd.melt(train_data,  value_vars=categorical_features)</span><br><span class="line">g = sns.FacetGrid(f, col=<span class="string">"variable"</span>,  col_wrap=<span class="number">3</span>, sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>, size=<span class="number">5</span>)</span><br><span class="line">g = g.map(count_plot, <span class="string">"value"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8Lda4I.png" alt></p>
<blockquote>
<p>规整数据/转换数据:melt()</p>
<p>df.melt() 是 df.pivot() 逆转操作函数,将列名转换为列数据(columns name → column values)，重构DataFrame。如果说 df.pivot() 将长数据集转换成宽数据集，df.melt() 则是将宽数据集变成长数据集。</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>frame</td>
<td>dataframe</td>
<td>被 melt 的数据集名称</td>
</tr>
<tr>
<td>id_vars</td>
<td>tuple、list、ndarray</td>
<td><strong>不需要被转换的列名</strong>，在转换后作为标识符列（不是索引列）</td>
</tr>
<tr>
<td>value_vars</td>
<td>tuple、list、ndarray</td>
<td><strong>需要被转换的现有列</strong>，如果未指明，除 id_vars 之外的其他列都被转换</td>
</tr>
<tr>
<td>var_name</td>
<td>ndarray</td>
<td>自定义列名名称，<strong>设置由 ‘value_vars’ 组成的新的 column name</strong></td>
</tr>
<tr>
<td>value_name</td>
<td>string</td>
<td>自定义列名名称，<strong>设置由 ‘value_vars’ 的数据组成的新的 column name</strong></td>
</tr>
<tr>
<td>col_level</td>
<td>int、string</td>
<td>如果列是MultiIndex，则使用此级别</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df = pd.DataFrame(&#123;<span class="string">'A'</span>: &#123;<span class="number">0</span>: <span class="string">'a'</span>, <span class="number">1</span>: <span class="string">'b'</span>, <span class="number">2</span>: <span class="string">'c'</span>&#125;,</span><br><span class="line"><span class="meta">... </span>                   <span class="string">'B'</span>: &#123;<span class="number">0</span>: <span class="number">1</span>, <span class="number">1</span>: <span class="number">3</span>, <span class="number">2</span>: <span class="number">5</span>&#125;,</span><br><span class="line"><span class="meta">... </span>                   <span class="string">'C'</span>: &#123;<span class="number">0</span>: <span class="number">2</span>, <span class="number">1</span>: <span class="number">4</span>, <span class="number">2</span>: <span class="number">6</span>&#125;&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df</span><br><span class="line">   A  B  C</span><br><span class="line"><span class="number">0</span>  a  <span class="number">1</span>  <span class="number">2</span></span><br><span class="line"><span class="number">1</span>  b  <span class="number">3</span>  <span class="number">4</span></span><br><span class="line"><span class="number">2</span>  c  <span class="number">5</span>  <span class="number">6</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#保留 B 列</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.melt(id_vars=[<span class="string">'A'</span>], value_vars=[<span class="string">'B'</span>])</span><br><span class="line">   A variable  value</span><br><span class="line"><span class="number">0</span>  a        B      <span class="number">1</span></span><br><span class="line"><span class="number">1</span>  b        B      <span class="number">3</span></span><br><span class="line"><span class="number">2</span>  c        B      <span class="number">5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#保留 B C 列</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.melt(id_vars=[<span class="string">'A'</span>], value_vars=[<span class="string">'B'</span>, <span class="string">'C'</span>])</span><br><span class="line">   A variable  value</span><br><span class="line"><span class="number">0</span>  a        B      <span class="number">1</span></span><br><span class="line"><span class="number">1</span>  b        B      <span class="number">3</span></span><br><span class="line"><span class="number">2</span>  c        B      <span class="number">5</span></span><br><span class="line"><span class="number">3</span>  a        C      <span class="number">2</span></span><br><span class="line"><span class="number">4</span>  b        C      <span class="number">4</span></span><br><span class="line"><span class="number">5</span>  c        C      <span class="number">6</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#自定义列名</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.melt(id_vars=[<span class="string">'A'</span>], value_vars=[<span class="string">'B'</span>],var_name=<span class="string">'myVarname'</span>, value_name=<span class="string">'myValname'</span>)</span><br><span class="line">   A myVarname  myValname</span><br><span class="line"><span class="number">0</span>  a         B          <span class="number">1</span></span><br><span class="line"><span class="number">1</span>  b         B          <span class="number">3</span></span><br><span class="line"><span class="number">2</span>  c         B          <span class="number">5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#如果 columns 是MultiIndex</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.columns = [list(<span class="string">'ABC'</span>), list(<span class="string">'DEF'</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df</span><br><span class="line">   A  B  C</span><br><span class="line">   D  E  F</span><br><span class="line"><span class="number">0</span>  a  <span class="number">1</span>  <span class="number">2</span></span><br><span class="line"><span class="number">1</span>  b  <span class="number">3</span>  <span class="number">4</span></span><br><span class="line"><span class="number">2</span>  c  <span class="number">5</span>  <span class="number">6</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.melt(col_level=<span class="number">0</span>, id_vars=[<span class="string">'A'</span>], value_vars=[<span class="string">'B'</span>])</span><br><span class="line">   A variable  value</span><br><span class="line"><span class="number">0</span>  a        B      <span class="number">1</span></span><br><span class="line"><span class="number">1</span>  b        B      <span class="number">3</span></span><br><span class="line"><span class="number">2</span>  c        B      <span class="number">5</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.melt(id_vars=[(<span class="string">'A'</span>, <span class="string">'D'</span>)], value_vars=[(<span class="string">'B'</span>, <span class="string">'E'</span>)])</span><br><span class="line">  (A, D) variable_0 variable_1  value</span><br><span class="line"><span class="number">0</span>      a          B          E      <span class="number">1</span></span><br><span class="line"><span class="number">1</span>      b          B          E      <span class="number">3</span></span><br><span class="line"><span class="number">2</span>      c          B          E      <span class="number">5</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/70.png" alt></p>
</blockquote>
<h2 id="shu-zhi-te-zheng-de-tan-suo">数值特征的探索</h2>
<p>数值特征的探索我们要分析相关性等，也会学习各种相关性可视化的技巧：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">numeric_train_data = train_data[numeric_features]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把price这一列加上，这个也是数值</span></span><br><span class="line">numeric_train_data[<span class="string">'price'</span>] = Y_train</span><br><span class="line"></span><br><span class="line"><span class="string">"""相关性分析"""</span></span><br><span class="line">correlation = numeric_train_data.corr()</span><br><span class="line">print(correlation[<span class="string">'price'</span>].sort_values(ascending=<span class="literal">False</span>), <span class="string">'\n'</span>)   <span class="comment"># 与price相关的特征排序</span></span><br></pre></td></tr></table></figure>
<p><code>.corr()</code>可以看到每个特征与 price 的相关性，并且排了个序。下面进行相关性可视化，使用热力图比较合适。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 热力图可视化</span></span><br><span class="line">f, ax = plt.subplots(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">plt.title(<span class="string">'Correlation of Numeric Features with Price'</span>, y=<span class="number">1</span>, size=<span class="number">16</span>)</span><br><span class="line">sns.heatmap(correlation, square=<span class="literal">True</span>, vmax=<span class="number">0.8</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8LcF3j.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 删除price</span></span><br><span class="line"><span class="keyword">del</span> numeric_train_data[<span class="string">'price'</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">"""查看几个数值特征的偏度和峰度"""</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> numeric_train_data.columns:</span><br><span class="line">     print(<span class="string">'&#123;:15&#125;'</span>.format(col), </span><br><span class="line">          <span class="string">'Skewness: &#123;:05.2f&#125;'</span>.format(numeric_train_data[col].skew()) , </span><br><span class="line">          <span class="string">'   '</span> ,</span><br><span class="line">          <span class="string">'Kurtosis: &#123;:06.2f&#125;'</span>.format(numeric_train_data[col].kurt())  </span><br><span class="line">         )</span><br><span class="line"></span><br><span class="line"><span class="string">"""每个数字特征得分布可视化"""</span></span><br><span class="line">f = pd.melt(train_data, value_vars=numeric_features)</span><br><span class="line">g = sns.FacetGrid(f, col=<span class="string">"variable"</span>,  col_wrap=<span class="number">5</span>, sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>)</span><br><span class="line">g = g.map(sns.distplot, <span class="string">"value"</span>)</span><br></pre></td></tr></table></figure>
<p>数值特征的分布可视化，从这里可以看到数值特征的分布情况，其中匿名特征的分布相对均匀.</p>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8LcgVf.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""数字特征相互之间的关系可视化"""</span></span><br><span class="line">sns.set()</span><br><span class="line">columns = [<span class="string">'price'</span>, <span class="string">'v_12'</span>, <span class="string">'v_8'</span> , <span class="string">'v_0'</span>, <span class="string">'power'</span>, <span class="string">'v_5'</span>,  <span class="string">'v_2'</span>, <span class="string">'v_6'</span>, <span class="string">'v_1'</span>, <span class="string">'v_14'</span>]</span><br><span class="line">sns.pairplot(train_data[columns],size = <span class="number">2</span> ,kind =<span class="string">'scatter'</span>,diag_kind=<span class="string">'kde'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>这里面会看到有些特征之间是相关的， 比如 v_1 和 v_6:</p>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8L2c4S.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""多变量之间的关系可视化"""</span></span><br><span class="line">fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) = plt.subplots(nrows=<span class="number">5</span>, ncols=<span class="number">2</span>, figsize=(<span class="number">24</span>, <span class="number">20</span>))</span><br><span class="line"><span class="comment"># ['v_12', 'v_8' , 'v_0', 'power', 'v_5',  'v_2', 'v_6', 'v_1', 'v_14']</span></span><br><span class="line">v_12_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_12'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_12'</span>,y = <span class="string">'price'</span>, data = v_12_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax1)</span><br><span class="line"></span><br><span class="line">v_8_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_8'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_8'</span>,y = <span class="string">'price'</span>,data = v_8_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax2)</span><br><span class="line"></span><br><span class="line">v_0_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_0'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_0'</span>,y = <span class="string">'price'</span>,data = v_0_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax3)</span><br><span class="line"></span><br><span class="line">power_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'power'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'power'</span>,y = <span class="string">'price'</span>,data = power_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax4)</span><br><span class="line"></span><br><span class="line">v_5_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_5'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_5'</span>,y = <span class="string">'price'</span>,data = v_5_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax5)</span><br><span class="line"></span><br><span class="line">v_2_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_2'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_2'</span>,y = <span class="string">'price'</span>,data = v_2_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax6)</span><br><span class="line"></span><br><span class="line">v_6_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_6'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_6'</span>,y = <span class="string">'price'</span>,data = v_6_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax7)</span><br><span class="line"></span><br><span class="line">v_1_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_1'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_1'</span>,y = <span class="string">'price'</span>,data = v_1_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax8)</span><br><span class="line"></span><br><span class="line">v_14_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_14'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_14'</span>,y = <span class="string">'price'</span>,data = v_14_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax9)</span><br><span class="line"></span><br><span class="line">v_13_scatter_plot = pd.concat([Y_train,train_data[<span class="string">'v_13'</span>]],axis = <span class="number">1</span>)</span><br><span class="line">sns.regplot(x=<span class="string">'v_13'</span>,y = <span class="string">'price'</span>,data = v_13_scatter_plot,scatter= <span class="literal">True</span>, fit_reg=<span class="literal">True</span>, ax=ax10)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8L2Xv9.png" alt></p>
<h1 id="shu-ju-zhi-ming">数据知命</h1>
<p>这里会综合上面的这些过程，用 pandas_profiling 这个包使用函数 ProfileReport 生成一份数据探索性报告， 在这里面会看到：</p>
<ul>
<li>总体的数据信息（首先是数据集信息：变量数 (列)、观察数 (行)、数据缺失率、内存；数据类型的分布情况）</li>
<li>警告信息
<ul>
<li>类型，唯一值，缺失值</li>
<li>分位数统计量，如最小值，Q1，中位数，Q3，最大值，范围，四分位数范围</li>
<li>描述性统计数据，如均值，模式，标准差，总和，中位数绝对偏差，变异系数，峰度，偏度</li>
</ul>
</li>
<li>单变量描述（对每一个变量进行描述）</li>
<li>相关性分析（皮尔逊系数和斯皮尔曼系数）</li>
<li>采样查看等</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 两行简单的代码即可搞定上面的这些信息</span></span><br><span class="line">pfr = ppf.ProfileReport(train_data)</span><br><span class="line">pfr.to_file(<span class="string">"./EDA.html"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/01/machine_learning/exploratory_data_analysis/8LRgVx.png" alt></p>
<h1 id="can-kao">参考</h1>
<ol>
<li>
<p><a href="https://wmathor.com/index.php/archives/1425/" target="_blank" rel="noopener">数据探索性分析</a></p>
</li>
<li>
<p><a href="https://support.minitab.com/zh-cn/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/data-concepts/how-skewness-and-kurtosis-affect-your-distribution/" target="_blank" rel="noopener">偏度和峰度如何影响您的分布</a></p>
</li>
<li>
<p><a href="https://www.cnblogs.com/wyy1480/p/10474046.html" target="_blank" rel="noopener">数据的偏度和峰度——df.skew()、df.kurt()</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>EDA</tag>
      </tags>
  </entry>
  <entry>
    <title>文本分类</title>
    <url>/2020/03/29/text_classification/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/03/29/text_classification/QQ%E6%88%AA%E5%9B%BE20180116105758_%E5%89%AF%E6%9C%AC.png" alt></p>
<a id="more"></a>
<h1 id="xiang-mu-jie-shao">项目介绍</h1>
<ol>
<li>
<p>项目使用深度学习模型进行文本分类，所使用的模型主要包括：FastText，TextCNN，DPCNN，RNN系列(RNN，LSTM，GRU)，RNN-Attention，TextRCNN，HAN，Bert，BertCNN，BertRNN，BertRCNN,XLNet。</p>
</li>
<li>
<p>方法部分对每个模型及其结构给出简要介绍，并附上pytorch代码实现。</p>
</li>
<li>
<p>实验部分所采用的的数据集：weibo_senti_100k情感分类(二分类)，cnews新闻十分类，____文本多标签分类。</p>
</li>
</ol>
<p><strong>数据下载</strong>：微博情感分类数据在<a href="https://github.com/jeffery0628/text_classification" target="_blank" rel="noopener">github仓库</a>中给出, <a href="https://pan.baidu.com/s/1OOTK374IZ1DHnz5COUcfbw" target="_blank" rel="noopener">cnews新闻数据</a>  密码:hf6o, <a href>____文本多标签数据</a></p>
<p><strong>词向量下载</strong>：<a href="https://github.com/Embedding/Chinese-Word-Vectors" target="_blank" rel="noopener">词向量</a></p>
<p><strong>预训练模型下载</strong>：<a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">中文预训练bert模型下载</a>,<a href="https://github.com/ymcui/Chinese-XLNet" target="_blank" rel="noopener">中文预训练XLNet下载</a></p>
<p><strong>项目仓库地址</strong>：<a href="https://github.com/jeffery0628/text_classification" target="_blank" rel="noopener">中文文本分类</a></p>
<p>如出现数学公式乱码以及图片问题，请移步<a href="https://jeffery0628.github.io/" target="_blank" rel="noopener">github.io</a>来获得更好的阅读体验。</p>
<p>最后，欢迎star！</p>
<h1 id="jian-jie">简介</h1>
<p>文本分类在文本处理中是很重要的一个模块，它的应用也非常广泛，比如：新闻分类、简历分类、邮件分类、办公文档分类、区域分类等诸多方面，还能够实现文本过滤，从大量文本中快速识别和过滤出符合特殊要求的信息。。它和其他的分类没有本质的区别，核心方法为首先提取分类数据的特征，然后选择最优的匹配，从而分类。但是文本也有自己的特点，根据文本的特点，文本分类的一般流程为：1.预处理；2.文本表示及特征选择；3.构造分类器；4.分类。</p>
<p>通常来讲，文本分类任务是指在给定的分类体系中，将文本指定分到某个或某几个类别中。被分类的对象有短文本，例如句子、标题、商品评论等等，长文本，如文章等。分类体系一般人工划分，例如：1）政治、体育、军事 2）正能量、负能量 3）好评、中性、差评。此外，还有文本多标签分类，比如一篇博客的标签可以同时是：自然语言处理，文本分类等。因此，对应的分类模式可以分为：二分类、多分类以及多标签分类问题。</p>
<p><img src="/2020/03/29/text_classification/595c46f937d92.png" alt></p>
<ol>
<li>对文本分类的研究可以追溯到二十世纪五十年代，当时主要依据特定的人工规则进行文本分类。</li>
<li>到二十世纪九十年代，统计机器学习 (Statistical machine learning) 成为主流，一些统计机器学习方法，比如支持向量机和朴素贝叶斯等分类方法在文本分类中取得了非常高的分类准确率。然而，统计机器学习方法首先需要进行特征工程工作，该工作需要深入理解业务需求，并且非常耗时耗力。</li>
<li>随着大数据量和图形处理单元强计算力的支持，深度学习近年来发展迅速，与统计机器学习方法相比，深度学习方法可以自动提取特征，使得人们将注意力更多地集中在数据和模型上。</li>
</ol>
<h1 id="fang-fa">方法</h1>
<p>这里我们根据是否使用深度学习方法将文本分类主要分为一下两个大类：</p>
<ul>
<li>基于机器学习的文本分类（不涉及实现以及实验结果的比较）。</li>
<li>基于深度学习的文本分类。</li>
</ul>
<h2 id="ji-yu-ji-qi-xue-xi-de-wen-ben-fen-lei">基于机器学习的文本分类</h2>
<p>90年代后互联网在线文本数量增长和机器学习学科的兴起，逐渐形成了一套解决大规模文本分类问题的经典玩法，这个阶段的主要套路是人工特征工程+浅层分类模型。整个文本分类问题就拆分成了<strong>特征工程</strong>和<strong>分类器</strong>两部分。</p>
<h3 id="te-zheng-gong-cheng">特征工程</h3>
<p>特征工程也就是将文本表示为计算机可以识别的、能够代表该文档特征的特征矩阵的过程。在基于传统机器学习的文本分类中，通常将特征工程分为<strong>文本预处理、特征提取、文本表示</strong>等三个部分。</p>
<h4 id="wen-ben-yu-chu-li">文本预处理</h4>
<p>文本预处理过程是提取文本中的关键词来表示文本的过程。中文文本预处理主要包括文本分词和去停用词两个阶段。文本分词，是因为很多研究表明特征粒度为词粒度远好于字粒度（因为大部分分类算法不考虑词序信息，基于字粒度显然损失了过多<code>n-gram</code>信息）。具体到中文分词，不同于英文有天然的空格间隔，需要设计复杂的分词算法。传统分词算法主要有基于字符串匹配的正向/逆向/双向最大匹配；基于理解的句法和语义分析消歧；基于统计的互信息/CRF方法(<code>WordEmbedding+Bi-LSTM+CRF</code>方法逐渐成为主流)。 而停用词是文本中一些高频的代词、连词、介词等对文本分类无意义的词，通常维护一个停用词表，特征提取过程中删除停用表中出现的词，本质上属于特征选择的一部分。</p>
<h4 id="te-zheng-ti-qu">特征提取</h4>
<p>特征提取包括<strong>特征选择</strong>和<strong>特征权重计算</strong>两部分。 特征选择的基本思路是根据某个评价指标独立的对原始特征项（词项）进行评分排序，从中选择得分最高的一些特征项，过滤掉其余的特征项。常用的评价有：文档频率、互信息、信息增益、χ²统计量等。特征权重计算主要是经典的TF-IDF方法及其扩展方法。</p>
<h4 id="wen-ben-biao-shi">文本表示</h4>
<p>文本表示的目的是把文本预处理后的转换成计算机可理解的方式，是决定文本分类质量最重要的部分。</p>
<p><img src="/2020/03/29/text_classification/image-20200520093625468.png" alt></p>
<h5 id="ci-dai-fa">词袋法</h5>
<p>忽略其词序和语法，句法，将文本仅仅看做是一个词集合。若词集合共有NN个词，每个文本表示为一个<code>N</code>维向量，元素为<code>0/1</code>，表示该文本是否包含对应的词。<code>( 0, 0, 0, 0, .... , 1, ... 0, 0, 0, 0)</code>。一般来说词库量至少都是百万级别，因此词袋模型有个两个最大的问题：高纬度、高稀疏性。</p>
<h5 id="n-gram-ci-dai-mo-xing">n-gram 词袋模型</h5>
<p>与词袋模型类似，考虑了局部的顺序信息，但是向量的维度过大，基本不采用。如果词集合大小为<code>N</code>，则bi-gram的单词总数为\(n^2\)向量空间模型。</p>
<h5 id="xiang-liang-kong-jian-mo-xing">向量空间模型</h5>
<p>以词袋模型为基础，向量空间模型通过特征选择降低维度，通过特征权重计算增加稠密性。</p>
<h3 id="fen-lei-qi">分类器</h3>
<p>大部分机器学习方法都在文本分类领域有所应用，比如朴素贝叶斯分类算法、KNN、SVM、最大熵、GBDT/XGBoost等等。</p>
<h2 id="ji-yu-shen-du-xue-xi-de-wen-ben-fen-lei">基于深度学习的文本分类</h2>
<h3 id="fast-text">FastText</h3>
<h4 id="jian-jie-1">简介</h4>
<p>fastText是Facebook于2016年开源的一个词向量计算和文本分类工具,<a href="https://arxiv.org/pdf/1607.01759.pdf" target="_blank" rel="noopener">论文地址</a>,其<strong>特点</strong>就是<strong>fast</strong>。在文本分类任务中，fastText（浅层网络）往往能取得和深度网络相媲美的精度，却在训练时间上比深度网络快许多数量级。在标准的多核CPU上， 在10分钟之内能够训练10亿词级别语料库的词向量，在1分钟之内能够分类有着30万多类别的50多万句子。</p>
<p>fastText是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：</p>
<ol>
<li>fastText在保持高精度的情况下加快了训练速度和测试速度</li>
<li>fastText不需要预训练好的词向量，fastText会自己训练词向量</li>
<li>fastText两个重要的优化：Hierarchical Softmax、N-gram</li>
</ol>
<h4 id="fast-text-mo-xing-jia-gou">fastText模型架构</h4>
<p>fastText模型架构和word2vec中的CBOW很相似， 不同之处是fastText预测标签而CBOW预测的是中间词，即模型架构类似但是模型的任务不同：</p>
<p><img src="/2020/03/29/text_classification/image-20200520093324964.png" alt></p>
<p>word2vec将上下文关系转化为多分类任务，进而训练逻辑回归模型。通常的文本数据中，词库少则数万，多则百万，在训练中直接训练多分类逻辑回归并不现实。word2vec中提供了两种针对大规模多分类问题的优化手段， negative sampling 和hierarchical softmax。在优化中，negative sampling 只更新少量负面类，从而减轻了计算量。hierarchical softmax 将词库表示成前缀树，从树根到叶子的路径可以表示为一系列二分类器，一次多分类计算的复杂度从|V|降低到了树的高度。<br>
fastText模型架构:其中\(x_1,x_2,\ldots,x_{N−1},x_N\)表示一个文本中的n-gram向量，每个特征是词向量的平均值。</p>
<p><img src="/2020/03/29/text_classification/image-20200520093359191.png" alt></p>
<h4 id="que-dian">缺点：</h4>
<blockquote>
<p>我不喜欢这类电影，但是喜欢这一个。</p>
<p>我喜欢这类电影，但是不喜欢这一个。</p>
</blockquote>
<p><strong>这样的两句句子经过词向量平均以后已经送入单层神经网络的时候已经完全一模一样了，分类器不可能分辨出这两句话的区别</strong>，只有添加n-gram特征以后才可能有区别。因此，在实际应用的时候需要对数据有足够的了解,然后在选择模型。</p>
<h4 id="mo-xing-dai-ma">模型代码</h4>
<p><img src="/2020/03/29/text_classification/FastText_network_structure.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FastText</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, output_dim, word_embedding, freeze)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(self.embedding_size, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,_, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line">        pooled = F.avg_pool2d(embedded, (embedded.shape[<span class="number">1</span>], <span class="number">1</span>)).squeeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># pooled = [batch size, embedding_dim]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(pooled)</span><br></pre></td></tr></table></figure>
<h3 id="text-cnn">TextCNN</h3>
<h4 id="jian-jie-2">简介</h4>
<p><strong>Yoon Kim</strong>在论文<a href="https://arxiv.org/abs/1408.5882" target="_blank" rel="noopener">(2014 EMNLP) Convolutional Neural Networks for Sentence Classification</a>提出TextCNN。将卷积神经网络CNN应用到文本分类任务，利用多个不同size的kernel来提取句子中的关键信息，从而能够更好地捕捉局部相关性。</p>
<h4 id="wang-luo-jie-gou">网络结构</h4>
<p><img src="/2020/03/29/text_classification/textcnn.png" alt></p>
<h4 id="yuan-li">原理</h4>
<p>TextCNN的详细过程原理图如下：</p>
<p><img src="/2020/03/29/text_classification/textcnndetail.png" alt></p>
<p>TextCNN详细过程：</p>
<ul>
<li>Embedding：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点。</li>
<li>Convolution：然后经过 <code>kernel_sizes=(2,3,4) </code>的一维卷积层，每个kernel_size 有两个输出 channel。</li>
<li>MaxPolling：第三层是一个<code>1-max pooling</code>层，这样不同长度句子经过pooling层之后都能变成定长的表示。</li>
<li>FullConnection and Softmax：最后接一层全连接的 softmax 层，输出每个类别的概率。</li>
</ul>
<h4 id="que-dian-1">缺点</h4>
<p>TextCNN模型最大的问题也是这个全局的max pooling丢失了结构信息，因此很难去发现文本中的转折关系等复杂模式。针对这个问题，可以尝试k-max pooling做一些优化，k-max pooling针对每个卷积核都不只保留最大的值，他保留前k个最大值，并且保留这些值出现的顺序，也即按照文本中的位置顺序来排列这k个最大值。在某些比较复杂的文本上相对于1-max pooling会有提升。</p>
<h4 id="dai-ma">代码</h4>
<p><img src="/2020/03/29/text_classification/TextCNN_network_structure.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_filters, filter_sizes, output_dim, dropout, word_embedding, freeze)</span>:</span></span><br><span class="line">        <span class="comment"># n_filter 每个卷积核的个数</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line">        self.convs = nn.ModuleList(</span><br><span class="line">            [nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=n_filters, kernel_size=(fs, self.embedding_size)) <span class="keyword">for</span> fs <span class="keyword">in</span></span><br><span class="line">             filter_sizes])</span><br><span class="line">        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, _, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [batch size, sent len]</span></span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line">        embedded = embedded.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># embedded = [batch size, 1, sent len, emb dim]</span></span><br><span class="line">        conved = [F.relu(conv(embedded)).squeeze(<span class="number">3</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs]</span><br><span class="line">        <span class="comment"># conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]</span></span><br><span class="line">        pooled = [F.max_pool1d(conv, conv.shape[<span class="number">2</span>]).squeeze(<span class="number">2</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> conved]</span><br><span class="line">        <span class="comment"># pooled_n = [batch size, n_filters]</span></span><br><span class="line">        cat = self.dropout(torch.cat(pooled, dim=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># cat = [batch size, n_filters * len(filter_sizes)]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(cat)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN1d</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_filters, filter_sizes, output_dim, dropout, word_embedding, freeze)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line">        self.convs = nn.ModuleList(</span><br><span class="line">            [nn.Conv1d(in_channels=self.embedding_size, out_channels=n_filters, kernel_size=fs) <span class="keyword">for</span> fs <span class="keyword">in</span> filter_sizes])</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, _, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [batch size, sent len]</span></span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line">        embedded = embedded.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># embedded = [batch size, emb dim, sent len]</span></span><br><span class="line">        conved = [F.relu(conv(embedded)) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs]</span><br><span class="line">        <span class="comment"># conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]</span></span><br><span class="line">        pooled = [F.max_pool1d(conv, conv.shape[<span class="number">2</span>]).squeeze(<span class="number">2</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> conved]</span><br><span class="line">        <span class="comment"># pooled_n = [batch size, n_filters]</span></span><br><span class="line">        cat = self.dropout(torch.cat(pooled, dim=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># cat = [batch size, n_filters * len(filter_sizes)]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(cat)</span><br></pre></td></tr></table></figure>
<h3 id="dpcnn">DPCNN</h3>
<h4 id="jian-jie-3">简介：</h4>
<p>ACL2017年中，腾讯AI-lab提出了Deep Pyramid Convolutional Neural Networks for Text Categorization(<a href="https://ai.tencent.com/ailab/media/publications/ACL3-Brady.pdf" target="_blank" rel="noopener">DPCNN</a>)。论文中提出了一种基于word-level级别的网络-DPCNN，由于TextCNN 不能通过卷积获得文本的长距离依赖关系，而论文中DPCNN通过不断加深网络，可以抽取长距离的文本依赖关系。实验证明在不增加太多计算成本的情况下，增加网络深度就可以获得最佳的准确率。‍</p>
<h4 id="wang-luo-jie-gou-1">网络结构</h4>
<p><img src="/2020/03/29/text_classification/DPCNN.jpg" alt></p>
<h5 id="region-embedding">Region embedding</h5>
<p>作者将TextCNN的包含多尺寸卷积滤波器的卷积层的卷积结果称之为<code>Region embedding</code>，意思就是对一个文本区域/片段（比如<code>3-gram</code>）进行一组卷积操作后生成的embedding。<br>
卷积操作有两种选择：</p>
<ol>
<li>保留词序：也就是设置一组<code>size=3*D</code>的二维卷积核对<code>3-gram</code>进行卷积（其中D是word embedding维度）</li>
<li>不保留词序（即使用词袋模型），即首先对<code>3-gram</code>中的3个词的embedding取均值得到一个size=D的向量，然后设置一组size=D的一维卷积核对该<code>3-gram</code>进行卷积。</li>
</ol>
<p>TextCNN里使用的是保留词序的做法，而DPCNN使用的是词袋模型的做法，DPCNN作者认为前者做法更容易造成过拟合，后者的性能却跟前者差不多。</p>
<h4 id="juan-ji-he-quan-lian-jie-de-quan-heng">卷积和全连接的权衡</h4>
<p>产生<code>region embedding</code>后，按照经典的TextCNN的做法的话，就是从每个特征图中挑选出最有代表性的特征，也就是直接应用全局最大池化层，这样就生成了这段文本的特征向量,假如卷积滤波器的size有3，4，5这三种，每种size包含100个卷积核，那么当然就会产生3<em>100幅特征图，然后将max-over-time-pooling操作应用到每个特征图上，于是文本的特征向量即3</em>100=300维。<br>
TextCNN这样做的意义本质上与<code>词袋模型(n-gram)+weighting+NB/MaxEnt/SVM</code>的经典文本分类模型没本质区别，只不过one-hot表示到word embedding表示的转变避免了词袋模型遭遇的数据稀疏问题。TextCNN本质上收益于词向量的引入带来的近义词有相近向量表示的bonus，同时TextCNN可以较好的利用词向量中近义关系。<strong>经典模型里难以学习的远距离信息在TextCNN中依然难以学习</strong>。</p>
<h5 id="deng-chang-juan-ji">等长卷积</h5>
<p>假设输入的序列长度为\(n\)，卷积核大小为\(m\)，步长为\(s\),输入序列两端各填补\(p\)个零,那么该卷积层的输出序列为\(\frac{(n-m+2p)}{s}+1\)。</p>
<ol>
<li>窄卷积:步长\(s=1\),两端不补零，即\(p=0\)，卷积后输出长度为\(n-m+1\)。</li>
<li>宽卷积:步长\(s=1\),两端补零\(p=m-1\)，卷积后输出长度\(n+m-1\)。</li>
<li>等长卷积: 步长\(s=1\),两端补零\(p=(m-1)/2\)，卷积后输出长度为\(n\)。</li>
</ol>
<p>将输入输出序列的第n个embedding称为第n个词位，那么这时size为n的卷积核产生的等长卷积的意义就是将输入序列的每个词位及其左右\(\frac{n-1}{2}\)个词的上下文信息压缩为该词位的embedding，产生了每个词位的被上下文信息修饰过的更高level更加准确的语义。想要克服TextCNN的缺点，捕获长距离模式，显然就要用到深层CNN。</p>
<p>直接等长卷积堆等长卷积会让每个词位包含进去越来越多，越来越长的上下文信息，这种方式会让网络层数变得非常非常非常深，但是这种方式太笨重。不过，既然等长卷积堆等长卷积会让每个词位的embedding描述语义描述的更加丰富准确，可以适当的堆两层来提高词位embedding的表示的丰富性。<br>
<img src="/2020/03/29/text_classification/equal_cnn.png" alt></p>
<h5 id="gu-ding-feature-map-de-shu-liang">固定feature map的数量</h5>
<p>在表示好每个词位的语义后，很多邻接词或者邻接<code>ngram</code>的词义是可以合并，例如“小明 人 不要 太好”中的“不要”和“太好”虽然语义本来离得很远，但是作为邻接词“不要太好”出现时其语义基本等价为“很好”，完全可以把“不要”和“太好”的语义进行合并。同时，合并的过程完全可以在原始的embedding space中进行的，原文中直接把“不要太好”合并为“很好”是很可以的，完全没有必要动整个语义空间。<br>
实际上，相比图像中这种从“点、线、弧”这种low-level特征到“眼睛、鼻子、嘴”这种high-level特征的明显层次性的特征区分，文本中的特征进阶明显要扁平的多，即从单词（1gram）到短语再到3gram、4gram的升级，其实很大程度上均满足“语义取代”的特性。而图像中就很难发生这种“语义取代”现象。因此，DPCNN与ResNet很大一个不同就是，<strong>在DPCNN中固定死了feature map的数量</strong>，也就是固定住了embedding space的维度（为了方便理解，以下简称语义空间），使得网络有可能让整个邻接词（邻接ngram）的合并操作在原始空间或者与原始空间相似的空间中进行（当然，网络在实际中会不会这样做是不一定的，只是提供了这么一种条件）。也就是说，整个网络虽然形状上来看是深层的，但是从语义空间上来看完全可以是扁平的。而ResNet则是不断的改变语义空间，使得图像的语义随着网络层的加深也不断的跳向更高level的语义空间。</p>
<h5 id="strong-chi-hua-strong"><strong>池化</strong></h5>
<p>每经过一个\(size=3,stride=2\)的池化层(简称\(1/2\)池化层)，序列的长度就被压缩成了原来的一半。这样同样是\(size=3\)的卷积核，每经过一个\(1/2\)池化层后，其能感知到的文本片段就比之前长了一倍。例如之前是只能感知3个词位长度的信息，经过1/2池化层后就能感知6个词位长度的信息，这时把1/2池化层和size=3的卷积层组合起来如图：<br>
<img src="/2020/03/29/text_classification/dpcnn_pooling.png" alt></p>
<h5 id="can-chai-lian-jie">残差连接</h5>
<p>在初始化深度CNN时，往往各层权重都是初始化为一个很小的值，这就导致最开始的网络中，后续几乎每层的输入都是接近0，这时网络的输出自然是没意义的，而这些小权重同时也阻碍了梯度的传播，使得网络的初始训练阶段往往要迭代好久才能启动。同时，就算网络启动完成，由于深度网络中仿射矩阵近似连乘，训练过程中网络也非常容易发生梯度爆炸或弥散问题（虽然由于非共享权重，深度CNN网络比RNN网络要好点）。<br>
针对深度CNN网络的梯度弥散问题ResNet中提出的<code>shortcut-connection/skip-connection/residual-connection</code>（残差连接）就是一种非常简单、合理、有效的解决方案。<br>
<img src="/2020/03/29/text_classification/dpcnn_resnet.png" alt><br>
既然每个block的输入在初始阶段容易是0而无法激活，那么直接用一条线把region embedding层连接到每个block的输入乃至最终的池化层/输出层。有了shortcut后，梯度就可以忽略卷积层权重的削弱，从shortcut一路无损的传递到各个block，直至网络前端，从而极大的缓解了梯度消失问题。</p>
<h4 id="dai-ma-1">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DPCNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,num_filters, num_classes,word_embedding, freeze)</span>:</span></span><br><span class="line">        super(DPCNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line"></span><br><span class="line">        self.conv_region = nn.Conv2d(<span class="number">1</span>, num_filters, (<span class="number">3</span>, self.embedding_size), stride=<span class="number">1</span>)</span><br><span class="line">        self.conv = nn.Conv2d(num_filters, num_filters, (<span class="number">3</span>, <span class="number">1</span>), stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.max_pool = nn.MaxPool2d(kernel_size=(<span class="number">3</span>, <span class="number">1</span>), stride=<span class="number">2</span>)</span><br><span class="line">        self.padding1 = nn.ZeroPad2d((<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># top bottom</span></span><br><span class="line">        self.padding2 = nn.ZeroPad2d((<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>))  <span class="comment"># bottom</span></span><br><span class="line"></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.fc = nn.Linear(num_filters, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,_, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text [batch_size,seq_len]</span></span><br><span class="line">        x = self.embedding(text)  <span class="comment"># x=[batch_size,seq_len,embedding_dim]</span></span><br><span class="line">        x = x.unsqueeze(<span class="number">1</span>)  <span class="comment"># [batch_size, 1, seq_len, embedding_dim]</span></span><br><span class="line">        x = self.conv_region(x)  <span class="comment"># x = [batch_size, num_filters, seq_len-3+1, 1]</span></span><br><span class="line"></span><br><span class="line">        x = self.padding1(x)  <span class="comment"># [batch_size, num_filters, seq_len, 1]</span></span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv(x)  <span class="comment"># [batch_size, num_filters, seq_len-3+1, 1]</span></span><br><span class="line">        x = self.padding1(x)  <span class="comment"># [batch_size, num_filters, seq_len, 1]</span></span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv(x)  <span class="comment"># [batch_size, num_filters, seq_len-3+1, 1]</span></span><br><span class="line">        <span class="keyword">while</span> x.size()[<span class="number">2</span>] &gt;= <span class="number">2</span>:</span><br><span class="line">            x = self._block(x)  <span class="comment"># [batch_size, num_filters,1,1]</span></span><br><span class="line">        x = x.squeeze()  <span class="comment"># [batch_size, num_filters]</span></span><br><span class="line">        x = self.fc(x)  <span class="comment"># [batch_size, 1]</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_block</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.padding2(x)</span><br><span class="line">        px = self.max_pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.padding1(px)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line"></span><br><span class="line">        x = self.padding1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Short Cut</span></span><br><span class="line">        x = x + px</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="rnn-xi-lie">RNN系列</h3>
<h4 id="rnn">RNN</h4>
<p>通过将前一时刻的运算结果添加到当前的运算中，从而实现了“考虑上文信息”的功能。</p>
<p><img src="/2020/03/29/text_classification/rnn.png" alt></p>
<p>RNN可以考虑上文的信息，那么如何将下文的信息也添加进去呢？这就是BiRNN要做的事情。</p>
<p><img src="/2020/03/29/text_classification/birnn.png" alt></p>
<h4 id="lstm">LSTM</h4>
<p>因为RNN存在梯度弥散和梯度爆炸的问题，所以RNN很难完美地处理具有长期依赖的信息。既然仅仅依靠一条线连接后面的神经元不足以解决问题，那么就再加一条线(这条线实现的功能是把rnn中的累乘变成了累加)，这就是LSTM。</p>
<p>LSTM的关键在于细胞的状态和穿过细胞的线，细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流动保持不变会变得容易。</p>
<p><img src="/2020/03/29/text_classification/lstm_c.png" alt></p>
<p>在LSTM中，门可以实现选择性的让信息通过，主要通过一个sigmoid的神经层和一个逐点相乘的操作来实现。LSTM通过三个这样的门结构来实现信息的保护和控制，分别是遗忘门（forget gate）、输入门（input gate）与输出门（output gate）。</p>
<h5 id="strong-yi-wang-men-strong"><strong>遗忘门</strong></h5>
<p>在LSTM中的第一步是决定从细胞状态中丢弃什么信息。这个决定通过一个称为遗忘门的结构来完成。遗忘门会读取\(h_{t-1}\)和\(x_t\)，输出一个0到1之间的数值给细胞的状态\(c_{t-1}\)中的数字。1表示完全保留，0表示完全舍弃。</p>
<p><img src="/2020/03/29/text_classification/lstm_forget_gate.png" alt></p>
<h5 id="strong-shu-ru-men-strong"><strong>输入门</strong></h5>
<p>遗忘门决定让多少新的信息加入到cell的状态中来。实现这个需要两个步骤：</p>
<ol>
<li>
<p>首先一个叫“input gate layer”的sigmoid层决定哪些信息需要更新；一个tanh层生成一个向量，用来更新状态C。</p>
<p><img src="/2020/03/29/text_classification/input_gate.png" alt></p>
</li>
<li>
<p>把 1 中的两部分联合起来，对cell的状态进行更新，我们把旧的状态与\(f_t\)相乘，丢弃掉我们确定需要丢弃的信息，接着加上\(i_t * \tilde{C}_{t}\)</p>
</li>
</ol>
<h5 id="shu-chu-men">输出门</h5>
<p>最终，我们需要确定输出什么值，这个输出将会基于我们的细胞的状态，但是也是一个过滤后的版本。首先，我们通过一个sigmoid层来确定细胞状态的哪些部分将输出出去。接着，我们把细胞状态通过tanh进行处理（得到一个-1到1之间的值）并将它和sigmoid门的输出相乘，最终我们仅仅会输出我们确定输出的部分。</p>
<p><img src="/2020/03/29/text_classification/output_gate.png" alt></p>
<h5 id="gong-shi">公式</h5>
<p><img src="/2020/03/29/text_classification/lstm_all.png" alt></p>
<h4 id="gru">GRU</h4>
<p>在LSTM中引入了三个门函数：输入门、遗忘门和输出门来控制输入值、记忆值和输出值。而在GRU模型中只有两个门：分别是更新门和重置门。</p>
<p><img src="/2020/03/29/text_classification/GRU.png" alt></p>
<p>图中的\(z_t\)和\(r_t\)分别表示更新门和重置门。更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多。重置门控制前一状态有多少信息被写入到当前的候选集 \(\tilde{h}_{t}\)上，重置门越小，前一状态的信息被写入的越少。</p>
<p>LSTM和CRU都是通过各种门函数来将重要特征保留下来，这样就保证了在long-term传播的时候也不会丢失。此外GRU相对于LSTM少了一个门函数，因此在参数的数量上也是要少于LSTM的，所以整体上GRU的训练速度要快于LSTM的。</p>
<h4 id="dai-ma-2">代码</h4>
<p><img src="/2020/03/29/text_classification/TextBiRNN_network_structure.png" alt="TextBiRNN_network_structure"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RnnModel</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, hidden_dim, output_dim, n_layers,bidirectional, dropout,word_embedding, freeze,batch_first=True)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.embedding_size,</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.embedding_size,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.embedding_size,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(hidden_dim * n_layers, output_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,_, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># 按照句子长度从大到小排序</span></span><br><span class="line">        text, sorted_seq_lengths, desorted_indices = prepare_pack_padded_sequence(text, text_lengths)</span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        embedded = self.dropout(self.embedding(text))</span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_seq_lengths, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># output (seq_len, batch, num_directions * hidden_size)</span></span><br><span class="line">            <span class="comment"># hidden (num_layers * num_directions, batch, hidden_size)</span></span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line">        <span class="comment"># 把句子序列再调整成输入时的顺序</span></span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        <span class="comment"># output = [batch_size,seq_len,hidden_dim * num_directionns ]</span></span><br><span class="line">        batch_size, max_seq_len,hidden_dim = output.shape</span><br><span class="line">        hidden = torch.mean(torch.reshape(hidden,[batch_size,<span class="number">-1</span>,hidden_dim]),dim=<span class="number">1</span>)</span><br><span class="line">        output = torch.sum(output,dim=<span class="number">1</span>)</span><br><span class="line">        fc_input = self.dropout(output + hidden)</span><br><span class="line">        out = self.fc(fc_input)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h4 id="self-attention">Self-Attention</h4>
<p><img src="/2020/03/29/text_classification/self-attention.png" alt></p>
<ol>
<li>Encode所有输入序列,得到对应的\(h_1,h_2, \cdots ,h_T\)(T为输入序列长度)</li>
<li>Decode输出目标\(y_t\)之前，会将上一步输出的隐藏状态\(S_{t-1}\)与之前encode好的\(h_1,h_2,\cdots,h_T\)进行比对，计算相似度（\(e_{t,j}=a(s_{t-1},h_j)\)）,\(h_j\)为之前第j个输入encode得到的隐藏向量，a为任意一种计算相似度的方式</li>
<li>然后通过softmax，即\(a_{t,j}=\frac{exp(e_{t,j})}{\sum^{T_x}_{k=1}exp(e_{t,k})}\)将之前得到的各个部分的相关系数进行归一化，得到\(a_{t,1},a_{t,2},\cdots,a_{t,T}\)</li>
<li>在对输入序列的隐藏层进行相关性加权求和得到此时decode需要的context vector ：</li>
</ol>
<h4 id="rnn-attenton">Rnn-Attenton</h4>
<p><img src="/2020/03/29/text_classification/TextAttBiRNN_network_structure.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RnnAttentionModel</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, hidden_dim, output_dim, n_layers,bidirectional, dropout,word_embedding, freeze, batch_first=True)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.embedding_size,</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.embedding_size,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.embedding_size,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        self.tanh1 = nn.Tanh()</span><br><span class="line">        self.tanh2 = nn.Tanh()</span><br><span class="line">        <span class="comment"># self.u = nn.Parameter(torch.Tensor(self.hidden_dim * 2,self.hidden_dim*2))</span></span><br><span class="line">        self.w = nn.Parameter(torch.randn(hidden_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="keyword">if</span> bidirectional:</span><br><span class="line">            self.w = nn.Parameter(torch.randn(hidden_dim * <span class="number">2</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">            self.fc = nn.Linear(hidden_dim * <span class="number">2</span>, output_dim)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.w = nn.Parameter(torch.randn(hidden_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line">            self.fc = nn.Linear(hidden_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,_, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># 按照句子长度从大到小排序</span></span><br><span class="line">        text, sorted_seq_lengths, desorted_indices = prepare_pack_padded_sequence(text, text_lengths)</span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        embedded = self.dropout(self.embedding(text))</span><br><span class="line">        <span class="comment"># embedded = [batch size,sent len,  emb dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_seq_lengths, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        <span class="comment"># output = [sent len, batch size, hidden dim * num_direction]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line">        <span class="comment"># 把句子序列再调整成输入时的顺序</span></span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        <span class="comment"># attention</span></span><br><span class="line">        <span class="comment"># M = [sent len, batch size, hidden dim * num_direction]</span></span><br><span class="line">        <span class="comment"># M = self.tanh1(output)</span></span><br><span class="line">        alpha = F.softmax(torch.matmul(self.tanh1(output), self.w), dim=<span class="number">0</span>).unsqueeze(<span class="number">-1</span>)  <span class="comment"># dim=0表示针对文本中的每个词的输出softmax</span></span><br><span class="line">        output_attention = output * alpha</span><br><span class="line"></span><br><span class="line">        batch_size, max_seq_len,hidden_dim = output.shape</span><br><span class="line">        hidden = torch.mean(torch.reshape(hidden,[batch_size,<span class="number">-1</span>,hidden_dim]),dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        output_attention = torch.sum(output_attention, dim=<span class="number">1</span>)</span><br><span class="line">        output = torch.sum(output, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        fc_input = self.dropout(output + output_attention + hidden)</span><br><span class="line">        <span class="comment"># fc_input = self.dropout(output_attention)</span></span><br><span class="line">        out = self.fc(fc_input)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h3 id="text-rcnn">TextRCNN</h3>
<h4 id="jian-jie-4">简介</h4>
<p>RNN和CNN作为文本分类问题的主要模型架构，都存在各自的优点及局限性。RNN擅长处理序列结构，能够考虑到句子的上下文信息，但RNN属于“biased model”，一个句子中越往后的词重要性越高，这有可能影响最后的分类结果，因为对句子分类影响最大的词可能处在句子任何位置。CNN属于无偏模型，能够通过最大池化获得最重要的特征，但是CNN的滑动窗口大小不容易确定，选的过小容易造成重要信息丢失，选的过大会造成巨大参数空间。为了解决二者的局限性，<a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552" target="_blank" rel="noopener">RCNN</a>这篇文章提出了一种新的网络架构，用双向循环结构获取上下文信息，这比传统的基于窗口的神经网络更能减少噪声，而且在学习文本表达时可以大范围的保留词序。其次使用最大池化层获取文本的重要部分，自动判断哪个特征在文本分类过程中起更重要的作用。</p>
<h4 id="mo-xing-jie-gou">模型结构</h4>
<p><img src="/2020/03/29/text_classification/rcnn.png" alt></p>
<h4 id="word-representation-learning">Word Representation Learning</h4>
<p>作者提出将单词的左上下文、右上下文、单词本身结合起来作为单词表示。作者使用了双向RNN来分别提取句子的上下文信息。公式如下:<br>
\[
\begin{array}{l}
c_{l}\left(w_{i}\right)=f\left(W^{(l)} c_{l}\left(w_{i-1}\right)+W^{(s l)} e\left(w_{i-1}\right)\right)  \\
c_{r}\left(w_{i}\right)=f\left(W^{(r)} c_{r}\left(w_{i+1}\right)+W^{(s r)} e\left(w_{i+1}\right)\right)
\end{array}
\]<br>
其中，\(c_l(w_i)\)代表单词\(w_i\)的左上下文，\(c_l(w_i)\)由上一个单词的左上下文\(c_l\)和\(c_l(w_{i-1})\)上一个单词的词嵌入向量 \(e(w_{i-1})\)计算得到，如公式（1）所示，所有句子第一个单词的左侧上下文使用相同的共享参数\(c_l(w_1)\)。 \(W^{(l)},W^{(sl)}\)用于将上一个单词的左上下文语义和上一个单词的语义结合到单词 \(w_i\)的左上下文表示中。右上下文的处理与左上下文完全相同，同样所有句子最后一个单词的右侧上下文使用相同的共享参数\(c_r(w_n)\)。 得到句子中每个单词的左上下文表示和右上下文表示后，就可以定义单词  \(w_i\)的表示如下<br>
\[
\boldsymbol{x}_{i}=\left[\boldsymbol{c}_{l}\left(w_{i}\right) ; \boldsymbol{e}\left(w_{i}\right) ; \boldsymbol{c}_{r}\left(w_{i}\right)\right]
\]</p>
<p>实际就是单词\(w_i\)，单词的词嵌入表示向量 \(e(w_i)\)以及单词的右上下文向量\(c_e(w_i)\) 的拼接后的结果。得到\(w_i\)的表示\(x_i\)后，就可以输入激活函数得到\(w_i\)的潜在语义向量 \(y_i^{(2)}\) 。<br>
\[
\boldsymbol{y}_{i}^{(2)}=\tanh \left(W^{(2)} \boldsymbol{x}_{i}+\boldsymbol{b}^{(2)}\right)
\]</p>
<h4 id="text-representation-learning">Text Representation Learning</h4>
<p>经过卷积层后，获得了所有词的表示，首先对其进行最大池化操作，最大池化可以帮助找到句子中最重要的潜在语义信息。<br>
\[
\boldsymbol{y}^{(3)}=\max _{i=1}^{n} \boldsymbol{y}_{i}^{(2)}
\]<br>
然后经过全连接层得到文本的表示，最后通过softmax层进行分类。<br>
\[
\begin{aligned}
&amp;\boldsymbol{y}^{(4)}=W^{(4)} \boldsymbol{y}^{(3)}+\boldsymbol{b}^{(4)}\\
&amp;p_{i}=\frac{\exp \left(\boldsymbol{y}_{i}^{(4)}\right)}{\sum_{k=1}^{n} \exp \left(\boldsymbol{y}_{k}^{(4)}\right)}
\end{aligned}
\]</p>
<h4 id="dai-ma-3">代码</h4>
<p><img src="/2020/03/29/text_classification/RCNN_network_structure.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RCNNModel</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, hidden_dim, output_dim, n_layers,bidirectional, dropout,word_embedding, freeze, batch_first=True)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.embedding_size ,</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.embedding_size ,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.embedding_size ,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.fc_cat = nn.Linear(hidden_dim * n_layers + self.embedding_size,self.embedding_size)</span><br><span class="line">        self.fc = nn.Linear(self.embedding_size, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,_, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># 按照句子长度从大到小排序</span></span><br><span class="line">        text, sorted_seq_lengths, desorted_indices = prepare_pack_padded_sequence(text, text_lengths)</span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        embedded = self.dropout(self.embedding(text))</span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_seq_lengths, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># packed_output</span></span><br><span class="line">        <span class="comment"># hidden [n_layers * bi_direction,batch_size,hidden_dim]</span></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        <span class="comment"># output [sent len, batch_size * n_layers * bi_direction]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line">        <span class="comment"># 把句子序列再调整成输入时的顺序</span></span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        <span class="comment"># output = [batch_size,seq_len,hidden_dim * num_directionns ]</span></span><br><span class="line">        batch_size, max_seq_len,hidden_dim = output.shape</span><br><span class="line">        <span class="comment"># 拼接左右上下文信息</span></span><br><span class="line">        output = torch.tanh(self.fc_cat(torch.cat((output, embedded), dim=<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">        output = torch.transpose(output,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">        output = F.max_pool1d(output,max_seq_len).squeeze().contiguous()</span><br><span class="line">        output = self.fc(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h3 id="han">HAN</h3>
<p><img src="/2020/03/29/text_classification/HAN.png" alt></p>
<p>整个网络结构包括五个部分：</p>
<ol>
<li>词序列编码器</li>
<li>基于词级的注意力层</li>
<li>句子编码器</li>
<li>基于句子级的注意力层</li>
<li>分类</li>
</ol>
<p>整个网络结构由双向GRU网络和注意力机制组合而成。</p>
<h4 id="ci-xu-lie-bian-ma-qi">词序列编码器</h4>
<p>给定一个句子中的单词\(w_{it}\)，其中 \(i\) 表示第\(i\) 个句子，\(t\) 表示第 \(t\) 个词。通过一个词嵌入矩阵 \(W_e\) 将单词转换成向量表示，具体如下所示：<br>
\[
x_{it}=W_e w_{it}
\]</p>
<p>利用双向GRU实现的整个编码流程：<br>
\[
\begin{aligned}
x_{i t} &amp;=W_{e} w_{i t}, t \in[1, T] \\
\overrightarrow{h}_{i t} &amp;=\overrightarrow{\operatorname{GRU}}\left(x_{i t}\right), t \in[1, T] \\
\overleftarrow{h}_{i t} &amp;=\overleftarrow{\operatorname{GRU}}\left(x_{i t}\right), t \in[T, 1] \\
{h}_{i t} &amp;= [\overrightarrow{h}_{i t},\overleftarrow{h}_{i t} ]
\end{aligned}
\]</p>
<h4 id="ci-ji-de-zhu-yi-li-ceng">词级的注意力层</h4>
<p>但是对于一句话中的单词，并不是每一个单词对分类任务都是有用的，比如在做文本的情绪分类时，可能我们就会比较关注“很好”、“伤感”这些词。为了能使循环神经网络也能自动将“注意力”放在这些词汇上，作者设计了基于单词的注意力层的具体流程如下：<br>
\[
\begin{aligned}
u_{i t} &amp;=\tanh \left(W_{w} h_{i t}+b_{w}\right) \\
\alpha_{i t} &amp;=\frac{\exp \left(u_{i t}^{\top} u_{w}\right)}{\sum_{t} \exp \left(u_{i t}^{\top} u_{w}\right)} \\
s_{i} &amp;=\sum_{t} \alpha_{i t} h_{i t}
\end{aligned}
\]<br>
上面式子中，\(u_{it}\) 是 \(h_{it}\) 的隐层表示，\(a_{it}\) 是经 softmax 函数处理后的归一化权重系数，\(u_w\)是一个随机初始化的向量，之后会作为模型的参数一起被训练，\(s_i\) 就是我们得到的第 i 个句子的向量表示。</p>
<h4 id="ju-zi-bian-ma-qi">句子编码器</h4>
<p>句子编码器也是基于双向GRU实现编码的，<br>
\[
\begin{aligned}
&amp;\overrightarrow{h}_{i}=\overrightarrow{\operatorname{GRU}}\left(s_{i}\right), i \in[1, L]\\
&amp;\overleftarrow{h}_{i}=\overleftarrow{\operatorname{GRU}}\left(s_{i}\right), t \in[L, 1]
\end{aligned}
\]<br>
公式和词编码类似，最后的 \(h_i\) 也是通过拼接得到的.</p>
<h4 id="ju-zi-ji-zhu-yi-li-ceng">句子级注意力层</h4>
<p>注意力层的流程如下，和词级的一致:<br>
\[
\begin{aligned}
u_{i} &amp;=\tanh \left(W_{s} h_{i}+b_{s}\right) \\
\alpha_{i} &amp;=\frac{\exp \left(u_{i}^{\top} u_{s}\right)}{\sum_{i} \exp \left(u_{i}^{\top} u_{s}\right)} \\
v &amp;=\sum_{i} \alpha_{i} h_{i}
\end{aligned}
\]<br>
最后得到的向量\(v\) 就是文档的向量表示，这是文档的高层表示。接下来就可以用可以用这个向量表示作为文档的特征。</p>
<h4 id="fen-lei">分类</h4>
<p>使用最常用的softmax分类器对整个文本进行分类了<br>
\[
p=\operatorname{softmax}\left(W_{c} v+b_{c}\right)
\]<br>
损失函数<br>
\[
L=-\sum_{d} \log p_{d j}
\]</p>
<h4 id="dai-ma-4">代码</h4>
<p><img src="/2020/03/29/text_classification/HAN_network_structure.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HierAttNet</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,rnn_type, word_hidden_size, sent_hidden_size, num_classes, word_embedding,n_layers,bidirectional,batch_first,freeze,dropout)</span>:</span></span><br><span class="line">        super(HierAttNet, self).__init__()</span><br><span class="line">        self.word_embedding = word_embedding</span><br><span class="line">        self.word_hidden_size = word_hidden_size</span><br><span class="line">        self.sent_hidden_size = sent_hidden_size</span><br><span class="line">        self.word_att_net = WordAttNet(rnn_type,word_embedding, word_hidden_size,n_layers,bidirectional,batch_first,dropout,freeze)</span><br><span class="line">        self.sent_att_net = SentAttNet(rnn_type,sent_hidden_size, word_hidden_size,n_layers,bidirectional,batch_first,dropout, num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, batch_doc, _, text_lengths)</span>:</span></span><br><span class="line">        output_list = []</span><br><span class="line">        <span class="comment"># ############################ 词级 #########################################</span></span><br><span class="line">        <span class="keyword">for</span> idx,doc <span class="keyword">in</span> enumerate(batch_doc):</span><br><span class="line">            <span class="comment"># 把一篇文档拆成多个句子</span></span><br><span class="line">            doc = doc[:text_lengths[idx]]</span><br><span class="line">            doc_list = doc.cpu().numpy().tolist()</span><br><span class="line">            sep_index = [i <span class="keyword">for</span> i, num <span class="keyword">in</span> enumerate(doc_list) <span class="keyword">if</span> num == self.word_embedding.stoi[<span class="string">'[SEP]'</span>]]</span><br><span class="line">            sentence_list = []</span><br><span class="line">            <span class="keyword">if</span> sep_index:</span><br><span class="line">                pre = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> cur <span class="keyword">in</span> sep_index:</span><br><span class="line">                    sentence_list.append(doc_list[pre:cur])</span><br><span class="line">                    pre = cur</span><br><span class="line"></span><br><span class="line">                sentence_list.append(doc_list[cur:])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sentence_list.append(doc_list)</span><br><span class="line">            max_sentence_len = len(max(sentence_list,key=<span class="keyword">lambda</span> x:len(x)))</span><br><span class="line">            seq_lens = []</span><br><span class="line">            input_token_ids = []</span><br><span class="line">            <span class="keyword">for</span> sent <span class="keyword">in</span> sentence_list:</span><br><span class="line">                cur_sent_len = len(sent)</span><br><span class="line">                seq_lens.append(cur_sent_len)</span><br><span class="line">                input_token_ids.append(sent+[self.word_embedding.stoi[<span class="string">'PAD'</span>]]*(max_sentence_len-cur_sent_len))</span><br><span class="line">            input_token_ids = torch.LongTensor(np.array(input_token_ids)).to(batch_doc.device)</span><br><span class="line">            seq_lens = torch.LongTensor(np.array(seq_lens)).to(batch_doc.device)</span><br><span class="line">            word_output, hidden = self.word_att_net(input_token_ids,seq_lens)</span><br><span class="line">            <span class="comment"># word_output = [bs,hidden_size]</span></span><br><span class="line">            output_list.append(word_output)</span><br><span class="line"></span><br><span class="line">        max_doc_sent_num = len(max(output_list,key=<span class="keyword">lambda</span> x: len(x)))</span><br><span class="line">        batch_sent_lens = []</span><br><span class="line">        batch_sent_inputs = []</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ############################ 句子级 #########################################</span></span><br><span class="line">        <span class="keyword">for</span> doc <span class="keyword">in</span> output_list:</span><br><span class="line">            cur_doc_sent_len = len(doc)</span><br><span class="line">            batch_sent_lens.append(cur_doc_sent_len)</span><br><span class="line">            expand_doc = torch.cat([doc,torch.zeros(size=((max_doc_sent_num-cur_doc_sent_len),len(doc[<span class="number">0</span>]))).to(doc.device)],dim=<span class="number">0</span>)</span><br><span class="line">            batch_sent_inputs.append(expand_doc.unsqueeze(dim=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        batch_sent_inputs = torch.cat(batch_sent_inputs, <span class="number">0</span>)</span><br><span class="line">        batch_sent_lens = torch.LongTensor(np.array(batch_sent_lens)).to(doc.device)</span><br><span class="line">        output = self.sent_att_net(batch_sent_inputs,batch_sent_lens)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h3 id="bert">Bert</h3>
<h4 id="bert-1">BERT</h4>
<p><img src="/2020/03/29/text_classification/bert_gpt_elmo.png" alt></p>
<h5 id="task-1-mlm">Task 1: MLM</h5>
<p>由于BERT需要通过上下文信息，来预测中心词的信息，同时又不希望模型提前看见中心词的信息，因此提出了一种 Masked Language Model 的预训练方式，即随机从输入预料上 mask 掉一些单词，然后通过的上下文预测该单词，类似于一个完形填空任务。</p>
<p>在预训练任务中，15%的 Word Piece 会被mask，这15%的 Word Piece 中，80%的时候会直接替换为 [Mask] ，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token</p>
<ul>
<li>没有100%mask的原因
<ul>
<li>如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词</li>
</ul>
</li>
<li>加入10%随机token的原因
<ul>
<li>Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘</li>
<li>另外编码器不知道哪些词需要预测的，哪些词是错误的，因此被迫需要学习每一个token的表示向量</li>
</ul>
</li>
<li>另外，每个batchsize只有15%的单词被mask的原因，是因为性能开销的问题，双向编码器比单项编码器训练要更慢</li>
</ul>
<h5 id="task-2-nsp">Task 2: NSP</h5>
<p>仅仅一个MLM任务是不足以让 BERT 解决阅读理解等句子关系判断任务的，因此添加了额外的一个预训练任务，即 Next Sequence Prediction。</p>
<p>具体任务即为一个句子关系判断任务，即判断句子B是否是句子A的下文，如果是的话输出’IsNext‘，否则输出’NotNext‘。</p>
<p>训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在图4中的[CLS]符号中</p>
<h5 id="shu-ru">输入</h5>
<ul>
<li>Token Embeddings：即传统的词向量层，每个输入样本的首字符需要设置为[CLS]，可以用于之后的分类任务，若有两个不同的句子，需要用[SEP]分隔，且最后一个字符需要用[SEP]表示终止</li>
<li>Segment Embeddings：为[0,1][0,1]序列，用来在NSP任务中区别两个句子，便于做句子关系判断任务</li>
<li>Position Embeddings：与Transformer中的位置向量不同，BERT中的位置向量是直接训练出来的</li>
</ul>
<h5 id="fine-tunninng">Fine-tunninng</h5>
<p>对于不同的下游任务，我们仅需要对BERT不同位置的输出进行处理即可，或者直接将BERT不同位置的输出直接输入到下游模型当中。具体的如下所示：</p>
<ul>
<li>对于情感分析等单句分类任务，可以直接输入单个句子（不需要[SEP]分隔双句），将[CLS]的输出直接输入到分类器进行分类</li>
<li>对于句子对任务（句子关系判断任务），需要用[SEP]分隔两个句子输入到模型中，然后同样仅须将[CLS]的输出送到分类器进行分类</li>
<li>对于问答任务，将问题与答案拼接输入到BERT模型中，然后将答案位置的输出向量进行二分类并在句子方向上进行softmax（只需预测开始和结束位置即可）</li>
<li>对于命名实体识别任务，对每个位置的输出进行分类即可，如果将每个位置的输出作为特征输入到CRF将取得更好的效果。</li>
</ul>
<h5 id="que-dian-2">缺点</h5>
<ul>
<li>BERT的预训练任务MLM使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务</li>
<li>另外，BERT没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计</li>
<li>由于最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）</li>
<li>适合处理自然语义理解类任务(NLU)，而不适合自然语言生成类任务(NLG)</li>
</ul>
<h5 id="dai-ma-5">代码</h5>
<h6 id="bert-2">Bert</h6>
<figure class="highlight python"><figcaption><span>bert</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bert</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bert_path, num_classes, word_embedding, trained=True)</span>:</span></span><br><span class="line">        super(Bert, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="comment"># 不对bert进行训练</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.bert.parameters():</span><br><span class="line">            param.requires_grad = trained</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>], num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, context, bert_masks, seq_lens)</span>:</span></span><br><span class="line">        <span class="comment"># context  输入的句子序列</span></span><br><span class="line">        <span class="comment"># seq_len  句子长度</span></span><br><span class="line">        <span class="comment"># mask     对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        <span class="comment"># cls [batch_size, 768]</span></span><br><span class="line">        <span class="comment"># sentence [batch size,sen len,  768]</span></span><br><span class="line">        sentence, cls = self.bert(context, attention_mask=bert_masks)</span><br><span class="line">        sentence = torch.sum(sentence,dim=<span class="number">1</span>)</span><br><span class="line">        out = self.fc(sentence)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h6 id="bert-cnn">BertCNN</h6>
<figure class="highlight python"><figcaption><span>BertCNN</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertCNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bert_path, num_filters, hidden_size, filter_sizes, dropout, num_classes, word_embedding,</span></span></span><br><span class="line"><span class="function"><span class="params">                 trained=True)</span>:</span></span><br><span class="line">        super(BertCNN, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.bert.parameters():</span><br><span class="line">            param.requires_grad = trained</span><br><span class="line">        self.convs = nn.ModuleList(</span><br><span class="line">            [nn.Conv2d(<span class="number">1</span>, num_filters, (k, hidden_size)) <span class="keyword">for</span> k <span class="keyword">in</span> filter_sizes])</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.fc_cnn = nn.Linear(num_filters * len(filter_sizes), num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conv_and_pool</span><span class="params">(self, x, conv)</span>:</span></span><br><span class="line">        x = F.relu(conv(x)).squeeze(<span class="number">3</span>)</span><br><span class="line">        x = F.max_pool1d(x, x.size(<span class="number">2</span>)).squeeze(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, context, bert_masks, seq_len)</span>:</span></span><br><span class="line">        <span class="comment"># context    输入的句子</span></span><br><span class="line">        <span class="comment"># mask  对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        encoder_out, text_cls = self.bert(context, attention_mask=bert_masks)</span><br><span class="line">        out = encoder_out.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        out = torch.cat([self.conv_and_pool(out, conv) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs], <span class="number">1</span>)</span><br><span class="line">        out = self.dropout(out)</span><br><span class="line">        out = self.fc_cnn(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h6 id="bert-rnn">BertRNN</h6>
<figure class="highlight python"><figcaption><span>BertRNN</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, bert_path, hidden_dim, n_layers, bidirectional, batch_first, word_embedding,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout, num_classes, trained)</span>:</span></span><br><span class="line">        super(BertRNN, self).__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.bert.parameters():</span><br><span class="line">            param.requires_grad = trained</span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                               hidden_size=hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.fc_rnn = nn.Linear(hidden_dim * <span class="number">2</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, bert_masks, seq_lens)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        <span class="comment"># context    输入的句子</span></span><br><span class="line">        <span class="comment"># mask  对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        bert_sentence, bert_cls = self.bert(text, attention_mask=bert_masks)</span><br><span class="line">        sentence_len = bert_sentence.shape[<span class="number">1</span>]</span><br><span class="line">        bert_cls = bert_cls.unsqueeze(dim=<span class="number">1</span>).repeat(<span class="number">1</span>, sentence_len, <span class="number">1</span>)</span><br><span class="line">        bert_sentence = bert_sentence + bert_cls</span><br><span class="line">        <span class="comment"># 按照句子长度从大到小排序</span></span><br><span class="line">        bert_sentence, sorted_seq_lengths, desorted_indices = prepare_pack_padded_sequence(bert_sentence, seq_lens)</span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(bert_sentence, sorted_seq_lengths,</span><br><span class="line">                                                            batch_first=self.batch_first)</span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output = [ batch size,sent len, hidden_dim * bidirectional]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        batch_size, max_seq_len, hidden_dim = output.shape</span><br><span class="line">        hidden = torch.mean(torch.reshape(hidden, [batch_size, <span class="number">-1</span>, hidden_dim]), dim=<span class="number">1</span>)</span><br><span class="line">        output = torch.sum(output, dim=<span class="number">1</span>)</span><br><span class="line">        fc_input = self.dropout(output + hidden)</span><br><span class="line">        out = self.fc_rnn(fc_input)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h6 id="bert-rcnn">BertRCNN</h6>
<figure class="highlight python"><figcaption><span>BertRCNN</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertRCNN</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, bert_path, hidden_dim, n_layers, bidirectional, dropout, num_classes, word_embedding,</span></span></span><br><span class="line"><span class="function"><span class="params">                 trained, batch_first=True)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.bert.parameters():</span><br><span class="line">            param.requires_grad = trained</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.maxpool = nn.MaxPool1d()</span></span><br><span class="line">        self.fc = nn.Linear(hidden_dim * n_layers, num_classes)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, bert_masks, seq_lens)</span>:</span></span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        <span class="comment"># context    输入的句子</span></span><br><span class="line">        <span class="comment"># mask  对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        bert_sentence, bert_cls = self.bert(text, attention_mask=bert_masks)</span><br><span class="line">        sentence_len = bert_sentence.shape[<span class="number">1</span>]</span><br><span class="line">        bert_cls = bert_cls.unsqueeze(dim=<span class="number">1</span>).repeat(<span class="number">1</span>, sentence_len, <span class="number">1</span>)</span><br><span class="line">        bert_sentence = bert_sentence + bert_cls</span><br><span class="line">        <span class="comment"># 按照句子长度从大到小排序</span></span><br><span class="line">        bert_sentence, sorted_seq_lengths, desorted_indices = prepare_pack_padded_sequence(bert_sentence, seq_lens)</span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(bert_sentence, sorted_seq_lengths,</span><br><span class="line">                                                            batch_first=self.batch_first)</span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        batch_size, max_seq_len, hidden_dim = output.shape</span><br><span class="line">        out = torch.transpose(output.relu(), <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        out = F.max_pool1d(out, max_seq_len).squeeze()</span><br><span class="line">        out = self.fc(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h6 id="xlnet">xlnet</h6>
<figure class="highlight python"><figcaption><span>xlnet</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XLNet</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, xlnet_path, num_classes, word_embedding, trained=True)</span>:</span></span><br><span class="line">        super(XLNet, self).__init__()</span><br><span class="line">        self.xlnet = XLNetModel.from_pretrained(xlnet_path)</span><br><span class="line">        <span class="comment"># 不对bert进行训练</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.xlnet.parameters():</span><br><span class="line">            param.requires_grad = trained</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(self.xlnet.d_model, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, context, xlnet_masks, seq_lens)</span>:</span></span><br><span class="line">        <span class="comment"># context  输入的句子序列</span></span><br><span class="line">        <span class="comment"># seq_len  句子长度</span></span><br><span class="line">        <span class="comment"># mask     对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        <span class="comment"># cls [batch_size, 768]</span></span><br><span class="line">        <span class="comment"># sentence [batch size,sen len,  768]</span></span><br><span class="line">        sentence_encoder = self.xlnet(context, attention_mask=xlnet_masks)</span><br><span class="line">        sentence_encoder = torch.sum(sentence_encoder[<span class="number">0</span>], dim=<span class="number">1</span>)</span><br><span class="line">        out = self.fc(sentence_encoder)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h4 id="bert-config-json">bert_config.json</h4>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "attention_probs_dropout_prob": 0.1, #乘法attention时，softmax后dropout概率 </span><br><span class="line">  "directionality": "bidi", </span><br><span class="line">  "hidden_act": "gelu",   #激活函数 </span><br><span class="line">  "hidden_dropout_prob": 0.1, #隐藏层dropout概率 </span><br><span class="line">  "hidden_size": 768, #隐藏单元数 </span><br><span class="line">  "initializer_range": 0.02, #初始化范围 </span><br><span class="line">  "intermediate_size": 3072, #升维维度</span><br><span class="line">  "max_position_embeddings": 512, #一个大于seq_length的参数，用于生成position_embedding</span><br><span class="line">  "num_attention_heads": 12,#每个隐藏层中的attention head数 </span><br><span class="line">  "num_hidden_layers": 2, #隐藏层数 </span><br><span class="line">  "pooler_fc_size": 768, </span><br><span class="line">  "pooler_num_attention_heads": 12, </span><br><span class="line">  "pooler_num_fc_layers": 3, </span><br><span class="line">  "pooler_size_per_head": 128, </span><br><span class="line">  "pooler_type": "first_token_transform", </span><br><span class="line">  "type_vocab_size": 2, #segment_ids类别 [0,1] </span><br><span class="line">  "vocab_size": 21128#词典中词数</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="dui-bi">对比</h1>
<h2 id="wei-bo-qing-gan-fen-lei">微博情感分类</h2>
<h3 id="shu-ju">数据</h3>
<p>weibo_senti_100k：共119988条数据，正例：59993,负例59995</p>
<p>句子最大长度：260，最小长度：3，平均长度：66.04</p>
<p>部分样例:</p>
<table>
<thead>
<tr>
<th>label</th>
<th>review</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]</td>
</tr>
<tr>
<td>1</td>
<td>@张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心，酒店都全部OK啦。</td>
</tr>
<tr>
<td>1</td>
<td>姑娘都羡慕你呢…还有招财猫高兴……//@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢//@李欣芸SharonLee:大佬范儿[书呆子]</td>
</tr>
<tr>
<td>1</td>
<td>美~~~~~[爱你]</td>
</tr>
<tr>
<td>1</td>
<td>梦想有多大，舞台就有多大![鼓掌]</td>
</tr>
<tr>
<td>0</td>
<td>今天真冷啊，难道又要穿棉袄了[晕]？今年的春天真的是百变莫测啊[抓狂]</td>
</tr>
<tr>
<td>0</td>
<td>[衰][衰][衰]像给剥了皮的蛇</td>
</tr>
<tr>
<td>0</td>
<td>酒驾的危害，这回是潜水艇。//@上海译文丁丽洁:[泪]</td>
</tr>
<tr>
<td>0</td>
<td>积压了这么多的枕边书，没一本看完了的，现在我读书的最佳地点尽然是公交车[晕]</td>
</tr>
<tr>
<td>0</td>
<td>[泪]错过了……</td>
</tr>
</tbody>
</table>
<p><a href="https://github.com/Embedding/Chinese-Word-Vectors" target="_blank" rel="noopener">词向量下载</a></p>
<p><a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">中文预训练bert模型下载</a></p>
<h3 id="fen-xi-ji-bi-jiao">分析及比较</h3>
<p>分成三类进行比较：</p>
<ol>
<li>FastText，TextCNN，DPCNN，RNN，RNN-Attention，TextRCNN(词向量/字向量)</li>
<li>RNN，LSTM，GRU，RNN-Attention</li>
<li>Bert，BertCNN，BertRNN，BertRCNN</li>
</ol>
<h4 id="fast-text-text-cnn-dpcnn-rnn-rnn-attention-text-rcnn">FastText，TextCNN，DPCNN，RNN，RNN-Attention，TextRCNN</h4>
<p>训练集上（词向量）的表现：</p>
<p><img src="/2020/03/29/text_classification/image-20200520140510805.png" alt></p>
<p>训练集上的速度：</p>
<p><img src="/2020/03/29/text_classification/image-20200520140848340.png" alt></p>
<p>验证集上的表现：</p>
<p><img src="/2020/03/29/text_classification/image-20200520135717244.png" alt></p>
<p>验证集上的速度</p>
<p><img src="/2020/03/29/text_classification/image-20200520140931559.png" alt="image-20200520140931559"></p>
<ol>
<li>从验证集上的表现来看，rcnn的表现比较稳定，0.985，但是其训练以及预测速度却是最慢的一个。FastText是速度最快的一个，在验证集上也能取得0.961的表现。可以根据任务的需求进行取舍。</li>
<li>对于FastText 来说，embedding的训练一定要打开。</li>
<li>对于过拟合现象，可以通过freeze word embedding 来缓解。</li>
<li>是使用字向量还是词向量？
<ul>
<li>对于FastText来说，使用词向量会比使用字向量精确度高出1%左右。原因就向上面FastText的缺点所述部分，使用词向量的时候会添加额外的<code>n-gram</code>信息。</li>
<li>对于TextCNN来说，网络自身就能够提取<code>n-gram</code>特征，如果再使用词向量，对于短文本来说，句子信息被压缩，容易出现过拟合现象（DPCNN同样出现过拟合）。在短文本的数据集上，TextCNN还是使用字向量比较好。</li>
<li>对于rnn来说，本身就存在梯度弥散和梯度爆炸的问题，所以使用词向量，使得句子序列会变长，会加剧这个问题。对于lstm来说也是同样的。</li>
<li>对于rcnn来说，使用词向量还是字向量基本没有任何区别。</li>
<li>对于加attention的rnn，每个时间步会attention到整个序列的word embedding，所以词向量或者字向量带来的影响并不明显。</li>
</ul>
</li>
</ol>
<h4 id="rnn-lstm-gru">RNN，LSTM，GRU</h4>
<p>训练集和验证集上的表现：</p>
<p><img src="/2020/03/29/text_classification/image-20200520144027325.png" alt></p>
<p>速度比较：</p>
<p><img src="/2020/03/29/text_classification/image-20200520144139492.png" alt></p>
<ol>
<li>速度上相差无几，能用lstm就用lstm把。</li>
<li>不要仅仅使用rnn最后输出的hidden来做分类。（如果只使用hidden来做分类，准确度50%.）</li>
<li>是使用sum求和来获取整句话的语义还是使用mean来获取整句话的语义其实影响不大。</li>
<li>在rnn上使用attention 精度上会略有提升，但是相比于速度的下降，感觉有些得不偿失，如果追求精度可以加上attention。</li>
</ol>
<h4 id="bert-bert-cnn-bert-rnn-bert-rcnn">Bert，BertCNN，BertRNN，BertRCNN</h4>
<p><img src="/2020/03/29/text_classification/image-20200521150254593.png" alt></p>
<ol>
<li>通常来说bert的模型的train不用打开，如果打开，在bert后面接的层的学习率应大于bert学习率一两个数量级，使得后面的层得到充分的训练。</li>
<li>bert模型本身就可以达到98.3%左右的精确度，在后面添加其他模型看不出效果。</li>
</ol>
<h2 id="cnews-xin-wen-shi-fen-lei-jie-guo-bi-jiao">Cnews新闻十分类结果比较</h2>
<h3 id="shu-ju-1">数据</h3>
<p>类别：‘体育’ ‘娱乐’ ‘家居’ ‘房产’ ‘教育’ ‘时尚’ ‘时政’ ‘游戏’ ‘科技’ ‘财经’<br>
训练集：50000 条数据，最大长度：27467，最小长度：8，类别个数：10,平均长度：913.31<br>
验证集：5000 条数据，最大长度：10919，最小长度：15，类别个数：10<br>
测试集：10000 条数据，最大长度：14720，最小长度：13，类别个数：10</p>
<p>使用数据集<a href="https://pan.baidu.com/s/1OOTK374IZ1DHnz5COUcfbw" target="_blank" rel="noopener">cnews</a>  密码:hf6o</p>
<p>训练集部分样例及每个类别的统计：</p>
<table>
<thead>
<tr>
<th>label</th>
<th>text</th>
<th>数量</th>
</tr>
</thead>
<tbody>
<tr>
<td>体育</td>
<td>黄蜂vs湖人首发：科比带伤战保罗 加索尔救赎之战 新浪体育讯北京时间4月27日，NBA季后赛首轮洛杉矶湖人主场迎战新奥尔良黄蜂，此前的比赛中，双方战成2-2平，因此本场比赛对于两支球队来说都非常重要，赛前双方也公布了首发阵容：湖人队：费舍尔、科比、阿泰斯特、加索尔、拜纳姆黄蜂队：保罗、贝里内利、阿里扎、兰德里、奥卡福[新浪NBA官方微博][新浪NBA湖人新闻动态微博][新浪NBA专题]<a href="%E6%96%B0%E6%B5%AA%E4%BD%93%E8%82%B2">黄蜂vs湖人图文直播室</a></td>
<td>5000</td>
</tr>
<tr>
<td>娱乐</td>
<td>皮克斯首部3D动画《飞屋历险记》预告发布(图)视频：动画片《飞屋历险记》先行版43秒预告新浪娱乐讯 迪士尼、皮克斯2009暑期3D动画力作《飞屋历险记》(Up)发布预告片，虽然这款预告片仅有43秒，并且只出现了被汽球吊起来的房屋，但门前老爷爷卡尔的一声“下午好”着实让人忍俊不禁。该片由《怪兽电力公司》导演彼特·道格特(Pete Docter)执导，曾在《海底总动员》、《料理鼠王》担任编剧的皮克斯老班底鲍勃-派特森(Bob Peterson)亦将在本片担任共同导演，献出自己的导演处女作。《飞屋历险记》同时会是皮克斯有史以来第一部以3-D电影呈现的里程碑作品，之后皮克斯的所有影片都将制作成立体电影。《飞屋历险记》讲述了一老一少的冒险旅程。78岁的老翁卡尔·弗雷德里克森(Carl Fredricksen)一生中都梦想着能环游世界、出没于异境险地体验，却平淡地渡过了一生。在他生活的最后阶段，卡而仿佛在命运的安排下，带着8岁的亚裔小鬼头Russell一同踏上了冒险的旅程。这对一老一小的奇特组合肩并肩闯荡江湖，共同经历了荒野跋涉、丛林野兽与反派坏蛋的狙击。田野/文</td>
<td>5000</td>
</tr>
<tr>
<td>家居</td>
<td>橱柜价格关键在计价方式 教你如何挑选买过橱柜的人都知道，橱柜的计价很复杂，商家的报价方式也不尽相同，那么哪种计价方式对消费者更有利？在计价过程中应该注意哪些问题？消费者在购买橱柜之前一定要了解清楚。 橱柜的主要计价方式——延米计价和单元柜体计价 现在市场上橱柜主要有两种计价方式——延米计价和单元柜体计价。 延米计价是指地柜和吊柜各一米的总价(有些还包含台面)。在此基础上，如果有局部区域只要地柜不要吊柜，就会按就会按“2/8”或“4/6”的比例折算。如某橱柜材料的延米价为2000元/延米，某顾客做2米的吊柜、4米的地柜，则吊柜价=2000X0.4X2=1600元，地柜价=2000X0.6X4=4800元(此吊柜、地柜价按4/6的比例计算)，再加上所选台面、配件、电器等附加费用即为整套橱柜的价格。 延米报价有许多不合理之处，水槽、燃气灶、嵌入式电器等部分所需门板很少，但仍按延米来算价，对消费者来说很不划算。例如一款1000元/延米的橱柜，一个水槽约0.8米长，但消费者还是要按1000元的单价乘以0.8米付费，这个实际上只是几块材料简单组合的水槽柜需要消费者花800元，而同样材质、同样大小的水槽柜仅需400元左右，二者价格相差数百元。 按延米计价，所有的配件费用都是在原有的基础上增加，虽然有些厂家宣称抽屉、拉篮不加钱，但其实那是最基本的配置，一旦顾客要求调整方案，就会要多加钱，此外不足一米的部分要按一米计价，因此对顾客来说，如此计价会多花不少冤枉钱。 “单元柜体计价”是国际惯例的橱柜计价方式，是按每一个组成厨柜的单元柜价格进行计算，然后加出总价。具体为：某吊柜单价×个数+某地柜单价×个数……。利用单元柜体计价，更为合理。举个例子说，外观相同的柜体，抽屉数量、五金件、托架数量如果不同，在以延米计价时，商家往往只给消费者最简单、最省成本的产品。而按单元柜体计价，一款尺寸相同的抽屉柜可按不同配置报出不同价格：同样是一款30cm宽、66cm高的单体柜，如果门改成弧型，是多少钱；如果抽屉里加上可拆装的金属篮，是多少钱；如果抽屉的侧板是木质的多少钱……把橱柜的每个细节都分解开来，消费者可以在预算之内把可有可无的配置省掉，把钱花在自己更需要的功能上。 两全其美报价方式——延米计价和单元柜体计价相结合 现在中国橱柜市场上仍普遍采用延米计价，但进口品牌及国内一些大品牌橱柜都采用单元柜体计价方式，如德宝·西克曼、海尔等品牌即是采用单元柜体计价方式。不过德宝·西克曼厨柜的工作人员介绍到，如果一开始就用单元柜体计价来进行报价，不够直观，同时为了便于顾客进行比较，他们会用延米计价给顾客所选定的材料进行一个初始报价，让顾客对自己的厨房装修要花多少钱心里大概有个底。在对厨房进行量尺后，设计师会按照顾客的需求，设计出厨房效果图。这时，销售人员会按单元柜体计价给顾客进行一个报价。对于每一种标准柜体都有相应的报价，顾客实际用到几组柜子，将这些柜子价格累加，再加上台面及其他相关费用，便是整个橱柜的价格。</td>
<td>5000</td>
</tr>
<tr>
<td>房产</td>
<td>冯仑：继续增持高GDP城市商业地产确立商业地产投资战略不久的万通地产(企业专区,旗下楼盘)(600246)，今年上半年遭遇了业绩下滑。公司昨日公布的半年报显示，其商业物业在报告期内实现的营业收入同比下降33.71%，营业利润率比上年同期下降47.29个百分点。不过，公司董事长冯仑日前表示，依然看好人均GDP8000美元以上城市的商业地产，万通将继续增加高GDP城市的商业地产；计划用5-10年，商业物业收入占比达到30%-50%。逆向运作地产投资冯仑指出，根据历史经验，GDP的增长、城市化的增长，和房地产物业形态有一定关系，即人均GDP在8000美元以下时，住宅是市场的核心，主流产品都将围绕住宅展开。目前，在中国的城市中，人均GDP8000美元的城市大约有十个，大部分省会城市依然在3000美元至5000美元之间，因此，未来5-10年，中国房地产市场的产品结构仍然是以住宅为主。 冯仑认为，万通地产从现在开始扩大商业地产的比重，在目前的市场中，是一种逆向运作的思维，但符合长期趋势。他指出，在人均GDP达到8000美元的经济实体中，商用不动产会成为地产业的主角。以美国为例，商业地产的市场规模大约是住宅的两倍。中国商业地产未来的市场空间很大。根据万通地产的发展战略，除了在环渤海区域内发展住宅以外，还会重点发展商业不动产。未来，公司业务结构将逐步调整，商用地产的收入会逐年增加；今后，公司商业物业收入将占到整体营业收入的一半左右。对于目前商业地产面临的不景气局面，万通地产董事会秘书程晓?指出，公司战略不会因市场的短期波动而改变，公司将继续加大商用物业项目的投资力度，以营运带动开发，以财务安排的多样化实施商用物业投资。改变商业模式冯仑表示，就房地产开发模式而言，过去两百年主要经历了三次变化，即从“地主加工头”到“厂长加资本家”，再到“导演加制片”。目前，国内多数地产商的开发模式属于“地主加工头”和“厂长加资本家”的阶段；而商业地产的开发模式，不能停留在这两个阶段。所谓“导演加制片”模式，即由专业的房地产投资和资产管理公司负责运营商业地产项目，实现收入的多元化。而这种模式需要相应的金融创新产品支持。业内人士指出，房地产金融领域内的REITS、抵押贷款等金融产品体系的完善，将支持商用地产在一个多元化的不动产经营环境中快速的成长。而商业模式的改变需要较长一段时间。数据显示，香港主流房地产企业在人均GDP10000美元的时候开始逐步发展商业地产，先后经过13-15年确立起新的商业模式。其中，长江实业经过13年的发展，商业地产在业务机构的比重占到30%，新鸿基则经过15年的调整，商业地产比重占到50%。SOHO中国(企业专区,旗下楼盘)董事长潘石屹也指出，现在的市场虽然在调整，不过也给从事商业地产开发的企业提供了良好机会和平台，应及时在地域、开发物业的品种、品牌的建设、销售和持有物业的比重四个方面做出调整。 我要评论</td>
<td>5000</td>
</tr>
<tr>
<td>教育</td>
<td>2010年6月英语六级考试考后难度调查2010年6月大学英语六级考试将于19日下午举行，欢迎各位考生在考试后参加难度调查，发表你对这次考试的看法。点击进入论坛，参与考后大讨论</td>
<td>5000</td>
</tr>
<tr>
<td>时尚</td>
<td>组图：萝莉潮人示范春季复古实穿导语：萝莉潮人示范春季复古实穿，在乍暖还寒的初春，有的甜美、有的优雅、有的性感，但无论是哪种风格都给人强烈的视觉冲击力，在这个缤纷的春季更加脱俗动人。</td>
<td>5000</td>
</tr>
<tr>
<td>时政</td>
<td>香港特区行政长官曾荫权将离港休假中新社香港八月七日电 香港特区行政长官曾荫权将于八月九日至十五日离港休假。特区政府发言人七日透露，曾荫权离港期间，八月九日由特区财政司司长曾俊华署理行政长官职务；八月十日至十五日由政务司司长唐英年署理行政长官职务。(完)</td>
<td>5000</td>
</tr>
<tr>
<td>游戏</td>
<td>全国人大常委会将对59件法律相关条文作出修改新华社快讯：全国人大常委会27日表决通过了关于修改部分法律的决定，对59件法律的相关条文作出修改。</td>
<td>5000</td>
</tr>
<tr>
<td>科技</td>
<td>入门级时尚卡片机 尼康S220套装仅1150尼康S220延续了S系列纤巧超薄的机身设计，采用铝合金材质打造，表面质地细腻，不易沾染指纹。S220拥有紫色、深蓝、洋红、水晶绿和柔银五款靓丽颜色可供选择。</td>
<td>5000</td>
</tr>
</tbody>
</table>
<h3 id="fen-xi-bi-jiao">分析比较</h3>
<p>训练集上的表现（字向量）（序列长度：2000）：</p>
<p><img src="/2020/03/29/text_classification/image-20200526204848210.png" alt="各种模型"></p>
<p><img src="/2020/03/29/text_classification/image-20200526205351509.png" alt="rnn/cnn-attention/fast_text"></p>
<p><img src="/2020/03/29/text_classification/image-20200526205849399.png" alt="speed"></p>
<p><img src="/2020/03/29/text_classification/image-20200526210508245.png" alt="蓝色为字向量/绿色为词向量"></p>
<ol>
<li>rnn在面对长文本时直接崩溃了（感觉自己说了一个废话）。再给rnn加上attention之后，rnn得到了救赎，但是效果和FastText基本持平。由此推断lstm+attention 能够获得一个相对较好的结果（如果不考虑速度的话）。</li>
<li>从speed图来看，最快的当然是FastText，可以并行的CNN处于第二梯队，最后的是RNN系列的模型。</li>
<li>词向量还是字向量：
<ol>
<li>对于FastText来说，在长文本上，词向量的表现要远远好于字向量。</li>
<li>可能是文本长度过长了吧，实验结果表明：在长文本数据上，词向量的表现要好于字向量，对于长文本分类来说，整句话的语义要高于某些特定的词吧。</li>
<li>对于rnn系列来说，毫无疑问，词向量的表现远好于字向量，因为句子序列长度变短了。</li>
<li>rcnn模型一般来说不会比rnn，或者cnn表现差。</li>
</ol>
</li>
<li>xlnet由于显存的原因，序列长度只取到1500（只要你显存够，在一定意义上来说是解决了bert的长度限制的问题）。但是在11g单卡上，batch_size=2，跑完一步，FastText可以跑一个epoch。不知道知识蒸馏效果会怎样。</li>
</ol>
<h2 id="wen-ben-duo-biao-qian-fen-lei">文本多标签分类</h2>
<p>（先挖一个坑）</p>
<h2 id="fen-xi">分析</h2>
<h4 id="chang-duan-wen-ben-fen-lei-de-bi-jiao">长短文本分类的比较</h4>
<p>对于词嵌入技术的文本表示，短文本和长文本表示上没有差别，此时分类效果的优劣主要在分类模型和训练数据上。</p>
<p>对于数据而言：随着文本越长，语义的重要性就越高，在文本很短的情况下，语义的重要性就很小，比如：“今天 天气 怎么样”，“今天 怎么样 天气”，“怎么样 天气 今天”。你甚至不必要考虑句子是否通顺，基本上可以当一句话处理，没有第二个意思。但是随着文本越来越长，比如512个字符，颠倒一下可能就要归为两类了。</p>
<p>对于模型而言：对于短文本，CNN配合Max-pooling池化(如TextCNN模型)速度快，而且效果也很好。因为短文本上的关键词比较容易找到，而且Max-pooling会直接过滤掉模型认为不重要特征。具体工作机制是：卷积窗口沿着长度为n的文本一个个滑动，类似于n-gram机制对文本切词，然后和文本中的每个词进行相似度计算，因为后面接了个Max-pooling，因此只会保留和卷积核最相近的词。微博数据集属于情感分类，为了判断句子的情感极性，只需要让分类器能识别出“不开心”这类词是个负极性的词，“高兴”、“开心”等这类词是正极性的词，其他词是偏中性词就可以了。因此，当把该句子中的各个词条输入给模型去分类时，并不需要去“瞻前顾后”，因此使用一个关注局部的前馈神经网络往往表现更佳。虽然Attention也突出了重点特征，但是难以过滤掉所有低分特征。但是对于长文本直接用CNN就不行了，TextCNN会比HAN模型泛化能力差很多。<strong>如果在TextCNN前加一层LSTM，这样效果可以提升很大</strong>。</p>
<h4 id="wei-shi-yao-chang-wen-ben-fen-lei-de-shi-yan-zhong-cnn-he-rnn-mei-you-la-kai-chai-ju">为什么长文本分类的实验中，cnn 和 rnn 没有拉开差距？</h4>
<p>cnn和rnn的精度都很高，分析主要还是分类的文章规则性比较强，且属于特定领域，词量不多，类别差异可能比较明显。</p>
<h1 id="wen-ben-fen-lei-tricks">文本分类tricks</h1>
<h2 id="fen-ci-qi">分词器</h2>
<p><strong>分词器所分出的词与词向量表中的token粒度match是更重要的事情</strong></p>
<h2 id="yi-zhi-yu-xun-lian-ci-xiang-liang-de-fen-ci-qi">已知预训练词向量的分词器</h2>
<p>像word2vec、glove、fasttext这些官方release的预训练词向量都会公布相应训练语料的信息，包括预处理策略如分词，这种情况下直接使用官方的训练该词向量所使用的分词器，此分词器在下游任务的表现十之八九会比其他花里胡哨的分词器好用。</p>
<h2 id="bu-zhi-dao-yu-xun-lian-ci-xiang-liang-de-fen-ci-qi">不知道预训练词向量的分词器</h2>
<p>这时就需要去“猜”一下分词器。怎么猜呢？<br>
首先，拿到预训练词向量表后，去里面search一些特定词汇比如一些网站、邮箱、成语、人名等，英文里还有n’t等，看看训练词向量使用的分词器是把它们分成什么粒度。<br>
然后跑几个分词器，看看哪个分词器的粒度跟他最接近就用哪个，如果不放心，就放到下游任务里跑跑看。</p>
<p>最理想的情况是：先确定最适合当前任务数据集的分词器，再使用同分词器产出的预训练词向量。如果无法满足理想情况，则需要自己在下游任务训练集或者大量同分布无监督语料上训练的词向量更有利于进一步压榨模型的性能。</p>
<h2 id="guan-yu-zhong-wen-zi-xiang-liang">关于中文字向量</h2>
<p>预训练中文字向量的时候，把窗口开大一些，不要直接使用word-level的窗口大小，效果会比随机初始化的字向量明显的好。</p>
<h2 id="shu-ju-ji-zao-sheng-hen-yan-zhong">数据集噪声很严重</h2>
<p>里噪声严重有两种情况。对于数据集D(X, Y)，一种是X内部噪声很大（比如文本为口语化表述或由互联网用户生成），一种是Y的噪声很大（一些样本被明显的错误标注，一些样本人也很难定义是属于哪一类，甚至具备类别二义性）。</p>
<h2 id="x-nei-bu-zao-sheng-hen-da">X内部噪声很大</h2>
<p>法一：直接将模型的输入变成char-level（中文中就是字的粒度），然后train from scratch（不使用预训练词向量）去跟word-level的对比一下，如果char-level的明显的效果好，那么短时间之内就直接基于char-level去做模型。</p>
<p>法二：使用特殊超参的FastText去训练一份词向量：<br>
一般来说fasttext在英文中的char ngram的窗口大小一般取值3～6，但是在处理中文时，如果我们的目的是为了去除输入中的噪声，那么我们可以把这个窗口限制为1～2，这种小窗口有利于模型去捕获错别字（比如，我们打一个错误词的时候，一般都是将其中的一个字达成同音异形的另一个字），比如word2vec学出来的“似乎”的最近词可能是“好像”，然而小ngram窗口fasttext学出来的“似乎”最近词则很有可能是“是乎”等内部包含错别字的词，这样就一下子让不太过分的错别字构成的词们又重新回到了一起，甚至可以一定程度上对抗分词器产生的噪声（把一个词切分成多个字）。</p>
<h2 id="y-de-zao-sheng-hen-da">Y的噪声很大</h2>
<p>首先忽略这个噪声，强行的把模型尽可能好的训出来。然后让训练好的模型去跑训练集和开发集，取出训练集中的错误样本和开发集中那些以很高的置信度做出错误决策的样本（比如以99%的把握把一个标签为0的样本预测为1），然后去做这些bad cases的分析，如果发现错误标注有很强的规律性，则直接撸一个脚本批量纠正一下（只要确保纠正后的标注正确率比纠正前明显高就行）。<br>
如果没有什么规律，但是发现模型高置信度做错的这些样本大部分都是标注错误的话，就直接把这些样本都删掉，常常也可以换来性能的小幅提升，毕竟测试集都是人工标注的，困难样本和错标样本不会太多。</p>
<h2 id="baseline-xuan-yong-cnn-huan-shi-rnn">baseline选用CNN还是RNN？</h2>
<p>看数据集，如果感觉数据集里很多很强的ngram可以直接帮助生成正确决策，那就CNN。<br>
如果感觉很多case都是那种需要把一个句子看完甚至看两三遍才容易得出正确tag，那就RNN。<br>
还可以CNN、RNN的模型都跑出来简单集成一下。</p>
<h2 id="dropout-jia-zai-na-li">Dropout加在哪里</h2>
<p>word embedding层后、pooling层后、FC层（全联接层）后。</p>
<h2 id="er-fen-lei">二分类</h2>
<p>二分类问题不一定要用sigmoid作为输出层的激活函数，尝试一下包含俩类别的softmax。可能多一条分支就多一点信息，实践中常常带来零点几个点的提升。</p>
<h2 id="yang-ben-lei-bie-bu-jun-heng-wen-ti">样本类别不均衡问题</h2>
<p>如果正负样本比小于9:1的话，继续做深度模型调超参，决策阈值也完全不用手调。但是，如果经常一个batch中完全就是同一个类别的样本，或者一些类别的样本经过好多batch都难遇到一个的话，均衡就非常非常有必要了。</p>
<h2 id="zui-hou">最后</h2>
<ol>
<li>别太纠结文本截断长度使用120还是150</li>
<li>别太纠结对性能不敏感的超参数带来的开发集性能的微小提升</li>
<li>别太纠结未登陆词的embedding是初始化成全0还是随机初始化，别跟PAD共享embedding就行</li>
<li>别太纠结优化器用Adam还是MomentumSGD，如果跟SGD的感情还不深，无脑Adam，最后再用MomentumSGD跑几遍</li>
<li>还是不会用tricks但是就是想跑出个好结果，bert大力出奇迹。</li>
</ol>
<h1 id="zong-jie">总结</h1>
<p>复杂的模型未必会有很好的结果，简单模型效果未必不理想，没必要一味追求深度学习、复杂模型什么的。选什么样的模型还是要根据数据来的。同一类问题，不同的数据效果差异很大，不要小看任何一类问题，例如分类，我们通常觉得它很简单，但有些数据并非你所想。</p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://blog.csdn.net/feilong_csdn/article/details/88655927" target="_blank" rel="noopener">fastText原理和文本分类实战，看这一篇就够了</a></li>
<li><a href="https://blog.csdn.net/asialee_bird/article/details/88813385#%E4%B8%80%E3%80%81%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0" target="_blank" rel="noopener">TextCNN文本分类（keras实现）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/55263066" target="_blank" rel="noopener">浅谈基于深度学习的文本分类问题</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2018-06-22-4" target="_blank" rel="noopener">从DPCNN出发，撩一下深层word-level文本分类模型</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2019-01-24-5" target="_blank" rel="noopener">文本分类有哪些论文中很少提及却对性能有重要影响的tricks？</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247484682&amp;idx=1&amp;sn=51520138eed826ec891ac8154ee550f9&amp;chksm=970c2ddca07ba4ca33ee14542cff0457601bb16236f8edc1ff0e617051d1f063354dda0f8893&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">从前馈到反馈：解析循环神经网络（RNN）及其tricks</a></li>
<li>[<a href="http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%81%9A%E5%A5%BD%E9%95%BF%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%B3%95%E5%BE%8B%E6%96%87%E4%B9%A6%E6%99%BA%E8%83%BD%E5%8C%96" target="_blank" rel="noopener">达观数据曾彦能：如何用深度学习做好长文本分类与法律文书智能化处理</a>](<a href="http://www.52nlp.cn/tag/%E9%95%BF%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB" target="_blank" rel="noopener">http://www.52nlp.cn/tag/长文本分类</a>)</li>
<li><a href="https://www.zhihu.com/question/326770917/answer/698646465" target="_blank" rel="noopener">短文本分类和长文本分类的模型如何进行选择？</a></li>
<li><a href="https://www.pianshen.com/article/4319299677/" target="_blank" rel="noopener">NLP实践九：HAN原理与文本分类实践</a></li>
<li><a href="https://www.jianshu.com/p/56061b8f463a" target="_blank" rel="noopener">NLP之文本分类</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
      <tags>
        <tag>FastText</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>数据预处理</title>
    <url>/2020/03/28/data_preprocessing/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/03/28/data_preprocessing/image-20200528110918927.png" alt></p>
<a id="more"></a>
<h1 id="shu-ju-qing-xi">数据清洗</h1>
<h2 id="jian-jie">简介</h2>
<p>要获得优秀的模型，首先需要清洗数据。在拟合机器学习或统计模型之前，我们通常需要清洗数据。用杂乱数据训练出的模型无法输出有意义的结果。</p>
<p>数据清洗：从记录集、表或数据库中检测和修正（或删除）受损或不准确记录的过程。它识别出数据中不完善、不准确或不相关的部分，并替换、修改或删除这些脏乱的数据。</p>
<p>整个清洗流程以kaggle的<a href="https://www.kaggle.com/c/sberbank-russian-housing-market/data" target="_blank" rel="noopener">Sberbank 俄罗斯房地产价值预测竞赛数据</a>为例。</p>
<h2 id="cha-kan-shu-ju">查看数据</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import packages</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.mlab <span class="keyword">as</span> mlab</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="comment"># 使用自带的样式进行美化</span></span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> figure</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment"># 设置默认参数</span></span><br><span class="line">matplotlib.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">12</span>,<span class="number">8</span>)</span><br><span class="line"><span class="comment"># 关闭copywarning  据说速度会更快</span></span><br><span class="line">pd.options.mode.chained_assignment = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># read the data</span></span><br><span class="line">df = pd.read_csv(<span class="string">'sberbank.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># shape and data types of the data</span></span><br><span class="line">print(df.shape)</span><br><span class="line">print(df.dtypes)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">(<span class="number">30471</span>, <span class="number">292</span>)</span><br><span class="line">id                      int64</span><br><span class="line">timestamp              object</span><br><span class="line">full_sq                 int64</span><br><span class="line">life_sq               float64</span><br><span class="line">floor                 float64</span><br><span class="line">                       ...   </span><br><span class="line">mosque_count_5000       int64</span><br><span class="line">leisure_count_5000      int64</span><br><span class="line">sport_count_5000        int64</span><br><span class="line">market_count_5000       int64</span><br><span class="line">price_doc               int64</span><br><span class="line">Length: <span class="number">292</span>, dtype: object</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># select numeric columns</span></span><br><span class="line">df_numeric = df.select_dtypes(include=[np.number])</span><br><span class="line">numeric_cols = df_numeric.columns.values</span><br><span class="line">print(numeric_cols)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">[<span class="string">'id'</span> <span class="string">'full_sq'</span> <span class="string">'life_sq'</span> <span class="string">'floor'</span> <span class="string">'max_floor'</span> <span class="string">'material'</span> <span class="string">'build_year'</span></span><br><span class="line"> <span class="string">'num_room'</span> <span class="string">'kitch_sq'</span> <span class="string">'state'</span> <span class="string">'area_m'</span> <span class="string">'raion_popul'</span> <span class="string">'green_zone_part'</span></span><br><span class="line"> <span class="string">'indust_part'</span> <span class="string">'children_preschool'</span> <span class="string">'preschool_quota'</span></span><br><span class="line"> <span class="string">'preschool_education_centers_raion'</span> <span class="string">'children_school'</span> <span class="string">'school_quota'</span></span><br><span class="line"> <span class="string">'school_education_centers_raion'</span> <span class="string">'school_education_centers_top_20_raion'</span></span><br><span class="line"> <span class="string">'hospital_beds_raion'</span> <span class="string">'healthcare_centers_raion'</span>...]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># select non numeric columns</span></span><br><span class="line">df_non_numeric = df.select_dtypes(exclude=[np.number])</span><br><span class="line">non_numeric_cols = df_non_numeric.columns.values</span><br><span class="line">print(non_numeric_cols)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">[<span class="string">'timestamp'</span> <span class="string">'product_type'</span> <span class="string">'sub_area'</span> <span class="string">'culture_objects_top_25'</span></span><br><span class="line"> <span class="string">'thermal_power_plant_raion'</span> <span class="string">'incineration_raion'</span> <span class="string">'oil_chemistry_raion'</span></span><br><span class="line"> <span class="string">'radiation_raion'</span> <span class="string">'railroad_terminal_raion'</span> <span class="string">'big_market_raion'</span></span><br><span class="line"> <span class="string">'nuclear_reactor_raion'</span> <span class="string">'detention_facility_raion'</span> <span class="string">'water_1line'</span></span><br><span class="line"> <span class="string">'big_road1_1line'</span> <span class="string">'railroad_1line'</span> <span class="string">'ecology'</span>]</span><br></pre></td></tr></table></figure>
<p>从以上结果中，我们可以看到该数据集共有 30,471 行、292 列，还可以辨别特征属于数值变量还是分类变量。这些都是有用的信息。</p>
<h2 id="que-shi-shu-ju">缺失数据</h2>
<h3 id="re-tu">热图</h3>
<p>当特征数量较少时，可以通过热图对缺失数据进行可视化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cols = df.columns[:<span class="number">30</span>] <span class="comment"># first 30 columns</span></span><br><span class="line">colours = [<span class="string">'#000099'</span>, <span class="string">'#ffff00'</span>] <span class="comment"># specify the colours - yellow is missing. blue is not missing.</span></span><br><span class="line">sns.heatmap(df[cols].isnull(), cmap=sns.color_palette(colours))</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/output_4_1.png" alt></p>
<p>上图展示了前 30 个特征的缺失数据模式。横轴表示特征名，纵轴表示观察值/行数，黄色表示缺失数据，蓝色表示非缺失数据。例如，图中特征 life_sq 在多个行中存在缺失值。而特征 floor 只在第 7000 行左右出现零星缺失值。</p>
<h3 id="bai-fen-bi-lie-biao">百分比列表</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># if it's a larger dataset and the visualization takes too long can do this.</span></span><br><span class="line"><span class="comment"># % of missing.</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">    pct_missing = np.mean(df[col].isnull())</span><br><span class="line">    print(<span class="string">'&#123;&#125; - &#123;&#125;%'</span>.format(col, round(pct_missing*<span class="number">100</span>)))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">id - <span class="number">0.0</span>%</span><br><span class="line">timestamp - <span class="number">0.0</span>%</span><br><span class="line">full_sq - <span class="number">0.0</span>%</span><br><span class="line">life_sq - <span class="number">21.0</span>%</span><br><span class="line">floor - <span class="number">1.0</span>%</span><br><span class="line">big_market_raion - <span class="number">0.0</span>%</span><br><span class="line">nuclear_reactor_raion - <span class="number">0.0</span>%</span><br><span class="line">detention_facility_raion - <span class="number">0.0</span>%</span><br><span class="line">full_all - <span class="number">0.0</span>%</span><br><span class="line">male_f - <span class="number">0.0</span>%</span><br><span class="line">hospital_beds_raion - <span class="number">47.0</span>%</span><br><span class="line">raion_build_count_with_material_info - <span class="number">16.0</span>%</span><br><span class="line">build_count_block - <span class="number">16.0</span>%</span><br><span class="line">build_count_wood - <span class="number">16.0</span>%</span><br><span class="line">build_count_frame - <span class="number">16.0</span>%</span><br><span class="line">build_count_brick - <span class="number">16.0</span>%</span><br><span class="line">build_count_monolith - <span class="number">16.0</span>%</span><br><span class="line">build_count_panel - <span class="number">16.0</span>%</span><br><span class="line">build_count_foam - <span class="number">16.0</span>%</span><br><span class="line">build_count_slag - <span class="number">16.0</span>%</span><br><span class="line">build_count_mix - <span class="number">16.0</span>%</span><br><span class="line">raion_build_count_with_builddate_info - <span class="number">16.0</span>%</span><br><span class="line">build_count_before_1920 - <span class="number">16.0</span>%</span><br><span class="line">build_count_1921<span class="number">-1945</span> - <span class="number">16.0</span>%</span><br><span class="line">build_count_1946<span class="number">-1970</span> - <span class="number">16.0</span>%</span><br><span class="line">build_count_1971<span class="number">-1995</span> - <span class="number">16.0</span>%</span><br><span class="line">build_count_after_1995 - <span class="number">16.0</span>%</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>输出展示了每个特征的缺失值百分比。具体而言，可以从输出中看到特征 life_sq 有 21% 的缺失数据，而特征 floor 仅有 1% 的缺失数据。该列表有效地总结了每个特征的缺失数据百分比情况，是对热图可视化的补充。</p>
<h3 id="zhi-fang-tu">直方图</h3>
<p>在存在很多特征时，缺失数据直方图也不失为一种有效方法。要想更深入地了解观察值中的缺失值模式，我们可以用直方图的形式进行可视化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># first create missing indicator for features with missing data</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">    missing = df[col].isnull()</span><br><span class="line">    num_missing = np.sum(missing)</span><br><span class="line">  <span class="comment"># 如果该列存在缺失值，对该列中每一行是否缺失进行标记</span></span><br><span class="line">    <span class="keyword">if</span> num_missing &gt; <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'created missing indicator for: &#123;&#125;'</span>.format(col))</span><br><span class="line">        df[<span class="string">'&#123;&#125;_ismissing'</span>.format(col)] = missing</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># then based on the indicator, plot the histogram of missing values</span></span><br><span class="line">ismissing_cols = [col <span class="keyword">for</span> col <span class="keyword">in</span> df.columns <span class="keyword">if</span><span class="string">'ismissing'</span><span class="keyword">in</span> col]</span><br><span class="line"><span class="comment"># 统计一行中，有多少缺失值</span></span><br><span class="line">df[<span class="string">'num_missing'</span>] = df[ismissing_cols].sum(axis=<span class="number">1</span>)</span><br><span class="line">df[<span class="string">'num_missing'</span>].value_counts().reset_index().sort_values(by=<span class="string">'index'</span>).plot.bar(x=<span class="string">'index'</span>, y=<span class="string">'num_missing'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/output_6_2.png" alt></p>
<p>直方图可以帮助在 30,471 个观察值中识别缺失值状况。例如，从上图中可以看到，超过 6000 个观察值不存在缺失值，接近 4000 个观察值具备一个缺失值。</p>
<h3 id="ru-he-chu-li-que-shi-shu-ju">如何处理缺失数据</h3>
<h4 id="shan-chu-guan-cha-zhi">删除观察值</h4>
<p>在统计学中，该方法叫做成列删除(listwise deletion)，需要丢弃包含缺失值的整列观察值。只有在确定缺失数据无法提供信息时，才可以执行该操作。否则，我们应当考虑其他解决方案。</p>
<p>例如，从缺失数据直方图中，我们可以看到只有少量观察值的缺失值数量超过 35。因此，可以创建一个新的数据集 df_less_missing_rows，该数据集删除了缺失值数量超过 35 的观察值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># drop rows with a lot of missing values.</span></span><br><span class="line">ind_missing = df[df[<span class="string">'num_missing'</span>] &gt; <span class="number">35</span>].index</span><br><span class="line">df_less_missing_rows = df.drop(ind_missing, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="diu-qi-te-zheng">丢弃特征</h4>
<p>只在确定某个特征无法提供有用信息时才丢弃它。</p>
<p>例如，从缺失数据百分比列表中，我们可以看到 hospital_beds_raion 具备较高的缺失值百分比——47%，因此我们丢弃这一整个特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hospital_beds_raion has a lot of missing. If we want to drop.</span></span><br><span class="line">cols_to_drop = [<span class="string">'hospital_beds_raion'</span>]</span><br><span class="line">df_less_hos_beds_raion = df.drop(cols_to_drop, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="tian-chong-que-shi-shu-ju">填充缺失数据</h4>
<p>当特征是数值变量时，执行缺失数据填充。对同一特征的其他非缺失数据取平均值或中位数，用这个值来替换缺失值。当特征是分类变量时，用众数（最频值）来填充缺失值。以特征 life_sq 为例，可以用特征中位数来替换缺失值。（或者用回归的方式填补缺失值）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># replace missing values with the median.</span></span><br><span class="line">med = df[<span class="string">'life_sq'</span>].median()</span><br><span class="line">print(med)</span><br><span class="line">df[<span class="string">'life_sq'</span>] = df[<span class="string">'life_sq'</span>].fillna(med)</span><br></pre></td></tr></table></figure>
<p>其他方式填补缺失数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 调包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认用均值添补</span></span><br><span class="line">imp_mean = SimpleImputer()</span><br><span class="line">imp_mean = imp_mean.fit_transform(Age) </span><br><span class="line"></span><br><span class="line"><span class="comment">#用中位数填补</span></span><br><span class="line">imp_median = SimpleImputer(strategy=<span class="string">"median"</span>)  </span><br><span class="line">imp_median = imp_median.fit_transform(Age) </span><br><span class="line"></span><br><span class="line"><span class="comment">#用0填补</span></span><br><span class="line">imp_0 = SimpleImputer(strategy=<span class="string">"constant"</span>,fill_value=<span class="number">0</span>) </span><br><span class="line">imp_0 = imp_0.fit_transform(Age)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在这里我们使用中位数填补Age </span></span><br><span class="line">data.loc[:,<span class="string">"Age"</span>] = imp_median</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用Pandas和Numpy进行填补其实更加简单</span></span><br><span class="line">data.loc[:,<span class="string">"Age"</span>] = data.loc[:,<span class="string">"Age"</span>].fillna(data.loc[:,<span class="string">"Age"</span>].median())</span><br></pre></td></tr></table></figure>
<p>可以对所有数值特征一次性应用同样的填充策略。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># impute the missing values and create the missing value indicator variables for each numeric column.</span></span><br><span class="line">df_numeric = df.select_dtypes(include=[np.number])</span><br><span class="line">numeric_cols = df_numeric.columns.values</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> numeric_cols:</span><br><span class="line">    missing = df[col].isnull()</span><br><span class="line">    num_missing = np.sum(missing)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># only do the imputation for the columns that have missing values.</span></span><br><span class="line">    <span class="keyword">if</span> num_missing &gt; <span class="number">0</span>:  </span><br><span class="line">        print(<span class="string">'imputing missing values for: &#123;&#125;'</span>.format(col))</span><br><span class="line">        df[<span class="string">'&#123;&#125;_ismissing'</span>.format(col)] = missing</span><br><span class="line">        med = df[col].median()</span><br><span class="line">        df[col] = df[col].fillna(med)</span><br></pre></td></tr></table></figure>
<p>对所有分类特征一次性应用众数填充策略。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># impute the missing values and create the missing value indicator variables for each non-numeric column.</span></span><br><span class="line">df_non_numeric = df.select_dtypes(exclude=[np.number])</span><br><span class="line">non_numeric_cols = df_non_numeric.columns.values</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> non_numeric_cols:</span><br><span class="line">    missing = df[col].isnull()</span><br><span class="line">    num_missing = np.sum(missing)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># only do the imputation for the columns that have missing values.</span></span><br><span class="line">    <span class="keyword">if</span> num_missing &gt; <span class="number">0</span>:  </span><br><span class="line">        print(<span class="string">'imputing missing values for: &#123;&#125;'</span>.format(col))</span><br><span class="line">        df[<span class="string">'&#123;&#125;_ismissing'</span>.format(col)] = missing</span><br><span class="line">        </span><br><span class="line">        top = df[col].describe()[<span class="string">'top'</span>] <span class="comment"># impute with the most frequent value.</span></span><br><span class="line">        df[col] = df[col].fillna(top)</span><br></pre></td></tr></table></figure>
<h4 id="ti-huan-que-shi-zhi">替换缺失值</h4>
<p>对于分类特征，可以添加新的带值类别，如 _MISSING_。对于数值特征，可以用特定值（如-999）来替换缺失值。这样，我们就可以保留缺失值，使之提供有价值的信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># categorical</span></span><br><span class="line">df[<span class="string">'sub_area'</span>] = df[<span class="string">'sub_area'</span>].fillna(<span class="string">'_MISSING_'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># numeric</span></span><br><span class="line">df[<span class="string">'life_sq'</span>] = df[<span class="string">'life_sq'</span>].fillna(<span class="number">-999</span>)</span><br></pre></td></tr></table></figure>
<h2 id="bu-gui-ze-shu-ju-yi-chang-zhi">不规则数据（异常值）</h2>
<p>异常值指与其他观察值具备显著差异的数据，它们可能是真的异常值也可能是错误。</p>
<h3 id="zhi-fang-tu-xiang-xing-tu">直方图+箱形图</h3>
<h4 id="zhi-fang-tu-1">直方图</h4>
<p>当特征是数值变量时，使用直方图和箱形图来检测异常值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># histogram of life_sq.</span></span><br><span class="line">df[<span class="string">'life_sq'</span>].hist(bins=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/21B8CEEC-A71F-4D53-9A70-A539685A1B39.png" alt></p>
<p>由于数据中可能存在异常值，因此图中数据高度偏斜。</p>
<p>比较正常的情况</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">'floor'</span>].hist(bins=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/722E5EFF-CD89-4994-90A4-7AA1ADE60CA5.png" alt></p>
<h4 id="xiang-xing-tu">箱形图</h4>
<p>为了进一步研究特征，看一下箱形图。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># box plot.</span></span><br><span class="line">df.boxplot(column=[<span class="string">'life_sq'</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/924F9994-B7CF-478E-B4BA-68CE136FE990.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># box plot.</span></span><br><span class="line">df.boxplot(column=[<span class="string">'floor'</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/65762A44-008B-4239-9024-97A9CE86EFD3.png" alt></p>
<h3 id="miao-shu-xing-tong-ji">描述性统计</h3>
<p>对于数值特征，当异常值过于独特时，箱形图无法显示该值。因此，可以查看其描述统计学。<br>
例如，对于特征 life_sq，可以看到其最大值是 7478，而上四分位数（数据的第 75 个百分位数据）是 43。因此值 7478 是异常值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">'life_sq'</span>].describe()</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">count    <span class="number">24088.000000</span></span><br><span class="line">mean        <span class="number">34.403271</span></span><br><span class="line">std         <span class="number">52.285733</span></span><br><span class="line">min          <span class="number">0.000000</span></span><br><span class="line"><span class="number">25</span>%         <span class="number">20.000000</span></span><br><span class="line"><span class="number">50</span>%         <span class="number">30.000000</span></span><br><span class="line"><span class="number">75</span>%         <span class="number">43.000000</span></span><br><span class="line">max       <span class="number">7478.000000</span></span><br><span class="line">Name: life_sq, dtype: float64</span><br></pre></td></tr></table></figure>
<h3 id="tiao-xing-tu">条形图</h3>
<p>当特征是<strong>分类变量</strong>时，可以使用条形图来了解其类别和分布。</p>
<p>例如，特征 ecology 具备合理的分布。但如果某个类别「other」仅有一个值，则它就是异常值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># barchart -  distributionofacategoricalvariabledf['ecology'].value_counts().plot.bar()</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/9F67A55F-2E88-4704-87FF-89DB72CE37FB.png" alt></p>
<p>其他方法：还有很多方法可以找出异常值，如散点图、z 分数和聚类。</p>
<h3 id="ru-he-chu-li-yi-chang-zhi">如何处理异常值</h3>
<p>处理异常值的方法与处理缺失值有些类似：要么丢弃，要么修改，要么保留。</p>
<h2 id="bu-bi-yao-shu-ju">不必要数据</h2>
<p>处理异常值的方法与处理缺失值有些类似：要么丢弃，要么修改，要么保留。</p>
<h3 id="bu-bi-yao-shu-ju-lei-xing-1-xin-xi-bu-zu-zhong-fu">不必要数据类型1：信息不足/重复</h3>
<p>有时一个特征不提供信息，是因为它拥有太多具备相同值的行。需要了解重复特征背后的原因。当它们的确无法提供有用信息时，我们就可以丢弃它。</p>
<p>可以为具备高比例相同值的特征创建一个列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_rows = len(df.index)</span><br><span class="line">low_information_cols = [] <span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">    cnts = df[col].value_counts(dropna=<span class="literal">False</span>)</span><br><span class="line">    top_pct = (cnts/num_rows).iloc[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> top_pct &gt; <span class="number">0.95</span>:</span><br><span class="line">        low_information_cols.append(col)</span><br><span class="line">        print(<span class="string">'&#123;0&#125;: &#123;1:.5f&#125;%'</span>.format(col, top_pct*<span class="number">100</span>))</span><br><span class="line">        print(cnts)</span><br><span class="line">        print()</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">oil_chemistry_raion: <span class="number">99.02858</span>%</span><br><span class="line">no     <span class="number">30175</span></span><br><span class="line">yes      <span class="number">296</span></span><br><span class="line">Name: oil_chemistry_raion, dtype: int64</span><br><span class="line"></span><br><span class="line">railroad_terminal_raion: <span class="number">96.27187</span>%</span><br><span class="line">no     <span class="number">29335</span></span><br><span class="line">yes     <span class="number">1136</span></span><br><span class="line">Name: railroad_terminal_raion, dtype: int64</span><br><span class="line"></span><br><span class="line">nuclear_reactor_raion: <span class="number">97.16780</span>%</span><br><span class="line">no     <span class="number">29608</span></span><br><span class="line">yes      <span class="number">863</span></span><br><span class="line">Name: nuclear_reactor_raion, dtype: int64</span><br><span class="line"></span><br><span class="line">big_road1_1line: <span class="number">97.43691</span>%</span><br><span class="line">no     <span class="number">29690</span></span><br><span class="line">yes      <span class="number">781</span></span><br><span class="line">Name: big_road1_1line, dtype: int64</span><br></pre></td></tr></table></figure>
<h3 id="bu-bi-yao-shu-ju-lei-xing-2-bu-xiang-guan">不必要数据类型2：不想关</h3>
<p>数据需要为项目提供有价值的信息。如果特征与项目试图解决的问题无关，则这些特征是不相关数据。当这些特征无法服务于项目目标时，删除之。</p>
<h3 id="bu-bi-yao-shu-ju-lei-xing-3-fu-zhi">不必要数据类型3：复制</h3>
<p>复制数据即，观察值存在副本。复制数据有两个主要类型。</p>
<p>复制数据类型 1：基于所有特征，这种复制发生在观察值内所有特征的值均相同的情况下，很容易找出。</p>
<p>需要先删除数据集中的唯一标识符 id，然后删除复制数据得到数据集 df_dedupped。对比 df 和 df_dedupped 这两个数据集的形态，找出复制行的数量，删除这些复制数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># we know that column 'id' is unique, but what if we drop it?</span></span><br><span class="line">df_dedupped = df.drop(<span class="string">'id'</span>, axis=<span class="number">1</span>).drop_duplicates()</span><br><span class="line"></span><br><span class="line"><span class="comment"># there were duplicate rows</span></span><br><span class="line">print(df.shape)</span><br><span class="line">print(df_dedupped.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">(<span class="number">30471</span>, <span class="number">344</span>)</span><br><span class="line">(<span class="number">30461</span>, <span class="number">343</span>)</span><br><span class="line"><span class="comment"># 可以发现，有 10 行是完全复制的观察值。</span></span><br></pre></td></tr></table></figure>
<p>复制数据类型 2：基于关键特征</p>
<p>例如，相同使用面积、相同价格、相同建造年限的两次房产交易同时发生的概率接近零。</p>
<p>我们可以设置一组关键特征作为唯一标识符，比如 timestamp、full_sq、life_sq、floor、build_year、num_room、price_doc。然后基于这些特征检查是否存在复制数据，删除这些复制数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">key = [<span class="string">'timestamp'</span>, <span class="string">'full_sq'</span>, <span class="string">'life_sq'</span>, <span class="string">'floor'</span>, <span class="string">'build_year'</span>, <span class="string">'num_room'</span>, <span class="string">'price_doc'</span>]</span><br><span class="line">df.fillna(<span class="number">-999</span>).groupby(key)[<span class="string">'id'</span>].count().sort_values(ascending=<span class="literal">False</span>).head(<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">timestamp   full_sq  life_sq  floor  build_year  num_room  price_doc</span><br><span class="line"><span class="number">2014</span><span class="number">-12</span><span class="number">-09</span>  <span class="number">40</span>       <span class="number">-999.0</span>   <span class="number">17.0</span>   <span class="number">-999.0</span>       <span class="number">1.0</span>      <span class="number">4607265</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2014</span><span class="number">-04</span><span class="number">-15</span>  <span class="number">134</span>       <span class="number">134.0</span>   <span class="number">1.0</span>     <span class="number">0.0</span>         <span class="number">3.0</span>      <span class="number">5798496</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2013</span><span class="number">-08</span><span class="number">-30</span>  <span class="number">40</span>       <span class="number">-999.0</span>   <span class="number">12.0</span>   <span class="number">-999.0</span>       <span class="number">1.0</span>      <span class="number">4462000</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2012</span><span class="number">-09</span><span class="number">-05</span>  <span class="number">43</span>       <span class="number">-999.0</span>   <span class="number">21.0</span>   <span class="number">-999.0</span>      <span class="number">-999.0</span>    <span class="number">6229540</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2013</span><span class="number">-12</span><span class="number">-05</span>  <span class="number">40</span>       <span class="number">-999.0</span>   <span class="number">5.0</span>    <span class="number">-999.0</span>       <span class="number">1.0</span>      <span class="number">4414080</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2014</span><span class="number">-12</span><span class="number">-17</span>  <span class="number">62</span>       <span class="number">-999.0</span>   <span class="number">9.0</span>    <span class="number">-999.0</span>       <span class="number">2.0</span>      <span class="number">6552000</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2013</span><span class="number">-05</span><span class="number">-22</span>  <span class="number">68</span>       <span class="number">-999.0</span>   <span class="number">2.0</span>    <span class="number">-999.0</span>      <span class="number">-999.0</span>    <span class="number">5406690</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2012</span><span class="number">-08</span><span class="number">-27</span>  <span class="number">59</span>       <span class="number">-999.0</span>   <span class="number">6.0</span>    <span class="number">-999.0</span>      <span class="number">-999.0</span>    <span class="number">4506800</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2013</span><span class="number">-04</span><span class="number">-03</span>  <span class="number">42</span>       <span class="number">-999.0</span>   <span class="number">2.0</span>    <span class="number">-999.0</span>      <span class="number">-999.0</span>    <span class="number">3444000</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2015</span><span class="number">-03</span><span class="number">-14</span>  <span class="number">62</span>       <span class="number">-999.0</span>   <span class="number">2.0</span>    <span class="number">-999.0</span>       <span class="number">2.0</span>      <span class="number">6520500</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2014</span><span class="number">-01</span><span class="number">-22</span>  <span class="number">46</span>        <span class="number">28.0</span>    <span class="number">1.0</span>     <span class="number">1968.0</span>      <span class="number">2.0</span>      <span class="number">3000000</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2012</span><span class="number">-10</span><span class="number">-22</span>  <span class="number">61</span>       <span class="number">-999.0</span>   <span class="number">18.0</span>   <span class="number">-999.0</span>      <span class="number">-999.0</span>    <span class="number">8248500</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2013</span><span class="number">-09</span><span class="number">-23</span>  <span class="number">85</span>       <span class="number">-999.0</span>   <span class="number">14.0</span>   <span class="number">-999.0</span>       <span class="number">3.0</span>      <span class="number">7725974</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2013</span><span class="number">-06</span><span class="number">-24</span>  <span class="number">40</span>       <span class="number">-999.0</span>   <span class="number">12.0</span>   <span class="number">-999.0</span>      <span class="number">-999.0</span>    <span class="number">4112800</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2015</span><span class="number">-03</span><span class="number">-30</span>  <span class="number">41</span>        <span class="number">41.0</span>    <span class="number">11.0</span>    <span class="number">2016.0</span>      <span class="number">1.0</span>      <span class="number">4114580</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2013</span><span class="number">-12</span><span class="number">-18</span>  <span class="number">39</span>       <span class="number">-999.0</span>   <span class="number">6.0</span>    <span class="number">-999.0</span>       <span class="number">1.0</span>      <span class="number">3700946</span>      <span class="number">2</span></span><br><span class="line"><span class="number">2013</span><span class="number">-08</span><span class="number">-29</span>  <span class="number">58</span>        <span class="number">58.0</span>    <span class="number">13.0</span>    <span class="number">2013.0</span>      <span class="number">2.0</span>      <span class="number">5764128</span>      <span class="number">1</span></span><br><span class="line">            <span class="number">50</span>        <span class="number">33.0</span>    <span class="number">2.0</span>     <span class="number">1972.0</span>      <span class="number">2.0</span>      <span class="number">8150000</span>      <span class="number">1</span></span><br><span class="line">            <span class="number">52</span>        <span class="number">30.0</span>    <span class="number">9.0</span>     <span class="number">2006.0</span>      <span class="number">2.0</span>      <span class="number">10000000</span>     <span class="number">1</span></span><br><span class="line"><span class="number">2013</span><span class="number">-08</span><span class="number">-30</span>  <span class="number">38</span>        <span class="number">17.0</span>    <span class="number">15.0</span>    <span class="number">2004.0</span>      <span class="number">1.0</span>      <span class="number">6400000</span>      <span class="number">1</span></span><br><span class="line">Name: id, dtype: int64</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 找到了 16 条复制数据。</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># drop duplicates based on an subset of variables.</span></span><br><span class="line">key = [<span class="string">'timestamp'</span>, <span class="string">'full_sq'</span>, <span class="string">'life_sq'</span>, <span class="string">'floor'</span>, <span class="string">'build_year'</span>, <span class="string">'num_room'</span>, <span class="string">'price_doc'</span>]</span><br><span class="line">df_dedupped2 = df.drop_duplicates(subset=key)</span><br><span class="line"></span><br><span class="line">print(df.shape)</span><br><span class="line">print(df_dedupped2.shape)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">(<span class="number">30471</span>, <span class="number">344</span>)</span><br><span class="line">(<span class="number">30455</span>, <span class="number">344</span>)</span><br></pre></td></tr></table></figure>
<h2 id="bu-yi-zhi-shu-ju">不一致数据</h2>
<p>在拟合模型时，数据集遵循特定标准也是很重要的一点。我们需要使用不同方式来探索数据，找出不一致数据。大部分情况下，这取决于观察和经验。不存在运行和修复不一致数据的既定代码。</p>
<h3 id="strong-bu-yi-zhi-shu-ju-lei-xing-1-da-xie-strong"><strong>不一致数据类型 1：大写</strong></h3>
<p>在类别值中混用大小写是一种常见的错误。这可能带来一些问题，因为 Python 分析对大小写很敏感。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">'sub_area'</span>].value_counts(dropna=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">Poselenie Sosenskoe               <span class="number">1776</span></span><br><span class="line">Nekrasovka                        <span class="number">1611</span></span><br><span class="line">Poselenie Vnukovskoe              <span class="number">1372</span></span><br><span class="line">Poselenie Moskovskij               <span class="number">925</span></span><br><span class="line">Poselenie Voskresenskoe            <span class="number">713</span></span><br><span class="line">                                  ... </span><br><span class="line">Molzhaninovskoe                      <span class="number">3</span></span><br><span class="line">Poselenie Shhapovskoe                <span class="number">2</span></span><br><span class="line">Poselenie Kievskij                   <span class="number">2</span></span><br><span class="line">Poselenie Mihajlovo-Jarcevskoe       <span class="number">1</span></span><br><span class="line">Poselenie Klenovskoe                 <span class="number">1</span></span><br><span class="line">Name: sub_area, Length: <span class="number">146</span>, dtype: int64</span><br><span class="line"><span class="comment"># 存储了不同地区的名称，看起来非常标准化。但是，有时候相同特征内存在不一致的大小写使用情况。「Poselenie Sosenskoe」和「pOseleNie sosenskeo」指的是相同的地区。</span></span><br><span class="line"><span class="comment"># 为了避免这个问题，可以将所有字母设置为小写（或大写）。</span></span><br><span class="line"><span class="comment"># make everything lower case.</span></span><br><span class="line">df[<span class="string">'sub_area_lower'</span>] = df[<span class="string">'sub_area'</span>].str.lower()</span><br><span class="line">df[<span class="string">'sub_area_lower'</span>].value_counts(dropna=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h3 id="strong-bu-yi-zhi-shu-ju-lei-xing-2-ge-shi-strong"><strong>不一致数据类型 2：格式</strong></h3>
<p>需要执行的另一个标准化是数据格式。比如将特征从字符串格式转换为 DateTime 格式。特征 timestamp 在表示日期时是字符串格式。</p>
<p>处理格式不一致的数据，可以使用以下代码进行格式转换，并提取日期或时间值。然后，就可以很容易地用年或月的方式分析交易量数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">'timestamp_dt'</span>] = pd.to_datetime(df[<span class="string">'timestamp'</span>], format=<span class="string">'%Y-%m-%d'</span>)</span><br><span class="line">df[<span class="string">'year'</span>] = df[<span class="string">'timestamp_dt'</span>].dt.year</span><br><span class="line">df[<span class="string">'month'</span>] = df[<span class="string">'timestamp_dt'</span>].dt.month</span><br><span class="line">df[<span class="string">'weekday'</span>] = df[<span class="string">'timestamp_dt'</span>].dt.weekday</span><br><span class="line"></span><br><span class="line">print(df[<span class="string">'year'</span>].value_counts(dropna=<span class="literal">False</span>))</span><br><span class="line">print()</span><br><span class="line">print(df[<span class="string">'month'</span>].value_counts(dropna=<span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="number">2014</span>    <span class="number">13662</span></span><br><span class="line"><span class="number">2013</span>     <span class="number">7978</span></span><br><span class="line"><span class="number">2012</span>     <span class="number">4839</span></span><br><span class="line"><span class="number">2015</span>     <span class="number">3239</span></span><br><span class="line"><span class="number">2011</span>      <span class="number">753</span></span><br><span class="line">Name: year, dtype: int64</span><br><span class="line"></span><br><span class="line"><span class="number">12</span>    <span class="number">3400</span></span><br><span class="line"><span class="number">4</span>     <span class="number">3191</span></span><br><span class="line"><span class="number">3</span>     <span class="number">2972</span></span><br><span class="line"><span class="number">11</span>    <span class="number">2970</span></span><br><span class="line"><span class="number">10</span>    <span class="number">2736</span></span><br><span class="line"><span class="number">6</span>     <span class="number">2570</span></span><br><span class="line"><span class="number">5</span>     <span class="number">2496</span></span><br><span class="line"><span class="number">9</span>     <span class="number">2346</span></span><br><span class="line"><span class="number">2</span>     <span class="number">2275</span></span><br><span class="line"><span class="number">7</span>     <span class="number">1875</span></span><br><span class="line"><span class="number">8</span>     <span class="number">1831</span></span><br><span class="line"><span class="number">1</span>     <span class="number">1809</span></span><br><span class="line">Name: month, dtype: int64</span><br></pre></td></tr></table></figure>
<h3 id="strong-bu-yi-zhi-shu-ju-lei-xing-3-lei-bie-zhi-strong"><strong>不一致数据类型 3：类别值</strong></h3>
<p>分类特征的值数量有限。有时由于拼写错误等原因可能出现其他值。需要观察特征来找出类别值不一致的情况。</p>
<p>举例来说：city 的值被错误输入为「torontoo」和「tronto」，其实二者均表示「toronto」（正确值）。</p>
<p>识别它们的一种简单方式是模糊逻辑（或编辑距离）。该方法可以衡量使一个值匹配另一个值需要更改的字母数量（距离）。</p>
<p>已知这些类别应仅有四个值：「toronto」、「vancouver」、「montreal」和「calgary」。计算所有值与单词「toronto」（和「vancouver」）之间的距离，我们可以看到疑似拼写错误的值与正确值之间的距离较小，因为它们只有几个字母不同。</p>
<figure class="highlight prolog"><table><tr><td class="code"><pre><span class="line">from nltk.metrics import edit_distance</span><br><span class="line"></span><br><span class="line">df_city_ex = pd.<span class="symbol">DataFrame</span>(data=&#123;<span class="string">'city'</span>: [<span class="string">'torontoo'</span>, <span class="string">'toronto'</span>, <span class="string">'tronto'</span>, <span class="string">'vancouver'</span>, <span class="string">'vancover'</span>, <span class="string">'vancouvr'</span>, <span class="string">'montreal'</span>, <span class="string">'calgary'</span>]&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df_city_ex[<span class="string">'city_distance_toronto'</span>] = df_city_ex[<span class="string">'city'</span>].map(lambda x: edit_distance(x, <span class="string">'toronto'</span>))</span><br><span class="line">df_city_ex[<span class="string">'city_distance_vancouver'</span>] = df_city_ex[<span class="string">'city'</span>].map(lambda x: edit_distance(x, <span class="string">'vancouver'</span>))</span><br><span class="line">df_city_ex</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/image-20200528145223629.png" alt></p>
<p>处理类别值不一致的数据，可以设置标准将这些拼写错误转换为正确值。</p>
<p>例如，下列代码规定所有值与「toronto」的距离在 2 个字母以内。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">msk = df_city_ex[<span class="string">'city_distance_toronto'</span>] &lt;= <span class="number">2</span></span><br><span class="line">df_city_ex.loc[msk, <span class="string">'city'</span>] = <span class="string">'toronto'</span></span><br><span class="line"></span><br><span class="line">msk = df_city_ex[<span class="string">'city_distance_vancouver'</span>] &lt;= <span class="number">2</span></span><br><span class="line">df_city_ex.loc[msk, <span class="string">'city'</span>] = <span class="string">'vancouver'</span></span><br><span class="line"></span><br><span class="line">df_city_ex</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/image-20200528145327127.png" alt></p>
<h3 id="strong-bu-yi-zhi-shu-ju-lei-xing-4-di-zhi-strong"><strong>不一致数据类型 4：地址</strong></h3>
<p>由于人们往数据库中输入数据时通常不会遵循标准格式，所以地址处理是一个比较难的问题。</p>
<p>处理地址不一致的数据，可以用浏览的方式可以找出混乱的地址数据。即便有时我们看不出什么问题，也可以运行代码执行标准化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># no address column in the housing dataset. So create one to show the code.</span></span><br><span class="line">df_add_ex = pd.DataFrame([<span class="string">'123 MAIN St Apartment 15'</span>, <span class="string">'123 Main Street Apt 12   '</span>, <span class="string">'543 FirSt Av'</span>, <span class="string">'  876 FIRst Ave.'</span>], columns=[<span class="string">'address'</span>])</span><br><span class="line">df_add_ex</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/image-20200528150318273.png" alt></p>
<p>处理地址不一致的数据，运行以下代码将所有字母转为小写，删除空格，删除句号，并将措辞标准化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_add_ex[<span class="string">'address_std'</span>] = df_add_ex[<span class="string">'address'</span>].str.lower()</span><br><span class="line">df_add_ex[<span class="string">'address_std'</span>] = df_add_ex[<span class="string">'address_std'</span>].str.strip() <span class="comment"># remove leading and trailing whitespace.</span></span><br><span class="line">df_add_ex[<span class="string">'address_std'</span>] = df_add_ex[<span class="string">'address_std'</span>].str.replace(<span class="string">'\\.'</span>, <span class="string">''</span>) <span class="comment"># remove period.</span></span><br><span class="line">df_add_ex[<span class="string">'address_std'</span>] = df_add_ex[<span class="string">'address_std'</span>].str.replace(<span class="string">'\\bstreet\\b'</span>, <span class="string">'st'</span>) <span class="comment"># replace street with st.</span></span><br><span class="line">df_add_ex[<span class="string">'address_std'</span>] = df_add_ex[<span class="string">'address_std'</span>].str.replace(<span class="string">'\\bapartment\\b'</span>, <span class="string">'apt'</span>) <span class="comment"># replace apartment with apt.</span></span><br><span class="line">df_add_ex[<span class="string">'address_std'</span>] = df_add_ex[<span class="string">'address_std'</span>].str.replace(<span class="string">'\\bav\\b'</span>, <span class="string">'ave'</span>) <span class="comment"># replace apartment with apt.</span></span><br><span class="line"></span><br><span class="line">df_add_ex</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/28/data_preprocessing/image-20200528150416408.png" alt></p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://mp.weixin.qq.com/s/TpiLd94iet902WK6SEjHrQ" target="_blank" rel="noopener">数据缺失、混乱、重复怎么办？最全数据清洗指南让你所向披靡</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>注意力机制</title>
    <url>/2020/03/20/deeplearning/attention/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/03/20/deeplearning/attention/10.11_attention.svg" alt></p>
<a id="more"></a>
<h1 id="zhu-yi-li-ji-zhi">注意力机制</h1>
<p>在seq2seq里，解码器在各个时间步依赖相同的背景变量来获取输入序列信息。当编码器为循环神经网络时，背景变量来自它最终时间步的隐藏状态。</p>
<p>现在，再次思考那一节提到的翻译例子：输入为英语序列“They”“are”“watching”“.”，输出为法语列“Ils”“regardent”“.”。不难想到，解码器在生成输出序列中的每一个词时可能只需利用输入序列某一部分的信息。例如，在输出序列的时间步1，解码器可以主要依赖“They”“are”的信息来生成“Ils”，在时间步2则主要使用来自“watching”的编码信息生成“regardent”，最后在时间步3则直接映射句号“.”。这看上去就像是在解码器的每一时间步对输入序列中不同时间步的表征或编码信息分配不同的注意力一样。这也是注意力机制的由来。</p>
<p>仍然以循环神经网络为例，注意力机制通过对编码器所有时间步的隐藏状态做加权平均来得到背景变量。解码器在每一时间步调整这些权重，即注意力权重，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量。</p>
<p>在seq2seq里区分了输入序列或编码器的索引\(t\)与输出序列或解码器的索引\(t'\)。解码器在时间步\(t'\)的隐藏状态<br>
\[
\boldsymbol{s}_{t'} = g(\boldsymbol{y}_{t'-1}, \boldsymbol{c},\boldsymbol{s}_{t'-1})
\]<br>
，其中\(\boldsymbol{y}_{t'-1}\)是上一时间步\(t'-1\)的输出\(y_{t'-1}\)的表征，且任一时间步\(t'\)使用相同的背景变量\(\boldsymbol{c}\)。但在注意力机制中，解码器的每一时间步将使用可变的背景变量。记\(\boldsymbol{c}_{t'}\)是解码器在时间步\(t'\)的背景变量，那么解码器在该时间步的隐藏状态可以改写为：</p>
<p>\[
\boldsymbol{s}_{t'} = g(\boldsymbol{y}_{t'-1}, \boldsymbol{c}_{t'}, \boldsymbol{s}_{t'-1})
\]</p>
<p>这里的关键是如何计算背景变量\(\boldsymbol{c}_{t'}\)和如何利用它来更新隐藏状态\(\boldsymbol{s}_{t'}\)。下面将分别描述这两个关键点。</p>
<h2 id="ji-suan-bei-jing-bian-liang">计算背景变量</h2>
<p>先描述第一个关键点，即计算背景变量。下图描绘了注意力机制如何为解码器在时间步2计算背景变量。首先，函数\(a\)根据解码器在时间步1的隐藏状态和编码器在各个时间步的隐藏状态计算softmax运算的输入。softmax运算输出概率分布并对编码器各个时间步的隐藏状态做加权平均，从而得到背景变量。</p>
<p><img src="/2020/03/20/deeplearning/attention/10.11_attention.svg" alt="编码器—解码器上的注意力机制"></p>
<p>具体来说，令编码器在时间步\(t\)的隐藏状态为\(\boldsymbol{h}_t\)，且总时间步数为\(T\)。那么解码器在时间步\(t'\)的背景变量为所有编码器隐藏状态的加权平均：</p>
<p>\[
\boldsymbol{c}_{t'} = \sum_{t=1}^T \alpha_{t' t} \boldsymbol{h}_t,
\]</p>
<p>其中给定\(t'\)时，权重\(\alpha_{t' t}\)在\(t=1,\ldots,T\)的值是一个概率分布。为了得到概率分布，可以使用softmax运算:</p>
<p>\[
\alpha_{t' t} = \frac{\exp(e_{t' t})}{ \sum_{k=1}^T \exp(e_{t' k}) },\quad t=1,\ldots,T.
\]</p>
<p>现在，需要定义如何计算上式中softmax运算的输入\(e_{t' t}\)。由于\(e_{t' t}\)同时取决于解码器的时间步\(t'\)和编码器的时间步\(t\)，不妨以解码器在时间步\(t'-1\)的隐藏状态\(\boldsymbol{s}_{t' - 1}\)与编码器在时间步\(t\)的隐藏状态\(\boldsymbol{h}_t\)为输入，并通过函数\(a\)计算\(e_{t' t}\)：</p>
<p>\[
e_{t' t} = a(\boldsymbol{s}_{t' - 1}, \boldsymbol{h}_t).
\]</p>
<p>这里函数\(a\)有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积\(a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}\)。而最早提出注意力机制的论文则将输入连结后通过含单隐藏层的多层感知机变换 [1]：</p>
<p>\[
a(\boldsymbol{s}, \boldsymbol{h}) = \boldsymbol{v}^\top \tanh(\boldsymbol{W}_s \boldsymbol{s} + \boldsymbol{W}_h \boldsymbol{h}),
\]</p>
<p>其中\(\boldsymbol{v}\)、\(\boldsymbol{W}_s\)、\(\boldsymbol{W}_h\)都是可以学习的模型参数。</p>
<h3 id="shi-liang-hua-ji-suan">矢量化计算</h3>
<p>还可以对注意力机制采用更高效的矢量化计算。广义上，注意力机制的输入包括查询项以及一一对应的键项和值项，其中值项是需要加权平均的一组项。在加权平均中，值项的权重来自查询项以及与该值项对应的键项的计算。</p>
<p>在上面的例子中，查询项为解码器的隐藏状态，键项和值项均为编码器的隐藏状态。<br>
考虑一个常见的简单情形，即编码器和解码器的隐藏单元个数均为\(h\)，且函数\(a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}\)。假设我们希望根据解码器单个隐藏状态\(\boldsymbol{s}_{t' - 1} \in \mathbb{R}^{h}\)和编码器所有隐藏状态\(\boldsymbol{h}_t \in \mathbb{R}^{h}, t = 1,\ldots,T\)来计算背景向量\(\boldsymbol{c}_{t'}\in \mathbb{R}^{h}\)。<br>
我们可以将查询项矩阵\(\boldsymbol{Q} \in \mathbb{R}^{1 \times h}\)设为\(\boldsymbol{s}_{t' - 1}^\top\)，并令键项矩阵\(\boldsymbol{K} \in \mathbb{R}^{T \times h}\)和值项矩阵\(\boldsymbol{V} \in \mathbb{R}^{T \times h}\)相同且第\(t\)行均为\(\boldsymbol{h}_t^\top\)。此时，我们只需要通过矢量化计算</p>
<p>\[
\boldsymbol{c}_{t'}^\top =\text{softmax}(\boldsymbol{Q}\boldsymbol{K}^\top)\boldsymbol{V}
\]</p>
<p>即可算出转置后的背景向量\(\boldsymbol{c}_{t'}^\top\)。当查询项矩阵\(\boldsymbol{Q}\)的行数为\(n\)时，上式将得到\(n\)行的输出矩阵。输出矩阵与查询项矩阵在相同行上一一对应。</p>
<h2 id="geng-xin-yin-cang-zhuang-tai">更新隐藏状态</h2>
<p>现在描述第二个关键点，即更新隐藏状态。以门控循环单元为例，在解码器中我们可以对GRU中门控循环单元的设计稍作修改，从而变换上一时间步\(t'-1\)的输出\(\boldsymbol{y}_{t'-1}\)、隐藏状态\(\boldsymbol{s}_{t' - 1}\)和当前时间步\(t'\)的含注意力机制的背景变量\(\boldsymbol{c}_{t'}\) [1]。解码器在时间步\(t'\)的隐藏状态为</p>
<p>\[
\boldsymbol{s}_{t'} = \boldsymbol{z}_{t'} \odot \boldsymbol{s}_{t'-1}  + (1 - \boldsymbol{z}_{t'}) \odot \tilde{\boldsymbol{s}}_{t'}
\]</p>
<p>其中的重置门、更新门和候选隐藏状态分别为</p>
<p>\[
\begin{aligned}
\boldsymbol{r}_{t'} &amp;= \sigma(\boldsymbol{W}_{yr} \boldsymbol{y}_{t'-1} + \boldsymbol{W}_{sr} \boldsymbol{s}_{t' - 1} + \boldsymbol{W}_{cr} \boldsymbol{c}_{t'} + \boldsymbol{b}_r),\\
\boldsymbol{z}_{t'} &amp;= \sigma(\boldsymbol{W}_{yz} \boldsymbol{y}_{t'-1} + \boldsymbol{W}_{sz} \boldsymbol{s}_{t' - 1} + \boldsymbol{W}_{cz} \boldsymbol{c}_{t'} + \boldsymbol{b}_z),\\
\tilde{\boldsymbol{s}}_{t'} &amp;= \text{tanh}(\boldsymbol{W}_{ys} \boldsymbol{y}_{t'-1} + \boldsymbol{W}_{ss} (\boldsymbol{s}_{t' - 1} \odot \boldsymbol{r}_{t'}) + \boldsymbol{W}_{cs} \boldsymbol{c}_{t'} + \boldsymbol{b}_s),
\end{aligned}
\]</p>
<p>其中含下标的\(\boldsymbol{W}\)和\(\boldsymbol{b}\)分别为门控循环单元的权重参数和偏差参数。</p>
<h2 id="fa-zhan">发展</h2>
<p>本质上，注意力机制能够为表征中较有价值的部分分配较多的计算资源。这个有趣的想法自提出后得到了快速发展，特别是启发了依靠注意力机制来编码输入序列并解码出输出序列的变换器（Transformer）模型的设计 [2]。变换器抛弃了卷积神经网络和循环神经网络的架构。它在计算效率上比基于循环神经网络的编码器—解码器模型通常更具明显优势。含注意力机制的变换器的编码结构在后来的BERT预训练模型中得以应用并令后者大放异彩：微调后的模型在多达11项自然语言处理任务中取得了当时最先进的结果 [3]。不久后，同样是基于变换器设计的GPT-2模型于新收集的语料数据集预训练后，在7个未参与训练的语言模型数据集上均取得了当时最先进的结果 [4]。除了自然语言处理领域，注意力机制还被广泛用于图像分类、自动图像描述、唇语解读以及语音识别。</p>
<h1 id="zong-jie">总结</h1>
<ul>
<li>可以在解码器的每个时间步使用不同的背景变量，并对输入序列中不同时间步编码的信息分配不同的注意力。</li>
<li>广义上，注意力机制的输入包括查询项以及一一对应的键项和值项。</li>
<li>注意力机制可以采用更为高效的矢量化计算。</li>
</ul>
<h1 id="can-kao-wen-xian">参考文献</h1>
<p>[1] Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</p>
<p>[2] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems (pp. 5998-6008).</p>
<p>[3] Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>[4] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever I. (2019). Language Models are Unsupervised Multitask Learners. OpenAI.</p>
]]></content>
      <categories>
        <category>技术/深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>反向传播</title>
    <url>/2020/03/16/deeplearning/backprop/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/03/16/deeplearning/backprop/3.14_forward.svg" alt></p>
<a id="more"></a>
<h1 id="zheng-xiang-chuan-bo">正向传播</h1>
<p>正向传播是指对神经网络沿着从输入层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）。为简单起见，假设输入是一个特征为\(\boldsymbol{x} \in \mathbb{R}^d\)的样本，且不考虑偏差项，那么中间变量</p>
<p>\[
\boldsymbol{z} = \boldsymbol{W}^{(1)} \boldsymbol{x}
\]</p>
<p>其中\(\boldsymbol{W}^{(1)} \in \mathbb{R}^{h \times d}\)是隐藏层的权重参数。把中间变量\(\boldsymbol{z} \in \mathbb{R}^h\)输入按元素运算的激活函数\(\phi\)后，将得到向量长度为\(h\)的隐藏层变量</p>
<p>\[
\boldsymbol{h} = \phi (\boldsymbol{z})
\]</p>
<p>隐藏层变量\(\boldsymbol{h}\)也是一个中间变量。假设输出层参数只有权重\(\boldsymbol{W}^{(2)} \in \mathbb{R}^{q \times h}\)，可以得到向量长度为\(q\)的输出层变量</p>
<p>\[
\boldsymbol{o} = \boldsymbol{W}^{(2)} \boldsymbol{h}
\]</p>
<p>假设损失函数为\(\ell\)，且样本标签为\(y\)，可以计算出单个数据样本的损失项</p>
<p>\[
L = \ell(\boldsymbol{o}, y)
\]</p>
<p>根据\(L_2\)范数正则化的定义，给定超参数\(\lambda\)，正则化项即</p>
<p>\[
s = \frac{\lambda}{2} \left(\|\boldsymbol{W}^{(1)}\|_F^2 + \|\boldsymbol{W}^{(2)}\|_F^2\right)
\]</p>
<p>其中矩阵的Frobenius范数等价于将矩阵变平为向量后计算\(L_2\)范数。最终，模型在给定的数据样本上带正则化的损失为</p>
<p>\[
J = L + s
\]</p>
<p>将\(J\)称为有关给定数据样本的目标函数，并在以下的讨论中简称目标函数。</p>
<h2 id="zheng-xiang-chuan-bo-de-ji-suan-tu">正向传播的计算图</h2>
<p>通常绘制计算图来可视化运算符和变量在计算中的依赖关系。下图绘制了样例模型正向传播的计算图，其中左下角是输入，右上角是输出。可以看到，图中箭头方向大多是向右和向上，其中方框代表变量，圆圈代表运算符，箭头表示从输入到输出之间的依赖关系。</p>
<p><img src="/2020/03/16/deeplearning/backprop/3.14_forward.svg" alt></p>
<h1 id="fan-xiang-chuan-bo">反向传播</h1>
<p>反向传播指的是计算神经网络参数梯度的方法。总的来说，反向传播依据微积分中的链式法则，沿着从输出层到输入层的顺序，依次计算并存储目标函数有关神经网络各层的中间变量以及参数的梯度。对输入或输出\(\mathsf{X}, \mathsf{Y}, \mathsf{Z}\)为任意形状张量的函数\(\mathsf{Y}=f(\mathsf{X})\)和\(\mathsf{Z}=g(\mathsf{Y})\)，通过链式法则，有</p>
<p>\[
\frac{\partial \mathsf{Z}}{\partial \mathsf{X}} = \text{prod}\left(\frac{\partial \mathsf{Z}}{\partial \mathsf{Y}}, \frac{\partial \mathsf{Y}}{\partial \mathsf{X}}\right)
\]</p>
<p>其中\(\text{prod}\)运算符将根据两个输入的形状，在必要的操作（如转置和互换输入位置）后对两个输入做乘法。</p>
<p>回顾一下样例模型，它的参数是\(\boldsymbol{W}^{(1)}\)和\(\boldsymbol{W}^{(2)}\)，因此反向传播的目标是计算\(\partial J/\partial \boldsymbol{W}^{(1)}\)和\(\partial J/\partial \boldsymbol{W}^{(2)}\)。应用链式法则依次计算各中间变量和参数的梯度，其计算次序与前向传播中相应中间变量的计算次序恰恰相反。首先，分别计算目标函数\(J=L+s\)有关损失项\(L\)和正则项\(s\)的梯度</p>
<p>\[
\frac{\partial J}{\partial L} = 1, \quad \frac{\partial J}{\partial s} = 1
\]</p>
<p>其次，依据链式法则计算目标函数有关输出层变量的梯度\(\partial J/\partial \boldsymbol{o} \in \mathbb{R}^q\)：</p>
<p>\[
\frac{\partial J}{\partial \boldsymbol{o}}
= \text{prod}\left(\frac{\partial J}{\partial L}, \frac{\partial L}{\partial \boldsymbol{o}}\right)
= \frac{\partial L}{\partial \boldsymbol{o}}.
\]</p>
<p>接下来，计算正则项有关两个参数的梯度：</p>
<p>\[
\frac{\partial s}{\partial \boldsymbol{W}^{(1)}} = \lambda \boldsymbol{W}^{(1)},\quad\frac{\partial s}{\partial \boldsymbol{W}^{(2)}} = \lambda \boldsymbol{W}^{(2)}
\]</p>
<p>现在，可以计算最靠近输出层的模型参数的梯度\(\partial J/\partial \boldsymbol{W}^{(2)} \in \mathbb{R}^{q \times h}\)。依据链式法则，得到</p>
<p>\[
\frac{\partial J}{\partial \boldsymbol{W}^{(2)}}
= \text{prod}\left(\frac{\partial J}{\partial \boldsymbol{o}}, \frac{\partial \boldsymbol{o}}{\partial \boldsymbol{W}^{(2)}}\right) + \text{prod}\left(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \boldsymbol{W}^{(2)}}\right)
= \frac{\partial J}{\partial \boldsymbol{o}} \boldsymbol{h}^\top + \lambda \boldsymbol{W}^{(2)}.
\]</p>
<p>沿着输出层向隐藏层继续反向传播，隐藏层变量的梯度\(\partial J/\partial \boldsymbol{h} \in \mathbb{R}^h\)可以这样计算：</p>
<p>\[
\frac{\partial J}{\partial \boldsymbol{h}}
= \text{prod}\left(\frac{\partial J}{\partial \boldsymbol{o}}, \frac{\partial \boldsymbol{o}}{\partial \boldsymbol{h}}\right)
= {\boldsymbol{W}^{(2)}}^\top \frac{\partial J}{\partial \boldsymbol{o}}.
\]</p>
<p>由于激活函数\(\phi\)是按元素运算的，中间变量\(\boldsymbol{z}\)的梯度\(\partial J/\partial \boldsymbol{z} \in \mathbb{R}^h\)的计算需要使用按元素乘法符\(\odot\)：</p>
<p>\[
\frac{\partial J}{\partial \boldsymbol{z}}
= \text{prod}\left(\frac{\partial J}{\partial \boldsymbol{h}}, \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}\right)
= \frac{\partial J}{\partial \boldsymbol{h}} \odot \phi'\left(\boldsymbol{z}\right).
\]</p>
<p>最终，可以得到最靠近输入层的模型参数的梯度\(\partial J/\partial \boldsymbol{W}^{(1)} \in \mathbb{R}^{h \times d}\)。依据链式法则，得到</p>
<p>\[
\frac{\partial J}{\partial \boldsymbol{W}^{(1)}}
= \text{prod}\left(\frac{\partial J}{\partial \boldsymbol{z}}, \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}^{(1)}}\right) + \text{prod}\left(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \boldsymbol{W}^{(1)}}\right)
= \frac{\partial J}{\partial \boldsymbol{z}} \boldsymbol{x}^\top + \lambda \boldsymbol{W}^{(1)}.
\]</p>
<h1 id="xun-lian-shen-du-xue-xi-mo-xing">训练深度学习模型</h1>
<p>在训练深度学习模型时，正向传播和反向传播之间相互依赖。</p>
<p>一方面，正向传播的计算可能依赖于模型参数的当前值，而这些模型参数是在反向传播的梯度计算后通过优化算法迭代的。例如，计算正则化项\(s = (\lambda/2) \left(\|\boldsymbol{W}^{(1)}\|_F^2 + \|\boldsymbol{W}^{(2)}\|_F^2\right)\)依赖模型参数\(\boldsymbol{W}^{(1)}\)和\(\boldsymbol{W}^{(2)}\)的当前值，而这些当前值是优化算法最近一次根据反向传播算出梯度后迭代得到的。</p>
<p>另一方面，反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的。举例来说，参数梯度\(\partial J/\partial \boldsymbol{W}^{(2)} = (\partial J / \partial \boldsymbol{o}) \boldsymbol{h}^\top + \lambda \boldsymbol{W}^{(2)}\)的计算需要依赖隐藏层变量的当前值\(\boldsymbol{h}\)。这个当前值是通过从输入层到输出层的正向传播计算并存储得到的。</p>
<p>因此，在模型参数初始化完成后，交替地进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。既然在反向传播中使用了正向传播中计算得到的中间变量来避免重复计算，那么这个复用也导致正向传播结束后不能立即释放中间变量内存。这也是训练要比预测占用更多内存的一个重要原因。这些中间变量的个数大体上与网络层数线性相关，每个变量的大小跟批量大小和输入个数也是线性相关的，它们是导致较深的神经网络使用较大批量训练时更容易超内存的主要原因。</p>
<h1 id="zong-jie">总结</h1>
<ol>
<li>
<p>正向传播沿着从输入层到输出层的顺序，依次计算并存储神经网络的中间变量。</p>
</li>
<li>
<p>反向传播沿着从输出层到输入层的顺序，依次计算并存储神经网络中间变量和参数的梯度。</p>
</li>
<li>
<p>在训练深度学习模型时，正向传播和反向传播相互依赖。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>技术/深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>评价指标</title>
    <url>/2020/03/13/evaluation/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/03/13/evaluation/v2-9edc860c849ab1737d76d7ee138b31cd_1440w.jpg" alt></p>
<a id="more"></a>
<h1 id="fen-lei-ping-jie-zhi-biao">分类评价指标</h1>
<blockquote>
<p>类别：</p>
<p>​    实际：A  A  A  A  B  B  B  C  C</p>
<p>​    预测：A  A  B  C  B  B  C  B  C</p>
<p>​    Index:  1   2  3  4  5  6  7  8  9</p>
</blockquote>
<h2 id="accuracy">Accuracy</h2>
<p>预测正确的样本的占总样本的比例，取值范围为[0,1]，取值越大，模型预测能力越好。</p>
<p>\[
acc = \frac{true}{all}
\]</p>
<blockquote>
<p>Accuracy的缺陷</p>
<ul>
<li>
<p>对于有倾向性的问题，往往不能用精度指标来衡量。</p>
<p>比如，判断空中的飞行物是导弹还是其他飞行物，很显然为了减少损失，我们更倾向于相信是导弹而采用相应的防护措施。此时判断为导弹实际上是其他飞行物与判断为其他飞行物实际上是导弹这两种情况的重要性是不一样的；</p>
</li>
<li>
<p>对于样本类别数量严重不均衡的情况，也不能用精度指标来衡量。</p>
<p>比如银行客户样本中好客户990个，坏客户10个。如果一个模型直接把所有客户都判断为好客户，得到精度为99%，但这显然是没有意义的。</p>
</li>
</ul>
</blockquote>
<h2 id="precison-recall-f">Precison、Recall、F</h2>
<p>首先看真阳性(TP)：真阳性的定义是“预测为正，实际也是正”，就是指预测正确，是哪个类就被分到哪个类。对类A而言，TP的个位数为2，对类B而言，TP的个数为2，对类C而言，TP的个数为1。</p>
<p>然后看假阳性(FP)，假阳性的定义是“预测为正，实际为负”，就是预测为某个类，但是实际不是。对类A而言，FP个数为0，我们预测之后，把1和2分给了A，这两个都是正确的，并不存在把不是A类的值分给A的情况。类B的FP是2，&quot;3&quot;和&quot;8&quot;都不是B类，但却分给了B，所以为假阳性。类C的假阳性个数为2。</p>
<blockquote>
<p>假阳性 = 该类别下的样本总数 - 真阳性</p>
</blockquote>
<p>最后看一下假阴性(FN)，假阴性的定义是“预测为负，实际为正”，对类A而言，FN为2，&quot;3&quot;和&quot;4&quot;分别预测为B和C，但是实际是A，也就是预测为负，实际为正。对类B而言，FN为1，对类C而言，FN为1。</p>
<table>
<thead>
<tr>
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>总计</th>
</tr>
</thead>
<tbody>
<tr>
<td>TP</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>FP</td>
<td>0</td>
<td>2</td>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>FN</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>\[
P = \frac{TP}{TP+FP}\\
R = \frac{TP}{TP+FN}\\
F_1 = \frac{2 * P * R}{P + R}\\
F_\beta = \frac{(1+\beta^2)*P*R}{(\beta^2*P+R)}
\]</p>
<blockquote>
<p>除了F1分数之外，F0.5分数和F2分数，在统计学中也得到了大量应用。</p>
<p>其中，F2分数中，召回率的权重高于精确率，而F0.5分数中，精确率的权重高于召回率。</p>
</blockquote>
<h2 id="micro-f-1-he-macro-f-1">Micro-F1和Macro-F1</h2>
<p>在第一个多标签分类任务中，可以对每个“类”，计算F1，显然我们需要把所有类的F1合并起来考虑。</p>
<h3 id="micro-f-1">Micro-F1</h3>
<p>计算出所有类别总的Precision和Recall，然后计算F1。</p>
<p>依照上面的表格来计算:</p>
<blockquote>
<p>\(Precison=\frac{5}{5+4}=0.556\)</p>
<p>\(Recall=\frac{5}{5+4}=0.556\)</p>
<p>\(F_1 = \frac{2 * 0.556 * 0.556}{0.556+0.556}=0.556\)</p>
<p>求出F1，这种方式被称为Micro-F1(微平均)。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_true = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line">y_pred = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>]</span><br><span class="line">print(f1_score(y_true,y_pred,labels=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],average=<span class="string">'micro'</span>))</span><br></pre></td></tr></table></figure>
<h3 id="macro-f-1">Macro-F1</h3>
<p>计算出每一个类的Precison和Recall后计算F1，最后将F1平均。</p>
<p>例如上式</p>
<blockquote>
<p>A类：</p>
<p>\(P=\frac{2}{2+0}=1.0\)</p>
<p>\(R=\frac{2}{2+2}=0.5\)</p>
<p>\(F1=\frac{2*1*0.5}{1+0.5}=0.667\)</p>
<p>B类：</p>
<p>\(P=\frac{2}{2+2}=0.5\)</p>
<p>\(R=\frac{2}{2+1}=0.667\)</p>
<p>\(F1=\frac{2*0.5*0.667}{0.5+0.667}=0.571\)</p>
<p>C类：</p>
<p>\(P=\frac{1}{1+2}=0.333\)</p>
<p>\(R=\frac{1}{1+1}=0.5\)</p>
<p>\(F1=\frac{2*0.333*0.5}{0.333+0.5}=0.399\)</p>
<p>\(F_{macro} = \frac{0.667+0.571+0.399}{3} = 0.545\)<br>
最后求平均值，这种范式叫做Macro-F1宏平均。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#average=None,取出每一类的P,R,F1值</span></span><br><span class="line">p_class, r_class, f_class, support_micro=precision_recall_fscore_support(y_true=y_true, y_pred=y_pred, labels=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], average=<span class="literal">None</span>)</span><br><span class="line">print(<span class="string">'各类单独F1:'</span>,f_class)</span><br><span class="line">print(<span class="string">'各类F1取平均：'</span>,f_class.mean())</span><br><span class="line">print(f1_score(y_true,y_pred,labels=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],average=<span class="string">'macro'</span>))</span><br></pre></td></tr></table></figure>
<h2 id="roc-he-auc">ROC 和 AUC</h2>
<p>AUC是一种模型分类指标，且仅仅是二分类模型的评价指标。AUC是Area Under Curve的简称，那么Curve就是 ROC。也就是说ROC是一条曲线，AUC是 一个面积值。</p>
<h3 id="roc">ROC</h3>
<p>ROC曲线为 FPR 与 TPR 之间的关系曲线，这个组合以 FPR 对 TPR，即是以代价对收益，显然收益越高，代价越低，模型的性能就越好。</p>
<blockquote>
<p>x 轴为假阳性率（FPR）：\(F P R=\frac{F P}{F P+T N}\)，在所有的负样本中，分类器<strong>预测错误的比例</strong></p>
<p>y 轴为真阳性率（TPR）：\(T P R=\frac{T P}{T P+F N}\)，在所有的正样本中，分类器<strong>预测正确的比例</strong>（等于Recall）</p>
</blockquote>
<p>以FPR为横轴，TPR为纵轴，得到如下ROC空间</p>
<p><img src="/2020/03/13/evaluation/v2-de51547b4b0ab69983515d2f97c45f22_1440w.jpg" alt></p>
<p>可以看出，左上角的点(TPR=1，FPR=0)，为完美分类，全对。点A(TPR&gt;FPR),判断大体是正确的。中线上的点B(TPR=FPR),全都是蒙的，蒙对一半，蒙错一半；下半平面的点C(TPR&lt;FPR)和上半平面的点C相同，把类别换一下即可。上图中一个阈值，得到一个点，遍历所有的阈值,得到 ROC 曲线。</p>
<p>上图(蓝色)为负例，下图(红色)为正例模型输出概率分布图（横坐标表示模型输出概率，纵坐标表示概率对应的样本数量），显然正例样本概率值普遍低于负例样本的输出概率值。</p>
<p>竖线代表阈值，显然，图中给出了某个阈值对应的混淆矩阵，通过改变不同的阈值 ，得到一系列的混淆矩阵，进而得到一系列的TPR和FPR，绘制出ROC曲线。</p>
<p><img src="/2020/03/13/evaluation/v2-9edc860c849ab1737d76d7ee138b31cd_1440w.jpg" alt></p>
<h4 id="micro">micro</h4>
<p>\[
\begin{array}{l}
T P R=\frac{TP_1+TP_2+TP_3}{TP_1+FN_1+TP_2+FN_2+TP_3+FN_3} \\
F P R=\frac{FP_1+FP_2+FP_3}{FP_1+TN_1+FP_2+TN_2+FP_3+TN_3}
\end{array}
\]</p>
<h4 id="macro">macro</h4>
<p>\[
\begin{array}{l}
T P R=\frac{1}{3}\left(\frac{TP_1}{TP_1+FN_1}+\frac{TP_2}{TP_2+FN_2}+\frac{TP_3}{TP_3+FN_3}\right) \\
F P R=\frac{1}{3}\left(\frac{FP_1}{FP_1+TN_1}+\frac{FP_2}{FP_2+TN_2}+\frac{FP_3}{FP_3+TN_3}\right)
\end{array}
\]</p>
<h4 id="weight">weight</h4>
<p>\[
\begin{array}{l}
T P R=\frac{TP_1}{TP_1+FN_1} w_{1}+\frac{TP_2}{TP_2+FN_2} w_{2}+\frac{TP_3}{TP_3+FN_3} w_{3} \\
F P R=\frac{FP_1}{FP_1+TN_1} w_{1}+\frac{FP_2}{FP_2+TN_2} w_{2}+\frac{FP_3}{FP_3+TN_3} w_{3}
\end{array}
\]</p>
<h4 id="sample">sample</h4>
<p>对于样本很不均匀的类，可以采用该方法。</p>
<h3 id="auc">AUC</h3>
<p>定义：AUC 值为 ROC 曲线所覆盖的<strong>区域面积</strong>，显然，AUC越大，分类器分类效果越好。</p>
<blockquote>
<ul>
<li>AUC = 1，是完美分类器。</li>
<li>0.5 &lt; AUC &lt; 1，优于随机猜测。有预测价值。</li>
<li>AUC = 0.5，跟随机猜测一样（例：丢铜板），没有预测价值。</li>
<li>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。</li>
</ul>
</blockquote>
<p>以下为ROC曲线和AUC值的实例：</p>
<p><img src="/2020/03/13/evaluation/v2-18db5822a9accd275adf89ca3fa2d3cf_1440w.jpg" alt></p>
<p><strong>AUC的物理意义</strong></p>
<p>AUC的物理意义正样本的预测结果大于负样本的预测结果的概率。所以AUC反映的是分类器对样本的排序能力。值得注意的是，AUC对样本类别是否均衡并不敏感，这也是不均衡样本通常用AUC评价分类器性能的一个原因。</p>
<p>AUC只与概率的相对大小（概率排序）有关，和绝对大小没关系。</p>
<blockquote>
<p><strong>为什么说 ROC 和AUC都能应用于非均衡的分类问题？</strong></p>
<p>ROC曲线只与横坐标 (FPR) 和 纵坐标 (TPR) 有关系 。我们可以发现TPR只是正样本中预测正确的概率，而FPR只是负样本中预测错误的概率，和正负样本的比例没有关系。因此 ROC 的值与实际的正负样本比例无关，因此既可以用于均衡问题，也可以用于非均衡问题。而 AUC 的几何意义为ROC曲线下的面积，因此也和实际的正负样本比例无关。</p>
</blockquote>
<h3 id="dai-ma">代码</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="comment"># 二分类</span></span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">scores = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.35</span>, <span class="number">0.8</span>])</span><br><span class="line">fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># fpr</span></span><br><span class="line">array([ <span class="number">0.</span> ,  <span class="number">0.5</span>,  <span class="number">0.5</span>,  <span class="number">1.</span> ])</span><br><span class="line"><span class="comment"># tpr</span></span><br><span class="line">array([ <span class="number">0.5</span>,  <span class="number">0.5</span>,  <span class="number">1.</span> ,  <span class="number">1.</span> ])</span><br><span class="line"><span class="comment"># thresholds</span></span><br><span class="line">array([ <span class="number">0.8</span> ,  <span class="number">0.4</span> ,  <span class="number">0.35</span>,  <span class="number">0.1</span> ])</span><br><span class="line"></span><br><span class="line">auc = metrics.auc(fpr, tpr) <span class="comment">#0.75</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多分类</span></span><br><span class="line">fpr, tpr, thresholds = metrics.roc_auc_score(y_true, y_scores, multi_class=<span class="string">'ovo'</span>,labels=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],average=<span class="string">'macro'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://blog.csdn.net/sinat_28576553/article/details/80258619" target="_blank" rel="noopener">分类问题的几个评价指标（Precision、Recall、F1-Score、Micro-F1、Macro-F1）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/36305931" target="_blank" rel="noopener">机器学习评估指标</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>机器学习大总结</title>
    <url>/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="ji-qi-xue-xi-da-zong-jie">机器学习大总结</h2>
<h3 id="zhi-shi-dian">知识点</h3>
<h4 id="strong-jin-cheng-he-xian-cheng-strong"><strong>进程和线程</strong></h4>
<p>进程和线程都是一个时间段的描述，是CPU工作时间段的描述，不过是颗粒大小不同.进程就是包换上下文切换的程序执行时间总和 = CPU加载上下文+CPU执行+CPU保存上下文.线程是共享了进程的上下文环境的更为细小的CPU时间段。</p>
<h4 id="strong-pan-bie-shi-mo-xing-he-sheng-cheng-shi-mo-xing-strong"><strong>判别式模型和生成式模型</strong>:</h4>
<ol>
<li>判别式模型直接学习决策函数\(f(X)\)或条件概率分布\(P(Y|X)\)作为预测的模型.往往准确率更高,并且可以简化学习问题.
<ol>
<li>k近邻法、感知机、决策树、最大熵模型、Logistic回归、线性判别分析(LDA)、支持向量机(SVM)、Boosting、CRF、线性回归、神经网络</li>
</ol>
</li>
<li>生成式模型由数据学习<strong>联合概率分布P(X,Y)</strong>,然后由P(Y|X)=P(X,Y)/P(X)求出条件概率分布作为预测的模型,即生成模型.当存在隐变量时只能用生成方法学习.
<ol>
<li>混合高斯模型和其他混合模型、隐马尔可夫模型(HMM)、朴素贝叶斯、依赖贝叶斯(AODE)、LDA文档主题生成模型</li>
</ol>
</li>
</ol>
<a id="more"></a>
<h4 id="strong-gai-lu-zhi-liang-han-shu-gai-lu-mi-du-han-shu-lei-ji-fen-bu-han-shu-strong"><strong>概率质量函数,概率密度函数,累积分布函数</strong>:</h4>
<ol>
<li>概率质量函数 (probability mass function，PMF)是离散随机变量在各特定取值上的概率。</li>
<li>概率密度函数（probability density function，PDF)是对 连续随机变量 定义的，本身不是概率，只有对连续随机变量的取值进行积分后才是概率。</li>
<li>累积分布函数（cumulative distribution function，CDF)能完整描述一个实数随机变量X的概率分布，是概率密度函数的积分。对於所有实数x ，与pdf相对。</li>
</ol>
<h4 id="strong-ji-da-si-ran-gu-ji-strong"><strong>极大似然估计</strong></h4>
<p>已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。</p>
<h4 id="zui-xiao-er-cheng-fa">最小二乘法</h4>
<p>二乘的英文是least square,找一个（组）估计值,使得实际值与估计值之差的平方加总之后的值最小\(Q=\min \sum_{i}^{n}\left(y_{i e}-y_{i}\right)^{2}\).求解方式是对参数求偏导,令偏导为0即可.样本量小时速度快.</p>
<h4 id="strong-ti-du-xia-jiang-fa-strong"><strong>梯度下降法</strong></h4>
<p>负梯度方向是函数值下降最快的方向,每次更新值都等于原值加学习率(<strong>步长</strong>)乘损失函数的<strong>梯度</strong>.每次都试一个步长看会不会下降一定的程度,如果没有的话就按比例减小步长.不断应用参数更新公式直到收敛,可以得到局部最小值.初始值的不同组合可以得到不同局部最小值.在最优点时会有震荡.</p>
<ol>
<li>
<p><strong>批量梯度下降(BGD)</strong>:每次都使用所有的m个样本来更新,容易找到全局最优解,但是m较大时速度较慢。\(\theta_{j}^{\prime}=\theta_{j}+\frac{1}{m} \sum_{i=1}^{m}\left(y^{i}-h_{\theta}\left(x^{i}\right)\right) x_{j}^{i}\)</p>
</li>
<li>
<p><strong>随机梯度下降(SGD)</strong>：每次只使用一个样本来更新,训练速度快,但是噪音较多,不容易找到全局最优解,以损失很小的一部分精确度和增加一定数量的迭代次数为代价,换取了总体的优化效率的提升.注意控制步长缩小,减少震荡.</p>
</li>
</ol>
<p>\[
\theta_{j}=\theta_{j}+\left(y^{i}-h_{\theta}\left(x^{i}\right)\right) x_{j}^{i}
\]</p>
<ol start="3">
<li><strong>小批量梯度下降(MBGD)</strong>:每次使用一部分样本来更新.</li>
</ol>
<h4 id="strong-niu-dun-fa-strong"><strong>牛顿法</strong></h4>
<p>牛顿法是<strong>二次收敛</strong>,因此收敛速度快.从几何上看是每次用一个二次曲面来拟合当前所处位置的局部曲面,而梯度下降法是用一个平面来拟合.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/newton.png" alt="avatar"></p>
<ol>
<li><strong>黑塞矩阵</strong>是由目标函数f(x)在点X处的二阶偏导数组成的n*n阶对称矩阵。</li>
<li><strong>牛顿法</strong>:将f(x)在x(k)附近进行<strong>二阶泰勒展开</strong>:\(f(x)=f\left(x^{(k)}\right)+g_{k}^{\top}\left(x-x^{(k)}\right)+\frac{1}{2}\left(x-x^{(k)}\right)^{\mathrm{T}} H\left(x^{(k)}\right)\left(x-x^{(k)}\right)\)。其中\(g_k\)是\(f(x)\)的梯度向量在\(x^{(k)}\)的值,\(H(x^{(k)})\)是\(f(x)\)的黑塞矩阵在点\(x^k\)的值.牛顿法利用极小点的必要条件\(f(x)\)处的梯度为0,每次迭代中从点\(x^{(k)}\)开始,假设\(\nabla f\left(x^{(k+1)}\right)=0\)，对二阶泰勒展开求偏导有 \(\nabla f(x)=g_{k}+H_{k}\left(x-x^{(k)}\right)\)，代入得到\(g_{k}+H_{k}\left(x^{(k+1)}-x^{(k)}\right)=0\),即\(\quad x^{(k+1)}=x^{(k)}-H_{k}^{-1} g_{k}\),以此为迭代公式就是牛顿法.</li>
</ol>
<h4 id="ni-niu-dun-fa">拟牛顿法</h4>
<p>用一个<strong>n阶正定矩阵Gk</strong>=G(x(k))来<strong>近似代替</strong>黑塞矩阵的<strong>逆矩阵</strong>就是拟牛顿法的基本思想.在牛顿法中黑塞矩阵满足的条件如下:\(g_{k+1}-g_{k}=H_{k}\left(x^{(k+1)}-x^{(k)}\right)\),令$ {y_{k}}=g_{k+1}-g_{k}, \quad \delta_{k}=x<sup>{(k+1)}-x</sup>{(k)}$，则有  $ H_{k}^{-1} y_{k}=\delta_{k}$,称为拟牛顿条件.</p>
<ol>
<li><strong>DFP算法</strong>:假设每一步\(G_{k+1}=G_{k}+P_{k}+Q_{k}\)，为使\(G_{k+1}\)满足拟牛顿条件,可使\(P_k\)和\(Q_k\)满足\(P_{k} y_{k}=\delta_{k}, Q_{k} y_{k}=-G_{k} y_{k}\),例如取\(P_{k}=\frac{\delta_{k} \delta_{k}^{\tau}}{\delta_{k}^{\top} y_{k}} \quad Q_{k}=-\frac{G_{k} y_{k} y_{k}^{\top} G_{k}}{y_{k}^{\top} G_{k} y_{k}}\)，就得到迭代公式\(G_{k+1}=G_{k}+\frac{\delta_{k} \delta_{k}^{\top}}{\delta_{k}^{\tau} y_{k}}-\frac{G_{k} y_{k} y_{k}^{\top} G_{k}}{y_{k}^{T} G_{k} y_{k}}\)</li>
<li><strong>BFGS算法</strong>: 最流行的拟牛顿算法.它用\(B_k\)逼近黑塞矩阵,此时相应的拟牛顿条件是\(B_{k+1} \delta_{k}=y_{k}\),假设每一步\(B_{k+1}=B_{k}+P_{k}+Q_{k}\)，则 \(P_k\) 和 \(Q_k\) 满足\(P_{k} \delta_{k}=y_{k}, \quad Q_{k} \delta_{k}=-B_{k} \delta_{k}\)，类似得到迭代公式\(B_{k+1}=B_{k}+\frac{y_{k} y_{k}^{\top}}{y_{k}^{\top} \delta_{k}}-\frac{B_{k} \delta_{k} \delta_{k}^{\mathrm{T}} B_{k}}{\delta_{k}^{\top} B_{k} \delta_{k}}\)</li>
</ol>
<h4 id="strong-xian-yan-gai-lu-he-hou-yan-gai-lu-strong"><strong>先验概率和后验概率</strong></h4>
<ol>
<li>先验概率就是事情发生前的预测概率.</li>
<li>后验概率是一种条件概率，它限定了事件为隐变量取值，而条件为观测结果。一般的条件概率，条件和事件可以是任意的.</li>
<li>贝叶斯公式\(P(y|x) = ( P(x|y) * P(y) ) / P(x)\)中,\(P(y|x)\)是后验概率,\(P(x|y)\)是条件概率,\(P(y)\)是先验概率.</li>
</ol>
<h4 id="strong-pian-chai-fang-chai-zao-sheng-strong"><strong>偏差,方差,噪声</strong></h4>
<ol>
<li>偏差:度量了学习算法的期望预测和真实结果偏离程度</li>
<li>方差:度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响</li>
<li>噪声:可以认为是数据自身的波动性，表达了目前任何学习算法所能达到泛化误差的下限</li>
<li><strong>泛化误差</strong>可以分解为偏差、方差与噪声之和</li>
</ol>
<h4 id="strong-dui-ou-yuan-li-strong"><strong>对偶原理</strong></h4>
<p>一个优化问题可以从主问题和对偶问题两个方面考虑.在推导对偶问题时,通过将拉格朗日函数对\(x\)求导并使导数为0来获得对偶函数.对偶函数给出了主问题最优解的下界,因此对偶问题一般是凸问题,那么只需求解对偶函数的最优解就可以了.</p>
<h4 id="strong-kkt-tiao-jian-strong"><strong>KKT条件</strong></h4>
<p>通常我们要求解的最优化条件有如下三种:</p>
<ol>
<li>无约束优化问题:通常使用求导,使导数为零,求解候选最优值</li>
<li>有等式约束的优化问题:通常使用拉格朗日乘子法,即把等式约束用拉格朗日乘子和优化问题合并为一个式子,通过对各个变量求导使其为零,求解候选最优值.拉格朗日乘数法其实是KKT条件在等式约束优化问题的简化版.</li>
<li>有不等式约束的优化问题:通常使用KKT条件.即把不等式约束,等式约束和优化问题合并为一个式子.假设有多个等式约束\(h(x)\)和不等式约束\(g(x)\),\(L(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\mu})=f(\boldsymbol{x})+\sum_{i=1}^{m} \lambda_{i} h_{i}(\boldsymbol{x})+\sum_{i=1}^{n} \mu_{j} g_{j}(\boldsymbol{x})\),则不等式约束引入的KKT条件如下:<br>
\(\left\{\begin{array}{l}g_{j}(\boldsymbol{x}) \leqslant 0 \\ mu_{j} \geqslant 0  \\mu_{j} g_{j}(\boldsymbol{x})=0\end{array}\right.\) ,实质是最优解在\(g(x)&lt;0\)区域内时,约束条件不起作用,等价于对\(μ\)置零然后对原函数的偏导数置零;当\(g(x)=0\)时与情况2相近.结合两种情况,那么只需要使\(L\)对\(x\)求导为零,使\(h(x)\)为零,使\(μg(x)\)为零三式即可求解候选最优值.</li>
</ol>
<h4 id="strong-jiang-wei-fang-fa-strong"><strong>降维方法</strong></h4>
<ol>
<li>主成分分析(PCA):降维,不断选择与已有坐标轴正交且方差最大的坐标轴.</li>
<li>奇异值分解(SVD):矩阵分解,降维,推荐系统.</li>
<li>线性判别分析(LDA)</li>
</ol>
<h4 id="ji-cheng-fang-fa">集成方法</h4>
<h5 id="bagging">bagging</h5>
<p>装袋法的核心思想是构建多个相互独立的评估器，然后对其预测进行平均或多数表决原则来决定集成评估器的结果。</p>
<ol>
<li>评估器：相互独立，同时运行</li>
<li>抽样：有放回抽样</li>
<li>如何决定集成的结果：平均或者少数服从多数</li>
<li>目标：降低方差</li>
<li>基学习器过拟合：能够一定程度上解决基学习器过拟合的问题</li>
<li>基学习器学习能力弱：不是很有帮助</li>
<li>代表算法：随机森林</li>
</ol>
<h5 id="boosting">boosting</h5>
<p>提升方法中，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本进行预测，从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树。</p>
<ol>
<li>评估器：相互关联，按顺序依次构建，后建的模型在先建模型预测失败的样本上有更多的权重</li>
<li>抽样：有放回的采样，但会确认数据的权重，每次抽样都会给预测失败的样本更多的权重</li>
<li>如何决定集成的结果：加权平均，在训练集上表现好的模型会有更大的权重</li>
<li>目标：降低偏差，提高模型整体的精确度</li>
<li>基学习器过拟合：加剧过拟合问题</li>
<li>基学习器学习能力弱：提升模型表现</li>
<li>代表算法：GBDT,Adaboost</li>
</ol>
<h5 id="stacking">stacking</h5>
<p>Stacking模型是指将多种分类器组合在一起来取得更好表现的一种集成学习模型。一般情况下，Stacking模型分为两层。第一层中我们训练多个不同的模型，然后再以第一层训练的各个模型的输出作为输入来训练第二层的模型，以得到一个最终的输出。</p>
<h4 id="strong-xing-neng-du-liang-strong"><strong>性能度量</strong></h4>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center">预测值</th>
<th style="text-align:center">预测值</th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">真实值</td>
<td style="text-align:center">1</td>
<td style="text-align:center">11</td>
<td style="text-align:center">10</td>
<td style="text-align:center">Recall = \(\frac{11}{11+10}\)</td>
</tr>
<tr>
<td style="text-align:center">真实值</td>
<td style="text-align:center">0</td>
<td style="text-align:center">01</td>
<td style="text-align:center">00</td>
<td style="text-align:center">FPR = \(\frac{01}{01+00}\)</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">Precision=\(\frac{11}{11+01}\)</td>
<td style="text-align:center"></td>
<td style="text-align:center">Acc = \(\frac{11+00}{11+10+01+00}\)</td>
</tr>
</tbody>
</table>
<ol>
<li><strong>准确度</strong>,最常用,但在数据集不平衡的情况下不好</li>
<li><strong>Precision(精确度/查准率)</strong>:\(P=TP/(TP+FP)\)</li>
<li><strong>Recall(召回率/查全率)</strong>:\(R=TP/(TP+FN)\)</li>
<li><strong>Fβ度量</strong>:\(F_{\beta}=\frac{\left(1+\beta^{2}\right) r p}{\beta^{2} * p+r}\),当β=1时退化为F1度量,是精确率和召回率的调和均值.</li>
<li><strong>TPR(真正例率)</strong>:\(TPR=TP/(TP+FN)\)</li>
<li><strong>FPR(假正例率)</strong>:\(FPR=FP/(TN+FP)\)</li>
<li><strong>PR曲线</strong>:纵轴为Precision,横轴为Recall,一般使用平衡点(BEP,即Precsion=Recall的点)作为衡量标准.</li>
<li><strong>ROC(接受者操作特征)曲线</strong>:（<strong>每判断正确一个少数类，就有多少个多数类会被判断错误。</strong>）纵轴为TRP,横轴为FPR,在绘图时将分类阈值依次设为每个样例的预测值,再连接各点.ROC曲线围住的面积称为AOC,AOC越大则学习器性能越好.</li>
</ol>
<h4 id="strong-sun-shi-han-shu-he-feng-xian-han-shu-strong"><strong>损失函数和风险函数</strong></h4>
<ol>
<li>损失函数度量模型一次预测的好坏.常用的损失函数有:0-1损失函数,平方损失函数,绝对损失函数,对数似然损失函数.</li>
<li>损失函数的期望是理论上模型关于联合分布P(X,Y)的平均意义下的损失,称为风险函数,也叫<strong>期望风险</strong>.但是联合分布是未知的,期望风险不能直接计算.</li>
<li>当样本容量N趋于无穷时经验风险趋于期望风险,但现实中训练样本数目有限.</li>
</ol>
<h4 id="strong-jing-yan-feng-xian-zui-xiao-hua-he-jie-gou-feng-xian-zui-xiao-hua-strong"><strong>经验风险最小化和结构风险最小化</strong></h4>
<ol>
<li>模型关于训练数据集的平均损失称为经验风险.经验风险最小化的策略就是最小化经验风险.当样本数量足够大时学习效果较好.比如当模型是条件概率分布,损失函数是对数损失函数时,经验风险最小化就等价于极大似然估计.但是当样本容量很小时会出现过拟合.</li>
<li>结构风险最小化等于正则化.结构风险在经验风险上加上表示模型复杂度的正则化项.比如当模型是条件概率分布,损失函数是对数损失函数,模型复杂度由模型的先验概率表示时,结构风险最小化就等价于最大后验概率估计.</li>
</ol>
<h4 id="strong-guo-ni-he-strong"><strong>过拟合</strong></h4>
<p>指学习时选择的模型所包含的参数过多,以致于对已知数据预测得很好,但对未知数据预测很差的现象.模型选择旨在避免过拟合并提高模型的预测能力.</p>
<h4 id="strong-zheng-ze-hua-strong"><strong>正则化</strong></h4>
<p>模型选择的典型方法.正则化项一般是模型复杂度的单调递增函数,比如模型参数向量的范数.</p>
<h4 id="strong-jiao-cha-yan-zheng-strong"><strong>交叉验证</strong></h4>
<p>是另一常用的模型选择方法,可分为简单交叉验证,K折交叉验证,留一交叉验证等.</p>
<h4 id="sklearn">sklearn</h4>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/sklearn.png" alt="avatar"></p>
<h3 id="gan-zhi-ji">感知机</h3>
<p>感知机是<strong>二类分类</strong>的线性模型,属于判别模型.感知机学习旨在求出将训练数据进行线性划分的分离超平面.是神经网络和支持向量机的基础.</p>
<h4 id="mo-xing">模型</h4>
<p>\(f(x)=sign(wx + b)\),\(w\)叫作权值向量,\(b\)叫做偏置,\(sign\)是符号函数.</p>
<h4 id="strong-gan-zhi-ji-de-ji-he-jie-shi-strong"><strong>感知机的几何解释</strong></h4>
<p>\(wx+b\)对应于特征空间中的一个分离超平面S,其中\(w\)是\(S\)的法向量,\(b\)是\(S\)的截距.\(S\)将特征空间划分为两个部分,位于两个部分的点分别被分为正负两类.</p>
<h4 id="ce-lue">策略</h4>
<p>假设训练数据集是线性可分的,感知机的损失函数是误分类点到超平面\(S\)的总距离.因为误分类点到超平面\(S\)的距离是\(\frac{1}{\|w\|}\left|w \cdot x_{0}+b\right|\),且对于误分类的数据来说,总有\(-y_i(wx_i+b)>0\)成立,因此不考虑\(\frac{1}{\|w\|}\),就得到感知机的损失函数:</p>
<p>\(L(w, b)=-\sum_{x \in M} y_{i}\left(w \cdot x_{i}+b\right)\),其中M是误分类点的集合.感知机学习的策略就是选取使损失函数最小的模型参数.</p>
<h4 id="suan-fa">算法</h4>
<p>感知机的最优化方法采用<strong>随机梯度下降法</strong>.首先任意选取一个超平面\(w_0,b_0\),然后不断地极小化目标函数.在极小化过程中一次随机选取一个误分类点更新\(w,b\),直到损失函数为0.\(w \leftarrow w+\eta y_{i} x_{i}\) ， \(b \leftarrow b+\eta y_{i}\)，其中\(η\)表示步长.该算法的直观解释是:当一个点被误分类,就调整\(w,b\)使分离超平面向该误分类点接近.感知机的解可以不同.</p>
<h4 id="strong-dui-ou-xing-shi-strong"><strong>对偶形式</strong></h4>
<p>假设原始形式中的\(w_0\)和\(b_0\)均为0,设逐步修改\(w\)和\(b\)共n次,令\(a=nη\),最后学习到的\(w,b\)可以表示为\(w =\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}\),\(b =\sum_{i=1}^{N}{\alpha_{i} y_{i}}\) ,那么对偶算法就变为设初始\(a\)和\(b\)均为0,每次选取数据更新\(a\)和\(b\)直至没有误分类点为止.对偶形式的意义在于可以将训练集中实例间的内积计算出来,存在Gram矩阵中,可以大大加快训练速度.</p>
<h3 id="k-jin-lin-fa">k近邻法</h3>
<p>k近邻法根据其<strong>k个最近邻</strong>的训练实例的类别,通过多数表决等方式进行预测.当k=1时称为最近邻算法.</p>
<h4 id="san-ge-ji-ben-yao-su">三个基本要素:</h4>
<ol>
<li>k值的选择</li>
<li>距离度量</li>
<li>分类决策规则</li>
</ol>
<h4 id="mo-xing-1">模型</h4>
<p>当训练集,距离度量,k值以及分类决策规则确定后,特征空间已经根据这些要素被划分为一些子空间,且子空间里每个点所属的类也已被确定.</p>
<h4 id="strong-ce-lue-strong"><strong>策略</strong></h4>
<ol>
<li><strong>距离</strong>:特征空间中两个实例点的距离是相似程度的反映,k近邻算法一般使用欧氏距离,也可以使用更一般的Lp距离或Minkowski距离.</li>
<li><strong>k值</strong>:k值较小时,整体模型变得复杂,容易发生过拟合.k值较大时,整体模型变得简单.在应用中k一般取较小的值,通过交叉验证法选取最优的k.</li>
<li><strong>分类决策规则</strong>:k近邻中的分类决策规则往往是多数表决,多数表决规则等价于经验风险最小化.</li>
</ol>
<h4 id="suan-fa-1">算法</h4>
<p>根据给定的距离度量,在训练集中找出与x最邻近的k个点,根据分类规则决定x的类别y.</p>
<p><strong>kd树</strong></p>
<ol>
<li>
<p>kd树就是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构.kd树更适用于<strong>训练实例数远大于空间维数</strong>时的k近邻搜索.</p>
</li>
<li>
<p><strong>构造</strong>:可以通过如下<strong>递归</strong>实现:在超矩形区域上选择一个<strong>坐标轴</strong>和此坐标轴上的一个<strong>切分点</strong>,确定一个超平面,该超平面将当前超矩形区域切分为两个子区域.在子区域上重复切分直到子区域内没有实例时终止.通常依次选择坐标轴和选定坐标轴上的<strong>中位数点</strong>为切分点,这样可以得到平衡kd树.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/kd_tree.png" alt="avatar"></p>
</li>
</ol>
<h4 id="strong-sou-suo-strong"><strong>搜索</strong></h4>
<p>从根节点出发,若目标点x当前维的坐标小于切分点的坐标则移动到左子结点,否则移动到右子结点,直到子结点为叶结点为止.以此叶结点为&quot;当前最近点&quot;,<strong>递归</strong>地向上回退,在每个结点:(a)如果该结点比当前最近点距离目标点更近,则以该结点为&quot;当前最近点&quot;(b)&quot;当前最近点&quot;一定存在于该结点一个子结点对应的区域,检查该结点的另一子结点对应的区域是否与以目标点为球心,以目标点与&quot;当前最近点&quot;间的距离为半径的超球体相交.如果相交,移动到另一个子结点,如果不相交,向上回退.持续这个过程直到回退到根结点,最后的&quot;当前最近点&quot;即为最近邻点.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/search_kd.png" alt="avatar"></p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/search_kd_2.png" alt="avatar"></p>
<h3 id="po-su-bei-xie-si">朴素贝叶斯</h3>
<p>朴素贝叶斯是基于<strong>贝叶斯定理</strong>和<strong>特征条件独立假设</strong>的分类方法.首先学习输入/输出的联合概率分布,然后基于此模型,对给定的输入x,利用贝叶斯定理求出后验概率最大的输出y.属于生成模型.</p>
<h4 id="strong-mo-xing-strong"><strong>模型</strong></h4>
<ol>
<li>首先学习先验概率分布\(p(c_k),k=1,2,3,4,...,k\)</li>
<li>然后学习条件概率分布,\(P\left(X=x | Y=c_{k}\right)=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right)\).如果估计实际,需要指数级的计算,所以朴素贝叶斯法对条件概率分布作了条件独立性的假设,上式变成\(\prod_{j=1}^{n} P\left(X^{(n)}=x^{(n)} | Y=c_{k}\right)\)</li>
<li>在分类时,通过学习到的模型计算后验概率分布,由贝叶斯定理得到\(P\left(Y=c_{k} | X=x\right)=\frac{P\left(X=x | Y=c_{k}\right) P\left(Y=c_{k}\right)}{\sum_{k} P\left(X=x | Y=c_{k}\right) P\left(Y=c_{k}\right)}\)</li>
<li>将条件独立性假设得到的等式代入,并且注意到分母都是相同的,所以得到朴素贝叶斯分类器:\(y=\arg \max _{a_{k}} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(l)}=x^{(l)} | Y=c_{k}\right)\)</li>
</ol>
<p>朴素贝叶斯将实例分到后验概率最大的类中,这等价于期望风险最小化.</p>
<h4 id="strong-suan-fa-strong"><strong>算法</strong></h4>
<ol>
<li>使用<strong>极大似然估计法</strong>估计相应的先验概率:\(P\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}{N}, \quad k=1,2, \cdots, K\)和条件概率\(P\left(X^{(j)}=a_{n} | Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(n)}=a_{j i} y_{i}=c_{k}\right)}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}\)</li>
<li>计算条件独立性假设下的实例各个取值的可能性,\(P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right), \quad k=1,2, \cdots, K\)</li>
<li>选取其中的最大值作为输出.</li>
</ol>
<h4 id="te-shu-qing-kuang">特殊情况</h4>
<ol>
<li>用极大似然估计可能会出现所要估计的概率值为0的情况,在累乘后会影响后验概率的计算结果,使分类产生偏差.可以采用<strong>贝叶斯估计</strong>,在随机变量各个取值的频数上赋予一个正数.\(P_{\lambda}\left(X^{(1)}=a_{j q} | Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(l)}=a_{\beta}, y_{i}=c_{k}\right)+\lambda}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)+S_{j} \lambda}\)，\(S_j\)为j属性可能取值数量,当\(λ=0\)时就是极大似然估计.常取\(λ=1\),称为<strong>拉普拉斯平滑</strong>.</li>
<li>如果是连续值的情况,可以假设连续变量服从高斯分布,然后用训练数据估计参数.\(P\left(X_{i}=x_{i} | Y=y_{i}\right)=\frac{1}{\sqrt{2 \pi} \sigma_{i j}^{2}} e^{\frac{\left(x_{i}-\mu_{j}\right)^{2}}{2 \sigma_{i}^{2}}}\)</li>
</ol>
<h3 id="jue-ce-shu">决策树</h3>
<p>决策树是一种基本的分类与回归方法.它可以认为是<strong>if-then规则</strong>的集合,也可以认为是定义在特征空间与类空间上的<strong>条件概率分布</strong>.主要优点是模型具有可读性,分类速度快.其主要围绕着两个问题：</p>
<ol>
<li>如何从数据表中找出最佳节点和最佳分枝？</li>
<li>如何让决策树停止生长，防止过拟合？</li>
</ol>
<h4 id="mo-xing-2">模型</h4>
<p>分类决策树由<strong>结点</strong>和<strong>有向边</strong>组成.结点分为<strong>内部结点</strong>(表示一个特征或属性)和<strong>叶结点</strong>(表示一个类).决策树的路径具有<strong>互斥且完备</strong>的性质.</p>
<h4 id="ce-lue-1">策略</h4>
<p>决策树学习本质上是从训练数据集中归纳出一组分类规则.我们需要的是一个与训练数据<strong>矛盾较小</strong>,同时具有很好的<strong>泛化能力</strong>的决策树.从所有可能的决策树中选取最优决策树是NP完全问题,所以现实中常采用<strong>启发式方法</strong>近似求解.</p>
<h4 id="suan-fa-2">算法</h4>
<p>决策树学习算法包含<strong>特征选择</strong>,<strong>决策树的生成</strong>与<strong>决策树的剪枝过程</strong>.生成只考虑局部最优,剪枝则考虑全局最优。</p>
<h4 id="te-zheng-xuan-ze">特征选择</h4>
<p>如果利用一个特征进行分类的结果与随机分类的结果没有很大差别,则称这个特征是<strong>没有分类能力</strong>的.扔掉这样的特征对决策树学习的精度影响不大.</p>
<ol>
<li><strong>信息熵</strong>：熵是衡量<strong>随机变量不确定性</strong>的度量.熵越大,随机变量的不确定性就越大.信息熵是信息量的期望，\(\left.H(X)=-\sum_{x \in X} P(x) \log P(x)\right)\)</li>
<li><strong>条件熵</strong>：条件熵表示在已知随机变量X的条件下随机变量Y的不确定性.\(H(Y | X)=\sum_{i=1}^{n} p_{i} H\left(Y | X=x_{i}\right)\)</li>
<li><strong>信息增益</strong>：表示得知特征X的信息而使得类Y的信息的<strong>不确定性减少</strong>的程度.定义为集合D的经验熵与特征A在给定条件下D的经验条件熵之差\(g(D,A)=H(D)-H(D|A)\),也就是训练数据集中类与特征的<strong>互信息</strong>.</li>
<li><strong>信息增益算法</strong>:计算数据集D的经验熵\(H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|_{\mathrm{log}_{2}}\left|C_{k}\right|}{|D|}\),计算特征A对数据集D的经验条件熵\(H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{\mu}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{k}\right|}{\left|D_{i}\right|}\),计算信息增益,选取信息增益最大的特征.</li>
<li><strong>信息增益比</strong>:信息增益值的大小是相对于训练数据集而言的,并无绝对意义.使用信息增益比,\(g_{R}(D, A)=\frac{g(D, A)}{H(D)}\)可以对这一问题进行校正.</li>
</ol>
<h4 id="strong-jue-ce-shu-de-sheng-cheng-strong"><strong>决策树的生成</strong></h4>
<ol>
<li><strong>ID3算法</strong>:核心是在决策树各个结点上应用<strong>信息增益准则</strong>选择信息增益最大且大于阈值的特征,递归地构建决策树.ID3相当于用极大似然法进行概率模型的选择.由于算法只有树的生成,所以容易产生过拟合.</li>
<li><strong>C4.5算法</strong>:C4.5算法与ID3算法相似,改用<strong>信息增益比</strong>来选择特征.</li>
</ol>
<h4 id="strong-jue-ce-shu-de-jian-zhi-strong"><strong>决策树的剪枝</strong></h4>
<ol>
<li>在学习时过多考虑如何提高对训练数据的正确分类,从而构建出过于复杂的决策树,产生<strong>过拟合</strong>现象.解决方法是对已生成的决策树进行简化,称为剪枝.</li>
<li>设树的叶结点个数为\(|T|\),每个叶结点有\(N_t\)个样本点,其中\(k\)类样本点有\(N_{tk}\)个,剪枝往往通过极小化决策树整体的损失函数\(C_{\alpha}(T)=\sum_{i=1}^{\pi} N_{i} H_{i}(T)+\alpha|T|\)来实现,其中经验熵\(H_{t}(T)=-\sum_{k} \frac{N_{a}}{N_{t}} \log \frac{N_{u}}{N_{t}}\).剪枝通过加入\(a|T|\)项来考虑模型复杂度,实际上就是用正则化的极大似然估计进行模型选择.</li>
<li><strong>剪枝算法</strong>:剪去某一子结点,如果生成的新的整体树的<strong>损失函数值</strong>小于原树,则进行剪枝,直到不能继续为止.具体可以由动态规划实现.</li>
</ol>
<h4 id="strong-cart-suan-fa-strong"><strong>CART算法</strong></h4>
<ol>
<li>
<p>CART既可以用于<strong>分类也</strong>可以用于<strong>回归</strong>.它假设决策树是<strong>二叉树</strong>,内部结点特征的取值为&quot;是&quot;和&quot;否&quot;.递归地构建二叉树,对回归树用<strong>平方误差</strong>最小化准则,对分类数用<strong>基尼指数</strong>最小化准则.</p>
</li>
<li>
<p><strong>回归树的生成</strong>:在训练数据集所在的输入空间中,递归地将每个区域划分为两个子区域.选择第j个变量和它取的值s作为切分变量和切分点,并定义两个区域\(R_{1}(j, s)=\left\{x | x^{(j)} \leqslant s\right\} \quad\)和\(\quad R_{2}(j, s)=\left\{x | x^{(l)}>s\right\}\),遍历变量j,对固定的j扫描切分点s,求解\(\min _{j, z}\left[\min _{c_1} \sum_{x \in R_{1}(j, x)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x \in R_{2}(j, x)}\left(y_{i}-c_{2}\right)^{2}\right]\).用选定的对(j,s)划分区域并决定相应的输出值\(\hat{c}_{m}=\frac{1}{N_{m}} \sum_{x_{i} \in R_{m}(j, x)} y_{i}, x \in R_{m}, \quad m=1,2\),直到满足停止条件.</p>
</li>
<li>
<p><strong>基尼指数</strong>:假设有K个类,样本属于第k类的概率为\(p_k\),则概率分布的基尼指数为:\(\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}\),表示不确定性.在特征A的条件下集合D的基尼指数定义为\(\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)\),表示分割后集合D的不确定性.基尼指数越大,样本集合的<strong>不确定性</strong>也就越大.</p>
</li>
<li>
<p><strong>分类树的生成</strong></p>
<ol>
<li>从根结点开始,设结点的训练数据集为D,对每个特征A和其可能取的每个值a,计算A=a时的基尼指数,</li>
<li>选择<strong>基尼指数最小</strong>的特征及其对应的切分点作为<strong>最优特征</strong>与<strong>最优切分点</strong>,生成两个子结点</li>
<li>递归进行以上操作,直至满足<strong>停止条件</strong>.停止条件一般是结点中的样本个数小于阈值,或样本集的基尼指数小于阈值,或没有更多特征.</li>
</ol>
</li>
<li>
<p><strong>CART剪枝</strong></p>
<p>\(T_t\)表示以t为根结点的子树,\(|T_t|\)是\(T_t\)的叶结点个数.可以证明当\(\alpha=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}\)时,\(T_t\)与\(t\)有相同的损失函数值,且\(t\)的结点少,因此\(t\)比\(T_t\)更可取,对\(T_t\)进行剪枝.<strong>自下而上</strong>地对各内部结点t计算\(g(t)=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}\),并令\(a=min(g(t))\),<strong>自上而下</strong>地访问内部节点t,如果有\(g(t)=a\),进行剪枝,并对t以<strong>多数表决法</strong>决定其类,得到子树T,如此循环地生成一串<strong>子树序列</strong>,直到新生成的T是由根结点单独构成的树为止.利用<strong>交叉验证法</strong>在子树序列中选取最优子树.</p>
<p>如果是<strong>连续值</strong>的情况,一般用<strong>二分法</strong>作为结点来划分.</p>
</li>
</ol>
<h3 id="logistic-hui-gui-he-zui-da-shang-mo-xing">logistic回归和最大熵模型</h3>
<h4 id="strong-luo-ji-si-di-fen-bu-strong"><strong>逻辑斯谛分布</strong></h4>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/logistic.png" alt="avatar"></p>
<p>分布函数\(f(x)\)以点\((μ,1/2)\)为中心对称,\(γ\)的值越小,曲线在中心附近增长得越快.</p>
<h4 id="strong-luo-ji-si-di-hui-gui-mo-xing-strong"><strong>逻辑斯谛回归模型</strong></h4>
<p>对于给定的输入x,根据\(P(Y=1 | x)=\frac{\exp (w \cdot x+b)}{1+\exp (w \cdot x+b)}\) 和 \(P(Y=0 | x)=\frac{1}{1+\exp (w \cdot x+b)}\)计算出两个条件概率值的大小,将x分到概率值较大的那一类.将偏置b加入到权值向量w中,并在x的最后添加常数项1,得到\(P(Y=1 | x)=\frac{\exp (w \cdot x)}{1+\exp (w \cdot x)}\) 和 \(P(Y=0 | x)=\frac{1}{1+\exp (w \cdot x)}\)。</p>
<h4 id="dui-shu-ji-lu">对数几率</h4>
<p>如果某事件发生的概率是p,则该事件发生的<strong>几率</strong>(此处几率指该事件发生概率与不发生概率之比)是\(\frac{p}{1-p}\),<strong>对数几率</strong>是\(log(\frac{p}{1-p})\),那么\(\log \frac{P(Y=1 | x)}{1-P(Y=1 | x)}=w \cdot x\)，也就是说在逻辑斯谛回归模型中,输出Y=1的对数几率是输入x的<strong>线性函数</strong>,线性函数值越接近正无穷,概率值就越接近1,反之则越接近0.</p>
<h4 id="strong-si-ran-gu-ji-strong"><strong>似然估计</strong></h4>
<p>给定x的情况下参数θ是真实参数的可能性.</p>
<h4 id="strong-mo-xing-can-shu-gu-ji-strong"><strong>模型参数估计</strong></h4>
<p>对于给定的二分类训练数据集,对数似然函数为</p>
<p>\[
\begin{aligned} L(w) &amp;=\sum_{i=1}^{N}\left[y_{i} \log \pi\left(x_{i}\right)+\left(1-y_{i}\right)\log\left(1-\pi\left(x_{i}\right)\right)\right]\\ &amp;=\sum_{i=1}^{N}\left[y_{i} \log \frac{\pi\left(x_{i}\right)}{1-\pi\left(x_{i}\right)}+\log \left(1-\pi\left(x_{i}\right)\right)\right]\\&amp;=\sum_{i=1}^{N}\left[y_{i}\left(w \cdot x_{i}\right)-\log \left(1+\exp \left(w \cdot x_{i}\right)\right]\right.\end{aligned}
\]<br>
也就是<strong>损失函数</strong>.其中\(P(Y=1|x)=π(x)\),对\(L(w)\)求极大值,就可以得到\(w\)的估计值.问题变成了以对数似然函数为目标函数的最优化问题.</p>
<h4 id="strong-duo-xiang-luo-ji-si-di-hui-gui-strong"><strong>多项逻辑斯谛回归</strong></h4>
<p>当问题是多分类问题时,可以作如下推广:设Y有K类可能取值,\(P(Y=k | x)=\frac{\exp \left(w_{k} \cdot x\right)}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}, \quad k=1,2, \cdots, K-1 \quad P(Y=K | x)=\frac{1}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}\),实际上就是<strong>one-vs-all</strong>的思想,将其他所有类当作一个类,问题转换为二分类问题.</p>
<p>使用最大似然法衡量模型输出的概率与真实概率的差别，假设样本一共有N个，那么这组样本发生的总概率可以表示为：<br>
\[
P(\boldsymbol{W})=\prod_{n=1}^{N}\left(\frac{e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}}{\sum_{k^{\prime}}^{C} e^{\boldsymbol{w}_{k^{\prime}}^{T} \boldsymbol{x}_{n}}}\right)
\]<br>
对函数取对数再乘以-1，推导得到：<br>
\[
\begin{aligned}
F(\boldsymbol{W})=-\ln (P(\boldsymbol{W})) &amp;=\sum_{n=1}^{N} \ln \left(\frac{\sum_{k^{\prime}}^{C} e^{\boldsymbol{w}_{k^{\prime}}^{T} \boldsymbol{x}_{n}}}{e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}}\right) \\
&amp;=\sum_{n=1}^{N} \ln \left(\frac{e^{\boldsymbol{w}_{1}^{T} \boldsymbol{x}}+e^{\boldsymbol{w}_{2}^{T} \boldsymbol{x}}+\ldots+e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}+\ldots+e^{\boldsymbol{w}_{c}^{T} \boldsymbol{x}}}{e^{\boldsymbol{w}_{y_{n}}^{T} \boldsymbol{x}_{n}}}\right) \\
&amp;=\sum_{n=1}^{N} \ln \left(1+\sum_{k \neq y_{n}} e^{\boldsymbol{w}_{k} \boldsymbol{x}_{n}-\boldsymbol{w}_{y_{n}} \boldsymbol{x}_{n}}\right)
\end{aligned}
\]</p>
<h4 id="strong-zui-da-shang-yuan-li-strong"><strong>最大熵原理</strong></h4>
<p>学习概率模型时,在所有可能的概率模型中,<strong>熵最大</strong>的模型是最好的模型.直观地,最大熵原理认为模型首先要满足已有的事实,即<strong>约束条件</strong>.在没有更多信息的情况下,那些不确定的部分都是&quot;<strong>等可能的</strong>&quot;.</p>
<h4 id="strong-zui-da-shang-mo-xing-strong"><strong>最大熵模型</strong></h4>
<p>给定训练数据集,可以确定联合分布P(X,Y)的经验分布\(\tilde{P}(X=x, Y=y)=\frac{v(X=x, Y=y)}{N}\)和边缘分布P(X)的经验分布\(\tilde{P}(X=x)=\frac{v(X=x)}{N}\),其中v表示频数,N表示样本容量.用<strong>特征函数\(f(x,y)\)</strong>=1描述x与y满足某一事实,可以得到特征函数关于P(X,Y)的经验分布的期望值和关于模型P(Y|X)与P(X)的经验分布的期望值,假设两者相等,就得到了<strong>约束条件</strong>\(\sum_{x, y} \tilde{P}(x) P(y | x) f(x, y)=\sum_{x, y} \tilde{P}(x, y) f(x, y)\).定义在条件概率分布P(Y|X)上的条件熵为\(H(P)=-\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)\),则<strong>条件熵最大</strong>的模型称为最大熵模型.</p>
<h4 id="strong-zui-da-shang-mo-xing-de-xue-xi-strong"><strong>最大熵模型的学习</strong></h4>
<p>就是求解最大熵模型的过程.等价于<strong>约束最优化问题</strong><br>
\[
\begin{aligned}
&amp;\max _{P_{e c}} H(P)=-\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)\\
&amp;\text { s.t. } \quad E_{p}\left(f_{i}\right)=E_{p}\left(f_{i}\right), \quad i=1,2, \cdots, n\\
     &amp;\sum_{y} P(y | x)=1
\end{aligned}
\]<br>
,将求最大值问题改为等价的求最小值问题<br>
\[
\begin{aligned}
&amp;\min _{R \in C}-H(P)=\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)\\
&amp;\text { s.t. } \quad E_{P}\left(f_{i}\right)-E_{\beta}\left(f_{i}\right)=0, \quad i=1,2, \cdots, n\\
&amp;\sum_{y} P(y | x)=1
\end{aligned}
\]</p>
<p>引入<strong>拉格朗日乘子</strong><br>
\[
\begin{aligned}
L(P, w) &amp; \equiv-H(P)+w_{0}\left(1-\sum_{y} P(y | x)\right)+\sum_{i=1}^{n} w_{i}\left(E_{p}\left(f_{i}\right)-E_{P}\left(f_{i}\right)\right) \\
&amp;=\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)+w_{0}\left(1-\sum_{y} P(y | x)\right) \\
&amp;+\sum_{i=1}^{n} w_{i}\left(\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P(y | x) f_{i}(x, y)\right)
\end{aligned}
\]<br>
将原始问题\(\min _{p \in C} \max _{w} L(P, w)\)转换为无约束最优化的<strong>对偶问题</strong>\(\max _{w} \min _{P \in \mathbf{C}} L(P, w)\).首先求解内部的<strong>极小化问题</strong>,即求\(L(P,W)\)对\(P(y|x)\)的偏导数.<br>
\[
\begin{aligned}
\frac{\partial L(P, w)}{\partial P(y | x)} &amp;=\sum_{x, y} \tilde{P}(x)(\log P(y | x)+1)-\sum_{y} w_{0}-\sum_{x, y}\left(\tilde{P}(x) \sum_{i=1}^{n} w_{i} f_{i}(x, y)\right) \\
&amp;=\sum_{x, y} \tilde{P}(x)\left(\log P(y | x)+1-w_{0}-\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
\end{aligned}
\]<br>
,并令偏导数等于0,解得\(Z_{w}(x)=\sum_{y} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)\).可以证明对偶函数等价于对数似然函数,那么对偶函数极大化等价于最大熵模型的<strong>极大似然估计</strong>\(L(w)=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)\).之后可以用最优化算法求解得到w.</p>
<h4 id="luo-ji-hui-gui-yu-zui-da-shang-mo-xing-de-gong-tong-dian">逻辑回归与最大熵模型的共同点</h4>
<p>最大熵模型与逻辑斯谛回归模型有类似的形式,它们又称为<strong>对数线性模型</strong>.模型学习就是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计.</p>
<h4 id="you-hua-suan-fa">优化算法</h4>
<p>似然函数是<strong>光滑的凸函数</strong>,因此多种最优化方法都适用.</p>
<ol>
<li><strong>改进的迭代尺度法(IIS)</strong>:假设当前的参数向量是w,如果能找到一种方法<strong>w-&gt;w+δ</strong>使对数似然函数值变大,就可以<strong>重复</strong>使用这一方法,直到找到最大值.</li>
<li>逻辑斯谛回归常应用梯度下降法,牛顿法或拟牛顿法.</li>
</ol>
<h3 id="zhi-chi-xiang-liang-ji">支持向量机</h3>
<h4 id="mo-xing-3">模型</h4>
<p>支持向量机(SVM)是一种<strong>二类分类模型</strong>.它的基本模型是定义在特征空间上的<strong>间隔最大</strong>的线性分类器.支持向量机还包括<strong>核技巧</strong>,使它成为实质上的非线性分类器.<strong>分离超平面</strong>\(w^\star x+b^\star =0\),<strong>分类决策函数</strong>\(f(s)=sign(w^\star x + b^\star)\).</p>
<h4 id="ce-lue-2">策略</h4>
<p><strong>间隔最大化</strong>,可形式化为一个求解<strong>凸二次规划</strong>的问题,也等价于正则化的<strong>合页损失函数</strong>的最小化问题.</p>
<h4 id="shu-ju-ke-fen-jin-si-ke-fen-bu-ke-fen">数据可分、近似可分、不可分</h4>
<p>当训练数据<strong>线性可分</strong>时,通过硬间隔最大化,学习出<strong>线性可分支持向量机</strong>.当训练数据<strong>近似线性可分</strong>时,通过软间隔最大化,学习出<strong>线性支持向量机</strong>.当训练数据<strong>线性不可分</strong>时,通过使用核技巧及软间隔最大化,学习<strong>非线性支持向量机</strong>.</p>
<h4 id="he-ji-qiao">核技巧</h4>
<p>当输入空间为欧式空间或离散集合,特征空间为希尔伯特空间时,核函数表示将输入从输入空间<strong>映射</strong>到特征空间得到的特征向量之间的<strong>内积</strong>.通过核函数学习非线性支持向量机等价于在高维的特征空间中学习线性支持向量机.这样的方法称为核技巧.</p>
<h4 id="shu-ru-kong-jian-he-te-zheng-kong-jian">输入空间和特征空间</h4>
<p>考虑一个二类分类问题,假设输入空间与特征空间为两个不同的空间,输入空间为<strong>欧氏空间或离散集合</strong>,特征空间为<strong>欧氏空间或希尔伯特空间</strong>.支持向量机都将输入映射为特征向量,所以支持向量机的学习是在<strong>特征空间</strong>进行的.</p>
<h4 id="you-hua">优化</h4>
<p>支持向量机的最优化问题一般通过对偶问题化为<strong>凸二次规划问题</strong>求解,具体步骤是将等式约束条件代入优化目标,通过求偏导求得优化目标在不等式约束条件下的极值.</p>
<h4 id="strong-xian-xing-ke-fen-zhi-chi-xiang-liang-ji-strong"><strong>线性可分支持向量机</strong></h4>
<p>当训练数据集线性可分时,存在无穷个分离超平面可将两类数据正确分开.利用<strong>间隔最大化</strong>得到<strong>唯一</strong>最优分离超平面\(w^\star x +b = 0\)和相应的分类决策函数\(f(s)=sign(w^\star x + b^\star)\).称为线性可分支持向量机.</p>
<h5 id="han-shu-jian-ge">函数间隔</h5>
<p>一般来说,一个点距离分离超平面的<strong>远近</strong>可以表示分类预测的<strong>确信程度</strong>.在超平面\(w^\star x +b = 0\)确定的情况下,\(|wx+b|\)能够相对地表示点x距离超平面的远近,而\(wx+b\)与\(y\)的符号是否一致能够表示分类是否正确.所以可用\(\hat{\gamma}_{i}=y_{i}\left(w \cdot x_{i}+b\right)\)来表示分类的正确性及确信度,这就是<strong>函数间隔</strong>.注意到即使超平面不变,函数间隔仍会受w和b的绝对大小影响.</p>
<h5 id="strong-ji-he-jian-ge-strong"><strong>几何间隔</strong></h5>
<p>一般地,当样本点被超平面正确分类时,点x与超平面的距离是\(\gamma_{i}=y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right)\)其中\(||w||\)是\(w\)的\(l2\)范数.这就是<strong>几何间隔</strong>的定义.定义超平面关于训练数据集T的几何间隔为超平面关于T中所有样本点的几何间隔之<strong>最小值</strong>\(\gamma=\min _{i,…,N} \gamma_{1}\).可知\(\gamma=\frac{\hat{\gamma}}{\|\boldsymbol{w}\|}\)当\(||w||=1\)时几何间隔和函数间隔<strong>相等</strong>.</p>
<h5 id="strong-ying-jian-ge-zui-da-hua-strong"><strong>硬间隔最大化</strong></h5>
<p>对线性可分的训练集而言,这里的间隔最大化又称为<strong>硬间隔最大化</strong>.直观解释是对训练集找到几何间隔最大的超平面意味着以<strong>充分大的确信度</strong>对训练数据进行分类.求最大间隔分离超平面即约束最优化问题:<br>
\[
\begin{aligned}
&amp;\max _{w, b} \quad \gamma\\
&amp;\text { s.t. } \quad y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right) \geqslant \gamma, \quad i=1,2, \cdots, N
\end{aligned}
\]<br>
,将几何间隔用函数间隔表示:<br>
\[
\begin{aligned}
&amp;\max _{w, b} \frac{\hat{\gamma}}{\|w\|}\\
&amp;\text { s.t. } \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant \hat{p}, \quad i=1,2, \cdots, N
\end{aligned}
\]<br>
,并且注意到函数间隔的取值并不影响最优化问题的解,不妨令函数间隔=1,并让最大化\(\frac{1}{||w||}\)等价为最小化\(\frac{||w||^2}{2}\),问题变为<strong>凸二次规划问题</strong><br>
\[
\min _{w, b} \frac{1}{2}\|w\|^{2} \\
s.t. \quad y_{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N
\]</p>
<h5 id="strong-zhi-chi-xiang-liang-he-jian-ge-bian-jie-strong"><strong>支持向量和间隔边界</strong></h5>
<p>与分离超平面距离<strong>最近的样本点</strong>的实例称为<strong>支持向量</strong>.支持向量是使最优化问题中的约束条件等号成立的点.因此对\(y=+1\)的正例点和\(y=-1\)的负例点,支持向量分别在超平面H1:\(wx+b=+1\)和H2:\(wx+b=-1\).H1和H2平行,两者之间形成一条长带,长带的宽度!称\(\frac{2}{||w||}\)为<strong>间隔</strong>,H1和H2称为<strong>间隔边界</strong>.在决定分离超平面时只有支持向量起作用,所以支持向量机是由很少的&quot;重要的&quot;训练样本确定的.由对偶问题同样可以得到支持向量一定在间隔边界上.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/support_vector.png" alt="avatar"></p>
<h5 id="strong-dui-ou-suan-fa-strong"><strong>对偶算法</strong></h5>
<p>引进拉格朗日乘子,定义拉格朗日函数\(L(w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}\),根据拉格朗日对偶性,原始问题的对偶问题是极大极小问题:\(\max _{\alpha} \min _{w, b} L(w, b, \alpha)\).先求对w,b的<strong>极小值</strong>.将\(L(w,b,a)\)分别对w,b求偏导数并令其等于0,得<br>
\[
\begin{array}{l}
w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} \\
\sum_{i=1}^{N} \alpha_{i} y_{i}=0
\end{array}
\]<br>
,代入拉格朗日函数得<br>
\[
\begin{aligned}
L(w, b, \alpha) &amp;=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j}\right) \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i} \\
&amp;=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}
\end{aligned}
\]<br>
这就是极小值.接下来对极小值求对a的极大,即是<strong>对偶问题</strong><br>
\[
\begin{array}{l}
\max _{a}-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
\quad \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
\]<br>
.将求极大转换为求极小<br>
\[
\begin{array}{cl}
\min _{\alpha} &amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&amp; \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
\]<br>
.由<strong>KKT条件</strong>成立得到<br>
\[
\begin{aligned}
&amp;w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}\\
&amp;b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i} \cdot x_{j}\right)
\end{aligned}
\]<br>
,其中\(j\)为使\(a_j^*>0\)的下标之一.所以问题就变为求对偶问题的解\(a^*\),再求得原始问题的解\(w^*,b^*\),从而得分离超平面及分类决策函数可以看出\(w^*\)和\(b^*\)都只依赖训练数据中\(a_i^*>0\)的样本点Z\((x_i,y_i)\),这些实例点\(x_i\)被称为<strong>支持向量</strong>.</p>
<h4 id="strong-xian-xing-zhi-chi-xiang-liang-ji-strong"><strong>线性支持向量机</strong></h4>
<p>如果训练数据是<strong>线性不可分</strong>的(近似线性可分),那么上述方法中的不等式约束并不能都成立,需要修改硬间隔最大化,使其成为<strong>软间隔最大化</strong>.</p>
<h5 id="song-chi-bian-liang">松弛变量</h5>
<p>线性不可分意味着某些<strong>特异点</strong>不能满足函数间隔大于等于1的约束条件,可以对每个样本点引进一个<strong>松弛变量</strong>,使函数间隔加上松弛变量大于等于1,约束条件变为\(y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}\),同时对每个松弛变量,支付一个代价,目标函数变为\(\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}\),其中\(C>0\)称为<strong>惩罚参数</strong>,C值越大对误分类的惩罚也越大.新目标函数包含了两层含义:使<strong>间隔尽量大</strong>,同时使误分类点的<strong>个数尽量小</strong>.</p>
<h5 id="ruan-jian-ge-zui-da-hua">软间隔最大化</h5>
<p>学习问题变成如下<strong>凸二次规划</strong>问题:</p>
<p>\[
\min _{w, b, k} \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i} \\
s.t. \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N \\
\xi_{i} \geqslant 0, \quad i=1,2, \cdots, N
\]</p>
<p>,可以证明w的解是唯一的,但b的解存在一个<strong>区间</strong>.线性支持向量机包含线性可分支持向量机,因此<strong>适用性更广</strong>.</p>
<h5 id="dui-ou-suan-fa">对偶算法</h5>
<p>原始问题的对偶问题是,构造<strong>拉格朗日函数</strong> \(L(w, b, \xi, \alpha, \mu) \equiv \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}-\sum_{i=1}^{N} \alpha_{i}\left(y_{i}\left(w \cdot x_{i}+b\right)-1+\xi_{i}\right)-\sum_{i=1}^{N} \mu_{i} \xi_{i}\) ，先求对w,b,ξ的<strong>极小值</strong>,分别求偏导并令导数为0,得<br>
\[
\begin{aligned}
&amp;w=\sum_{i=1} \alpha_{i} y_{i} x_{i}\\
&amp;\sum_{i=1}^{N} \alpha_{i} y_{i}=0\\
&amp;C-\alpha_{i}-\mu_{i}=0
\end{aligned}
\]<br>
,代入原函数,再对极小值求a的<strong>极大值</strong>,得到<br>
\[
\begin{aligned}
&amp;\max _{a}-\frac{1}{2} \sum_{i=1}^{N} \sum_{i=1}^{N} \alpha_{i} \alpha, y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}\\
&amp;\text { s.t. } \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0\\
&amp;\begin{array}{l}
C-\alpha_{i}-\mu_{i}=0 \\
\alpha_{i} \geqslant 0 \\
\mu_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
\end{aligned}
\]<br>
,利用后三条约束<strong>消去μ</strong>,再将求极大转换为<strong>求极小</strong>,得到<strong>对偶问题</strong><br>
\[
\begin{aligned}
&amp;\min _{\alpha} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}\\
&amp;\text { s.t. } \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0\\
&amp;0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{aligned}
\]<br>
由<strong>KKT条件</strong>成立可以得到<br>
\[
\begin{array}{c}
w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \\
b^{*}=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{*}\left(x_{i} \cdot x_{j}\right)
\end{array}
\]<br>
\(j\)是满足\(0&lt;\alpha_j^*&lt;C\)的下标之一.问题就变为选择惩罚参数\(C>0\),求得对偶问题(<strong>凸二次规划问题</strong>)的<strong>最优解\(\alpha^*\)</strong>,代入计算\(w^*\)和\(b^*\),求得分离超平面和分类决策函数.因为\(b\)的解并不唯一,所以实际计算\(b^*\)时可以取所有样本点上的<strong>平均值</strong>.</p>
<h5 id="zhi-chi-xiang-liang">支持向量</h5>
<p>在<strong>线性不可分</strong>的情况下,将对应与\(\alpha_i^*>0\)的样本点\((x_i,y_i)\)的实例点\(x_i\)称为<strong>支持向量</strong>.软间隔的支持向量或者在间隔边界上,或者在间隔边界与分类超平面之间,或者再分离超平面误分一侧.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/support_soft.png" alt="avatar"></p>
<h5 id="he-ye-sun-shi">合页损失</h5>
<p>可以认为是0-1损失函数的上界,而线性支持向量机可以认为是优化合页损失函数构成的目标函数.</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/0_1loss.png" alt="avatar"></p>
<h4 id="strong-fei-xian-xing-zhi-chi-xiang-liang-ji-strong"><strong>非线性支持向量机</strong></h4>
<p>如果分类问题是<strong>非线性</strong>的,就要使用<strong>非线性支持向量机</strong>.主要特点是使用<strong>核技巧</strong>.</p>
<h5 id="strong-fei-xian-xing-fen-lei-wen-ti-strong"><strong>非线性分类问题</strong></h5>
<p>用线性分类方法求解非线性分类问题分为两步:首先使用一个变换将原空间的数据映射到新空间,然后在新空间里用线性分类学习方法从训练数据中学习分类模型.</p>
<h5 id="strong-he-han-shu-strong"><strong>核函数</strong></h5>
<p>设X是<strong>输入空间</strong>(欧式空间的子集或离散集合),H为<strong>特征空间</strong>(希尔伯特空间),一般是<strong>高维</strong>甚至无穷维的.如果存在一个从X到H的映射\(\phi(x): \mathcal{X} \rightarrow \mathcal{H}\)使得对所有x,z属于X,函数K(x,z)满足条件\(K(x, z)=\phi(x) \cdot \phi(z)\),点乘代表<strong>内积</strong>,则称K(x,z)为<strong>核函数</strong>.</p>
<h5 id="strong-he-ji-qiao-strong"><strong>核技巧</strong></h5>
<p>基本思想是通过一个<strong>非线性变换</strong>将输入空间对应于一个<strong>特征空间</strong>,使得在输入空间中的<strong>超曲面模型</strong>对应于特征空间中的<strong>超平面模型</strong>(支持向量机).在学习和预测中只定义核函数\(K(x,z)\),而<strong>不显式</strong>地定义映射函数.对于给定的核\(K(x,z)\),特征空间和映射函数的取法并<strong>不唯一</strong>.注意到在线性支持向量机的对偶问题中,目标函数和决策函数都只涉及输入实例与实例之间的<strong>内积</strong>,\(x_i,x_j\)可以用核函数\(K(x_i,x_j)=\phi (x_i)\phi (x_j)\)来<strong>代替</strong>.当映射函数是非线性函数时,学习到的含有核函数的支持向量机是非线性分类模型.在实际应用中,往往依赖领域知识<strong>直接选择</strong>核函数.</p>
<h5 id="strong-zheng-ding-he-strong"><strong>正定核</strong></h5>
<p>通常所说的核函数是指<strong>正定核函数</strong>.只要满足正定核的充要条件,那么给定的函数K(x,z)就是正定核函数.设K是定义在X*X上的<strong>对称函数</strong>,如果任意xi属于X,K(x,z)对应的<strong>Gram矩阵</strong>\(K=\left[K\left(x_{i}, x_{j}\right)\right]_{mxm}\)是<strong>半正定矩阵</strong>,则称\(K(x,z)\)是正定核.这一定义在构造核函数时很有用,但要验证一个具体函数是否为正定核函数并不容易,所以在实际问题中往往应用已有的核函数.</p>
<h5 id="strong-suan-fa-strong-1"><strong>算法</strong></h5>
<p>选取适当的核函数K(x,z)和适当的参数C,将线性支持向量机对偶形式中的内积换成核函数,构造并求解最优化问题</p>
<p>\[
\min _{a} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
s.t. \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\]<br>
,选择最优解\(a^*\)的一个正分量\(0&lt;a_j^*&lt;C\)计算\(b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K\left(x_{i} \cdot x_{j}\right)\),构造决策函数\(f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K\left(x \cdot x_{i}\right)+b^{*}\right)\)</p>
<h5 id="strong-chang-yong-he-han-shu-strong"><strong>常用核函数</strong></h5>
<ol>
<li><strong>多项式核函数(polynomial kernel function)</strong> :\(K(x, z)=(x \cdot z+1)^{p}\),对应的支持向量机是一个p次多项式分类器,分类决策函数为:\(f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{i}} a_{i}^{*} y_{i}\left(x_{i} \cdot x+1\right)^{p}+b^{*}\right)\)</li>
<li><strong>高斯核函数(Gaussian krenel function)</strong> :\(K(x, z)=\exp \left(-\frac{\|x-z\|^{2}}{2 \sigma^{2}}\right)\) ,对应的支持向量机是高斯径向基函数(RBF)分类器.分类决策函数为\(f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{t}} a_{i}^{*} y_{i} \exp \left(-\frac{\|x-z\|^{2}}{2 \sigma^{2}}\right)+b^{*}\right)\)</li>
<li><strong>字符串核函数(string kernel function)</strong>: 核函数不仅可以定义在欧氏空间上,还可以定义在<strong>离散数据的集合</strong>上.字符串核函数给出了字符串中长度等于n的所有子串组成的特征向量的余弦相似度.</li>
</ol>
<h5 id="strong-xu-lie-zui-xiao-zui-you-hua-smo-suan-fa-strong"><strong>序列最小最优化(SMO)算法</strong></h5>
<p>SMO是一种<em>快速求解凸二次规划问题</em><br>
\[
\begin{aligned}
&amp;\min _{\alpha} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}\\
&amp;\text { s.t. } \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0\\
&amp;0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{aligned}
\]<br>
的算法.基本思路是:如果所有变量都满足此优化问题的KKT条件,那么解就得到了.否则,选择<strong>两个变量</strong>,固定其他变量,针对这两个变量构建一个二次规划问题.不断地将原问题分解为<strong>子问题</strong>并对子问题求解,就可以求解原问题.注意子问题两个变量中只有一个是<strong>自由变量</strong>,另一个由<strong>等式约束</strong>确定.</p>
<h5 id="strong-liang-ge-bian-liang-er-ci-gui-hua-de-qiu-jie-fang-fa-strong"><strong>两个变量二次规划的求解方法</strong></h5>
<p>假设选择的两个变量是a1,a2,其他变量是固定的,于是得到子问题<br>
\[
\begin{aligned}
&amp;\begin{aligned}
\min _{\alpha, \alpha_{i}} W\left(\alpha_{1}, \alpha_{2}\right)=&amp; \frac{1}{2} K_{11} \alpha_{1}^{2}+\frac{1}{2} K_{22} \alpha_{2}^{2}+y_{1} y_{2} K_{12} \alpha_{1} \alpha_{2} \\
&amp;-\left(\alpha_{1}+\alpha_{2}\right)+y_{1} \alpha_{1} \sum_{i=1}^{N} y_{i} \alpha_{i} K_{n}+y_{2} \alpha_{2} \sum_{i=1}^{N} y_{i} \alpha_{i} K_{i}
\end{aligned}\\
&amp;\text { s.t. } \quad \alpha_{1} y_{1}+\alpha_{2} y_{2}=-\sum_{i=3}^{N} y_{i} \alpha_{i}=\zeta\\
&amp;0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2
\end{aligned}
\]<br>
,\(\epsilon\)是常数,目标函数式省略了不含\(a_1,a_2\)的常数项.考虑不等式约束和等式约束,要求的是目标函数在一条平行于对角线的线段上的最优值</p>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/1.png" alt="avatar"></p>
<p>问题变为<strong>单变量</strong>的最优化问题.假设初始可行解为aold,最优解为anew,考虑沿着约束方向未经剪辑的最优解anew,unc(即未考虑不等式约束).对该问题求偏导数,并令导数为0,代入原式,令\(E_{i}=g\left(x_{i}\right)-y_{i}=\left(\sum_{j=1}^{N} \alpha, y_{j} K\left(x_{j}, x_{i}\right)+b\right)-y_{i}, \quad i=1,2\),得到\(\alpha_{2}^{\text {new, } \text { unc }}=\alpha_{2}^{\text {old }}+\frac{y_{2}\left(E_{1}-E_{2}\right)}{\eta}\),经剪辑后a2的解是<br>
\[
\alpha_{2}^{\text {new }}=\left\{\begin{array}{ll}H, &amp; \alpha_{2}^{\text {new }, \text { unc }}>H \\ \alpha_{2}^{\text {new }, \text { unc }}, &amp; L \leqslant \alpha_{2}^{\text {new}, \text { unc}} \leqslant H \\ L, &amp; \alpha_{2}^{\text {new }, \text { unc }}&lt;L\end{array}\right.
\]<br>
L与H是\(a_2^{new}\)所在的对角线段端点的界.并解得\(\alpha_{1}^{\mathrm{new}}=\alpha_{1}^{\mathrm{old}}+y_{1} y_{2}\left(\alpha_{2}^{\mathrm{old}}-\alpha_{2}^{\mathrm{new}}\right)\)</p>
<h5 id="strong-bian-liang-de-xuan-ze-fang-fa-strong"><strong>变量的选择方法</strong></h5>
<p>在每个子问题中选择两个变量优化,其中至少一个变量是违反KKT条件的.第一个变量的选取标准是<strong>违反KKT条件最严重</strong>的样本点,第二个变量的选取标准是希望能使该变量有<strong>足够大的变化</strong>,一般可以选取使对应的\(|E1-E2|\)最大的点.在每次选取完点后,<strong>更新</strong>阈值\(b\)和差值\(Ei\).</p>
<h3 id="ti-sheng-fang-fa">提升方法</h3>
<h4 id="ti-sheng-fang-fa-1">提升方法</h4>
<p>boosting是<strong>一种常用的统计学习方法,是集成学习的一种.它通过改变训练样本的权重(概率分布),学习</strong>多个<strong>弱分类器(基本分类器),并将这些分类器</strong>线性组合**来构成一个强分类器提高分类的性能.</p>
<h4 id="jia-fa-mo-xing-he-qian-xiang-fen-bu-suan-fa">加法模型和前向分步算法</h4>
<p>加法模型\(f(x)= \sum_{m=1}^M{\beta_mb(x;\gamma_m)}\),其中，\(b(x;\gamma_m)\)为基函数，\(\gamma_m\)为基函数参数，\(\beta_m\)为基函数的系数。在给定训练数据及损失函数\(L(y,f(x))\)的条件下，学习加法模型\(f(x)\)成为经验风险极小化即损失函数极小化问题：<br>
\[
min_{\beta_m,\gamma_m}\sum_{m=1}^{M}{\beta_m b(x_i;\gamma_m)}
\]<br>
通常这是一个复杂的优化问题。</p>
<p>前向分步算法求解这一优化问题的想法是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数：\(\left(\beta_{m}, \gamma_{m}\right)=\arg \min _{\beta, y} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\beta b\left(x_{i} ; \gamma\right)\right)\)，得到参数βm和γm,更新\(f_{m}(x)=f_{m-1}(x)+\beta_{m} b\left(x ; \gamma_{m}\right)\),逐步逼近优化目标函数式，那么就可以简化优化的复杂度，最终得到加法模型。</p>
<h4 id="strong-ada-boost-strong"><strong>AdaBoost</strong></h4>
<h5 id="si-xiang">思想：</h5>
<p>AdaBoost提高那些被前一轮弱分类器错误分类样本的权值,而降低那些被正确分类样本的权值.然后采取<strong>加权多数表决</strong>的方法组合弱分类器.</p>
<h5 id="strong-suan-fa-strong-2"><strong>算法</strong></h5>
<p>首先假设训练数据集具有均匀的权值分布D1,使用具有<strong>权值分布</strong>Dm的训练数据集学习得到<strong>基本分类器</strong>Gm(x),计算<strong>分类误差率</strong>\(e_{m}=P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)\)和Gm(x)的<strong>系数</strong>\(\alpha_{m}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}\),更新训练数据集的权值分布\(D_{m+1}=\left(w_{m+1,1}, \cdots, w_{m+1, i}, \cdots, w_{m+1, N}\right)\)，其中<br>
\[
w_{m+1, i}=\left\{\begin{array}{ll}
\frac{w_{mi}}{Z_{m}} \mathrm{e}^{-\alpha_{m}}, &amp; G_{m}\left(x_{i}\right)=y_{i} \\
\frac{w_{m i}}{Z_{m}} \mathrm{e}^{\alpha_{m}}, &amp; G_{m}\left(x_{i}\right) \neq y_{i}
\end{array}\right.
\]<br>
\(Z_m\)是使Dm+1成为概率分布的<strong>规范化因子</strong>\(Z_{m}=\sum_{i=1}^{N} w_{m i} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right)\).重复上述操作M次后得到M个弱分类器,构建线性组合得到<strong>最终分类器</strong>\(G(x)=\operatorname{sign}(f(x))=\operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)\)</p>
<h4 id="strong-ti-sheng-shu-strong"><strong>提升树</strong></h4>
<p>提升树是模型为加法模型,算法为前向分布算法,基函数为<strong>决策树</strong>的提升方法.第m步的模型是\(f_{m}(x)=f_{m-1}(x)+T\left(x ; \Theta_{m}\right)\),通过经验风险极小化确定下一棵决策树的参数\(\hat{\Theta}_{m}=\arg \min _{\theta_{m}} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \Theta_{m}\right)\right)\).不同问题的提升树学习算法主要区别在于使用的<strong>损失函数</strong>不同.</p>
<h5 id="strong-er-lei-fen-lei-wen-ti-strong"><strong>二类分类问题</strong></h5>
<p>只需将AdaBoost算法中的基本分类器限制为二类分类数即可.</p>
<h5 id="strong-hui-gui-wen-ti-strong"><strong>回归问题</strong></h5>
<p>如果将输入空间划分为J个互不相交的区域,并且在每个区域上确定输出的常量\(C_j\),那么树可表示为\(T(x ; \Theta)=\sum_{j=1}^{J} c_{j} I\left(x \in R_{j}\right)\),其中\(\Theta=\left\{\left(R_{1}, c_{1}\right),\left(R_{2}, c_{2}\right), \cdots,\left(R_{J}, c_{J}\right)\right\}\)提升树采用<strong>前向分步算法</strong>:<br>
\[
\begin{aligned}
&amp;f_{0}(x)=0\\
&amp;\begin{array}{l}
f_{m}(x)=f_{m-1}(x)+T\left(x ; \Theta_{m}\right), m=1,2, \cdots, M \\
f_{M}(x)=\sum_{m=1}^{M} T\left(x ; \Theta_{m}\right)
\end{array}
\end{aligned}
\]<br>
.当采用平方误差损失函数时,损失变为<br>
\[
\begin{array}{l}
L\left(y, f_{m-1}(x)+T\left(x ; \Theta_{m}\right)\right) \\
\quad=\left[y-f_{m-1}(x)-T\left(x ; \Theta_{m}\right)\right]^{2} \\
\quad=\left[r-T\left(x ; \Theta_{m}\right)\right]^{2}
\end{array}
\]<br>
,其中r是当前模型拟合数据的<strong>残差</strong>.每一步都只需<strong>拟合残差</strong>学习一个回归树即可.</p>
<h5 id="cun-zai-de-que-dian">存在的缺点</h5>
<p>当损失函数式平方误差损失函数或者交叉熵损失函数的时候，残差即为对应的梯度，这个时候损失函数沿着梯度的方向下降最快。当损失函数式其他损失函数的时候，残差和损失函数的导数并不相等，函数收敛的速度就会没有沿着梯度的方向收敛的快。</p>
<h4 id="ti-du-ti-sheng-shu-gbdt">梯度提升树(GBDT)</h4>
<p>利用最速下降法的近似方法来实现每一步的优化,关键在于用损失函数的<strong>负梯度</strong>在当前模型的值\(-\left[\frac{\partial L\left(y, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right]_{f(x)=f_{-1}(x)}\)作为回归问题中提升树算法中的残差的<strong>近似值</strong>,每一步以此来估计回归树叶结点区域以拟合残差的近似值,并利用线性搜索估计叶结点区域的值使损失函数最小化,然后更新回归树即可.</p>
<h4 id="xgboost">Xgboost</h4>
<p>相比传统GBDT有以下优点:</p>
<ol>
<li>在优化时用到了二阶导数信息.</li>
<li>在代价函数里加入了正则项.</li>
<li>每次迭代后都将叶子结点的权重乘上一个系数,削弱每棵树的影响.</li>
<li>列抽样.</li>
<li>在训练前对数据进行排序,保存为block结构,并行地对各个特征进行增益计算.</li>
</ol>
<h3 id="em-suan-fa">EM算法</h3>
<p>EM算法是一种<strong>迭代</strong>算法,用于含有<strong>隐变量</strong>的概率模型参数的极大似然估计.每次迭代由两步组成:E步,求<strong>期望</strong>(expectation),M步,求<strong>极大值</strong>(maximization),直至收敛为止.</p>
<h4 id="strong-yin-bian-liang-strong"><strong>隐变量</strong></h4>
<p>不能被直接观察到，但是对系统的状态和能观察到的输出存在影响的一种东西.</p>
<h4 id="strong-suan-fa-strong-3"><strong>算法</strong></h4>
<ol>
<li>选择参数的初始值θ(0),开始迭代.注意EM算法对初值是<strong>敏感</strong>的.</li>
<li><strong>E步</strong>:θ(i)为第i次迭代参数θ的估计值,在第i+1次迭代的E步,计算\(\begin{aligned} Q\left(\theta, \theta^{(i)}\right) &amp;=E_{z}\left[\log P(Y, Z | \theta) | Y, \theta^{(i)}\right] =\sum_{z} \log P(Y, Z | \theta) P\left(Z | Y, \theta^{(i)}\right) \end{aligned}\) ,\(P(Z|Y,θ(i))\)是在给定<strong>观测数据</strong>Y和当前参数估计θ(i)下<strong>隐变量数据</strong>Z的条件概率分布.</li>
<li><strong>M步</strong>:求使Q(θ,θ(i))极大化的θ,确定第i+1次迭代的参数的估计值\(\theta^{(i+1)}=\arg \max _{\theta} Q\left(\theta, \theta^{(i)}\right)\)</li>
<li>重复2和3直到<strong>收敛</strong>,一般是对较小的正数\(\varepsilon1\)和\(\varepsilon2\)满足\(\left\|\theta^{(i+1)}-\theta^{(i)}\right\|&lt;\varepsilon_{1} \quad\) 或\(\quad\left\|Q\left(\theta^{(i+1)}, \theta^{(i)}\right)-Q\left(\theta^{(i)}, \theta^{(i)}\right)\right\|&lt;\varepsilon_{2}\)则停止迭代.</li>
</ol>
<h4 id="ying-yong">应用</h4>
<p>EM算法是通过不断求解<strong>下界</strong>的极大化逼近求解对数似然函数极大化的算法.可以用于生成模型的<strong>非监督学习</strong>.生成模型由联合概率分布P(X,Y)表示.X为观测数据,Y为未观测数据.</p>
<h3 id="yin-ma-er-ke-fu-mo-xing-hmm">隐马尔科夫模型(HMM)</h3>
<p>隐马尔可夫模型是关于<strong>时序</strong>的概率模型,描述由一个隐藏的马尔可夫链随机生成不可观测的<strong>状态序列</strong>,再由各个状态生成一个观测而产生<strong>观测随机序列</strong>的过程.HMM是一种生成式模型，它的理论基础是朴素贝叶斯，本质上就类似于我们将朴素贝叶斯在单样本分类问题上的应用推广到序列样本分类问题上。</p>
<h4 id="mo-xing-biao-shi">模型表示</h4>
<p>设Q是所有可能的状态的集合\(Q=\left\{q_{1}, q_{2}, \cdots, q_{N}\right\}\),V是所有可能的观测的集合\(V=\left\{v_{1}, v_{2}, \cdots, v_{M}\right\}\),I是长度为T的状态序列\(I=\left(i_{1}, i_{2}, \cdots, i_{T}\right)\), O是对应的观测序列\(O=\left(o_{1}, o_{2}, \cdots, o_{T}\right)\),</p>
<ol>
<li>A是<strong>状态转移概率矩阵</strong>\(A=\left[a_{i j}\right]_{N \times N}\),\(a_{ij}\)表示在时刻t处于状态\(q_i\)的条件下在时刻t+1转移到状态\(q_j\)的概率.</li>
<li>.B是<strong>观测概率矩阵</strong> \(B=\left[b_{j}(k)\right]_{N \times M}\),\(b_{ij}\)是在时刻t处于状态\(q_j\)的条件下生成观测\(v_k\)的概率.</li>
<li>\(\pi\)是<strong>初始状态概率向量</strong>\(\pi=\pi(x)\),\(\pi_i\)表示时刻t=1处于状态qi的概率.</li>
</ol>
<p>隐马尔可夫模型由初始状态概率向量\(pi\),状态转移概率矩阵A以及观测概率矩阵B确定.\(\pi\)和A决定即隐藏的<strong>马尔可夫链</strong>,生成不可观测的<strong>状态序列</strong>.B决定如何从状态生成观测,与状态序列综合确定了<strong>观测序列</strong>.因此,隐马尔可夫模型可以用<strong>三元符号</strong>表示\(\lambda = (A,B,\pi)\)</p>
<h4 id="liang-ge-ji-ben-jia-she">两个基本假设</h4>
<ol>
<li><strong>齐次马尔可夫性假设</strong>:假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态.</li>
<li><strong>观测独立性假设</strong>:假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态.</li>
</ol>
<h4 id="san-ge-ji-ben-wen-ti">三个基本问题</h4>
<h5 id="strong-1-gai-lu-ji-suan-wen-ti-strong"><strong>1. 概率计算问题</strong></h5>
<p>给定模型\(\lambda = (A,B,\pi)\)和观测序列,\(O=\left(o_{1}, o_{2}, \cdots, o_{T}\right)\)计算在模型\(\lambda\)下观测序列O出现的概率\(P(O|λ)\).</p>
<ol>
<li>直接计算：列举所有可能长度为T的状态序列,求各个状态序列I与观测序列O的联合概率,但计算量太大,实际操作不可行.</li>
<li><strong>前向算法</strong>：定义到时刻t部分观测序列为\(o_1\)~\(o_t\)且状态为\(q_i\)的概率为<strong>前向概率</strong>,记作\(\alpha_{t}(i)=P\left(o_{1}, o_{2}, \cdots, o_{t}, i_{t}=q_{i} | \lambda\right)\).初始化前向概率\(\alpha_{1}(i)=\pi_{i} b_{i}\left(o_{1}\right), \quad i=1,2, \cdots, N\)，递推，对\(t=1\) ~ \(T-1\),\(\alpha_{t+1}(i)=\left[\sum_{j=1}^{N} \alpha_{t}(j) a_{j i}\right] b_{i}\left(o_{t+1}\right), \quad i=1,2, \cdots, N\),得到\(P(O | \lambda)=\sum_{i=1}^{N} \alpha_{T}(i)\)减少计算量的原因在于每一次计算直接引用前一个时刻的计算结果,避免重复计算.</li>
<li><strong>后向算法</strong>:定义在时刻t状态为\(q_i\)的条件下,从t+1到T的部分观测序列为\(o_{i+1}\)~\(o_T\)的概率为<strong>后向概率</strong>,记作\(\beta_{t}(i)=P\left(o_{t+1}, o_{t+2}, \cdots, o_{r} | i_{t}=q_{i}, \lambda\right)\).初始化后向概率\(\beta_{r}(i)=1, \quad i=1,2, \cdots, N\),递推,对\(t=T-1\)~\(1\)\(\beta_{t}(i)=\sum_{j=1}^{N} a_{i j} b_{j}\left(o_{i+1}\right) \beta_{i+1}(j), \quad i=1,2, \cdots, N\),得到\(P(O | \lambda)=\sum_{i=1}^{N} \pi_{i} b_{i}\left(o_{1}\right) \beta_{1}(i)\)</li>
</ol>
<h5 id="strong-2-xue-xi-suan-fa-strong"><strong>2. 学习算法</strong></h5>
<p>已知观测序列\(O=(o_1,o_2, \cdots,o_r)\),估计模型\(\lambda = (A,B,\pi)\),的参数,使得在该模型下观测序列概率\(p(O|\lambda)\)最大.根据训练数据是否包括观察序列对应的状态序列分别由监督学习与非监督学习实现.</p>
<ol>
<li>
<p>监督学习：估计转移概率\(\hat{a}_{i j}=\frac{A_{i j}}{\sum_{j=1}^{N} A_{i j}}, \quad i=1,2, \cdots, N ; \quad j=1,2, \cdots, N\) 和观测概率\(\hat{b}_{j}(k)=\frac{B_{j k}}{\sum_{k=1}^{M} B_{j k}}, \quad j=1,2, \cdots, N, k=1,2, \cdots, M\).初始状态概率\(\pi_i\)的估计为S个样本中初始状态为\(q_i\)的频率.</p>
</li>
<li>
<p><strong>非监督学习(Baum-Welch算法)</strong>:将观测序列数据看作观测数据O,状态序列数据看作不可观测的<strong>隐数据</strong>I.首先确定完全数据的对数似然函数\(log p(O,I|\lambda)\),求Q函数<br>
\[
\begin{aligned}
Q(\lambda, \bar{\lambda})=&amp; \sum_{t} \log \pi_{i_1} P(O, I | \bar{\lambda}) \\
&amp;+\sum_{I}\left(\sum_{i=1}^{I-1} \log a_{i_t,i_{t+1}}\right) P(O, I | \bar{\lambda}) \\
&amp;+\sum_{I}\left(\sum_{i=1}^{T} \log b_{i_t}\left(o_{t}\right)\right) P(O, I | \bar{\lambda})
\end{aligned}
\]<br>
,用拉格朗日乘子法极大化Q函数求模型参数\(\pi_{i}=\frac{P\left(O, i_{1}=i | \bar{\lambda}\right)}{P(O | \bar{\lambda})}\),\(a_{i j}=\frac{\sum_{i=1}^{T-1} P\left(O, i_{t}=i, i_{t+1}=j | \bar{\lambda}\right)}{\sum_{t=1}^{T-1} P\left(O, i_{t}=i | \bar{\lambda}\right)}\),\(b_{j}(k)=\frac{\sum_{i=1}^{T} P\left(O, i_{t}=j | \bar{\lambda}\right) I\left(o_{i}=v_{k}\right)}{\sum_{i=1}^{T} P\left(O, i_{t}=j | \bar{\lambda}\right)}\),</p>
</li>
</ol>
<h5 id="strong-3-yu-ce-wen-ti-strong"><strong>3. 预测问题</strong></h5>
<p>也称为解码问题.已知模型\(\lambda = (A,B,\pi)\)和观测序列\(O=(O_1,O_2,\cdots,O_T)\),求对给定观测序列条件概率\(P(I|O)\)最大的状态序列\(I=(i_1,i_2,\cdots,i_T)\)</p>
<ol>
<li>
<p><strong>近似算法</strong>: 在每个时刻t选择在该时刻最有可能出现的状态\(i_t^*\),从而得到一个状态序列作为预测的结果.优点是<strong>计算简单</strong>,缺点是不能保证状态序列整体是最有可能的状态序列</p>
</li>
<li>
<p><strong>维特比算法</strong>:用<strong>动态规划</strong>求概率最大路径,这一条路径对应着一个状态序列.从t=1开始,递推地计算在时刻t状态为i的各条部分路径的最大概率,直至得到时刻t=T状态为i的各条路径的最大概率.时刻t=T的最大概率即为<strong>最优路径</strong>的概率\(P^\star\),最优路径的<strong>终结点</strong>\(i_t^\star\)也同时得到,之后从终结点开始由后向前逐步求得<strong>结点</strong>,得到最优路径.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(obs, states, Pi, A, B)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param obs:观测序列</span></span><br><span class="line"><span class="string">    :param states:隐状态</span></span><br><span class="line"><span class="string">    :param Pi:初始概率（隐状态）</span></span><br><span class="line"><span class="string">    :param A:转移概率（隐状态）</span></span><br><span class="line"><span class="string">    :param B: 发射概率 （隐状态表现为显状态的概率）</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 路径概率表 V[时间][隐状态] = 概率</span></span><br><span class="line">    V = [&#123;&#125;]</span><br><span class="line">    <span class="comment"># 一个中间变量，代表当前状态是哪个隐状态</span></span><br><span class="line">    path = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化初始状态 (t == 0)</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">        V[<span class="number">0</span>][y] = Pi[y] * B[y][obs[<span class="number">0</span>]]</span><br><span class="line">        path[y] = [y]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对 t &gt; 0 跑一遍维特比算法</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, len(obs)):</span><br><span class="line">        V.append(&#123;&#125;)</span><br><span class="line">        newpath = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">            <span class="comment"># 概率 隐状态 =    前状态是y0的概率 * y0转移到y的概率 * y表现为当前状态的概率</span></span><br><span class="line">            (prob, state) = max([(V[t - <span class="number">1</span>][y0] * A[y0][y] * B[y][obs[t]], y0) <span class="keyword">for</span> y0 <span class="keyword">in</span> states])</span><br><span class="line">            <span class="comment"># 记录最大概率</span></span><br><span class="line">            V[t][y] = prob</span><br><span class="line">            <span class="comment"># 记录路径</span></span><br><span class="line">            newpath[y] = path[state] + [y]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 不需要保留旧路径</span></span><br><span class="line">        path = newpath</span><br><span class="line"></span><br><span class="line">    print_dptable(V)</span><br><span class="line">    (prob, state) = max([(V[len(obs) - <span class="number">1</span>][y], y) <span class="keyword">for</span> y <span class="keyword">in</span> states])</span><br><span class="line">    <span class="keyword">return</span> (prob, path[state])</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 打印路径概率表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_dptable</span><span class="params">(V)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"    "</span>,</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(V)): <span class="keyword">print</span> <span class="string">"%7d"</span> % i,</span><br><span class="line">    <span class="keyword">print</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> V[<span class="number">0</span>].keys():</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"%.5s: "</span> % y,</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(len(V)):</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"%.7s"</span> % (<span class="string">"%f"</span> % V[t][y]),</span><br><span class="line">        <span class="keyword">print</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="zui-da-shang-ma-er-ke-fu-mo-xing-memm">最大熵马尔科夫模型(MEMM)</h3>
<p>最大熵马尔科夫模型利用判别式模型的特点，直接对每一个时刻的状态建立一个分类器，然后将所有的分类器的概率值连乘起来\(P\left(y_{1}^{n} | x_{1}^{n}\right)=\prod_{t=1}^{n} P\left(y_{t} | y_{t-1}, x_{t}\right)\)。为了实现是对整个序列进行的分类，在每个时刻t时，它的特征不仅来自当前观测值\(x_t\)，而且还来自前一状态值\(y_{t-1}\),通过最大熵分类器建模\(P\left(y_{t}=y^{*} | y_{t-1}=y^{\prime}, x_{t}\right)=\frac{1}{Z\left(x_{t}, y^{\prime}\right)} \exp \left(\sum_{a} \lambda_{a} f_{a}\left(x_{t}, y^{\prime}, y^{*}\right)\right)\),其中，\(Z\left(x_{t}, y \prime\right)=\sum_{y} \exp \left(\sum_{a} \lambda_{a} f_{a}\left(x_{t}, y \prime, y\right)\right)\)，</p>
<h4 id="biao-zhu-pian-zhi-wen-ti">标注偏置问题</h4>
<p>使用维特比算法进行解码时，\(v_{t}(j)=\max _{i} v_{t-1}(i) * P\left(y_{j} | y_{i}, x_{t}\right) 1 \leq j \leq n, 1&lt;t&lt;T\)。最大熵模型在每一个时刻，针对不同的前一状态y′进行归一化操作，这是一种局部的归一化操作，会存在标签偏置问题。</p>
<h5 id="li-zi">例子</h5>
<p><img src="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/label_bias.png" alt="avatar"></p>
<p>状态转换(1→2),(2→3),(4→5),(5→3)的概率值都是1，而无论观测值是什么，换言之有\(P(2|1,i)=P(2|1,o)=1\)</p>
<p>你可能会很惊讶\(P(2∣1,i)=1,P(2∣1,o)=1\)怎么可能会成立呢？你可以套用上面的公式试一试，由于状态&quot;1&quot;的只能转换为&quot;2&quot;，所以计算归一化项时， 其实只有一个枚举值，就是状态&quot;2&quot;，所以无论你分子为多少，分母都和它一样，所以概率值就是1。在这种情况下，其实观测值并没有任何作用，这就是标签偏置。</p>
<h5 id="strong-hou-guo-strong"><strong>后果：</strong></h5>
<p>它会造成什么后果呢？<br>
他会导致模型进行预测时只依赖数据统计出来的概率值，没有利用到样本的特征。<br>
假设训练集现在有3个rib和1个rob，当我们在测试阶段，遇到词rob，它会被解码成什么状态序列呢？答案是(0→1→2→3)！你可以套公式试一试，因为\(P(1∣0,r)>P(4∣0,r)\),\(P(2∣1,o)=P(5∣4,0)=1,P(3∣2,b)>P(3∣5,b)\)。</p>
<h5 id="strong-yuan-yin-strong"><strong>原因</strong></h5>
<p>那么问题出在哪里呢？因为MEMM中在每一时刻t，都在前一时刻某状态y′下做了局部的归一化操作，如何解决这种标签偏置问题呢？<br>
在CRF中并不是对每个时刻都进行一次分类，而是直接对整个序列进行分类，做一个全局的归一化操作。</p>
<h3 id="tiao-jian-sui-ji-chang-crf">条件随机场CRF</h3>
<h4 id="gai-lu-tu-mo-xing">概率图模型</h4>
<p>结点表示随机变量，边表示随机变量之间的概率依赖关系。</p>
<h5 id="ma-er-ke-fu-xing">马尔科夫性</h5>
<ol>
<li>成对马尔可夫性</li>
<li>局部马尔科夫性</li>
<li>全局马尔科夫性</li>
</ol>
<h5 id="gai-lu-wu-xiang-tu-mo-xing-de-yin-shi-fen-jie">概率无向图模型的因式分解</h5>
<h6 id="tuan-yu-zui-da-tuan">团与最大团</h6>
<p>无向图G中任何两个结点均有边连接的结点子集称为<strong>团</strong>。若C是无向图G的一个团，并且不能再加进任何一个G的结点使其成为一个更大的团，称此C为<strong>最大团</strong>。</p>
<p>给定概率无向图模型，设其无向图为G，C为G上的最大团，\(Y_C\)表示C对应的随机变量。那么概率无向图模型的联合概率分布\(P(Y)\)可写作图中所有最大团C上的函数\(\phi_C(Y_C)\)的乘积的形式。</p>
<p><strong>Hammersley-Clifford定理</strong><br>
概率无向图模型的联合概率分布\(P(Y)\)可以表示为如下形式：<br>
\[
P(Y)=\frac{1}{Z}\prod_C{\phi_C(Y_C)}\\
Z=\sum_Y\prod_C{\phi_C(Y_C)}
\]</p>
<h5 id="tiao-jian-sui-ji-chang-crf-1">条件随机场CRF</h5>
<p>条件随机场基于概率无向图模型，利用最大团理论，对随机变量的联合分布\(P(Y)\)进行建模。</p>
<p>在序列标注任务中的条件随机场，往往是指<strong>线性链条件随机场</strong></p>
<h6 id="tiao-jian-sui-ji-chang-de-can-shu-hua-xing-shi">条件随机场的参数化形式</h6>
<p>\[
P(y|x)=\frac{1}{Z(x)} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, j} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right) 
\\
Z(x)=\sum_{y} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, j} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right)
\]</p>
<h6 id="tiao-jian-sui-ji-chang-de-jian-hua-xing-shi">条件随机场的简化形式</h6>
<p>\[
\begin{aligned}
P(y | x) &amp;=\frac{1}{Z(x)} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x) \\
Z(x) &amp;=\sum_{y} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x)
\end{aligned}
\]</p>
<h6 id="tiao-jian-sui-ji-chang-de-ju-zhen-xing-shi">条件随机场的矩阵形式</h6>
<p>\[
P_{w}(y | x)=\frac{1}{Z_{w}(x)} \prod_{i=1}^{n+1} M_{i}\left(y_{i-1}, y_{i} | x\right)\\
Z_{w}(x)=\left(M_{1}(x) M_{2}(x) \cdots M_{n+1}(x)\right)_{\text {start, stop }}
\]</p>
<h3 id="k-means">K-Means</h3>
<p>K-Means是<strong>无监督</strong>的<strong>聚类</strong>算法.思想是对于给定的样本集,按照样本之间的距离大小将样本集划分为K个簇,让簇内的点尽量紧密地连在一起,而让簇间的距离尽量的大.</p>
<h4 id="chuan-tong-suan-fa">传统算法</h4>
<ol>
<li>用先验知识或交叉验证选择一个合适的<strong>k</strong>值.</li>
<li>随机选择k个样本作为初始的<strong>质心</strong>.注意初始化质心的选择对最后的聚类结果和运行时间都有很大的影响.</li>
<li>计算每个样本点和各个质心的距离,将样本点标记为<strong>距离最小</strong>的质心所对应的簇.</li>
<li>重新计算每个<strong>簇</strong>的质心,取该簇中每个点位置的平均值.</li>
<li>重复2,3,4步直到k个质心都没有发生变化为止.</li>
</ol>
<h4 id="k-means-1">K-Means++</h4>
<p>用于优化随机初始化质心的方法</p>
<ol>
<li>从输入样本点中随机选择一个点作为第一个质心.</li>
<li>计算每一个样本点到已选择的质心中<strong>最近质心</strong>的距离D(x).</li>
<li>选择一个新的样本点作为新的质心,选择原则是D(x)越大的点被选中的概率越大.</li>
<li>重复2和3直到选出k个质心.</li>
</ol>
<h4 id="strong-elkan-k-means-strong"><strong>Elkan K-Means</strong></h4>
<p>利用两边之和大于第三边以及两边之差小于第三边来减少距离的计算.不适用于特征稀疏的情况.</p>
<h4 id="strong-mini-batch-k-means-strong"><strong>Mini Batch K-Means</strong></h4>
<p>样本量很大时,只用其中的一部分来做传统的K-Means.一般多用几次该算法,从不同的随即采样中选择最优的聚类簇.</p>
<h3 id="apriori">Apriori</h3>
<p>Apriori是常用的挖掘出<strong>数据关联规则</strong>的算法,用于找出数据值中<strong>频繁</strong>出现的数据集合.一般使用支持度或者支持度与置信度的组合作为<strong>评估标准</strong>.</p>
<ol>
<li>支持度：几个关联的数据在数据集中出现的次数占总数据集的比重Support\((X, Y)=P(X Y)=\frac{\text {number}(X Y)}{\text {num}(\text {AllSamples})}\)</li>
<li><strong>置信度</strong>：一个数据出现后.另一个数据出现的概率Confidence\((X \Leftarrow Y)=P(X | Y)=P(X Y) / P(Y)\)</li>
</ol>
<p>Apriori算法的目标是找到最大的<strong>K项频繁集</strong>.假设使用支持度来作为评估标准,首先搜索出<strong>候选1项集</strong>及对应的支持度,<strong>剪枝</strong>去掉低于支持度的1项集,得到<strong>频繁1项集</strong>.然后对剩下的频繁1项集进行<strong>连接</strong>,得到候选的频繁2项集…以此类推,不断迭代,直到无法找到频繁k+1项集为止,对应的频繁k项集的集合即为输出结果.</p>
<h3 id="can-kao">参考</h3>
<ol>
<li><a href="https://blog.csdn.net/qq_20989105/article/details/81218696" target="_blank" rel="noopener">HMM隐马尔可夫模型与viterbi维特比算法</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title>python速查表</title>
    <url>/2020/01/12/basic_skills/quick_check_list/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2020/01/12/basic_skills/quick_check_list/sousuoying.jpg" alt></p>
<a id="more"></a>
<h1 id="python">python</h1>
<p><a href="https://docs.python.org/zh-cn/3.6/tutorial/index.html" target="_blank" rel="noopener">python api</a></p>
<h1 id="numpy">numpy</h1>
<p><a href="https://www.numpy.org.cn/reference/" target="_blank" rel="noopener">numpy api</a></p>
<h1 id="pandas">pandas</h1>
<p><a href="https://www.pypandas.cn/docs/" target="_blank" rel="noopener">pandas api</a></p>
<h1 id="matplotlib">matplotlib</h1>
<p><a href="https://matplotlib.org/gallery.html" target="_blank" rel="noopener">examples</a></p>
<p><a href="https://www.matplotlib.org.cn/tutorials/#%E5%BA%8F%E8%A8%80" target="_blank" rel="noopener">user tutorial</a></p>
<h1 id="seaborn">seaborn</h1>
<p><a href="http://seaborn.pydata.org/examples/index.html" target="_blank" rel="noopener">examples</a></p>
<p><a href="http://seaborn.pydata.org/tutorial.html" target="_blank" rel="noopener">user tutorial</a></p>
<p><a href="http://seaborn.pydata.org/api.html" target="_blank" rel="noopener">api</a></p>
]]></content>
      <categories>
        <category>技术/API</category>
      </categories>
  </entry>
  <entry>
    <title>scrapy</title>
    <url>/2019/08/31/scrapy/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id></h3>
<p><img src="/2019/08/31/scrapy/scrapy_framework.png" alt="avatar"></p>
<a id="more"></a>
<p><a href="https://scrapy-chs.readthedocs.io/zh_CN/1.0/" target="_blank" rel="noopener">scrapy中文文档</a></p>
<h3 id="1-bian-xie-scrapy-pa-chong-bu-zou">1. 编写scrapy爬虫步骤</h3>
<ol>
<li>
<p>新建项目：（scrapy startproject projectname）:新建爬虫项目</p>
</li>
<li>
<p>创建爬虫：scrapy genspider spidername “<a href="http://www.xxx.cn/" target="_blank" rel="noopener">http://www.xxx.cn/</a>”</p>
</li>
<li>
<p>明确目标：（<a href="http://xn--items-by5h803y.py" target="_blank" rel="noopener">编写items.py</a>）：明确想要抓取的目标</p>
</li>
<li>
<p>制作爬虫：（spiders/xxspider.py）:制作爬虫开始爬取的网页</p>
</li>
<li>
<p><a href="http://xn--pipeline-ts6mn078a.py" target="_blank" rel="noopener">编写pipeline.py</a>，处理spider返回的item数据。写Pipeline函数</p>
</li>
<li>
<p><a href="http://xn--settings-ts6mn078a.py" target="_blank" rel="noopener">编写settings.py</a>,启动管道组件ITEM_PIPELINES={}，以及其他相关设置USER_AGENT,DEFAULT_REQUEST_HEADERS</p>
</li>
<li>
<p>执行爬虫</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> cmdline</span><br><span class="line">cmdline.execute([<span class="string">'scrapy'</span>,<span class="string">'crawl'</span>,<span class="string">'cib'</span>])</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="2-chuang-jian-pa-chong">2. 创建爬虫</h3>
<ol>
<li>scrapy genspider spidername “<a href="http://www.xxx.cn/" target="_blank" rel="noopener">http://www.xxx.cn/</a>”
<ul>
<li>genspider:表示生成一个爬虫（默认是scrapy.Spider类）</li>
<li>spidername：表示爬虫名（对应爬虫代码里的name参数）</li>
<li>“<a href="http://www.xxx.cn/" target="_blank" rel="noopener">http://www.xxx.cn/</a>” ：表示允许爬虫爬取的域范围</li>
</ul>
</li>
<li><a href="http://spidername.py" target="_blank" rel="noopener">spidername.py</a>
<ul>
<li>name= ‘’:爬虫的识别名称，唯一</li>
<li>allow_domains=[] ：搜索的域名范围，爬虫的约束区域，规定爬虫只爬取这个域名下的网页，不存在的url会被忽略</li>
<li>start_urls=():爬取的url列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成</li>
<li>parse(self,response):解析的方法，每个初始URL完成下载后将被调用，调用的时候传入每一个URL传回的Response对象来作为唯一参数，主要作用如下：</li>
<li>负责解析返回的网页数据（response.body），提取结构化数据（生成item）</li>
<li>生成需要下一页的URL请求</li>
<li>start_requests(self):这个方法必须返回一个可迭代对象。该对象包含spider用于爬取（默认实现是使用start_urls的url）的第一个Request。当spider启动爬取并且为指定start_urls时，调用该方法</li>
<li>log（self,message[,level,component]）:使用scrapy.log.msg()方法记录（log）message</li>
</ul>
</li>
</ol>
<h3 id="3-zhi-xing-pa-chong">3. 执行爬虫</h3>
<ol>
<li>scrapy crawl spidername -o save_filename
<ul>
<li>crawl：表示启动一个scrapy爬虫</li>
<li>spidername：表示需要启动的爬虫名（对应爬虫代码里的name参数）</li>
<li>-0 :表示输出到文件</li>
<li>save_filename:表示保存文件的名称,，默认4种输出文件格式：json，jsonl，csv，xml</li>
</ul>
</li>
</ol>
<h3 id="4-cha-kan-dang-qian-xiang-mu-xia-de-pa-chong">4. 查看当前项目下的爬虫</h3>
<ol>
<li>scrapy list</li>
</ol>
<h3 id="5-pipeline-de-yi-xie-dian-xing-ying-yong">5. pipeline 的一些典型应用</h3>
<ol>
<li>验证爬取的数据（检查item包含某些字段，比如说name）</li>
<li>数据查重（并丢弃）</li>
<li>将爬取结果保存到文件或者数据库中</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SomethingPiple</span><span class="params">(object)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="comment"># 可选实现，做参数初始化，比如打开文件操作f.open('xxx','w',edcoding='utf-8')</span></span><br><span class="line">            <span class="comment"># doing something</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">            <span class="comment"># spider(Spider 对象) - 被关闭的spider</span></span><br><span class="line">            <span class="comment"># 可选实现，当spider被开启时，这个方法被调用。</span></span><br><span class="line">            <span class="comment"># 该方法和__init__方法功能基本相同。</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self,item,spider)</span>:</span></span><br><span class="line">            <span class="comment"># item(Item对象) - 被爬取的item</span></span><br><span class="line">            <span class="comment"># spider （Spider对象） - 爬取该item的spider</span></span><br><span class="line">            <span class="comment"># 这个方法必须实现，每个item pipeline 组件都需要调用该方法</span></span><br><span class="line">            <span class="comment"># 这个方法必须返回一个Item对象，被丢弃的item将不会被之后的pipeline组件处理</span></span><br><span class="line">            <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">            <span class="comment"># spider(Spider 对象) - 被关闭的spider</span></span><br><span class="line">            <span class="comment"># 可选实现，当spider被关闭时，这个方法被调用。</span></span><br><span class="line">            <span class="comment"># 比如关闭初始化打开的文件f.close()</span></span><br></pre></td></tr></table></figure>
<h3 id="6-qi-dong-scrapy-shell">6. 启动Scrapy Shell</h3>
<p>命令：scrapy shell “<a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a>”</p>
<h3 id="7-selector-xuan-ze-qi">7. selector 选择器</h3>
<ol>
<li>
<p>Selector有四个基本方法，最常用的是xpath：</p>
<ol>
<li>
<p>xpath（）：传入xpath表达式，返回该表达式所对应的所有节点的selector list列表</p>
<p>XPath表达式的例子及对应含义：</p>
<ul>
<li>/html/head/title:选择<html>文档中<head>标签内的<title>元素</title></head></html></li>
<li>/html/head/title/text():选择<html>文档中<head>标签内的<title>元素的文字</title></head></html></li>
<li>//td：选择所有的<td>元素</td></li>
<li>//div[@class=“mine”]:选择所有具有class=&quot;mine&quot;属性的div元素</li>
</ul>
</li>
<li>
<p>extract（）：序列化该结点为Unicode字符串，并返回list</p>
</li>
<li>
<p>css()：传入css表达式，返回该表达式所对应的所有节点的selector list列表，语法同BeautifulSoup4</p>
</li>
<li>
<p>re()：根据传入的正则表达式对数据进行提取，返回Unicode字符串list列表</p>
</li>
</ol>
</li>
<li>
<p>注意</p>
<ol>
<li>
<p>xpath 返回的是一个列表</p>
</li>
<li>
<p>xpath.extract():将xpath对象转换成Unicode字符串</p>
</li>
<li>
<p>settings设置</p>
<ul>
<li>
<p>HTTPERROR_ALLOWED_CODES = [403, 500, 404]</p>
</li>
<li>
<p>ROBOTSTXT_OBEY = False</p>
</li>
<li>
<p>下载中间件：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DOWNLOAD_DELAY = <span class="number">2</span></span><br><span class="line">        RANDOMIZE_DOWNLOAD_DELAY = <span class="literal">True</span></span><br><span class="line">        COOKIES_ENABLED = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">        DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">        <span class="string">'bank_info.middlewares.MyUserAgentMiddleware'</span>: <span class="number">300</span>,</span><br><span class="line">        <span class="string">'bank_info.middlewares.BankInfoDownloaderMiddleware'</span>: <span class="number">543</span>, <span class="comment"># 值越小优先级越高</span></span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        ITEM_PIPELINES = &#123;</span><br><span class="line">        <span class="string">'bank_info.pipelines.BankInfoPipeline'</span>: <span class="number">300</span>, <span class="comment"># 值越小优先级越高</span></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="8-scrapy-gao-ji">8. scrapy高级</h3>
<ol>
<li>
<p>翻页功能：scrapy.follow(next_page,callback=self.parse)   会自动拼接url和next_page</p>
</li>
<li>
<p>抽取response中满足xpath规则的链接：LinkExtractor(restrict_xpath=‘xxxx’), links = link.extract_links(response)</p>
</li>
<li>
<p>要防止scrapy被ban，主要有以下几个策略：</p>
<ul>
<li>
<p>动态设置user agent（ 在middleware.py中随机选取user-agent,并把它赋值给request）</p>
<ol>
<li>
<p>在settings开启UAMiddleware这个中间件：DOWNLOADER_MIDDLEWARES</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UAMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 定义一个User-Agent的List</span></span><br><span class="line">    ua_list = [</span><br><span class="line">    <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 '</span>,</span><br><span class="line">    <span class="string">'(KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)'</span>,</span><br><span class="line">    <span class="string">'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)'</span>,</span><br><span class="line">    <span class="string">'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'</span>,</span><br><span class="line">    <span class="string">'Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)'</span>,</span><br><span class="line">    ]</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span> <span class="comment"># 对request进行拦截</span></span><br><span class="line">        ua = random.choices(self.ua_list) <span class="comment"># 使用random模块，随机在ua_list中选取User-Agent</span></span><br><span class="line">        request.headers[<span class="string">'User-Agent'</span>] = ua <span class="comment"># 把选取出来的User-Agent赋给request</span></span><br><span class="line">        print(request.url) <span class="comment"># 打印出request的url</span></span><br><span class="line">        print(request.headers[<span class="string">'User-Agent'</span>]) <span class="comment"># 打印出request的headers</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span><span class="params">(self, request, response, spider)</span>:</span> <span class="comment"># 对response进行拦截</span></span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_exception</span><span class="params">(self, request, exception, spider)</span>:</span> <span class="comment"># 对process_request方法传出来的异常进行处理</span></span><br><span class="line">       <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li>
<p>禁用cookies ： COOKIES_ENABLED=False</p>
</li>
<li>
<p>设置延迟下载 ： DOWNLOAD_DELAY=2</p>
</li>
<li>
<p>使用Google cache</p>
</li>
<li>
<p>使用IP地址池（Tor project、VPN和代理IP）</p>
</li>
<li>
<p>使用Crawlera</p>
</li>
</ul>
</li>
<li>
<p>scrapy中间件的分类：</p>
<ul>
<li>scrapy的中间件理论上有三种(Schduler Middleware,Spider Middleware,Downloader Middleware),在应用上一般有以下两种：
<ol>
<li>爬虫中间件Spider Middleware：可以添加代码来处理发送给 Spiders 的response及spider产生的item和request.
<ul>
<li>当蜘蛛传递请求和items给引擎的过程中，蜘蛛中间件可以对其进行处理（过滤出 URL 长度比 URLLENGTH_LIMIT 的 request。）</li>
<li>当引擎传递响应给蜘蛛的过程中，蜘蛛中间件可以对响应进行过滤（例如过滤出所有失败(错误)的 HTTP response）</li>
</ul>
</li>
<li>下载器中间件Downloader Middleware：主要功能在请求到网页后,页面被下载时进行一些处理.（反爬策略都是部署在下载中间件的）
<ul>
<li>当引擎传递请求给下载器的过程中，下载中间件可以对请求进行处理 （例如增加http header信息，增加proxy信息等）</li>
<li>在下载器完成http请求，传递响应给引擎的过程中， 下载中间件可以对响应进行处理（例如进行gzip的解压等）</li>
<li>下载中间件三大函数：
<ol>
<li>process_request(request, spider)——主要函数
<ul>
<li>process_request() 必须返回其中之一: 返回 None 、返回一个 Response 对象、返回一个 Request 对象或raise IgnoreRequest</li>
<li>如果其返回 None： Scrapy将继续处理该request，执行其他的中间件的相应方法，直到合适的下载器处理函数(download handler)被调用， 该request被执行(其response被下载)</li>
<li>如果其返回Response 对象： Scrapy将不会调用任何其他的process_request()或 process_exception()方法，或相应的下载函数。其将返回该response，已安装的中间件的 process_response() 方法则会在每个response返回时被调用</li>
<li>如果其返回 Request对象 ： Scrapy则会停止调用 process_request方法并重新调度返回的request，也就是把request重新返回，进入调度器重新入队列</li>
<li>如果其返回raise IgnoreRequest异常 ： 则安装的下载中间件的 process_exception()方法 会被调用。如果没有任何一个方法处理该异常， 则request的errback(Request.errback)方法会被调用。如果没有代码处理抛出的异常， 则该异常被忽略且不记录(不同于其他异常那样)</li>
</ul>
</li>
<li>process_response(request, response, spider)——主要函数
<ul>
<li>process_response() 必须返回以下之一：返回一个Response 对象、 返回一个Request 对象或raise IgnoreRequest 异常</li>
<li>如果其返回一个 Response对象： (可以与传入的response相同，也可以是全新的对象)， 该response会被在链中的其他中间件的 process_response() 方法处理</li>
<li>如果其返回一个 Request对象： 则中间件链停止， 返回的request会被重新调度下载。处理类似于 process_request() 返回request所做的那样</li>
<li>如果其抛出一个IgnoreRequest异常 ：则调用request的errback(Request.errback)。</li>
<li>如果没有代码处理抛出的异常，则该异常被忽略且不记录(不同于其他异常那样)</li>
</ul>
</li>
<li>process_exception(request, exception, spider)
<ul>
<li>如果其返回 None ： Scrapy将会继续处理该异常，接着调用已安装的其他中间件的 process_exception()方法，直到所有中间件都被调用完毕，则调用默认的异常处理</li>
<li>如果其返回一个 Response 对象： 相当于异常被纠正了，则已安装的中间件链的 process_response()方法被调用。Scrapy将不会调用任何其他中间件的 process_exception()方法</li>
<li>如果其返回一个 Request 对象： 则返回的request将会被重新调用下载。这将停止中间件的 process_exception() 方法执行，就如返回一个response的那样</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li>
<p>其他内置downloader middleware</p>
<table>
<thead>
<tr>
<th>item</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td>DefaultHeadersMiddleware</td>
<td>将所有request的头设置为默认模式</td>
</tr>
<tr>
<td>DownloadTimeoutMiddleware</td>
<td>设置request的timeout</td>
</tr>
<tr>
<td>HttpAuthMiddleware</td>
<td>对来自特定spider的request授权</td>
</tr>
<tr>
<td>HttpCacheMiddleware</td>
<td>给request&amp;response设置缓存策略</td>
</tr>
<tr>
<td>HttpProxyMiddleware</td>
<td>给所有request设置http代理</td>
</tr>
<tr>
<td>RedirectMiddleware</td>
<td>处理request的重定向</td>
</tr>
<tr>
<td>MetaRefreshMiddleware</td>
<td>根据meta-refresh html tag处理重定向</td>
</tr>
<tr>
<td>RetryMiddleware</td>
<td>失败重试策略</td>
</tr>
<tr>
<td>RobotsTxtMiddleware</td>
<td>robots封禁处理</td>
</tr>
<tr>
<td>UserAgentMiddleware</td>
<td>支持user agent重写</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>把数据保存到json文件</p>
<p>下面这个例子将会把所有爬虫所爬取到的数据保存到 items.jl 文件中，.jl既是表示 JSON Lines 格式，既是每一行存储一个 item；</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWriterPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file = open(<span class="string">'items.jl'</span>, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        line = json.dumps(dict(item)) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(line)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>把数据写到MongoDB</p>
<p>MongoDB address 以及 database name 是通过 Scrapy settings 配置的；下面这个用例主要用来展示如何使用 from_crawler 的用法以及如何正确的清理掉这些相关的 resources</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    collection_name = <span class="string">'scrapy_items'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DATABASE'</span>, <span class="string">'items'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.db[self.collection_name].insert_one(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="9-pa-qu-dong-tai-ye-mian">9. 爬取动态页面</h3>
<p>在scrapy中使用splash</p>
<ol>
<li>
<p>安装docker ：</p>
</li>
<li>
<p>安装splash：docker pull scrapinghub/splash</p>
</li>
<li>
<p>开启端口:docker run -p 5023:5023 -p 8050:8050 -p 8051:8051 scrapinghub/splash</p>
</li>
<li>
<p>安装 scrapy-splash: pip install scrapy-splash</p>
</li>
<li>
<p>修改setting.py文件对scrapy-splash进行配置</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Splash服务器地址</span></span><br><span class="line"><span class="string">SPLASH_URL</span> <span class="string">=</span> <span class="string">'http://localhost:8050'</span></span><br><span class="line"><span class="comment">#开启Splash的两个下载中间件并调整HttpCompressionMiddleware的次序</span></span><br><span class="line"> <span class="string">DOWNLOADER_MIDDLEWARES</span> <span class="string">=</span> <span class="string">&#123;</span></span><br><span class="line">    <span class="attr">'scrapy_splash.SplashCookiesMiddleware':</span> <span class="number">723</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy_splash.SplashMiddleware':</span> <span class="number">725</span><span class="string">,</span></span><br><span class="line">    <span class="attr">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware':</span> <span class="number">810</span><span class="string">,</span></span><br><span class="line"> <span class="string">&#125;</span></span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line"> <span class="comment">#设置去重过滤器</span></span><br><span class="line"> <span class="string">DUPEFILTER_CLASS</span> <span class="string">=</span> <span class="string">'scrapy_splash.SplashAwareDupeFilter'</span></span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line"> <span class="comment">#用来支持cache_args（可选）</span></span><br><span class="line"> <span class="string">SPIDER_MIDDLEWARES</span> <span class="string">=</span> <span class="string">&#123;</span></span><br><span class="line">    <span class="attr">'scrapy_splash.SplashDeduplicateArgsMiddleware':</span> <span class="number">100</span><span class="string">,</span></span><br><span class="line"> <span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>技术/爬虫</category>
      </categories>
      <tags>
        <tag>专业技能</tag>
      </tags>
  </entry>
  <entry>
    <title>selenium</title>
    <url>/2019/08/09/selenium/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2019/08/09/selenium/1_hdbXQfve5Yfuo0qEnS8K9Q.jpg" alt></p>
<a id="more"></a>
<h1 id="an-zhuang-pip-install-selenium">安装：pip install selenium</h1>
<p>因为selenium是配合浏览器一起使用，所以需要下载浏览器的驱动(webdriver)，以chrome为例：chrome的webdriver： <a href="http://chromedriver.storage.googleapis.com/index.html" target="_blank" rel="noopener">chromedriver</a> 不同的Chrome的版本对应的chromedriver.exe 版本也不一样。如果是最新的Chrome, 下载最新的chromedriver.exe 就可以。把chromedriver的路径也加到<strong>环境变量</strong>里</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver      <span class="comment"># 引入webdriver api</span></span><br><span class="line"></span><br><span class="line">driver = webdriver.Chrome()     <span class="comment"># 使用chrome浏览器声明一个webdriver对象</span></span><br><span class="line"><span class="comment"># driver = webdriver.Chrome('/your path /webdriver')</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  driver.get(<span class="string">'http://www.baidu.com/'</span>) <span class="comment"># 表示使用chrome以get的方式请求百度的url</span></span><br><span class="line">  driver.find_element_by_id(<span class="string">"kw"</span>).send_keys(<span class="string">"selenium"</span>)   <span class="comment"># 检索到百度的输入框，输入selenium</span></span><br><span class="line">  driver.find_element_by_id(<span class="string">"su"</span>).click() <span class="comment"># 检索到百度的搜索按钮并点击</span></span><br><span class="line">  wait = WebDriverWait(driever,<span class="number">10</span>) <span class="comment"># 等待加载</span></span><br><span class="line">  wait.until(EC.presence_of_element_located((By.ID,<span class="string">'content_left'</span>)))</span><br><span class="line">  print(driver.current_url)  <span class="comment"># 输出当前页面url</span></span><br><span class="line">  print(driver.get_cookies) <span class="comment"># 输出cookies</span></span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">  driver.close()</span><br></pre></td></tr></table></figure>
<h1 id="yuan-su-xuan-qu">元素选取</h1>
<h2 id="dan-yuan-su-xuan-qu">单元素选取</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_element_by_id      <span class="comment"># 通过元素id</span></span><br><span class="line">find_element_by_name    <span class="comment"># 通过name属性</span></span><br><span class="line">find_element_by_xpath   <span class="comment"># 通过xpath</span></span><br><span class="line">find_element_by_link_text   <span class="comment"># 通过链接文本</span></span><br><span class="line">find_element_by_partial_link_text</span><br><span class="line">find_element_by_tag_name    <span class="comment"># 通过标签名</span></span><br><span class="line">find_element_by_class_name      <span class="comment"># 通过class名称定位</span></span><br><span class="line">find_element_by_css_selector    <span class="comment"># 通过css选择器定位</span></span><br></pre></td></tr></table></figure>
<h2 id="duo-yuan-su-xuan-qu">多元素选取</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_elements_by_name</span><br><span class="line">find_elements_by_xpath</span><br><span class="line">find_elements_by_link_text</span><br><span class="line">find_elements_by_partial_link_text</span><br><span class="line">find_elements_by_tag_name</span><br><span class="line">find_elements_by_class_name</span><br><span class="line">find_elements_by_css_selector</span><br></pre></td></tr></table></figure>
<h2 id="yuan-su-cao-zuo">元素操作</h2>
<ul>
<li>clear 清除元素的内容：clear(self)</li>
<li>send_keys 模拟按键输入：send_keys(self, *value)</li>
<li>click 点击元素：click(self)</li>
<li>submit 提交表单：submit(self)</li>
<li>获取元素属性：get_attribute(self, name)</li>
<li>获取元素文本：text</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver.find_element_by_id(<span class="string">"kw"</span>).clear()</span><br></pre></td></tr></table></figure>
<h1 id="ye-mian-cao-zuo-fang-fa">页面操作方法</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打开浏览器</span></span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line"><span class="comment"># 请求一个url</span></span><br><span class="line">driver.get(<span class="string">"www.baidu.com"</span>)</span><br><span class="line"><span class="comment"># 返回当前页面的title</span></span><br><span class="line">title = driver.title</span><br><span class="line"><span class="comment"># 返回当前页面的url</span></span><br><span class="line">url = driver.current_url</span><br><span class="line"><span class="comment"># 返回当前页面的源码</span></span><br><span class="line">source = driver.page_source</span><br><span class="line"><span class="comment"># 关闭当前页面</span></span><br><span class="line">driver.close()</span><br><span class="line"><span class="comment"># 注销并关闭浏览器</span></span><br><span class="line">driver.quit()</span><br><span class="line"><span class="comment"># 浏览器前进</span></span><br><span class="line">driver.forward()</span><br><span class="line"><span class="comment"># 浏览器后退</span></span><br><span class="line">driver.back()</span><br><span class="line"><span class="comment"># 刷新当前页面</span></span><br><span class="line">driver.refresh()</span><br><span class="line"><span class="comment"># 获取当前session中的全部cookie</span></span><br><span class="line">get_cookies(self)</span><br><span class="line"><span class="comment"># 获取当前会中中的指定cookie</span></span><br><span class="line">get_cookie(self, name)</span><br><span class="line"><span class="comment"># 在当前会话中添加cookie</span></span><br><span class="line">add_cookie(self, cookie_dict)</span><br><span class="line"><span class="comment"># 添加浏览器User-Agent：</span></span><br><span class="line">options.add_argument(<span class="string">'User-Agent=Mozilla/5.0 (Linux; U; Android 4.0.2; en-us; Galaxy Nexus Build/ICL53F) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30'</span>)</span><br><span class="line"><span class="comment"># 添加设置项Chrome Options：</span></span><br><span class="line">options = webdriver.ChromeOptions()</span><br><span class="line">options.add_argument(<span class="string">'xxxx'</span>)</span><br><span class="line">driver = webdriver.Chrome(chrome_options=options)</span><br></pre></td></tr></table></figure>
<h2 id="ye-mian-deng-dai">页面等待</h2>
<h3 id="yin-shi-deng-dai-jian-dan-de-she-zhi-deng-dai-shi-jian-dan-wei-wei-miao">隐式等待：简单的设置等待时间，单位为：秒</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">driver.implicitly_wait(<span class="number">10</span>) <span class="comment"># seconds</span></span><br><span class="line">driver.get(<span class="string">"http://somedomain/url_that_delays_loading"</span>)</span><br><span class="line">myDynamicElement = driver.find_element_by_id(<span class="string">"myDynamicElement"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="xian-shi-deng-dai-zhi-ding-mou-ge-tiao-jian-ran-hou-she-zhi-zui-chang-deng-dai-shi-jian-ru-guo-zai-zhe-ge-shi-jian-huan-mei-you-zhao-dao-yuan-su-bian-hui-pao-chu-yi-chang">显式等待：指定某个条件，然后设置最长等待时间。如果在这个时间还没有找到元素，便会抛出异常。</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">driver.get(<span class="string">"http://somedomain/url_that_delays_loading"</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line"> 	<span class="comment"># 这里需要特别注意的是until或until_not中的可执行方法method参数，很多人传入了WebElement对象</span></span><br><span class="line">  <span class="comment"># WebDriverWait(driver, 10).until(driver.find_element_by_id('kw'))  错误</span></span><br><span class="line">  element = WebDriverWait(driver, <span class="number">10</span>).until(</span><br><span class="line">            EC.presence_of_element_located((By.ID, <span class="string">"myDynamicElement"</span>))</span><br><span class="line">            )</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">driver.quit()</span><br></pre></td></tr></table></figure>
<h3 id="expected-conditions-17-ge-tiao-jian-expected-conditions-shi-selenium-de-yi-ge-mo-kuai-qi-zhong-bao-han-yi-xi-lie-ke-yong-yu-pan-duan-de-tiao-jian">expected_conditions(17个条件)：expected_conditions是selenium的一个模块，其中包含一系列可用于判断的条件</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">selenium.webdriver.support.expected_conditions</span><br><span class="line"></span><br><span class="line">这两个条件类验证title，验证传入的参数title是否等于或包含于driver.title</span><br><span class="line">title_is</span><br><span class="line">title_contains</span><br><span class="line"></span><br><span class="line">一个只要一个符合条件的元素加载出来就通过；另一个必须所有符合条件的元素都加载出来才行</span><br><span class="line">presence_of_element_located</span><br><span class="line">presence_of_all_elements_located</span><br><span class="line"></span><br><span class="line">这三个条件验证元素是否可见，前两个传入参数是元组类型的locator，第三个传入WebElement</span><br><span class="line">第一个和第三个其实质是一样的</span><br><span class="line">visibility_of_element_located</span><br><span class="line">invisibility_of_element_located</span><br><span class="line">visibility_of</span><br><span class="line"></span><br><span class="line">这两个人条件判断某段文本是否出现在某元素中，一个判断元素的text，一个判断元素的value</span><br><span class="line">text_to_be_present_in_element</span><br><span class="line">text_to_be_present_in_element_value</span><br><span class="line"></span><br><span class="line">这个条件判断frame是否可切入，可传入locator元组或者直接传入定位方式：id、name、index或WebElement</span><br><span class="line">frame_to_be_available_and_switch_to_it</span><br><span class="line"></span><br><span class="line">这个条件判断是否有alert出现</span><br><span class="line">alert_is_present</span><br><span class="line"></span><br><span class="line">这个条件判断元素是否可点击，传入locator</span><br><span class="line">element_to_be_clickable</span><br><span class="line"></span><br><span class="line">这四个条件判断元素是否被选中，第一个条件传入WebElement对象，第二个传入locator元组</span><br><span class="line">第三个传入WebElement对象以及状态，相等返回<span class="literal">True</span>，否则返回<span class="literal">False</span></span><br><span class="line">第四个传入locator以及状态，相等返回<span class="literal">True</span>，否则返回<span class="literal">False</span></span><br><span class="line">element_to_be_selected</span><br><span class="line">element_located_to_be_selected</span><br><span class="line">element_selection_state_to_be</span><br><span class="line">element_located_selection_state_to_be</span><br><span class="line"></span><br><span class="line">最后一个条件判断一个元素是否仍在DOM中，传入WebElement对象，可以判断页面是否刷新了</span><br><span class="line">staleness_of</span><br></pre></td></tr></table></figure>
<h2 id="shu-biao-cao-zuo">鼠标操作</h2>
<ul>
<li>context_click(elem) 右击鼠标点击元素elem，另存为等行为</li>
<li>double_click(elem) 双击鼠标点击元素elem，地图web可实现放大功能</li>
<li>drag_and_drop(source,target) 拖动鼠标，源元素按下左键移动至目标元素释放</li>
<li>move_to_element(elem) 鼠标移动到一个元素上</li>
<li>click_and_hold(elem) 按下鼠标左键在一个元素上</li>
<li>perform() 在通过调用该函数执行ActionChains中存储行为</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取通过鼠标右键另存为百度图片logo的例子</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.action_chains <span class="keyword">import</span> ActionChains</span><br><span class="line"> </span><br><span class="line">driver = webdriver.Firefox()</span><br><span class="line">driver.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#鼠标移动至图片上 右键保存图片</span></span><br><span class="line">elem_pic = driver.find_element_by_xpath(<span class="string">"//div[@id='lg']/img"</span>)</span><br><span class="line"><span class="keyword">print</span> elem_pic.get_attribute(<span class="string">"src"</span>)</span><br><span class="line">action = ActionChains(driver).move_to_element(elem_pic)</span><br><span class="line">action.context_click(elem_pic)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#重点:当右键鼠标点击键盘光标向下则移动至右键菜单第一个选项</span></span><br><span class="line">action.send_keys(Keys.ARROW_DOWN)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line">action.send_keys(<span class="string">'v'</span>) <span class="comment">#另存为</span></span><br><span class="line">action.perform()</span><br><span class="line"> </span><br><span class="line"><span class="comment">#获取另存为对话框(失败)</span></span><br><span class="line">alert.switch_to_alert()</span><br><span class="line">alert.accept()</span><br><span class="line"><span class="comment"># driver.switch_to_alert().accept()  # 点击弹出里面的确定按钮</span></span><br><span class="line"><span class="comment"># driver.switch_to_alert().dismiss() # 点击弹出上面的X按钮</span></span><br></pre></td></tr></table></figure>
<h2 id="jian-pan-cao-zuo">键盘操作</h2>
<ul>
<li>send_keys(Keys.ENTER) 按下回车键</li>
<li>send_keys(Keys.TAB) 按下Tab制表键</li>
<li>send_keys(Keys.SPACE) 按下空格键space</li>
<li>send_keys(Kyes.ESCAPE) 按下回退键Esc</li>
<li>send_keys(Keys.BACK_SPACE) 按下删除键BackSpace</li>
<li>send_keys(Keys.SHIFT) 按下shift键</li>
<li>send_keys(Keys.CONTROL) 按下Ctrl键</li>
<li>send_keys(Keys.ARROW_DOWN) 按下鼠标光标向下按键</li>
<li>send_keys(Keys.CONTROL,‘a’) 组合键全选Ctrl+A</li>
<li>send_keys(Keys.CONTROL,‘c’) 组合键复制Ctrl+C</li>
<li>send_keys(Keys.CONTROL,‘x’) 组合键剪切Ctrl+X</li>
<li>send_keys(Keys.CONTROL,‘v’) 组合键粘贴Ctrl+V</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"> </span><br><span class="line">driver = webdriver.Firefox()</span><br><span class="line">driver.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#输入框输入内容</span></span><br><span class="line">elem = driver.find_element_by_id(<span class="string">"kw"</span>)</span><br><span class="line">elem.send_keys(<span class="string">"Eastmount CSDN"</span>)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#删除一个字符CSDN 回退键</span></span><br><span class="line">elem.send_keys(Keys.BACK_SPACE)</span><br><span class="line">elem.send_keys(Keys.BACK_SPACE)</span><br><span class="line">elem.send_keys(Keys.BACK_SPACE)</span><br><span class="line">elem.send_keys(Keys.BACK_SPACE)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#输入空格+"博客"</span></span><br><span class="line">elem.send_keys(Keys.SPACE)</span><br><span class="line">elem.send_keys(<span class="string">u"博客"</span>)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#ctrl+a 全选输入框内容</span></span><br><span class="line">elem.send_keys(Keys.CONTROL,<span class="string">'a'</span>)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#ctrl+x 剪切输入框内容</span></span><br><span class="line">elem.send_keys(Keys.CONTROL,<span class="string">'x'</span>)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#输入框重新输入搜索</span></span><br><span class="line">elem.send_keys(Keys.CONTROL,<span class="string">'v'</span>)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#通过回车键替代点击操作</span></span><br><span class="line">driver.find_element_by_id(<span class="string">"su"</span>).send_keys(Keys.ENTER)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">driver.quit()</span><br></pre></td></tr></table></figure>
<h2 id="jie-tu-bao-cun">截图保存</h2>
<p>当爬虫出错时可以截图保存当时页面，以便复现bug，进行分析。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">driver.maximize_window()</span><br><span class="line">driver.get_screenshot_as_file(<span class="string">"screen shot saved path/pic_&#123;&#125;.png"</span>.format(xx))</span><br></pre></td></tr></table></figure>
<h3 id="dui-selenium-er-ci-feng-zhuang">对selenium二次封装</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasePage</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    主要是把常用的几个Selenium方法封装到BasePage这个类，我们这里演示以下几个方法</span></span><br><span class="line"><span class="string">    back()</span></span><br><span class="line"><span class="string">    forward()</span></span><br><span class="line"><span class="string">    get()</span></span><br><span class="line"><span class="string">    quit()</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, driver)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        写一个构造函数，有一个参数driver</span></span><br><span class="line"><span class="string">        :param driver:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.driver = driver</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">back</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        浏览器后退按钮</span></span><br><span class="line"><span class="string">        :param none:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.driver.back()</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        浏览器前进按钮</span></span><br><span class="line"><span class="string">        :param none:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.driver.forward()</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_url</span><span class="params">(self, url)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        打开url站点</span></span><br><span class="line"><span class="string">        :param url:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.driver.get(url)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">quit_browser</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        关闭并停止浏览器服务</span></span><br><span class="line"><span class="string">        :param none:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.driver.quit()</span><br><span class="line">       </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BrowserEngine</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    定义一个浏览器引擎类，根据browser_type的值去，控制启动不同的浏览器，这里主要是IE，Firefox, Chrome</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, driver)</span>:</span></span><br><span class="line">        self.driver = driver</span><br><span class="line"> </span><br><span class="line">    browser_type = <span class="string">"IE"</span>   <span class="comment"># maybe Firefox, Chrome, IE</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_browser</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        通过if语句，来控制初始化不同浏览器的启动，默认是启动Chrome</span></span><br><span class="line"><span class="string">        :return: driver</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> self.browser_type == <span class="string">'Firefox'</span>:</span><br><span class="line">            driver = webdriver.Firefox()</span><br><span class="line">        <span class="keyword">elif</span> self.browser_type == <span class="string">'Chrome'</span>:</span><br><span class="line">            driver = webdriver.Chrome()</span><br><span class="line">        <span class="keyword">elif</span> self.browser_type == <span class="string">'IE'</span>:</span><br><span class="line">            driver = webdriver.Ie()</span><br><span class="line">        <span class="keyword">else</span>: driver = webdriver.Chrome()</span><br><span class="line"> </span><br><span class="line">        driver.maximize_window()</span><br><span class="line">        driver.implicitly_wait(<span class="number">10</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> driver</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>技术/爬虫</category>
      </categories>
      <tags>
        <tag>专业技能</tag>
      </tags>
  </entry>
  <entry>
    <title>seq2seq</title>
    <url>/2019/07/20/research/seq2seq/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2019/07/20/research/seq2seq/10.9_seq2seq.svg" alt></p>
<a id="more"></a>
<h1 id="bian-ma-qi-jie-ma-qi-seq-2-seq">编码器—解码器（seq2seq）</h1>
<p>在自然语言处理的很多应用中，输入和输出都可以是不定长序列。以机器翻译为例，输入可以是一段不定长的英语文本序列，输出可以是一段不定长的法语文本序列，例如</p>
<blockquote>
<p>英语输入：“They”、“are”、“watching”、“.”<br>
法语输出：“Ils”、“regardent”、“.”</p>
</blockquote>
<p>当输入和输出都是不定长序列时，可以使用编码器—解码器[1] 或者seq2seq模型 [2]。这两个模型本质上都用到了两个循环神经网络，分别叫做编码器和解码器。编码器用来分析输入序列，解码器用来生成输出序列。</p>
<p>下图描述了使用编码器—解码器将上述英语句子翻译成法语句子的一种方法。在训练数据集中，可以在每个句子后附上特殊符号“&lt;eos&gt;”（end of sequence）以表示序列的终止。编码器每个时间步的输入依次为英语句子中的单词、标点和特殊符号“&lt;eos&gt;”。图中使用了编码器在最终时间步的隐藏状态作为输入句子的表征或编码信息。解码器在各个时间步中使用输入句子的编码信息和上个时间步的输出以及隐藏状态作为输入。我们希望解码器在各个时间步能正确依次输出翻译后的法语单词、标点和特殊符号&quot;&lt;eos&gt;&quot;。需要注意的是，解码器在最初时间步的输入用到了一个表示序列开始的特殊符号&quot;&lt;bos&gt;&quot;（beginning of sequence）。</p>
<p><img src="/2019/07/20/research/seq2seq/10.9_seq2seq.svg" alt="使用编码器—解码器将句子由英语翻译成法语。编码器和解码器分别为循环神经网络"></p>
<p>接下来，分别介绍编码器和解码器的定义。</p>
<h2 id="bian-ma-qi">编码器</h2>
<p>编码器的作用是把一个不定长的输入序列变换成一个定长的背景变量\(\boldsymbol{c}\)，并在该背景变量中编码输入序列信息。常用的编码器是循环神经网络。</p>
<p>考虑批量大小为1的时序数据样本。假设输入序列是\(x_1,\ldots,x_T\)，例如\(x_i\)是输入句子中的第\(i\)个词。在时间步\(t\)，循环神经网络将输入\(x_t\)的特征向量\(\boldsymbol{x}_t\)和上个时间步的隐藏状态\(\boldsymbol{h}_{t-1}\)变换为当前时间步的隐藏状态\(\boldsymbol{h}_t\)。我们可以用函数\(f\)表达循环神经网络隐藏层的变换：</p>
<p>\[
\boldsymbol{h}_t = f(\boldsymbol{x}_t, \boldsymbol{h}_{t-1}).
\]</p>
<p>接下来，编码器通过自定义函数\(q\)将各个时间步的隐藏状态变换为背景变量</p>
<p>\[
\boldsymbol{c} =  q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T).
\]</p>
<p>例如，当选择\(q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T) = \boldsymbol{h}_T\)时，背景变量是输入序列最终时间步的隐藏状态\(\boldsymbol{h}_T\)。</p>
<p>以上描述的编码器是一个单向的循环神经网络，每个时间步的隐藏状态只取决于该时间步及之前的输入子序列。也可以使用双向循环神经网络构造编码器。在这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列（包括当前时间步的输入），并编码了整个序列的信息。</p>
<h2 id="jie-ma-qi">解码器</h2>
<p>编码器输出的背景变量\(\boldsymbol{c}\)编码了整个输入序列\(x_1, \ldots, x_T\)的信息。给定训练样本中的输出序列\(y_1, y_2, \ldots, y_{T'}\)，对每个时间步\(t'\)（符号与输入序列或编码器的时间步\(t\)有区别），解码器输出\(y_{t'}\)的条件概率将基于之前的输出序列\(y_1,\ldots,y_{t'-1}\)和背景变量\(\boldsymbol{c}\)，即\(P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c})\)。</p>
<p>为此，我们可以使用另一个循环神经网络作为解码器。在输出序列的时间步\(t^\prime\)，解码器将上一时间步的输出\(y_{t^\prime-1}\)以及背景变量\(\boldsymbol{c}\)作为输入，并将它们与上一时间步的隐藏状态\(\boldsymbol{s}_{t^\prime-1}\)变换为当前时间步的隐藏状态\(\boldsymbol{s}_{t^\prime}\)。因此，我们可以用函数\(g\)表达解码器隐藏层的变换：</p>
<p>\[
\boldsymbol{s}_{t^\prime} = g(y_{t^\prime-1}, \boldsymbol{c}, \boldsymbol{s}_{t^\prime-1}).
\]</p>
<p>有了解码器的隐藏状态后，可以使用自定义的输出层和softmax运算来计算\(P(y_{t^\prime} \mid y_1, \ldots, y_{t^\prime-1}, \boldsymbol{c})\)，例如，基于当前时间步的解码器隐藏状态 \(\boldsymbol{s}_{t^\prime}\)、上一时间步的输出\(y_{t^\prime-1}\)以及背景变量\(\boldsymbol{c}\)来计算当前时间步输出\(y_{t^\prime}\)的概率分布。</p>
<h2 id="xun-lian-mo-xing">训练模型</h2>
<p>根据最大似然估计，可以最大化输出序列基于输入序列的条件概率</p>
<p>\[
\begin{aligned}
P(y_1, \ldots, y_{T'} \mid x_1, \ldots, x_T)
&amp;= \prod_{t'=1}^{T'} P(y_{t'} \mid y_1, \ldots, y_{t'-1}, x_1, \ldots, x_T)\\
&amp;= \prod_{t'=1}^{T'} P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c}),
\end{aligned}
\]</p>
<p>并得到该输出序列的损失</p>
<p>\[
-\log P(y_1, \ldots, y_{T'} \mid x_1, \ldots, x_T) = -\sum_{t'=1}^{T'} \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c}),
\]</p>
<p>在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。在上图所描述的模型预测中，需要将解码器在上一个时间步的输出作为当前时间步的输入。与此不同，在训练中也可以将标签序列（训练集的真实输出序列）在上一个时间步的标签作为解码器在当前时间步的输入。这叫作强制教学。</p>
<h1 id="zong-jie">总结</h1>
<ul>
<li>编码器-解码器（seq2seq）可以输入并输出不定长的序列。</li>
<li>编码器—解码器使用了两个循环神经网络。</li>
<li>在编码器—解码器的训练中，可以采用强制教学。</li>
</ul>
<h1 id="can-kao-wen-xian">参考文献</h1>
<p>[1] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.</p>
<p>[2] Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in neural information processing systems (pp. 3104-3112).</p>
]]></content>
      <categories>
        <category>技术/自然语言处理</category>
      </categories>
  </entry>
  <entry>
    <title>requests</title>
    <url>/2019/07/19/requests/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><ol>
<li>爬虫的基本流程</li>
</ol>
<ul>
<li><strong>发起请求</strong><br>
通过HTTP库向目标站点发起请求，也就是发送一个Request，请求可以包含额外的header等信息，等待服务器响应</li>
<li><strong>获取响应内容</strong><br>
如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能是HTML,Json字符串，二进制数据（图片或者视频）等类型</li>
<li><strong>解析内容</strong><br>
得到的内容可能是HTML,可以用正则表达式，页面解析库进行解析，可能是Json,可以直接转换为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理</li>
<li><strong>保存数据</strong><br>
保存形式多样，可以存为文本，也可以保存到数据库，或者保存特定格式的文件</li>
</ul>
<a id="more"></a>
<ol start="2">
<li>
<p><strong>Request中包含什么？</strong></p>
<ul>
<li>
<p><strong>请求方式</strong></p>
<p>主要有：GET/POST两种类型常用，另外还有HEAD/PUT/DELETE/OPTIONS</p>
<ol>
<li>
<p>GET和POST的区别就是：请求的数据GET是在url中，POST则是存放在头部</p>
<p>GET:向指定的资源发出“显示”请求。使用GET方法应该只用在读取数据，而不应当被用于产生“副作用”的操作中，例如在Web Application中。其中一个原因是GET可能会被网络蜘蛛等随意访问</p>
<p>POST:向指定资源提交数据，请求服务器进行处理（例如提交表单或者上传文件）。数据被包含在请求本文中。这个请求可能会创建新的资源或修改现有资源，或二者皆有。</p>
</li>
<li>
<p>HEAD：与GET方法一样，都是向服务器发出指定资源的请求。只不过服务器将不传回资源的本文部分。它的好处在于，使用这个方法可以在不必传输全部内容的情况下，就可以获取其中“关于该资源的信息”（元信息或称元数据）。</p>
</li>
<li>
<p>PUT：向指定资源位置上传其最新内容。</p>
</li>
<li>
<p>OPTIONS：这个方法可使服务器传回该资源所支持的所有HTTP请求方法。用’*'来代替资源名称，向Web服务器发送OPTIONS请求，可以测试服务器功能是否正常运作。</p>
</li>
<li>
<p>DELETE：请求服务器删除Request-URI所标识的资源。</p>
</li>
</ol>
</li>
</ul>
</li>
<li>
<p><strong>请求URL</strong></p>
<p>URL，即统一资源定位符，也就是我们说的网址，统一资源定位符是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址。互联网上的每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。</p>
<p>URL的格式由三个部分组成：</p>
<ul>
<li>第一部分是协议(或称为服务方式)。</li>
<li>第二部分是存有该资源的主机IP地址(有时也包括端口号)。</li>
<li>第三部分是主机资源的具体地址，如目录和文件名等。</li>
</ul>
<p>爬虫爬取数据时必须要有一个目标的URL才可以获取数据，因此，它是爬虫获取数据的基本依据。</p>
</li>
<li>
<p><strong>请求头</strong></p>
<p>包含请求时的头部信息，如User-Agent,Host,Cookies等信息.</p>
</li>
<li>
<p><strong>请求体</strong></p>
<p>请求是携带的数据，如提交表单数据时候的表单数据（POST）</p>
</li>
<li>
<p>能爬取什么样的数据？</p>
<ul>
<li>网页文本：如HTML文档，Json格式化文本等</li>
<li>图片：获取到的是二进制文件，保存为图片格式</li>
<li>视频：同样是二进制文件</li>
<li>其他：只要请求到的，都可以获取</li>
</ul>
</li>
<li>
<p><strong>如何解析数据？</strong></p>
<ol>
<li>直接处理</li>
<li>Json解析</li>
<li>正则表达式处理</li>
<li>BeautifulSoup解析处理</li>
<li>PyQuery解析处理</li>
<li>XPath解析处理</li>
</ol>
</li>
<li>
<p><strong>关于抓取的页面数据和浏览器里看到的不一样的问题？</strong></p>
<p>出现这种情况是因为，很多网站中的数据都是通过js，ajax动态加载的，所以直接通过get请求获取的页面和浏览器显示的不同。</p>
</li>
<li>
<p><strong>如何解决js渲染的问题？</strong></p>
<ul>
<li>分析ajax</li>
<li>Selenium/webdriver</li>
<li>Splash</li>
</ul>
</li>
<li>
<p><strong>怎样保存数据？</strong></p>
</li>
</ol>
<ul>
<li>文本：纯文本，Json,Xml等</li>
<li>关系型数据库：如mysql,oracle,sql server等结构化数据库</li>
<li>非关系型数据库：MongoDB,Redis等key-value形式存储</li>
</ul>
<ol start="11">
<li>
<p><strong>request的应用</strong></p>
<ol>
<li>
<p>Requests是用python语言基于urllib编写的，采用的是Apache2 Licensed开源协议的HTTP库。一句话，requests是python实现的最简单易用的HTTP库，建议爬虫使用requests库。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response  = requests.get(<span class="string">"https://www.baidu.com"</span>)</span><br><span class="line">print(type(response))</span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.text) </span><br><span class="line">print(response.cookies)</span><br><span class="line">print(response.content) <span class="comment">#这样获取的数据是二进制数据</span></span><br><span class="line">print(response.content.decode(<span class="string">"utf-8"</span>))</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>很多情况下的网站如果直接response.text会出现乱码的问题，所以这个使用response.content这样返回的数据格式其实是二进制格式，然后通过decode()转换为utf-8，这样就解决了通过response.text直接返回显示乱码的问题.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response =requests.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line">response.encoding=<span class="string">"utf-8"</span></span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>
<p>不管是通过response.content.decode(&quot;utf-8)的方式还是通过response.encoding=&quot;utf-8&quot;的方式都可以避免乱码的问题发生</p>
</li>
<li>
<p><strong>requests包中的请求方式</strong></p>
<p>requests里提供个各种请求方式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">requests.post(<span class="string">"http://httpbin.org/post"</span>)</span><br><span class="line">requests.put(<span class="string">"http://httpbin.org/put"</span>)</span><br><span class="line">requests.delete(<span class="string">"http://httpbin.org/delete"</span>)</span><br><span class="line">requests.head(<span class="string">"http://httpbin.org/get"</span>)</span><br><span class="line">requests.options(<span class="string">"http://httpbin.org/get"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基本GET请求</span></span><br><span class="line">requests.get(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line"><span class="comment"># 带参数的GET请求</span></span><br><span class="line">requests.get(<span class="string">"http://httpbin.org/get?name=zhaofan&amp;age=23"</span>)</span><br><span class="line"><span class="comment"># 或者 使用params关键字传递参数，如果字典中的参数为None则不会添加到url上</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">"name"</span>:<span class="string">"zhaofan"</span>,</span><br><span class="line">    <span class="string">"age"</span>:<span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">"http://httpbin.org/get"</span>,params=data)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>response的主要属性</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response = requests.get(<span class="string">'http://www.jianshu.com'</span>)</span><br><span class="line">print(type(response.status_code), response.status_code)</span><br><span class="line">print(type(response.headers), response.headers)</span><br><span class="line">print(type(response.cookies), response.cookies)</span><br><span class="line">print(type(response.url), response.url)</span><br><span class="line">print(type(response.history), response.history)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>解析json</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response = requests.get(<span class="string">"http://httpbin.org/get"</span>)</span><br><span class="line"><span class="comment"># requests里面集成的json其实就是执行了json.loads()方法，两者的结果是一样的</span></span><br><span class="line">print(response.json())</span><br><span class="line">print(json.loads(response.text))</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>添加headers</strong></p>
<p>可以定制headers的信息，如当我们直接通过requests请求知乎网站的时候，默认是无法访问的,因为访问知乎需要头部信息，这个时候我们在谷歌浏览器里输入chrome://version,就可以看到用户代理，将用户代理添加到头部信息.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="string">"User-Agent"</span>:<span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"</span></span><br><span class="line">&#125;</span><br><span class="line">response =requests.get(<span class="string">"https://www.zhihu.com"</span>,headers=headers)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>基本POST请求</strong></p>
<p>通过在发送post请求时添加一个data参数，这个data参数可以通过字典构造成，这样对于发送post请求就非常方便.同样的在发送post请求的时候也可以和发送get请求一样通过headers参数传递一个字典类型的数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = &#123;</span><br><span class="line">    <span class="string">"name"</span>:<span class="string">"zhaofan"</span>,</span><br><span class="line">    <span class="string">"age"</span>:<span class="number">23</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.post(<span class="string">"http://httpbin.org/post"</span>,data=data)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>关于请求状态</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response= requests.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"><span class="keyword">if</span> response.status_code == requests.codes.ok:</span><br><span class="line">    print(<span class="string">"访问成功"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>文件上传</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">files= &#123;<span class="string">"files"</span>:open(<span class="string">"git.jpeg"</span>,<span class="string">"rb"</span>)&#125;</span><br><span class="line">response = requests.post(<span class="string">"http://httpbin.org/post"</span>,files=files)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>获取cookie</strong></p>
</li>
</ol>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response = requests.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line">print(response.cookies)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key,value <span class="keyword">in</span> response.cookies.items():</span><br><span class="line">    print(key+<span class="string">"="</span>+value)</span><br></pre></td></tr></table></figure>
<ol start="11">
<li>
<p><strong>会话维持</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = requests.Session()</span><br><span class="line">s.get(<span class="string">"http://httpbin.org/cookies/set/number/123456"</span>)</span><br><span class="line">response = s.get(<span class="string">"http://httpbin.org/cookies"</span>)</span><br><span class="line">print(response.text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面是错误示范</span></span><br><span class="line">requests.get(<span class="string">"http://httpbin.org/cookies/set/number/123456"</span>)</span><br><span class="line">response = requests.get(<span class="string">"http://httpbin.org/cookies"</span>)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>证书验证</strong></p>
<p>现在很多网站都需要证书的验证，没有证书会出现“你访问的不是一个私密链接”之类的错误。对于Https协议，直首先会检查证书是否合法，如果证书不合法，则会抛出：SSLError。针对这一点有两种措施：<br>
下面这种方法：在访问的时候，设置不进行证书的验证，此时返回状态码200，但是依旧会有警告。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.packages <span class="keyword">import</span> urllib3</span><br><span class="line">urllib3.disable_warnings()</span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, verify=<span class="literal">False</span>)</span><br><span class="line">print(response.status_code)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动指定证书</span></span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>, cert=(<span class="string">'/path/server.crt'</span>, <span class="string">'/path/key'</span>))</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>代理的设置</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">proxies = &#123;</span><br><span class="line">  <span class="string">"http"</span>: <span class="string">"http://127.0.0.1:9743"</span>,</span><br><span class="line">  <span class="string">"https"</span>: <span class="string">"https://127.0.0.1:9743"</span>,</span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">"https://www.taobao.com"</span>, proxies=proxies)</span><br><span class="line">print(response.status_code)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于需要用户名和密码的代理</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">"http"</span>: <span class="string">"http://user:password@127.0.0.1:9743/"</span>,<span class="comment">#指定好用户名和密码</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">"https://www.taobao.com"</span>, proxies=proxies)</span><br><span class="line">print(response.status_code)</span><br><span class="line"><span class="comment"># socks代理：先安装该模块   pip3 install 'requests[socks]'</span></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'socks5://127.0.0.1:9742'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'socks5://127.0.0.1:9742'</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">"https://www.taobao.com"</span>, proxies=proxies)</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>超时设置</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> ReadTimeout</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = requests.get(<span class="string">"http://httpbin.org/get"</span>, timeout = <span class="number">0.5</span>)</span><br><span class="line">    print(response.status_code)</span><br><span class="line"><span class="keyword">except</span> ReadTimeout:</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>需要登录认证才能访问的网站</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.auth <span class="keyword">import</span> HTTPBasicAuth</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'http://120.27.34.24:9001'</span>, auth=HTTPBasicAuth(<span class="string">'user'</span>, <span class="string">'123'</span>))</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">r = requests.get(<span class="string">'http://120.27.34.24:9001'</span>, auth=(<span class="string">'user'</span>, <span class="string">'123'</span>))</span><br><span class="line">print(r.status_code)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>异常处理</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> ReadTimeout, ConnectionError, RequestException</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = requests.get(<span class="string">"http://httpbin.org/get"</span>, timeout = <span class="number">0.5</span>)</span><br><span class="line">    print(response.status_code)</span><br><span class="line"><span class="keyword">except</span> ReadTimeout:</span><br><span class="line">    print(<span class="string">'Timeout'</span>)</span><br><span class="line"><span class="keyword">except</span> ConnectionError:</span><br><span class="line">    print(<span class="string">'Connection error'</span>)</span><br><span class="line"><span class="keyword">except</span> RequestException:</span><br><span class="line">    print(<span class="string">'Error'</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>技术/爬虫</category>
      </categories>
      <tags>
        <tag>专业技能</tag>
      </tags>
  </entry>
  <entry>
    <title>优化算法</title>
    <url>/2019/07/18/deeplearning/optimization/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2019/07/18/deeplearning/optimization/7.1_output1.svg" alt></p>
<a id="more"></a>
<h1 id="you-hua-yu-shen-du-xue-xi">优化与深度学习</h1>
<p>在一个深度学习问题中，通常会预先定义一个损失函数。有了损失函数以后，就可以使用优化算法试图将其最小化。在优化中，这样的损失函数通常被称作优化问题的目标函数。依据惯例，优化算法通常只考虑最小化目标函数。其实，任何最大化问题都可以很容易地转化为最小化问题，只需令目标函数的相反数为新的目标函数即可。</p>
<h2 id="you-hua-yu-shen-du-xue-xi-de-guan-xi">优化与深度学习的关系</h2>
<p>虽然优化为深度学习提供了最小化损失函数的方法，但本质上，优化与深度学习的目标是有区别的。由于优化算法的目标函数通常是一个基于训练数据集的损失函数，优化的目标在于降低训练误差。而深度学习的目标在于降低泛化误差。为了降低泛化误差，除了使用优化算法降低训练误差以外，还需要注意应对过拟合。</p>
<h2 id="you-hua-zai-shen-du-xue-xi-zhong-de-tiao-zhan">优化在深度学习中的挑战</h2>
<p>深度学习中绝大多数目标函数都很复杂。因此，很多优化问题并不存在解析解，而需要使用基于数值方法的优化算法找到近似解，即数值解。为了求得最小化目标函数的数值解，将通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。</p>
<p>优化在深度学习中有很多挑战。下面描述了其中的两个挑战，即局部最小值和鞍点。</p>
<h3 id="ju-bu-zui-xiao-zhi">局部最小值</h3>
<p>对于目标函数\(f(x)\)，如果\(f(x)\)在\(x\)上的值比在\(x\)邻近的其他点的值更小，那么\(f(x)\)可能是一个局部最小值（local minimum）。如果\(f(x)\)在\(x\)上的值是目标函数在整个定义域上的最小值，那么\(f(x)\)是全局最小值（global minimum）。</p>
<p>举个例子，给定函数</p>
<p>\[
f(x) = x \cdot \text{cos}(\pi x), \qquad -1.0 \leq x \leq 2.0,
\]</p>
<p>可以大致找出该函数的局部最小值和全局最小值的位置。需要注意的是，图中箭头所指示的只是大致位置。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.1_output1.svg" alt></p>
<p>深度学习模型的目标函数可能有若干局部最优值。当一个优化问题的数值解在局部最优解附近时，由于目标函数有关解的梯度接近或变成零，最终迭代求得的数值解可能只令目标函数局部最小化而非全局最小化。</p>
<h3 id="an-dian">鞍点</h3>
<p>梯度接近或变成零可能是由于当前解在局部最优解附近造成的。事实上，另一种可能性是当前解在鞍点（saddle point）附近。举个例子，给定函数</p>
<p>\[
f(x) = x^3
\]</p>
<p>可以找出该函数的鞍点位置。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.1_output2.svg" alt></p>
<p>再举个定义在二维空间的函数的例子，例如：</p>
<p>\[f(x, y) = x^2 - y^2.\]</p>
<p>可以找出该函数的鞍点位置。该函数看起来像一个马鞍，而鞍点恰好是马鞍上可坐区域的中心。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.1_output3.svg" alt></p>
<p>在图的鞍点位置，目标函数在\(x\)轴方向上是局部最小值，但在\(y\)轴方向上是局部最大值。</p>
<p>假设一个函数的输入为\(k\)维向量，输出为标量，那么它的海森矩阵（Hessian matrix）有\(k\)个特征值。该函数在梯度为0的位置上可能是局部最小值、局部最大值或者鞍点。</p>
<ul>
<li>当函数的海森矩阵在梯度为零的位置上的特征值全为正时，该函数得到局部最小值。</li>
<li>当函数的海森矩阵在梯度为零的位置上的特征值全为负时，该函数得到局部最大值。</li>
<li>当函数的海森矩阵在梯度为零的位置上的特征值有正有负时，该函数得到鞍点。</li>
</ul>
<p>随机矩阵理论告诉我们，对于一个大的高斯随机矩阵来说，任一特征值是正或者是负的概率都是0.5 。那么，以上第一种情况的概率为 \(0.5^k\)。由于深度学习模型参数通常都是高维的（\(k\)很大），目标函数的鞍点通常比局部最小值更常见。</p>
<h1 id="ti-du-xia-jiang-he-sui-ji-ti-du-xia-jiang">梯度下降和随机梯度下降</h1>
<p>虽然梯度下降在深度学习中很少被直接使用，但理解梯度的意义以及沿着梯度反方向更新自变量可能降低目标函数值的原因是学习后续优化算法的基础。</p>
<h2 id="yi-wei-ti-du-xia-jiang">一维梯度下降</h2>
<p>先以简单的一维梯度下降为例，解释梯度下降算法可能降低目标函数值的原因。假设连续可导的函数\(f: \mathbb{R} \rightarrow \mathbb{R}\)的输入和输出都是标量。给定绝对值足够小的数\(\epsilon\)，根据泰勒展开公式，我们得到以下的近似：</p>
<p>\[
f(x + \epsilon) \approx f(x) + \epsilon f'(x)
\]</p>
<p>这里\(f'(x)\)是函数\(f\)在\(x\)处的梯度。一维函数的梯度是一个标量，也称导数。接下来，找到一个常数\(\eta > 0\)，使得\(\left|\eta f'(x)\right|\)足够小，那么可以将\(\epsilon\)替换为\(-\eta f'(x)\)并得到</p>
<p>\[
f(x - \eta f'(x)) \approx f(x) -  \eta f'(x)^2
\]</p>
<p>如果导数\(f'(x) \neq 0\)，那么\(\eta f'(x)^2>0\)，所以</p>
<p>\[
f(x - \eta f'(x)) \lesssim f(x)
\]</p>
<p>这意味着，如果通过</p>
<p>\[
x \leftarrow x - \eta f'(x)
\]</p>
<p>来迭代\(x\)，函数\(f(x)\)的值可能会降低。因此在梯度下降中，我们先选取一个初始值\(x\)和常数\(\eta > 0\)，然后不断通过上式来迭代\(x\)，直到达到停止条件，例如\(f'(x)^2\)的值已足够小或迭代次数已达到某个值。</p>
<p>下面以目标函数\(f(x)=x^2\)为例来看一看梯度下降是如何工作的。虽然知道最小化\(f(x)\)的解为\(x=0\)，这里依然使用这个简单函数来观察\(x\)是如何被迭代的。</p>
<p>接下来使用\(x=10\)作为初始值，并设\(\eta=0.2\)。使用梯度下降对\(x\)迭代10次，可见最终\(x\)的值较接近最优解。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gd</span><span class="params">(eta)</span>:</span></span><br><span class="line">    x = <span class="number">10</span></span><br><span class="line">    results = [x]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        x -= eta * <span class="number">2</span> * x  <span class="comment"># f(x) = x * x的导数为f'(x) = 2 * x</span></span><br><span class="line">        results.append(x)</span><br><span class="line">    print(<span class="string">'epoch 10, x:'</span>, x)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">res = gd(<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.2_output1.svg" alt></p>
<h2 id="xue-xi-lu">学习率</h2>
<p>上述梯度下降算法中的正数\(\eta\)通常叫作学习率。这是一个超参数，需要人工设定。如果使用过小的学习率，会导致\(x\)更新缓慢从而需要更多的迭代才能得到较好的解。</p>
<p>下面展示使用学习率\(\eta=0.05\)时自变量\(x\)的迭代轨迹。可见，同样迭代10次后，当学习率过小时，最终\(x\)的值依然与最优解存在较大偏差。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.2_output2.svg" alt></p>
<p>如果使用过大的学习率，\(\left|\eta f'(x)\right|\)可能会过大从而使前面提到的一阶泰勒展开公式不再成立：这时无法保证迭代\(x\)会降低\(f(x)\)的值。</p>
<p>举个例子，当设学习率\(\eta=1.1\)时，可以看到\(x\)不断越过（overshoot）最优解\(x=0\)并逐渐发散。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.2_output3.svg" alt></p>
<h2 id="duo-wei-ti-du-xia-jiang">多维梯度下降</h2>
<p>在了解了一维梯度下降之后，再考虑一种更广义的情况：目标函数的输入为向量，输出为标量。假设目标函数\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)的输入是一个\(d\)维向量\(\boldsymbol{x} = [x_1, x_2, \ldots, x_d]^\top\)。目标函数\(f(\boldsymbol{x})\)有关\(\boldsymbol{x}\)的梯度是一个由\(d\)个偏导数组成的向量：</p>
<p>\[
\nabla_{\boldsymbol{x}} f(\boldsymbol{x}) = \bigg[\frac{\partial f(\boldsymbol{x})}{\partial x_1}, \frac{\partial f(\boldsymbol{x})}{\partial x_2}, \ldots, \frac{\partial f(\boldsymbol{x})}{\partial x_d}\bigg]^\top
\]</p>
<p>为表示简洁，用\(\nabla f(\boldsymbol{x})\)代替\(\nabla_{\boldsymbol{x}} f(\boldsymbol{x})\)。梯度中每个偏导数元素\(\partial f(\boldsymbol{x})/\partial x_i\)代表着\(f\)在\(\boldsymbol{x}\)有关输入\(x_i\)的变化率。为了测量\(f\)沿着单位向量\(\boldsymbol{u}\)（即\(\|\boldsymbol{u}\|=1\)）方向上的变化率，在多元微积分中，定义\(f\)在\(\boldsymbol{x}\)上沿着\(\boldsymbol{u}\)方向的方向导数为</p>
<p>\[
\text{D}_{\boldsymbol{u}} f(\boldsymbol{x}) = \lim_{h \rightarrow 0}  \frac{f(\boldsymbol{x} + h \boldsymbol{u}) - f(\boldsymbol{x})}{h}
\]</p>
<p>依据方向导数性质，以上方向导数可以改写为</p>
<p>\[
\text{D}_{\boldsymbol{u}} f(\boldsymbol{x}) = \nabla f(\boldsymbol{x}) \cdot \boldsymbol{u}
\]</p>
<p>方向导数\(\text{D}_{\boldsymbol{u}} f(\boldsymbol{x})\)给出了\(f\)在\(\boldsymbol{x}\)上沿着所有可能方向的变化率。为了最小化\(f\)，希望找到\(f\)能被降低最快的方向。因此，可以通过单位向量\(\boldsymbol{u}\)来最小化方向导数\(\text{D}_{\boldsymbol{u}} f(\boldsymbol{x})\)。</p>
<p>由于<br>
\[
\text{D}_{\boldsymbol{u}} f(\boldsymbol{x}) = \|\nabla f(\boldsymbol{x})\| \cdot \|\boldsymbol{u}\|  \cdot \text{cos} (\theta) = \|\nabla f(\boldsymbol{x})\|  \cdot \text{cos} (\theta)
\]<br>
其中\(\theta\)为梯度\(\nabla f(\boldsymbol{x})\)和单位向量\(\boldsymbol{u}\)之间的夹角，当\(\theta = \pi\)时，\(\text{cos}(\theta)\)取得最小值\(-1\)。因此，当\(\boldsymbol{u}\)在梯度方向\(\nabla f(\boldsymbol{x})\)的相反方向时，方向导数\(\text{D}_{\boldsymbol{u}} f(\boldsymbol{x})\)被最小化。因此，我们可能通过梯度下降算法来不断降低目标函数\(f\)的值：</p>
<p>\[
\boldsymbol{x} \leftarrow \boldsymbol{x} - \eta \nabla f(\boldsymbol{x})
\]</p>
<p>同样，其中\(\eta\)（取正数）称作学习率。</p>
<p>下面构造一个输入为二维向量\(\boldsymbol{x} = [x_1, x_2]^\top\)和输出为标量的目标函数\(f(\boldsymbol{x})=x_1^2+2x_2^2\)。那么，梯度\(\nabla f(\boldsymbol{x}) = [2x_1, 4x_2]^\top\)。观察梯度下降从初始位置\([-5,-2]\)开始对自变量\(\boldsymbol{x}\)的迭代轨迹。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.2_output4.svg" alt></p>
<h2 id="sui-ji-ti-du-xia-jiang">随机梯度下降</h2>
<p>在深度学习里，目标函数通常是训练数据集中有关各个样本的损失函数的平均。设\(f_i(\boldsymbol{x})\)是有关索引为\(i\)的训练数据样本的损失函数，\(n\)是训练数据样本数，\(\boldsymbol{x}\)是模型的参数向量，那么目标函数定义为</p>
<p>\[
f(\boldsymbol{x}) = \frac{1}{n} \sum_{i = 1}^n f_i(\boldsymbol{x})
\]</p>
<p>目标函数在\(\boldsymbol{x}\)处的梯度计算为</p>
<p>\[
\nabla f(\boldsymbol{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\boldsymbol{x})
\]</p>
<p>如果使用梯度下降，每次自变量迭代的计算开销为\(\mathcal{O}(n)\)，它随着\(n\)线性增长。因此，当训练数据样本数很大时，梯度下降每次迭代的计算开销很高。</p>
<p>随机梯度下降减少了每次迭代的计算开销。在随机梯度下降的每次迭代中，我们随机均匀采样的一个样本索引\(i\in\{1,\ldots,n\}\)，并计算梯度\(\nabla f_i(\boldsymbol{x})\)来迭代\(\boldsymbol{x}\)：</p>
<p>\[
\boldsymbol{x} \leftarrow \boldsymbol{x} - \eta \nabla f_i(\boldsymbol{x})
\]</p>
<p>这里\(\eta\)同样是学习率。可以看到每次迭代的计算开销从梯度下降的\(\mathcal{O}(n)\)降到了常数\(\mathcal{O}(1)\)。值得强调的是，随机梯度\(\nabla f_i(\boldsymbol{x})\)是对梯度\(\nabla f(\boldsymbol{x})\)的无偏估计：</p>
<p>\[
E_i \nabla f_i(\boldsymbol{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\boldsymbol{x}) = \nabla f(\boldsymbol{x})
\]</p>
<p>这意味着，平均来说，随机梯度是对梯度的一个良好的估计。</p>
<p>下面我们通过在梯度中添加均值为0的随机噪声来模拟随机梯度下降，以此来比较它与梯度下降的区别。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.2_output5.svg" alt></p>
<p>可以看到，随机梯度下降中自变量的迭代轨迹相对于梯度下降中的来说更为曲折。这是由于实验所添加的噪声使模拟的随机梯度的准确度下降。在实际中，这些噪声通常指训练数据集中的无意义的干扰。</p>
<h2 id="xiao-pi-liang-sui-ji-ti-du-xia-jiang">小批量随机梯度下降</h2>
<p>在每一次迭代中，梯度下降使用整个训练数据集来计算梯度，因此它有时也被称为批量梯度下降（batch gradient descent）。而随机梯度下降在每次迭代中只随机采样一个样本来计算梯度。还可以在每轮迭代中随机均匀采样多个样本来组成一个小批量，然后使用这个小批量来计算梯度。</p>
<p>设目标函数\(f(\boldsymbol{x}): \mathbb{R}^d \rightarrow \mathbb{R}\)。在迭代开始前的时间步设为0。该时间步的自变量记为\(\boldsymbol{x}_0\in \mathbb{R}^d\)，通常由随机初始化得到。在接下来的每一个时间步\(t>0\)中，小批量随机梯度下降随机均匀采样一个由训练数据样本索引组成的小批量\(\mathcal{B}_t\)。我们可以通过重复采样（sampling with replacement）或者不重复采样（sampling without replacement）得到一个小批量中的各个样本。前者允许同一个小批量中出现重复的样本，后者则不允许如此，且更常见。对于这两者间的任一种方式，都可以使用</p>
<p>\[
\boldsymbol{g}_t \leftarrow \nabla f_{\mathcal{B}_t}(\boldsymbol{x}_{t-1}) = \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t}\nabla f_i(\boldsymbol{x}_{t-1})
\]</p>
<p>来计算时间步\(t\)的小批量\(\mathcal{B}_t\)上目标函数位于\(\boldsymbol{x}_{t-1}\)处的梯度\(\boldsymbol{g}_t\)。这里\(|\mathcal{B}|\)代表批量大小，即小批量中样本的个数，是一个超参数。同随机梯度一样，重复采样所得的小批量随机梯度\(\boldsymbol{g}_t\)也是对梯度\(\nabla f(\boldsymbol{x}_{t-1})\)的无偏估计。给定学习率\(\eta_t\)（取正数），小批量随机梯度下降对自变量的迭代如下：</p>
<p>\[
\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \eta_t \boldsymbol{g}_t
\]</p>
<p>基于随机采样得到的梯度的方差在迭代过程中无法减小，因此在实际中，（小批量）随机梯度下降的学习率可以在迭代过程中自我衰减，例如\(\eta_t=\eta t^\alpha\)（通常\(\alpha=-1\)或者\(-0.5\)）、\(\eta_t = \eta \alpha^t\)（如\(\alpha=0.95\)）或者每迭代若干次后将学习率衰减一次。如此一来，学习率和（小批量）随机梯度<strong>乘积</strong>的方差会减小。而梯度下降在迭代过程中一直使用目标函数的真实梯度，无须自我衰减学习率。</p>
<p>小批量随机梯度下降中每次迭代的计算开销为\(\mathcal{O}(|\mathcal{B}|)\)。当批量大小为1时，该算法即为随机梯度下降；当批量大小等于训练数据样本数时，该算法即为梯度下降。当批量较小时，每次迭代中使用的样本少，这会导致并行处理和内存使用效率变低。这使得在计算同样数目样本的情况下比使用更大批量时所花时间更多。当批量较大时，每个小批量梯度里可能含有更多的冗余信息。为了得到较好的解，批量较大时比批量较小时需要计算的样本数目可能更多，例如增大迭代周期数。</p>
<h3 id="dai-ma-shi-xian">代码实现</h3>
<p>向小批量随机梯度下降算法添加了一个状态输入<code>states</code>并将超参数放在字典<code>hyperparams</code>里。此外，将在训练函数里对各个小批量样本的损失求平均，因此优化算法里的梯度不需要除以批量大小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, states,hyperparams,grads)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i,p <span class="keyword">in</span> enumerate(params):</span><br><span class="line">        p.assign_sub(hyperparams[<span class="string">'lr'</span>] * grads[i])</span><br></pre></td></tr></table></figure>
<p>当批量大小为样本总数1,500时，优化使用的是梯度下降。梯度下降的1个迭代周期对模型参数只迭代1次。可以看到6次迭代后目标函数值的下降趋向了平稳。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_sgd(<span class="number">1</span>, <span class="number">1500</span>, <span class="number">6</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.3_output1.png" alt></p>
<p>当批量大小为1时，优化使用的是随机梯度下降。为了简化实现，有关（小批量）随机梯度下降的实验中，未对学习率进行自我衰减，而是直接采用较小的常数学习率。随机梯度下降中，每处理一个样本会更新一次自变量（模型参数），一个迭代周期里会对自变量进行1,500次更新。可以看到，目标函数值的下降在1个迭代周期后就变得较为平缓。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_sgd(<span class="number">0.005</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.3_output2.png" alt></p>
<p>虽然随机梯度下降和梯度下降在一个迭代周期里都处理了1,500个样本，但实验中随机梯度下降的一个迭代周期耗时更多。这是因为随机梯度下降在一个迭代周期里做了更多次的自变量迭代，而且单样本的梯度计算难以有效利用矢量计算。</p>
<p>当批量大小为10时，优化使用的是小批量随机梯度下降。它在每个迭代周期的耗时介于梯度下降和随机梯度下降的耗时之间。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_sgd(<span class="number">0.05</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.3_output3.png" alt></p>
<p>无须自己实现小批量随机梯度下降算法。tensorflow.keras.optimizers 模块提供了很多常用的优化算法比如SGD、Adam和RMSProp等。下面创建一个用于优化model 所有参数的优化器实例，并指定学习率为0.05的小批量随机梯度下降（SGD）为优化算法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line">trainer = optimizers.SGD(learning_rate=<span class="number">0.05</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.3_output4.png" alt></p>
<h1 id="dong-liang-fa">动量法</h1>
<p>梯度下降和随机梯度下降中我们提到，目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫作最陡下降。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。</p>
<h2 id="ti-du-xia-jiang-de-wen-ti">梯度下降的问题</h2>
<p>考虑一个输入和输出分别为二维向量\(\boldsymbol{x} = [x_1, x_2]^\top\)和标量的目标函数\(f(\boldsymbol{x})=0.1x_1^2+2x_2^2\)。这里将\(x_1^2\)系数从\(1\)减小到了\(0.1\)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">0.4</span> <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gd_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x1 - eta * <span class="number">0.2</span> * x1, x2 - eta * <span class="number">4</span> * x2, <span class="number">0</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.4_output1.png" alt></p>
<p>可以看到，同一位置上，目标函数在竖直方向（\(x_2\)轴方向）比在水平方向（\(x_1\)轴方向）的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。那么，就需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这会造成自变量在水平方向上朝最优解移动变慢。</p>
<p>下面我们试着将学习率调得稍大一点，此时自变量在竖直方向不断越过最优解并逐渐发散。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.4_output2.png" alt></p>
<h2 id="dong-liang-fa-1">动量法</h2>
<p>动量法的提出是为了解决梯度下降的上述问题。由于小批量随机梯度下降比梯度下降更为广义，后续讨论将沿用小批量随机梯度下降中时间步\(t\)的小批量随机梯度\(\boldsymbol{g}_t\)的定义。设时间步\(t\)的自变量为\(\boldsymbol{x}_t\)，学习率为\(\eta_t\)。<br>
在时间步\(0\)，动量法创建速度变量\(\boldsymbol{v}_0\)，并将其元素初始化成0。在时间步\(t>0\)，动量法对每次迭代的步骤做如下修改：<br>
\[
\begin{aligned}
\boldsymbol{v}_t &amp;\leftarrow \gamma \boldsymbol{v}_{t-1} + \eta_t \boldsymbol{g}_t, \\
\boldsymbol{x}_t &amp;\leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{v}_t,
\end{aligned}
\]</p>
<p>其中，动量超参数\(\gamma\)满足\(0 \leq \gamma &lt; 1\)。当\(\gamma=0\)时，动量法等价于小批量随机梯度下降。</p>
<p>在解释动量法的数学原理前，让我们先从实验中观察梯度下降在使用动量法后的迭代轨迹。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">momentum_2d</span><span class="params">(x1, x2, v1, v2)</span>:</span></span><br><span class="line">    v1 = gamma * v1 + eta * <span class="number">0.2</span> * x1</span><br><span class="line">    v2 = gamma * v2 + eta * <span class="number">4</span> * x2</span><br><span class="line">    <span class="keyword">return</span> x1 - v1, x2 - v2, v1, v2</span><br><span class="line"></span><br><span class="line">eta, gamma = <span class="number">0.4</span>, <span class="number">0.5</span></span><br><span class="line">show_trace_2d(f_2d, d2l.train_2d(momentum_2d))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.4_output3.png" alt></p>
<p>可以看到使用较小的学习率\(\eta=0.4\)和动量超参数\(\gamma=0.5\)时，动量法在竖直方向上的移动更加平滑，且在水平方向上更快逼近最优解。下面使用较大的学习率\(\eta=0.6\)，此时自变量也不再发散。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">0.6</span></span><br><span class="line">show_trace_2d(f_2d, d2l.train_2d(momentum_2d))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.4_output4.png" alt></p>
<h3 id="zhi-shu-jia-quan-yi-dong-ping-jun">指数加权移动平均</h3>
<p>为了从数学上理解动量法，先解释一下指数加权移动平均（exponentially weighted moving average）。给定超参数\(0 \leq \gamma &lt; 1\)，当前时间步\(t\)的变量\(y_t\)是上一时间步\(t-1\)的变量\(y_{t-1}\)和当前时间步另一变量\(x_t\)的线性组合：</p>
<p>\[
y_t = \gamma y_{t-1} + (1-\gamma) x_t
\]</p>
<p>可以对\(y_t\)展开：</p>
<p>\[
\begin{aligned}
y_t  &amp;= (1-\gamma) x_t + \gamma y_{t-1}\\
         &amp;= (1-\gamma)x_t + (1-\gamma) \cdot \gamma x_{t-1} + \gamma^2y_{t-2}\\
         &amp;= (1-\gamma)x_t + (1-\gamma) \cdot \gamma x_{t-1} + (1-\gamma) \cdot \gamma^2x_{t-2} + \gamma^3y_{t-3}\\
         &amp;\ldots
\end{aligned}
\]</p>
<p>令\(n = 1/(1-\gamma)\)，那么 \(\left(1-1/n\right)^n = \gamma^{1/(1-\gamma)}\)。因为</p>
<p>\[
\lim_{n \rightarrow \infty}  \left(1-\frac{1}{n}\right)^n = \exp(-1) \approx 0.3679
\]</p>
<p>所以当\(\gamma \rightarrow 1\)时，\(\gamma^{1/(1-\gamma)}=\exp(-1)\approx 0.3679\)，如\(0.95^{20} \approx \exp(-1)\)。如果把\(\exp(-1)\)当作一个比较小的数，我们可以在近似中忽略所有含\(\gamma^{1/(1-\gamma)}\)和比\(\gamma^{1/(1-\gamma)}\)更高阶的系数的项。例如，当\(\gamma=0.95\)时，</p>
<p>\[
y_t \approx 0.05 \sum_{i=0}^{19} 0.95^i x_{t-i}
\]</p>
<p>因此，在实际中，常常将\(y_t\)看作是对最近\(1/(1-\gamma)\)个时间步的\(x_t\)值的加权平均。例如，当\(\gamma = 0.95\)时，\(y_t\)可以被看作对最近20个时间步的\(x_t\)值的加权平均；当\(\gamma = 0.9\)时，\(y_t\)可以看作是对最近10个时间步的\(x_t\)值的加权平均。而且，离当前时间步\(t\)越近的\(x_t\)值获得的权重越大（越接近1）。</p>
<h3 id="you-zhi-shu-jia-quan-yi-dong-ping-jun-li-jie-dong-liang-fa">由指数加权移动平均理解动量法</h3>
<p>现在，对动量法的速度变量做变形：</p>
<p>\[
\boldsymbol{v}_t \leftarrow \gamma \boldsymbol{v}_{t-1} + (1 - \gamma) \left(\frac{\eta_t}{1 - \gamma} \boldsymbol{g}_t\right)
\]</p>
<p>由指数加权移动平均的形式可得，速度变量\(\boldsymbol{v}_t\)实际上对序列\(\{\eta_{t-i}\boldsymbol{g}_{t-i} /(1-\gamma):i=0,\ldots,1/(1-\gamma)-1\}\)做了指数加权移动平均。换句话说，相比于小批量随机梯度下降，<strong>动量法在每个时间步的自变量更新量近似于将最近\(1/(1-\gamma)\)个时间步的普通更新量（即学习率乘以梯度）做了指数加权移动平均后再除以\(1-\gamma\)</strong>。所以，在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。在本节之前示例的优化问题中，所有梯度在水平方向上为正（向右），而在竖直方向上时正（向上）时负（向下）。这样，我们就可以使用较大的学习率，从而使自变量向最优解更快移动。</p>
<h2 id="dai-ma-shi-xian-1">代码实现</h2>
<p>相对于小批量随机梯度下降，动量法需要对每一个自变量维护一个同它一样形状的速度变量，且超参数里多了动量超参数。实现中，将速度变量用更广义的状态变量<code>states</code>表示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_momentum_states</span><span class="params">()</span>:</span></span><br><span class="line">    v_w = tf.zeros((features.shape[<span class="number">1</span>],<span class="number">1</span>))</span><br><span class="line">    v_b = tf.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (v_w, v_b)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_momentum</span><span class="params">(params, states, hyperparams,grads)</span>:</span></span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p,v <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        v=hyperparams[<span class="string">'momentum'</span>] * v + hyperparams[<span class="string">'lr'</span>] * grads[i]</span><br><span class="line">        p.assign_sub(v)</span><br><span class="line">        i+=<span class="number">1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_momentum_states</span><span class="params">()</span>:</span></span><br><span class="line">    v_w = tf.zeros((features.shape[<span class="number">1</span>],<span class="number">1</span>))</span><br><span class="line">    v_b = tf.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (v_w, v_b)</span><br></pre></td></tr></table></figure>
<p>先将动量超参数<code>momentum</code>设0.5，这时可以看成是特殊的小批量随机梯度下降：其小批量随机梯度为最近2个时间步的2倍小批量梯度的加权平均。</p>
<blockquote>
<p>注：这里不应该是“加权平均”而应该是“加权和”，因为加权平均最后除以了\(1-\gamma\)，所以就相当于没有进行平均。</p>
</blockquote>
<p><img src="/2019/07/18/deeplearning/optimization/7.4_output5.png" alt></p>
<p>将动量超参数<code>momentum</code>增大到0.9，这时依然可以看成是特殊的小批量随机梯度下降：其小批量随机梯度为最近10个时间步的10倍小批量梯度的加权平均。先保持学习率0.02不变。</p>
<blockquote>
<p>同理，这里不应该是“加权平均”而应该是“加权和”。</p>
</blockquote>
<p><img src="/2019/07/18/deeplearning/optimization/7.4_output6.png" alt></p>
<p>可见目标函数值在后期迭代过程中的变化不够平滑。直觉上，10倍小批量梯度比2倍小批量梯度大了5倍，可以试着将学习率减小到原来的1/5。此时目标函数值在下降了一段时间后变化更加平滑。</p>
<blockquote>
<p>这也印证了刚刚的观点。</p>
</blockquote>
<p><img src="/2019/07/18/deeplearning/optimization/7.4_output7.png" alt></p>
<p>在Tensorflow中，只需要通过参数<code>momentum</code>来指定动量超参数即可使用动量法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line">trainer = optimizers.SGD(learning_rate=<span class="number">0.004</span>,momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.4_output8.png" alt></p>
<h1 id="ada-grad-suan-fa">AdaGrad算法</h1>
<p>目标函数自变量的每一个元素在相同时间步都使用同一个学习率来自我迭代。举个例子，假设目标函数为\(f\)，自变量为一个二维向量\([x_1, x_2]^\top\)，该向量中每一个元素在迭代时都使用相同的学习率。例如，在学习率为\(\eta\)的梯度下降中，元素\(x_1\)和\(x_2\)都使用相同的学习率\(\eta\)来自我迭代：</p>
<p>\[
x_1 \leftarrow x_1 - \eta \frac{\partial{f}}{\partial{x_1}}, \quad
x_2 \leftarrow x_2 - \eta \frac{\partial{f}}{\partial{x_2}}.
\]</p>
<p>当\(x_1\)和\(x_2\)的梯度值有较大差别时，需要选择足够小的学习率使得自变量在梯度值较大的维度上不发散。但这样会导致自变量在梯度值较小的维度上迭代过慢。动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能。<strong>AdaGrad算法，它根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题</strong>。</p>
<h2 id="suan-fa">算法</h2>
<p>AdaGrad算法会使用一个小批量随机梯度\(\boldsymbol{g}_t\)按元素平方的累加变量\(\boldsymbol{s}_t\)。在时间步0，AdaGrad将\(\boldsymbol{s}_0\)中每个元素初始化为0。在时间步\(t\)，首先将小批量随机梯度\(\boldsymbol{g}_t\)按元素平方后累加到变量\(\boldsymbol{s}_t\)：</p>
<p>\[
\boldsymbol{s}_t \leftarrow \boldsymbol{s}_{t-1} + \boldsymbol{g}_t \odot \boldsymbol{g}_t
\]</p>
<p>其中\(\odot\)是按元素相乘。接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整一下：</p>
<p>\[
\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t
\]</p>
<p>其中\(\eta\)是学习率，\(\epsilon\)是为了维持数值稳定性而添加的常数，如\(10^{-6}\)。这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。</p>
<h2 id="te-dian">特点</h2>
<p>需要强调的是，小批量随机梯度按元素平方的累加变量\(\boldsymbol{s}_t\)出现在学习率的分母项中。因此，如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快；反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。然而，由于\(\boldsymbol{s}_t\)一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。所以，<strong>当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解</strong>。</p>
<p>下面以目标函数\(f(\boldsymbol{x})=0.1x_1^2+2x_2^2\)为例观察AdaGrad算法对自变量的迭代轨迹。实现AdaGrad算法并使用学习率0.4。可以看到，自变量的迭代轨迹较平滑。但由于\(\boldsymbol{s}_t\)的累加效果使学习率不断衰减，自变量在迭代后期的移动幅度较小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adagrad_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    g1, g2, eps = <span class="number">0.2</span> * x1, <span class="number">4</span> * x2, <span class="number">1e-6</span>  <span class="comment"># 前两项为自变量梯度</span></span><br><span class="line">    s1 += g1 ** <span class="number">2</span></span><br><span class="line">    s2 += g2 ** <span class="number">2</span></span><br><span class="line">    x1 -= eta / math.sqrt(s1 + eps) * g1</span><br><span class="line">    x2 -= eta / math.sqrt(s2 + eps) * g2</span><br><span class="line">    <span class="keyword">return</span> x1, x2, s1, s2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">eta = <span class="number">0.4</span></span><br><span class="line">show_trace_2d(f_2d, train_2d(adagrad_2d))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.5_output1.png" alt></p>
<p>下面将学习率增大到2。可以看到自变量更为迅速地逼近了最优解。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">2</span></span><br><span class="line">show_trace_2d(f_2d, train_2d(adagrad_2d))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.5_output2.png" alt></p>
<h2 id="dai-ma-shi-xian-2">代码实现</h2>
<p>同动量法一样，AdaGrad算法需要对每个自变量维护同它一样形状的状态变量。根据AdaGrad算法中的公式实现该算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adagrad_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w = tf.zeros((features.shape[<span class="number">1</span>],<span class="number">1</span>),dtype=tf.float32)</span><br><span class="line">    s_b = tf.zeros(<span class="number">1</span>,dtype=tf.float32)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adagrad</span><span class="params">(params, states, hyperparams,grads)</span>:</span></span><br><span class="line">    eps = <span class="number">1e-6</span></span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s += (grads[i]**<span class="number">2</span>)</span><br><span class="line">        p.assign_sub(hyperparams[<span class="string">'lr'</span>]*grads[i]/tf.sqrt(s+eps))</span><br><span class="line">        i+=<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>这里使用更大的学习率来训练模型。</p>
<p><img src="/2019/07/18/deeplearning/optimization/7.5_output3.png" alt></p>
<p>使用Tensorflow2提供的AdaGrad算法来训练模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line">trainer = optimizers.Adagrad(learning_rate=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.5_output4.png" alt></p>
<h1 id="rms-prop-suan-fa">RMSProp算法</h1>
<p>因为调整学习率时分母上的变量\(\boldsymbol{s}_t\)一直在累加按元素平方的小批量随机梯度，所以目标函数自变量每个元素的学习率在迭代过程中一直在降低（或不变）。因此，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSProp算法对AdaGrad算法做了一点小小的修改。该算法源自Coursera上的一门课程，即“机器学习的神经网络” 。</p>
<h2 id="suan-fa-1">算法</h2>
<p>动量法里介绍过指数加权移动平均。不同于AdaGrad算法里状态变量\(\boldsymbol{s}_t\)是截至时间步\(t\)所有小批量随机梯度\(\boldsymbol{g}_t\)按元素平方和，RMSProp算法将这些梯度按元素平方做指数加权移动平均。具体来说，给定超参数\(0 \leq \gamma &lt; 1\)，RMSProp算法在时间步\(t>0\)计算</p>
<p>\[
\boldsymbol{s}_t \leftarrow \gamma \boldsymbol{s}_{t-1} + (1 - \gamma) \boldsymbol{g}_t \odot \boldsymbol{g}_t
\]</p>
<p>和AdaGrad算法一样，RMSProp算法将目标函数自变量中每个元素的学习率通过按元素运算重新调整，然后更新自变量</p>
<p>\[
\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t
\]</p>
<p>其中\(\eta\)是学习率，\(\epsilon\)是为了维持数值稳定性而添加的常数，如\(10^{-6}\)。因为RMSProp算法的状态变量\(\boldsymbol{s}_t\)是对平方项\(\boldsymbol{g}_t \odot \boldsymbol{g}_t\)的指数加权移动平均，所以可以看作是最近\(1/(1-\gamma)\)个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。</p>
<p>先观察RMSProp算法对目标函数\(f(\boldsymbol{x})=0.1x_1^2+2x_2^2\)中自变量的迭代轨迹。使用的学习率为0.4的AdaGrad算法，自变量在迭代后期的移动幅度较小。但在同样的学习率下，RMSProp算法可以更快逼近最优解。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    g1, g2, eps = <span class="number">0.2</span> * x1, <span class="number">4</span> * x2, <span class="number">1e-6</span></span><br><span class="line">    s1 = gamma * s1 + (<span class="number">1</span> - gamma) * g1 ** <span class="number">2</span></span><br><span class="line">    s2 = gamma * s2 + (<span class="number">1</span> - gamma) * g2 ** <span class="number">2</span></span><br><span class="line">    x1 -= eta / math.sqrt(s1 + eps) * g1</span><br><span class="line">    x2 -= eta / math.sqrt(s2 + eps) * g2</span><br><span class="line">    <span class="keyword">return</span> x1, x2, s1, s2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">eta, gamma = <span class="number">0.4</span>, <span class="number">0.9</span></span><br><span class="line">show_trace_2d(f_2d, train_2d(rmsprop_2d))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.6_output1.png" alt></p>
<h2 id="dai-ma-shi-xian-3">代码实现</h2>
<p>接下来按照RMSProp算法中的公式实现该算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rmsprop_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w = tf.zeros((features.shape[<span class="number">1</span>],<span class="number">1</span>),dtype=tf.float32)</span><br><span class="line">    s_b = tf.zeros(<span class="number">1</span>,dtype=tf.float32)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop</span><span class="params">(params, states, hyperparams,grads)</span>:</span></span><br><span class="line">    gamma, eps, i = hyperparams[<span class="string">'gamma'</span>], <span class="number">1e-6</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s=gamma*s+(<span class="number">1</span>-gamma)*(grads[i])**<span class="number">2</span></span><br><span class="line">        p.assign_sub(hyperparams[<span class="string">'lr'</span>]*grads[i]/tf.sqrt(s+eps))</span><br><span class="line">        i+=<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>将初始学习率设为0.01，并将超参数\(\gamma\)设为0.9。此时，变量\(\boldsymbol{s}_t\)可看作是最近\(1/(1-0.9) = 10\)个时间步的平方项\(\boldsymbol{g}_t \odot \boldsymbol{g}_t\)的加权平均。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_ch7(rmsprop, init_rmsprop_states(), &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>, <span class="string">'gamma'</span>: <span class="number">0.9</span>&#125;,</span><br><span class="line">              features, labels)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.6_output2.png" alt></p>
<p>通过名称为<code>RMSprop</code>的优化器方法，便可使用Tensorflow2中提供的RMSProp算法来训练模型。注意，超参数\(\gamma\)通过<code>alpha</code>指定。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line">trainer = optimizers.RMSprop(learning_rate=<span class="number">0.01</span>,rho=<span class="number">0.9</span>)</span><br><span class="line">d2l.train_tensorflow2_ch7(trainer, &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>&#125;,</span><br><span class="line">                    features, labels)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.6_output3.png" alt></p>
<p>RMSProp算法和AdaGrad算法的不同在于，RMSProp算法使用了小批量随机梯度按元素平方的指数加权移动平均来调整学习率。</p>
<h1 id="ada-delta-suan-fa">AdaDelta算法</h1>
<p>除了RMSProp算法以外，另一个常用优化算法AdaDelta算法也针对AdaGrad算法在迭代后期可能较难找到有用解的问题做了改进 。有意思的是，<strong>AdaDelta算法没有学习率这一超参数</strong>。</p>
<h2 id="suan-fa-2">算法</h2>
<p>AdaDelta算法也像RMSProp算法一样，使用了小批量随机梯度\(\boldsymbol{g}_t\)按元素平方的指数加权移动平均变量\(\boldsymbol{s}_t\)。在时间步0，它的所有元素被初始化为0。给定超参数\(0 \leq \rho &lt; 1\)（对应RMSProp算法中的\(\gamma\)），在时间步\(t>0\)，同RMSProp算法一样计算</p>
<p>\[
\boldsymbol{s}_t \leftarrow \rho \boldsymbol{s}_{t-1} + (1 - \rho) \boldsymbol{g}_t \odot \boldsymbol{g}_t
\]</p>
<p>与RMSProp算法不同的是，AdaDelta算法还维护一个额外的状态变量\(\Delta\boldsymbol{x}_t\)，其元素同样在时间步0时被初始化为0。使用\(\Delta\boldsymbol{x}_{t-1}\)来计算自变量的变化量：</p>
<p>\[
\boldsymbol{g}_t' \leftarrow \sqrt{\frac{\Delta\boldsymbol{x}_{t-1} + \epsilon}{\boldsymbol{s}_t + \epsilon}}   \odot \boldsymbol{g}_t
\]</p>
<p>其中\(\epsilon\)是为了维持数值稳定性而添加的常数，如\(10^{-5}\)。接着更新自变量：</p>
<p>\[
\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}'_t
\]</p>
<p>最后，使用\(\Delta\boldsymbol{x}_t\)来记录自变量变化量\(\boldsymbol{g}'_t\)按元素平方的指数加权移动平均：</p>
<p>\[
\Delta\boldsymbol{x}_t \leftarrow \rho \Delta\boldsymbol{x}_{t-1} + (1 - \rho) \boldsymbol{g}'_t \odot \boldsymbol{g}'_t
\]</p>
<p>可以看到，如不考虑\(\epsilon\)的影响，<strong>AdaDelta算法跟RMSProp算法的不同之处在于使用\(\sqrt{\Delta\boldsymbol{x}_{t-1}}\)来替代学习率\(\eta\)</strong>。</p>
<h2 id="dai-ma-shi-xian-4">代码实现</h2>
<p>AdaDelta算法需要对每个自变量维护两个状态变量，即\(\boldsymbol{s}_t\)和\(\Delta\boldsymbol{x}_t\)。我们按AdaDelta算法中的公式实现该算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adadelta_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w, s_b = np.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=float), np.zeros(<span class="number">1</span>, dtype=float)</span><br><span class="line">    delta_w, delta_b = np.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=float), np.zeros(<span class="number">1</span>, dtype=float)</span><br><span class="line">    <span class="keyword">return</span> ((s_w, delta_w), (s_b, delta_b))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adadelta</span><span class="params">(params, states, hyperparams,grads)</span>:</span></span><br><span class="line">    rho, eps,i = hyperparams[<span class="string">'rho'</span>], <span class="number">1e-5</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p, (s, delta) <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s[:] = rho * s + (<span class="number">1</span> - rho) * (grads[i]**<span class="number">2</span>)</span><br><span class="line">        g =  grads[i] * np.sqrt((delta + eps) / (s + eps))</span><br><span class="line">        p.assign_sub(g)</span><br><span class="line">        delta[:] = rho * delta + (<span class="number">1</span> - rho) * g * g</span><br><span class="line">        i+=<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>使用超参数\(\rho=0.9\)来训练模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(adadelta, init_adadelta_states(), &#123;<span class="string">'rho'</span>: <span class="number">0.9</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.7_output1.png" alt></p>
<p>通过名称为<code>Adadelta</code>的优化器方法，便可使用Tensorflow2提供的AdaDelta算法。它的超参数可以通过<code>rho</code>来指定。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line">trainer = keras.optimizers.Adadelta(learning_rate=<span class="number">0.01</span>,rho=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.7_output2.png" alt></p>
<p>AdaDelta算法没有学习率超参数，它通过使用有关自变量更新量平方的指数加权移动平均的项来替代RMSProp算法中的学习率。</p>
<h1 id="adam-suan-fa">Adam算法</h1>
<p>Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均 。</p>
<blockquote>
<p>所以Adam算法可以看做是RMSProp算法与动量法的结合。</p>
</blockquote>
<h2 id="suan-fa-3">算法</h2>
<p>Adam算法使用了动量变量\(\boldsymbol{v}_t\)和RMSProp算法中小批量随机梯度按元素平方的指数加权移动平均变量\(\boldsymbol{s}_t\)，并在时间步0将它们中每个元素初始化为0。给定超参数\(0 \leq \beta_1 &lt; 1\)（算法作者建议设为0.9），时间步\(t\)的动量变量\(\boldsymbol{v}_t\)即小批量随机梯度\(\boldsymbol{g}_t\)的指数加权移动平均：</p>
<p>\[
\boldsymbol{v}_t \leftarrow \beta_1 \boldsymbol{v}_{t-1} + (1 - \beta_1) \boldsymbol{g}_t
\]</p>
<p>和RMSProp算法中一样，给定超参数\(0 \leq \beta_2 &lt; 1\)（算法作者建议设为0.999），<br>
将小批量随机梯度按元素平方后的项\(\boldsymbol{g}_t \odot \boldsymbol{g}_t\)做指数加权移动平均得到\(\boldsymbol{s}_t\)：</p>
<p>\[
\boldsymbol{s}_t \leftarrow \beta_2 \boldsymbol{s}_{t-1} + (1 - \beta_2) \boldsymbol{g}_t \odot \boldsymbol{g}_t
\]</p>
<p>由于将\(\boldsymbol{v}_0\)和\(\boldsymbol{s}_0\)中的元素都初始化为0，在时间步\(t\)得到\(\boldsymbol{v}_t =  (1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} \boldsymbol{g}_i\)。将过去各时间步小批量随机梯度的权值相加，得到 \((1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} = 1 - \beta_1^t\)。需要注意的是，当\(t\)较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当\(\beta_1 = 0.9\)时，\(\boldsymbol{v}_1 = 0.1\boldsymbol{g}_1\)。为了消除这样的影响，对于任意时间步\(t\)，可以将\(\boldsymbol{v}_t\)再除以\(1 - \beta_1^t\)，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作偏差修正。在Adam算法中，对变量\(\boldsymbol{v}_t\)和\(\boldsymbol{s}_t\)均作偏差修正：</p>
<p>\[
\hat{\boldsymbol{v}}_t \leftarrow \frac{\boldsymbol{v}_t}{1 - \beta_1^t}
\]</p>
<p>\[
\hat{\boldsymbol{s}}_t \leftarrow \frac{\boldsymbol{s}_t}{1 - \beta_2^t}
\]</p>
<p>接下来，Adam算法使用以上偏差修正后的变量\(\hat{\boldsymbol{v}}_t\)和\(\hat{\boldsymbol{s}}_t\)，将模型参数中每个元素的学习率通过按元素运算重新调整：</p>
<p>\[
\boldsymbol{g}_t' \leftarrow \frac{\eta \hat{\boldsymbol{v}}_t}{\sqrt{\hat{\boldsymbol{s}}_t} + \epsilon}
\]</p>
<p>其中\(\eta\)是学习率，\(\epsilon\)是为了维持数值稳定性而添加的常数，如\(10^{-8}\)。和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用\(\boldsymbol{g}_t'\)迭代自变量：</p>
<p>\[
\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}_t'
\]</p>
<h2 id="dai-ma-shi-xian-5">代码实现</h2>
<p>按照Adam算法中的公式实现该算法。其中时间步\(t\)通过<code>hyperparams</code>参数传入<code>adam</code>函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adam_states</span><span class="params">()</span>:</span></span><br><span class="line">    v_w, v_b = np.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=float), np.zeros(<span class="number">1</span>, dtype=float)</span><br><span class="line">    s_w, s_b = np.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=float), np.zeros(<span class="number">1</span>, dtype=float)</span><br><span class="line">    <span class="keyword">return</span> ((v_w, s_w), (v_b, s_b))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span><span class="params">(params, states, hyperparams, grads)</span>:</span></span><br><span class="line">    beta1, beta2, eps, i = <span class="number">0.9</span>, <span class="number">0.999</span>, <span class="number">1e-6</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p, (v, s) <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        v[:] = beta1 * v + (<span class="number">1</span> - beta1) * grads[i]</span><br><span class="line">        s[:] = beta2 * s + (<span class="number">1</span> - beta2) * grads[i]**<span class="number">2</span></span><br><span class="line">        v_bias_corr = v / (<span class="number">1</span> - beta1 ** hyperparams[<span class="string">'t'</span>])</span><br><span class="line">        s_bias_corr = s / (<span class="number">1</span> - beta2 ** hyperparams[<span class="string">'t'</span>])</span><br><span class="line">        p.assign_sub(hyperparams[<span class="string">'lr'</span>]*v_bias_corr/(np.sqrt(s_bias_corr) + eps))</span><br><span class="line">        i+=<span class="number">1</span></span><br><span class="line">    hyperparams[<span class="string">'t'</span>] += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>使用学习率为0.01的Adam算法来训练模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(adam, init_adam_states(), &#123;<span class="string">'lr'</span>: <span class="number">0.01</span>, <span class="string">'t'</span>: <span class="number">1</span>&#125;, features,labels)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.8_output1.png" alt></p>
<p>通过名称为“adam”的<code>Trainer</code>实例，我们便可使用Tensorflow2提供的Adam算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line">trainer = keras.optimizers.Adam(learning_rate=<span class="number">0.01</span>)</span><br><span class="line">d2l.train_tensorflow2_ch7(trainer, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.01</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/07/18/deeplearning/optimization/7.8_output2.png" alt></p>
<p>Adam算法在RMSProp算法的基础上对小批量随机梯度也做了指数加权移动平均。<br>
Adam算法使用了偏差修正。</p>
]]></content>
      <categories>
        <category>技术/深度学习</category>
      </categories>
      <tags>
        <tag>optimization</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉基础</title>
    <url>/2019/05/27/deeplearning/computer_vision/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2019/05/27/deeplearning/computer_vision/1.jpeg" alt></p>
<a id="more"></a>
<h1 id="tu-xiang-zeng-yan">图像增广</h1>
<p>大规模数据集是成功应用深度神经网络的前提。图像增广技术通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模。图像增广的另一种解释是，随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力。例如，可以对图像进行不同方式的裁剪，使感兴趣的物体出现在不同位置，从而减轻模型对物体出现位置的依赖性。也可以调整亮度、色彩等因素来降低模型对色彩的敏感度。可以说，在当年AlexNet的成功中，图像增广技术功不可没。</p>
<p>读取一张形状为\(400\times 500\)的图像作为实验的样例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = plt.imread(<span class="string">'../img/cat1.jpg'</span>)</span><br><span class="line">plt.imshow(img)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_3_1.png" alt></p>
<p>定义绘图函数<code>show_images</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_images</span><span class="params">(imgs, num_rows, num_cols, scale=<span class="number">2</span>)</span>:</span></span><br><span class="line">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class="line">    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_rows):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_cols):</span><br><span class="line">            axes[i][j].imshow(imgs[i * num_cols + j])</span><br><span class="line">            axes[i][j].axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">            axes[i][j].axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> axes</span><br></pre></td></tr></table></figure>
<p>大部分图像增广方法都有一定的随机性。为了方便观察图像增广的效果，接下来定义一个辅助函数<code>apply</code>。这个函数对输入图像<code>img</code>多次运行图像增广方法<code>aug</code>并展示所有的结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span><span class="params">(img, aug, num_rows=<span class="number">2</span>, num_cols=<span class="number">4</span>, scale=<span class="number">1.5</span>)</span>:</span></span><br><span class="line">    Y = [aug(img) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_rows * num_cols)]</span><br><span class="line">    show_images(Y, num_rows, num_cols, scale)</span><br></pre></td></tr></table></figure>
<h2 id="fan-zhuan-he-cai-jian">翻转和裁剪</h2>
<p>左右翻转图像通常不改变物体的类别。它是最早也是最广泛使用的一种图像增广方法。下面通过<code>tf.image.random_flip_left_right</code>来实现一半概率的图像左右翻转。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">apply(img, tf.image.random_flip_left_right)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_9_0.png" alt></p>
<p>上下翻转不如左右翻转通用。但是至少对于样例图像，上下翻转不会造成识别障碍。下面创建<code>tf.image.random_flip_up_down</code>实例来实现一半概率的图像上下翻转。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">apply(img, tf.image.random_flip_up_down)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_11_0.png" alt></p>
<p>在使用的样例图像里，猫在图像正中间，但一般情况下可能不是这样。池化层能降低卷积层对目标位置的敏感度,除此之外，还可以通过对图像随机裁剪来让物体以不同的比例出现在图像的不同位置，这同样能够降低模型对目标位置的敏感性。</p>
<p>在下面的代码里，每次随机裁剪出一块面积为原面积\(10\% \sim 100\%\)的区域，且该区域的宽和高之比随机取自\(0.5 \sim 2\)，然后再将该区域的宽和高分别缩放到200像素。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">aug=tf.image.random_crop</span><br><span class="line">num_rows=<span class="number">2</span></span><br><span class="line">num_cols=<span class="number">4</span></span><br><span class="line">scale=<span class="number">1.5</span></span><br><span class="line">crop_size=<span class="number">200</span></span><br><span class="line"></span><br><span class="line">Y = [aug(img, (crop_size, crop_size, <span class="number">3</span>)) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_rows * num_cols)]</span><br><span class="line">show_images(Y, num_rows, num_cols, scale)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_13_1.png" alt></p>
<h2 id="bian-hua-yan-se">变化颜色</h2>
<p>另一类增广方法是变化颜色。可以从4个方面改变图像的颜色：亮度、对比度、饱和度和色调。将图像的亮度随机变化为原图亮度的\(50\%\)（即\(1-0.5\)）\(\sim 150\%\)（即\(1+0.5\)）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">aug=tf.image.random_brightness</span><br><span class="line">num_rows=<span class="number">2</span></span><br><span class="line">num_cols=<span class="number">4</span></span><br><span class="line">scale=<span class="number">1.5</span></span><br><span class="line">max_delta=<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">Y = [aug(img, max_delta) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_rows * num_cols)]</span><br><span class="line">show_images(Y, num_rows, num_cols, scale)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_15_1.png" alt></p>
<p>类似地，也可以随机变化图像的色调。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">aug=tf.image.random_hue</span><br><span class="line">num_rows=<span class="number">2</span></span><br><span class="line">num_cols=<span class="number">4</span></span><br><span class="line">scale=<span class="number">1.5</span></span><br><span class="line">max_delta=<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">Y = [aug(img, max_delta) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_rows * num_cols)]</span><br><span class="line">show_images(Y, num_rows, num_cols, scale)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_17_1.png" alt></p>
<h1 id="wei-diao">微调</h1>
<p>ImageNet有超过1,000万的图像和1,000类的物体。然而，我们平常接触到数据集的规模通常小于1000万。</p>
<p>假设我们想从图像中识别出不同种类的椅子，然后将购买链接推荐给用户。一种可能的方法是先找出100种常见的椅子，为每种椅子拍摄1,000张不同角度的图像，然后在收集到的图像数据集上训练一个分类模型。这个椅子数据集虽然可能比Fashion-MNIST数据集要庞大，但样本数仍然不及ImageNet数据集中样本数的十分之一。这可能会导致适用于ImageNet数据集的复杂模型在这个椅子数据集上过拟合。同时，因为数据量有限，最终训练得到的模型的精度也可能达不到实用的要求。</p>
<p>为了应对上述问题，一个显而易见的解决办法是收集更多的数据。然而，收集和标注数据会花费大量的时间和资金。例如，为了收集ImageNet数据集，研究人员花费了数百万美元的研究经费。虽然目前的数据采集成本已降低了不少，但其成本仍然不可忽略。</p>
<p>另外一种解决办法是应用迁移学习，将从源数据集学到的知识迁移到目标数据集上。虽然ImageNet数据集的图像大多跟椅子无关，但在该数据集上训练的模型可以抽取较通用的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于识别椅子也可能同样有效。</p>
<p>迁移学习中的一种常用技术：微调。如下图所示，微调由以下4步构成。</p>
<ol>
<li>在源数据集上预训练一个神经网络模型，即源模型。</li>
<li>创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。还假设源模型的输出层跟源数据集的标签紧密相关，因此在目标模型中不予采用。</li>
<li>为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。</li>
<li>在目标数据集（如椅子数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。</li>
</ol>
<p><img src="/2019/05/27/deeplearning/computer_vision/finetune.svg" alt="微调"></p>
<p>当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。</p>
<h2 id="ding-yi-he-chu-shi-hua-mo-xing">定义和初始化模型</h2>
<p>这里使用在ImageNet数据集上预训练的ResNet-50作为源模型。这里指定<code>weights='imagenet'</code>来自动下载并加载预训练的模型参数。在第一次使用时需要联网下载模型参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ResNet50 = tf.keras.applications.resnet_v2.ResNet50V2(weights=<span class="string">'imagenet'</span>, input_shape=(<span class="number">224</span>,<span class="number">224</span>,<span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> ResNet50.layers:</span><br><span class="line">    layer.trainable = <span class="literal">False</span></span><br><span class="line">net = tf.keras.models.Sequential()</span><br><span class="line">net.add(ResNet50)</span><br><span class="line">net.add(tf.keras.layers.Flatten())</span><br><span class="line">net.add(tf.keras.layers.Dense(<span class="number">2</span>, activation=<span class="string">'softmax'</span>))</span><br></pre></td></tr></table></figure>
<h1 id="mu-biao-jian-ce-he-bian-jie-kuang">目标检测和边界框</h1>
<h2 id="mu-biao-jian-ce">目标检测</h2>
<p>在图像分类任务里，我们假设图像里只有一个主体目标，并关注如何识别该目标的类别。然而，很多时候图像里有多个我们感兴趣的目标，我们不仅想知道它们的类别，还想得到它们在图像中的具体位置。在计算机视觉里，将这类任务称为目标检测或物体检测。</p>
<p>目标检测在多个领域中被广泛使用。例如，在无人驾驶里，需要通过识别拍摄到的视频图像里的车辆、行人、道路和障碍的位置来规划行进线路。机器人也常通过该任务来检测感兴趣的目标。安防领域则需要检测异常目标，如歹徒或者炸弹。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img = plt.imread(<span class="string">'catdog.jpg'</span>)</span><br><span class="line">plt.imshow(img)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_2_1.png" alt></p>
<h2 id="bian-jie-kuang">边界框</h2>
<p>在目标检测里，通常使用边界框来描述目标位置。边界框是一个矩形框，可以由矩形左上角的\(x\)和\(y\)轴坐标与右下角的\(x\)和\(y\)轴坐标确定。根据上面的图的坐标信息来定义图中狗和猫的边界框。图中的坐标原点在图像的左上角，原点往右和往下分别为\(x\)轴和\(y\)轴的正方向。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># bbox是bounding box的缩写</span></span><br><span class="line">dog_bbox, cat_bbox = [<span class="number">60</span>, <span class="number">45</span>, <span class="number">378</span>, <span class="number">516</span>], [<span class="number">400</span>, <span class="number">112</span>, <span class="number">655</span>, <span class="number">493</span>]</span><br></pre></td></tr></table></figure>
<p>可以在图中将边界框画出来，以检查其是否准确。画之前，定义一个辅助函数<code>bbox_to_rect</code>。它将边界框表示成matplotlib的边界框格式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox_to_rect</span><span class="params">(bbox, color)</span>:</span></span><br><span class="line">    <span class="comment"># 将边界框(左上x, 左上y, 右下x, 右下y)格式转换成matplotlib格式：</span></span><br><span class="line">    <span class="comment"># ((左上x, 左上y), 宽, 高)</span></span><br><span class="line">    <span class="keyword">return</span> plt.Rectangle(</span><br><span class="line">        xy=(bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), width=bbox[<span class="number">2</span>]-bbox[<span class="number">0</span>], height=bbox[<span class="number">3</span>]-bbox[<span class="number">1</span>],</span><br><span class="line">        fill=<span class="literal">False</span>, edgecolor=color, linewidth=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>将边界框加载在图像上，可以看到目标的主要轮廓基本在框内。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig = plt.imshow(img)</span><br><span class="line">fig.axes.add_patch(bbox_to_rect(dog_bbox, <span class="string">'blue'</span>))</span><br><span class="line">fig.axes.add_patch(bbox_to_rect(cat_bbox, <span class="string">'red'</span>));</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_8_0.png" alt></p>
<h1 id="mao-kuang">锚框</h1>
<p>目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边缘从而更准确地预测目标的真实边界框。不同的模型使用的区域采样方法可能不同。这里介绍其中的一种方法：它以每个像素为中心生成多个大小和宽高比不同的边界框。这些边界框被称为锚框。</p>
<h2 id="sheng-cheng-duo-ge-mao-kuang">生成多个锚框</h2>
<p>假设输入图像高为 h ，宽为 w 。分别以图像的每个像素为中心生成不同形状的锚框。设大小为 \(s \in (0,1]\) 且宽高比为 \(r>0\) ，那么锚框的宽和高将分别为\(ws\sqrt{r}\)和\(\frac{hs}{\sqrt{r}}\) 。当中心位置给定时，已知宽和高的锚框是确定的。</p>
<p>下面分别设定好一组大小 \(s_1,\ldots,s_n\) 和一组宽高比 \(r_1,\ldots,r_m\) 。如果以每个像素为中心时使用所有的大小与宽高比的组合，输入图像将一共得到 \(w \times h \times n \times m\) 个锚框。虽然这些锚框可能覆盖了所有的真实边界框，但计算复杂度容易过高。因此，通常只对包含 s1 或 r1 的大小与宽高比的组合感兴趣，即</p>
<p>\[
(s_1,r_1),(s_1,r_2),\ldots,(s_1,r_m),(s_2,r_1),(s_3,r_1),\ldots,(s_n,r_1)
\]</p>
<p>也就是说，以相同像素为中心的锚框的数量为 \(n+m−1\) 。对于整个输入图像，将一共生成 \(w \times h \times (n+m−1)\) 个锚框。</p>
<p>生成锚框的方法已实现在MultiBoxPrior函数中。指定输入、一组大小和一组宽高比，该函数将返回输入的所有锚框。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img_raw = tf.io.read_file(<span class="string">'catdog.jpg'</span>)</span><br><span class="line">img = tf.image.decode_jpeg(img_raw).numpy()</span><br><span class="line">h, w = img.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">print(h, w) <span class="comment"># 252 322</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MultiBoxPrior</span><span class="params">(feature_map, sizes=[<span class="number">0.75</span>, <span class="number">0.5</span>, <span class="number">0.25</span>], ratios=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>])</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        feature_map: torch tensor, Shape: [N, C, H, W].</span></span><br><span class="line"><span class="string">        sizes: List of sizes (0~1) of generated MultiBoxPriores. </span></span><br><span class="line"><span class="string">        ratios: List of aspect ratios (non-negative) of generated MultiBoxPriores. </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        anchors of shape (1, num_anchors, 4). 由于batch里每个都一样, 所以第一维为1</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    pairs = [] <span class="comment"># pair of (size, sqrt(ratio))</span></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> ratios:</span><br><span class="line">        pairs.append([sizes[<span class="number">0</span>], np.sqrt(r)])</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> sizes[<span class="number">1</span>:]:</span><br><span class="line">        pairs.append([s, np.sqrt(ratios[<span class="number">0</span>])])</span><br><span class="line">    </span><br><span class="line">    pairs = np.array(pairs)</span><br><span class="line"></span><br><span class="line">    ss1 = pairs[:, <span class="number">0</span>] * pairs[:, <span class="number">1</span>] <span class="comment"># size * sqrt(ration)</span></span><br><span class="line">    ss2 = pairs[:, <span class="number">0</span>] / pairs[:, <span class="number">1</span>] <span class="comment"># size / sqrt(retion)</span></span><br><span class="line"></span><br><span class="line">    base_anchors = tf.stack([-ss1, -ss2, ss1, ss2], axis=<span class="number">1</span>) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    h, w = feature_map.shape[<span class="number">-2</span>:]</span><br><span class="line">    shifts_x = tf.divide(tf.range(<span class="number">0</span>, w), w)</span><br><span class="line">    shifts_y = tf.divide(tf.range(<span class="number">0</span>, h), h)</span><br><span class="line">    shift_x, shift_y = tf.meshgrid(shifts_x, shifts_y)</span><br><span class="line">    shift_x = tf.reshape(shift_x, (<span class="number">-1</span>,))</span><br><span class="line">    shift_y = tf.reshape(shift_y, (<span class="number">-1</span>,))</span><br><span class="line">    shifts = tf.stack((shift_x, shift_y, shift_x, shift_y), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    anchors = tf.add(tf.reshape(shifts, (<span class="number">-1</span>,<span class="number">1</span>,<span class="number">4</span>)), tf.reshape(base_anchors, (<span class="number">1</span>,<span class="number">-1</span>,<span class="number">4</span>)))</span><br><span class="line">    <span class="keyword">return</span> tf.cast(tf.reshape(anchors, (<span class="number">1</span>,<span class="number">-1</span>,<span class="number">4</span>)), tf.float32)</span><br><span class="line"></span><br><span class="line">x = tf.zeros((<span class="number">1</span>,<span class="number">3</span>,h,w))</span><br><span class="line">y = MultiBoxPrior(x)</span><br><span class="line">y.shape</span><br></pre></td></tr></table></figure>
<p>返回锚框变量y的形状为（1，锚框个数，4）。将锚框变量y的形状变为（图像高，图像宽，以相同像素为中心的锚框个数，4）后，就可以通过指定像素位置来获取所有以该像素为中心的锚框了。下面的例子里我们访问以（250，250）为中心的第一个锚框。它有4个元素，分别是锚框左上角的x和y轴坐标和右下角的x和y轴坐标，其中x和y轴的坐标值分别已除以图像的宽和高，因此值域均为0和1之间。</p>
<p>为了描绘图像中以某个像素为中心的所有锚框，先定义show_bboxes函数以便在图像上画出多个边界框。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox_to_rect</span><span class="params">(bbox, color)</span>:</span></span><br><span class="line">    <span class="comment"># 将边界框(左上x, 左上y, 右下x, 右下y)格式转换成matplotlib格式：</span></span><br><span class="line">    <span class="comment"># ((左上x, 左上y), 宽, 高)</span></span><br><span class="line">    <span class="keyword">return</span> plt.Rectangle(</span><br><span class="line">        xy=(bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), width=bbox[<span class="number">2</span>]-bbox[<span class="number">0</span>], height=bbox[<span class="number">3</span>]-bbox[<span class="number">1</span>],</span><br><span class="line">        fill=<span class="literal">False</span>, edgecolor=color, linewidth=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_bboxes</span><span class="params">(axes, bboxes, labels=None, colors=None)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_list</span><span class="params">(obj, default_values=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> obj <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            obj = default_values</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> isinstance(obj, (list, tuple)):</span><br><span class="line">            obj = [obj]</span><br><span class="line">        <span class="keyword">return</span> obj</span><br><span class="line">    </span><br><span class="line">    labels = _make_list(labels)</span><br><span class="line">    colors = _make_list(colors, [<span class="string">'b'</span>, <span class="string">'g'</span>, <span class="string">'r'</span>, <span class="string">'m'</span>, <span class="string">'c'</span>])</span><br><span class="line">    <span class="keyword">for</span> i, bbox <span class="keyword">in</span> enumerate(bboxes):</span><br><span class="line">        color = colors[i % len(colors)]</span><br><span class="line">        rect = bbox_to_rect(bbox.numpy(), color)</span><br><span class="line">        axes.add_patch(rect)</span><br><span class="line">        <span class="keyword">if</span> labels <span class="keyword">and</span> len(labels) &gt; i:</span><br><span class="line">            text_color = <span class="string">'k'</span> <span class="keyword">if</span> color == <span class="string">'w'</span> <span class="keyword">else</span> <span class="string">'w'</span></span><br><span class="line">            axes.text(rect.xy[<span class="number">0</span>], rect.xy[<span class="number">1</span>], labels[i],</span><br><span class="line">                va=<span class="string">'center'</span>, ha=<span class="string">'center'</span>, fontsize=<span class="number">6</span>,</span><br><span class="line">                color=text_color, bbox=dict(facecolor=color, lw=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<p>刚刚我们看到，变量boxes中x和y轴的坐标值分别已除以图像的宽和高。在绘图时，我们需要恢复锚框的原始坐标值，并因此定义了变量bbox_scale。现在可以画出图像中以(250, 250)为中心的所有锚框了。可以看到，大小为0.75且宽高比为1的锚框较好地覆盖了图像中的狗。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_svg_display</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""Use svg format to display plot in jupyter"""</span></span><br><span class="line">    display.set_matplotlib_formats(<span class="string">'svg'</span>)</span><br><span class="line"></span><br><span class="line">use_svg_display()</span><br><span class="line"><span class="comment"># 设置图的尺寸</span></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">3.5</span>, <span class="number">2.5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig = plt.imshow(img)</span><br><span class="line">bbox_scale = tf.constant([[w,h,w,h]], dtype=tf.float32)</span><br><span class="line">show_bboxes(fig.axes, tf.multiply(boxes[<span class="number">200</span>,<span class="number">250</span>,:,:], bbox_scale), </span><br><span class="line">    [<span class="string">'s=0.75, r=1'</span>, <span class="string">'s=0.75, r=2'</span>, <span class="string">'s=0.55, r=0.5'</span>, </span><br><span class="line">     <span class="string">'s=0.5, r=1'</span>, <span class="string">'s=0.25, r=1'</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/chapter_computer-vision_anchor_9_0.svg" alt></p>
<h2 id="jiao-bing-bi">交并比</h2>
<p>刚刚提到某个锚框较好地覆盖了图像中的狗。如果该目标的真实边界框已知，这里的“较好”该如何量化呢？一种直观的方法是衡量锚框和真实边界框之间的相似度。Jaccard系数可以衡量两个集合的相似度。给定集合A和B，它们的Jaccard系数即二者交集大小除以二者并集大小：<br>
\[
J(\mathcal{A}, \mathcal{B})=\frac{|\mathcal{A} \cap \mathcal{B}|}{|\mathcal{A} \cup \mathcal{B} \mid}
\]<br>
实际上，可以把边界框内的像素区域看成是像素的集合。如此一来，可以用两个边界框的像素集合的Jaccard系数衡量这两个边界框的相似度。当衡量两个边界框的相似度时，通常将Jaccard系数称为交并比，即两个边界框相交面积与相并面积之比，如下图所示。交并比的取值范围在0和1之间：0表示两个边界框无重合像素，1表示两个边界框相等。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/9.4_iou.svg" alt></p>
<p>下面对其进行实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">set_1 = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]]</span><br><span class="line">set_2 = [[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]]</span><br><span class="line">lower_bounds = tf.maximum(tf.expand_dims(set_1, axis=<span class="number">1</span>), tf.expand_dims(set_2, axis=<span class="number">0</span>)) <span class="comment"># (n1, n2, 2)</span></span><br><span class="line">upper_bounds = tf.minimum(tf.expand_dims(set_1, axis=<span class="number">1</span>), tf.expand_dims(set_2, axis=<span class="number">0</span>)) <span class="comment"># (n1, n2, 2)</span></span><br><span class="line"></span><br><span class="line">tf.expand_dims(set_1, axis=<span class="number">1</span>), tf.expand_dims(set_2, axis=<span class="number">0</span>), lower_bounds, tf.multiply(set_1, set_2), tf.subtract(set_1, set_2)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_intersection</span><span class="params">(set_1, set_2)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算anchor之间的交集</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)</span></span><br><span class="line"><span class="string">        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># tensorflow auto-broadcasts singleton dimensions</span></span><br><span class="line">    lower_bounds = tf.maximum(tf.expand_dims(set_1[:,:<span class="number">2</span>], axis=<span class="number">1</span>), tf.expand_dims(set_2[:,:<span class="number">2</span>], axis=<span class="number">0</span>)) <span class="comment"># (n1, n2, 2)</span></span><br><span class="line">    upper_bounds = tf.minimum(tf.expand_dims(set_1[:,<span class="number">2</span>:], axis=<span class="number">1</span>), tf.expand_dims(set_2[:,<span class="number">2</span>:], axis=<span class="number">0</span>)) <span class="comment"># (n1, n2, 2)</span></span><br><span class="line">    <span class="comment"># 设置最小值</span></span><br><span class="line">    intersection_dims = tf.clip_by_value(upper_bounds - lower_bounds, clip_value_min=<span class="number">0</span>, clip_value_max=<span class="number">3</span>) <span class="comment"># (n1, n2, 2)</span></span><br><span class="line">    <span class="keyword">return</span> tf.multiply(intersection_dims[:, :, <span class="number">0</span>], intersection_dims[:, :, <span class="number">1</span>]) <span class="comment"># (n1, n2)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_jaccard</span><span class="params">(set_1, set_2)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算anchor之间的Jaccard系数(IoU)</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)</span></span><br><span class="line"><span class="string">        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Find intersections</span></span><br><span class="line">    intersection = compute_intersection(set_1, set_2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Find areas of each box in both sets</span></span><br><span class="line">    areas_set_1 = tf.multiply(tf.subtract(set_1[:, <span class="number">2</span>], set_1[:, <span class="number">0</span>]), tf.subtract(set_1[:, <span class="number">3</span>], set_1[:, <span class="number">1</span>]))  <span class="comment"># (n1)</span></span><br><span class="line">    areas_set_2 = tf.multiply(tf.subtract(set_2[:, <span class="number">2</span>], set_2[:, <span class="number">0</span>]), tf.subtract(set_2[:, <span class="number">3</span>], set_2[:, <span class="number">1</span>]))  <span class="comment"># (n2)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Find the union</span></span><br><span class="line">    union = tf.add(tf.expand_dims(areas_set_1, axis=<span class="number">1</span>), tf.expand_dims(areas_set_2, axis=<span class="number">0</span>))  <span class="comment"># (n1, n2)</span></span><br><span class="line">    union = tf.subtract(union, intersection)  <span class="comment"># (n1, n2)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.divide(intersection, union) <span class="comment">#(n1, n2)</span></span><br></pre></td></tr></table></figure>
<p>使用交并比来衡量锚框与真实边界框以及锚框与锚框之间的相似度。</p>
<h2 id="biao-zhu-xun-lian-ji-de-mao-kuang">标注训练集的锚框</h2>
<p>在训练集中，将每个锚框视为一个训练样本。为了训练目标检测模型，需要为每个锚框标注两类标签：一是锚框所含目标的类别，简称类别；二是真实边界框相对锚框的偏移量，简称偏移量。在目标检测时，首先生成多个锚框，然后为每个锚框预测类别以及偏移量，接着根据预测的偏移量调整锚框位置从而得到预测边界框，最后筛选需要输出的预测边界框。</p>
<p>在目标检测的训练集中，每个图像已标注了真实边界框的位置以及所含目标的类别。在生成锚框之后，主要依据与锚框相似的真实边界框的位置和类别信息为锚框标注。那么，该如何为锚框分配与其相似的真实边界框呢？</p>
<p>假设图像中锚框分别为 \(A_1,A_2,\dots,A_{na}\) ，真实边界框分别为 \(B_1,B_2,\ldots,B_{nb}\) ，且 \(na\ge nb\) 。定义矩阵 \(X \in R_{na×nb}\) ，其中第 i 行第 j 列的元素 \(x_{ij}\) 为锚框 Ai 与真实边界框 Bj 的交并比。 首先，找出矩阵 \(X\) 中最大元素，并将该元素的行索引与列索引分别记为 \(i_1,j_1\) 。为锚框 \(A_{i_1}\) 分配真实边界框 \(B_{j_1}\) 。显然，锚框 \(A_{i_1}\) 和真实边界框 \(B_{j_1}\) 在所有的“锚框—真实边界框”的配对中相似度最高。接下来，将矩阵 \(X\) 中第 \(i_1\) 行和第 \(j_1\) 列上的所有元素丢弃。找出矩阵 X 中剩余的最大元素，并将该元素的行索引与列索引分别记为 \(i_2,j_2\) 。为锚框 \(A_{i_2}\) 分配真实边界框 \(B_{j_2}\) ，再将矩阵 X 中第 \(i_2\) 行和第 \(j_2\) 列上的所有元素丢弃。此时矩阵 X 中已有2行2列的元素被丢弃。 依此类推，直到矩阵 X 中所有 \(n_b\) 列元素全部被丢弃。这个时候，已为 \(n_b\) 个锚框各分配了一个真实边界框。 接下来，只遍历剩余的 \(n_a−n_b\) 个锚框：给定其中的锚框 \(A_i\) ，根据矩阵 X 的第 \(i\) 行找到与 \(A_i\) 交并比最大的真实边界框 \(B_j\) ，且只有当该交并比大于预先设定的阈值时，才为锚框 \(A_i\) 分配真实边界框 \(B_j\) 。</p>
<p>如下图所示，假设矩阵 X 中最大值为 \(x_{23}\) ，将为锚框 \(A_2\) 分配真实边界框 \(B_3\) 。然后，丢弃矩阵中第2行和第3列的所有元素，找出剩余阴影部分的最大元素 \(x_{71}\) ，为锚框 \(A_7\) 分配真实边界框 \(B_1\) 。接着如下图（中）所示，丢弃矩阵中第7行和第1列的所有元素，找出剩余阴影部分的最大元素 \(x_{54}\) ，为锚框 \(A_5\) 分配真实边界框 \(B_4\) 。最后如下图（右）所示，丢弃矩阵中第5行和第4列的所有元素，找出剩余阴影部分的最大元素 \(x_{92}\) ，为锚框 \(A_9\) 分配真实边界框 \(B_2\) 。之后，我们只需遍历除去 \(A_2,A_5,A_7,A_9\) 的剩余锚框，并根据阈值判断是否为剩余锚框分配真实边界框。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/anchor-label.svg" alt></p>
<p>现在可以标注锚框的类别和偏移量了。如果一个锚框 A 被分配了真实边界框 B ，将锚框 A 的类别设为 B 的类别，并根据 B 和 A 的中心坐标的相对位置以及两个框的相对大小为锚框 A 标注偏移量。由于数据集中各个框的位置和大小各异，因此这些相对位置和相对大小通常需要一些特殊变换，才能使偏移量的分布更均匀从而更容易拟合。设锚框 A 及其被分配的真实边界框 B 的中心坐标分别为 \((x_a,y_a)\) 和 \((x_b,y_b)\) ， A 和 B 的宽分别为 \(w_a\) 和 \(w_b\) ，高分别为 \(h_a\) 和 \(h_b\) ，一个常用的技巧是将 A 的偏移量标注为<br>
\[
\left(\frac{\frac{x_{b}-x_{a}}{w_{a}}-\mu_{x}}{\sigma_{x}} ,
\frac{\frac{y_{b}-y_{a}}{h_{a}}-\mu_{y}}{\sigma_{y}} ,
\frac{\log \frac{w_{b}}{w_{a}}-\mu_{w}}{\sigma_{w}},
\frac{\log \frac{h_{b}}{h_{a}}-\mu_{h}}{\sigma_{h}}
\right)
\]<br>
其中常数的默认值为 \(\mu_x=\mu_y=\mu_w=\mu_h=0\),\(\sigma_x=\sigma_y=0.1\),\(\sigma_w=\sigma_h=0.2\) 。如果一个锚框没有被分配真实边界框，只需将该锚框的类别设为背景。类别为背景的锚框通常被称为负类锚框，其余则被称为正类锚框。</p>
<p>下面演示一个具体的例子。为读取的图像中的猫和狗定义真实边界框，其中第一个元素为类别（0为狗，1为猫），剩余4个元素分别为左上角的 x 和 y 轴坐标以及右下角的 x 和 y 轴坐标（值域在0到1之间）。这里通过左上角和右下角的坐标构造了5个需要标注的锚框，分别记为 \(A_0,\ldots,A_4\) 。先画出这些锚框与真实边界框在图像中的位置。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bbox_scale = tf.constant([[w,h,w,h]], dtype=tf.float32)</span><br><span class="line">ground_truth = tf.constant([[<span class="number">0</span>, <span class="number">0.13</span>, <span class="number">0</span>, <span class="number">0.47</span>, <span class="number">1</span>],</span><br><span class="line">                [<span class="number">1</span>, <span class="number">0.47</span>, <span class="number">0.15</span>, <span class="number">0.84</span>, <span class="number">1</span>]])</span><br><span class="line">anchors = tf.constant([[<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>],</span><br><span class="line">            [<span class="number">0.15</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.4</span>],</span><br><span class="line">            [<span class="number">0.63</span>, <span class="number">0.05</span>, <span class="number">0.88</span>, <span class="number">0.98</span>],</span><br><span class="line">            [<span class="number">0.66</span>, <span class="number">0.45</span>, <span class="number">0.8</span>, <span class="number">0.8</span>],</span><br><span class="line">            [<span class="number">0.57</span>, <span class="number">0.3</span>,  <span class="number">0.92</span>, <span class="number">0.9</span>]])</span><br><span class="line"></span><br><span class="line">fig = plt.imshow(img)</span><br><span class="line">show_bboxes(fig.axes, tf.multiply(ground_truth[:, <span class="number">1</span>:], bbox_scale),</span><br><span class="line">        [<span class="string">'dog'</span>, <span class="string">'cat'</span>], <span class="string">'k'</span>)</span><br><span class="line">show_bboxes(fig.axes, tf.multiply(anchors, bbox_scale),</span><br><span class="line">        [<span class="string">'0'</span>, <span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>, <span class="string">'4'</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_38_0.svg" alt></p>
<p>下面MultiBoxTarget函数来为锚框标注类别和偏移量。该函数将背景类别设为0，并令从零开始的目标类别的整数索引自加1（1为狗，2为猫）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">assign_anchor</span><span class="params">(bb, anchor, jaccard_threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        bb: 真实边界框(bounding box), shape:（nb, 4）</span></span><br><span class="line"><span class="string">        anchor: 待分配的anchor, shape:（na, 4）</span></span><br><span class="line"><span class="string">        jaccard_threshold: 预先设定的阈值</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        assigned_idx: shape: (na, ), 每个anchor分配的真实bb对应的索引, 若未分配任何bb则为-1</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    na = anchor.shape[<span class="number">0</span>]</span><br><span class="line">    nb = bb.shape[<span class="number">0</span>]</span><br><span class="line">    jaccard = compute_jaccard(anchor, bb).numpy()   <span class="comment"># shape: (na, nb)</span></span><br><span class="line">    assigned_idx = np.ones(na) * <span class="number">-1</span> <span class="comment"># 初始全为-1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 先为每个bb分配一个anchor（不要求满足jaccard_threshold）</span></span><br><span class="line">    jaccard_cp = jaccard.copy()</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(nb):</span><br><span class="line">        i = np.argmax(jaccard_cp[:, j])</span><br><span class="line">        assigned_idx[i] = j</span><br><span class="line">        jaccard_cp[i, :] = float(<span class="string">"-inf"</span>)    <span class="comment"># 赋值为负无穷, 相当于去掉这一行</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 处理还未被分配的anchor， 要求满足jaccard_threshold</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(na):</span><br><span class="line">        <span class="keyword">if</span> assigned_idx[i] == <span class="number">-1</span>:</span><br><span class="line">            j = np.argmax(jaccard[i, :])</span><br><span class="line">            <span class="keyword">if</span> jaccard[i, j] &gt;= jaccard_threshold:</span><br><span class="line">                assigned_idx[i] = j</span><br><span class="line">    <span class="keyword">return</span> tf.cast(assigned_idx, tf.int32)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xy_to_cxcy</span><span class="params">(xy)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将(x_min, y_min, x_max, y_max)形式的anchor转换成(center_x, center_y, w, h)形式的.</span></span><br><span class="line"><span class="string">    https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        xy: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> tf.concat(((xy[:, <span class="number">2</span>:] + xy[:, :<span class="number">2</span>]) / <span class="number">2</span>,  <span class="comment">#c_x, c_y</span></span><br><span class="line">              xy[:, <span class="number">2</span>:] - xy[:, :<span class="number">2</span>]), axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MultiBoxTarget</span><span class="params">(anchor, label)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        anchor: torch tensor, 输入的锚框, 一般是通过MultiBoxPrior生成, shape:（1，锚框总数，4）</span></span><br><span class="line"><span class="string">        label: 真实标签, shape为(bn, 每张图片最多的真实锚框数, 5)</span></span><br><span class="line"><span class="string">               第二维中，如果给定图片没有这么多锚框, 可以先用-1填充空白, 最后一维中的元素为[类别标签, 四个坐标值]</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        列表, [bbox_offset, bbox_mask, cls_labels]</span></span><br><span class="line"><span class="string">        bbox_offset: 每个锚框的标注偏移量，形状为(bn，锚框总数*4)</span></span><br><span class="line"><span class="string">        bbox_mask: 形状同bbox_offset, 每个锚框的掩码, 一一对应上面的偏移量, 负类锚框(背景)对应的掩码均为0, 正类锚框的掩码均为1</span></span><br><span class="line"><span class="string">        cls_labels: 每个锚框的标注类别, 其中0表示为背景, 形状为(bn，锚框总数)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">assert</span> len(anchor.shape) == <span class="number">3</span> <span class="keyword">and</span> len(label.shape) == <span class="number">3</span></span><br><span class="line">    bn = label.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">MultiBoxTarget_one</span><span class="params">(anchor, label, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        MultiBoxTarget函数的辅助函数, 处理batch中的一个</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            anchor: shape of (锚框总数, 4)</span></span><br><span class="line"><span class="string">            label: shape of (真实锚框数, 5), 5代表[类别标签, 四个坐标值]</span></span><br><span class="line"><span class="string">            eps: 一个极小值, 防止log0</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            offset: (锚框总数*4, )</span></span><br><span class="line"><span class="string">            bbox_mask: (锚框总数*4, ), 0代表背景, 1代表非背景</span></span><br><span class="line"><span class="string">            cls_labels: (锚框总数, 4), 0代表背景</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        an = anchor.shape[<span class="number">0</span>]</span><br><span class="line">        assigned_idx = assign_anchor(label[:, <span class="number">1</span>:], anchor) <span class="comment">## (锚框总数, )</span></span><br><span class="line">        <span class="comment"># 决定anchor留下或者舍去</span></span><br><span class="line">        bbox_mask = tf.repeat(tf.expand_dims(tf.cast((assigned_idx &gt;= <span class="number">0</span>), dtype=tf.double), axis=<span class="number">-1</span>), repeats=<span class="number">4</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        cls_labels = np.zeros(an, dtype=int) <span class="comment"># 0表示背景</span></span><br><span class="line">        assigned_bb = np.zeros((an, <span class="number">4</span>), dtype=float) <span class="comment"># 所有anchor对应的bb坐标</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(an):</span><br><span class="line">            bb_idx = assigned_idx[i]</span><br><span class="line">            <span class="keyword">if</span> bb_idx &gt;= <span class="number">0</span>: <span class="comment"># 即非背景</span></span><br><span class="line">                cls_labels[i] = label.numpy()[bb_idx, <span class="number">0</span>] + <span class="number">1</span> <span class="comment"># 要注意加1</span></span><br><span class="line">                assigned_bb[i, :] = label.numpy()[bb_idx, <span class="number">1</span>:]</span><br><span class="line">        </span><br><span class="line">        center_anchor = tf.cast(xy_to_cxcy(anchor), dtype=tf.double)  <span class="comment"># (center_x, center_y, w, h)</span></span><br><span class="line">        center_assigned_bb = tf.cast(xy_to_cxcy(assigned_bb), dtype=tf.double) <span class="comment"># (center_x, center_y, w, h)</span></span><br><span class="line"></span><br><span class="line">        offset_xy = <span class="number">10.0</span> * (center_assigned_bb[:,:<span class="number">2</span>] - center_anchor[:,:<span class="number">2</span>]) / center_anchor[:,<span class="number">2</span>:]</span><br><span class="line">        offset_wh = <span class="number">5.0</span> * tf.math.log(eps + center_assigned_bb[:, <span class="number">2</span>:] / center_anchor[:, <span class="number">2</span>:])</span><br><span class="line">        offset = tf.multiply(tf.concat((offset_xy, offset_wh), axis=<span class="number">1</span>), bbox_mask)    <span class="comment"># (锚框总数, 4)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tf.reshape(offset, (<span class="number">-1</span>,)), tf.reshape(bbox_mask, (<span class="number">-1</span>,)), cls_labels</span><br><span class="line">    </span><br><span class="line">    batch_offset = []</span><br><span class="line">    batch_mask = []</span><br><span class="line">    batch_cls_labels = []</span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> range(bn):</span><br><span class="line">        offset, bbox_mask, cls_labels = MultiBoxTarget_one(anchor[<span class="number">0</span>, :, :], label[b,:,:])</span><br><span class="line"></span><br><span class="line">        batch_offset.append(offset)</span><br><span class="line">        batch_mask.append(bbox_mask)</span><br><span class="line">        batch_cls_labels.append(cls_labels)</span><br><span class="line">    </span><br><span class="line">    batch_offset = tf.convert_to_tensor(batch_offset)</span><br><span class="line">    batch_mask = tf.convert_to_tensor(batch_mask)</span><br><span class="line">    batch_cls_labels = tf.convert_to_tensor(batch_cls_labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [batch_offset, batch_mask, batch_cls_labels]</span><br></pre></td></tr></table></figure>
<p>我们通过tf.expand_dims函数为锚框和真实边界框添加样本维。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">labels = MultiBoxTarget(tf.expand_dims(anchors, axis=<span class="number">0</span>), </span><br><span class="line">            tf.expand_dims(ground_truth, axis=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<p>根据锚框与真实边界框在图像中的位置来分析这些标注的类别。首先，在所有的“锚框—真实边界框”的配对中，锚框 A4 与猫的真实边界框的交并比最大，因此锚框 A4 的类别标注为猫。不考虑锚框 A4 或猫的真实边界框，在剩余的“锚框—真实边界框”的配对中，最大交并比的配对为锚框 A1 和狗的真实边界框，因此锚框 A1 的类别标注为狗。接下来遍历未标注的剩余3个锚框：与锚框 A0 交并比最大的真实边界框的类别为狗，但交并比小于阈值（默认为0.5），因此类别标注为背景；与锚框 A2 交并比最大的真实边界框的类别为猫，且交并比大于阈值，因此类别标注为猫；与锚框 A3 交并比最大的真实边界框的类别为猫，但交并比小于阈值，因此类别标注为背景。</p>
<p>返回值的第二项为掩码（mask）变量，形状为(批量大小, 锚框个数的四倍)。掩码变量中的元素与每个锚框的4个偏移量一一对应。 由于不关心对背景的检测，有关负类的偏移量不应影响目标函数。通过按元素乘法，掩码变量中的0可以在计算目标函数之前过滤掉负类的偏移量。</p>
<h2 id="shu-chu-yu-ce-bian-jie-kuang">输出预测边界框</h2>
<p>在模型预测阶段，先为图像生成多个锚框，并为这些锚框一一预测类别和偏移量。随后，根据锚框及其预测偏移量得到预测边界框。当锚框数量较多时，同一个目标上可能会输出较多相似的预测边界框。为了使结果更加简洁，可以移除相似的预测边界框。常用的方法叫作非极大值抑制。</p>
<p>来描述一下非极大值抑制的工作原理。对于一个预测边界框 B ，模型会计算各个类别的预测概率。设其中最大的预测概率为 p ，该概率所对应的类别即 B 的预测类别。也将 p 称为预测边界框 B 的置信度。在同一图像上，将预测类别非背景的预测边界框按置信度从高到低排序，得到列表 L 。从 L 中选取置信度最高的预测边界框 B1 作为基准，将所有与 B1 的交并比大于某阈值的非基准预测边界框从 L 中移除。这里的阈值是预先设定的超参数。此时， L 保留了置信度最高的预测边界框并移除了与其相似的其他预测边界框。 接下来，从 L 中选取置信度第二高的预测边界框 B2 作为基准，将所有与 B2 的交并比大于某阈值的非基准预测边界框从 L 中移除。重复这一过程，直到 L 中所有的预测边界框都曾作为基准。此时 L 中任意一对预测边界框的交并比都小于阈值。最终，输出列表 L 中的所有预测边界框。</p>
<p>先构造4个锚框。简单起见，假设预测偏移量全是0：预测边界框即锚框。最后，我们构造每个类别的预测概率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">anchors = tf.convert_to_tensor([[<span class="number">0.1</span>, <span class="number">0.08</span>, <span class="number">0.52</span>, <span class="number">0.92</span>],</span><br><span class="line">                [<span class="number">0.08</span>, <span class="number">0.2</span>, <span class="number">0.56</span>, <span class="number">0.95</span>],</span><br><span class="line">                [<span class="number">0.15</span>, <span class="number">0.3</span>, <span class="number">0.62</span>, <span class="number">0.91</span>],</span><br><span class="line">                [<span class="number">0.55</span>, <span class="number">0.2</span>, <span class="number">0.9</span>, <span class="number">0.88</span>]])</span><br><span class="line">offset_preds = tf.convert_to_tensor([<span class="number">0.0</span>] * (<span class="number">4</span> * len(anchors)))</span><br><span class="line">cls_probs = tf.convert_to_tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>], <span class="comment"># 背景的预测概率</span></span><br><span class="line">                [<span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.7</span>, <span class="number">0.1</span>],    <span class="comment"># 狗的预测概率</span></span><br><span class="line">                [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.9</span>]])   <span class="comment"># 猫的预测概率</span></span><br><span class="line">anchors, offset_preds, cls_probs</span><br><span class="line"></span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">(&lt;tf.Tensor: shape=(<span class="number">4</span>, <span class="number">4</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">0.1</span> , <span class="number">0.08</span>, <span class="number">0.52</span>, <span class="number">0.92</span>],</span><br><span class="line">        [<span class="number">0.08</span>, <span class="number">0.2</span> , <span class="number">0.56</span>, <span class="number">0.95</span>],</span><br><span class="line">        [<span class="number">0.</span>  , <span class="number">0.3</span> , <span class="number">0.62</span>, <span class="number">0.91</span>],</span><br><span class="line">        [<span class="number">0.55</span>, <span class="number">0.2</span> , <span class="number">0.9</span> , <span class="number">0.88</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">16</span>,), dtype=float32, numpy=</span><br><span class="line"> array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">4</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.</span> ],</span><br><span class="line">        [<span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.7</span>, <span class="number">0.1</span>],</span><br><span class="line">        [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.9</span>]], dtype=float32)&gt;)</span><br></pre></td></tr></table></figure>
<p>在图像上打印预测边界框和它们的置信度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig = plt.imshow(img)</span><br><span class="line">show_bboxes(fig.axes, anchors * bbox_scale,</span><br><span class="line">        [<span class="string">'dog=0.9'</span>, <span class="string">'dog=0.8'</span>, <span class="string">'dog=0.7'</span>, <span class="string">'cat=0.9'</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_56_0.svg" alt></p>
<p>下面实现MultiBoxDetection函数来执行非极大值抑制。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line">Pred_BB_Info = namedtuple(<span class="string">"Pred_BB_Info"</span>, </span><br><span class="line">        [<span class="string">"index"</span>, <span class="string">"class_id"</span>, <span class="string">"confidence"</span>, <span class="string">"xyxy"</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">non_max_suppression</span><span class="params">(bb_info_list, nms_threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    非极大抑制处理预测的边界框</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        bb_info_list: Pred_BB_Info的列表, 包含预测类别、置信度等信息</span></span><br><span class="line"><span class="string">        nms_threshold: 阈值</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        output: Pred_BB_Info的列表, 只保留过滤后的边界框信息</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    output = []</span><br><span class="line">    <span class="comment"># 现根据置信度从高到底排序</span></span><br><span class="line">    sorted_bb_info_list = sorted(bb_info_list,</span><br><span class="line">                    key = <span class="keyword">lambda</span> x: x.confidence, </span><br><span class="line">                    reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">while</span> len(sorted_bb_info_list) != <span class="number">0</span>:</span><br><span class="line">        best = sorted_bb_info_list.pop(<span class="number">0</span>)</span><br><span class="line">        output.append(best)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> len(sorted_bb_info_list) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        bb_xyxy = []</span><br><span class="line">        <span class="keyword">for</span> bb <span class="keyword">in</span> sorted_bb_info_list:</span><br><span class="line">            bb_xyxy.append(bb.xyxy)</span><br><span class="line">        </span><br><span class="line">        iou = compute_jaccard(tf.convert_to_tensor(best.xyxy),</span><br><span class="line">                    tf.squeeze(tf.convert_to_tensor(bb_xyxy), axis=<span class="number">1</span>))[<span class="number">0</span>] <span class="comment"># shape: (len(sorted_bb_info_list), )</span></span><br><span class="line">        n = len(sorted_bb_info_list)</span><br><span class="line">        sorted_bb_info_list = [</span><br><span class="line">                    sorted_bb_info_list[i] <span class="keyword">for</span> i <span class="keyword">in</span> </span><br><span class="line">                    range(n) <span class="keyword">if</span> iou[i] &lt;= nms_threshold]</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MultiBoxDetection</span><span class="params">(cls_prob, loc_pred, anchor, nms_threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        cls_prob: 经过softmax后得到的各个锚框的预测概率, shape:(bn, 预测总类别数+1, 锚框个数)</span></span><br><span class="line"><span class="string">        loc_pred: 预测的各个锚框的偏移量, shape:(bn, 锚框个数*4)</span></span><br><span class="line"><span class="string">        anchor: MultiBoxPrior输出的默认锚框, shape: (1, 锚框个数, 4)</span></span><br><span class="line"><span class="string">        nms_threshold: 非极大抑制中的阈值</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        所有锚框的信息, shape: (bn, 锚框个数, 6)</span></span><br><span class="line"><span class="string">        每个锚框信息由[class_id, confidence, xmin, ymin, xmax, ymax]表示</span></span><br><span class="line"><span class="string">        class_id=-1 表示背景或在非极大值抑制中被移除了</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">assert</span> len(cls_prob.shape) == <span class="number">3</span> <span class="keyword">and</span> len(loc_pred.shape) == <span class="number">2</span> <span class="keyword">and</span> len(anchor.shape) == <span class="number">3</span></span><br><span class="line">    bn = cls_prob.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">MultiBoxDetection_one</span><span class="params">(c_p, l_p, anc, nms_threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        MultiBoxDetection的辅助函数, 处理batch中的一个</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            c_p: (预测总类别数+1, 锚框个数)</span></span><br><span class="line"><span class="string">            l_p: (锚框个数*4, )</span></span><br><span class="line"><span class="string">            anc: (锚框个数, 4)</span></span><br><span class="line"><span class="string">            nms_threshold: 非极大抑制中的阈值</span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            output: (锚框个数, 6)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        pred_bb_num = c_p.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 加上偏移量</span></span><br><span class="line">        anc = tf.add(anc, tf.reshape(l_p, (pred_bb_num, <span class="number">4</span>))).numpy()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最大的概率</span></span><br><span class="line">        confidence = tf.reduce_max(c_p, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 最大概率对应的id</span></span><br><span class="line">        class_id = tf.argmax(c_p, axis=<span class="number">0</span>)</span><br><span class="line">        confidence = confidence.numpy()</span><br><span class="line">        class_id = class_id.numpy()</span><br><span class="line"></span><br><span class="line">        pred_bb_info = [Pred_BB_Info(index=i,</span><br><span class="line">                    class_id=class_id[i]<span class="number">-1</span>,</span><br><span class="line">                    confidence=confidence[i],</span><br><span class="line">                    xyxy=[anc[i]]) <span class="comment"># xyxy是个列表</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(pred_bb_num)]</span><br><span class="line">        <span class="comment"># 正类的index</span></span><br><span class="line">        obj_bb_idx = [bb.index <span class="keyword">for</span> bb </span><br><span class="line">                <span class="keyword">in</span> non_max_suppression(pred_bb_info,</span><br><span class="line">                            nms_threshold)]</span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> bb <span class="keyword">in</span> pred_bb_info:</span><br><span class="line">            output.append(np.append([</span><br><span class="line">                (bb.class_id <span class="keyword">if</span> bb.index <span class="keyword">in</span> obj_bb_idx </span><br><span class="line">                        <span class="keyword">else</span> <span class="number">-1.0</span>),</span><br><span class="line">                bb.confidence],</span><br><span class="line">                bb.xyxy))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> tf.convert_to_tensor(output) <span class="comment"># shape: (锚框个数， 6)</span></span><br><span class="line">    </span><br><span class="line">    batch_output = []</span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> range(bn):</span><br><span class="line">        batch_output.append(MultiBoxDetection_one(cls_prob[b],</span><br><span class="line">                        loc_pred[b], anchor[<span class="number">0</span>],</span><br><span class="line">                        nms_threshold))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tf.convert_to_tensor(batch_output)</span><br></pre></td></tr></table></figure>
<p>然后运行MultiBoxDetection函数并设阈值为0.5。这里为输入都增加了样本维。看到返回的结果的形状为(批量大小, 锚框个数, 6)。其中每一行的6个元素代表同一个预测边界框的输出信息。第一个元素是索引从0开始计数的预测类别（0为狗，1为猫），其中-1表示背景或在非极大值抑制中被移除。第二个元素是预测边界框的置信度。剩余的4个元素分别是预测边界框左上角的xx和yy轴坐标以及右下角的xx和yy轴坐标（值域在0到1之间）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output = MultiBoxDetection(</span><br><span class="line">    tf.expand_dims(cls_probs, <span class="number">0</span>),</span><br><span class="line">    tf.expand_dims(offset_preds, <span class="number">0</span>),</span><br><span class="line">    tf.expand_dims(anchors, <span class="number">0</span>),</span><br><span class="line">    nms_threshold=<span class="number">0.5</span>)</span><br><span class="line">output</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">1</span>, <span class="number">4</span>, <span class="number">6</span>), dtype=float64, numpy=</span><br><span class="line">array([[[ <span class="number">0.</span>        ,  <span class="number">0.89999998</span>,  <span class="number">0.1</span>       ,  <span class="number">0.08</span>      ,</span><br><span class="line">          <span class="number">0.51999998</span>,  <span class="number">0.92000002</span>],</span><br><span class="line">        [<span class="number">-1.</span>        ,  <span class="number">0.80000001</span>,  <span class="number">0.08</span>      ,  <span class="number">0.2</span>       ,</span><br><span class="line">          <span class="number">0.56</span>      ,  <span class="number">0.94999999</span>],</span><br><span class="line">        [<span class="number">-1.</span>        ,  <span class="number">0.69999999</span>,  <span class="number">0.</span>        ,  <span class="number">0.30000001</span>,</span><br><span class="line">          <span class="number">0.62</span>      ,  <span class="number">0.91000003</span>],</span><br><span class="line">        [ <span class="number">1.</span>        ,  <span class="number">0.89999998</span>,  <span class="number">0.55000001</span>,  <span class="number">0.2</span>       ,</span><br><span class="line">          <span class="number">0.89999998</span>,  <span class="number">0.88</span>      ]]])&gt;</span><br></pre></td></tr></table></figure>
<p>移除掉类别为-1的预测边界框，并可视化非极大值抑制保留的结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig = plt.imshow(img)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> output[<span class="number">0</span>].numpy():</span><br><span class="line">    <span class="keyword">if</span> i[<span class="number">0</span>] == <span class="number">-1</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    label = (<span class="string">'dog='</span>, <span class="string">'cat='</span>)[int(i[<span class="number">0</span>])] + str(i[<span class="number">1</span>])</span><br><span class="line">    show_bboxes(fig.axes, tf.multiply(i[<span class="number">2</span>:], bbox_scale), label)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_63_0.svg" alt></p>
<p>实践中，可以在执行非极大值抑制前将置信度较低的预测边界框移除，从而减小非极大值抑制的计算量。还可以筛选非极大值抑制的输出，例如，只保留其中置信度较高的结果作为最终输出。</p>
<h1 id="duo-chi-du-mu-biao-jian-ce">多尺度目标检测</h1>
<p>在实验中以输入图像的每个像素为中心生成多个锚框,这些锚框是对输入图像不同区域的采样。然而，如果以图像每个像素为中心都生成锚框，很容易生成过多锚框而造成计算量过大。举个例子，假设输入图像的高和宽分别为561像素和728像素，如果以每个像素为中心生成5个不同形状的锚框，那么一张图像上则需要标注并预测200多万个锚框（\(561 \times 728 \times 5\)）。</p>
<p>减少锚框个数并不难。一种简单的方法是在输入图像中均匀采样一小部分像素，并以采样的像素为中心生成锚框。此外，在不同尺度下，可以生成不同数量和不同大小的锚框。值得注意的是，较小目标比较大目标在图像上出现位置的可能性更多。举个简单的例子：形状为\(1 \times 1\)、\(1 \times 2\)和\(2 \times 2\)的目标在形状为\(2 \times 2\)的图像上可能出现的位置分别有4、2和1种。因此，当使用较小锚框来检测较小目标时，可以采样较多的区域；而当使用较大锚框来检测较大目标时，我们可以采样较少的区域。可以通过定义特征图的形状来确定任一图像上均匀采样的锚框中心。</p>
<p>下面定义<code>display_anchors</code>函数。在特征图<code>fmap</code>上以每个单元（像素）为中心生成锚框<code>anchors</code>。由于锚框<code>anchors</code>中\(x\)和\(y\)轴的坐标值分别已除以特征图<code>fmap</code>的宽和高，这些值域在0和1之间的值表达了锚框在特征图中的相对位置。由于锚框<code>anchors</code>的中心遍布特征图<code>fmap</code>上的所有单元，<code>anchors</code>的中心在任一图像的空间相对位置一定是均匀分布的。具体来说，当特征图的宽和高分别设为<code>fmap_w</code>和<code>fmap_h</code>时，该函数将在任一图像上均匀采样<code>fmap_h</code>行<code>fmap_w</code>列个像素，并分别以它们为中心生成大小为<code>s</code>（假设列表<code>s</code>长度为1）的不同宽高比（<code>ratios</code>）的锚框。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_anchors</span><span class="params">(fmap_w, fmap_h, s)</span>:</span></span><br><span class="line">    fmap = np.zeros((<span class="number">1</span>, <span class="number">10</span>, fmap_w, fmap_h))  <span class="comment"># 前两维的取值不影响输出结果</span></span><br><span class="line">    anchors = MultiBoxPrior(fmap, sizes=s, ratios=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>])</span><br><span class="line">    bbox_scale = np.array((w, h, w, h))</span><br><span class="line">    show_bboxes(plt.imshow(img).axes,</span><br><span class="line">                    anchors[<span class="number">0</span>] * bbox_scale)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MultiBoxPrior</span><span class="params">(feature_map, sizes=[<span class="number">0.75</span>, <span class="number">0.5</span>, <span class="number">0.25</span>], ratios=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>])</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        feature_map: torch tensor, Shape: [N, C, H, W].</span></span><br><span class="line"><span class="string">        sizes: List of sizes (0~1) of generated MultiBoxPriores. </span></span><br><span class="line"><span class="string">        ratios: List of aspect ratios (non-negative) of generated MultiBoxPriores. </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        anchors of shape (1, num_anchors, 4).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    pairs = [] <span class="comment"># pair of (size, sqrt(ratio))</span></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> ratios:</span><br><span class="line">        pairs.append([sizes[<span class="number">0</span>], np.sqrt(r)])</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> sizes[<span class="number">1</span>:]:</span><br><span class="line">        pairs.append([s, np.sqrt(ratios[<span class="number">0</span>])])</span><br><span class="line">    </span><br><span class="line">    pairs = np.array(pairs)</span><br><span class="line"></span><br><span class="line">    ss1 = pairs[:, <span class="number">0</span>] * pairs[:, <span class="number">1</span>] <span class="comment"># size * sqrt(ration)</span></span><br><span class="line">    ss2 = pairs[:, <span class="number">0</span>] / pairs[:, <span class="number">1</span>] <span class="comment"># size / sqrt(retion)</span></span><br><span class="line"></span><br><span class="line">    base_anchors = tf.stack([-ss1, -ss2, ss1, ss2], axis=<span class="number">1</span>) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    h, w = feature_map.shape[<span class="number">-2</span>:]</span><br><span class="line">    shifts_x = tf.divide(tf.range(<span class="number">0</span>, w), w)</span><br><span class="line">    shifts_y = tf.divide(tf.range(<span class="number">0</span>, h), h)</span><br><span class="line">    shift_x, shift_y = tf.meshgrid(shifts_x, shifts_y)</span><br><span class="line">    shift_x = tf.reshape(shift_x, (<span class="number">-1</span>,))</span><br><span class="line">    shift_y = tf.reshape(shift_y, (<span class="number">-1</span>,))</span><br><span class="line">    shifts = tf.stack((shift_x, shift_y, shift_x, shift_y), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    anchors = tf.add(tf.reshape(shifts, (<span class="number">-1</span>,<span class="number">1</span>,<span class="number">4</span>)), tf.reshape(base_anchors, (<span class="number">1</span>,<span class="number">-1</span>,<span class="number">4</span>)))</span><br><span class="line">    <span class="keyword">return</span> tf.cast(tf.reshape(anchors, (<span class="number">1</span>,<span class="number">-1</span>,<span class="number">4</span>)), tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_bboxes</span><span class="params">(axes, bboxes, labels=None, colors=None)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_list</span><span class="params">(obj, default_values=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> obj <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            obj = default_values</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> isinstance(obj, (list, tuple)):</span><br><span class="line">            obj = [obj]</span><br><span class="line">        <span class="keyword">return</span> obj</span><br><span class="line">    </span><br><span class="line">    labels = _make_list(labels)</span><br><span class="line">    colors = _make_list(colors, [<span class="string">'b'</span>, <span class="string">'g'</span>, <span class="string">'r'</span>, <span class="string">'m'</span>, <span class="string">'c'</span>])</span><br><span class="line">    <span class="keyword">for</span> i, bbox <span class="keyword">in</span> enumerate(bboxes):</span><br><span class="line">        color = colors[i % len(colors)]</span><br><span class="line">        rect = bbox_to_rect(bbox.numpy(), color)</span><br><span class="line">        axes.add_patch(rect)</span><br><span class="line">        <span class="keyword">if</span> labels <span class="keyword">and</span> len(labels) &gt; i:</span><br><span class="line">            text_color = <span class="string">'k'</span> <span class="keyword">if</span> color == <span class="string">'w'</span> <span class="keyword">else</span> <span class="string">'w'</span></span><br><span class="line">            axes.text(rect.xy[<span class="number">0</span>], rect.xy[<span class="number">1</span>], labels[i],</span><br><span class="line">                va=<span class="string">'center'</span>, ha=<span class="string">'center'</span>, fontsize=<span class="number">6</span>,</span><br><span class="line">                color=text_color, bbox=dict(facecolor=color, lw=<span class="number">0</span>))</span><br><span class="line">            </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox_to_rect</span><span class="params">(bbox, color)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> plt.Rectangle(</span><br><span class="line">        xy=(bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), width=bbox[<span class="number">2</span>]-bbox[<span class="number">0</span>], height=bbox[<span class="number">3</span>]-bbox[<span class="number">1</span>],</span><br><span class="line">        fill=<span class="literal">False</span>, edgecolor=color, linewidth=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">x = tf.zeros((<span class="number">1</span>,<span class="number">3</span>,h,w))</span><br><span class="line">y = MultiBoxPrior(x)</span><br><span class="line">y.shape <span class="comment"># TensorShape([1, 405720, 4])</span></span><br></pre></td></tr></table></figure>
<p>先关注小目标的检测。为了在显示时更容易分辨，这里令不同中心的锚框不重合：设锚框大小为0.15，特征图的高和宽分别为4。可以看出，图像上4行4列的锚框中心分布均匀。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">display_anchors(fmap_w=<span class="number">4</span>, fmap_h=<span class="number">4</span>, s=[<span class="number">0.15</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_5_0.png" alt></p>
<p>将特征图的高和宽分别减半，并用更大的锚框检测更大的目标。当锚框大小设0.4时，有些锚框的区域有重合。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">display_anchors(fmap_w=<span class="number">2</span>, fmap_h=<span class="number">2</span>, s=[<span class="number">0.4</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_7_0.png" alt></p>
<p>最后，将特征图的高和宽进一步减半至1，并将锚框大小增至0.8。此时锚框中心即图像中心。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">display_anchors(fmap_w=<span class="number">1</span>, fmap_h=<span class="number">1</span>, s=[<span class="number">0.8</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_9_0-5863308.png" alt></p>
<p>既然已在多个尺度上生成了不同大小的锚框，相应地，需要在不同尺度下检测不同大小的目标。</p>
<p>下面介绍一种基于卷积神经网络的方法。</p>
<p>在某个尺度下，假设依据\(c_i\)张形状为\(h \times w\)的特征图生成\(h \times w\)组不同中心的锚框，且每组的锚框个数为\(a\)。例如，在刚才实验的第一个尺度下，依据10（通道数）张形状为\(4 \times 4\)的特征图生成了16组不同中心的锚框，且每组含3个锚框。<br>
接下来，依据真实边界框的类别和位置，每个锚框将被标注类别和偏移量。在当前的尺度下，目标检测模型需要根据输入图像预测\(h \times w\)组不同中心的锚框的类别和偏移量。</p>
<p>假设这里的\(c_i\)张特征图为卷积神经网络根据输入图像做前向计算所得的中间输出。既然每张特征图上都有\(h \times w\)个不同的空间位置，那么相同空间位置可以看作含有\(c_i\)个单元。<br>
根据二维卷积层中感受野的定义，特征图在相同空间位置的\(c_i\)个单元在输入图像上的感受野相同，并表征了同一感受野内的输入图像信息。<br>
因此，可以将特征图在相同空间位置的\(c_i\)个单元变换为以该位置为中心生成的\(a\)个锚框的类别和偏移量。<br>
本质上是用输入图像在某个感受野区域内的信息来预测输入图像上与该区域位置相近的锚框的类别和偏移量。</p>
<p>当不同层的特征图在输入图像上分别拥有不同大小的感受野时，它们将分别用来检测不同大小的目标。例如，可以通过设计网络，令较接近输出层的特征图中每个单元拥有更广阔的感受野，从而检测输入图像中更大尺寸的目标。</p>
<h1 id="r-cnn">R-CNN</h1>
<p>区域卷积神经网络（R-CNN）是将深度模型应用于目标检测的开创性工作之一 。下面介绍R-CNN和它的一系列改进方法：快速的R-CNN（Fast R-CNN）、更快的R-CNN（Faster R-CNN） 以及掩码R-CNN（Mask R-CNN）。</p>
<h2 id="r-cnn-1">R-CNN</h2>
<p>R-CNN首先对图像选取若干提议区域（如锚框也是一种选取方法）并标注它们的类别和边界框（如偏移量）。然后，用卷积神经网络对每个提议区域做前向计算抽取特征。之后，我们用每个提议区域的特征预测类别和边界框。下图描述了R-CNN模型。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/r-cnn.svg" alt></p>
<p>具体来说，R-CNN主要由以下4步构成。</p>
<ol>
<li>对输入图像使用选择性搜索来选取多个高质量的提议区域。这些提议区域通常是在多个尺度下选取的，并具有不同的形状和大小。每个提议区域将被标注类别和真实边界框。</li>
<li>选取一个预训练的卷积神经网络，并将其在输出层之前截断。将每个提议区域变形为网络需要的输入尺寸，并通过前向计算输出抽取的提议区域特征。</li>
<li>将每个提议区域的特征连同其标注的类别作为一个样本，训练多个支持向量机对目标分类。其中每个支持向量机用来判断样本是否属于某一个类别。</li>
<li>将每个提议区域的特征连同其标注的边界框作为一个样本，训练线性回归模型来预测真实边界框。</li>
</ol>
<p>R-CNN虽然通过预训练的卷积神经网络有效抽取了图像特征，但它的主要缺点是速度慢。想象一下，我们可能从一张图像中选出上千个提议区域，对该图像做目标检测将导致上千次的卷积神经网络的前向计算。这个巨大的计算量令R-CNN难以在实际应用中被广泛采用。</p>
<h2 id="fast-r-cnn">Fast R-CNN</h2>
<p>R-CNN的主要性能瓶颈在于需要对每个提议区域独立抽取特征。由于这些区域通常有大量重叠，独立的特征抽取会导致大量的重复计算。Fast R-CNN对R-CNN的一个主要改进在于只对整个图像做卷积神经网络的前向计算。</p>
<p>下图描述了Fast R-CNN模型。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/fast-rcnn.svg" alt></p>
<p>它的主要计算步骤如下。</p>
<ol>
<li>与R-CNN相比，Fast R-CNN用来提取特征的卷积神经网络的输入是整个图像，而不是各个提议区域。而且，这个网络通常会参与训练，即更新模型参数。设输入为一张图像，将卷积神经网络的输出的形状记为\(1×c×h_1×w_1\)。</li>
<li>假设选择性搜索生成n个提议区域。这些形状各异的提议区域在卷积神经网络的输出上分别标出形状各异的兴趣区域。这些兴趣区域需要抽取出形状相同的特征（假设高和宽均分别指定为h2和w2）以便于连结后输出。Fast R-CNN引入兴趣区域池化（region of interest pooling，RoI池化）层，将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征，形状为\(n×c×h_2×w_2\)。</li>
<li>通过全连接层将输出形状变换为\(n×d\)，其中超参数d取决于模型设计。</li>
<li>预测类别时，将全连接层的输出的形状再变换为\(n×q\)并使用softmax回归（q为类别个数）。预测边界框时，将全连接层的输出的形状变换为\(n×4\)。也就是说，我们为每个提议区域预测类别和边界框。</li>
</ol>
<p>Fast R-CNN中提出的兴趣区域池化层与池化层有所不同。在池化层中，通过设置池化窗口、填充和步幅来控制输出形状。而兴趣区域池化层对每个区域的输出形状是可以直接指定的，例如，指定每个区域输出的高和宽分别为\(h_2\)和\(w_2\)。假设某一兴趣区域窗口的高和宽分别为h和w，该窗口将被划分为形状为\(h_2×w_2\)的子窗口网格，且每个子窗口的大小大约为\((\frac{h}{h_2})×(\frac{w}{w_2})\)。任一子窗口的高和宽要取整，其中的最大元素作为该子窗口的输出。因此，兴趣区域池化层可从形状各异的兴趣区域中均抽取出形状相同的特征。</p>
<p>上图中，在4×4的输入上选取了左上角的3×3区域作为兴趣区域。对于该兴趣区域，通过2×2兴趣区域池化层得到一个2×2的输出。4个划分后的子窗口分别含有元素0、1、4、5（5最大），2、6（6最大），8、9（9最大），10。</p>
<p>使用ROIPooling函数来演示兴趣区域池化层的计算。假设卷积神经网络抽取的特征X的高和宽均为4且只有单通道。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = tf.convert_to_tensor(np.arange(<span class="number">16</span>).reshape((<span class="number">1</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>)))</span><br><span class="line">x</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>), dtype=int64, numpy=</span><br><span class="line">array([[[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">         [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">         [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">         [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]]]])&gt;</span><br></pre></td></tr></table></figure>
<p>假设图像的高和宽均为40像素。再假设选择性搜索在图像上生成了两个提议区域：每个区域由5个元素表示，分别为区域目标类别、左上角的x和y轴坐标以及右下角的x和y轴坐标。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rois = tf.convert_to_tensor(np.array([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">20</span>,<span class="number">20</span>], [<span class="number">0</span>,<span class="number">0</span>,<span class="number">10</span>,<span class="number">30</span>,<span class="number">30</span>]]))</span><br><span class="line">rois</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">5</span>), dtype=int64, numpy=</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>, <span class="number">20</span>, <span class="number">20</span>],</span><br><span class="line">       [ <span class="number">0</span>,  <span class="number">0</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">30</span>]])&gt;</span><br></pre></td></tr></table></figure>
<p>由于X的高和宽是图像的高和宽的1/10，以上两个提议区域中的坐标先按spatial_scale自乘0.1，然后在X上分别标出兴趣区域X[:,:,0:3,0:3]和X[:,:,1:4,0:4]。最后对这两个兴趣区域分别划分子窗口网格并抽取高和宽为2的特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">roi_pool</span><span class="params">(x, rois, output_size, spatial_scale)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Shape Of x:</span></span><br><span class="line"><span class="string">        [batch_size, h, w, c]</span></span><br><span class="line"><span class="string">    Shape Of rois:</span></span><br><span class="line"><span class="string">        [num_rois, 4]</span></span><br><span class="line"><span class="string">    Shape Of output_size:</span></span><br><span class="line"><span class="string">        [2,]    #(h, w)</span></span><br><span class="line"><span class="string">    Type Of spatial_scale:</span></span><br><span class="line"><span class="string">        0-1</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.shape) == <span class="number">4</span> <span class="keyword">and</span> len(rois.shape) == <span class="number">2</span> <span class="keyword">and</span> len(output_size) == <span class="number">2</span></span><br><span class="line">    <span class="comment"># feature_map_height = int(x.shape[1])</span></span><br><span class="line">    <span class="comment"># feature_map_width = int(x.shape[2])</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_curried_pool_rois</span><span class="params">(x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> _pool_rois(x, rois)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_pool_rois</span><span class="params">(feature_map, rois)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Applies ROI pooling for a single image and varios ROIs</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">curried_pool_roi</span><span class="params">(roi)</span>:</span></span><br><span class="line">            <span class="comment"># print(feature_map.shape)</span></span><br><span class="line">            <span class="keyword">return</span> _pool_roi(feature_map, roi)</span><br><span class="line">        pooled_areas = tf.map_fn(curried_pool_roi, rois, dtype=tf.float32)</span><br><span class="line">        <span class="keyword">return</span> pooled_areas</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_pool_roi</span><span class="params">(x, roi)</span>:</span></span><br><span class="line">        <span class="comment"># Shape Of x is [h, w, c]</span></span><br><span class="line">        h_start = tf.cast(roi[<span class="number">1</span>] * spatial_scale, dtype=tf.int32)</span><br><span class="line">        w_start = tf.cast(roi[<span class="number">2</span>] * spatial_scale, dtype=tf.int32)</span><br><span class="line">        h_end = tf.cast(roi[<span class="number">3</span>] * spatial_scale, dtype=tf.int32)</span><br><span class="line">        w_end = tf.cast(roi[<span class="number">4</span>] * spatial_scale, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(x.shape, roi.shape)</span></span><br><span class="line">        region = x[h_start:h_end+<span class="number">1</span>, w_start:w_end+<span class="number">1</span>, :]</span><br><span class="line"></span><br><span class="line">        region_height = h_end - h_start + <span class="number">1</span></span><br><span class="line">        region_width = w_end - w_start + <span class="number">1</span></span><br><span class="line">        h_step = tf.cast((region_height+output_size[<span class="number">0</span>]<span class="number">-1</span>)/output_size[<span class="number">0</span>], dtype=tf.int32)</span><br><span class="line">        w_step = tf.cast((region_height+output_size[<span class="number">1</span>]<span class="number">-1</span>)/output_size[<span class="number">1</span>], dtype=tf.int32)</span><br><span class="line">        <span class="comment"># print(region_height, output_size[0])</span></span><br><span class="line"></span><br><span class="line">        areas = [[(</span><br><span class="line">            i * h_step,</span><br><span class="line">            j * w_step,</span><br><span class="line">            (i+<span class="number">1</span>) * h_step <span class="keyword">if</span> i+<span class="number">1</span> &lt; output_size[<span class="number">0</span>] <span class="keyword">else</span> region_height,</span><br><span class="line">            (j+<span class="number">1</span>) * w_step <span class="keyword">if</span> j+<span class="number">1</span> &lt; output_size[<span class="number">1</span>] <span class="keyword">else</span> region_width</span><br><span class="line">            ) </span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(output_size[<span class="number">1</span>])]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(output_size[<span class="number">0</span>])]</span><br><span class="line">        <span class="comment"># take the maximum of each area and stack the result</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">pool_area</span><span class="params">(x)</span>:</span></span><br><span class="line">          <span class="keyword">return</span> tf.math.reduce_max(region[x[<span class="number">0</span>]:x[<span class="number">2</span>], x[<span class="number">1</span>]:x[<span class="number">3</span>], :], axis=[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">        </span><br><span class="line">        pooled_features = tf.stack([[pool_area(item) <span class="keyword">for</span> item <span class="keyword">in</span> row] <span class="keyword">for</span> row <span class="keyword">in</span> areas])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tf.cast(pooled_features, dtype=tf.float32)</span><br><span class="line">    pooled_areas = tf.map_fn(_curried_pool_rois, x, dtype=tf.float32)</span><br><span class="line">    <span class="keyword">return</span> pooled_areas</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = tf.convert_to_tensor(np.arange(<span class="number">16</span>).reshape(<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>), dtype=tf.float32)</span><br><span class="line">rois = tf.convert_to_tensor(np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">20</span>, <span class="number">20</span>], [<span class="number">0</span>, <span class="number">10</span>, <span class="number">0</span>, <span class="number">30</span>, <span class="number">30</span>]]), dtype=tf.float32)</span><br><span class="line">output_size=(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">roi_pool(x, rois, output_size=output_size, spatial_scale=<span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>), dtype=float32, numpy=</span><br><span class="line">array([[[[[ <span class="number">5.</span>],</span><br><span class="line">          [ <span class="number">6.</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">9.</span>],</span><br><span class="line">          [<span class="number">10.</span>]]],</span><br><span class="line">         [[[ <span class="number">9.</span>],</span><br><span class="line">          [<span class="number">11.</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">13.</span>],</span><br><span class="line">          [<span class="number">15.</span>]]]]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="faster-r-cnn">Faster R-CNN</h2>
<p>Fast R-CNN通常需要在选择性搜索中生成较多的提议区域，以获得较精确的目标检测结果。Faster R-CNN提出将选择性搜索替换成区域提议网络（region proposal network），从而减少提议区域的生成数量，并保证目标检测的精度。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/9.8_faster-rcnn.svg" alt="Faster R-CNN模型"></p>
<p>与Fast R-CNN相比，只有生成提议区域的方法从选择性搜索变成了区域提议网络，而其他部分均保持不变。具体来说，区域提议网络的计算步骤如下。</p>
<ol>
<li>使用填充为1的3×33×3卷积层变换卷积神经网络的输出，并将输出通道数记为cc。这样，卷积神经网络为图像抽取的特征图中的每个单元均得到一个长度为cc的新特征。</li>
<li>以特征图每个单元为中心，生成多个不同大小和宽高比的锚框并标注它们。</li>
<li>用锚框中心单元长度为cc的特征分别预测该锚框的二元类别（含目标还是背景）和边界框。</li>
<li>使用非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测边界框即兴趣区域池化层所需要的提议区域。</li>
</ol>
<p>值得一提的是，区域提议网络作为Faster R-CNN的一部分，是和整个模型一起训练得到的。也就是说，Faster R-CNN的目标函数既包括目标检测中的类别和边界框预测，又包括区域提议网络中锚框的二元类别和边界框预测。最终，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少提议区域数量的情况下也能保证目标检测的精度。</p>
<h2 id="mask-r-cnn">Mask R-CNN</h2>
<p>如果训练数据还标注了每个目标在图像上的像素级位置，那么Mask R-CNN能有效利用这些详尽的标注信息进一步提升目标检测的精度。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/mask-rcnn.svg" alt="Mask R-CNN模型"></p>
<p>如上图所示，Mask R-CNN在Faster R-CNN的基础上做了修改。Mask R-CNN将兴趣区域池化层替换成了兴趣区域对齐层，即通过双线性插值（bilinear interpolation）来保留特征图上的空间信息，从而更适于像素级预测。兴趣区域对齐层的输出包含了所有兴趣区域的形状相同的特征图。它们既用来预测兴趣区域的类别和边界框，又通过额外的全卷积网络预测目标的像素级位置。我们将在9.10节（全卷积网络）介绍如何使用全卷积网络预测图像中像素级的语义。</p>
<h1 id="yu-yi-fen-ge-he-shu-ju-ji">语义分割和数据集</h1>
<p>语义分割问题关注如何将图像分割成属于不同语义类别的区域。值得一提的是，这些语义区域的标注和预测都是像素级的。下图展示了语义分割中图像有关狗、猫和背景的标签。可以看到，与目标检测相比，语义分割标注的像素级的边框显然更加精细。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/9.9_segmentation.svg" alt></p>
<p>语义分割中图像有关狗、猫和背景的标签</p>
<h2 id="tu-xiang-fen-ge-he-shi-li-fen-ge">图像分割和实例分割</h2>
<p>计算机视觉领域还有2个与语义分割相似的重要问题，即图像分割和实例分割。在这里将它们与语义分割简单区分一下。</p>
<ul>
<li>图像分割将图像分割成若干组成区域。这类问题的方法通常利用图像中像素之间的相关性。它在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义。以图9.10的图像为输入，图像分割可能将狗分割成两个区域：一个覆盖以黑色为主的嘴巴和眼睛，而另一个覆盖以黄色为主的其余部分身体。</li>
<li>实例分割又叫同时检测并分割。它研究如何识别图像中各个目标实例的像素级区域。与语义分割有所不同，实例分割不仅需要区分语义，还要区分不同的目标实例。如果图像中有两只狗，实例分割需要区分像素属于这两只狗中的哪一只。</li>
</ul>
<h2 id="yu-chu-li-shu-ju">预处理数据</h2>
<p>通常图像处理包括缩放图像使其符合模型的输入形状。然而在语义分割里，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像。这样的映射难以做到精确，尤其在不同语义的分割区域。为了避免这个问题，将图像裁剪成固定尺寸而不是缩放。具体来说，我们使用图像增广里的随机裁剪，并对输入图像和标签裁剪相同区域。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">voc_rand_crop</span><span class="params">(feature, label, height, width)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Random crop feature (tf image) and label (tf image).</span></span><br><span class="line"><span class="string">    先将channel合并，剪裁之后再分开</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    combined = tf.concat([feature, label], axis=<span class="number">2</span>)</span><br><span class="line">    last_label_dim = tf.shape(label)[<span class="number">-1</span>]</span><br><span class="line">    last_feature_dim = tf.shape(feature)[<span class="number">-1</span>]</span><br><span class="line">    combined_crop = tf.image.random_crop(combined,</span><br><span class="line">                        size=tf.concat([(height, width), [last_label_dim + last_feature_dim]],axis=<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">return</span> combined_crop[:, :, :last_feature_dim], combined_crop[:, :, last_feature_dim:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">imgs = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(n):</span><br><span class="line">    imgs += voc_rand_crop(train_features[<span class="number">0</span>], train_labels[<span class="number">0</span>], <span class="number">200</span>, <span class="number">300</span>)</span><br><span class="line"></span><br><span class="line">show_images(imgs[::<span class="number">2</span>] + imgs[<span class="number">1</span>::<span class="number">2</span>], <span class="number">2</span>, n)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_31_1.png" alt></p>
<h1 id="quan-juan-ji-wang-luo-fcn">全卷积网络（FCN）</h1>
<p>全卷积网络（FCN）采用卷积神经网络实现了从图像像素到像素类别的变换 [1]。全卷积网络通过转置卷积层将中间层特征图的高和宽变换回输入图像的尺寸，从而令预测结果与输入图像在空间维（高和宽）上一一对应：给定空间维上的位置，通道维的输出即该位置对应像素的类别预测。</p>
<h2 id="zhuan-zhi-juan-ji-ceng">转置卷积层</h2>
<p>顾名思义，转置卷积层得名于矩阵的转置操作。事实上，卷积运算还可以通过矩阵乘法来实现。在下面的例子中，定义一个高和宽分别为4的输入<code>X</code>，以及高和宽分别为3的卷积核<code>K</code>。打印二维卷积运算的输出以及卷积核。可以看到，输出的高和宽分别为2。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = np.arange(<span class="number">1</span>, <span class="number">17</span>).reshape((<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">X=tf.convert_to_tensor(X, dtype=tf.float32)</span><br><span class="line">K = np.arange(<span class="number">1</span>, <span class="number">10</span>).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">K = initializers.Constant(K)</span><br><span class="line">conv = layers.Conv2D(filters=<span class="number">1</span>, kernel_size=<span class="number">3</span>, kernel_initializer=K)</span><br><span class="line">conv(X),np.shape(K.value)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">(&lt;tf.Tensor: shape=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>), dtype=float32, numpy=</span><br><span class="line"> array([[[[<span class="number">348.</span>],</span><br><span class="line">          [<span class="number">393.</span>]],</span><br><span class="line"> </span><br><span class="line">         [[<span class="number">528.</span>],</span><br><span class="line">          [<span class="number">573.</span>]]]], dtype=float32)&gt;, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<p>下面将卷积核<code>K</code>改写成含有大量零元素的稀疏矩阵<code>W</code>，即权重矩阵。权重矩阵的形状为(4, 16)，其中的非零元素来自卷积核<code>K</code>中的元素。将输入<code>X</code>逐行连结，得到长度为16的向量。然后将<code>W</code>与向量化的<code>X</code>做矩阵乘法，得到长度为4的向量。对其变形后，可以得到和上面卷积运算相同的结果。可见，在这个例子中使用矩阵乘法实现了卷积运算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W, k = np.zeros((<span class="number">4</span>, <span class="number">16</span>)), np.zeros(<span class="number">11</span>)</span><br><span class="line">k[:<span class="number">3</span>], k[<span class="number">4</span>:<span class="number">7</span>], k[<span class="number">8</span>:] = K.value[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, :], K.value[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, :], K.value[<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, :]</span><br><span class="line">W[<span class="number">0</span>, <span class="number">0</span>:<span class="number">11</span>], W[<span class="number">1</span>, <span class="number">1</span>:<span class="number">12</span>], W[<span class="number">2</span>, <span class="number">4</span>:<span class="number">15</span>], W[<span class="number">3</span>, <span class="number">5</span>:<span class="number">16</span>] = k, k, k, k</span><br><span class="line">tf.matmul(tf.convert_to_tensor(W, dtype=tf.float32), tf.reshape(X, (<span class="number">-1</span>, <span class="number">1</span>))), W</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">(&lt;tf.Tensor: shape=(<span class="number">4</span>, <span class="number">1</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">348.</span>],</span><br><span class="line">        [<span class="number">393.</span>],</span><br><span class="line">        [<span class="number">528.</span>],</span><br><span class="line">        [<span class="number">573.</span>]], dtype=float32)&gt;,</span><br><span class="line"> array([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">0.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">0.</span>, <span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">0.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">0.</span>, <span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">0.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">0.</span>, <span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">0.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">0.</span>, <span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>]]))</span><br></pre></td></tr></table></figure>
<p>现在从矩阵乘法的角度来描述卷积运算。设输入向量为\(\boldsymbol{x}\)，权重矩阵为\(\boldsymbol{W}\)，卷积的前向计算函数的实现可以看作将函数输入乘以权重矩阵，并输出向量\(\boldsymbol{y} = \boldsymbol{W}\boldsymbol{x}\)。反向传播需要依据链式法则。由于\(\nabla_{\boldsymbol{x}} \boldsymbol{y} = \boldsymbol{W}^\top\)，卷积的反向传播函数的实现可以看作将函数输入乘以转置后的权重矩阵\(\boldsymbol{W}^\top\)。而转置卷积层正好交换了卷积层的前向计算函数与反向传播函数：转置卷积层的这两个函数可以看作将函数输入向量分别乘以\(\boldsymbol{W}^\top\)和\(\boldsymbol{W}\)。</p>
<p>不难想象，转置卷积层可以用来交换卷积层输入和输出的形状。继续用矩阵乘法描述卷积。设权重矩阵是形状为\(4\times16\)的矩阵，对于长度为16的输入向量，卷积前向计算输出长度为4的向量。假如输入向量的长度为4，转置权重矩阵的形状为\(16\times4\)，那么转置卷积层将输出长度为16的向量。在模型设计中，转置卷积层常用于将较小的特征图变换为更大的特征图。在全卷积网络中，当输入是高和宽较小的特征图时，转置卷积层可以用来将高和宽放大到输入图像的尺寸。</p>
<p>来看一个例子。构造一个卷积层<code>conv</code>，并设输入<code>X</code>的形状为(1, 64, 64, 3)。卷积输出<code>Y</code>的通道数增加到10，但高和宽分别缩小了一半。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv = layers.Conv2D(<span class="number">10</span>, <span class="number">4</span>, padding=<span class="string">'same'</span>, strides=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">x = np.random.uniform(size=(<span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>))</span><br><span class="line">x = tf.convert_to_tensor(x, dtype=tf.float32)</span><br><span class="line">y = conv(x)</span><br><span class="line">y.shape <span class="comment"># TensorShape([1, 32, 32, 10])</span></span><br></pre></td></tr></table></figure>
<p>下面通过创建<code>Conv2DTranspose</code>实例来构造转置卷积层<code>conv_trans</code>。这里设<code>conv_trans</code>的卷积核形状、填充以及步幅与<code>conv</code>中的相同，并设输出通道数为3。当输入为卷积层<code>conv</code>的输出<code>Y</code>时，转置卷积层输出与卷积层输入的高和宽相同：转置卷积层将特征图的高和宽分别放大了2倍。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv_trans = layers.Convolution2DTranspose(filters=<span class="number">3</span>,</span><br><span class="line">                          kernel_size=<span class="number">4</span>,</span><br><span class="line">                          padding=<span class="string">'same'</span>,</span><br><span class="line">                          strides=<span class="number">2</span>)</span><br><span class="line">conv_trans(y).shape <span class="comment"># TensorShape([1, 64, 64, 3])</span></span><br></pre></td></tr></table></figure>
<p>在有些文献中，转置卷积也被称为分数步长卷积[2]。</p>
<h2 id="gou-zao-mo-xing">构造模型</h2>
<p>这里给出全卷积网络模型最基本的设计。如下图所示，全卷积网络先使用卷积神经网络抽取图像特征，然后通过\(1\times 1\)卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。模型输出与输入图像的高和宽相同，并在空间位置上一一对应：最终输出的通道包含了该空间位置像素的类别预测。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/fcn.svg" alt="全卷积网络"></p>
<p>下面使用一个基于ImageNet数据集预训练的ResNet-101模型来抽取图像特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pretrained_net = applications.ResNet101V2(include_top=<span class="literal">False</span>, weights=<span class="string">'imagenet'</span>, input_shape=(<span class="number">320</span>, <span class="number">480</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建全卷积网络实例net，它复制了pretrained_net实例成员变量features里除去最后两层的所有层以及预训练得到的模型参数。</span></span><br><span class="line">net = keras.Sequential()</span><br><span class="line">net.add(pretrained_net)</span><br></pre></td></tr></table></figure>
<p>给定高和宽分别为320和480的输入，net的前向计算将输入的高和宽减小至原来的 1/32 ，即10和15。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.random.uniform(size=(<span class="number">1</span>, <span class="number">320</span>, <span class="number">480</span>, <span class="number">3</span>))</span><br><span class="line">x = tf.convert_to_tensor(x, dtype=tf.float32)</span><br><span class="line">net(x).shape <span class="comment"># TensorShape([1, 10, 15, 2048])</span></span><br></pre></td></tr></table></figure>
<p>接下来，通过\(1\times 1\)卷积层将输出通道数变换为Pascal VOC2012数据集的类别个数21。最后，将特征图的高和宽放大32倍，从而变回输入图像的高和宽。由于\((320-64+16\times2+32)/32=10\)且\((480-64+16\times2+32)/32=15\)，构造一个步幅为32的转置卷积层，并将卷积核的高和宽设为64、填充设为16。不难发现，如果步幅为\(s\)、填充为\(s/2\)（假设\(s/2\)为整数）、卷积核的高和宽为\(2s\)，转置卷积核将输入的高和宽分别放大\(s\)倍。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_classes = <span class="number">21</span></span><br><span class="line">net.add(keras.layers.Conv2D(num_classes, kernel_size=<span class="number">1</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">print(net(x).shape) <span class="comment"># (1, 10, 15, 21)</span></span><br><span class="line">net.add(keras.layers.Conv2DTranspose(num_classes,</span><br><span class="line">                kernel_size=<span class="number">64</span>,</span><br><span class="line">                padding=<span class="string">'same'</span>,</span><br><span class="line">                strides=<span class="number">32</span>))</span><br><span class="line">print(net(x).shape) <span class="comment"># (1, 320, 480, 21)</span></span><br><span class="line">net.add(keras.layers.Softmax(axis=<span class="number">-1</span>))</span><br></pre></td></tr></table></figure>
<h2 id="chu-shi-hua-zhuan-zhi-juan-ji-ceng">初始化转置卷积层</h2>
<p>转置卷积层可以放大特征图，在图像处理中，有时需要将图像放大，即上采样（upsample）。上采样的方法有很多，常用的有双线性插值。简单来说，为了得到输出图像在坐标\((x,y)\)上的像素，先将该坐标映射到输入图像的坐标\((x',y')\)，例如，根据输入与输出的尺寸之比来映射。映射后的\(x'\)和\(y'\)通常是实数。然后，在输入图像上找到与坐标\((x',y')\)最近的4个像素。最后，输出图像在坐标\((x,y)\)上的像素依据输入图像上这4个像素及其与\((x',y')\)的相对距离来计算。双线性插值的上采样可以通过由以下<code>bilinear_kernel</code>函数构造的卷积核的转置卷积层来实现。这里给出<code>bilinear_kernel</code>函数的实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 目的就是找出 kernel_size*kernel_size 的框，框内的权值中间大外面小</span></span><br><span class="line"><span class="comment"># 然后每个通道只乘以自己的权值 就像上面图一样（kernel_size=2）</span></span><br><span class="line"><span class="comment"># 每个channel乘以对应的权值</span></span><br><span class="line"><span class="comment"># in_channels, out_channels 应该相同</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bilinear_kernel</span><span class="params">(in_channels, out_channels, kernel_size)</span>:</span></span><br><span class="line">    factor = (kernel_size + <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> kernel_size % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">        center = factor - <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        center = factor - <span class="number">0.5</span></span><br><span class="line">    og = np.ogrid[:kernel_size, :kernel_size]</span><br><span class="line">    filt = (<span class="number">1</span>-abs(og[<span class="number">0</span>]-center)/factor) * (<span class="number">1</span>-abs(og[<span class="number">1</span>]-center)/factor)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># kernel_size = (kernel_size, kernel_size, in_channels, out_channels)</span></span><br><span class="line">    weight = np.zeros((kernel_size, kernel_size, in_channels, out_channels), dtype=<span class="string">'float32'</span>)</span><br><span class="line">    weight[:, :, range(in_channels), range(out_channels)] = filt.reshape((kernel_size,kernel_size,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> weight</span><br></pre></td></tr></table></figure>
<p>用转置卷积层实现的双线性插值的上采样。构造一个将输入的高和宽放大2倍的转置卷积层，并将其卷积核用<code>bilinear_kernel</code>函数初始化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">init_param = initializers.Constant(bilinear_kernel(<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">conv_trans = layers.Conv2DTranspose(<span class="number">3</span>,</span><br><span class="line">                kernel_size=<span class="number">4</span>,</span><br><span class="line">                padding=<span class="string">'same'</span>,</span><br><span class="line">                strides=<span class="number">2</span>,</span><br><span class="line">                kernel_initializer=init_param,)</span><br></pre></td></tr></table></figure>
<p>读取图像<code>X</code>，将上采样的结果记作<code>Y</code>。为了打印图像，我们需要调整通道维的位置。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = tf.io.read_file(<span class="string">'catdog.jpg'</span>)</span><br><span class="line">img_org = tf.image.decode_jpeg(img, channels=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">img = tf.cast(img_org, tf.float32)/<span class="number">255.</span></span><br><span class="line">print(img.shape) <span class="comment"># (561, 728, 3)</span></span><br><span class="line">plt.imshow(img.numpy())</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_23_2.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 增加维度</span></span><br><span class="line">y = conv_trans(tf.expand_dims(img, axis=<span class="number">0</span>))</span><br><span class="line">print(y.shape) <span class="comment"># (1, 1122, 1456, 3)</span></span><br><span class="line"><span class="comment"># 颜色会变浅</span></span><br><span class="line">plt.imshow(y[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_24_2.png" alt></p>
<p>在全卷积网络中，将转置卷积层初始化为双线性插值的上采样。对于 1×1 卷积层，采用Xavier随机初始化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">-2</span>, <span class="number">-1</span>):</span><br><span class="line">    bias_weight = net.layers[i].get_weights()[<span class="number">1</span>]</span><br><span class="line">    kernel_size = net.layers[i].get_config()[<span class="string">'kernel_size'</span>][<span class="number">0</span>]</span><br><span class="line">    net.layers[i].set_weights([bilinear_kernel(num_classes, num_classes, kernel_size), </span><br><span class="line">                bias_weight])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    print(net.layers[i].trainable)</span><br><span class="line"><span class="comment"># net[-3] 不需要动了，tf默认Xavier初始化</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h1 id="yang-shi-qian-yi">样式迁移</h1>
<p>滤镜能改变照片的颜色样式，从而使风景照更加锐利或者令人像更加美白。但一个滤镜通常只能改变照片的某个方面。如果要照片达到理想中的样式，经常需要尝试大量不同的组合，其复杂程度不亚于模型调参。</p>
<p>如何使用卷积神经网络自动将某图像中的样式应用在另一图像之上，即样式迁移[1]。这里需要两张输入图像，一张是内容图像，另一张是样式图像，使用神经网络修改内容图像使其在样式上接近样式图像。下图中的内容图像为西雅图郊区的雷尼尔山国家公园的风景照，而样式图像则是一幅主题为秋天橡树的油画。最终输出的合成图像在保留了内容图像中物体主体形状的情况下应用了样式图像的油画笔触，同时也让整体颜色更加鲜艳。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/style-transfer.svg" alt="输入内容图像和样式图像，输出样式迁移后的合成图像"></p>
<h2 id="yang-shi-qian-yi-fang-fa">样式迁移方法</h2>
<p>上图用一个例子来阐述基于卷积神经网络的样式迁移方法。首先，初始化合成图像，例如将其初始化成内容图像。该合成图像是样式迁移过程中唯一需要更新的变量，即样式迁移所需迭代的模型参数。然后，选择一个预训练的卷积神经网络来抽取图像的特征，其中的模型参数在训练中无须更新。深度卷积神经网络凭借多个层逐级抽取图像的特征。可以选择其中某些层的输出作为内容特征或样式特征。以上图为例，这里选取的预训练的神经网络含有3个卷积层，其中第二层输出图像的内容特征，而第一层和第三层的输出被作为图像的样式特征。接下来，通过正向传播（实线箭头方向）计算样式迁移的损失函数，并通过反向传播（虚线箭头方向）迭代模型参数，即不断更新合成图像。样式迁移常用的损失函数由3部分组成：内容损失使合成图像与内容图像在内容特征上接近，样式损失令合成图像与样式图像在样式特征上接近，而总变差损失则有助于减少合成图像中的噪点。最后，当模型训练结束时，输出样式迁移的模型参数，即得到最终的合成图像。</p>
<p><img src="/2019/05/27/deeplearning/computer_vision/neural-style.svg" alt="基于卷积神经网络的样式迁移。实线箭头和虚线箭头分别表示正向传播和反向传播"></p>
<h2 id="du-qu-nei-rong-tu-xiang-he-yang-shi-tu-xiang">读取内容图像和样式图像</h2>
<p>首先，分别读取内容图像和样式图像。从打印出的图像坐标轴可以看出，它们的尺寸并不一样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">content_img = tf.io.read_file(<span class="string">'rainier.jpg'</span>)</span><br><span class="line">content_img = tf.image.decode_jpeg(content_img)</span><br><span class="line">plt.imshow(content_img)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_12_1.png" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">style_img = tf.io.read_file(<span class="string">'../../data/autumn_oak.jpg'</span>)</span><br><span class="line">style_img = tf.image.decode_jpeg(style_img)</span><br><span class="line">plt.imshow(style_img)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_13_1-5863677.png" alt></p>
<h2 id="yu-chu-li-he-hou-chu-li-tu-xiang">预处理和后处理图像</h2>
<p>下面定义图像的预处理函数和后处理函数。预处理函数preprocess对先对更改输入图像的尺寸，然后再将PIL图片转成卷积神经网络接受的输入格式，再在RGB三个通道分别做标准化，由于预训练模型是在均值为[0.485, 0.456, 0.406]标准差为[0.229, 0.224, 0.225]的图片数据上预训练的，所以要将图片标准化保持相同的均值和标准差。后处理函数postprocess则将输出图像中的像素值还原回标准化之前的值。由于图像每个像素的浮点数值在0到1之间，可以使用clamp函数对小于0和大于1的值分别取0和1。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rgb_mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">rgb_std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(img_tensor, image_shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    image_shape = (h, w)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    img = tf.image.resize(img_tensor, image_shape[<span class="number">0</span>:<span class="number">2</span>])</span><br><span class="line">    img = tf.divide(img, <span class="number">255.</span>)</span><br><span class="line">    img = tf.divide(tf.subtract(img, rgb_mean), rgb_std)</span><br><span class="line">    img = tf.expand_dims(img, axis=<span class="number">0</span>) <span class="comment"># (batch_size, h, w, 3)</span></span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">postprocess</span><span class="params">(img_tensor)</span>:</span></span><br><span class="line">    mean = -rgb_mean / rgb_std</span><br><span class="line">    std= <span class="number">1</span> / rgb_std</span><br><span class="line">    img = tf.divide(tf.subtract(img_tensor, mean), std)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure>
<h2 id="chou-qu-te-zheng">抽取特征</h2>
<p>使用基于ImageNet数据集预训练的VGG-19模型来抽取图像特征 [1]。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pretrained_net = keras.applications.vgg19.VGG19(weights=<span class="string">"imagenet"</span>)</span><br></pre></td></tr></table></figure>
<p>为了抽取图像的内容特征和样式特征，可以选择VGG网络中某些层的输出。一般来说，越靠近输入层的输出越容易抽取图像的细节信息，反之则越容易抽取图像的全局信息。为了避免合成图像过多保留内容图像的细节，选择VGG较靠近输出的层，也称内容层，来输出图像的内容特征。另外，还从VGG中选择不同层的输出来匹配局部和全局的样式，这些层也叫样式层。VGG网络使用了5个卷积块。实验中，选择第四卷积块的最后一个卷积层作为内容层，以及每个卷积块的第一个卷积层作为样式层。这些层的索引可以通过打印pretrained_net实例来获取</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">need = [<span class="string">'name'</span>, <span class="string">'filters'</span>, <span class="string">'kernel_size'</span>, <span class="string">'strides'</span>, <span class="string">'pool_size'</span>, <span class="string">'padding'</span>, <span class="string">'batch_input_shape'</span>]</span><br><span class="line">print(<span class="string">"&#123;&#125;"</span>.format(<span class="string">"\t"</span>), end=<span class="string">""</span>)</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> need:</span><br><span class="line">    print(n.ljust(<span class="number">13</span>,<span class="string">' '</span>), end=<span class="string">'\t'</span>)</span><br><span class="line">print()</span><br><span class="line"><span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(pretrained_net.layers[:]):</span><br><span class="line">    config = layer.get_config()</span><br><span class="line">    params = [config.get(key) <span class="keyword">for</span> key <span class="keyword">in</span> need]</span><br><span class="line">    print(<span class="string">"(&#123;&#125;)"</span>.format(i), end=<span class="string">""</span>)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> params:</span><br><span class="line">        print(str(p).ljust(<span class="number">13</span>,<span class="string">' '</span>), end=<span class="string">'\t'</span>)</span><br><span class="line">    print()</span><br><span class="line">   </span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">  name         	filters      	kernel_size  	strides      	pool_size    	padding      	batch_input_shape	</span><br><span class="line">(<span class="number">0</span>)input_1      	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	(<span class="literal">None</span>, <span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>)	</span><br><span class="line">(<span class="number">1</span>)block1_conv1 	<span class="number">64</span>           	(<span class="number">3</span>, <span class="number">3</span>)       	(<span class="number">1</span>, <span class="number">1</span>)       	<span class="literal">None</span>         	same         	<span class="literal">None</span>         	</span><br><span class="line">(<span class="number">2</span>)block1_conv2 	<span class="number">64</span>           	(<span class="number">3</span>, <span class="number">3</span>)       	(<span class="number">1</span>, <span class="number">1</span>)       	<span class="literal">None</span>         	same         	<span class="literal">None</span>         	</span><br><span class="line">...</span><br><span class="line">(<span class="number">24</span>)fc2          	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	</span><br><span class="line">(<span class="number">25</span>)predictions  	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span>         	<span class="literal">None</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># style_layers = [block1_conv1, block2_conv1, block3_conv1, block3_conv4, block4_conv1, block5_conv1]</span></span><br><span class="line"><span class="comment"># content_layers = [block3_conv1]</span></span><br><span class="line">style_layers, content_layers = [<span class="number">1</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">17</span>], [<span class="number">7</span>]</span><br></pre></td></tr></table></figure>
<p>在抽取特征时，只需要用到VGG从输入层到最靠近输出层的内容层或样式层之间的所有层。下面构建一个新的网络net，它只保留需要用到的VGG的所有层。我们将使用net来抽取特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net_list = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(max(content_layers + style_layers) + <span class="number">1</span>):</span><br><span class="line">    net_list.append(pretrained_net.layers[i])</span><br><span class="line">net = keras.Sequential()</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net_list:</span><br><span class="line">        net.add(layer)</span><br></pre></td></tr></table></figure>
<p>给定输入X，如果简单调用前向计算net(X)，只能获得最后一层的输出。由于还需要中间层的输出，因此这里逐层计算，并保留内容层和样式层的输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_feature</span><span class="params">(x, content_layers, style_layers)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.shape) == <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    contents = []</span><br><span class="line">    styles = []</span><br><span class="line">    <span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(net.layers):</span><br><span class="line">        x = layer(x)</span><br><span class="line">        <span class="keyword">if</span> i+<span class="number">1</span> <span class="keyword">in</span> style_layers:</span><br><span class="line">            styles.append(x)</span><br><span class="line">        <span class="keyword">if</span> i+<span class="number">1</span> <span class="keyword">in</span> content_layers:</span><br><span class="line">            contents.append(x)</span><br><span class="line">    <span class="keyword">return</span> contents, styles</span><br></pre></td></tr></table></figure>
<p>下面定义两个函数，其中get_contents函数对内容图像抽取内容特征，而get_styles函数则对样式图像抽取样式特征。因为在训练时无须改变预训练的VGG的模型参数，所以可以在训练开始之前就提取出内容图像的内容特征，以及样式图像的样式特征。由于合成图像是样式迁移所需迭代的模型参数，只能在训练过程中通过调用extract_features函数来抽取合成图像的内容特征和样式特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_contents</span><span class="params">(image_shape)</span>:</span></span><br><span class="line">    <span class="comment"># 统一尺寸</span></span><br><span class="line">    content_x = preprocess(content_img, image_shape)</span><br><span class="line">    <span class="comment"># 获得内容特征</span></span><br><span class="line">    contents_y, _ = extract_feature(content_x, content_layers, style_layers)</span><br><span class="line">    <span class="keyword">return</span> content_x, contents_y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_styles</span><span class="params">(image_shape)</span>:</span></span><br><span class="line">    <span class="comment"># 统一尺寸</span></span><br><span class="line">    style_x = preprocess(style_img, image_shape)</span><br><span class="line">    <span class="comment"># 获得样式特征</span></span><br><span class="line">    _, styles_y = extract_feature(style_x, content_layers, style_layers)</span><br><span class="line">    <span class="keyword">return</span> style_x, styles_y</span><br></pre></td></tr></table></figure>
<h2 id="ding-yi-sun-shi-han-shu">定义损失函数</h2>
<p>下面描述样式迁移的损失函数。它由内容损失、样式损失和总变差损失3部分组成。</p>
<h3 id="nei-rong-sun-shi">内容损失</h3>
<p>与线性回归中的损失函数类似，内容损失通过平方误差函数衡量合成图像与内容图像在内容特征上的差异。平方误差函数的两个输入均为extract_features函数计算所得到的内容层的输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">content_loss</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    c_l = keras.losses.mean_squared_error(y, y_hat)</span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(c_l)</span><br></pre></td></tr></table></figure>
<h3 id="yang-shi-sun-shi">样式损失</h3>
<p>样式损失也一样通过平方误差函数衡量合成图像与样式图像在样式上的差异。为了表达样式层输出的样式，先通过extract_features函数计算样式层的输出。假设该输出的样本数为1，通道数为 c ，高和宽分别为 h 和 w ，可以把输出变换成 c 行 \((h\times w)\) 列的矩阵 X 。矩阵 X 可以看作由 c 个长度为 \((h \times w)\) 的向量 x1,…,xc 组成的。其中向量 \(x_i\) 代表了通道 \(i\) 上的样式特征。这些向量的格拉姆矩阵（Gram matrix） \(XX^⊤ \in R_{c \times c}\) 中 i 行 j 列的元素 \(x_{ij}\) 即向量 \(x_i\) 与 \(x_j\) 的内积，它表达了通道 i 和通道 j 上样式特征的相关性。用这样的格拉姆矩阵表达样式层输出的样式。需要注意的是，当 hw 的值较大时，格拉姆矩阵中的元素容易出现较大的值。此外，格拉姆矩阵的高和宽皆为通道数 c 。为了让样式损失不受这些值的大小影响，下面定义的gram函数将格拉姆矩阵除以了矩阵中元素的个数，即 chw 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># x.shape=(batch_size, h, w, c)</span></span><br><span class="line">    num_channels, n = x.shape[<span class="number">3</span>], x.shape[<span class="number">1</span>] * x.shape[<span class="number">2</span>]</span><br><span class="line">    x = tf.reshape(x, shape=(num_channels, n))</span><br><span class="line">    x_big = tf.matmul(x, tf.transpose(x))</span><br><span class="line">    x_big = tf.divide(x_big, num_channels * n)</span><br><span class="line">    <span class="keyword">return</span> x_big</span><br></pre></td></tr></table></figure>
<p>自然地，样式损失的平方误差函数的两个格拉姆矩阵输入分别基于合成图像与样式图像的样式层输出。这里假设基于样式图像的格拉姆矩阵gram_Y已经预先计算好了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">style_loss</span><span class="params">(y_hat, gram_y)</span>:</span></span><br><span class="line">    s_l = keras.losses.mean_squared_error(gram_y, gram(y_hat))</span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(s_l)</span><br></pre></td></tr></table></figure>
<h3 id="zong-bian-chai-sun-shi">总变差损失</h3>
<p>学到的合成图像里面有大量高频噪点，即有特别亮或者特别暗的颗粒像素。一种常用的降噪方法是总变差降噪。假设 \(x_{i,j}\) 表示坐标为 (i,j) 的像素值，降低总变差损失</p>
<p>\[
\sum_{i,j} \left|x_{i,j} - x_{i+1,j}\right| + \left|x_{i,j} - x_{i,j+1}\right|
\]</p>
<p>能够尽可能使邻近的像素值相似。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tv_loss</span><span class="params">(y_hat)</span>:</span></span><br><span class="line">    t1 = keras.losses.mean_absolute_error(y_hat[:, <span class="number">1</span>:, :, :], </span><br><span class="line">                        y_hat[:, :<span class="number">-1</span>, :, :])</span><br><span class="line">    t1 = tf.reduce_mean(t1)</span><br><span class="line">    t2 = keras.losses.mean_absolute_error(y_hat[:, :, <span class="number">1</span>:, :], </span><br><span class="line">                        y_hat[:, :, :<span class="number">-1</span>, :])</span><br><span class="line">    t2 = tf.reduce_mean(t2)</span><br><span class="line">    </span><br><span class="line">    tv = tf.add(t1, t2)</span><br><span class="line">    <span class="keyword">return</span> tf.multiply(<span class="number">0.5</span>, tv)</span><br></pre></td></tr></table></figure>
<h2 id="sun-shi-han-shu">损失函数</h2>
<p>样式迁移的损失函数即内容损失、样式损失和总变差损失的加权和。通过调节这些权值超参数，可以权衡合成图像在保留内容、迁移样式以及降噪三方面的相对重要性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">content_weight, style_weight, tv_weight = <span class="number">1</span>, <span class="number">1e3</span>, <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span><span class="params">(x, contents_y_hat, styles_y_hat, contents_y, styles_y_gram)</span>:</span></span><br><span class="line">    <span class="comment"># 分别计算内容损失、样式损失和总变差损失</span></span><br><span class="line">    contents_l = [content_loss(y_hat, y) * content_weight <span class="keyword">for</span> y_hat, y <span class="keyword">in</span> zip(</span><br><span class="line">        contents_y_hat, contents_y)]</span><br><span class="line">    styles_l = [style_loss(y_hat, y) * style_weight <span class="keyword">for</span> y_hat, y <span class="keyword">in</span> zip(</span><br><span class="line">        styles_y_hat, styles_y_gram)]</span><br><span class="line">    tv_l = tv_loss(x) * tv_weight</span><br><span class="line">    <span class="comment"># 对所有损失求和</span></span><br><span class="line">    l = tf.reduce_sum(tf.reshape(contents_l,(<span class="number">-1</span>,))) + tf.reduce_sum(tf.reshape(styles_l,(<span class="number">-1</span>,))) + tv_l</span><br><span class="line">    <span class="keyword">return</span> contents_l, styles_l, tv_l, l</span><br></pre></td></tr></table></figure>
<h3 id="chuang-jian-he-chu-shi-hua-he-cheng-tu-xiang">创建和初始化合成图像</h3>
<p>在样式迁移中，合成图像是唯一需要更新的变量。因此，可以定义一个简单的模型GeneratedImage，并将合成图像视为模型参数。模型的前向计算只需返回模型参数即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GeneratedImage</span><span class="params">(keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, img)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weight = tf.Variable(img, dtype=tf.float32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.weight</span><br></pre></td></tr></table></figure>
<p>下面，定义get_inits函数。该函数创建了合成图像的模型实例，并将其初始化为图像X。样式图像在各个样式层的格拉姆矩阵<code>styles_Y_gram</code>将在训练前预先计算好。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_inits</span><span class="params">(x, lr, styles_y)</span>:</span></span><br><span class="line">    gen_img = GeneratedImage(x)</span><br><span class="line">    <span class="comment"># 这里如果像pytorch那样直接赋值会报错，所以我在上面那个代码赋值的</span></span><br><span class="line">    <span class="comment"># gen_img.weight = x</span></span><br><span class="line">    optimizer = keras.optimizers.Adam(lr)</span><br><span class="line">    styles_y_gram = [gram(y) <span class="keyword">for</span> y <span class="keyword">in</span> styles_y]</span><br><span class="line">    <span class="keyword">return</span> gen_img(), styles_y_gram, optimizer</span><br></pre></td></tr></table></figure>
<h2 id="xun-lian">训练</h2>
<p>在训练模型时，不断抽取合成图像的内容特征和样式特征，并计算损失函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(x, contents_y, styles_y, lr, max_epochs, lr_decay_epoch)</span>:</span></span><br><span class="line">    x, styles_y_gram, optimizer = get_inits(x, lr, styles_y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_epochs):</span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> t:</span><br><span class="line">            t.watch(x)</span><br><span class="line">            contents_y_hat, styles_y_hat = extract_feature(x, content_layers,</span><br><span class="line">                                style_layers)</span><br><span class="line">            contents_l, styles_l, tv_l, l = compute_loss(x,</span><br><span class="line">                                contents_y_hat,</span><br><span class="line">                                styles_y_hat,</span><br><span class="line">                                contents_y,</span><br><span class="line">                                styles_y_gram)</span><br><span class="line">            </span><br><span class="line">        grads = t.gradient(l, x)</span><br><span class="line">        optimizer.apply_gradients(zip([grads], [x]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span> <span class="keyword">and</span> i != <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %3d, content loss %.2f, style loss %.2f, '</span></span><br><span class="line">                  <span class="string">'TV loss %.2f, %.2f sec'</span></span><br><span class="line">                  % (i, tf.reduce_sum(contents_l), </span><br><span class="line">                     tf.reduce_sum(styles_l), </span><br><span class="line">                     tf.reduce_sum(tv_l),</span><br><span class="line">                     time.time() - start))</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>下面开始训练模型。首先将内容图像和样式图像的高和宽分别调整为150和225像素。合成图像将由内容图像来初始化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">image_shape =  (<span class="number">150</span>, <span class="number">225</span>, <span class="number">3</span>)</span><br><span class="line">content_x, contents_y = get_contents(image_shape)</span><br><span class="line">style_x, styles_y = get_styles(image_shape)</span><br><span class="line">output = train(content_x, contents_y, styles_y, <span class="number">0.1</span>, <span class="number">501</span>, <span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = postprocess(output)[<span class="number">0</span>]</span><br><span class="line">img = tf.clip_by_value(img, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">plt.imsave(<span class="string">'1.jpg'</span>, img.numpy())</span><br><span class="line">plt.imshow(img)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_57_1.png" alt></p>
<p>为了得到更加清晰的合成图像，下面在更大的300×450尺寸上训练。将上图的高和宽放大2倍，以初始化更大尺寸的合成图像。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">image_shape =  (<span class="number">300</span>, <span class="number">450</span>, <span class="number">3</span>)</span><br><span class="line">content_x, contents_y = get_contents(image_shape)</span><br><span class="line">style_x, styles_y = get_styles(image_shape)</span><br><span class="line">big_output = train(content_x, contents_y, styles_y, <span class="number">0.01</span>, <span class="number">1001</span>, <span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img=postprocess(big_output)[<span class="number">0</span>]</span><br><span class="line">img = tf.clip_by_value(img, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">plt.imsave(<span class="string">'2.jpg'</span>, img.numpy())</span><br><span class="line">plt.imshow(img)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/05/27/deeplearning/computer_vision/output_60_1.png" alt></p>
<h1 id="zong-jie">总结</h1>
<ol>
<li>迁移学习将从源数据集学到的知识迁移到目标数据集上。微调是迁移学习的一种常用技术。</li>
<li>目标模型复制了源模型上除了输出层外的所有模型设计及其参数，并基于目标数据集微调这些参数。而目标模型的输出层需要从头训练。</li>
<li>一般来说，微调参数会使用较小的学习率，而从头训练输出层可以使用较大的学习率。</li>
<li>在目标检测里不仅需要找出图像里面所有感兴趣的目标，而且要知道它们的位置。位置一般由矩形边界框来表示。</li>
<li>以每个像素为中心，生成多个大小和宽高比不同的锚框。</li>
<li>交并比是两个边界框相交面积与相并面积之比。</li>
<li>在训练集中，为每个锚框标注两类标签：一是锚框所含目标的类别；二是真实边界框相对锚框的偏移量。</li>
<li>预测时，可以使用非极大值抑制来移除相似的预测边界框，从而令结果简洁。</li>
<li>可以在多个尺度下生成不同数量和不同大小的锚框，从而在多个尺度下检测不同大小的目标。</li>
<li>特征图的形状能确定任一图像上均匀采样的锚框中心。</li>
<li>用输入图像在某个感受野区域内的信息来预测输入图像上与该区域相近的锚框的类别和偏移量。</li>
<li>R-CNN对图像选取若干提议区域，然后用卷积神经网络对每个提议区域做前向计算抽取特征，再用这些特征预测提议区域的类别和边界框。</li>
<li>Fast R-CNN对R-CNN的一个主要改进在于只对整个图像做卷积神经网络的前向计算。它引入了兴趣区域池化层，从而令兴趣区域能够抽取出形状相同的特征。</li>
<li>Faster R-CNN将Fast R-CNN中的选择性搜索替换成区域提议网络，从而减少提议区域的生成数量，并保证目标检测的精度。</li>
<li>Mask R-CNN在Faster R-CNN基础上引入一个全卷积网络，从而借助目标的像素级位置进一步提升目标检测的精度。</li>
<li>语义分割关注如何将图像分割成属于不同语义类别的区域。</li>
<li>语义分割的一个重要数据集叫作Pascal VOC2012。</li>
<li>由于语义分割的输入图像和标签在像素上一一对应，所以将图像随机裁剪成固定尺寸而不是缩放。</li>
<li>可以通过矩阵乘法来实现卷积运算。</li>
<li>全卷积网络先使用卷积神经网络抽取图像特征，然后通过\(1\times 1\)卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸，从而输出每个像素的类别。</li>
<li>在全卷积网络中，可以将转置卷积层初始化为双线性插值的上采样。</li>
<li>样式迁移常用的损失函数由3部分组成：内容损失使合成图像与内容图像在内容特征上接近，样式损失令合成图像与样式图像在样式特征上接近，而总变差损失则有助于减少合成图像中的噪点。</li>
<li>可以通过预训练的卷积神经网络来抽取图像的特征，并通过最小化损失函数来不断更新合成图像。</li>
<li>用格拉姆矩阵表达样式层输出的样式。</li>
</ol>
<h1 id="can-kao-wen-xian">参考文献</h1>
<p>[1] Long, J., Shelhamer, E., &amp; Darrell, T. (2015). Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3431-3440).</p>
<p>[2] Dumoulin, V., &amp; Visin, F. (2016). A guide to convolution arithmetic for deep learning. arXiv preprint arXiv:1603.07285.</p>
]]></content>
      <categories>
        <category>技术/深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>卷积神经网络</title>
    <url>/2019/04/17/deeplearning/cnn/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2019/04/17/deeplearning/cnn/5.1_correlation.svg" alt></p>
<a id="more"></a>
<h1 id="er-wei-juan-ji-ceng">二维卷积层</h1>
<p>卷积神经网络是含有卷积层的神经网络，它有高和宽两个空间维度，常用来处理图像数据。</p>
<h2 id="two-dimentional-cross-correlation">two dimentional cross-correlation</h2>
<p>虽然卷积层得名于卷积运算，但我们通常在卷积层中使用更加直观的互相关运算。在二维卷积层中，一个二维输入数组和一个二维核数组通过互相关运算输出一个二维数组。<br>
用一个具体例子来解释二维互相关运算的含义。如下图所示，输入是一个高和宽均为3的二维数组。我们将该数组的形状记为\(3 \times 3\)。核数组的高和宽分别为2。该数组在卷积计算中又称卷积核或过滤器。卷积核窗口的形状取决于卷积核的高和宽，即\(2 \times 2\)。下图中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：\(0\times0+1\times1+3\times2+4\times3=19\)。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.1_correlation-4944107.svg" alt="二维互相关运算"></p>
<p>在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。图中的输出数组高和宽分别为2，其中的4个元素由二维互相关运算得出：</p>
<p>\[
0\times0+1\times1+3\times2+4\times3=19,\\
1\times0+2\times1+4\times2+5\times3=25,\\
3\times0+4\times1+6\times2+7\times3=37,\\
4\times0+5\times1+7\times2+8\times3=43.\\
\]</p>
<p>下面将上述过程实现在<code>corr2d</code>函数里。它接受输入数组<code>X</code>与核数组<code>K</code>，并输出数组<code>Y</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = tf.Variable(tf.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w +<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i,j].assign(tf.cast(tf.reduce_sum(X[i:i+h, j:j+w] * K), dtype=tf.float32))</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<h2 id="conv-2-d">Conv2d</h2>
<p>二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。</p>
<p>下面基于<code>corr2d</code>函数来实现一个自定义的二维卷积层。在构造函数<code>__init__</code>里声明<code>weight</code>和<code>bias</code>这两个模型参数。前向计算函数<code>forward</code>则是直接调用<code>corr2d</code>函数再加上偏差。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.units = units</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, kernel_size)</span>:</span></span><br><span class="line">        self.w = self.add_weight(name=<span class="string">'w'</span>,</span><br><span class="line">                                shape=kernel_size,</span><br><span class="line">                                initializer=tf.random_normal_initializer())</span><br><span class="line">        self.b = self.add_weight(name=<span class="string">'b'</span>,</span><br><span class="line">                                shape=(<span class="number">1</span>,),</span><br><span class="line">                                initializer=tf.random_normal_initializer())</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(inputs, self.w) + self.b</span><br></pre></td></tr></table></figure>
<p>卷积窗口形状为\(p \times q\)的卷积层称为\(p \times q\)卷积层。同样，\(p \times q\)卷积或\(p \times q\)卷积核说明卷积核的高和宽分别为\(p\)和\(q\)。</p>
<h2 id="hu-xiang-guan-yun-suan-he-juan-ji-yun-suan">互相关运算和卷积运算</h2>
<p>实际上，卷积运算与互相关运算类似。为了得到卷积运算的输出，只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算。可见，卷积运算和互相关运算虽然类似，但如果它们使用相同的核数组，对于同一个输入，输出往往并不相同。</p>
<p>那么，卷积层为何能使用互相关运算替代卷积运算？</p>
<p>在深度学习中核数组都是学出来的：卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出。为了解释这一点，假设卷积层使用互相关运算学出上图的核数组。设其他条件不变，使用卷积运算学出的核数组即上图中的核数组按上下、左右翻转。也就是说，图中的输入与学出的已翻转的核数组再做卷积运算时，依然得到图中的输出。</p>
<h2 id="te-zheng-tu-he-gan-shou-ye">特征图和感受野</h2>
<p>二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图。影响元素\(x\)的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做\(x\)的感受野。以上图为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图中形状为\(2 \times 2\)的输出记为\(Y\)，并考虑一个更深的卷积神经网络：将\(Y\)与另一个形状为\(2 \times 2\)的核数组做互相关运算，输出单个元素\(z\)。那么，\(z\)在\(Y\)上的感受野包括\(Y\)的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。</p>
<h2 id="xiao-jie">小结</h2>
<ol>
<li>二维卷积层的核心计算是二维互相关运算。在最简单的形式下，它对二维输入数据和卷积核做互相关运算然后加上偏差。</li>
<li>可以设计卷积核来检测图像中的边缘。</li>
<li>可以通过数据来学习卷积核。</li>
</ol>
<h1 id="tian-chong-he-bu-fu">填充和步幅</h1>
<p>一般来说，假设输入形状是\(n_h\times n_w\)，卷积核窗口形状是\(k_h\times k_w\)，那么输出形状将会是</p>
<p>\[
(n_h-k_h+1) \times (n_w-k_w+1)
\]</p>
<p>所以卷积层的输出形状由输入形状和卷积核窗口形状决定。而填充和步幅可以对给定形状的输入和卷积核改变输出形状。</p>
<h2 id="padding">padding</h2>
<p>填充是指在输入高和宽的两侧填充元素（通常是0元素）。如下图所示，在原输入高和宽的两侧分别添加了值为0的元素，使得输入高和宽从3变成了5，并导致输出高和宽由2增加到4。图5.2中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：\(0\times0+0\times1+0\times2+0\times3=0\)。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.2_conv_pad.svg" alt="在输入的高和宽两侧分别填充了0元素的二维互相关计算"></p>
<p>一般来说，如果在高的两侧一共填充\(p_h\)行，在宽的两侧一共填充\(p_w\)列，那么输出形状将会是</p>
<p>\[
(n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1)
\]</p>
<p>也就是说，输出的高和宽会分别增加\(p_h\)和\(p_w\)。</p>
<p>在很多情况下，我们会设置\(p_h=k_h-1\)和\(p_w=k_w-1\)来使输入和输出具有相同的高和宽。这样会方便在构造网络时推测每个层的输出形状。假设这里\(k_h\)是奇数，我们会在高的两侧分别填充\(p_h/2\)行。如果\(k_h\)是偶数，一种可能是在输入的顶端一侧填充\(\lceil p_h/2\rceil\)行，而在底端一侧填充\(\lfloor p_h/2\rfloor\)行。在宽的两侧填充同理。</p>
<p>卷积神经网络经常使用奇数高宽的卷积核，如1、3、5和7，所以两端上的填充个数相等。对任意的二维数组<code>X</code>，设它的第<code>i</code>行第<code>j</code>列的元素为<code>X[i,j]</code>。当两端上的填充个数相等，并使输入和输出具有相同的高和宽时，就可以知道输出<code>Y[i,j]</code>是由输入以<code>X[i,j]</code>为中心的窗口同卷积核进行互相关计算得到的。</p>
<h2 id="stride">stride</h2>
<p>卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。每次滑动的行数和列数称为步幅。</p>
<p>以使用更大步幅。如下图展示了在高上步幅为3、在宽上步幅为2的二维互相关运算。可以看到，输出第一列第二个元素时，卷积窗口向下滑动了3行，而在输出第一行第二个元素时卷积窗口向右滑动了2列。当卷积窗口在输入上再向右滑动2列时，由于输入元素无法填满窗口，无结果输出。图5.3中的阴影部分为输出元素及其计算所使用的输入和核数组元素：\(0\times0+0\times1+1\times2+2\times3=8\)、\(0\times0+6\times1+0\times2+0\times3=6\)。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.2_conv_stride.svg" alt="高和宽上步幅分别为3和2的二维互相关运算"></p>
<p>一般来说，当高上步幅为\(s_h\)，宽上步幅为\(s_w\)时，输出形状为</p>
<p>\[\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.\]</p>
<p>如果设置\(p_h=k_h-1\)和\(p_w=k_w-1\)，那么输出形状将简化为\(\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor\)。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是\((n_h/s_h) \times (n_w/s_w)\)。</p>
<ol start="3">
<li></li>
</ol>
<h1 id="duo-shu-ru-tong-dao-he-duo-shu-chu-tong-dao">多输入通道和多输出通道</h1>
<p>真实数据的维度通常很高。如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是\(h\)和\(w\)（像素），那么它可以表示为一个\(3\times h\times w\)的多维数组。我们将大小为3的这一维称为通道维。</p>
<h2 id="muti-channels-in">muti-channels in</h2>
<p>当输入数据含多个通道时，需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。假设输入数据的通道数为\(c_i\)，那么卷积核的输入通道数同样为\(c_i\)。设卷积核窗口形状为\(k_h\times k_w\)。当\(c_i=1\)时，我们知道卷积核只包含一个形状为\(k_h\times k_w\)的二维数组。当\(c_i > 1\)时，我们将会为每个输入通道各分配一个形状为\(k_h\times k_w\)的核数组。把这\(c_i\)个数组在输入通道维上连结，即得到一个形状为\(c_i\times k_h\times k_w\)的卷积核。由于输入和卷积核各有\(c_i\)个通道，可以在各个通道上对输入的二维数组和卷积核的二维核数组做互相关运算，再将这\(c_i\)个互相关运算的二维输出按通道相加，得到一个二维数组。这就是含多个通道的输入数据与多输入通道的卷积核做二维互相关运算的输出。</p>
<p>下图展示了含2个输入通道的二维互相关计算的例子。在每个通道上，二维输入数组与二维核数组做互相关运算，再按通道相加即得到输出。下图中阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：\((1\times1+2\times2+4\times3+5\times4)+(0\times0+1\times1+3\times2+4\times3)=56\)。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.3_conv_multi_in.svg" alt="含2个输入通道的互相关计算"></p>
<p>接下来实现含多个输入通道的互相关运算。只需要对每个通道做互相关运算，然后进行累加。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d_multi_in</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_sum([corr2d(X[i], K[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">0</span>])],axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="multi-channels-out">multi-channels out</h2>
<p>当输入通道有多个时，因为对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为1。设卷积核输入通道数和输出通道数分别为\(c_i\)和\(c_o\)，高和宽分别为\(k_h\)和\(k_w\)。如果希望得到含多个通道的输出，可以为每个输出通道分别创建形状为\(c_i\times k_h\times k_w\)的核数组。将它们在输出通道维上连结，卷积核的形状即\(c_o\times c_i\times k_h\times k_w\)。在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与整个输入数组计算而来。</p>
<p>下面我们实现一个互相关运算函数来计算多个通道的输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d_multi_in_out</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K],axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="convolution">convolution</h2>
<p>卷积窗口形状为\(1\times 1\)（\(k_h=k_w=1\)）的多通道卷积层。通常称之为\(1\times 1\)卷积层，并将其中的卷积运算称为\(1\times 1\)卷积。因为使用了最小窗口，\(1\times 1\)卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，\(1\times 1\)卷积的主要计算发生在通道维上。展示了使用输入通道数为3、输出通道数为2的\(1\times 1\)卷积核的互相关计算。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么\(1\times 1\)卷积层的作用与全连接层等价。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.3_conv_1x1.svg" alt="使用输入通道数为3、输出通道数为2的1 x 1卷积核的互相关计算。输入和输出具有相同的高和宽"></p>
<p>使用全连接层中的矩阵乘法来实现\(1\times 1\)卷积。这里需要在矩阵乘法运算前后对数据形状做一些调整。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d_multi_in_out_1x1</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line">    X = tf.reshape(X,(c_i, h * w))</span><br><span class="line">    K = tf.reshape(K,(c_o, c_i))</span><br><span class="line">    Y = tf.matmul(K, X)</span><br><span class="line">    <span class="keyword">return</span> tf.reshape(Y, (c_o, h, w))</span><br></pre></td></tr></table></figure>
<p>在之后的模型里我们将会看到\(1\times 1\)卷积层被当作保持高和宽维度形状不变的全连接层使用。于是，可以通过调整网络层之间的通道数来控制模型复杂度。</p>
<h2 id="xiao-jie-1">小结</h2>
<ol>
<li>使用多通道可以拓展卷积层的模型参数。</li>
<li>假设将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么\(1\times 1\)卷积层的作用与全连接层等价。</li>
<li>\(1\times 1\)卷积层通常用来调整网络层之间的通道数，并控制模型复杂度。</li>
</ol>
<h1 id="chi-hua-ceng">池化层</h1>
<p>在图像物体边缘检测应用中，构造卷积核从而精确地找到了像素变化的位置。设任意二维数组<code>X</code>的<code>i</code>行<code>j</code>列的元素为<code>X[i, j]</code>。如果我们构造的卷积核输出<code>Y[i, j]=1</code>，那么说明输入中<code>X[i, j]</code>和<code>X[i, j+1]</code>数值不一样。这可能意味着物体边缘通过这两个元素之间。但实际图像里，我们感兴趣的物体不会总出现在固定位置：即使我们连续拍摄同一个物体也极有可能出现像素位置上的偏移。这会导致同一个边缘对应的输出可能出现在卷积输出<code>Y</code>中的不同位置，进而对后面的模式识别造成不便。</p>
<p>池化层的提出是为了缓解卷积层对位置的过度敏感性。</p>
<h2 id="2-dimentional-max-pooling-and-avg-pooling">2-dimentional max_pooling and avg_pooling</h2>
<p>同卷积层一样，池化层每次对输入数据的一个固定形状窗口中的元素计算输出。不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。在二维最大池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当池化窗口滑动到某一位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.4_pooling.svg" alt="池化窗口形状为2 x 2的最大池化"></p>
<p>上图展示了池化窗口形状为\(2\times 2\)的最大池化，阴影部分为第一个输出元素及其计算所使用的输入元素。输出数组的高和宽分别为2，其中的4个元素由取最大值运算\(\text{max}\)得出：</p>
<p>\[
\max(0,1,3,4)=4,\\
\max(1,2,4,5)=5,\\
\max(3,4,6,7)=7,\\
\max(4,5,7,8)=8.\\
\]</p>
<p>二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。池化窗口形状为\(p \times q\)的池化层称为\(p \times q\)池化层，其中的池化运算叫作\(p \times q\)池化。</p>
<p>在物体边缘检测的例子中，将卷积层的输出作为\(2\times 2\)最大池化的输入。设该卷积层输入是<code>X</code>、池化层输出为<code>Y</code>。无论是<code>X[i, j]</code>和<code>X[i, j+1]</code>值不同，还是<code>X[i, j+1]</code>和<code>X[i, j+2]</code>不同，池化层输出均有<code>Y[i, j]=1</code>。也就是说，使用\(2\times 2\)最大池化层时，只要卷积层识别的模式在高和宽上移动不超过一个元素，我们依然可以将它检测出来。</p>
<p>下面把池化层的前向计算实现在<code>pool2d</code>函数里。它跟二维卷积层里<code>corr2d</code>函数非常类似，唯一的区别在计算输出<code>Y</code>上。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool2d</span><span class="params">(X, pool_size, mode=<span class="string">'max'</span>)</span>:</span></span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = tf.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w +<span class="number">1</span>))</span><br><span class="line">    Y = tf.Variable(Y)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">'max'</span>:</span><br><span class="line">                Y[i,j].assign(tf.reduce_max(X[i:i+p_h, j:j+p_w]))</span><br><span class="line">            <span class="keyword">elif</span> mode ==<span class="string">'avg'</span>:</span><br><span class="line">                Y[i,j].assign(tf.reduce_mean(X[i:i+p_h, j:j+p_w]))</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<h2 id="padding-and-stride">padding and stride</h2>
<p>同卷积层一样，池化层也可以在输入的高和宽两侧的填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。下面将通过<code>nn</code>模块里的二维最大池化层MaxPool2D来演示池化层填充和步幅的工作机制。我们先构造一个形状为(1, 1, 4, 4)的输入数据，前两个维度分别是批量和通道。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#tensorflow default data_format == 'channels_last'</span></span><br><span class="line"><span class="comment">#so (1,4,4,1) instead of (1,1,4,4)</span></span><br><span class="line">X = tf.reshape(tf.constant(range(<span class="number">16</span>)), (<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>默认情况下，<code>MaxPool2D</code>实例里步幅和池化窗口形状相同。下面使用形状为(3, 3)的池化窗口，默认获得形状为(3, 3)的步幅。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pool2d = tf.keras.layers.MaxPool2D(pool_size=[<span class="number">3</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#I guess no custom padding settings in keras.layers?</span></span><br><span class="line">pool2d = tf.keras.layers.MaxPool2D(pool_size=[<span class="number">3</span>,<span class="number">3</span>],padding=<span class="string">'same'</span>,strides=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="multi-channels">multi-channels</h2>
<p>在处理多通道输入数据时，池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加。这意味着池化层的输出通道数与输入通道数相等。</p>
<h2 id="xiao-jie-2">小结</h2>
<ol>
<li>最大池化和平均池化分别取池化窗口中输入元素的最大值和平均值作为输出。</li>
<li>池化层的一个主要作用是缓解卷积层对位置的过度敏感性。</li>
<li>可以指定池化层的填充和步幅。</li>
<li>池化层的输出通道数跟输入通道数相同。</li>
</ol>
<h1 id="juan-ji-shen-jing-wang-luo">卷积神经网络</h1>
<h2 id="le-net-juan-ji-shen-jing-wang-luo">LeNet:卷积神经网络</h2>
<p>卷积层尝试解决这两个问题。</p>
<ol>
<li>一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；</li>
<li>另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</li>
</ol>
<p>卷积神经网络就是含卷积层的网络。早期用来识别手写数字图像的卷积神经网络：LeNet [1]。这个名字来源于LeNet论文的第一作者Yann LeCun。LeNet展示了通过梯度下降训练卷积神经网络可以达到手写数字识别在当时最先进的结果。这个奠基性的工作第一次将卷积神经网络推上舞台，为世人所知。</p>
<h3 id="le-net-mo-xing">LeNet模型</h3>
<p><img src="/2019/04/17/deeplearning/cnn/1_1TI1aGBZ4dybR6__DI9dzA.png" alt="LeNet"></p>
<p>LeNet分为卷积层块和全连接层块两个部分。</p>
<p>卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用\(5\times 5\)的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为\(2\times 2\)，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。</p>
<p>卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。</p>
<p>通过<code>Sequential</code>类来实现LeNet模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Conv2D(filters=<span class="number">6</span>,kernel_size=<span class="number">5</span>,activation=<span class="string">'sigmoid'</span>,input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)),</span><br><span class="line">    tf.keras.layers.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(filters=<span class="number">16</span>,kernel_size=<span class="number">5</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">    tf.keras.layers.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">120</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">84</span>,activation=<span class="string">'sigmoid'</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>,activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>可以看到，在卷积层块中输入的高和宽在逐层减小。卷积层由于使用高和宽均为5的卷积核，从而将高和宽分别减小4，而池化层则将高和宽减半，但通道数则从1增加到16。全连接层则逐层减少输出个数，直到变成图像的类别数10。</p>
<h3 id="xiao-jie-3">小结</h3>
<ol>
<li>卷积神经网络就是含卷积层的网络。</li>
<li>LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。</li>
</ol>
<h2 id="alex-net-shen-du-juan-ji-wang-luo">AlexNet:深度卷积网络</h2>
<p>在LeNet提出后的将近20年里，神经网络一度被其他机器学习方法超越，如支持向量机。虽然LeNet可以在早期的小数据集上取得好的成绩，但是在更大的真实数据集上的表现并不尽如人意。一方面，神经网络计算复杂。虽然20世纪90年代也有过一些针对神经网络的加速硬件，但并没有像之后GPU那样大量普及。因此，训练一个多通道、多层和有大量参数的卷积神经网络在当年很难完成。另一方面，当年研究者还没有大量深入研究参数初始化和非凸优化算法等诸多领域，导致复杂的神经网络的训练通常较困难。</p>
<p>神经网络可以直接基于图像的原始像素进行分类。这种称为端到端（end-to-end）的方法节省了很多中间步骤。然而，在很长一段时间里更流行的是研究者通过勤劳与智慧所设计并生成的手工特征。这类图像分类研究的主要流程是：</p>
<ol>
<li>获取图像数据集；</li>
<li>使用已有的特征提取函数生成图像的特征；</li>
<li>使用机器学习模型对图像的特征分类。</li>
</ol>
<p>当时认为的机器学习部分仅限最后这一步。如果那时候跟机器学习研究者交谈，他们会认为机器学习既重要又优美。优雅的定理证明了许多分类器的性质。机器学习领域生机勃勃、严谨而且极其有用。然而，如果跟计算机视觉研究者交谈，则是另外一幅景象。他们会告诉你图像识别里“不可告人”的现实是：计算机视觉流程中真正重要的是数据和特征。也就是说，使用较干净的数据集和较有效的特征甚至比机器学习模型的选择对图像分类结果的影响更大。</p>
<h3 id="xue-xi-te-zheng-biao-shi">学习特征表示</h3>
<p>既然特征如此重要，它该如何表示呢？</p>
<p>在相当长的时间里，特征都是基于各式各样手工设计的函数从数据中提取的。事实上，不少研究者通过提出新的特征提取函数不断改进图像分类结果。这一度为计算机视觉的发展做出了重要贡献。</p>
<p>然而，另一些研究者则持异议。他们认为特征本身也应该由学习得来。他们还相信，为了表征足够复杂的输入，特征本身应该分级表示。持这一想法的研究者相信，多层神经网络可能可以学得数据的多级表征，并逐级表示越来越抽象的概念或模式。在多层神经网络中，图像的第一级的表示可以是在特定的位置和⻆度是否出现边缘；而第二级的表示说不定能够将这些边缘组合出有趣的模式，如花纹；在第三级的表示中，也许上一级的花纹能进一步汇合成对应物体特定部位的模式。这样逐级表示下去，最终，模型能够较容易根据最后一级的表示完成分类任务。需要强调的是，输入的逐级表示由多层模型中的参数决定，而这些参数都是学出来的。</p>
<p>尽管一直有一群执着的研究者不断钻研，试图学习视觉数据的逐级表征，然而很长一段时间里这些野心都未能实现。这其中有诸多因素值得我们一一分析。</p>
<h4 id="que-shi-yao-su-yi-shu-ju">缺失要素一：数据</h4>
<p>包含许多特征的深度模型需要大量的有标签的数据才能表现得比其他经典方法更好。限于早期计算机有限的存储和90年代有限的研究预算，大部分研究只基于小的公开数据集。例如，不少研究论文基于加州大学欧文分校（UCI）提供的若干个公开数据集，其中许多数据集只有几百至几千张图像。这一状况在2010年前后兴起的大数据浪潮中得到改善。特别是，2009年诞生的ImageNet数据集包含了1,000大类物体，每类有多达数千张不同的图像。这一规模是当时其他公开数据集无法与之相提并论的。ImageNet数据集同时推动计算机视觉和机器学习研究进入新的阶段，使此前的传统方法不再有优势。</p>
<h4 id="que-shi-yao-su-er-ying-jian">缺失要素二：硬件</h4>
<p>深度学习对计算资源要求很高。早期的硬件计算能力有限，这使训练较复杂的神经网络变得很困难。然而，通用GPU的到来改变了这一格局。很久以来，GPU都是为图像处理和计算机游戏设计的，尤其是针对大吞吐量的矩阵和向量乘法从而服务于基本的图形变换。值得庆幸的是，这其中的数学表达与深度网络中的卷积层的表达类似。通用GPU这个概念在2001年开始兴起，涌现出诸如OpenCL和CUDA之类的编程框架。这使得GPU也在2010年前后开始被机器学习社区使用。</p>
<h3 id="alex-net">AlexNet</h3>
<p>2012年，AlexNet横空出世。这个模型的名字来源于论文第一作者的姓名Alex Krizhevsky [2]。AlexNet使用了8层卷积神经网络，并以很大的优势赢得了ImageNet 2012图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。</p>
<p>AlexNet与LeNet的设计理念非常相似，但也有显著的区别。</p>
<p><strong>第一</strong>，与相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。下面来详细描述这些层的设计。</p>
<p>AlexNet第一层中的卷积窗口形状是\(11\times11\)。因为ImageNet中绝大多数图像的高和宽均比MNIST图像的高和宽大10倍以上，ImageNet图像的物体占用更多的像素，所以需要更大的卷积窗口来捕获物体。第二层中的卷积窗口形状减小到\(5\times5\)，之后全采用\(3\times3\)。此外，第一、第二和第五个卷积层之后都使用了窗口形状为\(3\times3\)、步幅为2的最大池化层。而且，AlexNet使用的卷积通道数也大于LeNet中的卷积通道数数十倍。</p>
<p>紧接着最后一个卷积层的是两个输出个数为4096的全连接层。这两个巨大的全连接层带来将近1 GB的模型参数。由于早期显存的限制，最早的AlexNet使用双数据流的设计使一个GPU只需要处理一半模型。幸运的是，显存在过去几年得到了长足的发展，因此通常我们不再需要这样的特别设计了。</p>
<p><strong>第二</strong>，AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。一方面，ReLU激活函数的计算更简单，例如它并没有sigmoid激活函数中的求幂运算。另一方面，ReLU激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度几乎为0，从而造成反向传播无法继续更新部分模型参数；而ReLU激活函数在正区间的梯度恒为1。因此，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，从而令模型无法得到有效训练。</p>
<p><strong>第三</strong>，AlexNet通过丢弃法来控制全连接层的模型复杂度。而LeNet并没有使用丢弃法。</p>
<p><strong>第四</strong>，AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。</p>
<p>下面实现稍微简化过的AlexNet。</p>
<p><img src="/2019/04/17/deeplearning/cnn/1_qyc21qM0oxWEuRaj-XJKcw.png" alt></p>
<p>建议采用GPU进行训练，需要使用tensorflow-gpu-2.0并设置memory_growth:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> gpu <span class="keyword">in</span> tf.config.experimental.list_physical_devices(<span class="string">'GPU'</span>):</span><br><span class="line">    tf.config.experimental.set_memory_growth(gpu, <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Conv2D(filters=<span class="number">96</span>,kernel_size=<span class="number">11</span>,strides=<span class="number">4</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(filters=<span class="number">256</span>,kernel_size=<span class="number">5</span>,padding=<span class="string">'same'</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(filters=<span class="number">384</span>,kernel_size=<span class="number">3</span>,padding=<span class="string">'same'</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(filters=<span class="number">384</span>,kernel_size=<span class="number">3</span>,padding=<span class="string">'same'</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(filters=<span class="number">256</span>,kernel_size=<span class="number">3</span>,padding=<span class="string">'same'</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">4096</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">4096</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>,activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h3 id="xiao-jie-4">小结</h3>
<ul>
<li>AlexNet跟LeNet结构类似，但使用了更多的卷积层和更大的参数空间来拟合大规模数据集ImageNet。它是浅层神经网络和深度神经网络的分界线。</li>
<li>虽然看上去AlexNet的实现比LeNet的实现也就多了几行代码而已，但这个观念上的转变和真正优秀实验结果的产生令学术界付出了很多年。</li>
</ul>
<h2 id="vgg-shi-yong-zhong-fu-yuan-su-de-wang-luo">VGG:使用重复元素的网络</h2>
<p>AlexNet在LeNet的基础上增加了3个卷积层。但AlexNet作者对它们的卷积窗口、输出通道数和构造顺序均做了大量的调整。虽然AlexNet指明了深度卷积神经网络可以取得出色的结果，但并没有提供简单的规则以指导后来的研究者如何设计新的网络。</p>
<p>VGG的名字来源于论文作者所在的实验室Visual Geometry Group。VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路。</p>
<h3 id="vgg-kuai">VGG块</h3>
<p>VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为\(3\times 3\)的卷积层后接上一个步幅为2、窗口形状为\(2\times 2\)的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用<code>vgg_block</code>函数来实现这个基础的VGG块，它可以指定卷积层的数量<code>num_convs</code>和输出通道数<code>num_channels</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_block</span><span class="params">(num_convs, num_channels)</span>:</span></span><br><span class="line">    blk = tf.keras.models.Sequential()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_convs):</span><br><span class="line">        blk.add(tf.keras.layers.Conv2D(num_channels,kernel_size=<span class="number">3</span>,</span><br><span class="line">                                    padding=<span class="string">'same'</span>,activation=<span class="string">'relu'</span>))</span><br><span class="line">    </span><br><span class="line">    blk.add(tf.keras.layers.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>
<h3 id="vgg-wang-luo">VGG网络</h3>
<p>与AlexNet和LeNet一样，VGG网络由卷积层模块后接全连接层模块构成。卷积层模块串联数个<code>vgg_block</code>，其超参数由变量<code>conv_arch</code>定义。该变量指定了每个VGG块里卷积层个数和输出通道数。全连接模块则跟AlexNet中的一样。</p>
<p><img src="/2019/04/17/deeplearning/cnn/14596362-cf346be06be6a51c.jpg.png" alt="VGG"></p>
<p>现在构造一个VGG网络。它有5个卷积块，前2块使用单卷积层，而后3块使用双卷积层。第一块的输出通道是64，之后每次对输出通道数翻倍，直到变为512。因为这个网络使用了8个卷积层和3个全连接层，所以经常被称为VGG-11。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br></pre></td></tr></table></figure>
<p>下面实现VGG-11:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg</span><span class="params">(conv_arch)</span>:</span></span><br><span class="line">    net = tf.keras.models.Sequential()</span><br><span class="line">    <span class="keyword">for</span> (num_convs, num_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        net.add(vgg_block(num_convs,num_channels))</span><br><span class="line">    net.add(tf.keras.models.Sequential([tf.keras.layers.Flatten(),</span><br><span class="line">             tf.keras.layers.Dense(<span class="number">4096</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">             tf.keras.layers.Dropout(<span class="number">0.5</span>),</span><br><span class="line">             tf.keras.layers.Dense(<span class="number">4096</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">             tf.keras.layers.Dropout(<span class="number">0.5</span>),</span><br><span class="line">             tf.keras.layers.Dense(<span class="number">10</span>,activation=<span class="string">'sigmoid'</span>)]))</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/deeplearning/cnn/2.png" alt></p>
<p>可以看到，每次我们将输入的高和宽减半，直到最终高和宽变成7后传入全连接层。与此同时，输出通道数每次翻倍，直到变成512。因为每个卷积层的窗口大小一样，所以每层的模型参数尺寸和计算复杂度与输入高、输入宽、输入通道数和输出通道数的乘积成正比。VGG这种高和宽减半以及通道翻倍的设计使得多数卷积层都有相同的模型参数尺寸和计算复杂度。</p>
<h3 id="xiao-jie-5">小结</h3>
<ol>
<li>VGG-11通过5个可以重复使用的卷积块来构造网络。根据每块里卷积层个数和输出通道数的不同可以定义出不同的VGG模型。</li>
</ol>
<h2 id="ni-n-wang-luo-zhong-de-wang-luo">NiN:网络中的网络</h2>
<p>LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深。网络中的网络（NiN）。它提出了另外一个思路，即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。</p>
<h3 id="ni-n-kuai">NiN块</h3>
<p>卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维。\(1\times 1\)卷积层可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。因此，NiN使用\(1\times 1\)卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。下图对比了NiN同AlexNet和VGG等网络在结构上的主要区别。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.8_nin-5032783.svg" alt="左图是AlexNet和VGG的网络结构局部，右图是NiN的网络结构局部"></p>
<p>NiN块是NiN中的基础块。它由一个卷积层加两个充当全连接层的\(1\times 1\)卷积层串联而成。其中第一个卷积层的超参数可以自行设置，而第二和第三个卷积层的超参数一般是固定的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nin_block</span><span class="params">(num_channels, kernel_size, strides, padding)</span>:</span></span><br><span class="line">    blk = tf.keras.models.Sequential()</span><br><span class="line">    blk.add(tf.keras.layers.Conv2D(num_channels, kernel_size,</span><br><span class="line">                                   strides=strides, padding=padding, activation=<span class="string">'relu'</span>)) </span><br><span class="line">    blk.add(tf.keras.layers.Conv2D(num_channels, kernel_size=<span class="number">1</span>,activation=<span class="string">'relu'</span>)) </span><br><span class="line">    blk.add(tf.keras.layers.Conv2D(num_channels, kernel_size=<span class="number">1</span>,activation=<span class="string">'relu'</span>))    </span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>
<h3 id="ni-n-mo-xing">NiN模型</h3>
<p>NiN是在AlexNet问世不久后提出的。它们的卷积层设定有类似之处。NiN使用卷积窗口形状分别为\(11\times 11\)、\(5\times 5\)和\(3\times 3\)的卷积层，相应的输出通道数也与AlexNet中的一致。每个NiN块后接一个步幅为2、窗口形状为\(3\times 3\)的最大池化层。</p>
<p>除使用NiN块以外，NiN还有一个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使用了输出通道数等于标签类别数的NiN块，然后使用全局平均池化层对<strong>每个通道</strong>中所有元素求平均并直接用于分类。这里的全局平均池化层即窗口形状等于输入空间维形状的平均池化层。NiN的这个设计的好处是可以显著减小模型参数尺寸，从而缓解过拟合。然而，该设计有时会造成获得有效模型的训练时间的增加。</p>
<p><img src="/2019/04/17/deeplearning/cnn/v2-406a9c2c203abc36d55af1bdbb7002f7_r.jpg" alt="nin"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = tf.keras.models.Sequential()</span><br><span class="line">net.add(nin_block(<span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, padding=<span class="string">'valid'</span>))</span><br><span class="line">net.add(tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>))</span><br><span class="line">net.add(nin_block(<span class="number">256</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="string">'same'</span>))</span><br><span class="line">net.add(tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>))</span><br><span class="line">net.add(nin_block(<span class="number">384</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="string">'same'</span>))</span><br><span class="line">net.add(tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>))</span><br><span class="line">net.add(tf.keras.layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">net.add(nin_block(<span class="number">10</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="string">'same'</span>))</span><br><span class="line">net.add(tf.keras.layers.GlobalAveragePooling2D())</span><br><span class="line">net.add(tf.keras.layers.Flatten())</span><br></pre></td></tr></table></figure>
<h3 id="xiao-jie-6">小结</h3>
<ol>
<li>NiN重复使用由卷积层和代替全连接层的\(1\times 1\)卷积层构成的NiN块来构建深层网络。</li>
<li>NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块和全局平均池化层。</li>
<li>NiN的以上设计思想影响了后面一系列卷积神经网络的设计。</li>
</ol>
<h2 id="goog-le-net-han-bing-xing-lian-jie-de-wang-luo">GoogLeNet:含并行连接的网络</h2>
<p>在2014年的ImageNet图像识别挑战赛中，一个名叫GoogLeNet的网络结构大放异彩 。它虽然在名字上向LeNet致敬，但在网络结构上已经很难看到LeNet的影子。GoogLeNet吸收了NiN中网络串联网络的思想，并在此基础上做了很大改进。</p>
<h3 id="inception-kuai">Inception 块</h3>
<p>GoogLeNet中的基础卷积块叫作Inception块，得名于同名电影《盗梦空间》（Inception）。与NiN块相比，这个基础块在结构上更加复杂，如图所示。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.9_inception-5033243.svg" alt="Inception块的结构"></p>
<p>由上图可以看出，Inception块里有4条并行的线路。前3条线路使用窗口大小分别是\(1\times 1\)、\(3\times 3\)和\(5\times 5\)的卷积层来抽取不同空间尺寸下的信息，其中中间2个线路会对输入先做\(1\times 1\)卷积来减少输入通道数，以降低模型复杂度。第四条线路则使用\(3\times 3\)最大池化层，后接\(1\times 1\)卷积层来改变通道数。4条线路都使用了合适的填充来使输入与输出的高和宽一致。最后我们将每条线路的输出在通道维上连结，并输入接下来的层中去。</p>
<p>Inception块中可以自定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,c1, c2, c3, c4)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="comment"># 线路1，单1 x 1卷积层</span></span><br><span class="line">        self.p1_1 = tf.keras.layers.Conv2D(c1, kernel_size=<span class="number">1</span>, activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>)</span><br><span class="line">        <span class="comment"># 线路2，1 x 1卷积层后接3 x 3卷积层</span></span><br><span class="line">        self.p2_1 = tf.keras.layers.Conv2D(c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.p2_2 = tf.keras.layers.Conv2D(c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="string">'same'</span>,</span><br><span class="line">                              activation=<span class="string">'relu'</span>)</span><br><span class="line">        <span class="comment"># 线路3，1 x 1卷积层后接5 x 5卷积层</span></span><br><span class="line">        self.p3_1 = tf.keras.layers.Conv2D(c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.p3_2 = tf.keras.layers.Conv2D(c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="string">'same'</span>,</span><br><span class="line">                              activation=<span class="string">'relu'</span>)</span><br><span class="line">        <span class="comment"># 线路4，3 x 3最大池化层后接1 x 1卷积层</span></span><br><span class="line">        self.p4_1 = tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, padding=<span class="string">'same'</span>, strides=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = tf.keras.layers.Conv2D(c4, kernel_size=<span class="number">1</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        p1 = self.p1_1(x)</span><br><span class="line">        p2 = self.p2_2(self.p2_1(x))</span><br><span class="line">        p3 = self.p3_2(self.p3_1(x))</span><br><span class="line">        p4 = self.p4_2(self.p4_1(x))</span><br><span class="line">        <span class="keyword">return</span> tf.concat([p1, p2, p3, p4], axis=<span class="number">-1</span>)  <span class="comment"># 在通道维上连结输出</span></span><br></pre></td></tr></table></figure>
<h3 id="goog-le-net-mo-xing">GoogLeNet模型</h3>
<p>GoogLeNet跟VGG一样，在主体卷积部分中使用5个模块，每个模块之间使用步幅为2的\(3\times 3\)最大池化层来减小输出高宽。第一模块使用一个64通道的\(7\times 7\)卷积层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b1 = tf.keras.models.Sequential()</span><br><span class="line">b1.add(tf.keras.layers.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">b1.add(tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>))</span><br></pre></td></tr></table></figure>
<p>第二模块使用2个卷积层：首先是64通道的\(1\times 1\)卷积层，然后是将通道增大3倍的\(3\times 3\)卷积层。它对应Inception块中的第二条线路。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b2 = tf.keras.models.Sequential()</span><br><span class="line">b2.add(tf.keras.layers.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">1</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">b2.add(tf.keras.layers.Conv2D(<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">b2.add(tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>))</span><br></pre></td></tr></table></figure>
<p>第三模块串联2个完整的Inception块。第一个Inception块的输出通道数为\(64+128+32+32=256\)，其中4条线路的输出通道数比例为\(64:128:32:32=2:4:1:1\)。其中第二、第三条线路先分别将输入通道数减小至\(96/192=1/2\)和\(16/192=1/12\)后，再接上第二层卷积层。第二个Inception块输出通道数增至\(128+192+96+64=480\)，每条线路的输出通道数之比为\(128:192:96:64 = 4:6:3:2\)。其中第二、第三条线路先分别将输入通道数减小至\(128/256=1/2\)和\(32/256=1/8\)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b3 = tf.keras.models.Sequential()</span><br><span class="line">b3.add(Inception(<span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>))</span><br><span class="line">b3.add(Inception(<span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>))</span><br><span class="line">b3.add(tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>))</span><br></pre></td></tr></table></figure>
<p>第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是\(192+208+48+64=512\)、\(160+224+64+64=512\)、\(128+256+64+64=512\)、\(112+288+64+64=528\)和\(256+320+128+128=832\)。这些线路的通道数分配和第三模块中的类似，首先含\(3\times 3\)卷积层的第二条线路输出最多通道，其次是仅含\(1\times 1\)卷积层的第一条线路，之后是含\(5\times 5\)卷积层的第三条线路和含\(3\times 3\)最大池化层的第四条线路。其中第二、第三条线路都会先按比例减小通道数。这些比例在各个Inception块中都略有不同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b4 = tf.keras.models.Sequential()</span><br><span class="line">b4.add(Inception(<span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>))</span><br><span class="line">b4.add(Inception(<span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>))</span><br><span class="line">b4.add(Inception(<span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>))</span><br><span class="line">b4.add(Inception(<span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>))</span><br><span class="line">b4.add(Inception(<span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>))</span><br><span class="line">b4.add(tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>))</span><br></pre></td></tr></table></figure>
<p>第五模块有输出通道数为\(256+320+128+128=832\)和\(384+384+128+128=1024\)的两个Inception块。其中每条线路的通道数的分配思路和第三、第四模块中的一致，只是在具体数值上有所不同。需要注意的是，第五模块的后面紧跟输出层，该模块同NiN一样使用全局平均池化层来将每个通道的高和宽变成1。最后我们将输出变成二维数组后接上一个输出个数为标签类别数的全连接层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b5 = tf.keras.models.Sequential()</span><br><span class="line">b5.add(Inception(<span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>))</span><br><span class="line">b5.add(Inception(<span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>))</span><br><span class="line">b5.add(tf.keras.layers.GlobalAvgPool2D())</span><br><span class="line"></span><br><span class="line">net = tf.keras.models.Sequential([b1, b2, b3, b4, b5, tf.keras.layers.Dense(<span class="number">10</span>)])</span><br></pre></td></tr></table></figure>
<p>GoogLeNet模型的计算复杂，而且不如VGG那样便于修改通道数。</p>
<h3 id="xiao-jie-7">小结</h3>
<ol>
<li>Inception块相当于一个有4条线路的子网络。它通过不同窗口形状的卷积层和最大池化层来并行抽取信息，并使用\(1\times 1\)卷积层减少通道数从而降低模型复杂度。</li>
<li>GoogLeNet将多个设计精细的Inception块和其他层串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。</li>
<li>GoogLeNet和它的后继者们一度是ImageNet上最高效的模型之一：在类似的测试精度下，它们的计算复杂度往往更低。</li>
</ol>
<h2 id="res-net-can-chai-wang-luo">ResNet:残差网络</h2>
<p>先思考一个问题：对神经网络模型添加新的层，充分训练后的模型是否只可能更有效地降低训练误差？理论上，原模型解的空间只是新模型解的空间的子空间。也就是说，如果能将新添加的层训练成恒等映射\(f(x) = x\)，新模型和原模型将同样有效。由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。然而在实践中，添加过多的层后训练误差往往不降反升。即使利用批量归一化带来的数值稳定性使训练深层模型更加容易，该问题仍然存在。针对这一问题，何恺明等人提出了残差网络（ResNet）。它在2015年的ImageNet图像识别挑战赛夺魁，并深刻影响了后来的深度神经网络的设计。</p>
<h3 id="can-chai-kuai">残差块</h3>
<p>让我们聚焦于神经网络局部。如下图所示，设输入为\(\boldsymbol{x}\)。假设希望学出的理想映射为\(f(\boldsymbol{x})\)，从而作为图上方激活函数的输入。左图虚线框中的部分需要直接拟合出该映射\(f(\boldsymbol{x})\)，而右图虚线框中的部分则需要拟合出有关恒等映射的残差映射\(f(\boldsymbol{x})-\boldsymbol{x}\)。残差映射在实际中往往更容易优化。以恒等映射作为希望学出的理想映射\(f(\boldsymbol{x})\)。只需将图中右图虚线框内上方的加权运算（如仿射）的权重和偏差参数学成0，那么\(f(\boldsymbol{x})\)即为恒等映射。实际中，当理想映射\(f(\boldsymbol{x})\)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。右图也是ResNet的基础块，即残差块。在残差块中，输入可通过跨层的数据线路更快地向前传播。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.11_residual-block.svg" alt="普通的网络结构（左）与加入残差连接的网络结构（右）"></p>
<p>ResNet沿用了VGG全\(3\times 3\)卷积层的设计。残差块里首先有2个有相同输出通道数的\(3\times 3\)卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。然后将输入跳过这两个卷积运算后直接加在最后的ReLU激活函数前。这样的设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的\(1\times 1\)卷积层来将输入变换成需要的形状后再做相加运算。</p>
<p>残差块的实现如下。它可以设定输出通道数、是否使用额外的\(1\times 1\)卷积层来修改通道数以及卷积层的步幅。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,activations</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_channels, use_1x1conv=False, strides=<span class="number">1</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Residual, self).__init__(**kwargs)</span><br><span class="line">        self.conv1 = layers.Conv2D(num_channels,</span><br><span class="line">                                   padding=<span class="string">'same'</span>,</span><br><span class="line">                                   kernel_size=<span class="number">3</span>,</span><br><span class="line">                                   strides=strides)</span><br><span class="line">        self.conv2 = layers.Conv2D(num_channels, kernel_size=<span class="number">3</span>,padding=<span class="string">'same'</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = layers.Conv2D(num_channels,</span><br><span class="line">                                       kernel_size=<span class="number">1</span>,</span><br><span class="line">                                       strides=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = layers.BatchNormalization()</span><br><span class="line">        self.bn2 = layers.BatchNormalization()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        Y = activations.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        <span class="keyword">return</span> activations.relu(Y + X)</span><br></pre></td></tr></table></figure>
<h3 id="res-net-mo-xing">ResNet模型</h3>
<p>ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的\(7\times 7\)卷积层后接步幅为2的\(3\times 3\)的最大池化层。不同之处在于ResNet每个卷积层后增加的批量归一化层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = tf.keras.models.Sequential(</span><br><span class="line">    [layers.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>),</span><br><span class="line">    layers.BatchNormalization(), layers.Activation(<span class="string">'relu'</span>),</span><br><span class="line">    layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>)])</span><br></pre></td></tr></table></figure>
<p>一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResnetBlock</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,num_channels, num_residuals, first_block=False,**kwargs)</span>:</span></span><br><span class="line">        super(ResnetBlock, self).__init__(**kwargs)</span><br><span class="line">        self.listLayers=[]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_residuals):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">                self.listLayers.append(Residual(num_channels, use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.listLayers.append(Residual(num_channels))      </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.listLayers.layers:</span><br><span class="line">            X = layer(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<p>接着为ResNet加入所有残差块。这里每个模块使用两个残差块。</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">class ResNet(tf.keras.Model):</span><br><span class="line">    def __init__(self,num_blocks,**kwargs):</span><br><span class="line">        super(ResNet, self).__init__(**kwargs)</span><br><span class="line">        self.<span class="attribute">conv</span>=layers.Conv2D(64, <span class="attribute">kernel_size</span>=7, <span class="attribute">strides</span>=2, <span class="attribute">padding</span>=<span class="string">'same'</span>)</span><br><span class="line">        self.<span class="attribute">bn</span>=layers.BatchNormalization()</span><br><span class="line">        self.<span class="attribute">relu</span>=layers.Activation('relu')</span><br><span class="line">        self.<span class="attribute">mp</span>=layers.MaxPool2D(pool_size=3, <span class="attribute">strides</span>=2, <span class="attribute">padding</span>=<span class="string">'same'</span>)</span><br><span class="line">        self.<span class="attribute">resnet_block1</span>=ResnetBlock(64,num_blocks[0], <span class="attribute">first_block</span>=<span class="literal">True</span>)</span><br><span class="line">        self.<span class="attribute">resnet_block2</span>=ResnetBlock(128,num_blocks[1])</span><br><span class="line">        self.<span class="attribute">resnet_block3</span>=ResnetBlock(256,num_blocks[2])</span><br><span class="line">        self.<span class="attribute">resnet_block4</span>=ResnetBlock(512,num_blocks[3])</span><br><span class="line">        self.<span class="attribute">gap</span>=layers.GlobalAvgPool2D()</span><br><span class="line">        self.<span class="attribute">fc</span>=layers.Dense(units=10,activation=tf.keras.activations.softmax)</span><br><span class="line"></span><br><span class="line">    def call(self, x):</span><br><span class="line">        <span class="attribute">x</span>=self.conv(x)</span><br><span class="line">        <span class="attribute">x</span>=self.bn(x)</span><br><span class="line">        <span class="attribute">x</span>=self.relu(x)</span><br><span class="line">        <span class="attribute">x</span>=self.mp(x)</span><br><span class="line">        <span class="attribute">x</span>=self.resnet_block1(x)</span><br><span class="line">        <span class="attribute">x</span>=self.resnet_block2(x)</span><br><span class="line">        <span class="attribute">x</span>=self.resnet_block3(x)</span><br><span class="line">        <span class="attribute">x</span>=self.resnet_block4(x)</span><br><span class="line">        <span class="attribute">x</span>=self.gap(x)</span><br><span class="line">        <span class="attribute">x</span>=self.fc(x)</span><br><span class="line">        return x</span><br><span class="line">    </span><br><span class="line"><span class="attribute">mynet</span>=ResNet([2,2,2,2])</span><br></pre></td></tr></table></figure>
<p>最后，与GoogLeNet一样，加入全局平均池化层后接上全连接层输出。</p>
<p>这里每个模块里有4个卷积层（不计算 1×1卷积层），加上最开始的卷积层和最后的全连接层，共计18层。这个模型通常也被称为ResNet-18。通过配置不同的通道数和模块里的残差块数可以得到不同的ResNet模型，例如更深的含152层的ResNet-152。虽然ResNet的主体架构跟GoogLeNet的类似，但ResNet结构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用。</p>
<h3 id="xiao-jie-8">小结</h3>
<ol>
<li>残差块通过跨层的数据通道从而能够训练出有效的深度神经网络。</li>
<li>ResNet深刻影响了后来的深度神经网络的设计。</li>
</ol>
<h2 id="dense-net-chou-mi-lian-jie-wang-luo">DenseNet:稠密连接网络</h2>
<p>ResNet中的跨层连接设计引申出了数个后续工作。DenseNet是其中的一个：稠密连接网络（DenseNet）。 它与ResNet的主要区别如图5.10所示。</p>
<p><img src="/2019/04/17/deeplearning/cnn/5.12_densenet.svg" alt="ResNet（左）与DenseNet（右）在跨层连接上的主要区别：使用相加和使用连结"></p>
<p>上图中将部分前后相邻的运算抽象为模块\(A\)和模块\(B\)。与ResNet的主要区别在于，DenseNet里模块\(B\)的输出不是像ResNet那样和模块\(A\)的输出相加，而是在通道维上连结。这样模块\(A\)的输出可以直接传入模块\(B\)后面的层。在这个设计里，模块\(A\)直接跟模块\(B\)后面的所有层连接在了一起。这也是它被称为“稠密连接”的原因。</p>
<p>DenseNet的主要构建模块是稠密块和过渡层。前者定义了输入和输出是如何连结的，后者则用来控制通道数，使之不过大。</p>
<h3 id="chou-mi-kuai">稠密块</h3>
<p>DenseNet使用了ResNet改良版的“批量归一化、激活和卷积”结构，首先在<code>BottleNeck</code>函数里实现这个结构。在前向计算时，将每块的输入和输出在通道维上连结。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BottleNeck</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, growth_rate, drop_rate)</span>:</span></span><br><span class="line">        super(BottleNeck, self).__init__()</span><br><span class="line">        self.bn1 = tf.keras.layers.BatchNormalization()</span><br><span class="line">        self.conv1 = tf.keras.layers.Conv2D(filters=<span class="number">4</span> * growth_rate,</span><br><span class="line">                                            kernel_size=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                                            strides=<span class="number">1</span>,</span><br><span class="line">                                            padding=<span class="string">"same"</span>)</span><br><span class="line">        self.bn2 = tf.keras.layers.BatchNormalization()</span><br><span class="line">        self.conv2 = tf.keras.layers.Conv2D(filters=growth_rate,</span><br><span class="line">                                            kernel_size=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">                                            strides=<span class="number">1</span>,</span><br><span class="line">                                            padding=<span class="string">"same"</span>)</span><br><span class="line">        self.dropout = tf.keras.layers.Dropout(rate=drop_rate)</span><br><span class="line">        </span><br><span class="line">        self.listLayers = [self.bn1,</span><br><span class="line">                           tf.keras.layers.Activation(<span class="string">"relu"</span>),</span><br><span class="line">                           self.conv1,</span><br><span class="line">                           self.bn2,</span><br><span class="line">                           tf.keras.layers.Activation(<span class="string">"relu"</span>),</span><br><span class="line">                           self.conv2,</span><br><span class="line">                           self.dropout]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        y = x</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.listLayers.layers:</span><br><span class="line">            y = layer(y)</span><br><span class="line">        y = tf.keras.layers.concatenate([x,y], axis=<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<p>稠密块由多个<code>BottleNeck</code>组成，每块使用相同的输出通道数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseBlock</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_layers, growth_rate, drop_rate=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        super(DenseBlock, self).__init__()</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.growth_rate = growth_rate</span><br><span class="line">        self.drop_rate = drop_rate</span><br><span class="line">        self.listLayers = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers):</span><br><span class="line">            self.listLayers.append(BottleNeck(growth_rate=self.growth_rate, drop_rate=self.drop_rate))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.listLayers.layers:</span><br><span class="line">            x = layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率（growth rate）。</p>
<h3 id="guo-du-ceng">过渡层</h3>
<p>由于每个稠密块都会带来通道数的增加，使用过多则会带来过于复杂的模型。过渡层用来控制模型复杂度。它通过\(1\times1\)卷积层来减小通道数，并使用步幅为2的平均池化层减半高和宽，从而进一步降低模型复杂度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransitionLayer</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, out_channels)</span>:</span></span><br><span class="line">        super(TransitionLayer, self).__init__()</span><br><span class="line">        self.bn = tf.keras.layers.BatchNormalization()</span><br><span class="line">        self.conv = tf.keras.layers.Conv2D(filters=out_channels,</span><br><span class="line">                                           kernel_size=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                                           strides=<span class="number">1</span>,</span><br><span class="line">                                           padding=<span class="string">"same"</span>)</span><br><span class="line">        self.pool = tf.keras.layers.MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">                                              strides=<span class="number">2</span>,</span><br><span class="line">                                              padding=<span class="string">"same"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = self.bn(inputs)</span><br><span class="line">        x = tf.keras.activations.relu(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="dense-net-mo-xing">DenseNet模型</h3>
<p><img src="/2019/04/17/deeplearning/cnn/v2-acd8346dc4de74caac921b2b88088f1c_1440w.jpg" alt="Densenet"></p>
<p>DenseNet首先使用同ResNet一样的单卷积层和最大池化层。类似于ResNet接下来使用的4个残差块，DenseNet使用的是4个稠密块。同ResNet一样，可以设置每个稠密块使用多少个卷积层。这里设成4，从而与ResNet-18保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。</p>
<p>ResNet里通过步幅为2的残差块在每个模块之间减小高和宽。这里则使用过渡层来减半高和宽，并减半通道数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseNet</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_init_features, growth_rate, block_layers, compression_rate, drop_rate)</span>:</span></span><br><span class="line">        super(DenseNet, self).__init__()</span><br><span class="line">        self.conv = tf.keras.layers.Conv2D(filters=num_init_features,</span><br><span class="line">                                           kernel_size=(<span class="number">7</span>, <span class="number">7</span>),</span><br><span class="line">                                           strides=<span class="number">2</span>,</span><br><span class="line">                                           padding=<span class="string">"same"</span>)</span><br><span class="line">        self.bn = tf.keras.layers.BatchNormalization()</span><br><span class="line">        self.pool = tf.keras.layers.MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">                                              strides=<span class="number">2</span>,</span><br><span class="line">                                              padding=<span class="string">"same"</span>)</span><br><span class="line">        self.num_channels = num_init_features</span><br><span class="line">        self.dense_block_1 = DenseBlock(num_layers=block_layers[<span class="number">0</span>], growth_rate=growth_rate, drop_rate=drop_rate)</span><br><span class="line">        self.num_channels += growth_rate * block_layers[<span class="number">0</span>]</span><br><span class="line">        self.num_channels = compression_rate * self.num_channels</span><br><span class="line">        self.transition_1 = TransitionLayer(out_channels=int(self.num_channels))</span><br><span class="line">        self.dense_block_2 = DenseBlock(num_layers=block_layers[<span class="number">1</span>], growth_rate=growth_rate, drop_rate=drop_rate)</span><br><span class="line">        self.num_channels += growth_rate * block_layers[<span class="number">1</span>]</span><br><span class="line">        self.num_channels = compression_rate * self.num_channels</span><br><span class="line">        self.transition_2 = TransitionLayer(out_channels=int(self.num_channels))</span><br><span class="line">        self.dense_block_3 = DenseBlock(num_layers=block_layers[<span class="number">2</span>], growth_rate=growth_rate, drop_rate=drop_rate)</span><br><span class="line">        self.num_channels += growth_rate * block_layers[<span class="number">2</span>]</span><br><span class="line">        self.num_channels = compression_rate * self.num_channels</span><br><span class="line">        self.transition_3 = TransitionLayer(out_channels=int(self.num_channels))</span><br><span class="line">        self.dense_block_4 = DenseBlock(num_layers=block_layers[<span class="number">3</span>], growth_rate=growth_rate, drop_rate=drop_rate)</span><br><span class="line"></span><br><span class="line">        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()</span><br><span class="line">        self.fc = tf.keras.layers.Dense(units=<span class="number">10</span>,</span><br><span class="line">                                        activation=tf.keras.activations.softmax)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = self.conv(inputs)</span><br><span class="line">        x = self.bn(x)</span><br><span class="line">        x = tf.keras.activations.relu(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.dense_block_1(x)</span><br><span class="line">        x = self.transition_1(x)</span><br><span class="line">        x = self.dense_block_2(x)</span><br><span class="line">        x = self.transition_2(x)</span><br><span class="line">        x = self.dense_block_3(x)</span><br><span class="line">        x = self.transition_3(x,)</span><br><span class="line">        x = self.dense_block_4(x)</span><br><span class="line"></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">densenet</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> DenseNet(num_init_features=<span class="number">64</span>, growth_rate=<span class="number">32</span>, block_layers=[<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>], compression_rate=<span class="number">0.5</span>, drop_rate=<span class="number">0.5</span>)</span><br><span class="line">mynet=densenet()</span><br></pre></td></tr></table></figure>
<h3 id="xiao-jie-9">小结</h3>
<ol>
<li>在跨层连接上，不同于ResNet中将输入与输出相加，DenseNet在通道维上连结输入与输出。</li>
<li>DenseNet的主要构建模块是稠密块和过渡层。</li>
</ol>
]]></content>
      <categories>
        <category>技术/深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>信息论</title>
    <url>/2018/07/19/math/information_theory/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2018/07/19/math/information_theory/timg.jpeg" alt></p>
<a id="more"></a>
<h1 id="xin-xi-shang">信息熵</h1>
<h2 id="xin-xi">信息</h2>
<p>信息量可以被看成在学习 <code>x</code> 的值的时候的“惊讶程度”。如果有人告诉我们一个相当不可能的时间发生了，我们收到的信息要多于我们被告知某个很可能发生的事件发生时收到的信息，如果我们知道某件事情一定会发生,那么我们就不会接收到信息。于是，我们对于信息内容的度量将依赖于概率分布 <code>p(x)</code> ，因此我们想要寻找一个函数 <code>h(x)</code> ，它是概率 <code>p(x)</code> 的单调递减函数，表达了信息的内容。 <code>h(·)</code> 的形式可以这样寻找：如果我们有两个不相关的事件 <code>x</code> 和 <code>y</code> ,那么我们观察到两个事件同时发生时获得的信息应该等于观察到事件各自发生时获得的信息之和，即 \(h(x, y) = h(x) + h(y)\) {采用概率分布的对数作为信息的量度的原因是其可加性。}。两个不相关事件是统计独立的，因此 p(x, y) = p(x)p(y) 。根据这两个关系，很容易看出 h(x) 一定与 p(x) 的对数有关，满足这两个条件的函数肯定是负对数形式。因此，我们有<br>
\[
h(x) = -log_2 p(x) \tag{1.1}
\]</p>
<blockquote>
<p>负号确保了信息一定是正数或者是零。注意，低概率事件 x 对应于高的信息量。</p>
</blockquote>
<p>假设一个发送者想传输一个随机变量的值给接收者。这个过程中,他们传输的平均信息量通可以通过求公式<code>1.1</code>关于概率分布 <code>p(x)</code> 的期望得到。即事件的概率分布和每个事件的信息量构成了一个随机变量，这个随机变量的均值（即期望）就是这个分布产生的信息量的平均值（即<strong>熵</strong>）。</p>
<p>\[
H[x]=-\sum_{x} p(x) \ln p(x)
\]</p>
<h2 id="shang-de-ding-yi">熵的定义</h2>
<p>如果\(X\)是一个离散型随机变量，其概率分布为:\(p(x) = P(X=x), x \in X.\) \(X\)的熵\(H(X)\)为:<br>
\[
H(X) = -\sum_{x \in X} {p(x)log_2 p(x)} \tag{1}
\]<br>
其中，约定\(0log0=0.\) \(H(X)\)也可以写为\(H(p)\).通常熵的单位为二进制，比特(bit)。</p>
<p>依据<code>Boltzmann's H-theorem</code>，香农把随机变量 <code>X</code> 的熵值 \(H\)（希腊字母<code>Eta</code>）定义如下，其值域为 \({x_1, ...,x_n}\)：<br>
\[
\mathrm{H}(X)=\mathrm{E}[\mathrm{I}(X)]=\mathrm{E}[-\ln (\mathrm{P}(X))]
\]<br>
其中， <code>P</code> 为 <code>X</code> 的概率质量函数，<code>E</code> 为期望函数，而 <code>I(X)</code> 是<code>X</code> 的信息量（又称为自信息）。<code>I(X)</code> 本身是个随机变数。当取自有限的样本时，熵的公式可以表示为：</p>
<p>\[
\mathrm{H}(X)=\sum_{i} \mathrm{P}\left(x_{i}\right) \mathrm{I}\left(x_{i}\right)=-\sum_{i} \mathrm{P}\left(x_{i}\right) \log _{b} \mathrm{P}\left(x_{i}\right)
\]<br>
熵又称为自信息,表示信源 <code>X</code> 每发一个符号(不论发什么符号)所提供的平均信息量。</p>
<p>熵可以理解为不确定性的量度(或者说是多样性diversity的度量)，因为越随机的信源的熵越大。熵可以被视为描述一个随机变量的不确定性的数量。一个随机变量的熵越大,它的不确定性越大。那么,正确估计其值的可能性就越小。越不确定的随机变量越需要大的信息量用以确定其值。</p>
<h3 id="shang-de-dan-wei">熵的单位</h3>
<p>单位取决于定义用到对数的底。当<em>b</em> = 2，熵的单位是bit；当<em>b</em> = e，熵的单位是nat；而当<em>b</em> = 10,熵的单位是 Hart。</p>
<h3 id="shang-de-qu-zhi-fan-wei">熵的取值范围</h3>
<p>从定义式,可以看出,虽然信息熵经常被称为负熵,但是其取值恒为正,这是因为概率\(p(i)\)恒小于1。不过，由于信息的接受就是不肯定性的消除，即熵的消除，所以信息熵才常被人称作负熵。</p>
<p>熵的取值范围：\(0 \leq H(P) \leq log |X|\),其中\(|X|\)是<code>X</code>的取值个数。由于\(0\leq p(i) \leq 1\),因此上非负。当\(p(i)=1\)且其他的\(p(j \ne i)=0\)时，熵取最小值0。在概率归一化的限制下，使用拉格朗日乘数法可以找到熵的最大值。因此，需要最大化：<br>
\[
\tilde{H}=-\sum_{i} p\left(x_{i}\right) \ln p\left(x_{i}\right)+\lambda\left(\sum_{i} p\left(x_{i}\right)-1\right)
\]<br>
可以证明,当所有的 \(p(x_i)\)都相等,且值为 \(p(x_i) = \frac{1}{M}\) 时,熵取得最大值（当且仅当X的分布是均匀分布时右边的等号成立。即当X服从均匀分布时，熵最大）。其中, M 是状态\(x_i\)的总数。此时对应的熵值为 \(H = ln M\)。这个结果也可以通过 Jensen 不等式推导出来。</p>
<h2 id="shang-de-te-xing">熵的特性</h2>
<p>任何满足这些假设的熵的定义均正比以下形式：<br>
\[
-K \sum_{i=1}^{n} p_{i} \log \left(p_{i}\right)
\]<br>
其中，<code>K</code>是与选择的度量单位相对应的一个正比常数。下文中，\(p_i = Pr(X= x_i)\)且\(\mathrm{H}_{n}\left(p_{1}, \ldots, p_{n}\right)=\mathrm{H}(X)\)</p>
<p>###连续性</p>
<p>该量度应连续，概率值小幅变化只能引起熵的微小变化。</p>
<h3 id="dui-cheng-xing">对称性</h3>
<p>符号\(x_i\)重新排序后，该量度应不变:\(\mathrm{H}_{n}\left(p_{1}, p_{2}, \ldots\right)=\mathrm{H}_{n}\left(p_{2}, p_{1}, \ldots\right)\).</p>
<h3 id="ji-zhi-xing">极值性</h3>
<p>当所有符号等可能出现的情况下，熵达到最大值（所有可能的事件等概率时不确定性最高）.</p>
<p>\[
\mathrm{H}_{n}\left(p_{1}, \ldots, p_{n}\right) \leq \mathrm{H}_{n}\left(\frac{1}{n}, \ldots, \frac{1}{n}\right)=\log _{b}(n)
\]</p>
<p>等概率事件的熵应随符号的数量增加:<br>
\[
\mathrm{H}_{n}(\underbrace{\frac{1}{n}, \ldots, \frac{1}{n}}_{n})=\log _{b}(n)&lt;\log _{b}(n+1)=\mathrm{H}_{n+1}(\underbrace{\frac{1}{n+1}, \ldots, \frac{1}{n+1}}_{n+1})
\]</p>
<h3 id="ke-jia-xing">可加性</h3>
<p>熵的量与该过程如何被划分无关。</p>
<p>最后给出的这个函数关系刻画了一个系统与其子系统的熵的关系。如果子系统之间的相互作用是已知的，则可以通过子系统的熵来计算一个系统的熵。</p>
<p>给定<code>n</code>个均匀分布元素的集合，分为<code>k</code>个箱（子系统），每个里面有 \(b_1, \ldots, b_k\) 个元素，合起来的熵应等于系统的熵与各个箱子的熵的和，每个箱子的权重为在该箱中的概率。</p>
<p>对于正整数\(b_i\),其中,\(b_1 + \dots + b_k = n\)来说:<br>
\[
\mathrm{H}_{n}\left(\frac{1}{n}, \ldots, \frac{1}{n}\right)=\mathrm{H}_{k}\left(\frac{b_{1}}{n}, \ldots, \frac{b_{k}}{n}\right)+\sum_{i=1}^{k} \frac{b_{i}}{n} \mathrm{H}_{b_{i}}\left(\frac{1}{b_{i}}, \ldots, \frac{1}{b_{i}}\right)
\]</p>
<h3 id="jin-yi-bu-xing-zhi">进一步性质</h3>
<ol>
<li>
<p>增减概率为零的事件不改变熵<br>
\[
\mathrm{H}_{n+1}\left(p_{1}, \ldots, p_{n}, 0\right)=\mathrm{H}_{n}\left(p_{1}, \ldots, p_{n}\right)
\]</p>
</li>
<li>
<p>计算 (<em>X</em>,<em>Y</em>)得到的熵或信息量等于通过进行两个连续实验得到的信息：先计算<em>Y</em>的值，然后在知道<em>Y</em>的值条件下得出<em>X</em>的值：<br>
\[
\mathrm{H}(X, Y)=\mathrm{H}(X \mid Y)+\mathrm{H}(Y)=\mathrm{H}(Y \mid X)+\mathrm{H}(X)
\]</p>
</li>
<li>
<p>如果<code>X</code>和<code>Y</code>是两个独立实验，那么知道<code>Y</code>的值不影响我们对<code>X</code>值的认知（因为两者独立，所以互不影响）<br>
\[
\mathrm{H}(X \mid Y)=\mathrm{H}(X)
\]</p>
</li>
<li>
<p>两个事件同时发生的熵不大于每个事件单独发生的熵的总和，且仅当两个事件是独立的情况下相等。<br>
\[
\mathrm{H}(X, Y) \leq \mathrm{H}(X)+\mathrm{H}(Y)
\]</p>
</li>
</ol>
<h2 id="wei-fen-shang">微分熵</h2>
<h1 id="xiang-nong-shang-de-bian-xing">香农熵的变形</h1>
<h2 id="lian-he-shang">联合熵</h2>
<h2 id="tiao-jian-shang">条件熵</h2>
<h2 id="xiang-dui-shang-kl-ju-chi">相对熵( KL 距离)</h2>
<p>两个概率分布\(p(x)\)和\(q(x)\)的相对熵定义为：<br>
\[
D(p \| q)=\sum_{x \in X} p(x) \log \frac{p(x)}{q(x)}
\]<br>
约定\(0log(0/q)=0,plog(p/0)=\infty\).</p>
<p>可以把 <code>Kullback-Leibler</code> 散度（KL 散度之所以不说距离，是因为不满足对称性和三角形法则）。看做两个分布 <code>p(x)</code> 和 <code>q(x)</code> 之间不相似程度的度量。相对熵常被用以衡量两个随机分布的差距。当两个随机分布相同时,其相对熵为0。当两个随机分布的差别增加时,其相对熵也增加。当<code>q=p</code>时，该度量的结果是0，而其它度量的结果为正值。直观上，它度量了使用q而不是p的压缩损失（以二进制）的程度。</p>
<p><img src="/2018/07/19/math/information_theory/image-20200719225308164.png" alt></p>
<p>假设数据通过未知分布 <code>p(x)</code> 生成,我们想要对 <code>p(x)</code> 建模。可以试着使用一些参数分布 \(q(x|\theta)\) 来近似这个分布。 \(q(x|\theta)\) 由可调节的参数 \(\theta\) 控制(例如一个多元高斯分布)。一种确定 θ 的方式是最小化 \(p(x)\) 和 \(q(x|\theta)\) 之间关于 \(\theta\) 的 Kullback-Leibler 散度。我们不能直接这么做,因为我们不知道 <code>p(x)</code> 。但是,假设我们已经观察到了服从分布 <code>p(x)</code> 的有限数量的训练点 \(x_n\) ,其中 \(n = 1,\ldots , N\) 。那么,关于 \(p(x)\) 的期望就可以通过这些点的有限加和,使用下面公式来近似,即：<br>
\[
\mathrm{KL}(p \| q) \simeq \frac{1}{N} \sum_{n=1}^{N}\left\{-\ln q\left(\boldsymbol{x}_{n} \mid \boldsymbol{\theta}\right)+\ln p\left(\boldsymbol{x}_{n}\right)\right\}
\]<br>
公式右侧的第二项与\(\theta\)无关,第一项是使用训练集估计的分布 \(q(x|\theta)\)下的 \(\theta\) 的负对数似然函数。因此我们看到,最小化 <code>Kullback-Leibler</code> 散度等价于最大化似然函数。</p>
<h2 id="jiao-cha-shang">交叉熵</h2>
<p>如果一个随机变量 \(X \sim p(x)\),\(q(x)\)为用于近似\(p(x)\)的概率分布,那么,随机变量 <code>X</code> 和模型 <code>q</code> 之间的交叉熵定义为:<br>
\[
\begin{aligned}
H(X, q) &amp;=H(X)+D(p \| q) \\
&amp;=-\sum_{x} p(x) \log q(x)
\end{aligned}
\]<br>
交叉熵的概念用以衡量估计模型与真实概率分布之间的差异。</p>
<h3 id="kun-huo-du">困惑度</h3>
<p>在设计语言模型时,通常用困惑度来代替交叉熵衡量语言模型的好坏。困惑度是用在自然语言处理领域（NLP）中，衡量语言模型好坏的指标。它主要是根据每个词来估计一句话出现的概率，并用句子长度作normalize。</p>
<p>困惑度是交叉熵的指数形式。具体的：</p>
<p>将一个sentence看做一个随机变量，\(W=\left\{w_{0}, w_{1}, \ldots, w_{n}\right\}\),这里假定是有限长度n，那么它对应的熵为：<br>
\[
H\left(w_{1}, w_{2}, \ldots, w_{n}\right)=-\log _{w_{1}^{n} \in L} p\left(w_{1}^{n}\right) \log p\left(w_{1}^{n}\right)
\]<br>
对应的per-word 熵，也就是entroy rate为：<br>
\[
\frac{1}{n} H\left(w_{1}, w_{2}, \ldots, w_{n}\right)=-\frac{1}{n} \log _{w_{1}^{n} \in L} p\left(w_{1}^{n}\right) \log p\left(w_{1}^{n}\right)
\]<br>
引入交叉熵，真实分布<code>p</code>，通过语言模型生成的<code>sentence</code>，可以看作预测得到分布为<code>m</code>，那么交叉熵：<br>
\[
H(p, m)=-\frac{1}{n} \sum_{W \in L} p\left(w_{1}, w_{2}, \ldots, w_{n}\right) \log m\left(w_{1}, w_{2}, \ldots, w_{n}\right)
\]<br>
交叉熵的值越小，我们预测得到的分布也就越接近真实分布。因为训练样本已经给出，也就是p是定值，故 \(H(p,m)\) 可写成：<br>
\[
H(W)=-\frac{1}{N} \log P\left(w_{1} w_{2} \cdots w_{N}\right)
\]<br>
基于语言模型 \(M=P\left(w_{i} \mid w_{i-N+1} \cdots w_{i-1}\right)\)的困惑度perplexity可定义为交叉熵的指数形式：<br>
\[
\begin{aligned}
\operatorname{Perplexity}(W) &amp;=2^{H(W)} \\
&amp;=P\left(w_{1} w_{2} \ldots w_{N}\right)^{-\frac{1}{N}} \\
&amp;=\sqrt[N]{\frac{1}{P\left(w_{1} w_{2} \ldots w_{N}\right)}} \\
&amp;=\sqrt[N]{\prod_{i=1}^{N} \frac{1}{P\left(w_{i} \mid w_{1} \ldots w_{i-1}\right)}}
\end{aligned}
\]</p>
<p>由公式可知，**句子概率越大，语言模型越好，困惑度越小。**说模型的困惑度下降到90，可以直观地理解为，在模型生成一句话时下一个词有90个合理选择，可选词数越少，大致认为模型越准确。</p>
<p>通常使用困惑度来评价语言模型的好坏。特别地，</p>
<ul>
<li>最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；</li>
<li>最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；</li>
<li>基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。</li>
</ul>
<p>显然，任何一个有效模型的困惑度必须小于类别个数。</p>
<h2 id="hu-xin-xi">互信息</h2>
<p>如果 \((X, Y) \sim p(x, y)\), <code>X</code>, <code>Y</code> 之间的互信息 <code>I(X;Y)</code>定义为:<br>
\[
I(X;Y)=\sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}
\]<br>
互信息<code>I (X;Y)</code>取值为非负。当<code>X,Y</code>相互独立时，<code>I(X,Y)</code>最小为0。</p>
<blockquote>
<p>互信息实际上是更广泛的相对熵的特殊情形</p>
</blockquote>
<p>如果变量不是独立的,那么我们可以通过考察联合概率分布与边缘概率分布乘积之间的 <code>Kullback-Leibler</code> 散度来判断它们是否“接近”于相互独立。此时, Kullback-Leibler 散度为:<br>
\[
\begin{aligned}
I[\boldsymbol{x}, \boldsymbol{y}] &amp; \equiv \operatorname{KL}(p(\boldsymbol{x}, \boldsymbol{y}) \| p(\boldsymbol{x}) p(\boldsymbol{y})) \\
&amp;=-\iint p(\boldsymbol{x}, \boldsymbol{y}) \ln \left(\frac{p(\boldsymbol{x}) p(\boldsymbol{y})}{p(\boldsymbol{x}, \boldsymbol{y})}\right) \mathrm{d} \boldsymbol{x} \mathrm{d} \boldsymbol{y}
\end{aligned}
\]<br>
这被称为变量 x 和变量 y 之间的互信息。根据 <code>Kullback-Leibler</code> 散度的性质,我们看到 I[x, y] ≥ 0 ,当且仅当 x 和 y 相互独立时等号成立。</p>
<p>使用概率的加和规则和乘积规则,我们看到互信息和条件熵之间的关系为:<br>
\[
I[\boldsymbol{x}, \boldsymbol{y}]=H[\boldsymbol{x}]-H[\boldsymbol{x} \mid \boldsymbol{y}]=H[\boldsymbol{y}]-H[\boldsymbol{y} \mid \boldsymbol{x}]
\]<br>
可以把互信息看成由于知道 y 值而造成的 x 的不确定性的减小(反之亦然).</p>
<p>易知：<br>
\[
H(X)=-\sum_{x \in X} p(x) \log _{2} p(x) \\
H(X \mid Y)=-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log _{2} p(x \mid y)
\]<br>
可知：<br>
\[
\begin{array}{l}
I(X ; Y)&amp;=H(X)-H(X \mid Y) \\
&amp;=-\sum_{x \in X} p(x) \log _{2} p(x)+\sum_{x \in X} \sum_{y \in Y} p(x, y) \log _{2} p(x \mid y) \\
&amp;=\sum_{x \in X} \sum_{y \in Y} p(x, y)\left(\log _{2} p(x \mid y)-\log _{2} p(x)\right) \\
&amp;=\sum_{x \in X} \sum_{y \in Y} p(x, y)\left(\log _{2} \frac{p(x \mid y)}{p(x)}\right) 
\end{array}
\]<br>
即：<br>
\[
I(X ; Y)=\sum_{x \in X} \sum_{y \in Y} p(x, y) \log _{2} \frac{p(x, y)}{p(x) p(y)}
\]</p>
<h3 id="dian-hu-xin-xi-pmi">点互信息PMI</h3>
<p>PMI这个指标来衡量两个事物之间的相关性（比如两个词）。其原理很简单，公式如下：<br>
\[
\operatorname{PMI}(x ; y)=\log \frac{p(x, y)}{p(x) p(y)}=\log \frac{p(x \mid y)}{p(x)}=\log \frac{p(y \mid x)}{p(y)}
\]<br>
在概率论中，如果x跟y不相关，则\(p(x,y)=p(x)p(y)\)。二者相关性越大，则\(p(x,y)\)就相比于\(p(x)p(y)\)越大。用后面的式子可能更好理解，在<code>y</code>出现的情况下<code>x</code>出现的条件概率\(p(x|y)\)除以\(x\)本身出现的概率\(p(x)\)，自然就表示<code>x</code>跟<code>y</code>的相关程度。 这里的<code>log</code>来自于信息论的理论，可以简单理解为，当对\(p(x)\)取<code>log</code>之后就将一个概率转换为了信息量（要再乘以-1将其变为正数），以2为底时可以简单理解为用多少个bits可以表示这个变量。</p>
<p>点互信息PMI其实就是从信息论里面的互信息这个概念里面衍生出来的。其衡量的是两个随机变量之间的相关性，即<strong>一个随机变量中包含的关于另一个随机变量的信息量</strong>。</p>
<blockquote>
<p>所谓的随机变量，即随机试验结果的量的表示，可以简单理解为按照一个概率分布进行取值的变量，比如随机抽查的一个人的身高就是一个随机变量。</p>
</blockquote>
<p>可以看出，互信息其实就是对<code>X</code>和<code>Y</code>的所有可能的取值情况的点互信息PMI的加权和，而点互信息只是对其中两个点进行相关性判断。因此，点互信息这个名字还是很形象的。</p>
<h3 id="hu-xin-xi-tiao-jian-shang-yu-lian-he-shang-de-qu-bie-yu-lian-xi">互信息、条件熵与联合熵的区别与联系</h3>
<p><img src="/2018/07/19/math/information_theory/20160718112932230.png" alt></p>
<p>由于 H(X|X) = 0, 所以, H(X) = H(X) – H(X|X) = I(X; X)</p>
<p>这一方面说明了为什么熵又称自信息,另一方面说明了两个完全相互依赖的变量之间的互信息并不是一个常量,而是取决于它们的熵。</p>
<p>从图中可以看出，条件熵可以通过联合熵 - 熵（ H(X|Y) = H(X, Y) - H(Y) ）表示，也可以通过熵 - 互信息（ H(X|Y) = H(X) - I(X; Y) ）表示。</p>
<h2 id="jia-quan-shang">加权熵</h2>
<h2 id="renyi-shang">Renyi熵</h2>
<h1 id="shang-qiu-jie">熵求解</h1>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://blog.csdn.net/pipisorry/article/details/51695283" target="_blank" rel="noopener">信息论：熵与互信息</a></li>
<li><a href="%5Bhttps://p1htmlkernalweb.mybluemix.net/articles/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80%E7%86%B5_2708340_csdn.html%5D(https://p1htmlkernalweb.mybluemix.net/articles/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80%E7%86%B5_2708340_csdn.html)">信息论基础–熵</a></li>
</ol>
]]></content>
      <categories>
        <category>数学/信息论</category>
      </categories>
      <tags>
        <tag>信息论</tag>
      </tags>
  </entry>
  <entry>
    <title>概率论</title>
    <url>/2018/07/19/math/probability/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2018/07/19/math/probability/unnamed.jpg" alt></p>
<a id="more"></a>
<h3 id="tiao-jian-gai-lu">条件概率</h3>
<p>假设事件\(A\)和事件\(B\)的概率分别为\(P(A)\)和\(P(B)\)，两个事件同时发生的概率记作\(P(A \cap B)\)或\(P(A, B)\)。给定事件\(B\)，事件\(A\)的条件概率</p>
<p>\[
P(A \mid B) = \frac{P(A \cap B)}{P(B)}
\]</p>
<p>也就是说，</p>
<p>\[
P(A \cap B) = P(B) P(A \mid B) = P(A) P(B \mid A)
\]</p>
<p>当满足</p>
<p>\[
P(A \cap B) = P(A) P(B)
\]</p>
<p>时，事件\(A\)和事件\(B\)相互独立。</p>
<h3 id="qi-wang">期望</h3>
<p>离散的随机变量\(X\)的期望（或平均值）为</p>
<p>\[
E(X) = \sum_{x} x P(X = x)
\]</p>
<h3 id="jun-yun-fen-bu">均匀分布</h3>
<p>假设随机变量\(X\)服从\([a, b]\)上的均匀分布，即\(X \sim U(a, b)\)。随机变量\(X\)取\(a\)和\(b\)之间任意一个数的概率相等。</p>
]]></content>
  </entry>
  <entry>
    <title>微分</title>
    <url>/2018/07/19/math/differential/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2018/07/19/math/differential/image-20200719132233772.png" alt></p>
<a id="more"></a>
<h3 id="dao-shu-he-wei-fen">导数和微分</h3>
<p>假设函数\(f: \mathbb{R} \rightarrow \mathbb{R}\)的输入和输出都是标量。函数\(f\)的导数</p>
<p>\[
f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}
\]</p>
<p>且假定该极限存在。给定\(y = f(x)\)，其中\(x\)和\(y\)分别是函数\(f\)的自变量和因变量。以下有关导数和微分的表达式等价：</p>
<p>\[
f'(x) = y' = \frac{\text{d}y}{\text{d}x} = \frac{\text{d}f}{\text{d}x} = \frac{\text{d}}{\text{d}x} f(x) = \text{D}f(x) = \text{D}_x f(x)
\]</p>
<p>其中符号\(\text{D}\)和\(\text{d}/\text{d}x\)也叫微分运算符。常见的微分演算有\(\text{D}C = 0\)（\(C\)为常数）、\(\text{D}x^n = nx^{n-1}\)（\(n\)为常数）、\(\text{D}e^x = e^x\)、\(\text{D}\ln(x) = 1/x\)等。</p>
<p>如果函数\(f\)和\(g\)都可导，设\(C\)为常数，那么</p>
<p>\[
\begin{aligned}
\frac{\text{d}}{\text{d}x} [Cf(x)] &amp;= C \frac{\text{d}}{\text{d}x} f(x),\\
\frac{\text{d}}{\text{d}x} [f(x) + g(x)] &amp;= \frac{\text{d}}{\text{d}x} f(x) + \frac{\text{d}}{\text{d}x} g(x),\\ 
\frac{\text{d}}{\text{d}x} [f(x)g(x)] &amp;= f(x) \frac{\text{d}}{\text{d}x} [g(x)] + g(x) \frac{\text{d}}{\text{d}x} [f(x)],\\
\frac{\text{d}}{\text{d}x} \left[\frac{f(x)}{g(x)}\right] &amp;= \frac{g(x) \frac{\text{d}}{\text{d}x} [f(x)] - f(x) \frac{\text{d}}{\text{d}x} [g(x)]}{[g(x)]^2}.
\end{aligned}
\]</p>
<p>如果\(y=f(u)\)和\(u=g(x)\)都是可导函数，依据链式法则，</p>
<p>\[
\frac{\text{d}y}{\text{d}x} = \frac{\text{d}y}{\text{d}u} \frac{\text{d}u}{\text{d}x}
\]</p>
<h3 id="tai-le-zhan-kai">泰勒展开</h3>
<p>函数\(f\)的泰勒展开式是</p>
<p>\[
f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!} (x-a)^n
\]</p>
<p>其中\(f^{(n)}\)为函数\(f\)的\(n\)阶导数（求\(n\)次导数），\(n!\)为\(n\)的阶乘。假设\(\epsilon\)是一个足够小的数，如果将上式中\(x\)和\(a\)分别替换成\(x+\epsilon\)和\(x\)，可以得到</p>
<p>\[
f(x + \epsilon) \approx f(x) + f'(x) \epsilon + \mathcal{O}(\epsilon^2)
\]</p>
<p>由于\(\epsilon\)足够小，上式也可以简化成</p>
<p>\[
f(x + \epsilon) \approx f(x) + f'(x) \epsilon
\]</p>
<h3 id="pian-dao-shu">偏导数</h3>
<p>设\(u\)为一个有\(n\)个自变量的函数，\(u = f(x_1, x_2, \ldots, x_n)\)，它有关第\(i\)个变量\(x_i\)的偏导数为</p>
<p>\[
\frac{\partial u}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x_1, \ldots, x_{i-1}, x_i+h, x_{i+1}, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}
\]</p>
<p>以下有关偏导数的表达式等价：</p>
<p>\[
\frac{\partial u}{\partial x_i} = \frac{\partial f}{\partial x_i} = f_{x_i} = f_i = \text{D}_i f = \text{D}_{x_i} f
\]</p>
<p>为了计算\(\partial u/\partial x_i\)，只需将\(x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n\)视为常数并求\(u\)有关\(x_i\)的导数。</p>
<h3 id="ti-du">梯度</h3>
<p>假设函数\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)的输入是一个\(n\)维向量\(\boldsymbol{x} = [x_1, x_2, \ldots, x_n]^\top\)，输出是标量。函数\(f(\boldsymbol{x})\)有关\(\boldsymbol{x}\)的梯度是一个由\(n\)个偏导数组成的向量：</p>
<p>\[
\nabla_{\boldsymbol{x}} f(\boldsymbol{x}) = \bigg[\frac{\partial f(\boldsymbol{x})}{\partial x_1}, \frac{\partial f(\boldsymbol{x})}{\partial x_2}, \ldots, \frac{\partial f(\boldsymbol{x})}{\partial x_n}\bigg]^\top
\]</p>
<p>为表示简洁，我们有时用\(\nabla f(\boldsymbol{x})\)代替\(\nabla_{\boldsymbol{x}} f(\boldsymbol{x})\)。</p>
<p>假设\(\boldsymbol{x}\)是一个向量，常见的梯度演算包括</p>
<p>\[
\begin{aligned}
\nabla_{\boldsymbol{x}} \boldsymbol{A}^\top \boldsymbol{x} &amp;= \boldsymbol{A}, \\
\nabla_{\boldsymbol{x}} \boldsymbol{x}^\top \boldsymbol{A}  &amp;= \boldsymbol{A}, \\
\nabla_{\boldsymbol{x}} \boldsymbol{x}^\top \boldsymbol{A} \boldsymbol{x}  &amp;= (\boldsymbol{A} + \boldsymbol{A}^\top)\boldsymbol{x},\\
\nabla_{\boldsymbol{x}} \|\boldsymbol{x} \|^2 &amp;= \nabla_{\boldsymbol{x}} \boldsymbol{x}^\top \boldsymbol{x} = 2\boldsymbol{x}.
\end{aligned}
\]</p>
<p>类似地，假设\(\boldsymbol{X}\)是一个矩阵，那么<br>
\[
\nabla_{\boldsymbol{X}} \|\boldsymbol{X} \|_F^2 = 2\boldsymbol{X}
\]</p>
<h3 id="hai-sen-ju-zhen">海森矩阵</h3>
<p>假设函数\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)的输入是一个\(n\)维向量\(\boldsymbol{x} = [x_1, x_2, \ldots, x_n]^\top\)，输出是标量。假定函数\(f\)所有的二阶偏导数都存在，\(f\)的海森矩阵\(\boldsymbol{H}\)是一个\(n\)行\(n\)列的矩阵：</p>
<p>\[
\boldsymbol{H} = 
\begin{bmatrix}
    \frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \dots  &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
    \frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \dots  &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    \frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \dots  &amp; \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix},
\]</p>
<p>其中二阶偏导数为</p>
<p>\[
\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial }{\partial x_j} \left(\frac{\partial f}{ \partial x_i}\right)
\]</p>
]]></content>
      <categories>
        <category>数学/微分</category>
      </categories>
  </entry>
  <entry>
    <title>线性代数</title>
    <url>/2018/07/19/math/linear_algebra/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://img-blog.csdn.net/20170724210638122" alt></p>
<a id="more"></a>
<h1 id="xiang-liang">向量</h1>
<p>一个\(n\)维向量\(\boldsymbol{x}\)的表达式可写成</p>
<p>\[
\boldsymbol{x} = 
\begin{bmatrix}
    x_{1}  \\
    x_{2}  \\
    \vdots  \\
    x_{n} 
\end{bmatrix},
\]</p>
<p>其中\(x_1, \ldots, x_n\)是向量的元素。我们将各元素均为实数的\(n\)维向量\(\boldsymbol{x}\)记作\(\boldsymbol{x} \in \mathbb{R}^{n}\)或\(\boldsymbol{x} \in \mathbb{R}^{n \times 1}\)。</p>
<h1 id="ju-zhen">矩阵</h1>
<p>一个\(m\)行\(n\)列矩阵的表达式可写成</p>
<p>\[
\boldsymbol{X} = 
\begin{bmatrix}
    x_{11} &amp; x_{12}  &amp; \dots  &amp; x_{1n} \\
    x_{21} &amp; x_{22}  &amp; \dots  &amp; x_{2n} \\
    \vdots &amp; \vdots  &amp; \ddots &amp; \vdots \\
    x_{m1} &amp; x_{m2}  &amp; \dots  &amp; x_{mn}
\end{bmatrix},
\]</p>
<p>其中\(x_{ij}\)是矩阵\(\boldsymbol{X}\)中第\(i\)行第\(j\)列的元素（\(1 \leq i \leq m, 1 \leq j \leq n\)）。我们将各元素均为实数的\(m\)行\(n\)列矩阵\(\boldsymbol{X}\)记作\(\boldsymbol{X} \in \mathbb{R}^{m \times n}\)。不难发现，向量是特殊的矩阵。</p>
<h1 id="yun-suan">运算</h1>
<p>设\(n\)维向量\(\boldsymbol{a}\)中的元素为\(a_1, \ldots, a_n\)，\(n\)维向量\(\boldsymbol{b}\)中的元素为\(b_1, \ldots, b_n\)。向量\(\boldsymbol{a}\)与\(\boldsymbol{b}\)的点乘（内积）是一个标量：</p>
<p>\[
\boldsymbol{a} \cdot \boldsymbol{b} = a_1 b_1 + \ldots + a_n b_n
\]</p>
<p>设两个\(m\)行\(n\)列矩阵</p>
<p>\[
\boldsymbol{A} = 
\begin{bmatrix}
    a_{11} &amp; a_{12} &amp; \dots  &amp; a_{1n} \\
    a_{21} &amp; a_{22} &amp; \dots  &amp; a_{2n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{m1} &amp; a_{m2} &amp; \dots  &amp; a_{mn}
\end{bmatrix},\quad
\boldsymbol{B} = 
\begin{bmatrix}
    b_{11} &amp; b_{12} &amp; \dots  &amp; b_{1n} \\
    b_{21} &amp; b_{22} &amp; \dots  &amp; b_{2n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    b_{m1} &amp; b_{m2} &amp; \dots  &amp; b_{mn}
\end{bmatrix}.
\]</p>
<p>矩阵\(\boldsymbol{A}\)的转置是一个\(n\)行\(m\)列矩阵，它的每一行其实是原矩阵的每一列：<br>
\[
\boldsymbol{A}^\top = 
\begin{bmatrix}
    a_{11} &amp; a_{21} &amp; \dots  &amp; a_{m1} \\
    a_{12} &amp; a_{22} &amp; \dots  &amp; a_{m2} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{1n} &amp; a_{2n} &amp; \dots  &amp; a_{mn}
\end{bmatrix}.
\]</p>
<p>两个相同形状的矩阵的加法是将两个矩阵按元素做加法：</p>
<p>\[
\boldsymbol{A} + \boldsymbol{B} = 
\begin{bmatrix}
    a_{11} + b_{11} &amp; a_{12} + b_{12} &amp; \dots  &amp; a_{1n} + b_{1n} \\
    a_{21} + b_{21} &amp; a_{22} + b_{22} &amp; \dots  &amp; a_{2n} + b_{2n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{m1} + b_{m1} &amp; a_{m2} + b_{m2} &amp; \dots  &amp; a_{mn} + b_{mn}
\end{bmatrix}.
\]</p>
<p>我们使用符号\(\odot\)表示两个矩阵按元素乘法的运算，即阿达玛（Hadamard）积：</p>
<p>\[
\boldsymbol{A} \odot \boldsymbol{B} = 
\begin{bmatrix}
    a_{11}  b_{11} &amp; a_{12}  b_{12} &amp; \dots  &amp; a_{1n}  b_{1n} \\
    a_{21}  b_{21} &amp; a_{22}  b_{22} &amp; \dots  &amp; a_{2n}  b_{2n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{m1}  b_{m1} &amp; a_{m2}  b_{m2} &amp; \dots  &amp; a_{mn}  b_{mn}
\end{bmatrix}.
\]</p>
<p>定义一个标量\(k\)。标量与矩阵的乘法也是按元素做乘法的运算：</p>
<p>\[
k\boldsymbol{A} = 
\begin{bmatrix}
    ka_{11} &amp; ka_{12} &amp; \dots  &amp; ka_{1n} \\
    ka_{21} &amp; ka_{22} &amp; \dots  &amp; ka_{2n} \\
    \vdots &amp; \vdots   &amp; \ddots &amp; \vdots \\
    ka_{m1} &amp; ka_{m2} &amp; \dots  &amp; ka_{mn}
\end{bmatrix}.
\]</p>
<p>其他诸如标量与矩阵按元素相加、相除等运算与上式中的相乘运算类似。矩阵按元素开根号、取对数等运算也就是对矩阵每个元素开根号、取对数等，并得到和原矩阵形状相同的矩阵。</p>
<p>矩阵乘法和按元素的乘法不同。设\(\boldsymbol{A}\)为\(m\)行\(p\)列的矩阵，\(\boldsymbol{B}\)为\(p\)行\(n\)列的矩阵。两个矩阵相乘的结果</p>
<p>\[
\boldsymbol{A} \boldsymbol{B} = 
\begin{bmatrix}
    a_{11} &amp; a_{12} &amp; \dots  &amp; a_{1p} \\
    a_{21} &amp; a_{22} &amp; \dots  &amp; a_{2p} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{i1} &amp; a_{i2} &amp; \dots  &amp; a_{ip} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{m1} &amp; a_{m2} &amp; \dots  &amp; a_{mp}
\end{bmatrix}
\begin{bmatrix}
    b_{11} &amp; b_{12} &amp; \dots  &amp; b_{1j} &amp; \dots &amp; b_{1n} \\
    b_{21} &amp; b_{22} &amp; \dots  &amp; b_{2j} &amp; \dots  &amp; b_{2n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
    b_{p1} &amp; b_{p2} &amp; \dots  &amp; b_{pj} &amp; \dots  &amp; b_{pn}
\end{bmatrix}
\]</p>
<p>是一个\(m\)行\(n\)列的矩阵，其中第\(i\)行第\(j\)列（\(1 \leq i \leq m, 1 \leq j \leq n\)）的元素为</p>
<p>\[
a_{i1}b_{1j}  + a_{i2}b_{2j} + \ldots + a_{ip}b_{pj} = \sum_{k=1}^p a_{ik}b_{kj}
\]</p>
<h1 id="fan-shu">范数</h1>
<p>设\(n\)维向量\(\boldsymbol{x}\)中的元素为\(x_1, \ldots, x_n\)。向量\(\boldsymbol{x}\)的\(L_p\)范数为</p>
<p>\[
\|\boldsymbol{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}
\]</p>
<p>例如，\(\boldsymbol{x}\)的\(L_1\)范数是该向量元素绝对值之和：</p>
<p>\[
\|\boldsymbol{x}\|_1 = \sum_{i=1}^n \left|x_i \right|
\]</p>
<p>而\(\boldsymbol{x}\)的\(L_2\)范数是该向量元素平方和的平方根：</p>
<p>\[
\|\boldsymbol{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}
\]</p>
<p>通常用\(\|\boldsymbol{x}\|\)指代\(\|\boldsymbol{x}\|_2\)。</p>
<p>设\(\boldsymbol{X}\)是一个\(m\)行\(n\)列矩阵。矩阵\(\boldsymbol{X}\)的Frobenius范数为该矩阵元素平方和的平方根：</p>
<p>\[
\|\boldsymbol{X}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}
\]</p>
<p>其中\(x_{ij}\)为矩阵\(\boldsymbol{X}\)在第\(i\)行第\(j\)列的元素。</p>
<h1 id="te-zheng-xiang-liang-he-te-zheng-zhi">特征向量和特征值</h1>
<p>对于一个\(n\)行\(n\)列的矩阵\(\boldsymbol{A}\)，假设有标量\(\lambda\)和非零的\(n\)维向量\(\boldsymbol{v}\)使</p>
<p>\[
\boldsymbol{A} \boldsymbol{v} = \lambda \boldsymbol{v}
\]</p>
<p>那么\(\boldsymbol{v}\)是矩阵\(\boldsymbol{A}\)的一个特征向量，标量\(\lambda\)是\(\boldsymbol{v}\)对应的特征值。</p>
]]></content>
      <categories>
        <category>数学/线性代数</category>
      </categories>
  </entry>
  <entry>
    <title>优化算法</title>
    <url>/2018/07/11/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="ti-du-xia-jiang-suan-fa">梯度下降算法</h2>
<p>梯度下降法(Gradient descent)或最速下降法(steepest descent)是求解无约束最优化问题的一种常用的、实现简单的方法。<br>
假设\(f(x)\)是\(R^n\)上具有一阶连续偏导数的函数。求解<br>
\[
   min_{x \in R^n}f(x)
\]<br>
无约束最优化问题。 \(f^\ast\)表示目标函数\(f(x)\)的极小点。<br>
梯度下降法是一种迭代算法，选取适当的初值\(x^0\),不断迭代，更新x的值，进行目标函数的极小化，直到收敛。由于负梯度的方向是使函数下降最快的方向，在迭代的每一步，以负梯度方向更新x的值，从而达到减少函数值的目的。<br>
第k+1次迭代值：<br>
\[
x^{(k+1)} \leftarrow x^{(k)}+\lambda_{k} p_{k}
\]<br>
其中，\(p_k\)是搜索方向，取负梯度方向\(p_k = - \nabla f(x^k)\),\(\lambda_k\)使得<br>
\[
f\left(x^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geq 0} f\left(x^{(k)}+\lambda p_{k}\right)
\]</p>
<a id="more"></a>
<h3 id="suan-fa-miao-shu">算法描述：</h3>
<p>输入:目标函数\(f(x)\)，梯度函数\(g(x^k)=\nabla f(x^k)\),计算\(\epsilon\).<br>
输出：\(f(x)\)的极小值点\(x^\star\)</p>
<ol>
<li>取初值\(x^0 \in R^n\),置k=0</li>
<li>计算\(f(x^k)\)</li>
<li>计算梯度\(g_k = g(x^k)\),当\(\left\|g_{k}\right\|&lt;\varepsilon\)时，停止迭代，令\(p_k = -g(x^k)\)求\(\lambda_k\),使<br>
\[
f\left(x^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geq 0} f\left(x^{(k)}+\lambda p_{k}\right)
\]</li>
<li>置\(x^{k+1}=x^k+\lambda_k p_k\),计算\(f(x^{k+1})\)当$<br>
\left|f\left(x<sup>{(k+1)}\right)-f\left(x</sup>{(k)}\right)\right|&lt;\varepsilon<br>
\(或\)\left|x<sup>{(k+1)}-x</sup>{(k)}\right|&lt;\varepsilon\(停止迭代。令\)x^\star = x^{k+1}$</li>
<li>否则，置\(k=k+1\),转3.</li>
</ol>
<h3 id="ti-du-xia-jiang-diao-you">梯度下降调优</h3>
<ol>
<li><strong>算法的步长选择</strong>。步长取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。</li>
<li><strong>算法参数的初始值选择</strong>。初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</li>
<li><strong>归一化</strong>。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化。</li>
</ol>
<h3 id="ti-du-xia-jiang-fa-da-jia-zu-bgd-sgd-mbgd">梯度下降法大家族（BGD，SGD，MBGD）</h3>
<ol>
<li><strong>批量梯度下降法（Batch Gradient Descent）</strong> 批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新。</li>
<li><strong>随机梯度下降法（Stochastic Gradient Descent）</strong> 其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的样本的数据，而是仅仅选取一个样本来求梯度。随机梯度下降法，和批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。</li>
<li><strong>小批量梯度下降法（Mini-batch Gradient Descent）</strong> 小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于\(m\)个样本，我们采用\(x\)个样本来迭代，\(1&lt;x&lt;m\)。一般可以取\(x=16,32,64...\)，当然根据样本的数据，可以调整这个\(x\) 的值。</li>
</ol>
<p>上述三种方法得到局部最优解的过程：<br>
<img src="/2018/07/11/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/C80A2811-1678-4371-BCCD-8AA51A777820.jpg" alt="avatar"></p>
<h3 id="dai-ma-shi-xian">代码实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(X, y, W, B, alpha, max_iters)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    使用了所有的样本进行梯度下降</span></span><br><span class="line"><span class="string">    X: 训练集,</span></span><br><span class="line"><span class="string">    y: 标签,</span></span><br><span class="line"><span class="string">    W: 权重向量,</span></span><br><span class="line"><span class="string">    B: bias,</span></span><br><span class="line"><span class="string">    alpha: 学习率,</span></span><br><span class="line"><span class="string">    max_iters: 最大迭代次数.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    dW = <span class="number">0</span> <span class="comment"># 权重梯度收集器</span></span><br><span class="line">    dB = <span class="number">0</span> <span class="comment"># Bias梯度的收集器</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>] <span class="comment"># 样本数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_iters):</span><br><span class="line">        dW = <span class="number">0</span> <span class="comment"># 每次迭代重置</span></span><br><span class="line">        dB = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment"># 1. 迭代所有的样本</span></span><br><span class="line">            <span class="comment"># 2. 计算权重和bias的梯度保存在w_grad和b_grad,</span></span><br><span class="line">            <span class="comment"># 3. 通过增加w_grad和b_grad来更新dW和dB</span></span><br><span class="line">            W = W - alpha * (dW / m) <span class="comment"># 更新权重</span></span><br><span class="line">            B = B - alpha * (dB / m) <span class="comment"># 更新bias</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> W, B</span><br></pre></td></tr></table></figure>
<h3 id="ti-du-xia-jiang-he-zui-xiao-er-cheng-fa">梯度下降和最小二乘法</h3>
<p>梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。</p>
<h2 id="niu-dun-fa">牛顿法</h2>
<p>一般来说，牛顿法主要应用在两个方面，1：求方程的根；2：最优化。</p>
<h3 id="qiu-jie-guo-cheng">求解过程</h3>
<p>并不是所有的方程都有求根公式，或者求根公式很复杂，导致求解困难。利用牛顿法，可以迭代求解。</p>
<p>原理是利用泰勒公式，在 \(x_0\)处展开，且展开到一阶, 即 \(f(x)=f(x_0)+f^\prime{x_0}(x-x_0)\) 。求解方程\(f(x)=0\)，等价于\(f(x_0)+f^\prime(x_0)(x-x_0)=0\)，求解 \(x=x_1=x_0-\frac{f(x_0)}{f^\prime(x_0)}\)。因为这是利用泰勒公式的一阶展开,\(f(x)=f(x_0)+f^\prime(x_0)(x-x_0)\) 处并不是完全相等，而是近似相等，这里求得的的\(x_1\)并不能让 \(f(x)=0\)，只能说 \(f(x_1)\) 的值比\(f(x_9)\)的值更接近于0，于是迭代的想法就很自然了，可以进而推出 \(x_{n+1}=x_n-\frac{f(x_n)}{f\prime(x_n)}\)，通过迭代，这个式子必然在 \(f(x^\star)=0\) 的时候收敛。整个过程如下：<br>
<img src="/2018/07/11/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/B03C2A8B-AF9F-4871-94F6-0E7F6D6EC890.png" alt="avatar"></p>
<h3 id="zui-you-hua">最优化</h3>
<p>无约束最优化问题<br>
\[
min_{x \in R^N}f(x)
\]<br>
其中\(s^\star\)为目标函数的极小点</p>
<p>设\(f(x)\)具有二阶连续偏导，若第\(k\)次迭代值为\(x^k\),则可将\(f(x)\)在\(x^k\)附近进行二阶泰勒展开：<br>
\[
f(x)=f\left(x^{(k)}\right)+g_{k}^{T}\left(x-x^{(k)}\right)+\frac{1}{2}\left(x-x^{(k)}\right)^{T} H\left(x^{(k)}\right)\left(x-x^{(x)}\right)
\]<br>
其中，\(g_k=g(x^k)=\nabla f(x^k)\)是\(f(x)\)的梯度向量在点\(x^k\)的值，\(H(x^k)\)是\(f(x)\)的海塞矩阵<br>
\[
H(x)=\left[\frac{\partial^{2} f}{\partial x_{i} \partial x_{j}}\right]_{n \times n}
\]<br>
在点\(x^k\)的值。</p>
<p>函数\(f(x)\)有极值的必要条件是在极值点处一阶导数为0，即梯度向量为0。特别的当\(H(x^k)\)是正定矩阵时，函数\(f(x)\)的极值为最小值。</p>
<p>为了的得到一阶导数为0的点，可以用到<strong>求解方程</strong>部分的方法。根据二阶泰勒展开，对\(\nabla f(x)\)在\(x^k\)进行展开得（也可以对泰勒公式再进行求导）：<br>
\[
\nabla f(x)=g_{k}+H_{k}\left(x-x^{(k)}\right)
\]<br>
其中，\(H_k=H(x^k)\)则：<br>
\[
\begin{aligned}&amp;g_{k}+H_{k}\left(x^{(k+1)}-x^{(k)}\right)=0\\&amp;x^{(k+1)}=x^{(k)}-H_{k}^{-1} g_{k}\end{aligned}
\]<br>
令<br>
\[
H_kp_k = -g_k
\]<br>
得到迭代公式：<br>
\[
x^{k+1} = x^k+p_k
\]<br>
最终在\(\nabla f(x^\star)=0\)收敛</p>
<h3 id="suan-fa-miao-shu-1">算法描述</h3>
<p>输入：目标函数\(f(x)\)，梯度\(g(x) = \nabla f(x)\),海塞矩阵\(H(x)\),精度(阈值)\(\epsilon\)</p>
<p>输出：\(f(x)\)的极小值点\(x^\star\)</p>
<ol>
<li>
<p>取初始点\(x^0\),置k=0</p>
</li>
<li>
<p>计算\(g_k = g(x^k)\)</p>
</li>
<li>
<p>若\(\left\|g_k \right\| &lt; \epsilon\)，则停止计算，求得近似解:\(x^\star = x^k\)</p>
</li>
<li>
<p>计算\(H_k = H(x^k)\),并求\(p_k\)。<br>
\[
H_kp_k = -g_k
\]</p>
</li>
<li>
<p>置 \(x(k+1) = x^k + p_k\)</p>
</li>
<li>
<p>置 k=k+1, 转2.</p>
</li>
</ol>
<h3 id="niu-dun-fa-he-ti-du-xia-jiang">牛顿法和梯度下降</h3>
<p>梯度下降法和牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法是用二阶的海森矩阵的逆矩阵求解。相对而言，使用牛顿法收敛更快（迭代更少次数）。但是每次迭代的时间比梯度下降法长。</p>
<p>梯度下降法：<br>
\[
x^{k+1} = x^k - \lambda \nabla f(x^k)
\]<br>
牛顿法：<br>
\[
x^{k+1} = x^k-\lambda(H^k)^{-1}\nabla f(x^k)
\]<br>
至于为什么牛顿法收敛更快，通俗来说梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。<br>
<img src="/2018/07/11/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/84835FEB-0EA4-41E5-B07D-41BBEB49AE9B.jpg" alt="9c6e73f0200cbe504e0370a18ca3a95a"><br>
红色的牛顿法的迭代路径，绿色的是梯度下降法的迭代路径。<br>
牛顿法是梯度下降法的进一步发展，梯度法利用了目标函数的一阶偏导信息、一负梯度方向作为搜索方向，只考虑目标函数在迭代点的局部性质；而牛顿法不仅使用目标函数的一阶偏导数，还进一步利用了目标函数的二阶偏导数，这样就考虑了梯度变化的趋势，因而能更全面地确定合适的搜索方向以加快收敛，它具有二阶收敛速度，但是牛顿法也存在两个<strong>缺点</strong> ：</p>
<ul>
<li>对目标函数有较严格的要求，函数必须具有连续的一、二阶偏导，海塞矩阵必须正定。</li>
<li>计算相当复杂，除了需要计算梯度外，还需计算二阶偏导矩阵和它的逆矩阵。计算量、存储量都很大。且均以维数N的平方比增加，当N很大时，这个问题就更加突出。</li>
</ul>
<h2 id="ni-niu-dun-fa">拟牛顿法</h2>
<p>在牛顿法的迭代计算中，需要计算海塞矩阵的逆矩阵\(H^-1\)，这一计算比较复杂，考虑用一个n阶矩阵\(G_k=G(x^k)\)来近似替代\(H^{-1}_{k}=H^{-1}(x^k)\)。这就是拟牛顿的基本想法。</p>
<p>要找到近似的替代矩阵，必定要和\(H_k\)有类似的性质。先看下牛顿法迭代中海塞矩阵\(H_k\)满足的条件。首先，\(H_k\)满足以下关系：</p>
<p>取\(x=x^{k-1}\)，由<br>
\[
\nabla f(x)=g_k+H_k(x-x^k)
\]<br>
得：<br>
\[
g_{k-1}-g_k = H_k(x^{k-1}-x^k)
\]<br>
记\(y_k=g_k-g_{k-1},\delta_k = x^k-x^{k-1}\),则：<br>
\[
y_{k-1}=H_k\delta_{k-1}H^{-1}_ky_{k-1}=\delta_{k-1}
\]<br>
称为拟牛顿条件。</p>
<p>其次，如果\(H_k\)是正定的（\(H_k^{-1}\)也是正定的），那么保证牛顿法的搜索方向\(p_k\)是下降方向。这是因为搜索方向是\(p_k = -H_k^{-1}g_k\),</p>
<p>由<br>
\[
x^{k+1} = x^k -H^{-1}_kg_k
\]<br>
有<br>
\[
x = x^k - \lambda H_k^{-1}g_k = x^k+\lambda p_k
\]<br>
则\(f(x)\)在\(x^k\)的泰勒展开可近似为：<br>
\[
f(x) = f(x^k) - \lambda g^T_kH_k^{-1}g_k
\]<br>
由于\(H_k^{-1}\)正定，故\(g_k^TH_k^{-1}g_k>0\)。当\(\lambda\)为一个充分小的正数时，有\(f(x)&lt;f(x^k)\),即搜索方向\(p_k\)是下降方向。</p>
<p>因此拟牛顿法将\(G_k\)作为\(H^{-1}_k\)近似。要求\(G_k\)满足同样的条件。首先，每次迭代矩阵\(G_k\)是正定的。同时，$G_k满足下面的拟牛顿条件<br>
\[
G_{k+1}y_k = \delta_k
\]<br>
按照拟牛顿条件，在每次迭代中可以选择更新矩阵\(G_{k+1}\):<br>
\[
G_{k+1} = G_k + \nabla G_k
\]</p>
<h3 id="dfp">DFP</h3>
<h3 id="bfgs">BFGS</h3>
<h3 id="broyden">Broyden</h3>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title>linux命令速查</title>
    <url>/2018/06/25/linux_quickcheck/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2018/06/25/linux_quickcheck/terminal_command.jpg" alt></p>
<a id="more"></a>
<h1 id="a-href-https-linux-gaomeluo-com-linux-ming-ling-a"><a href="https://linux.gaomeluo.com/" target="_blank" rel="noopener">Linux 命令</a></h1>
<p>linux命令是对Linux系统进行管理的命令。对于Linux系统来说，无论是中央处理器、内存、磁盘驱动器、键盘、鼠标，还是用户等都是文件，Linux系统管理的命令是它正常运行的核心，与之前的DOS命令类似。linux命令在系统中有两种类型：内置Shell命令和Linux命令。</p>
<h2 id="linux-de-wen-jian-xi-tong">linux的文件系统</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;boot:系统启动相关的文件，如内核、initrd、以及grub（bootloader）</span><br><span class="line">&#x2F;dev:设备文件：</span><br><span class="line">   1. 块设备：随机访问，数据块</span><br><span class="line">   2. 字符设备：线性访问，按字符为单位</span><br><span class="line">   3. 设备号：主设备号（major）和次设备号（minor）</span><br><span class="line">&#x2F;etc：配置文件</span><br><span class="line">&#x2F;home：用户的家目录，每一个用户的家目录通常默认为&#x2F;home&#x2F;username</span><br><span class="line">&#x2F;root：管理员的家目录；</span><br><span class="line">&#x2F;lib:库文件</span><br><span class="line">   1. 静态库：.a</span><br><span class="line">   2. 动态库：.dll, .so(shared object)</span><br><span class="line">   3. &#x2F;lib&#x2F;moudules:内核模块文件</span><br><span class="line">&#x2F;lib64</span><br><span class="line">&#x2F;media:挂载点目录，移动设备</span><br><span class="line">&#x2F;mnt:挂载点目录，额外的临时文件系统</span><br><span class="line">&#x2F;opt:可选目录，第三方程序的安装目录</span><br><span class="line">&#x2F;proc：伪文件系统，内核映射文件</span><br><span class="line">&#x2F;sys：伪文件系统，跟硬件设备相关的属性映射文件</span><br><span class="line">&#x2F;tmp:临时文件，&#x2F;var&#x2F;tmp</span><br><span class="line">&#x2F;var:可变化的文件</span><br><span class="line">&#x2F;bin:可执行文件，用户命令</span><br><span class="line">&#x2F;sbin：管理命令</span><br></pre></td></tr></table></figure>
<h2 id="yong-hu-guan-li">用户管理</h2>
<h3 id="tian-jia-yong-hu">添加用户</h3>
<figure class="highlight armasm"><table><tr><td class="code"><pre><span class="line"><span class="symbol">sudo</span> <span class="keyword">adduser </span>xxx</span><br></pre></td></tr></table></figure>
<h3 id="tian-jia-guan-li-yuan-quan-xian">添加管理员权限</h3>
<figure class="highlight ada"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/sudoers</span><br><span class="line"># 添加 xxx <span class="keyword">ALL</span>=(<span class="keyword">ALL</span>:<span class="keyword">ALL</span>) <span class="keyword">ALL</span></span><br></pre></td></tr></table></figure>
<h3 id="fu-yu-yong-hu-dui-mou-ge-mu-lu-de-du-xie-quan-xian">赋予用户对某个目录的读写权限：</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sudo setfacl -R -m u:username:rwx your_file_or_fir</span><br><span class="line"><span class="comment"># 如sudo setfacl -R -m u:ssr:rwx /data/ssr</span></span><br></pre></td></tr></table></figure>
<h2 id="chang-yong-ming-ling">常用命令</h2>
<h3 id="ya-suo-jie-ya-suo">压缩/解压缩</h3>
<ol>
<li>
<p>tar</p>
<ol>
<li>解包：tar zxvf filename.tar</li>
<li>打包：tar czvf filename.tar dirname</li>
</ol>
</li>
<li>
<p>gz命令</p>
<ol>
<li>解压1：gunzip filename.gz</li>
<li>解压2：gzip -d filename.gz</li>
<li>压缩：gzip filename
<ol>
<li>.tar.gz 和  .tgz</li>
<li>解压：tar zxvf filename.tar.gz</li>
<li>压缩：tar zcvf filename.tar.gz dirname</li>
<li>压缩多个文件：tar zcvf filename.tar.gz dirname1 dirname2 dirname3…</li>
</ol>
</li>
</ol>
</li>
<li>
<p>bz2命令</p>
<ol>
<li>解压1：bzip2 -d filename.bz2</li>
<li>解压2：bunzip2 filename.bz2</li>
<li>压缩：bzip2 -z filename</li>
<li>.tar.bz2
<ol>
<li>解压：tar jxvf filename.tar.bz2</li>
<li>压缩：tar jcvf filename.tar.bz2 dirname</li>
</ol>
</li>
</ol>
</li>
<li>
<p>bz命令</p>
<ol>
<li>解压1：bzip2 -d <a href="http://filename.bz" target="_blank" rel="noopener">filename.bz</a></li>
<li>解压2：bunzip2 <a href="http://filename.bz" target="_blank" rel="noopener">filename.bz</a></li>
<li>.tar.bz
<ol>
<li>解压：tar jxvf <a href="http://filename.tar.bz" target="_blank" rel="noopener">filename.tar.bz</a></li>
</ol>
</li>
<li>z命令
<ol>
<li>解压：uncompress filename.z</li>
<li>压缩：compress filename</li>
<li>.tar.z<br>
1.   解压：tar zxvf filename.tar.z<br>
2.   压缩：tar zcvf filename.tar.z dirname</li>
</ol>
</li>
<li>zip命令<br>
1.   解压：unzip filename.zip<br>
2.   压缩：zip filename.zip dirname</li>
</ol>
</li>
<li>
<p>总结：z 以gzip格式压缩，c表示create，v显示压缩的详细情况 f file</p>
<ol>
<li>压缩</li>
<li>tar –cvf jpg.tar *.jpg <a href="//xn--jpgtar-k18i60zu6u9rh3xhkqa1h032blufb97e1p9e.jpg" target="_blank" rel="noopener">//将目录里所有jpg文件打包成tar.jpg</a></li>
<li>tar –czf jpg.tar.gz *.jpg <a href="//xn--jpgjpg-k18i60zu6u9rh3xhkqa1h032blufb97e1p9e.xn--tar-x33e" target="_blank" rel="noopener">//将目录里所有jpg文件打包成jpg.tar后</a>，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gz</li>
<li>tar –cjf jpg.tar.bz2 *.jpg <a href="//xn--jpgjpg-k18i60zu6u9rh3xhkqa1h032blufb97e1p9e.xn--tar-x33e" target="_blank" rel="noopener">//将目录里所有jpg文件打包成jpg.tar后</a>，并且将其用bzip2压缩，生成一个bzip2压缩过的包，命名为jpg.tar.bz2</li>
<li>tar –cZf jpg.tar.Z *.jpg <a href="//xn--jpgjpg-k18i60zu6u9rh3xhkqa1h032blufb97e1p9e.xn--tar-x33e" target="_blank" rel="noopener">//将目录里所有jpg文件打包成jpg.tar后</a>，并且将其用compress压缩，生成一个umcompress压缩过的包，命名为jpg.tar.Z</li>
<li>rar a jpg.rar *.jpg //rar格式的压缩，需要先下载rar for linux</li>
<li>zip jpg.zip *.jpg //zip格式的压缩，需要先下载zip for linux</li>
</ol>
</li>
</ol>
<pre><code>2. 解压
    1. tar –xvf file.tar //解压 tar包 
    2. tar -xzvf file.tar.gz //解压tar.gz 
    3. tar -xjvf file.tar.bz2 //解压 
    4. tar.bz2 tar –xZvf file.tar.Z //解压tar.Z 
    5. unrar e file.rar //解压rar 
    6. unzip file.zip //解压zip 
</code></pre>
<h3 id="screen">screen</h3>
<ol>
<li>新建一个叫session_name的session：screen -S session_name</li>
<li>列出当前所有的session：screen -ls</li>
<li>回到session_name这个session：screen -r session_name</li>
<li>远程detach某个session：screen -d session_name</li>
<li>结束当前session并回到session_name这个session：screen -d -r session_name</li>
<li>利用exit退出并kill掉session</li>
</ol>
<h3 id="cha-kan-xian-qia-he-nei-cun-shi-yong-qing-kuang">查看显卡和内存使用情况</h3>
<ol>
<li>查看显卡使用情况：watch -n 5 nvidia-smi</li>
<li>查看磁盘使用：df -h</li>
</ol>
<h3 id="yi-dong-zhong-ming-ming">移动重命名</h3>
<ol>
<li>将/usr/udt中的所有文件移到当前目录(用”.”表示)中：$ mv /usr/udt/* .</li>
<li>将文件test.txt重命名为wbk.txt：$ mv test.txt wbk.txt</li>
</ol>
<h3 id="mu-lu-guan-li">目录管理</h3>
<ol>
<li>ls</li>
<li>cd</li>
<li>pwd</li>
<li>mkdir</li>
<li>rmdir</li>
<li>tree</li>
</ol>
<h3 id="wen-jian-guan-li">文件管理</h3>
<ol>
<li>touch</li>
<li>stat</li>
<li>file</li>
<li>rm
<ol>
<li>-i: 若指定目录已有同名文件，则先询问是否覆盖旧文件;</li>
<li>-f: 在mv操作要覆盖某已有的目标文件时不给任何指示;</li>
</ol>
</li>
<li>cp [options] source dest
<ol>
<li>-a:此选项通常在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容。其作用等于dpr参数组合。</li>
<li>-d：复制时保留链接。这里所说的链接相当于Windows系统中的快捷方式。</li>
<li>-f：覆盖已经存在的目标文件而不给出提示。</li>
<li>-i：与-f选项相反，在覆盖目标文件之前给出提示，要求用户确认是否覆盖，回答&quot;y&quot;时目标文件将被覆盖。</li>
<li>-p：除复制文件的内容外，还把修改时间和访问权限也复制到新文件中。</li>
<li>-r：若给出的源文件是一个目录文件，此时将复制该目录下所有的子目录和文件。</li>
<li>-l：不复制文件，只是生成链接文件。</li>
</ol>
</li>
<li>mv</li>
<li>nano</li>
<li>vi</li>
<li>vim</li>
</ol>
<h3 id="ri-qi-shi-jian">日期时间</h3>
<ol>
<li>data</li>
<li>clock</li>
<li>hwclock</li>
<li>cal</li>
<li>ntpdate</li>
</ol>
<h3 id="cha-kan-wen-ben">查看文本</h3>
<ol>
<li>cat</li>
<li>tac</li>
<li>more</li>
<li>less</li>
<li>head</li>
<li>tail</li>
</ol>
<h3 id="wen-jian-cha-zhao-find">文件查找:find</h3>
<h4 id="shuo-ming">说明</h4>
<p><strong>find命令</strong> 用来在指定目录下查找文件。任何位于参数之前的字符串都将被视为欲查找的目录名。如果使用该命令时，不设置任何参数，则find命令将在当前目录下查找子目录与文件。并且将查找到的子目录和文件全部进行显示。</p>
<h4 id="yu-fa">语法</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find(选项)(参数)</span><br></pre></td></tr></table></figure>
<h4 id="xuan-xiang">选项</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-amin&lt;分钟&gt;：查找在指定时间曾被存取过的文件或目录，单位以分钟计算；</span><br><span class="line">-anewer&lt;参考文件或目录&gt;：查找其存取时间较指定文件或目录的存取时间更接近现在的文件或目录；</span><br><span class="line">-atime&lt;24小时数&gt;：查找在指定时间曾被存取过的文件或目录，单位以24小时计算；</span><br><span class="line">-cmin&lt;分钟&gt;：查找在指定时间之时被更改过的文件或目录；</span><br><span class="line">-cnewer&lt;参考文件或目录&gt;查找其更改时间较指定文件或目录的更改时间更接近现在的文件或目录；</span><br><span class="line">-ctime&lt;24小时数&gt;：查找在指定时间之时被更改的文件或目录，单位以24小时计算；</span><br><span class="line">-daystart：从本日开始计算时间；</span><br><span class="line">-depth：从指定目录下最深层的子目录开始查找；</span><br><span class="line">-expty：寻找文件大小为0 Byte的文件，或目录下没有任何子目录或文件的空目录；</span><br><span class="line">-exec&lt;执行指令&gt;：假设find指令的回传值为True，就执行该指令；</span><br><span class="line">-false：将find指令的回传值皆设为False；</span><br><span class="line">-fls&lt;列表文件&gt;：此参数的效果和指定“-ls”参数类似，但会把结果保存为指定的列表文件；</span><br><span class="line">-follow：排除符号连接；</span><br><span class="line">-fprint&lt;列表文件&gt;：此参数的效果和指定“-print”参数类似，但会把结果保存成指定的列表文件；</span><br><span class="line">-fprint0&lt;列表文件&gt;：此参数的效果和指定“-print0”参数类似，但会把结果保存成指定的列表文件；</span><br><span class="line">-fprintf&lt;列表文件&gt;&lt;输出格式&gt;：此参数的效果和指定“-printf”参数类似，但会把结果保存成指定的列表文件；</span><br><span class="line">-fstype&lt;文件系统类型&gt;：只寻找该文件系统类型下的文件或目录；</span><br><span class="line">-gid&lt;群组识别码&gt;：查找符合指定之群组识别码的文件或目录；</span><br><span class="line">-group&lt;群组名称&gt;：查找符合指定之群组名称的文件或目录；</span><br><span class="line">-help或——help：在线帮助；</span><br><span class="line">-ilname&lt;范本样式&gt;：此参数的效果和指定“-lname”参数类似，但忽略字符大小写的差别；</span><br><span class="line">-iname&lt;范本样式&gt;：此参数的效果和指定“-name”参数类似，但忽略字符大小写的差别；</span><br><span class="line">-inum&lt;inode编号&gt;：查找符合指定的inode编号的文件或目录；</span><br><span class="line">-ipath&lt;范本样式&gt;：此参数的效果和指定“-path”参数类似，但忽略字符大小写的差别；</span><br><span class="line">-iregex&lt;范本样式&gt;：此参数的效果和指定“-regexe”参数类似，但忽略字符大小写的差别；</span><br><span class="line">-links&lt;连接数目&gt;：查找符合指定的硬连接数目的文件或目录；</span><br><span class="line">-iname&lt;范本样式&gt;：指定字符串作为寻找符号连接的范本样式；</span><br><span class="line">-ls：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出；</span><br><span class="line">-maxdepth&lt;目录层级&gt;：设置最大目录层级；</span><br><span class="line">-mindepth&lt;目录层级&gt;：设置最小目录层级；</span><br><span class="line">-mmin&lt;分钟&gt;：查找在指定时间曾被更改过的文件或目录，单位以分钟计算；</span><br><span class="line">-mount：此参数的效果和指定“-xdev”相同；</span><br><span class="line">-mtime&lt;24小时数&gt;：查找在指定时间曾被更改过的文件或目录，单位以24小时计算；</span><br><span class="line">-name&lt;范本样式&gt;：指定字符串作为寻找文件或目录的范本样式；</span><br><span class="line">-newer&lt;参考文件或目录&gt;：查找其更改时间较指定文件或目录的更改时间更接近现在的文件或目录；</span><br><span class="line">-nogroup：找出不属于本地主机群组识别码的文件或目录；</span><br><span class="line">-noleaf：不去考虑目录至少需拥有两个硬连接存在；</span><br><span class="line">-nouser：找出不属于本地主机用户识别码的文件或目录；</span><br><span class="line">-ok&lt;执行指令&gt;：此参数的效果和指定“-exec”类似，但在执行指令之前会先询问用户，若回答“y”或“Y”，则放弃执行命令；</span><br><span class="line">-path&lt;范本样式&gt;：指定字符串作为寻找目录的范本样式；</span><br><span class="line">-perm&lt;权限数值&gt;：查找符合指定的权限数值的文件或目录；</span><br><span class="line">-print：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出。格式为每列一个名称，每个名称前皆有“.&#x2F;”字符串；</span><br><span class="line">-print0：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出。格式为全部的名称皆在同一行；</span><br><span class="line">-printf&lt;输出格式&gt;：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出。格式可以自行指定；</span><br><span class="line">-prune：不寻找字符串作为寻找文件或目录的范本样式;</span><br><span class="line">-regex&lt;范本样式&gt;：指定字符串作为寻找文件或目录的范本样式；</span><br><span class="line">-size&lt;文件大小&gt;：查找符合指定的文件大小的文件；</span><br><span class="line">-true：将find指令的回传值皆设为True；</span><br><span class="line">-type&lt;文件类型&gt;：只寻找符合指定的文件类型的文件；</span><br><span class="line">-uid&lt;用户识别码&gt;：查找符合指定的用户识别码的文件或目录；</span><br><span class="line">-used&lt;日数&gt;：查找文件或目录被更改之后在指定时间曾被存取过的文件或目录，单位以日计算；</span><br><span class="line">-user&lt;拥有者名称&gt;：查找符和指定的拥有者名称的文件或目录；</span><br><span class="line">-version或——version：显示版本信息；</span><br><span class="line">-xdev：将范围局限在先行的文件系统中；</span><br><span class="line">-xtype&lt;文件类型&gt;：此参数的效果和指定“-type”参数类似，差别在于它针对符号连接检查。</span><br></pre></td></tr></table></figure>
<h4 id="can-shu">参数</h4>
<p>起始目录：查找文件的起始目录。</p>
<h4 id="shi-li">实例</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 当前目录搜索所有文件，文件内容 包含 “140.206.111.111” 的内容</span><br><span class="line">find . -type f -name &quot;*&quot; | xargs grep &quot;140.206.111.111&quot;</span><br><span class="line"></span><br><span class="line"># 列出当前目录及子目录下所有文件和文件夹</span><br><span class="line">find .</span><br><span class="line"></span><br><span class="line"># 在&#x2F;home目录下查找以.txt结尾的文件名</span><br><span class="line">find &#x2F;home -name &quot;*.txt&quot;</span><br><span class="line"></span><br><span class="line"># 在&#x2F;home目录下查找以.txt结尾的文件名 忽略大小写</span><br><span class="line">find &#x2F;home -iname &quot;*.txt&quot;</span><br><span class="line"></span><br><span class="line"># 当前目录及子目录下查找所有以.txt和.pdf结尾的文件</span><br><span class="line">find . \( -nam &quot;*.txt&quot; -o -name &quot;*.pdf&quot;\)  或 find . -name &quot;*.txt&quot; -o -name &quot;*.pdf&quot;</span><br><span class="line"></span><br><span class="line"># 匹配文件路径或者文件</span><br><span class="line">find &#x2F;usr&#x2F; -path &quot;*local*&quot;</span><br><span class="line"></span><br><span class="line"># 基于正则表达式匹配文件路径</span><br><span class="line">find . -regex &quot;.*\(\.txt\|\.pdf\)$&quot;</span><br><span class="line"></span><br><span class="line"># 基于正则表达式匹配文件路径(同上，但是忽略大小写)</span><br><span class="line">find . -iregex &quot;.*\(\.txt\|\.pdf\)$&quot;</span><br><span class="line"></span><br><span class="line"># 要列出所有长度为零的文件</span><br><span class="line">find . -empty</span><br><span class="line"></span><br><span class="line"># 如果你有一些以 jpg 结尾的目录呢？ 所以要给搜索加上文件类型</span><br><span class="line">find ~ \( -iname &#39;*jpeg&#39; -o -iname &#39;*jpg&#39; \) -type f </span><br><span class="line"></span><br><span class="line"># 在 log 目录下找到所有巨大的（定义为“大于 1GB”）文件：</span><br><span class="line">find &#x2F;var&#x2F;log -size +1G</span><br><span class="line"></span><br><span class="line"># 在主目录中找到对所有人可读的文件</span><br><span class="line">find ~ -perm -o&#x3D;r</span><br><span class="line"></span><br><span class="line"># 删除 mac 下自动生成的文件</span><br><span class="line">find .&#x2F; -name &#39;__MACOSX&#39; -depth -exec rm -rf &#123;&#125; \;</span><br><span class="line"></span><br><span class="line"># 统计代码行数</span><br><span class="line">find . -name &quot;*.java&quot;|xargs cat|grep -v ^$|wc -l # 代码行数统计, 排除空行</span><br></pre></td></tr></table></figure>
<h5 id="fou-ding-can-shu">否定参数</h5>
<figure class="highlight arduino"><table><tr><td class="code"><pre><span class="line"># 找出/<span class="built_in">home</span>下不是以.txt结尾的文件</span><br><span class="line"><span class="built_in">find</span> /<span class="built_in">home</span> ! -name <span class="string">"*.txt"</span></span><br></pre></td></tr></table></figure>
<h5 id="gen-ju-wen-jian-lei-xing-jin-xing-sou-suo">根据文件类型进行搜索</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find . -type 类型参数</span><br><span class="line"></span><br><span class="line">类型参数列表：</span><br><span class="line">    f 普通文件</span><br><span class="line">    l 符号连接</span><br><span class="line">    d 目录</span><br><span class="line">    c 字符设备</span><br><span class="line">    b 块设备</span><br><span class="line">    s 套接字</span><br><span class="line">    p Fifo</span><br></pre></td></tr></table></figure>
<h5 id="ji-yu-mu-lu-shen-du-sou-suo">基于目录深度搜索</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 向下最大深度限制为3</span><br><span class="line">find . -maxdepth 3 -type f</span><br><span class="line"></span><br><span class="line"># 搜索出深度距离当前目录至少2个子目录的所有文件</span><br><span class="line">find . -mindepth 2 -type f</span><br></pre></td></tr></table></figure>
<h5 id="gen-ju-wen-jian-shi-jian-chuo-jin-xing-sou-suo">根据文件时间戳进行搜索</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find . -type f 时间戳</span><br><span class="line"></span><br><span class="line">UNIX&#x2F;Linux文件系统每个文件都有三种时间戳：</span><br><span class="line">    访问时间 （-atime&#x2F;天，-amin&#x2F;分钟）：用户最近一次访问时间。</span><br><span class="line">    修改时间 （-mtime&#x2F;天，-mmin&#x2F;分钟）：文件最后一次修改时间。</span><br><span class="line">    变化时间 （-ctime&#x2F;天，-cmin&#x2F;分钟）：文件数据元（例如权限等）最后一次修改时间。</span><br><span class="line">    </span><br><span class="line"># 搜索最近七天内被访问过的所有文件</span><br><span class="line">find . -type f -atime -7</span><br><span class="line"></span><br><span class="line"># 搜索恰好在七天前被访问过的所有文件</span><br><span class="line">find . -type f -atime 7</span><br><span class="line"></span><br><span class="line"># 搜索超过七天内被访问过的所有文件</span><br><span class="line">find . -type f -atime +7</span><br><span class="line"></span><br><span class="line"># 搜索访问时间超过10分钟的所有文件</span><br><span class="line">find . -type f -amin +10</span><br><span class="line"></span><br><span class="line"># 找出比file.log修改时间更长的所有文件</span><br><span class="line">find . -type f -newer file.log</span><br></pre></td></tr></table></figure>
<h5 id="gen-ju-wen-jian-da-xiao-jin-xing-pi-pei">根据文件大小进行匹配</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find . -type f -size 文件大小单元</span><br><span class="line"></span><br><span class="line">文件大小单元：</span><br><span class="line">    b —— 块（512字节）</span><br><span class="line">    c —— 字节</span><br><span class="line">    w —— 字（2字节）</span><br><span class="line">    k —— 千字节</span><br><span class="line">    M —— 兆字节</span><br><span class="line">    G —— 吉字节</span><br><span class="line"></span><br><span class="line"># 搜索大于10KB的文件</span><br><span class="line">find . -type f -size +10k</span><br><span class="line"></span><br><span class="line"># 搜索小于10KB的文件</span><br><span class="line">find . -type f -size -10k</span><br><span class="line"></span><br><span class="line"># 搜索等于10KB的文件</span><br><span class="line">find . -type f -size 10k</span><br></pre></td></tr></table></figure>
<h5 id="shan-chu-pi-pei-wen-jian">删除匹配文件</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 删除当前目录下所有.txt文件</span><br><span class="line">find . -type f -name &quot;*.txt&quot; -delete</span><br></pre></td></tr></table></figure>
<h5 id="gen-ju-wen-jian-quan-xian-suo-you-quan-jin-xing-pi-pei">根据文件权限/所有权进行匹配</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 当前目录下搜索出权限为777的文件</span><br><span class="line">find . -type f -perm 777</span><br><span class="line"></span><br><span class="line"># 找出当前目录下权限不是644的php文件</span><br><span class="line">find . -type f -name &quot;*.php&quot; ! -perm 644</span><br><span class="line"></span><br><span class="line"># 找出当前目录用户tom拥有的所有文件</span><br><span class="line">find . -type f -user tom</span><br><span class="line"></span><br><span class="line"># 找出当前目录用户组sunk拥有的所有文件</span><br><span class="line">find . -type f -group sunk</span><br></pre></td></tr></table></figure>
<h5 id="jie-zhu-code-exec-code-xuan-xiang-yu-qi-ta-ming-ling-jie-he-shi-yong">借助<code>-exec</code>选项与其他命令结合使用</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 找出当前目录下所有root的文件，并把所有权更改为用户tom，&#123;&#125; 用于与 -exec 选项结合使用来匹配所有文件，然后会被替换为相应的文件名。</span><br><span class="line">find .-type f -user root -exec chown tom &#123;&#125; \;</span><br><span class="line"></span><br><span class="line"># 找出自己home目录下所有的.txt文件并删除， -ok 和 -exec 行为一样，不过它会给出提示，是否执行相应的操作。</span><br><span class="line">find $HOME&#x2F;. -name &quot;*.txt&quot; -ok rm &#123;&#125; \;</span><br><span class="line"></span><br><span class="line"># 查找当前目录下所有.txt文件并把他们拼接起来写入到all.txt文件中</span><br><span class="line">find . -type f -name &quot;*.txt&quot; -exec cat &#123;&#125; \;&gt; &#x2F;all.txt</span><br><span class="line"></span><br><span class="line"># 将30天前的.log文件移动到old目录中</span><br><span class="line">find . -type f -mtime +30 -name &quot;*.log&quot; -exec cp &#123;&#125; old \;</span><br><span class="line"></span><br><span class="line"># 找出当前目录下所有.txt文件并以“File:文件名”的形式打印出来</span><br><span class="line">find . -type f -name &quot;*.txt&quot; -exec printf &quot;File: %s\n&quot; &#123;&#125; \;</span><br><span class="line"></span><br><span class="line"># 因为单行命令中-exec参数中无法使用多个命令，以下方法可以实现在-exec之后接受多条命令</span><br><span class="line">-exec .&#x2F;text.sh &#123;&#125; \;</span><br></pre></td></tr></table></figure>
<h5 id="sou-suo-dan-shi-tiao-guo-zhi-ding-mu-lu">搜索但是跳过指定目录</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查找当前目录或者子目录下所有.txt文件，但是跳过子目录sk</span><br><span class="line">find . -path &quot;.&#x2F;sk&quot; -prune -o -name &quot;*.txt&quot; -print</span><br></pre></td></tr></table></figure>
<h3 id="wen-ben-nei-rong-sou-suo-grep">文本内容搜索:grep</h3>
<h4 id="shuo-ming-1">说明</h4>
<p><strong>grep</strong> （global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。用于过滤/搜索的特定字符。可使用正则表达式能多种命令配合使用。</p>
<h4 id="yu-fa-1">语法</h4>
<p>在文件中搜索一个单词，命令会返回一个包含 <strong>“match_pattern”</strong> 的文本行：</p>
<figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line"><span class="keyword">grep</span> match_pattern file_name</span><br><span class="line"><span class="keyword">grep</span> <span class="string">"match_pattern"</span> file_name</span><br></pre></td></tr></table></figure>
<h4 id="xuan-xiang-1">选项</h4>
<figure class="highlight haml"><table><tr><td class="code"><pre><span class="line">-<span class="ruby">a --text  <span class="comment"># 不要忽略二进制数据。</span></span></span><br><span class="line"><span class="ruby">-A &lt;显示行数&gt;   --after-context=&lt;显示行数&gt;   <span class="comment"># 除了显示符合范本样式的那一行之外，并显示该行之后的内容。</span></span></span><br><span class="line"><span class="ruby">-b --byte-offset                           <span class="comment"># 在显示符合范本样式的那一行之外，并显示该行之前的内容。</span></span></span><br><span class="line"><span class="ruby">-B&lt;显示行数&gt;   --before-context=&lt;显示行数&gt;   <span class="comment"># 除了显示符合样式的那一行之外，并显示该行之前的内容。</span></span></span><br><span class="line"><span class="ruby">-c --count    <span class="comment"># 计算符合范本样式的列数。</span></span></span><br><span class="line"><span class="ruby">-C&lt;显示行数&gt; --context=&lt;显示行数&gt;或-&lt;显示行数&gt; <span class="comment"># 除了显示符合范本样式的那一列之外，并显示该列之前后的内容。</span></span></span><br><span class="line"><span class="ruby">-d&lt;进行动作&gt; --directories=&lt;动作&gt;  <span class="comment"># 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep命令将回报信息并停止动作。</span></span></span><br><span class="line"><span class="ruby">-e&lt;范本样式&gt; --regexp=&lt;范本样式&gt;   <span class="comment"># 指定字符串作为查找文件内容的范本样式。</span></span></span><br><span class="line"><span class="ruby">-E --extended-regexp             <span class="comment"># 将范本样式为延伸的普通表示法来使用，意味着使用能使用扩展正则表达式。</span></span></span><br><span class="line"><span class="ruby">-f&lt;范本文件&gt; --file=&lt;规则文件&gt;     <span class="comment"># 指定范本文件，其内容有一个或多个范本样式，让grep查找符合范本条件的文件内容，格式为每一列的范本样式。</span></span></span><br><span class="line"><span class="ruby">-F --fixed-regexp   <span class="comment"># 将范本样式视为固定字符串的列表。</span></span></span><br><span class="line"><span class="ruby">-G --basic-regexp   <span class="comment"># 将范本样式视为普通的表示法来使用。</span></span></span><br><span class="line"><span class="ruby">-h --no-filename    <span class="comment"># 在显示符合范本样式的那一列之前，不标示该列所属的文件名称。</span></span></span><br><span class="line"><span class="ruby">-H --with-filename  <span class="comment"># 在显示符合范本样式的那一列之前，标示该列的文件名称。</span></span></span><br><span class="line"><span class="ruby">-i --ignore-<span class="keyword">case</span>    <span class="comment"># 忽略字符大小写的差别。</span></span></span><br><span class="line"><span class="ruby">-l --file-with-matches   <span class="comment"># 列出文件内容符合指定的范本样式的文件名称。</span></span></span><br><span class="line"><span class="ruby">-L --files-without-match <span class="comment"># 列出文件内容不符合指定的范本样式的文件名称。</span></span></span><br><span class="line"><span class="ruby">-n --line-number         <span class="comment"># 在显示符合范本样式的那一列之前，标示出该列的编号。</span></span></span><br><span class="line"><span class="ruby">-P --perl-regexp         <span class="comment"># PATTERN 是一个 Perl 正则表达式</span></span></span><br><span class="line"><span class="ruby">-q --quiet或--silent     <span class="comment"># 不显示任何信息。</span></span></span><br><span class="line"><span class="ruby">-R/-r  --recursive       <span class="comment"># 此参数的效果和指定“-d recurse”参数相同。</span></span></span><br><span class="line"><span class="ruby">-s --no-messages  <span class="comment"># 不显示错误信息。</span></span></span><br><span class="line"><span class="ruby">-v --revert-match <span class="comment"># 反转查找。</span></span></span><br><span class="line"><span class="ruby">-V --version      <span class="comment"># 显示版本信息。   </span></span></span><br><span class="line"><span class="ruby">-w --word-regexp  <span class="comment"># 只显示全字符合的列。</span></span></span><br><span class="line"><span class="ruby">-x --line-regexp  <span class="comment"># 只显示全列符合的列。</span></span></span><br><span class="line"><span class="ruby">-y <span class="comment"># 此参数效果跟“-i”相同。</span></span></span><br><span class="line"><span class="ruby">-o <span class="comment"># 只输出文件中匹配到的部分。</span></span></span><br><span class="line"><span class="ruby">-m &lt;num&gt; --max-count=&lt;num&gt; <span class="comment"># 找到num行结果后停止查找，用来限制匹配行数</span></span></span><br></pre></td></tr></table></figure>
<h4 id="zheng-ze-biao-da-shi">正则表达式</h4>
<figure class="highlight autoit"><table><tr><td class="code"><pre><span class="line">^    <span class="meta"># 锚定行的开始 如：<span class="string">'^grep'</span>匹配所有以grep开头的行。    </span></span><br><span class="line">$    <span class="meta"># 锚定行的结束 如：<span class="string">'grep$'</span> 匹配所有以grep结尾的行。</span></span><br><span class="line">.    <span class="meta"># 匹配一个非换行符的字符 如：<span class="string">'gr.p'</span>匹配gr后接一个任意字符，然后是p。    </span></span><br><span class="line">*    <span class="meta"># 匹配零个或多个先前字符 如：<span class="string">'*grep'</span>匹配所有一个或多个空格后紧跟grep的行。    </span></span><br><span class="line">.*   <span class="meta"># 一起用代表任意字符。   </span></span><br><span class="line">[]   <span class="meta"># 匹配一个指定范围内的字符，如<span class="string">'[Gg]rep'</span>匹配Grep和grep。    </span></span><br><span class="line">[^]  <span class="meta"># 匹配一个不在指定范围内的字符，如：<span class="string">'[^A-FH-Z]rep'</span>匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。    </span></span><br><span class="line">\(..\)  <span class="meta"># 标记匹配字符，如<span class="string">'\(love\)'</span>，love被标记为1。    </span></span><br><span class="line">\&lt;      <span class="meta"># 锚定单词的开始，如:<span class="string">'\&lt;grep'</span>匹配包含以grep开头的单词的行。    </span></span><br><span class="line">\&gt;      <span class="meta"># 锚定单词的结束，如<span class="string">'grep\&gt;'</span>匹配包含以grep结尾的单词的行。    </span></span><br><span class="line">x\&#123;m\&#125;  <span class="meta"># 重复字符x，m次，如：<span class="string">'0\&#123;5\&#125;'</span>匹配包含5个o的行。    </span></span><br><span class="line">x\&#123;m,\&#125;   <span class="meta"># 重复字符x,至少m次，如：<span class="string">'o\&#123;5,\&#125;'</span>匹配至少有5个o的行。    </span></span><br><span class="line">x\&#123;m,n\&#125;  <span class="meta"># 重复字符x，至少m次，不多于n次，如：<span class="string">'o\&#123;5,10\&#125;'</span>匹配5--10个o的行。   </span></span><br><span class="line">\w    <span class="meta"># 匹配文字和数字字符，也就是[A-Za-z0-9]，如：<span class="string">'G\w*p'</span>匹配以G后跟零个或多个文字或数字字符，然后是p。   </span></span><br><span class="line">\W    <span class="meta"># \w的反置形式，匹配一个或多个非单词字符，如点号句号等。   </span></span><br><span class="line">\b    <span class="meta"># 单词锁定符，如: <span class="string">'\bgrep\b'</span>只匹配grep。</span></span><br></pre></td></tr></table></figure>
<h4 id="shi-li-1">实例</h4>
<figure class="highlight vala"><table><tr><td class="code"><pre><span class="line"><span class="meta"># 在多个文件中查找：</span></span><br><span class="line">grep <span class="string">"match_pattern"</span> file_1 file_2 file_3 ...</span><br><span class="line"></span><br><span class="line"><span class="meta">#  -v 选项：输出除match_pattern之外的所有行</span></span><br><span class="line">grep -v <span class="string">"match_pattern"</span> file_name</span><br><span class="line"></span><br><span class="line"><span class="meta"># 标记匹配颜色 --color=auto 选项：</span></span><br><span class="line">grep <span class="string">"match_pattern"</span> file_name --color=auto</span><br><span class="line"></span><br><span class="line"><span class="meta"># 使用正则表达式 -E 选项：</span></span><br><span class="line">grep -E <span class="string">"[1-9]+"</span>  or  egrep <span class="string">"[1-9]+"</span></span><br><span class="line">echo <span class="keyword">this</span> is a test line. | grep -o -E <span class="string">"[a-z]+\."</span> 输出：line.</span><br><span class="line">echo <span class="keyword">this</span> is a test line. | egrep -o <span class="string">"[a-z]+\."</span> 输出：line.</span><br><span class="line"></span><br><span class="line"><span class="meta"># 使用正则表达式 -P 选项：</span></span><br><span class="line">grep -P <span class="string">"(\d&#123;3&#125;\-)&#123;2&#125;\d&#123;4&#125;"</span> file_name</span><br><span class="line"></span><br><span class="line"><span class="meta"># -c 选项：统计文件或者文本中包含匹配字符串的行数 </span></span><br><span class="line">grep -c <span class="string">"text"</span> file_name</span><br><span class="line"></span><br><span class="line"><span class="meta"># -n 选项：输出包含匹配字符串的行数 </span></span><br><span class="line">grep <span class="string">"text"</span> -n file_name   or  cat file_name | grep <span class="string">"text"</span> -n</span><br><span class="line"><span class="meta"># 多个文件</span></span><br><span class="line">grep <span class="string">"text"</span> -n file_1 file_2</span><br><span class="line"></span><br><span class="line"><span class="meta"># 打印样式匹配所位于的字符或字节偏移：</span></span><br><span class="line">echo gun is not unix | grep -b -o <span class="string">"not"</span>  输出：<span class="number">7</span>:not</span><br><span class="line"><span class="meta"># 一行中字符串的字符偏移是从该行的第一个字符开始计算，起始值为0。选项  -b -o  一般总是配合使用。</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 搜索多个文件并查找匹配文本在哪些文件中：</span></span><br><span class="line">grep -l <span class="string">"text"</span> file1 file2 file3...</span><br></pre></td></tr></table></figure>
<h4 id="di-gui-sou-suo">递归搜索</h4>
<figure class="highlight perl"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在多级目录中对文本进行递归搜索, .表示当前目录。</span></span><br><span class="line"><span class="keyword">grep</span> <span class="string">"text"</span> . -r -n</span><br></pre></td></tr></table></figure>
<h4 id="hu-lue-da-xiao-xie">忽略大小写</h4>
<figure class="highlight 1c"><table><tr><td class="code"><pre><span class="line"><span class="meta"># 忽略匹配样式中的字符大小写：</span></span><br><span class="line">echo <span class="string">"hello world"</span> <span class="string">| grep -i "</span>HELLO<span class="string">"   输出:hello</span></span><br></pre></td></tr></table></figure>
<h4 id="duo-ge-pi-pei-yang-shi">多个匹配样式</h4>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -e:自动多个匹配样式</span></span><br><span class="line">echo this is a text line | grep -e <span class="string">"is"</span> -e <span class="string">"line"</span> -o  输出:is line</span><br><span class="line"></span><br><span class="line"><span class="comment"># -f:自动匹配多个样式</span></span><br><span class="line">cat patfile</span><br><span class="line">aaa</span><br><span class="line">bbb</span><br><span class="line"></span><br><span class="line">echo<span class="built_in"> aaa </span>bbb ccc ddd eee | grep -f patfile -o</span><br></pre></td></tr></table></figure>
<h4 id="zai-grep-sou-suo-jie-guo-zhong-bao-gua-huo-zhe-pai-chu-zhi-ding-wen-jian">在grep搜索结果中包括或者排除指定文件</h4>
<figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line"># 只在目录中所有的.php和.html文件中递归搜索字符<span class="string">"main()"</span></span><br><span class="line"><span class="keyword">grep</span> <span class="string">"main()"</span> . -r --<span class="keyword">include</span> *.&#123;php,html&#125;</span><br><span class="line"></span><br><span class="line"># 在搜索结果中排除所有README文件</span><br><span class="line"><span class="keyword">grep</span> <span class="string">"main()"</span> . -r --<span class="keyword">exclude</span> <span class="string">"README"</span></span><br><span class="line"></span><br><span class="line"># 在搜索结果中排除filelist文件列表里的文件</span><br><span class="line"><span class="keyword">grep</span> <span class="string">"main()"</span> . -r --<span class="keyword">exclude</span>-<span class="keyword">from</span> filelist</span><br></pre></td></tr></table></figure>
<h4 id="shi-yong-0-zhi-zi-jie-hou-zhui">使用0值字节后缀</h4>
<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line"># 测试文件：</span><br><span class="line"><span class="keyword">echo</span> <span class="string">"aaa"</span> &gt; file1</span><br><span class="line"><span class="keyword">echo</span> <span class="string">"bbb"</span> &gt; file2</span><br><span class="line"><span class="keyword">echo</span> <span class="string">"aaa"</span> &gt; file3</span><br><span class="line"></span><br><span class="line"><span class="keyword">grep</span> <span class="string">"aaa"</span> <span class="keyword">file</span>* -lZ | xargs -<span class="number">0</span> rm</span><br><span class="line"></span><br><span class="line"># 执行后会删除file1和file3，<span class="keyword">grep</span>输出用-Z选项来指定以<span class="number">0</span>值字节作为终结符文件名（\<span class="number">0</span>），xargs -<span class="number">0</span> 读取输入并用<span class="number">0</span>值字节终结符分隔文件名，然后删除匹配文件，-Z通常和-<span class="keyword">l</span>结合使用。</span><br></pre></td></tr></table></figure>
<h4 id="jing-mo-shu-chu">静默输出</h4>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line"># 不会输出任何信息，如果命令运行成功返回<span class="number">0</span>，失败则返回非<span class="number">0</span>值。一般用于条件测试。</span><br><span class="line">grep -q <span class="string">"test"</span> filename</span><br></pre></td></tr></table></figure>
<h4 id="da-yin-chu-pi-pei-wen-ben-zhi-qian-huo-zhe-zhi-hou-de-xing">打印出匹配文本之前或者之后的行</h4>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line"># 显示匹配某个结果之后的<span class="number">3</span>行，使用 -A 选项：</span><br><span class="line">seq <span class="number">10</span> | grep <span class="string">"5"</span> -A <span class="number">3</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">8</span></span><br><span class="line"></span><br><span class="line"># 显示匹配某个结果之前的<span class="number">3</span>行，使用 -B 选项：</span><br><span class="line">seq <span class="number">10</span> | grep <span class="string">"5"</span> -B <span class="number">3</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"></span><br><span class="line"># 显示匹配某个结果的前三行和后三行，使用 -C 选项：</span><br><span class="line">seq <span class="number">10</span> | grep <span class="string">"5"</span> -C <span class="number">3</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">8</span></span><br><span class="line"></span><br><span class="line"># 如果匹配结果有多个，会用“--”作为各匹配结果之间的分隔符：</span><br><span class="line">echo -e <span class="string">"a<span class="subst">\n</span>b<span class="subst">\n</span>c<span class="subst">\n</span>a<span class="subst">\n</span>b<span class="subst">\n</span>c"</span> | grep a -A <span class="number">1</span></span><br><span class="line">a</span><br><span class="line">b</span><br><span class="line">--</span><br><span class="line">a</span><br><span class="line">b</span><br></pre></td></tr></table></figure>
<h3 id="wen-ben-bian-ji-sed">文本编辑:sed</h3>
<h4 id="shuo-ming-2">说明</h4>
<p><strong>sed</strong> 是一种流编辑器，它是文本处理中非常重要的工具，能够完美的配合正则表达式使用，功能不同凡响。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有 改变，除非你使用重定向存储输出。Sed主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等。</p>
<h4 id="yu-fa-2">语法</h4>
<figure class="highlight gams"><table><tr><td class="code"><pre><span class="line">sed [<span class="keyword">options</span>] <span class="string">'command'</span> <span class="keyword">file</span>(s)</span><br><span class="line">sed [<span class="keyword">options</span>] -f scriptfile <span class="keyword">file</span>(s)</span><br></pre></td></tr></table></figure>
<h4 id="xuan-xiang-2">选项</h4>
<figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line">-e&lt;<span class="keyword">script</span>&gt;或<span class="comment">--expression=&lt;script&gt;：以选项中的指定的script来处理输入的文本文件；</span></span><br><span class="line">-f&lt;<span class="keyword">script</span>文件&gt;或<span class="comment">--file=&lt;script文件&gt;：以选项中指定的script文件来处理输入的文本文件；</span></span><br><span class="line">-h或<span class="comment">--help：显示帮助；</span></span><br><span class="line">-n或<span class="comment">--quiet或——silent：仅显示script处理后的结果；</span></span><br><span class="line">-V或<span class="comment">--version：显示版本信息。</span></span><br></pre></td></tr></table></figure>
<h4 id="sed-ming-ling">sed 命令</h4>
<figure class="highlight clean"><table><tr><td class="code"><pre><span class="line">a\ # 在当前行下面插入文本。</span><br><span class="line">i\ # 在当前行上面插入文本。</span><br><span class="line">c\ # 把选定的行改为新的文本。</span><br><span class="line">d # 删除，删除选择的行。</span><br><span class="line">D # 删除模板块的第一行。</span><br><span class="line">s # 替换指定字符</span><br><span class="line">h # 拷贝模板块的内容到内存中的缓冲区。</span><br><span class="line">H # 追加模板块的内容到内存中的缓冲区。</span><br><span class="line">g # 获得内存缓冲区的内容，并替代当前模板块中的文本。</span><br><span class="line">G # 获得内存缓冲区的内容，并追加到当前模板块文本的后面。</span><br><span class="line">l # 列表不能打印字符的清单。</span><br><span class="line">n # 读取下一个输入行，用下一个命令处理新的行而不是用第一个命令。</span><br><span class="line">N # 追加下一个输入行到模板块后面并在二者间嵌入一个新行，改变当前行号码。</span><br><span class="line">p # 打印模板块的行。</span><br><span class="line">P # (大写) 打印模板块的第一行。</span><br><span class="line">q # 退出Sed。</span><br><span class="line">b lable # 分支到脚本中带有标记的地方，如果分支不存在则分支到脚本的末尾。</span><br><span class="line">r file # 从file中读行。</span><br><span class="line">t label # <span class="keyword">if</span>分支，从最后一行开始，条件一旦满足或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。</span><br><span class="line">T label # 错误分支，从最后一行开始，一旦发生错误或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。</span><br><span class="line">w file # 写并追加模板块到file末尾。  </span><br><span class="line">W file # 写并追加模板块的第一行到file末尾。  </span><br><span class="line">! # 表示后面的命令对所有没有被选定的行发生作用。  </span><br><span class="line">= # 打印当前行号码。  </span><br><span class="line"># # 把注释扩展到下一个换行符以前。  </span><br><span class="line"></span><br><span class="line"># ##############################替换标记###################################</span><br><span class="line">g # 表示行内全面替换。  </span><br><span class="line">p # 表示打印行。  </span><br><span class="line">w # 表示把行写入一个文件。  </span><br><span class="line">x # 表示互换模板块中的文本和缓冲区中的文本。  </span><br><span class="line">y # 表示把一个字符翻译为另外的字符（但是不用于正则表达式）</span><br><span class="line">\<span class="number">1</span> # 子串匹配标记</span><br><span class="line">&amp; # 已匹配字符串标记</span><br><span class="line"></span><br><span class="line"># ##############################sed 正则###################################</span><br><span class="line">^ # 匹配行开始，如：/^sed/匹配所有以sed开头的行。</span><br><span class="line">$ # 匹配行结束，如：/sed$/匹配所有以sed结尾的行。</span><br><span class="line">. # 匹配一个非换行符的任意字符，如：/s.d/匹配s后接一个任意字符，最后是d。</span><br><span class="line">* # 匹配<span class="number">0</span>个或多个字符，如：<span class="comment">/*sed/匹配所有模板是一个或多个空格后紧跟sed的行。</span></span><br><span class="line"><span class="comment">[] # 匹配一个指定范围内的字符，如/[ss]ed/匹配sed和Sed。  </span></span><br><span class="line"><span class="comment">[^] # 匹配一个不在指定范围内的字符，如：/[^A-RT-Z]ed/匹配不包含A-R和T-Z的一个字母开头，紧跟ed的行。</span></span><br><span class="line"><span class="comment">\(..\) # 匹配子串，保存匹配的字符，如s/\(love\)able/\1rs，loveable被替换成lovers。</span></span><br><span class="line"><span class="comment">&amp; # 保存搜索字符用来替换其他字符，如s/love/ **&amp;** /，love这成 **love** 。</span></span><br><span class="line"><span class="comment">\&lt; # 匹配单词的开始，如:/\&lt;love/匹配包含以love开头的单词的行。</span></span><br><span class="line"><span class="comment">\&gt; # 匹配单词的结束，如/love\&gt;/匹配包含以love结尾的单词的行。</span></span><br><span class="line"><span class="comment">x\&#123;m\&#125; # 重复字符x，m次，如：/0\&#123;5\&#125;/匹配包含5个0的行。</span></span><br><span class="line"><span class="comment">x\&#123;m,\&#125; # 重复字符x，至少m次，如：/0\&#123;5,\&#125;/匹配至少有5个0的行。</span></span><br><span class="line"><span class="comment">x\&#123;m,n\&#125; # 重复字符x，至少m次，不多于n次，如：/0\&#123;5,10\&#125;/匹配5~10个0的行。</span></span><br></pre></td></tr></table></figure>
<h4 id="yong-fa-shi-li">用法实例</h4>
<h5 id="ti-huan-cao-zuo-s-ming-ling">替换操作：s命令</h5>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 替换文本中的字符串：</span></span><br><span class="line">sed <span class="string">'s/book/books/'</span> <span class="built_in">file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -n选项 和 p命令 一起使用表示只打印那些发生替换的行：</span></span><br><span class="line">sed -n <span class="string">'s/test/TEST/p'</span> <span class="built_in">file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接编辑文件 选项-i ，会匹配file文件中每一行的所有book替换为books：</span></span><br><span class="line">sed -i <span class="string">'s/book/books/g'</span> <span class="built_in">file</span></span><br></pre></td></tr></table></figure>
<h5 id="quan-mian-ti-huan-biao-ji-g">全面替换标记g</h5>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用后缀 /g 标记会替换每一行中的所有匹配：</span></span><br><span class="line">sed <span class="string">'s/book/books/g'</span> file</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当需要从第N处匹配开始替换时，可以使用 /Ng：</span></span><br><span class="line"><span class="built_in">echo</span> sksksksksksk | sed <span class="string">'s/sk/SK/2g'</span></span><br><span class="line">skSKSKSKSKSK</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> sksksksksksk | sed <span class="string">'s/sk/SK/3g'</span></span><br><span class="line">skskSKSKSKSK</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> sksksksksksk | sed <span class="string">'s/sk/SK/4g'</span></span><br><span class="line">skskskSKSKSK</span><br></pre></td></tr></table></figure>
<h5 id="ding-jie-fu">定界符</h5>
<figure class="highlight sml"><table><tr><td class="code"><pre><span class="line"> #  / 在sed中作为定界符使用，也可以使用任意的定界符</span><br><span class="line"> sed <span class="symbol">'s</span>:test:<span class="type">TEXT</span>:g'</span><br><span class="line">sed <span class="symbol">'s</span>|test|<span class="type">TEXT</span>|g'</span><br><span class="line"></span><br><span class="line"># 定界符出现在样式内部时，需要进行转义：</span><br><span class="line">sed <span class="symbol">'s</span>/\/bin/\/usr\/<span class="keyword">local</span>\/bin/g'</span><br></pre></td></tr></table></figure>
<h5 id="shan-chu-cao-zuo-d-ming-ling">删除操作：d命令</h5>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 删除空白行：</span></span><br><span class="line">sed <span class="string">'/^$/d'</span> <span class="built_in">file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件的第2行：</span></span><br><span class="line">sed <span class="string">'2d'</span> <span class="built_in">file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件的第2行到末尾所有行：</span></span><br><span class="line">sed <span class="string">'2,$d'</span> <span class="built_in">file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件最后一行：</span></span><br><span class="line">sed <span class="string">'$d'</span> <span class="built_in">file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件中所有开头是test的行：</span></span><br><span class="line">sed <span class="string">'/^test/'</span>d <span class="built_in">file</span></span><br></pre></td></tr></table></figure>
<h5 id="yi-pi-pei-zi-fu-chuan-biao-ji-amp">已匹配字符串标记&amp;</h5>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"># 正则表达式 \w+ 匹配每一个单词，使用 [&amp;] 替换它，&amp; 对应于之前所匹配到的单词</span><br><span class="line">echo <span class="keyword">this</span> <span class="keyword">is</span> a test line | sed <span class="string">'s/\w\+/[&amp;]/g'</span></span><br><span class="line">[<span class="keyword">this</span>] [<span class="keyword">is</span>] [a] [test] [line]</span><br><span class="line"></span><br><span class="line"># 所有以<span class="number">192.168</span><span class="number">.0</span><span class="number">.1</span>开头的行都会被替换成它自已加localhost：</span><br><span class="line">sed <span class="string">'s/^192.168.0.1/&amp;localhost/'</span> file</span><br><span class="line"><span class="number">192.168</span><span class="number">.0</span><span class="number">.1</span>localhost</span><br></pre></td></tr></table></figure>
<h5 id="zi-chuan-pi-pei-biao-ji-1">子串匹配标记\1</h5>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 匹配给定样式的其中一部分：命令中 digit 7，被替换成了 7。样式匹配到的子串是 7，(..) 用于匹配子串，对于匹配到的第一个子串就标记为 \1 ，依此类推匹配到的第二个结果就是 \2</span></span><br><span class="line">echo this is digit 7 <span class="keyword">in</span> a number | sed <span class="string">'s/digit \([0-9]\)/\1/'</span></span><br><span class="line">this is 7 <span class="keyword">in</span> a number</span><br><span class="line"></span><br><span class="line">echo<span class="built_in"> aaa </span>BBB | sed <span class="string">'s/\([a-z]\+\) \([A-Z]\+\)/\2 \1/'</span></span><br><span class="line">BBB aaa</span><br><span class="line"></span><br><span class="line"><span class="comment"># love被标记为1，所有loveable会被替换成lovers，并打印出来：</span></span><br><span class="line">sed -n <span class="string">'s/\(love\)able/\1rs/p'</span> file</span><br></pre></td></tr></table></figure>
<h5 id="zu-he-duo-ge-biao-da-shi">组合多个表达式</h5>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="attribute">sed</span> <span class="string">'表达式'</span> | sed <span class="string">'表达式'</span></span><br><span class="line"></span><br><span class="line">等价于：</span><br><span class="line"></span><br><span class="line">sed <span class="string">'表达式; 表达式'</span></span><br></pre></td></tr></table></figure>
<h5 id="yin-yong">引用</h5>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sed表达式可以使用单引号来引用，但是如果表达式内部包含变量字符串，就需要使用双引号。</span></span><br><span class="line"><span class="built_in">test</span>=hello</span><br><span class="line"><span class="built_in">echo</span> hello WORLD | sed <span class="string">"s/<span class="variable">$test</span>/HELLO"</span></span><br><span class="line">HELLO WORLD</span><br></pre></td></tr></table></figure>
<h5 id="xuan-ding-xing-de-fan-wei-dou-hao">选定行的范围：,（逗号）</h5>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 所有在模板test和check所确定的范围内的行都被打印</span></span><br><span class="line">sed -n '/<span class="keyword">test</span>/,/check/p' <span class="keyword">file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印从第5行开始到第一个包含以test开始的行之间的所有行：</span></span><br><span class="line">sed -n '<span class="number">5</span>,/^<span class="keyword">test</span>/p' <span class="keyword">file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于模板test和west之间的行，每行的末尾用字符串aaa bbb替换：</span></span><br><span class="line">sed '/<span class="keyword">test</span>/,/west/s/$/aaa bbb/' <span class="keyword">file</span></span><br></pre></td></tr></table></figure>
<h5 id="duo-dian-bian-ji-e-ming-ling">多点编辑：e命令</h5>
<figure class="highlight mel"><table><tr><td class="code"><pre><span class="line"># -e选项允许在同一行里执行多条命令：sed表达式的第一条命令删除<span class="number">1</span>至<span class="number">5</span>行，第二条命令用check替换test。命令的执行顺序对结果有影响。如果两个命令都是替换命令，那么第一个替换命令将影响第二个替换命令的结果。</span><br><span class="line">sed -e <span class="string">'1,5d'</span> -e <span class="string">'s/test/check/'</span> <span class="keyword">file</span></span><br><span class="line"></span><br><span class="line"># 和 -e 等价的命令是 --<span class="keyword">expression</span>：</span><br><span class="line">sed --<span class="keyword">expression</span>=<span class="string">'s/test/check/'</span> --<span class="keyword">expression</span>=<span class="string">'/love/d'</span> <span class="keyword">file</span></span><br><span class="line">从文件读入：r命令</span><br></pre></td></tr></table></figure>
<h5 id="cong-wen-jian-du-ru-r-ming-ling">从文件读入：r命令</h5>
<figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"># <span class="keyword">file</span>里的内容被读进来，显示在与<span class="keyword">test</span>匹配的行后面，如果匹配多行，则<span class="keyword">file</span>的内容将显示在所有匹配行的下面：</span><br><span class="line">sed '/<span class="keyword">test</span>/r <span class="keyword">file</span>' filename</span><br></pre></td></tr></table></figure>
<h5 id="xie-ru-wen-jian-w-ming-ling">写入文件：w命令</h5>
<figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"># 在example中所有包含<span class="keyword">test</span>的行都被写入<span class="keyword">file</span>里：</span><br><span class="line">sed -<span class="keyword">n</span> '/<span class="keyword">test</span>/w <span class="keyword">file</span>' example</span><br></pre></td></tr></table></figure>
<h5 id="zhui-jia-xing-xia-a-ming-ling">追加（行下）：a\命令</h5>
<figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"># 将 this is a <span class="keyword">test</span> <span class="keyword">line</span> 追加到 以<span class="keyword">test</span> 开头的行后面：</span><br><span class="line">sed '/^<span class="keyword">test</span>/a\this is a <span class="keyword">test</span> <span class="keyword">line</span>' <span class="keyword">file</span></span><br><span class="line"></span><br><span class="line"># 在 <span class="keyword">test</span>.<span class="keyword">conf</span> 文件第2行之后插入 this is a <span class="keyword">test</span> <span class="keyword">line</span>：</span><br><span class="line">sed -i '2a\this is a <span class="keyword">test</span> <span class="keyword">line</span>' <span class="keyword">test</span>.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure>
<h5 id="cha-ru-xing-shang-i-ming-ling">插入（行上）：i\命令</h5>
<figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"># 将 this is a <span class="keyword">test</span> <span class="keyword">line</span> 追加到以<span class="keyword">test</span>开头的行前面</span><br><span class="line">sed '/^<span class="keyword">test</span>/i\this is a <span class="keyword">test</span> <span class="keyword">line</span>' <span class="keyword">file</span></span><br><span class="line"></span><br><span class="line"># 在<span class="keyword">test</span>.<span class="keyword">conf</span>文件第5行之前插入this is a <span class="keyword">test</span> <span class="keyword">line</span>：</span><br><span class="line">sed -i '5i\this is a <span class="keyword">test</span> <span class="keyword">line</span>' <span class="keyword">test</span>.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure>
<h5 id="xia-yi-ge-n-ming-ling">下一个：n命令</h5>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果test被匹配，则移动到匹配行的下一行，替换这一行的aa，变为bb，并打印该行，然后继续：</span></span><br><span class="line">sed '/<span class="keyword">test</span>/&#123; n; s/aa/bb/; &#125;' <span class="keyword">file</span></span><br></pre></td></tr></table></figure>
<h5 id="bian-xing-y-ming-ling">变形：y命令</h5>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line"># 把<span class="number">1</span>~<span class="number">10</span>行内所有abcde转变为大写，注意，正则表达式元字符不能使用这个命令：</span><br><span class="line">sed '<span class="number">1</span>,<span class="number">10</span>y/abcde/ABCDE/' file</span><br></pre></td></tr></table></figure>
<h5 id="tui-chu-q-ming-ling">退出：q命令</h5>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打印完第10行后，退出sed</span></span><br><span class="line">sed <span class="string">'10q'</span> <span class="built_in">file</span></span><br></pre></td></tr></table></figure>
<h5 id="bao-chi-he-huo-qu-h-ming-ling-he-g-ming-ling">保持和获取：h命令和G命令</h5>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在sed处理文件的时候，每一行都被保存在一个叫模式空间的临时缓冲区中，除非行被删除或者输出被取消，否则所有被处理的行都将 打印在屏幕上。接着模式空间被清空，并存入新的一行等待处理。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 匹配test的行被找到后，将存入模式空间，h命令将其复制并存入一个称为保持缓存区的特殊缓冲区内。第二条语句的意思是，当到达最后一行后，G命令取出保持缓冲区的行，然后把它放回模式空间中，且追加到现在已经存在于模式空间中的行的末尾。在这个例子中就是追加到最后一行。简单来说，任何包含test的行都被复制并追加到该文件的末尾。</span></span><br><span class="line"><span class="attribute">sed</span> -e <span class="string">'/test/h'</span> -e <span class="string">'<span class="variable">$G</span>'</span> file</span><br></pre></td></tr></table></figure>
<h5 id="bao-chi-he-hu-huan-h-ming-ling-he-x-ming-ling">保持和互换：h命令和x命令</h5>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 互换模式空间和保持缓冲区的内容。也就是把包含test与check的行互换：</span></span><br><span class="line">sed -e <span class="string">'/test/h'</span> -e <span class="string">'/check/x'</span> <span class="built_in">file</span></span><br></pre></td></tr></table></figure>
<h5 id="da-yin-qi-shu-xing-huo-ou-shu-xing">打印奇数行或偶数行</h5>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="attribute">sed</span> -n <span class="string">'p;n'</span> test.txt  <span class="comment">#奇数行</span></span><br><span class="line">sed -n <span class="string">'n;p'</span> test.txt  <span class="comment">#偶数行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">sed -n <span class="string">'1~2p'</span> test.txt  <span class="comment">#奇数行</span></span><br><span class="line">sed -n <span class="string">'2~2p'</span> test.txt  <span class="comment">#偶数行</span></span><br></pre></td></tr></table></figure>
<h5 id="da-yin-pi-pei-zi-fu-chuan-de-xia-yi-xing">打印匹配字符串的下一行</h5>
<figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line"><span class="keyword">grep</span> -A <span class="number">1</span> SCC URFILE</span><br><span class="line">sed -n <span class="string">'/SCC/&#123;n;p&#125;'</span> URFILE</span><br><span class="line">awk <span class="string">'/SCC/&#123;getline; print&#125;'</span> URFILE</span><br></pre></td></tr></table></figure>
<h3 id="guan-dao">管道</h3>
<ol>
<li>| 表示管道，表示管道左边命令的结果传给管道右边。eg: ls -l | more、 ps aux | grep xxx</li>
<li>命令1 | 命令2 | 命令3</li>
</ol>
<h3 id="wen-ben-chu-li">文本处理</h3>
<ol>
<li>cut
<ol>
<li>-d：指定字段分割符，默认是空格
<ol>
<li>-d&quot;,&quot;</li>
</ol>
</li>
<li>-f： 指定要显示的字段
<ol>
<li>-f 1,3</li>
<li>-f 1-3</li>
</ol>
</li>
</ol>
</li>
<li>sort：文本排序
<ol>
<li>-n：数值排序</li>
<li>-r：降序</li>
<li>-t：字段分隔符</li>
<li>-k：以哪个字段为关键字进行排序</li>
<li>-u：排序后相同的行只显示一次</li>
<li>-f：排序时忽略字符大小写</li>
</ol>
</li>
<li>join</li>
<li>sed</li>
<li>awk</li>
</ol>
<h2 id="xi-tong-guan-li-ming-ling">系统管理命令</h2>
<ol>
<li>ps 命令可以查看进程的详细状况
<ol>
<li>-a 显示终端上所有进程，包括其他用户的进程</li>
<li>-u 显示进程的详细状态</li>
<li>-x 显示没有控制终端的进程</li>
<li>-w 显示加宽，以便显示更多的信息</li>
<li>-r 只显示正在运行的进程</li>
</ol>
</li>
<li>top 命令用来动态显示运行中的进程，可以在使用top命令时加上-d来指定显示信息更新的时间间隔。在top命令下，通过按下下列字母，来对显示结果进行排序
<ol>
<li>m 根据内存使用量来排序</li>
<li>p  根据cpu占有率来排序</li>
<li>t   根据进程运行时间的长短来排序</li>
<li>u  可以根据后面输入的用户名来筛选进程</li>
<li>k  可以根据后面输入的pid来杀死进程</li>
<li>q  退出</li>
<li>h  获得帮助</li>
</ol>
</li>
<li>kill ，killall
<ol>
<li>kill -9 pid</li>
<li>killall 进程名</li>
</ol>
</li>
</ol>
<h2 id="ci-pan-kong-jian-ming-ling">磁盘空间命令</h2>
<ol>
<li>df 命令用于检测文件系统的磁盘空间占用和剩余情况，可以显示所有文件系统对节点和磁盘块的使用情况
<ol>
<li>a 显示所有文件系统的磁盘使用情况</li>
<li>m 以1024字节为单位显示</li>
<li>t 显示各指定文件系统的磁盘空间使用情况</li>
<li>T 显示文件系统</li>
</ol>
</li>
<li>du 命令用于统计目录或文件所占磁盘空间的大小，该命令的执行结果与df类似，du更侧重于磁盘的使用情况。 格式： du [选项] 目录或文件名
<ol>
<li>a 递归显示指定目录中各文件和子目录中文件占用的数据块</li>
<li>s 显示指定文件或目录占用的数据块</li>
<li>b 以字节为单位显示磁盘占用情况</li>
<li>l 计算所有文件大小，对硬链接文件计算多次</li>
</ol>
</li>
</ol>
<h2 id="wang-luo-ming-ling">网络命令</h2>
<h3 id="cha-kan-huo-pei-zhi-wang-qia-xin-xi-ifconfig">查看或配置网卡信息：ifconfig</h3>
<h3 id="ce-shi-yuan-cheng-zhu-ji-lian-tong-xing-ping">测试远程主机连通性：ping</h3>
<ol>
<li>ping -c4 <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a></li>
</ol>
<h3 id="cha-kan-wang-luo-qing-kuang-netstat-ntpl">查看网络情况： netstat -ntpl</h3>
<h1 id="ruan-jian-an-zhuang-he-guan-li">软件安装和管理</h1>
<h2 id="ruan-jian-bao">软件包</h2>
<ol>
<li>bin文件（适合所有Linux发行版），是可执行的文件</li>
<li>rpm包，yum（redhat系列）
<ol>
<li>rpm命令：安装过程中不需要指定安装路径，rpm文件在制作的时候已经确定了安装路径</li>
<li>查询软件安装路径：rpm -ql xxx</li>
</ol>
</li>
<li>源码压缩包（适合所有的Linux发行版）</li>
<li>官方已经编译好的，下载软件包直接可以使用</li>
<li>安装步骤：
<ol>
<li>检查是否已经安装：rpm -qa | grep jdk</li>
<li>下载软件包</li>
<li>安装依赖</li>
</ol>
</li>
</ol>
<h2 id="yum">yum</h2>
<ol>
<li>解决rpm下载问题</li>
<li>解决rpm文件的查询问题</li>
<li>解决rpm安装问题</li>
<li>解决rpm的依赖问题</li>
</ol>
]]></content>
      <categories>
        <category>技术/linux</category>
      </categories>
      <tags>
        <tag>速查</tag>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title>Vim速查</title>
    <url>/2018/05/29/Vim%E9%80%9F%E6%9F%A5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="vim-visual-interface">vim（Visual Interface）</h2>
<p><img src="/2018/05/29/Vim%E9%80%9F%E6%9F%A5/NSFileHandle.png" alt="avatar"></p>
<a id="more"></a>
<h3 id="ming-ling-mo-shi">命令模式</h3>
<ol>
<li>h：左移</li>
<li>j：下移</li>
<li>k：上移</li>
<li>l(L)：右移</li>
<li><code>^</code> 或 <code>0</code>：光标移动到当前行的行首。</li>
<li><code>1$</code>：光标移动到当前行的行末，<code>2$</code>:光标移动到下一行的行末,…。</li>
<li>M：光标移动到中间行</li>
<li>L：光标移动到屏幕最后一行行首</li>
<li>G：移动到指定行，行号 -G，只有G的话，到文件最末尾</li>
<li>gg：文件第一个字符</li>
<li>w:向后一次移动一个字</li>
<li>b：向前一次移动一个字</li>
<li>{：按段移动，上移</li>
<li>}：按段下移，下移</li>
<li>ctrl+d：向下翻半屏</li>
<li>ctrl+u：向上翻半屏</li>
<li>ctrl+f：向下翻一屏</li>
<li>ctrl+b：向上翻一屏</li>
</ol>
<h4 id="ke-shi-mo-shi">可视模式</h4>
<ol>
<li>v:按字符移动，结合h,j,k,l选中文本内容。</li>
<li>V：按行移动，选中文本可视模式，可以配合d，y，&gt;&gt;,&lt;&lt;实现对文本块的删除，复制，左右移动。</li>
</ol>
<h4 id="shan-chu-ming-ling">删除命令</h4>
<ol>
<li>x：删除光标后一个字符，n x 删除光标后的n个字符</li>
<li>X：删除光标前一个字符，相当于Backspace</li>
<li>dd：删除光标所在行，n dd删除指定的n行</li>
<li>D：删除光标后本行所有内容，包含光标所在字符</li>
<li>d0：删除光标前本行所有内容，不包含光标所在字符</li>
<li>dw：删除光标开始位置的字，包含光标所在字符</li>
</ol>
<h4 id="che-xiao-ming-ling">撤销命令</h4>
<ol>
<li>u：一步一步撤销</li>
<li>ctrl+r：反撤销</li>
</ol>
<h4 id="zhong-fu-ming-ling">重复命令</h4>
<ol>
<li>. ：重复上一次操作的命令</li>
</ol>
<h4 id="wen-ben-yi-dong">文本移动</h4>
<ol>
<li>&gt;&gt;:文本行右移</li>
<li>&lt;&lt;:文本行左移</li>
</ol>
<h4 id="fu-zhi-nian-tie">复制粘贴</h4>
<ol>
<li>yy：复制当前行，n yy复制n行</li>
<li>在末行模式，输入：a,by 复制从第a行开始，到第b行结束的内容</li>
<li>p：在光标坐在位置向下新开辟一行，粘贴</li>
<li>d：剪切选中内容</li>
</ol>
<h4 id="cha-zhao-ming-ling">查找命令</h4>
<ol>
<li>/str : 查找str，从光标所在行往下查找</li>
<li>？str：查找str，从光标所在行往上查找</li>
<li>n：查找下一个</li>
<li>N：查找上一个</li>
</ol>
<h4 id="ti-huan-cao-zuo">替换操作</h4>
<ol>
<li>r：替换当前字符</li>
<li>R替换当前行光标后的字符</li>
</ol>
<h4 id="ti-huan-ming-ling">替换命令</h4>
<ol>
<li>末行模式下，将光标所在行的abc替换成123，：s/abc/123/g</li>
<li>末行模式下，将第一行到第10行之间的abc替换成123，:1,10s/abc/123/g</li>
<li>末行模式下，把文件中的abc全部替换成123，:%s/abc/123</li>
</ol>
<h3 id="shu-ru-mo-shi">输入模式</h3>
<ol>
<li>i：插入光标前一个字符</li>
<li>I：插入行首</li>
<li>a：插入光标后一个字符</li>
<li>A: 插入行末</li>
<li>o：向下新开一行，插入行首</li>
<li>O：向上新开一行，插入行首</li>
</ol>
<h3 id="mo-xing-mo-shi">末行模式</h3>
<ol>
<li>：set nu 显示行号</li>
<li>! shell命令</li>
<li>wq：保存退出</li>
<li>ZZ： 保存退出</li>
<li>q！：不保存退出</li>
</ol>
<h3 id="bu-chong">补充</h3>
<p>修改用户目录下的vimrc（~/.vimrc）文件，修改vim配置</p>
<ol>
<li>set nu :设置每次vim 打开文件显示行号。</li>
<li>set ts=4:设置tab键每次4个空格</li>
</ol>
]]></content>
      <categories>
        <category>soft skill</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Vim</tag>
        <tag>速查</tag>
      </tags>
  </entry>
  <entry>
    <title>latex公式速查</title>
    <url>/2018/05/10/latex%E5%85%AC%E5%BC%8F%E9%80%9F%E6%9F%A5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="shang-xia-biao">上下标</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>x_{2}</td>
<td>\(x_{2}\)</td>
</tr>
<tr>
<td>x^2</td>
<td>\(x^{2}\)</td>
</tr>
</tbody>
</table>
<a id="more"></a>
<h3 id="gong-shi-xu-hao">公式序号</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y &#x3D; wx+b \tag&#123;1.1&#125;</span><br></pre></td></tr></table></figure>
<p>\[
y = wx+b \tag{1.1}
\]</p>
<h3 id="gua-hao">括号</h3>
<ol>
<li>
<p>() [] 直接写就行，而 {} 则需要转义</p>
<ol>
<li>y \in {3, 4, 5} : \(y \in \{3, 4, 5\}\)</li>
</ol>
</li>
<li>
<p>有时候括号需要大号的，此时需要使用\left和\right加大括号的大小。</p>
<ol>
<li>\left(\frac {x} {y} \right)^2 : \(\left(\frac {x} {y} \right)^2\)</li>
</ol>
</li>
<li>
<p>\left 和 \right必须成对出现，对于不显示的一边可以使用 . 代替</p>
</li>
<li>
<p>\left. \frac{du}{dx} \right| _{x=0} : \(\left. \frac{du}{dx} \right| _{x=0}\)</p>
</li>
<li>
<p>双括号\(\left\|g_{k}\right\|\)</p>
</li>
<li>
<p>左大括号：</p>
<p><code>\left\{\begin{array}{ll}  x=\frac{3 \pi}{2}(1+2 t) \cos \left(\frac{3 \pi}{2}(1+2 t)\right) &amp; \\  y=s, &amp; 0 \leq s \leq L,|t| \leq 1 \\  z=\frac{3 \pi}{2}(1+2 t) \sin \left(\frac{3 \pi}{2}(1+2 t)\right)  \end{array}\right.</code><br>
\[
 \left\{\begin{array}{ll}
 x=\frac{3 \pi}{2}(1+2 t) \cos \left(\frac{3 \pi}{2}(1+2 t)\right) &amp; \\
 y=s, &amp; 0 \leq s \leq L,|t| \leq 1 \\
 z=\frac{3 \pi}{2}(1+2 t) \sin \left(\frac{3 \pi}{2}(1+2 t)\right)
 \end{array}\right.
 \]</p>
</li>
</ol>
<h3 id="fen-shu">分数</h3>
<p>两种写法：</p>
<table>
<thead>
<tr>
<th style="text-align:center">命令</th>
<th style="text-align:center">显示</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">\frac{1}{2x+1}</td>
<td style="text-align:center">\(\frac{1}{2x+1}\)</td>
</tr>
</tbody>
</table>
<h3 id="kai-fang">开方</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\sqrt[n]{a}</td>
<td>\(\sqrt[n]{a}\)</td>
</tr>
</tbody>
</table>
<h3 id="dui-shu">对数</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\log</td>
<td>\(\log\)</td>
</tr>
<tr>
<td>\lg</td>
<td>\(\lg\)</td>
</tr>
<tr>
<td>\ln</td>
<td>\(\ln\)</td>
</tr>
</tbody>
</table>
<h3 id="xiang-liang">向量</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\vec{a}</td>
<td>\(\vec{a}\)</td>
</tr>
</tbody>
</table>
<h3 id="san-jiao-yun-suan">三角运算</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\bot</td>
<td>\(\bot\)</td>
<td></td>
<td>\angle</td>
<td>\(\angle\)</td>
</tr>
<tr>
<td>\sin</td>
<td>\(\sin\)</td>
<td></td>
<td>\cos</td>
<td>\(\cos\)</td>
</tr>
<tr>
<td>\tan</td>
<td>\(\tan\)</td>
<td></td>
<td>\cot</td>
<td>\(\cot\)</td>
</tr>
<tr>
<td>\sec</td>
<td>\(\sec\)</td>
<td></td>
<td>\csc</td>
<td>\(\csc\)</td>
</tr>
</tbody>
</table>
<h3 id="sheng-lue-hao">省略号</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\ldots</td>
<td>\(\ldots\)</td>
</tr>
<tr>
<td>\cdots</td>
<td>\(\cdots\)</td>
</tr>
<tr>
<td>\cdot</td>
<td>\(\cdot\)</td>
</tr>
</tbody>
</table>
<h3 id="xu-yao-zhuan-yi-de-zi-fu">需要转义的字符</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\#</td>
<td>\(\#\)</td>
<td></td>
<td>\_</td>
<td>\(\_\)</td>
</tr>
<tr>
<td>\$</td>
<td>$$$</td>
<td></td>
<td>\%</td>
<td>\(\%\)</td>
</tr>
<tr>
<td>\&amp;</td>
<td>\(\&amp;\)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\{</td>
<td>\(\{\)</td>
<td></td>
<td>\}</td>
<td>\(\}\)</td>
</tr>
</tbody>
</table>
<h3 id="pu-tong-fu-hao">普通符号</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\pm</td>
<td>\(\pm\)</td>
<td></td>
<td>\times</td>
<td>\(\times\)</td>
</tr>
<tr>
<td>\div</td>
<td>\(\div\)</td>
<td></td>
<td>\mid</td>
<td>\(\mid\)</td>
</tr>
<tr>
<td>\cdot</td>
<td>\(\cdot\)</td>
<td></td>
<td>\circ</td>
<td>\(\circ\)</td>
</tr>
<tr>
<td>\ast</td>
<td>\(\ast\)</td>
<td></td>
<td>\bigodot</td>
<td>\(\bigodot\)</td>
</tr>
<tr>
<td>\bigotimes</td>
<td>\(\bigotimes\)</td>
<td></td>
<td>\equiv</td>
<td>\(\equiv\)</td>
</tr>
<tr>
<td>\leq or \le</td>
<td>\(\leq\)</td>
<td></td>
<td>\geq or \ge</td>
<td>\(\geq\)</td>
</tr>
<tr>
<td>\ll</td>
<td>\(\ll\)</td>
<td></td>
<td>\gg</td>
<td>\(\gg\)</td>
</tr>
<tr>
<td>\neq or  \ne</td>
<td>\(\neq\)</td>
<td></td>
<td>\approx</td>
<td>\(\approx\)</td>
</tr>
<tr>
<td>\succeq</td>
<td>\(\succeq\)</td>
<td></td>
<td>\preceq</td>
<td>\(\preceq\)</td>
</tr>
<tr>
<td>\doteq</td>
<td>\(\doteq\)</td>
<td></td>
<td>\cong</td>
<td>\(\cong\)</td>
</tr>
<tr>
<td>\sim</td>
<td>\(\sim\)</td>
<td></td>
<td>\simeq</td>
<td>\(\simeq\)</td>
</tr>
<tr>
<td>\vdash</td>
<td>\(\vdash\)</td>
<td></td>
<td>\dashv</td>
<td>\(\dashv\)</td>
</tr>
<tr>
<td>\models</td>
<td>\(\models\)</td>
<td></td>
<td>\perp</td>
<td>\(\perp\)</td>
</tr>
<tr>
<td>\smile</td>
<td>\(\smile\)</td>
<td></td>
<td>\frown</td>
<td>\(\frown\)</td>
</tr>
<tr>
<td>\saymp</td>
<td>\(\asymp\)</td>
<td></td>
<td>:</td>
<td>\(\:\)</td>
</tr>
</tbody>
</table>
<h3 id="xi-la-zi-mu">希腊字母</h3>
<p>大写：开头字母大写<br>
斜体：命令前面加var</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th>大写</th>
<th>斜体</th>
<th></th>
<th>命令</th>
<th>显示</th>
<th>大写</th>
<th>斜体</th>
</tr>
</thead>
<tbody>
<tr>
<td>\alpha</td>
<td>\(\alpha\)</td>
<td>A</td>
<td></td>
<td></td>
<td>\beta</td>
<td>\(\beta\)</td>
<td>B</td>
<td></td>
</tr>
<tr>
<td>\gamma</td>
<td>\(\gamma\)</td>
<td>\(\Gamma\)</td>
<td>\(\varGamma\)</td>
<td></td>
<td>\delta</td>
<td>\(\delta\)</td>
<td>\(\Delta\)</td>
<td>\(\varDelta\)</td>
</tr>
<tr>
<td>\epsilon</td>
<td>\(\epsilon\)</td>
<td>E</td>
<td>\(\varepsilon\)</td>
<td></td>
<td>\zeta</td>
<td>\(\zeta\)</td>
<td>Z</td>
<td></td>
</tr>
<tr>
<td>\eta</td>
<td>\(\eta\)</td>
<td>H</td>
<td></td>
<td></td>
<td>\theta</td>
<td>\(\theta\)</td>
<td>\(\Theta\)</td>
<td>\(\varTheta\)</td>
</tr>
<tr>
<td>\iota</td>
<td>\(\iota\)</td>
<td>I</td>
<td></td>
<td></td>
<td>\kappa</td>
<td>\(\kappa\)</td>
<td>K</td>
<td></td>
</tr>
<tr>
<td>\lambda</td>
<td>\(\lambda\)</td>
<td>\(\Lambda\)</td>
<td>\(\varLambda\)</td>
<td></td>
<td>\mu</td>
<td>\(\mu\)</td>
<td>M</td>
<td></td>
</tr>
<tr>
<td>\xi</td>
<td>\(\xi\)</td>
<td>\(\Xi\)</td>
<td>\(\varXi\)</td>
<td></td>
<td>\nu</td>
<td>\(\nu\)</td>
<td>N</td>
<td></td>
</tr>
<tr>
<td>\pi</td>
<td>\(\pi\)</td>
<td>\(\Pi\)</td>
<td>\(\varPi\)</td>
<td></td>
<td>\rho</td>
<td>\(\rho\)</td>
<td>P</td>
<td>\(\varrho\)</td>
</tr>
<tr>
<td>\sigma</td>
<td>\(\sigma\)</td>
<td>\(\Sigma\)</td>
<td>\(\varSigma\)</td>
<td></td>
<td>\tau</td>
<td>\(\tau\)</td>
<td>T</td>
<td></td>
</tr>
<tr>
<td>\upsilon</td>
<td>\(\upsilon\)</td>
<td>\(\Upsilon\)</td>
<td>\(\varUpsilon\)</td>
<td></td>
<td>\phi</td>
<td>\(\phi\)</td>
<td>\(\Phi\)</td>
<td>\(\varPhi\)</td>
</tr>
<tr>
<td>\chi</td>
<td>\(\chi\)</td>
<td>X</td>
<td></td>
<td></td>
<td>\psi</td>
<td>\(\psi\)</td>
<td>\(\Psi\)</td>
<td>\(\varPsi\)</td>
</tr>
<tr>
<td>\omega</td>
<td>\(\omega\)</td>
<td>\(\Omega\)</td>
<td>\(\varOmega\)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="/2018/05/10/latex%E5%85%AC%E5%BC%8F%E9%80%9F%E6%9F%A5/NSFileHandle.png" alt="5f979c551b937e468a1ed99ff62134d4"></p>
<h3 id="ji-he-yun-suan">集合运算</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\emptyset</td>
<td>\(\emptyset\)</td>
<td></td>
<td>\propto</td>
<td>\(\propto\)</td>
</tr>
<tr>
<td>\notin</td>
<td>\(\notin\)</td>
<td></td>
<td>\Join or \bowtie</td>
<td>\(\Join\)</td>
</tr>
<tr>
<td>\supset</td>
<td>\(\supset\)</td>
<td></td>
<td>\subseteq</td>
<td>\(\subseteq\)</td>
</tr>
<tr>
<td>\subset</td>
<td>\(\subset\)</td>
<td></td>
<td>\supseteq</td>
<td>\(\supseteq\)</td>
</tr>
<tr>
<td>\sqsubset</td>
<td>\(\sqsubset\)</td>
<td></td>
<td>\sqsupseteq</td>
<td>\(\sqsupseteq\)</td>
</tr>
<tr>
<td>\bigcap</td>
<td>\(\bigcap\)</td>
<td></td>
<td>\bigcup</td>
<td>\(\bigcup\)</td>
</tr>
<tr>
<td>\bigvee</td>
<td>\(\bigvee\)</td>
<td></td>
<td>\bigwedge</td>
<td>\(\bigwedge\)</td>
</tr>
<tr>
<td>\biguplus</td>
<td>\(\biguplus\)</td>
<td></td>
<td>\bigsqcup</td>
<td>\(\bigsqcup\)</td>
</tr>
<tr>
<td>\in</td>
<td>\(\in\)</td>
<td></td>
<td>\ni or \owns</td>
<td>\(\owns\)</td>
</tr>
</tbody>
</table>
<h3 id="luo-ji-yun-suan">逻辑运算</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\because</td>
<td>\(\because\)</td>
<td></td>
<td>\therefore</td>
<td>\(\therefore\)</td>
</tr>
<tr>
<td>\forall</td>
<td>\(\forall\)</td>
<td></td>
<td>\exists</td>
<td>\(\exists\)</td>
</tr>
</tbody>
</table>
<h3 id="wei-fen">微分</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>y{\prime}x</td>
<td>\(y{\prime}x\)</td>
<td></td>
<td>\int</td>
<td>\(\int\)</td>
</tr>
<tr>
<td>\iint</td>
<td>\(\iint\)</td>
<td></td>
<td>\iiint</td>
<td>\(\iiint\)</td>
</tr>
<tr>
<td>\oint</td>
<td>\(oint\)</td>
<td></td>
<td>\lim</td>
<td>\(lim\)</td>
</tr>
<tr>
<td>\infty</td>
<td>\(\infty\)</td>
<td></td>
<td>\nabla</td>
<td>\(\nabla\)</td>
</tr>
<tr>
<td>\partial</td>
<td>\(\partial\)</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="jian-tou">箭头</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\uparrow</td>
<td>\(\uparrow\)</td>
<td></td>
<td>\downarrow</td>
<td>\(\downarrow\)</td>
</tr>
<tr>
<td>\Uparrow</td>
<td>\(\Uparrow\)</td>
<td></td>
<td>\Downarrow</td>
<td>\(\Downarrow\)</td>
</tr>
<tr>
<td>\leftarrow</td>
<td>\(\leftarrow\)</td>
<td></td>
<td>\rightarrow</td>
<td>\(\rightarrow\)</td>
</tr>
<tr>
<td>\Leftarrow</td>
<td>\(\Leftarrow\)</td>
<td></td>
<td>\Rightarrow</td>
<td>\(\Rightarrow\)</td>
</tr>
<tr>
<td>\longleftarrow</td>
<td>\(\longleftarrow\)</td>
<td></td>
<td>\longrightarrow</td>
<td>\(\longrightarrow\)</td>
</tr>
<tr>
<td>\Longrightarrow</td>
<td>\(\Longrightarrow\)</td>
<td></td>
<td>\Longrightarrow</td>
<td>\(\Longrightarrow\)</td>
</tr>
</tbody>
</table>
<h3 id="qiu-he-lian-cheng-he-ji-fen-hao">求和、连乘和积分号</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th>使用</th>
<th>例子</th>
</tr>
</thead>
<tbody>
<tr>
<td>\sum</td>
<td>\(\sum\)</td>
<td>\sum_{i=1}^{N}</td>
<td>\(\sum_{i=1}^{N}\)</td>
</tr>
<tr>
<td>\prod</td>
<td>\(\prod\)</td>
<td>\prod_{i=1}^{N}</td>
<td>\(\prod_{i=1}^{N}\)</td>
</tr>
<tr>
<td>\coprod</td>
<td>\(\coprod\)</td>
<td>\coprod_{i=1}^{N}</td>
<td>\(\coprod_{i=1}^{N}\)</td>
</tr>
<tr>
<td>\int</td>
<td>\(\int\)</td>
<td>\int_{a}^{b}</td>
<td>\(\int_{a}^{b}\)</td>
</tr>
<tr>
<td>\iint</td>
<td>\(\iint\)</td>
<td>\iint_{a}^{b}</td>
<td>\(\iint_{a}^{b}\)</td>
</tr>
<tr>
<td>\bigcup</td>
<td>\(\bigcup\)</td>
<td>\bigcup_{i=1}^{N}</td>
<td>\(\bigcup_{i=1}^{N}\)</td>
</tr>
<tr>
<td>\bigcap</td>
<td>\(\bigcap\)</td>
<td>\bigcap_{i=1}^{N}</td>
<td>\(\bigcap_{i=1}^{N}\)</td>
</tr>
<tr>
<td>\lim_{n\rightarrow+\infty}</td>
<td>\(lim_{n\rightarrow+\infty}\)</td>
<td>\lim_{n\rightarrow+\infty}\frac{1}{n(n+1)}</td>
<td>\(\lim_{n\rightarrow+\infty}\frac{1}{n(n+1)}\)</td>
</tr>
</tbody>
</table>
<h3 id="shu-zi-mo-shi-zhong-yin-fu">数字模式重音符</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
<th></th>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\hat{a}</td>
<td>\(\hat{a}\)</td>
<td></td>
<td>\dot{a}</td>
<td>\(\dot{a}\)</td>
</tr>
<tr>
<td>\grave{a}</td>
<td>\(\grave{a}\)</td>
<td></td>
<td>\acute{a}</td>
<td>\(\acute{a}\)</td>
</tr>
<tr>
<td>\bar{a}</td>
<td>\(\bar{a}\)</td>
<td></td>
<td>\tilde{a}</td>
<td>\(\tilde{a}\)</td>
</tr>
<tr>
<td>\check{a}</td>
<td>\(\check{a}\)</td>
<td></td>
<td>\ddot{a}</td>
<td>\(\ddot{a}\)</td>
</tr>
<tr>
<td>\widehat{A}</td>
<td>\(\widehat{A}\)</td>
<td></td>
<td>\widetilde{A}</td>
<td>\(\widetilde{A}\)</td>
</tr>
<tr>
<td>\vec{a}</td>
<td>\(\vec{a}\)</td>
<td></td>
<td>\breve{a}</td>
<td>\(\breve{a}\)</td>
</tr>
</tbody>
</table>
<h3 id="lian-xian-fu-hao">连线符号</h3>
<table>
<thead>
<tr>
<th>命令</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td>\overline{a+b+c+d}</td>
<td>\(\overline{a+b+c+d}\)</td>
</tr>
<tr>
<td>\underline{a+b+c+d}</td>
<td>\(\underline{a+b+c+d}\)</td>
</tr>
<tr>
<td>\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}</td>
<td>\(\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}\)</td>
</tr>
</tbody>
</table>
<h3 id="ju-zhen">矩阵</h3>
<p>命令</p>
<p>\left|\begin{array}{cccc}
     1 &amp; 6 &amp; 9 \\\ 
     7 &amp; 9 &amp; 0 \\\
     9 &amp; 5 &amp; 0
\end{array}\right|</p>
<p>显示：<br>
\[
\left|\begin{array}{cccc}
     1 &amp; 6 &amp; 9 \\ 
     7 &amp; 9 &amp; 0 \\
     9 &amp; 5 &amp; 0
\end{array}\right|
\]</p>
<h3 id="gong-ju">工具</h3>
<p><a href="https://mathpix.com/" target="_blank" rel="noopener">https://mathpix.com/</a></p>
<p><a href="https://houmin.cc/posts/fedfc052/" target="_blank" rel="noopener">https://houmin.cc/posts/fedfc052/</a></p>
]]></content>
      <categories>
        <category>soft skill</category>
      </categories>
      <tags>
        <tag>速查</tag>
        <tag>latex</tag>
      </tags>
  </entry>
  <entry>
    <title>图论</title>
    <url>/2018/03/21/%E5%9B%BE%E8%AE%BA/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="wu-xiang-tu">无向图</h3>
<h4 id="ding-yi-wu-xiang-tu-shu-ju-jie-gou">定义无向图数据结构</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Graph</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        self.n = n</span><br><span class="line">        self.e = <span class="number">0</span></span><br><span class="line">        self.adj = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_edge</span><span class="params">(self, v, w)</span>:</span></span><br><span class="line">        self.adj[v].append(w)</span><br><span class="line">        self.adj[w].append(v)</span><br><span class="line">        self.e += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h4 id="wu-xiang-tu-pan-huan">无向图判环</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cycle</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, G)</span>:</span></span><br><span class="line">        self.vis = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(G.n)]</span><br><span class="line">        self.has_cycle = <span class="literal">False</span></span><br><span class="line">        self.G = G</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> range(G.n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.vis[s]:</span><br><span class="line">                self.dfs(s, s)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self, v, parent)</span>:</span></span><br><span class="line">        self.vis[v] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> self.G.adj[v]:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.vis[w]:</span><br><span class="line">                self.dfs(w, v)</span><br><span class="line">            <span class="comment"># 相邻节点已被访问过，同时还不是parent节点，则存在环</span></span><br><span class="line">            <span class="keyword">elif</span> w != parent:</span><br><span class="line">                self.has_cycle = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h4 id="wu-xiang-tu-shi-fou-wei-er-fen-tu">无向图是否为二分图</h4>
<p>二分图又叫二部图，是图论中的一种特殊模型。设G=(V,E)是一个无向图，如果顶点V可分割为两个互不相交的子集<code>(A,B)</code>，并且图中的每条边<code>（i，j）</code>所关联的两个顶点i和j分别属于这两个不同的顶点集<code>(i in A,j in B)</code>，则称图G为一个二分图。</p>
<p><img src="/2018/03/21/%E5%9B%BE%E8%AE%BA/%E4%BA%8C%E5%88%86%E5%9B%BE.png" alt="avatar"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoColor</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, G)</span>:</span></span><br><span class="line">        self.vis = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(G.n)]</span><br><span class="line">        self.color = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(G.n)]</span><br><span class="line">        self.G = G</span><br><span class="line">        self.is_two_colorable = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> range(G.n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.vis[s]:</span><br><span class="line">                self.dfs(s)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self, v)</span>:</span></span><br><span class="line">        self.vis[v] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> self.G.adj[v]:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.vis[w]:</span><br><span class="line">                self.color[w] = <span class="keyword">not</span> self.color[v]</span><br><span class="line">                self.dfs(w)</span><br><span class="line">            <span class="keyword">elif</span> self.color[w] == self.color[v]:</span><br><span class="line">                self.is_two_colorable = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="zui-xiao-sheng-cheng-shu">最小生成树</h3>
<h4 id="prime-suan-fa">prime 算法</h4>
<p><img src="/2018/03/21/%E5%9B%BE%E8%AE%BA/prime.gif" alt="avatar"></p>
<p>每次向最小生成树中加入权重最小的横切边，横切边是连接树与非树顶点的边。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prim</span><span class="params">(adj_matrix)</span>:</span></span><br><span class="line">    <span class="string">"""给定邻接矩阵，返回MST权值，返回-1表示图不连通"""</span></span><br><span class="line">    n = len(adj_matrix)  <span class="comment"># 顶点0~n-1</span></span><br><span class="line">    vis = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">    low_cut = [float(<span class="string">'inf'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]  <span class="comment"># 横切边的权重</span></span><br><span class="line"></span><br><span class="line">    ans = <span class="number">0</span></span><br><span class="line">    vis[<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        low_cut[i] = adj_matrix[<span class="number">0</span>][i]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        min_cut = float(<span class="string">'inf'</span>)</span><br><span class="line">        p = <span class="number">-1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[j] <span class="keyword">and</span> min_cut &gt; low_cut[j]:</span><br><span class="line">                min_cut = low_cut[j]</span><br><span class="line">                p = j</span><br><span class="line">        <span class="keyword">if</span> min_cut == float(<span class="string">'inf'</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>  <span class="comment"># 原图不连通</span></span><br><span class="line">        ans += min_cut</span><br><span class="line">        vis[p] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment"># 横切边更新为更小的权重</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[j] <span class="keyword">and</span> low_cut[j] &gt; adj_matrix[p][j]:</span><br><span class="line">                low_cut[j] = adj_matrix[p][j]</span><br><span class="line">    <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4 id="kuskal-suan-fa">Kuskal 算法</h4>
<p><img src="/2018/03/21/%E5%9B%BE%E8%AE%BA/Kuskal.png" alt="avatar"></p>
<p>将图中所有边按照从小到大的顺序加入最小生成树，加入的边不会与已加入的边构成换。(利用并查集UF判断连通性)，直到树中含有n-1条边为止。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kruskal</span><span class="params">(edges, n)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    求最小生成树权值</span></span><br><span class="line"><span class="string">    :param edges: [(u, v, w), ...]，三元组含义(顶点u，顶点v，边权重w)</span></span><br><span class="line"><span class="string">    :param n: 顶点数，顶点范围0~n-1</span></span><br><span class="line"><span class="string">    :return: 最小生成树权值（图不连通，返回-1）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    uf = UnionFind(n)  <span class="comment"># 并查集</span></span><br><span class="line">    edges.sort(key=<span class="keyword">lambda</span> item: item[<span class="number">2</span>])</span><br><span class="line">    edge_cnt = <span class="number">0</span></span><br><span class="line">    ans = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(edges)):</span><br><span class="line">        u, v, w = edges[i]</span><br><span class="line">        root_u, root_v = uf.find(u), uf.find(v)</span><br><span class="line">        <span class="keyword">if</span> root_u != root_v:</span><br><span class="line">            ans += w</span><br><span class="line">            uf.union(u, v)</span><br><span class="line">            edge_cnt += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> edge_cnt == n<span class="number">-1</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> edge_cnt &lt; n<span class="number">-1</span>:  <span class="comment"># 不连通</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">return</span> ans</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UnionFind</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""并查集类"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="string">"""长度为n的并查集"""</span></span><br><span class="line">        self.uf = [<span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n + <span class="number">1</span>)]    <span class="comment"># 列表0位置空出</span></span><br><span class="line">        self.sets_count = n                     <span class="comment"># 判断并查集里共有几个集合, 初始化默认互相独立</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># def find(self, p):</span></span><br><span class="line">    <span class="comment">#     """查找p的根结点(祖先)"""</span></span><br><span class="line">    <span class="comment">#     r = p                                   # 初始p</span></span><br><span class="line">    <span class="comment">#     while self.uf[p] &gt; 0:</span></span><br><span class="line">    <span class="comment">#         p = self.uf[p]</span></span><br><span class="line">    <span class="comment">#     while r != p:                           # 路径压缩, 把搜索下来的结点祖先全指向根结点</span></span><br><span class="line">    <span class="comment">#         self.uf[r], r = p, self.uf[r]</span></span><br><span class="line">    <span class="comment">#     return p</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># def find(self, p):</span></span><br><span class="line">    <span class="comment">#     while self.uf[p] &gt;= 0:</span></span><br><span class="line">    <span class="comment">#         p = self.uf[p]</span></span><br><span class="line">    <span class="comment">#     return p</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">(self, p)</span>:</span></span><br><span class="line">        <span class="string">"""尾递归"""</span></span><br><span class="line">        <span class="keyword">if</span> self.uf[p] &lt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> p</span><br><span class="line">        self.uf[p] = self.find(self.uf[p])</span><br><span class="line">        <span class="keyword">return</span> self.uf[p]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">union</span><span class="params">(self, p, q)</span>:</span></span><br><span class="line">        <span class="string">"""连通p,q 让q指向p"""</span></span><br><span class="line">        proot = self.find(p)</span><br><span class="line">        qroot = self.find(q)</span><br><span class="line">        <span class="keyword">if</span> proot == qroot:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">elif</span> self.uf[proot] &gt; self.uf[qroot]:   <span class="comment"># 负数比较, 左边规模更小</span></span><br><span class="line">            self.uf[qroot] += self.uf[proot]</span><br><span class="line">            self.uf[proot] = qroot</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.uf[proot] += self.uf[qroot]  <span class="comment"># 规模相加</span></span><br><span class="line">            self.uf[qroot] = proot</span><br><span class="line">        self.sets_count -= <span class="number">1</span>                    <span class="comment"># 连通后集合总数减一</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_connected</span><span class="params">(self, p, q)</span>:</span></span><br><span class="line">        <span class="string">"""判断pq是否已经连通"""</span></span><br><span class="line">        <span class="keyword">return</span> self.find(p) == self.find(q)     <span class="comment"># 即判断两个结点是否是属于同一个祖先</span></span><br></pre></td></tr></table></figure>
<h3 id="you-xiang-tu">有向图</h3>
<h4 id="ding-yi-wu-xiang-tu-de-shu-ju-jie-gou">定义无向图的数据结构</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiGraph</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        self.n = n</span><br><span class="line">        self.e = <span class="number">0</span></span><br><span class="line">        self.adj = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_edge</span><span class="params">(self, v, w)</span>:</span></span><br><span class="line">        self.adj[v].append(w)</span><br><span class="line">        self.e += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h4 id="you-xiang-tu-pan-huan">有向图判环</h4>
<p>对有向图使用dfs搜索，系统调用栈表示了当前遍历了的有向路径</p>
<p>若递归过程访问到出现在栈中的节点，则图中存在了环。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DirectedCycle</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, G)</span>:</span></span><br><span class="line">        self.on_stack = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(G.n)]</span><br><span class="line">        self.vis = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(G.n)]</span><br><span class="line">        self.has_cycle = <span class="literal">False</span></span><br><span class="line">        self.G = G</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> range(G.n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.vis[s]:</span><br><span class="line">                self.dfs(s)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self, v)</span>:</span></span><br><span class="line">        self.on_stack[v] = <span class="literal">True</span></span><br><span class="line">        self.vis[v] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> self.G.adj[v]:</span><br><span class="line">            <span class="keyword">if</span> self.has_cycle:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="keyword">not</span> self.vis[w]:</span><br><span class="line">                self.dfs(w)</span><br><span class="line">            <span class="keyword">elif</span> self.on_stack[w]:</span><br><span class="line">                self.has_cycle = <span class="literal">True</span></span><br><span class="line">        self.on_stack[v] = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h4 id="zong-jie">总结</h4>
<p>一个有向五环图的拓扑排序即为所有顶点dfs的逆后序排列</p>
<p>强连通性：有向图任意两点互相可达，则为强连通图。</p>
<p><strong>Kosaraju 算法</strong>：</p>
<ol>
<li>有向图G翻转得到\(G^R\)</li>
<li>dfs得到\(G^R\)中顶点的逆后序排列</li>
<li>按照这个你后序排列在G中执行标准dfs，每次递归调用所标记的顶点在同一强连通分量重</li>
</ol>
<p><strong>最短路径比较、总结</strong></p>
<table>
<thead>
<tr>
<th>算法</th>
<th>条件</th>
<th>平均</th>
<th>最坏</th>
<th>空间复杂度</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Dijkstra</td>
<td>边权为正</td>
<td>\(ElogV\)</td>
<td>\(ElogV\)</td>
<td>\(V\)</td>
<td></td>
</tr>
<tr>
<td>拓扑排序</td>
<td>无环</td>
<td>\(E\)</td>
<td>\(E+V\)</td>
<td>\(V\)</td>
<td>无环图最优算法</td>
</tr>
<tr>
<td>Bellman-Ford（基于队列）</td>
<td>无负权重</td>
<td>\(E+V\)</td>
<td>\(VE\)</td>
<td>\(V\)</td>
<td></td>
</tr>
</tbody>
</table>
<p>Dijkstra 算法：每次添加离起点最近的非树节点</p>
<p>prim：每次添加离树最近的非树节点</p>
<h4 id="dan-yuan-zui-duan-lu-jing-lin-jie-ju-zhen-xing-shi">单源最短路径，邻接矩阵形式</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dijkstra</span><span class="params">(adj_matrix, s)</span>:</span></span><br><span class="line">    <span class="string">"""给定邻接矩阵和起点s，返回s到所有点的最短路径"""</span></span><br><span class="line">    n = len(adj_matrix)  <span class="comment"># 顶点0~n-1</span></span><br><span class="line">    vis = [<span class="literal">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">    edge_to = [<span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]  <span class="comment"># 路径父节点</span></span><br><span class="line">    dist = [float(<span class="string">'inf'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]  <span class="comment"># 最短路径</span></span><br><span class="line"></span><br><span class="line">    dist[s] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        min_dist = float(<span class="string">'inf'</span>)</span><br><span class="line">        p = <span class="number">-1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[j] <span class="keyword">and</span> dist[j] &lt; min_dist:</span><br><span class="line">                min_dist = dist[j]</span><br><span class="line">                p = j</span><br><span class="line">        <span class="keyword">if</span> p == <span class="number">-1</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        vis[p] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[j] <span class="keyword">and</span> dist[p] + adj_matrix[p][j] &lt; dist[j]:</span><br><span class="line">                dist[j] = dist[p] + adj_matrix[p][j]</span><br><span class="line">                edge_to[j] = p</span><br><span class="line">    <span class="keyword">return</span> dist, edge_to</span><br></pre></td></tr></table></figure>
<p>单元最短路径，邻接表形式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dijkstra</span><span class="params">(adj, s)</span>:</span></span><br><span class="line">    n = len(adj)</span><br><span class="line">    dist = [float(<span class="string">'inf'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line"></span><br><span class="line">    dist[s] = <span class="number">0</span></span><br><span class="line">    pq = [(<span class="number">0</span>, s)]</span><br><span class="line">    <span class="keyword">while</span> pq:</span><br><span class="line">        <span class="comment"># u, v是顶点，d是距离，w是边权重</span></span><br><span class="line">        d, u = heappop(pq)</span><br><span class="line">        <span class="keyword">if</span> d &gt; dist[u]:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> v, w <span class="keyword">in</span> adj[u]:</span><br><span class="line">            <span class="keyword">if</span> d + w &lt; dist[v]:</span><br><span class="line">                dist[v] = d + w</span><br><span class="line">                heappush(pq, (dist[v], v))</span><br><span class="line">    <span class="keyword">return</span> dist</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>图论</tag>
      </tags>
  </entry>
  <entry>
    <title>字符串匹配问题</title>
    <url>/2018/03/14/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="zi-fu-chuan-pi-pei-wen-ti">字符串匹配问题</h2>
<p>给定两个字符串<code>S,P</code>,其中<code>S</code>串长度为<code>n</code>，<code>P</code>串长度为<code>m</code>, <code>(m&lt;=n)</code>，判断字符串<code>P</code>是否是<code>S</code>的子串</p>
<h4 id="fu-za-du">复杂度</h4>
<p>容易计算复杂度：<code>O(m*n)</code></p>
<h3 id="brute-force-jian-dan-pi-pei">Brute-Force(简单匹配)</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_match</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    m, n = len(s), len(p)</span><br><span class="line">    <span class="keyword">if</span> m-n &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m-n+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> s[i:i+n] == p:</span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="rabin-karp-suan-fa">Rabin-Karp算法</h3>
<h4 id="si-xiang">思想</h4>
<p>基本思想和暴力破解算法是一样的。也需要一个大小为<strong>m</strong>的窗口，但是不一样的是，不是直接比较两个长度为<strong>m</strong>的字符串，而是比较他们的哈希值。</p>
<h4 id="fu-za-du-fen-xi">复杂度分析</h4>
<p>一共会有(<strong>n-m+1</strong>)个窗口滑动，这一步的复杂度是<strong>O(n)</strong>。</p>
<p>计算哈希值:</p>
<p>假设现在窗口的起点在<strong>j</strong>这个位置，此时窗口内的字符串哈希值为，<br>
\[
H(S, j)=\sum_{i=0}^{m-1} \alpha^{m-(i+1)} \times \operatorname{char}\left(s_{i}\right)
\]<br>
那么，当计算下一个窗口的哈希值时，也就是当窗口的起点为<strong>j+1</strong>时，哈希函数值可由如下方法计算：<br>
\[
H(S, j+1)=\alpha\left(H(S, j)-\alpha^{m-1} \operatorname{char}\left(s_{j}\right)\right)+\operatorname{char}\left(s_{j+m}\right)
\]<br>
这样看来，在计算出第一个窗口的函数值之后，后面的每一个窗口哈希值都可以根据上述公式计算，只需要做一次减法，一次乘法，一次加法。之后的每一次哈希值计算都是<strong>O(1)<strong>的复杂度。所以计算第一个窗口的复杂度，<strong>O(m)</strong>，此后计算每一个窗口的复杂度</strong>O(1)</strong>，总的时间复杂度：<code>O(n+m)</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rabin_karp_match</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    n = len(s)</span><br><span class="line">    m = len(p)</span><br><span class="line">    h1 = hash(p)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, n-m+<span class="number">1</span>):</span><br><span class="line">        h2 = hash(s[i:i+m])</span><br><span class="line">        <span class="keyword">if</span> h1 != h2:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> s[i:i+m] == p:</span><br><span class="line">                <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<h3 id="kmp-suan-fa">KMP算法</h3>
<p>有点复杂，有时间再写上。</p>
<h4 id="si-xiang-1">思想</h4>
<h4 id="fu-za-du-fen-xi-1">复杂度分析</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># KMP算法：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmp_match</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    m, n = len(s), len(p)</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span> <span class="keyword">or</span> m - n &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    table = partial_table(p)</span><br><span class="line">    i, cur = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> i &lt; m-n+<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> p[cur] == s[i+cur]:</span><br><span class="line">            cur += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> cur == <span class="number">0</span>:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i += cur - table[cur<span class="number">-1</span>]  <span class="comment"># 移动位数=已匹配的字符数-对应的部分匹配值</span></span><br><span class="line">                cur = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> cur == n:</span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">partial_table</span><span class="params">(p)</span>:</span></span><br><span class="line">    m = len(p)</span><br><span class="line">    table = [<span class="number">0</span>] * m</span><br><span class="line">    k = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> q <span class="keyword">in</span> range(<span class="number">1</span>, m):</span><br><span class="line">        <span class="keyword">if</span> p[k] == p[q]:</span><br><span class="line">            k = k + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            k = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> p[k] == p[q]:</span><br><span class="line">                k = k + <span class="number">1</span></span><br><span class="line">        table[q] = k</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> table</span><br></pre></td></tr></table></figure>
<h3 id="horspool-suan-fa">Horspool算法</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shift_table</span><span class="params">(p)</span>:</span></span><br><span class="line">    <span class="comment"># 生成 Horspool 算法的移动表</span></span><br><span class="line">    <span class="comment"># 当前检测字符为c，模式长度为m</span></span><br><span class="line">    <span class="comment"># 如果当前c不包含在模式的前m-1个字符中，移动模式的长度m</span></span><br><span class="line">    <span class="comment"># 其他情况下移动最右边的的c到模式最后一个字符的距离</span></span><br><span class="line">    <span class="comment"># from collections import defaultdict</span></span><br><span class="line">    <span class="comment"># table = defaultdict(lambda: len(p))</span></span><br><span class="line">    table = dict()</span><br><span class="line">    <span class="comment"># print table</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, len(p) - <span class="number">1</span>):</span><br><span class="line">        table[p[index]] = len(p) - <span class="number">1</span> - index</span><br><span class="line">    <span class="comment"># print table</span></span><br><span class="line">    <span class="keyword">return</span> table</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">horspool_match</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    m, n = len(s), len(p)</span><br><span class="line">    table = shift_table(p)</span><br><span class="line">    index = len(p) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> index &lt;= m - <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># print("start matching at", index)</span></span><br><span class="line">        match_count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> match_count &lt; n <span class="keyword">and</span> p[n - <span class="number">1</span> - match_count] == s[index - match_count]:</span><br><span class="line">            match_count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> match_count == len(p):</span><br><span class="line">            <span class="keyword">return</span> index - match_count + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># print s[index], table.get(s[index])</span></span><br><span class="line">            <span class="keyword">if</span> table.get(s[index]) <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                table[s[index]] = n</span><br><span class="line">            index += table[s[index]]</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<h3 id="sunday-suan-fa">Sunday 算法</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sunday_match</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    char_pos = dict()</span><br><span class="line">    <span class="keyword">for</span> i, ch <span class="keyword">in</span> enumerate(p):</span><br><span class="line">        char_pos[ch] = i</span><br><span class="line">    <span class="comment"># print "char_pos:", char_pos</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    m, n = len(s), len(p)</span><br><span class="line">    <span class="keyword">while</span> i &lt;= m - n:</span><br><span class="line">        found = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> j, ch <span class="keyword">in</span> enumerate(p):</span><br><span class="line">            <span class="keyword">if</span> s[i + j] != ch:</span><br><span class="line">                found = <span class="literal">False</span></span><br><span class="line">                <span class="keyword">if</span> (i + n) &lt; m:</span><br><span class="line">                    <span class="keyword">if</span> s[i + n] <span class="keyword">not</span> <span class="keyword">in</span> char_pos:</span><br><span class="line">                        i += (n + <span class="number">1</span>)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        i += (n - char_pos[s[i + n]])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> found:</span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<h3 id="boyer-moore-suan-fa">Boyer_Moore算法</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">boyer_moore_search</span><span class="params">(string, des)</span>:</span></span><br><span class="line">    l = len(des) - <span class="number">1</span></span><br><span class="line">    strlen = len(string) - <span class="number">1</span></span><br><span class="line">    start, end = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> strlen &gt;= <span class="number">0</span>:</span><br><span class="line">        end = start + len(des)</span><br><span class="line">        <span class="comment"># print string[start:end]</span></span><br><span class="line">        cr = compare(string[start:end], des)</span><br><span class="line">        <span class="keyword">if</span> cr[<span class="number">0</span>] == <span class="number">-2</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'not found'</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> cr[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># return end - l</span></span><br><span class="line">            <span class="keyword">return</span> start</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pos = is_character_in(des, cr[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> cr[<span class="number">0</span>] == (len(des) - <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> pos != <span class="number">-1</span>:</span><br><span class="line">                    start += len(des) - <span class="number">1</span> - pos</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    start += len(des)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> pos == <span class="number">-1</span>:</span><br><span class="line">                    <span class="comment">#  have good  string</span></span><br><span class="line">                    goodPos = is_character_in(des, des[l])</span><br><span class="line">                    <span class="keyword">if</span> goodPos == l:</span><br><span class="line">                        start += l + <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        start += l - goodPos</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_character_in</span><span class="params">(s, c)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> pos <span class="keyword">in</span> range(len(s)):</span><br><span class="line">        <span class="keyword">if</span> s[pos] == c:</span><br><span class="line">            <span class="keyword">return</span> pos</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compare</span><span class="params">(str1, str2)</span>:</span></span><br><span class="line">    l1 = len(str1) - <span class="number">1</span></span><br><span class="line">    l2 = len(str2) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> l1 != l2:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-2</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> l1 &gt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> str1[l1] != str2[l1]:</span><br><span class="line">            <span class="keyword">return</span> l1, str1[l1]</span><br><span class="line">        l1 -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>, <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h3 id="bmhbnfs-suan-fa-kuai-su-pi-pei-suan-fa">BMHBNFS算法（快速匹配算法）</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_skip</span><span class="params">(p)</span>:</span></span><br><span class="line">    m = len(p)</span><br><span class="line">    a = p[m<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m<span class="number">-2</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">if</span> p[i] == a:</span><br><span class="line">            <span class="keyword">return</span> m<span class="number">-2</span>-i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bmhbnfs_find</span><span class="params">(s, p)</span>:</span></span><br><span class="line">    <span class="comment"># find first occurrence of p in s</span></span><br><span class="line">    m = len(s)</span><br><span class="line">    n = len(p)</span><br><span class="line">    <span class="comment"># skip是把离p[m-1]最近且字符相同的字符移到m-1位需要跳过的步数-1</span></span><br><span class="line">    <span class="comment"># skip = delta1(p)[p[m - 1]]</span></span><br><span class="line">    skip = compute_skip(p)</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt;= m - n:</span><br><span class="line">        <span class="keyword">if</span> s[i + n - <span class="number">1</span>] == p[n - <span class="number">1</span>]:  <span class="comment"># (boyer-moore)</span></span><br><span class="line">            <span class="comment"># potential match</span></span><br><span class="line">            <span class="keyword">if</span> s[i:i + n - <span class="number">1</span>] == p[:n - <span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">return</span> i</span><br><span class="line">            <span class="keyword">if</span> s[i + n] <span class="keyword">not</span> <span class="keyword">in</span> p:</span><br><span class="line">                i = i + n + <span class="number">1</span>  <span class="comment"># (sunday)</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i = i + skip  <span class="comment"># (horspool)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># skip</span></span><br><span class="line">            <span class="keyword">if</span> s[i + n] <span class="keyword">not</span> <span class="keyword">in</span> p:</span><br><span class="line">                i = i + n + <span class="number">1</span>  <span class="comment"># (sunday)</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i = i + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>  <span class="comment"># not found</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>字符串匹配</tag>
      </tags>
  </entry>
  <entry>
    <title>动态规划</title>
    <url>/2018/03/09/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2018/03/09/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/dynamic_programming.png" alt="avatar"></p>
<a id="more"></a>
<h2 id="ji-ben-si-lu">基本思路</h2>
<h3 id="si-kao-zhuang-tai">思考状态</h3>
<p>状态先尝试“题目问什么，就把什么设置为状态”。然后考虑“状态如何转移”，如果“状态转移方程”不容易得到，尝试修改定义，目的仍然是为了方便得到“状态转移方程”。</p>
<h3 id="si-kao-zhuang-tai-zhuan-yi-fang-cheng-he-xin-nan-dian">思考状态转移方程（核心、难点）</h3>
<p>状态转移方程是非常重要的，是动态规划的核心，也是难点，起到承上启下的作用。</p>
<blockquote>
<p>技巧是分类讨论。对状态空间进行分类，思考最优子结构到底是什么。即大问题的最优解如何由小问题的最优解得到。</p>
</blockquote>
<p>归纳“状态转移方程”是一个很灵活的事情，得具体问题具体分析，除了掌握经典的动态规划问题以外，还需要多做题。如果是针对面试，请自行把握难度，我个人觉得掌握常见问题的动态规划解法，明白动态规划的本质就是打表格，从一个小规模问题出发，逐步得到大问题的解，并记录过程。动态规划依然是“空间换时间”思想的体现。</p>
<h3 id="si-kao-chu-shi-hua">思考初始化</h3>
<p>初始化是非常重要的，一步错，步步错，初始化状态一定要设置对，才可能得到正确的结果。</p>
<p>角度 1：直接从状态的语义出发；</p>
<p>角度 2：如果状态的语义不好思考，就考虑“状态转移方程”的边界需要什么样初始化的条件；</p>
<p>角度 3：从“状态转移方程”方程的下标看是否需要多设置一行、一列表示“哨兵”，这样可以避免一些边界的讨论，使得代码变得比较短。</p>
<h3 id="si-kao-shu-chu">思考输出</h3>
<p>有些时候是最后一个状态，有些时候可能会综合所有计算过的状态。</p>
<h3 id="si-kao-zhuang-tai-ya-suo">思考状态压缩</h3>
<p>“状态压缩”会使得代码难于理解，初学的时候可以不一步到位。先把代码写正确，然后再思考状态压缩。</p>
<p>状态压缩在有一种情况下是很有必要的，那就是状态空间非常庞大的时候（处理海量数据），此时空间不够用，就必须状态压缩。</p>
<hr>
<h2 id="ti-mu-leet-code">题目（LeetCode）</h2>
<h3 id="5-a-href-https-leetcode-cn-com-problems-longest-palindromic-substring-zui-chang-hui-wen-zi-chuan-a">5. <a href="https://leetcode-cn.com/problems/longest-palindromic-substring/" target="_blank" rel="noopener">最长回文子串</a></h3>
<p>描述：给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。</p>
<p>示例 1：</p>
<p>输入: “babad”</p>
<p>输出: “bab”</p>
<p>注意: “aba” 也是一个有效答案。</p>
<p>示例 2：</p>
<p>输入: “cbbd”</p>
<p>输出: “bb”</p>
<p>“动态规划”最关键的步骤是想清楚“状态如何转移”，事实上，“回文”是天然具有“状态转移”性质的：</p>
<blockquote>
<p>一个回文去掉两头以后，剩下的部分依然是回文（这里暂不讨论边界）。</p>
</blockquote>
<p>依然从回文串的定义展开讨论：</p>
<ol>
<li>
<p>如果一个字符串的头尾两个字符都不相等，那么这个字符串一定不是回文串；</p>
</li>
<li>
<p>如果一个字符串的头尾两个字符相等，才有必要继续判断下去。</p>
</li>
<li>
<p>如果里面的子串是回文，整体就是回文串；</p>
</li>
<li>
<p>如果里面的子串不是回文串，整体就不是回文串。</p>
</li>
</ol>
<p>即在头尾字符相等的情况下，里面子串的回文性质据定了整个子串的回文性质，这就是状态转移。因此可以把“状态”定义为原字符串的一个子串是否为回文子串。</p>
<h4 id="di-1-bu-ding-yi-zhuang-tai">第 1 步：定义状态</h4>
<p><code>dp[i][j]</code> 表示子串 <code>s[i,j]</code> 是否为回文子串。</p>
<h4 id="di-2-bu-si-kao-zhuang-tai-zhuan-yi-fang-cheng">第 2 步：思考状态转移方程</h4>
<p>这一步在做分类讨论（根据头尾字符是否相等），根据上面的分析得到：</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">dp[<span class="string">i</span>][<span class="symbol">j</span>] = (s[<span class="string">i</span>] == s[<span class="string">j</span>]) and dp[<span class="string">i + 1</span>][<span class="symbol">j - 1</span>]</span><br></pre></td></tr></table></figure>
<p>分析：</p>
<ol>
<li>
<p><code>i</code> 和 <code>j</code> 的关系是 <code>i &lt;= j</code>，因此，只需要填这张表的上半部分；</p>
</li>
<li>
<p>看到 <code>dp[i + 1][j - 1]</code> 就得考虑边界情况。边界条件是：表达式 <code>[i + 1, j - 1]</code>不构成区间，即长度严格小于 2，即<code>j - 1 - (i + 1) + 1 &lt; 2</code> ，整理得 <code>j - i &lt; 3</code>。</p>
</li>
</ol>
<p>这个结论很显然：当子串 <code>s[i, j]</code> 的长度等于 2 或者等于 3 的时候，我其实只需要判断一下头尾两个字符是否相等就可以直接下结论了。</p>
<ul>
<li>
<p>如果子串 <code>s[i + 1, j - 1]</code> 只有 1 个字符，即去掉两头，剩下中间部分只有 1 个字符，当然是回文；</p>
</li>
<li>
<p>如果子串 <code>s[i + 1, j - 1]</code> 为空串，那么子串 <code>s[i, j]</code> 一定是回文子串。</p>
</li>
</ul>
<p>因此，在 <code>s[i] == s[j]</code> 成立和 <code>j - i &lt; 3</code> 的前提下，直接可以下结论，<code>dp[i][j] = true</code>，否则才执行状态转移。</p>
<h4 id="di-3-bu-kao-lu-chu-shi-hua">第 3 步:考虑初始化</h4>
<p>初始化的时候，单个字符一定是回文串，因此把对角线先初始化为 1，即 <code>dp[i][i] = 1 </code>。</p>
<h4 id="di-4-bu-kao-lu-shu-chu">第 4 步:考虑输出</h4>
<p>只要一得到 <code>dp[i][j] = true</code>，就记录子串的长度和起始位置，没有必要截取，因为截取字符串也要消耗性能，记录此时的回文子串的“起始位置”和“回文长度”即可。</p>
<h4 id="di-5-bu-kao-lu-zhuang-tai-shi-fou-ke-yi-ya-suo">第 5 步:考虑状态是否可以压缩</h4>
<p>在填表的过程中，只参考了左下方的数值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">longestPalindrome</span><span class="params">(self,s)</span>:</span></span><br><span class="line"></span><br><span class="line">size = len(s)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> size &lt; <span class="number">2</span>:</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line"></span><br><span class="line">dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(size)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(size)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(size):</span><br><span class="line"></span><br><span class="line">dp[_][_] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">start_idx = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">max_len = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,size):</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(j):</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> s[i] == s[j]:</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> j - i &lt; <span class="number">3</span>: <span class="comment"># i,j 之间只有一个字符或者没有字符</span></span><br><span class="line"></span><br><span class="line">dp[i][j] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">dp[i][j] = dp[i+<span class="number">1</span>][j<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> dp[i][j] == <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">cur_len = j - i + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> cur_len &gt; max_len:</span><br><span class="line"></span><br><span class="line">max_len = cur_len</span><br><span class="line"></span><br><span class="line">start_idx = i</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> s[start_idx:start_idx+max_len]</span><br></pre></td></tr></table></figure>
<h3 id="1143-a-href-https-leetcode-cn-com-problems-longest-common-subsequence-zui-chang-gong-gong-zi-xu-lie-a">1143. <a href="https://leetcode-cn.com/problems/longest-common-subsequence/" target="_blank" rel="noopener">最长公共子序列</a></h3>
<p>描述：给定两个字符串 text1 和 text2，返回这两个字符串的最长公共子序列。一个字符串的 子序列是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。</p>
<p>例如，“ace” 是 “abcde” 的子序列，但 “aec” 不是 “abcde” 的子序列。两个字符串的「公共子序列」是这两个字符串所共同拥有的子序列。若这两个字符串没有公共子序列，则返回 0。</p>
<p>示例 1:</p>
<p>输入：text1 = “abcde”, text2 = “ace”</p>
<p>输出：3</p>
<p>解释：最长公共子序列是 “ace”，它的长度为 3。</p>
<p>示例 2:</p>
<p>输入：text1 = “abc”, text2 = “abc”</p>
<p>输出：3</p>
<p>解释：最长公共子序列是 “abc”，它的长度为 3。</p>
<p>示例 3:</p>
<p>输入：text1 = “abc”, text2 = “def”</p>
<p>输出：0</p>
<p>解释：两个字符串没有公共子序列，返回 0。</p>
<p>提示:</p>
<p>1 &lt;= text1.length &lt;= 1000</p>
<p>1 &lt;= text2.length &lt;= 1000</p>
<p>输入的字符串只含有小写英文字符。</p>
<h4 id="si-lu">思路：</h4>
<ol>
<li>
<p>状态：问题要求最长公共子序列，那就让dp表存储长度。</p>
</li>
<li>
<p>状态转移：</p>
</li>
<li>
<p>当 <code>S1i==S2j</code> 时，那么就能在 <code>S1</code> 的前 <code>i-1</code> 个字符与 <code>S2</code> 的前 <code>j-1</code> 个字符最长公共子序列的基础上再加上 <code>S1i</code> 这个值，最长公共子序列长度加 <code>1</code>，即 <code>dp[i][j] = dp[i-1][j-1] + 1</code>。</p>
</li>
<li>
<p>当 <code>S1i != S2j</code> 时，此时最长公共子序列为 <code>S1</code> 的前 <code>i-1</code> 个字符和 <code>S2</code> 的前 <code>j</code> 个字符最长公共子序列，或者 <code>S1</code> 的前 i 个字符和 <code>S2</code> 的前 <code>j-1</code> 个字符最长公共子序列，取它们的最大者，即 <code>dp[i][j] = max{ dp[i-1][j], dp[i][j-1] }</code>。</p>
</li>
</ol>
<p>综上，状态转移方程：</p>
<p><img src="/2018/03/09/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/dp_1.png" alt="avatar"></p>
<ol start="3">
<li>解的形式：解在dp表的右下角</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">longestCommonSubsequence</span><span class="params">(self, text1, text2)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:type text1: str</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:type text2: str</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:rtype: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> text1 <span class="keyword">or</span> <span class="keyword">not</span> text2:</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">size_1 = len(text1)</span><br><span class="line"></span><br><span class="line">size_2 = len(text2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化表格</span></span><br><span class="line"></span><br><span class="line">dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(size_2+<span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(size_1+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(size_1):</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(size_2):</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> text1[i] == text2[j]:</span><br><span class="line"></span><br><span class="line">dp[i+<span class="number">1</span>][j+<span class="number">1</span>] = dp[i][j] + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">dp[i+<span class="number">1</span>][j+<span class="number">1</span>] = max(dp[i][j+<span class="number">1</span>],dp[i+<span class="number">1</span>][j])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> dp[size_1][size_2]</span><br></pre></td></tr></table></figure>
<h3 id="53-a-href-https-leetcode-cn-com-problems-maximum-subarray-zui-da-xu-lie-he-a">53. <a href="https://leetcode-cn.com/problems/maximum-subarray/" target="_blank" rel="noopener">最大序列和</a></h3>
<p>给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。</p>
<p>示例:</p>
<p>输入: [-2,1,-3,4,-1,2,1,-5,4],</p>
<p>输出: 6</p>
<p>解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。</p>
<p>进阶:</p>
<p>如果你已经实现复杂度为 O(n) 的解法，尝试使用更为精妙的分治法求解。</p>
<h4 id="si-lu-1">思路</h4>
<p>法一：暴力动态规划：超时</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxSubArray</span><span class="params">(self, nums)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:type nums: List[int]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:rtype: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">size = len(nums)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> size == <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> nums[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(size)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(size)]</span><br><span class="line"></span><br><span class="line">start = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">end = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">max_val = float(<span class="string">'-inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(size):</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(j):</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> j - i == <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">dp[i][j] = nums[i] + nums[j]</span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">dp[i][j] = dp[i][j<span class="number">-1</span>] + nums[j]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> max_val &lt; dp[i][j]:</span><br><span class="line"></span><br><span class="line">start = i</span><br><span class="line"></span><br><span class="line">end = j</span><br><span class="line"></span><br><span class="line">max_val = dp[i][j]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> max(max_val, max(nums))</span><br></pre></td></tr></table></figure>
<p>法二：DP法，用了些灵巧的思路</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxSubArray</span><span class="params">(self, nums)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:type nums: List[int]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:rtype: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">max_, sum_ = nums[<span class="number">0</span>],nums[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> nums[<span class="number">1</span>:]:</span><br><span class="line"></span><br><span class="line">sum_ = sum_ + num <span class="keyword">if</span> sum_ &gt; <span class="number">0</span> <span class="keyword">else</span> num</span><br><span class="line"></span><br><span class="line">max_ = max_ <span class="keyword">if</span> max_ &gt; sum_ <span class="keyword">else</span> sum_</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> max_</span><br></pre></td></tr></table></figure>
<h3 id="300-a-href-https-leetcode-cn-com-problems-longest-increasing-subsequence-zui-chang-shang-sheng-zi-xu-lie-a">300. <a href="https://leetcode-cn.com/problems/longest-increasing-subsequence/" target="_blank" rel="noopener">最长上升子序列</a></h3>
<p>给定一个无序的整数数组，找到其中最长上升子序列的长度。</p>
<p>示例:</p>
<p>输入: [10,9,2,5,3,7,101,18]</p>
<p>输出: 4</p>
<p>解释: 最长的上升子序列是 [2,3,7,101]，它的长度是 4。</p>
<p>说明:可能会有多种最长上升子序列的组合，你只需要输出对应的长度即可。你算法的时间复杂度应该为 \(O(n^2)\) 。</p>
<p>进阶: 你能将算法的时间复杂度降低到 <code>O(n log n)</code> 吗?</p>
<h4 id="si-lu-2">思路</h4>
<p>状态：定义 dp[i]为考虑前 i个元素，以第 i 个数字结尾的最长上升子序列的长度，注意 nums[i]必须被选取。</p>
<p>状态转移：从小到大计算 dp[] 数组的值，在计算 dp[i] 之前，我们已经计算出 dp[0…i−1] 的值，则状态转移方程为：</p>
<p><img src="/2018/03/09/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/dp_2.png" alt="avatar"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lengthOfLIS</span><span class="params">(self, nums)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:type nums: List[int]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">:rtype: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">dp = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line"></span><br><span class="line">dp.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> nums[i] &gt; nums[j]:</span><br><span class="line"></span><br><span class="line">dp[i] = max(dp[i], dp[j] + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> max(dp)</span><br></pre></td></tr></table></figure>
<h3 id="a-href-https-leetcode-cn-com-problems-regular-expression-matching-10-zheng-ze-biao-da-shi-pi-pei-a"><a href="https://leetcode-cn.com/problems/regular-expression-matching/" target="_blank" rel="noopener">10. 正则表达式匹配</a></h3>
<p><strong>状态</strong>：问题要求解的是正则串是否可以匹配，则把状态设为是否可匹配。<code>dp[i][j]</code> 表示 <code>s</code> 的前 <code>i</code>个是否能被 <code>p</code> 的前 <code>j</code> 个匹配</p>
<p><strong>转移方程</strong> ：</p>
<ol>
<li>
<p><code>p[j] == s[i] or p[j] == &quot;.&quot; : dp[i][j] = dp[i-1][j-1] </code></p>
</li>
<li>
<p><code>p[j] ==&quot; * &quot;</code>:     # 比较难想</p>
<ol>
<li><code>p[j-1] != s[i]: dp[i][j] = dp[i][j-2] </code> # <code>*</code> 前的字符和原字符串不匹配,相当于该字符匹配0次。</li>
<li><code>p[j-1] == s[i] or p[j-1] == &quot;.&quot;：</code> # 匹配<br>
3.  <code>dp[i][j] = dp[i-1][j]</code> # * 匹配多次<br>
2.  <code>dp[i][j] = dp[i][j-1]</code> # * 匹配一次<br>
3.  <code>dp[i][j] = dp[i][j-2]</code> # * 匹配0次</li>
</ol>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span><span class="params">(self, s, p)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> p:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">not</span> s</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> s <span class="keyword">and</span> len(p) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        nrow = len(s) + <span class="number">1</span></span><br><span class="line">        ncol = len(p) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        dp = [[<span class="literal">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(ncol)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(nrow)]</span><br><span class="line"></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">1</span>] = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">2</span>, ncol):</span><br><span class="line">            j = c - <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> p[j] == <span class="string">'*'</span>:</span><br><span class="line">                dp[<span class="number">0</span>][c] = dp[<span class="number">0</span>][c - <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">1</span>, nrow): <span class="comment"># r 原始串</span></span><br><span class="line">            i = r - <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">1</span>, ncol): <span class="comment"># c 正则串</span></span><br><span class="line">                j = c - <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> s[i] == p[j] <span class="keyword">or</span> p[j] == <span class="string">'.'</span>:</span><br><span class="line">                    dp[r][c] = dp[r - <span class="number">1</span>][c - <span class="number">1</span>]</span><br><span class="line">                <span class="keyword">elif</span> p[j] == <span class="string">'*'</span>:</span><br><span class="line">                    <span class="keyword">if</span> p[j - <span class="number">1</span>] == s[i] <span class="keyword">or</span> p[j - <span class="number">1</span>] == <span class="string">'.'</span>:</span><br><span class="line">                        dp[r][c] = dp[r - <span class="number">1</span>][c] <span class="keyword">or</span> dp[r][c - <span class="number">2</span>]</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        dp[r][c] = dp[r][c - <span class="number">2</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[r][c] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[nrow - <span class="number">1</span>][ncol - <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>专业技能</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>背包问题</title>
    <url>/2018/03/01/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="ti-mu">题目</h3>
<p>有N件物品和一个容量为<code>v</code>的背包，放入第i件物品耗费空间<code>c[i]</code>得到价值为<code>W[i]</code>,求哪些物品装入背包可使得总价值最大？</p>
<h4 id="dong-tai-gui-hua-qiu-jie">动态规划求解：</h4>
<p>状态：问题所求最大价值，设状态即为背包中物品的价值</p>
<p>状态转移方程：</p>
<p><code>dp[i][j]</code>表示前i件物品放入一个容量为V的背包获得的最大价值状态转移方程为<br>
\[
\mathrm{dp}[\mathrm{i}][\mathrm{v}]=\max \left\{\begin{array}{l}
dp[i-1][v](\text {not select}) \\
dp[i-1][v-C[i]]+W[i](\text {select})
\end{array}\right.
\]<br>
其中<code>dp[i-1][v-C[i]]</code>表示前<code>i-1</code>件物品放入容量为<code>v-C[i]</code>的背包中。</p>
<h4 id="dai-ma">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># F[0, 0...V] = 0</span></span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>):</span><br><span class="line">    dp[<span class="number">0</span>][v] = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># 容量装不下当前物品时，保留原值</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(<span class="number">0</span>, C[i]):</span><br><span class="line">        dp[i][v] = dp[i<span class="number">-1</span>][v]</span><br><span class="line">    <span class="comment"># 状态转移</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(C[i], V+<span class="number">1</span>):</span><br><span class="line">        dp[i][v] = max(dp[i<span class="number">-1</span>][v], dp[i<span class="number">-1</span>][v-C[i]] + W[i])</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h4 id="you-hua">优化</h4>
<ol>
<li>
<p>时间复杂度：<code>O(VN)</code></p>
</li>
<li>
<p>空间复杂度：<code>O(VN)</code>可优化成<code>O(N)</code></p>
<p><code>dp[i][v]</code>由<code>dp[i-1][v-C[i]]</code>和<code>dp[i-1][v]</code>两个子问题递推得到，所以第<code>i</code>次迭代时，<code>v</code>按照<code>V-&gt;0</code>的逆序计算<code>dp[v]</code>可以保证<code>dp[v]=max(dp[v],dp[v-C[i]]+W[i])</code>，这样在赋值之前，访问到的均为<code>i-1</code>次迭代保留的值。</p>
<p>空间为<code>c[i]</code>的物品不会受到状态<code>dp[0...C[i]]</code>的影响，即不影响装不下该物品的背包能装入的最大价值，所以<code>v</code>中<code>V</code>递减到<code>C[i]</code>就可以了。</p>
</li>
<li>
<p>代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dp = [<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(V, C[i]<span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        dp[v] = max(dp[v], dp[v-C[i]] + W[i])</span><br></pre></td></tr></table></figure>
<p>对处理0-1背包的物品进行抽象：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_one_pack</span><span class="params">(F, c, w)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(V, c<span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        dp[v] = max(dp[v], dp[v-c] + w)</span><br><span class="line"></span><br><span class="line">dp = [<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    zero_one_pack(dp, C[i], W[i])</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="chu-shi-hua-xi-jie">初始化细节</h4>
<p>初始化dp的过程，实际上就是在没有任何物品可以装入背包时的合法状态（必要但不充分条件）</p>
<ol>
<li>
<p>恰好装满背包时，求最优解：<code>dp[0]=0,dp[1...V]=</code>\(-\infty\)</p>
<ul>
<li>
<p>容量为0时，什么也装不下，价值为0，所以<code>dp[0]=0</code></p>
</li>
<li>
<p>容量不为0的时候，状态未定义，状态设为\(-\infty\)</p>
</li>
</ul>
<p>按此进行初始化，迭代过程中，只有能够恰好装入的物品才会更新出合理值（正值），基于<code>dp[0]=0</code></p>
</li>
<li>
<p>没要求把背包装满：<code>dp[0...V]=0</code></p>
<p>任何容量都有一个合法的“什么都不装”，所以设<code>dp[0...V]=0</code></p>
</li>
</ol>
<h3 id="wan-quan-bei-bao">完全背包</h3>
<p>![image-20200321181733468](/Users/lizhen/Library/Application Support/typora-user-images/image-20200321181733468.png)</p>
<h4 id="ti-mu-1">题目</h4>
<p>有N种物品和1个容量为V的背包，每种物品可以有任意多个，放入第i种物品的耗费空间<code>C[i]</code>，价值为<code>W[i]</code>,求背包能够装入的最大价值？</p>
<h4 id="si-lu">思路</h4>
<p>每种物品可以放入0件，1件，…，<code>int(V/C[i])</code>件，令<code>dp[i][v]</code>表示前i种物品放入容量v的背包获取的最大价值，则<code>dp[i][v]=max(dp[i-1][v-k*C[i]]+k*W[i]),0&lt;=k*C[i]&lt;=v</code></p>
<h4 id="you-hua-1">优化</h4>
<p>时间复杂度：<code>O(nv</code>\(\sum \frac{V}{C[i]}\))</p>
<p>简单优化：一个简单有效的优化思路是：单位价值更便宜的应该直接跳过</p>
<p>若两件物品<code>i,j</code>有<code>C[i]&lt;=C[j] and W[i]&gt;=W[j]</code>;则<code>j</code>可以直接跳过，不必考虑，对于随机数据，这会大大减少物品数量，加快速度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sieve = &#123;C[i]: <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>)&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># 跳过装不下的</span></span><br><span class="line">    <span class="keyword">if</span> C[i] &gt; V:</span><br><span class="line">        sieve[C[i]] = <span class="number">-1</span></span><br><span class="line">    <span class="comment"># 跳过 C[i] == C[j] and W[i] &gt; W[j]的j物品</span></span><br><span class="line">    <span class="keyword">elif</span> sieve[C[i]] &lt; W[i]:</span><br><span class="line">        sieve[C[i]] = W[i]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">if</span> sieve[C[i]] &lt; <span class="number">0</span> <span class="keyword">or</span> W[i] &lt; sieve[C[i]]:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(C[i], V+<span class="number">1</span>):</span><br><span class="line">        max_k = V // C[i]</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, max_k+<span class="number">1</span>):</span><br><span class="line">            dp[i][v] = max(dp[i<span class="number">-1</span>][v], dp[i<span class="number">-1</span>][v-k*C[i]] + k*W[i])</span><br></pre></td></tr></table></figure>
<p>最终优化方案：</p>
<p>按照正序容量更新当前总价值，<code>dp[v-C[i]]</code>中可能是<code>i-1</code>次迭代中保留的值，也可能是本次迭代中产生了变化的值。（如果加选第i件物品）</p>
<p>时间复杂度：<code>O(NV)</code></p>
<p>空间复杂度:<code>O(V)</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dp = [<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(C[i], V+<span class="number">1</span>):</span><br><span class="line">        dp[v] = max(dp[v], dp[v-C[i]] + W[i])</span><br></pre></td></tr></table></figure>
<p>对处理一种完全背包的物品过程进行抽象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">complete_pack</span><span class="params">(F, c, w)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(c, V+<span class="number">1</span>):</span><br><span class="line">        F[v] = max(F[v], F[v-c] + w)</span><br><span class="line"></span><br><span class="line">dp = [<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    complete_pack(dp, C[i], W[i])</span><br></pre></td></tr></table></figure>
<h4 id="bi-jiao">比较</h4>
<p>0-1 背包：倒序容量，当前新物品之多影响一次当前总价值</p>
<p>完全背包：正序容量，随着容量增加，当前新物品可以多次影响当前总价值</p>
<h3 id="duo-zhong-bei-bao">多重背包</h3>
<p>有<code>N</code>种物品和1个容量为<code>V</code>的背包，第i种物品有<code>M[i]</code>件，每件耗费空间<code>C[i]</code>,价值<code>W[i]</code>,求能装入背包的最大价值。</p>
<h4 id="si-lu-1">思路1</h4>
<p>和完全背包类似，对i种物品有<code>M[i]+1</code>种策略，取<code>0</code>件，取<code>1</code>件，…，取<code>M[i]</code>,令<code>dp[i][v]</code>表示前i中物品放入容量v中获得的最大价值，则<code>dp[i][v]=max(dp[i-1][v-k*C[i]]+k*W[i]),0&lt;=k&lt;=M[i]</code></p>
<p>时间复杂度：<code>O(V</code>\(\sum M_i\)`</p>
<h4 id="si-lu-2">思路2</h4>
<p>转化为0-1背包问题，把第i种物品换成<code>M[i]</code>件<code>0-1</code>背包中的物品，得到物品数为\(\sum M_i\)的<code>0-1</code>背包问题，时间复杂度仍为\(O(V\sum M_i)\)。使用二进制的思想，可以把时间复杂度下降为\(O(V\sum{log{M_i}})\),即按照如下的方式进行拆分：</p>
<p>\(2^0,2^1,2^2,...,2^{k-1},M_i-2^k+1\)</p>
<p>这样拼成的新物品总量等于原来物品的数量，第i件物品拆分成了\(O(log{m_i})\)</p>
<h5 id="dai-ma-1">代码</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiple_pack</span><span class="params">(F, c, w, m)</span>:</span></span><br><span class="line">    <span class="comment"># 如果物品足够多，就变成了完全背包</span></span><br><span class="line">    <span class="keyword">if</span> c * m &gt;= V:</span><br><span class="line">        complete_pack(F, c, w)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 多重背包按照2次幂进行划分成0-1背包</span></span><br><span class="line">    k = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> k &lt; m:</span><br><span class="line">        zero_one_pack(F, k*c, k*w)</span><br><span class="line">        m = m - k</span><br><span class="line">        k = <span class="number">2</span> * k</span><br><span class="line">    <span class="comment"># 把剩余的物品当作1件物品</span></span><br><span class="line">    zero_one_pack(F, c * m, w * m)</span><br><span class="line"></span><br><span class="line">dp = [<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    multiple_pack(dp, C[i], W[i], M[i])</span><br></pre></td></tr></table></figure>
<h3 id="er-wei-fei-yong-bei-bao">二维费用背包</h3>
<p>对于每件物品有两种不同的cost，每种cost有一个最大容纳值（背包容量），怎样选择物品使得背包容纳最大价值？第i种物品两种费用分别为<code>C[i]</code>和<code>D[i]</code>，两种cost最大容纳值为V,U，物品的价值为<code>W[i]</code>(题目场景样例：打矩形中放小矩形，求最大容量)</p>
<h4 id="si-lu-3">思路</h4>
<p>费用加1维，状态加1维；令<code>dp[i][v][u]</code>表示前i件物品付出两种费用分别为v，u时可获得的最大价值。则<code>dp[i][v][u]=max(dp[i-1][v][u],dp[i-1][v-C[i]][u-D[i]]+W[i])</code></p>
<h4 id="you-hua-2">优化</h4>
<p>类似于一维背包空间优化方法，二维背包与之类似：</p>
<ol>
<li>二维0-1背包：v和u采用逆序循环</li>
<li>二维完全背包：v和u采用顺序循环</li>
<li>二维多重背包：拆分物品</li>
</ol>
<h4 id="dai-ma-2">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_dim_zero_one_pack</span><span class="params">(F, c, d, w)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(V, c<span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> range(U, d<span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            F[v][u] = max(F[v][u], F[v-c][u-d] + w)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_dim_complete_pack</span><span class="params">(F, c, d, w)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> range(c, V+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> range(d, U+<span class="number">1</span>):</span><br><span class="line">            F[v][u] = max(F[v][u], F[v-c][u-d] + w)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_dim_multiple_pack</span><span class="params">(F, c, d, w, m)</span>:</span></span><br><span class="line">    <span class="comment"># 两种cost数量都充足，可直接按照完全背包处理</span></span><br><span class="line">    <span class="keyword">if</span> c * m &gt;= V <span class="keyword">and</span> d * m &gt;= U:</span><br><span class="line">        two_dim_complete_pack(F, c, d, w)</span><br><span class="line">    <span class="comment"># 多重背包按照2次幂进行划分成0-1背包</span></span><br><span class="line">    k = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> k &lt; m:</span><br><span class="line">        two_dim_complete_pack(F, k * c, k * d, k * w)</span><br><span class="line">        m = m - k</span><br><span class="line">        k = <span class="number">2</span> * k</span><br><span class="line">    <span class="comment"># 把剩余的物品当作1件物品</span></span><br><span class="line">    two_dim_zero_one_pack(F, m * c, m * d, m * w)</span><br><span class="line"></span><br><span class="line">F = [[<span class="number">0</span> <span class="keyword">for</span> v <span class="keyword">in</span> range(V+<span class="number">1</span>)] <span class="keyword">for</span> u <span class="keyword">in</span> range(U+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    two_dim_zero_one_pack(F, C[i], D[i], W[i])</span><br><span class="line">    <span class="comment"># two_dim_complete_pack(F, C[i], D[i], W[i])</span></span><br><span class="line">    <span class="comment"># two_dim_multiple_pack(F, C[i], D[i], W[i], M[i])</span></span><br></pre></td></tr></table></figure>
<h4 id="ying-yong">应用</h4>
<p>二维费用题目的一种隐含表述方式：最多取U件</p>
<p>对于有这种限制的题目，可以将’件数’看作一种费用，每件物品件数费用为1，可以达到的最大件数费用非U，<code>dp[v][u]</code>表示cost为v，最多选择u件物品时所达到的最大价值（再根据物品的特性（0-1,完全，多重）采用不同的方式进行循环，得出答案）</p>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>背包问题</tag>
      </tags>
  </entry>
  <entry>
    <title>排序算法</title>
    <url>/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="pai-xu-suan-fa">排序算法</h2>
<p><img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/%E6%8E%92%E5%BA%8F.png" alt="avatar"></p>
<a id="more"></a>
<h3 id="mou-pao-pai-xu">冒泡排序</h3>
<h4 id="si-xiang">思想</h4>
<p>是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。</p>
<h4 id="bu-zou">步骤</h4>
<ol>
<li>比较相邻的元素。如果第一个比第二个大，就交换它们两个；</li>
<li>对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数；</li>
<li>针对所有的元素重复以上的步骤，除了最后一个；</li>
<li>重复步骤1~3，直到排序完成。</li>
</ol>
<h4 id="fu-za-du">复杂度</h4>
<ol>
<li>时间复杂度：<code>O(1)</code></li>
<li>空间复杂度：\(O(n^2)\)</li>
</ol>
<h4 id="dai-ma">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubble_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="comment"># 外层循环: 走访数据的次数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 设置是否交换标志位</span></span><br><span class="line">        flag = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 内层循环: 每次走访数据时, 相邻对比次数</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(nums) - i - <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 要求从低到高</span></span><br><span class="line">            <span class="comment"># 如次序有误就交换</span></span><br><span class="line">            <span class="keyword">if</span> nums[j] &gt; nums[j + <span class="number">1</span>]:</span><br><span class="line">                nums[j], nums[j + <span class="number">1</span>] = nums[j + <span class="number">1</span>], nums[j]</span><br><span class="line">                <span class="comment"># 发生了数据交换</span></span><br><span class="line">                flag = <span class="literal">True</span></span><br><span class="line">        <span class="comment"># 如果未发生交换数据, 则说明后续数据均有序</span></span><br><span class="line">        <span class="keyword">if</span> flag == <span class="literal">False</span>:</span><br><span class="line">            <span class="keyword">break</span>  <span class="comment"># 跳出数据走访</span></span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<h3 id="xuan-ze-pai-xu">选择排序</h3>
<h4 id="si-xiang-1">思想</h4>
<p>它的工作原理：首先在未排序序列中找到最小元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。</p>
<h4 id="bu-zou-1">步骤</h4>
<p>n个记录的直接选择排序可经过n-1趟直接选择排序得到有序结果。</p>
<ol>
<li>初始状态：无序区为<code>R[1…n]</code>，有序区为空；</li>
<li>第i趟排序<code>(i=1,2,3…n-1)</code>开始时，当前有序区和无序区分别为<code>R[1…i-1]和R(i…n）</code>。该趟排序从当前无序区中-选出关键字最小的记录 <code>R[k]</code>，将它与无序区的第1个记录R交换，使<code>R[1…i]</code>和<code>R[i+1…n)</code>分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区；</li>
<li>n-1趟结束，数组有序化了。</li>
</ol>
<h4 id="fu-za-du-1">复杂度</h4>
<ol>
<li>时间复杂度：\(O(n^2)\)</li>
<li>空间复杂度：O(1)</li>
</ol>
<h4 id="dai-ma-1">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selection_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line">        <span class="keyword">return</span> nums</span><br><span class="line">    size = len(nums)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(size):</span><br><span class="line">        min_idx = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>,size):</span><br><span class="line">            <span class="keyword">if</span> nums[j] &lt; nums[min_idx]:</span><br><span class="line">                min_idx = j</span><br><span class="line">        temp = nums[min_idx]</span><br><span class="line">        nums[min_idx] = nums[i]</span><br><span class="line">        nums[i] = temp</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<h3 id="gui-bing-pai-xu">归并排序</h3>
<h4 id="si-xiang-2">思想</h4>
<p>和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是O(n log n）的时间复杂度。代价是需要额外的内存空间。</p>
<h4 id="bu-zou-2">步骤</h4>
<p><strong>归并排序</strong>是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。归并排序是一种稳定的排序方法。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。</p>
<ol>
<li>把长度为n的输入序列分成两个长度为n/2的子序列；</li>
<li>对这两个子序列分别采用归并排序；</li>
<li>将两个排序好的子序列合并成一个最终的排序序列。</li>
</ol>
<h4 id="fu-za-du-2">复杂度</h4>
<ol>
<li>时间复杂度\(O(nlog_2 n)\)</li>
</ol>
<h4 id="dai-ma-2">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(nums) &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> nums</span><br><span class="line"></span><br><span class="line">    mid_idx = len(nums) / <span class="number">2</span></span><br><span class="line">    left_array = nums[<span class="number">0</span>:mid_idx]</span><br><span class="line">    right_array = nums[mid_idx:len(nums)]</span><br><span class="line">    <span class="keyword">return</span> merge(merge_sort(left_array),merge_sort(right_array))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span><span class="params">(left_array,right_array)</span>:</span></span><br><span class="line">    <span class="comment"># 合并两个有序数组</span></span><br><span class="line">    res = []</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    j = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; len(left_array) <span class="keyword">and</span> j &lt; len(right_array):</span><br><span class="line">        <span class="keyword">if</span> left_array[i] &lt; right_array[j]:</span><br><span class="line">            res.append(left_array[i])</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            res.append(right_array[j])</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i &lt; len(left_array):</span><br><span class="line">        res.extend(left_array[i:])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        res.extend(right_array[j:])</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3 id="cha-ru-pai-xu">插入排序</h3>
<h4 id="si-xiang-3">思想</h4>
<p>通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。</p>
<h4 id="bu-zou-3">步骤</h4>
<p>一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下：</p>
<ol>
<li>从第一个元素开始，该元素可以认为已经被排序；</li>
<li>取出下一个元素，在已经排序的元素序列中从后向前扫描；</li>
<li>如果该元素（已排序）大于新元素，将该元素移到下一位置；</li>
<li>重复步骤3，直到找到已排序的元素小于或者等于新元素的位置；</li>
<li>将新元素插入到该位置后；</li>
<li>重复步骤2~5。</li>
</ol>
<h4 id="fu-za-du-3">复杂度</h4>
<ol>
<li>时间复杂度：<code>O(1)</code></li>
<li>空间复杂度：\(O(n^2)\)</li>
</ol>
<h4 id="dai-ma-3">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(nums)):</span><br><span class="line">        temp = nums[i]</span><br><span class="line">        pos = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> temp &lt; nums[j]:</span><br><span class="line">                nums[j + <span class="number">1</span>] = nums[j]</span><br><span class="line">                pos = j</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pos = j + <span class="number">1</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        nums[pos] = temp</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<h3 id="xi-er-pai-xu">希尔排序</h3>
<p>希尔排序是希尔（Donald Shell） 于1959年提出的一种排序算法。希尔排序也是一种插入排序，它是简单插入排序经过改进之后的一个更高效的版本，也称为缩小增量排序，同时该算法是冲破O(n2）的第一批算法之一。它与插入排序的不同之处在于，它会优先比较距离较远的元素。希尔排序又叫缩小增量排序。</p>
<h4 id="si-xiang-4">思想</h4>
<p>希尔排序是把记录按下表的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。</p>
<h4 id="bu-zou-4">步骤:</h4>
<p>先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序,具体步骤如下：</p>
<ol>
<li>选择一个增量序列<code>t1，t2，…，tk</code>，其中<code>ti&gt;tj，tk=1</code>；</li>
<li>按增量序列个数k，对序列进行 k 趟排序；</li>
<li>每趟排序，根据对应的增量ti，将待排序列分割成若干长度为 m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。<br>
<img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/aHR0cHM6Ly9pbWFnZXMyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvMTE5MjY5OS8yMDE4MDMvMTE5MjY5OS0yMDE4MDMxOTA5NDExNjA0MC0xNjM4NzY2MjcxLnBuZw.png" alt="avat"></li>
</ol>
<h4 id="fu-za-du-4">复杂度</h4>
<ol>
<li>时间复杂度\(O(nlog_2 n)\)</li>
</ol>
<h4 id="dai-ma-4">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shell_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    size = len(nums)</span><br><span class="line">    gap = size / <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> gap:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(gap, size):</span><br><span class="line">            temp = nums[i]</span><br><span class="line">            pre_index = i - gap</span><br><span class="line">            <span class="comment"># 对于划分到一组内的数进行排序</span></span><br><span class="line">            <span class="keyword">while</span> pre_index &gt;= <span class="number">0</span> <span class="keyword">and</span> nums[pre_index] &gt; temp:</span><br><span class="line">                nums[pre_index + gap] = nums[pre_index]</span><br><span class="line">                pre_index -= gap</span><br><span class="line">            <span class="comment"># 插入</span></span><br><span class="line">            nums[pre_index+gap] = temp</span><br><span class="line">        </span><br><span class="line">        gap /= <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<h3 id="kuai-pai">快排</h3>
<p><img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/1450319-20190525163106857-545290968.png" alt="avatar"></p>
<h4 id="bu-zou-5">步骤</h4>
<p>快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。</p>
<ol>
<li>
<p>从数列中挑出一个元素，称为 “基准”（pivot ）</p>
</li>
<li>
<p>重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作</p>
</li>
<li>
<p>递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。<br>
复杂度:</p>
</li>
<li>
<p>时间复杂度：\(O(nlogn)\)</p>
</li>
<li>
<p>空间复杂度：\(O(logn)\)</p>
</li>
<li>
<p>稳定性：不稳定</p>
</li>
</ol>
<h4 id="dai-ma-5">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="comment"># 递归退出条件</span></span><br><span class="line">    <span class="comment"># 仅剩一个元素无需继续分组</span></span><br><span class="line">    <span class="keyword">if</span> len(nums) &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> nums</span><br><span class="line">        <span class="comment"># 设置关键数据</span></span><br><span class="line">    a = nums[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 找出所有比 a 大的数据</span></span><br><span class="line">    big = [x <span class="keyword">for</span> x <span class="keyword">in</span> nums <span class="keyword">if</span> x &gt; a]</span><br><span class="line">    <span class="comment"># 找出所有比 a 小的数据</span></span><br><span class="line">    small = [x <span class="keyword">for</span> x <span class="keyword">in</span> nums <span class="keyword">if</span> x &lt; a]</span><br><span class="line">    <span class="comment"># 找出所有与 a 相等的数据</span></span><br><span class="line">    same = [x <span class="keyword">for</span> x <span class="keyword">in</span> nums <span class="keyword">if</span> x == a]</span><br><span class="line">    <span class="comment"># 拼接数据排序的结果</span></span><br><span class="line">    <span class="keyword">return</span> quick_sort(small) + same + quick_sort(big)</span><br></pre></td></tr></table></figure>
<h3 id="dui-pai-xu">堆排序</h3>
<p>堆排序（Heapsort） 是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。</p>
<h4 id="bu-zou-6">步骤</h4>
<ol>
<li>将初始待排序关键字序列(R1,R2….Rn)构建成大顶堆，此堆为初始的无序区；</li>
<li>将堆顶元素R[1]与最后一个元素R[n]交换，此时得到新的无序区<code>(R1,R2,……Rn-1)</code>和新的有序区(Rn),且满足<code>R[1,2…n-1]&lt;=R[n]</code>；</li>
<li>由于交换后新的堆顶R[1]可能违反堆的性质，因此需要对当前无序区<code>(R1,R2,……Rn-1)</code>调整为新堆，然后再次将R[1]与无序区最后一个元素交换，得到新的无序区<code>(R1,R2….Rn-2)</code>和新的有序区<code>(Rn-1,Rn)</code>。不断重复此过程直到有序区的元素个数为n-1，则整个排序过程完成。</li>
</ol>
<p><img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/CA6B626B-98BA-4AA5-BC01-FDAFE554BB87.gif" alt="avatar"></p>
<h4 id="fu-za-du-5">复杂度</h4>
<ol>
<li>时间复杂度：O(nlogn)</li>
</ol>
<h4 id="dai-ma-6">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">heap_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    size = len(nums)</span><br><span class="line">    <span class="keyword">if</span> size &lt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> nums</span><br><span class="line">    <span class="comment"># 构建一个最大堆</span></span><br><span class="line">    build_max_heap(nums)</span><br><span class="line">    <span class="comment"># 循环将根和最后一个元素交换，然后重新调整最大堆。</span></span><br><span class="line">    <span class="keyword">while</span> size &gt; <span class="number">0</span>:</span><br><span class="line">        temp = nums[<span class="number">0</span>]</span><br><span class="line">        nums[<span class="number">0</span>] = nums[size - <span class="number">1</span>]</span><br><span class="line">        nums[size - <span class="number">1</span>] = temp</span><br><span class="line"></span><br><span class="line">        size -= <span class="number">1</span></span><br><span class="line">        adjust_heap(nums, <span class="number">0</span>,size)</span><br><span class="line">    <span class="keyword">return</span> nums</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_max_heap</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="comment"># 从最后一个非叶子节点开始向上构造最大堆</span></span><br><span class="line">    <span class="comment"># i 的左子树和右子树分别为2i+1 和 2(i+1)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums) / <span class="number">2</span><span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        adjust_heap(nums, i,len(nums))</span><br><span class="line">    <span class="comment"># return nums</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adjust_heap</span><span class="params">(nums, i,size)</span>:</span></span><br><span class="line">    max_index = i</span><br><span class="line">    <span class="comment"># 如果有左子树，且左子树大于父节点，则将最大指针指向左子树</span></span><br><span class="line">    <span class="keyword">if</span> <span class="number">2</span> * i + <span class="number">1</span> &lt; size <span class="keyword">and</span> nums[<span class="number">2</span> * i + <span class="number">1</span>] &gt; nums[max_index]:</span><br><span class="line">        max_index = <span class="number">2</span> * i + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> <span class="number">2</span> * (i + <span class="number">1</span>) &lt; size <span class="keyword">and</span> nums[<span class="number">2</span> * (i + <span class="number">1</span>)] &gt; nums[max_index]:</span><br><span class="line">        max_index = <span class="number">2</span> * (i + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> max_index == i:</span><br><span class="line">        temp = nums[max_index]</span><br><span class="line">        nums[max_index] = nums[i]</span><br><span class="line">        nums[i] = temp</span><br><span class="line">        adjust_heap(nums, max_index,size)</span><br></pre></td></tr></table></figure>
<h3 id="ji-shu-pai-xu">计数排序</h3>
<p><strong>计数排序</strong>的核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。</p>
<h4 id="ji-ben-si-xiang">基本思想</h4>
<p>计数排序(Counting sort) 是一种稳定的排序算法。计数排序使用一个额外的数组C，其中第i个元素是待排序数组A中值等于i的元素的个数。然后根据数组C来将A中的元素排到正确的位置。它<strong>只能对整数进行排序</strong>。<br>
<img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/E4559E2C-6389-4EAB-976A-95289A079EB8.gif" alt="avatar"></p>
<h4 id="suan-fa-bu-zou">算法步骤</h4>
<ol>
<li>找出待排序的数组中最大和最小的元素</li>
<li>统计数组中每个值为i的元素出现的次数，存入数组C的第i项</li>
<li>对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）</li>
<li>反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。</li>
</ol>
<h4 id="fu-za-du-6">复杂度</h4>
<ol>
<li>时间复杂度：O(n+k)</li>
</ol>
<h4 id="dai-ma-7">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_sort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="comment"># 获取arr中的最大值和最小值</span></span><br><span class="line">    max_num = max(arr)</span><br><span class="line">    min_num = min(arr)</span><br><span class="line">    <span class="comment"># 以最大值和最小值的差作为中间数组的长度,并构建中间数组，初始化为0</span></span><br><span class="line">    length = max_num - min_num + <span class="number">1</span></span><br><span class="line">    temp_arr = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(length)]</span><br><span class="line">    <span class="comment"># 创建结果List，存放排序完成的结果</span></span><br><span class="line">    res_arr = list(range(len(arr)))</span><br><span class="line">    <span class="comment"># 第一次循环遍历</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> arr:</span><br><span class="line">        temp_arr[num - min_num] += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 第二次循环遍历</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, length):</span><br><span class="line">        temp_arr[i] = temp_arr[i] + temp_arr[i - <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 第三次循环遍历</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr) - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        res_arr[temp_arr[arr[i] - min_num] - <span class="number">1</span>] = arr[i]</span><br><span class="line">        temp_arr[arr[i] - min_num] -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res_arr</span><br></pre></td></tr></table></figure>
<h3 id="tong-pai-xu">桶排序</h3>
<p>桶排序 是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。</p>
<h4 id="ji-ben-si-xiang-1">基本思想</h4>
<p>假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排</p>
<p><img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/D7FA465B-93A5-493B-BE2B-15375A28FAE2.gif" alt="avatar"></p>
<h4 id="suan-fa-bu-zou-1">算法步骤</h4>
<ol>
<li>根据数据分桶</li>
<li>遍历数组，把数据一个一个放到对应的桶中。</li>
<li>对每个不是空的桶进行排序，可以使用其他排序方法，也可以递归使用桶排序。</li>
<li>从不是空的桶里把排好序的数据拼接起来。</li>
</ol>
<h4 id="fu-za-du-7">复杂度</h4>
<ol>
<li>时间复杂度：最好情况下使用线性时间O(n)，桶排序的时间复杂度，取决与对各个桶之间数据进行排序的时间复杂度，因为其它部分的时间复杂度都为O(n)。很显然，桶划分的越小，各个桶之间的数据越少，排序所用的时间也会越少。</li>
</ol>
<h4 id="dai-ma-8">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k)</span>:</span></span><br><span class="line">        self.key = k;</span><br><span class="line">        self.next = <span class="literal">None</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bucket_sort</span><span class="params">(arr,bucket_num)</span>:</span></span><br><span class="line">    h = [];</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, bucket_num):</span><br><span class="line">        h.append(node(<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(arr)):</span><br><span class="line">        tmp = node(arr[i])</span><br><span class="line">        map = arr[i] / bucket_num</span><br><span class="line">        p = h[map]</span><br><span class="line">        <span class="keyword">if</span> p.key <span class="keyword">is</span> <span class="number">0</span>:</span><br><span class="line">            h[map].next = tmp</span><br><span class="line">            h[map].key = h[map].key + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">while</span> (p.next != <span class="literal">None</span> <span class="keyword">and</span> p.next.key &lt;= tmp.key):</span><br><span class="line">                p = p.next</span><br><span class="line">            tmp.next = p.next</span><br><span class="line">            p.next = tmp</span><br><span class="line">            h[map].key = h[map].key + <span class="number">1</span></span><br><span class="line">    k = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">10</span>):</span><br><span class="line">        q = h[i].next</span><br><span class="line">        <span class="keyword">while</span> (q != <span class="literal">None</span>):</span><br><span class="line">            arr[k] = q.key</span><br><span class="line">            k = k + <span class="number">1</span></span><br><span class="line">            q = q.next</span><br><span class="line">    <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure>
<h3 id="ji-shu-pai-xu-1">基数排序</h3>
<p>基数排序也是非比较的排序算法，对每一位进行排序，从最低位开始排序，复杂度为O(kn),为数组长度，k为数组中的数的最大的位数；</p>
<h4 id="ji-ben-si-xiang-2">基本思想</h4>
<p>基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。基数排序基于分别排序，分别收集，所以是稳定的。<br>
<img src="/2018/02/25/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/2BA80F27-FCDC-4BEA-BD0B-2AAB59242752.gif" alt="avatar"></p>
<h4 id="suan-fa-bu-zou-2">算法步骤</h4>
<ol>
<li>取得数组中的最大数，并取得位数；</li>
<li>arr为原始数组，从最低位开始取每个位组成radix数组；</li>
<li>对radix进行计数排序（利用计数排序适用于小范围数的特点）；</li>
</ol>
<h4 id="fu-za-du-8">复杂度</h4>
<ol>
<li>时间复杂度：O（kn）</li>
</ol>
<h4 id="dai-ma-9">代码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">radix_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    k = len(str(max(nums)))  <span class="comment"># 最大的数的位数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        tong = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            tong[int(num / (<span class="number">10</span> ** i)) % <span class="number">10</span>].append(num)  <span class="comment"># 获取当前排序位数上的数字</span></span><br><span class="line">        <span class="comment">#print(tong)</span></span><br><span class="line">        nums = []</span><br><span class="line">        <span class="keyword">for</span> zitong <span class="keyword">in</span> tong:</span><br><span class="line">            nums = nums + zitong</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>专业技能</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>查找算法</title>
    <url>/2018/02/10/%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="jian-dan-sou-suo-cha-zhao">简单搜索查找</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">brute_force_search</span><span class="params">(nums,key)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">        <span class="keyword">if</span> num == key:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="er-fen-cha-zhao">二分查找</h3>
<p>前提：数组有序</p>
<h4 id="die-dai-ban-ben">迭代版本</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span><span class="params">(nums, key)</span>:</span></span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = len(nums) - <span class="number">1</span></span><br><span class="line">    time = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        time += <span class="number">1</span></span><br><span class="line">        mid = int((low + high) / <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> key &lt; nums[mid]:</span><br><span class="line">            high = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> key &gt; nums[mid]:</span><br><span class="line">            low = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 打印折半的次数</span></span><br><span class="line">            <span class="keyword">print</span> <span class="string">"success! times: %s"</span> % time</span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"fail! times: %s"</span> % time</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h4 id="di-gui-ban-ben">递归版本</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search_recursive</span><span class="params">(nums, item)</span>:</span></span><br><span class="line">    mid = len(nums) // <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> nums[mid] == item:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> mid == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># mid等于0就是找到最后一个元素了。</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> item &gt; nums[mid]:  <span class="comment"># 找后半部分</span></span><br><span class="line">            <span class="comment"># print(lst[mid:])</span></span><br><span class="line">            <span class="keyword">return</span> binary_search_recursive(nums[mid:], item)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> binary_search_recursive(nums[:mid], item)  <span class="comment"># 找前半部分</span></span><br></pre></td></tr></table></figure>
<h3 id="hash-cha-zhao">Hash 查找</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashTable</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size)</span>:</span></span><br><span class="line">        self.elem = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(size)]  <span class="comment"># 使用list数据结构作为哈希表元素保存方法</span></span><br><span class="line">        self.count = size  <span class="comment"># 最大表长</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hash</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> key % self.count  <span class="comment"># 散列函数采用除留余数法</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert_hash</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="string">"""插入关键字到哈希表内"""</span></span><br><span class="line">        address = self.hash(key)  <span class="comment"># 求散列地址</span></span><br><span class="line">        <span class="keyword">while</span> self.elem[address]:  <span class="comment"># 当前位置已经有数据了，发生冲突。</span></span><br><span class="line">            address = (address+<span class="number">1</span>) % self.count  <span class="comment"># 线性探测下一地址是否可用</span></span><br><span class="line">        self.elem[address] = key  <span class="comment"># 没有冲突则直接保存。</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search_hash</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="string">"""查找关键字，返回布尔值"""</span></span><br><span class="line">        star = address = self.hash(key)</span><br><span class="line">        <span class="keyword">while</span> self.elem[address] != key:</span><br><span class="line">            address = (address + <span class="number">1</span>) % self.count</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.elem[address] <span class="keyword">or</span> address == star:  <span class="comment"># 说明没找到或者循环到了开始的位置</span></span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">      </span><br><span class="line">     </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># hash查找</span></span><br><span class="line">    list_a = [<span class="number">12</span>, <span class="number">67</span>, <span class="number">56</span>, <span class="number">16</span>, <span class="number">25</span>, <span class="number">37</span>, <span class="number">22</span>, <span class="number">29</span>, <span class="number">15</span>, <span class="number">47</span>, <span class="number">48</span>, <span class="number">34</span>]</span><br><span class="line">    hash_table = HashTable(len(list_a))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> list_a:</span><br><span class="line">        hash_table.insert_hash(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> hash_table.elem:</span><br><span class="line">        <span class="keyword">if</span> i:</span><br><span class="line">            print(i, hash_table.elem.index(i))</span><br><span class="line"></span><br><span class="line">    print(hash_table.search_hash(<span class="number">15</span>))</span><br><span class="line">    print(hash_table.search_hash(<span class="number">33</span>))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>查找算法</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo-advanced-settings</title>
    <url>/2018/02/10/hexo/hexo-advanced-settings/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2018/02/10/hexo/hexo-advanced-settings/Moments.jpg" alt="These Magic Moments"></p>
<a id="more"></a>
<h1 id="shu-ju-tong-ji">数据统计</h1>
<h2 id="zhan-dian-fang-wen-liang-tong-ji">站点访问量统计</h2>
<p>该功能由 <a href="http://ibruce.info/2015/04/04/busuanzi/" target="_blank" rel="noopener">不蒜子</a> 提供，效果如下图：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511200426274.png" alt></p>
<p>左侧数据表示独立访客数 UV，右侧数据表示网站浏览量 PV，访客数和浏览量的区别在于一个用户连续点击 n 篇文章，会记录 n 次浏览量，但只记录一次访客数。</p>
<p>由于不蒜子是基于域名来进行统计计算的，所以通过 localhost:4000 端口访问的时候统计数据 PV 和 UV 都会异常的大，属于正常现象。</p>
<p>在页脚布局模板文件首行添加如下代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout_partial\footer.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&lt;script <span class="keyword">async</span>=<span class="string">""</span> src=<span class="string">"//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"</span>&gt;</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br></pre></td></tr></table></figure>
<p>在主题配置文件中做出如下修改：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">busuanzi_count:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">total_visitors:</span> <span class="literal">true</span> <span class="comment"># 访客数</span></span><br><span class="line">  <span class="attr">total_visitors_icon:</span> <span class="string">user</span></span><br><span class="line">  <span class="attr">total_views:</span> <span class="literal">true</span> <span class="comment"># 访问量</span></span><br><span class="line">  <span class="attr">total_views_icon:</span> <span class="string">eye</span></span><br><span class="line">  <span class="attr">post_views:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">post_views_icon:</span> <span class="string">eye</span></span><br></pre></td></tr></table></figure>
<p>刷新浏览器即可生效。</p>
<p>高阶用法：通过修改代码来自定义统计文案，如果你想使用本站统计文案，需要对不蒜子的代码做出如下修改：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout_third-party\analytics\busuanzi-counter.swig</span></figcaption><table><tr><td class="code"><pre><span class="line"> &#123;% if theme.busuanzi_count.total_visitors %&#125;</span><br><span class="line"><span class="deletion">-   &lt;span class="site-uv" title="&#123;&#123; __('footer.total_visitors') &#125;&#125;"&gt;</span></span><br><span class="line"><span class="addition">+   &lt;span class="site-uv"&gt;</span></span><br><span class="line"><span class="addition">+     &#123;&#123; __('footer.total_visitors', '&lt;span class="busuanzi-value" id="busuanzi_value_site_uv"&gt;&lt;/span&gt;') &#125;&#125;</span></span><br><span class="line"><span class="deletion">-     &lt;i class="fa fa-&#123;&#123; theme.busuanzi_count.total_visitors_icon &#125;&#125;"&gt;&lt;/i&gt;</span></span><br><span class="line"><span class="deletion">-     &lt;span class="busuanzi-value" id="busuanzi_value_site_uv"&gt;&lt;/span&gt;</span></span><br><span class="line">    &lt;/span&gt;</span><br><span class="line">  &#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">  &#123;% if theme.busuanzi_count.total_views %&#125;</span><br><span class="line"><span class="deletion">-   &lt;span class="site-pv" title="&#123;&#123; __('footer.total_views') &#125;&#125;"&gt;</span></span><br><span class="line"><span class="addition">+   &lt;span class="site-pv"&gt;</span></span><br><span class="line"><span class="addition">+     &#123;&#123; __('footer.total_views', '&lt;span class="busuanzi-value" id="busuanzi_value_site_pv"&gt;&lt;/span&gt;') &#125;&#125;</span></span><br><span class="line"><span class="deletion">-     &lt;i class="fa fa-&#123;&#123; theme.busuanzi_count.total_views_icon &#125;&#125;"&gt;&lt;/i&gt;</span></span><br><span class="line"><span class="deletion">-     &lt;span class="busuanzi-value" id="busuanzi_value_site_pv"&gt;&lt;/span&gt;</span></span><br><span class="line">    &lt;/span&gt;</span><br><span class="line">  &#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<p>在自定义样式文件中添加如下样式：</p>
<figure class="highlight css"><figcaption><span>themes/next/source/css/_custom/custom.styl</span></figcaption><table><tr><td class="code"><pre><span class="line">//修改不蒜子数据颜色</span><br><span class="line"><span class="selector-class">.busuanzi-value</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#1890ff</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后修改统计表述文案：</p>
<figure class="highlight yml"><figcaption><span>themes\next\languages\zh-CN.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">footer:</span></span><br><span class="line">  <span class="attr">total_views:</span> <span class="string">"历经 %s 次回眸才与你相遇"</span></span><br><span class="line">  <span class="attr">total_visitors:</span> <span class="string">"我的第 %s 位朋友，"</span></span><br></pre></td></tr></table></figure>
<h2 id="zhan-dian-yun-xing-shi-jian-tong-ji">站点运行时间统计</h2>
<p>在站点底部显示站点已运行时间，效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511201038905.png" alt></p>
<p>在主题自定义布局文件中添加以下代码：</p>
<figure class="highlight js"><figcaption><span>thems\next\layout_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;# 页脚站点运行时间统计 #&#125; &#123;% if theme.footer.ages.enable %&#125;</span><br><span class="line">&lt;script src=<span class="string">"https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">&lt;script src="https:/</span><span class="regexp">/cdn.jsdelivr.net/</span>npm/moment-precise-range-plugin@<span class="number">1.3</span><span class="number">.0</span>/moment-precise-range.min.js<span class="string">"&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="string">&lt;script&gt;</span></span><br><span class="line"><span class="string">  function timer() &#123;</span></span><br><span class="line"><span class="string">    var ages = moment.preciseDiff(moment(),moment(&#123;&#123; theme.footer.ages.birthday &#125;&#125;,"</span>YYYYMMDD<span class="string">"));</span></span><br><span class="line"><span class="string">    ages = ages.replace(/years?/, "</span>年<span class="string">");</span></span><br><span class="line"><span class="string">    ages = ages.replace(/months?/, "</span>月<span class="string">");</span></span><br><span class="line"><span class="string">    ages = ages.replace(/days?/, "</span>天<span class="string">");</span></span><br><span class="line"><span class="string">    ages = ages.replace(/hours?/, "</span>小时<span class="string">");</span></span><br><span class="line"><span class="string">    ages = ages.replace(/minutes?/, "</span>分<span class="string">");</span></span><br><span class="line"><span class="string">    ages = ages.replace(/seconds?/, "</span>秒<span class="string">");</span></span><br><span class="line"><span class="string">    ages = ages.replace(/\d+/g, '&lt;span style="</span>color:&#123;&#123; theme.footer.ages.color &#125;&#125;<span class="string">"&gt;$&amp;&lt;/span&gt;');</span></span><br><span class="line"><span class="string">    div.innerHTML = `&#123;&#123; __('footer.age')&#125;&#125; $&#123;ages&#125;`;</span></span><br><span class="line"><span class="string">  &#125;</span></span><br><span class="line"><span class="string">  var div = document.createElement("</span>div<span class="string">");</span></span><br><span class="line"><span class="string">  //插入到copyright之后</span></span><br><span class="line"><span class="string">  var copyright = document.querySelector("</span>.copyright<span class="string">");</span></span><br><span class="line"><span class="string">  document.querySelector("</span>.footer-inner<span class="string">").insertBefore(div, copyright.nextSibling);</span></span><br><span class="line"><span class="string">  timer();</span></span><br><span class="line"><span class="string">  setInterval("</span>timer()<span class="string">",1000)</span></span><br><span class="line"><span class="string">&lt;/script&gt;</span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure>
<p>如果 custom.swig 文件不存在，需要手动新建并在布局页面中 body 末尾引入：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout_layout.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">      ...</span><br><span class="line">      &#123;% include '_third-party/exturl.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/bookmark.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/copy-code.swig' %&#125;</span><br><span class="line"></span><br><span class="line"><span class="addition">+     &#123;% include '_custom/custom.swig' %&#125;</span></span><br><span class="line">    &lt;/body&gt;</span><br><span class="line">  &lt;/html&gt;</span><br></pre></td></tr></table></figure>
<p>修改主题配置文件：</p>
<figure class="highlight diff"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">  footer:</span><br><span class="line">    ...</span><br><span class="line"><span class="addition">+   ages:</span></span><br><span class="line"><span class="addition">+     # site running time</span></span><br><span class="line"><span class="addition">+     enable: true</span></span><br><span class="line"><span class="addition">+     # birthday of your site</span></span><br><span class="line"><span class="addition">+     birthday: 20181001</span></span><br><span class="line"><span class="addition">+     # color of number</span></span><br><span class="line"><span class="addition">+     color: "#1890ff"</span></span><br></pre></td></tr></table></figure>
<p>然后补全对应文案：</p>
<figure class="highlight diff"><figcaption><span>themes\next\languages\zh-CN.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">  footer:</span><br><span class="line">    powered: "由 %s 强力驱动"</span><br><span class="line">    theme: 主题</span><br><span class="line"><span class="addition">+   age: 我已在此等候你</span></span><br><span class="line">    total_views: "历经 %s 次回眸才与你相遇"</span><br><span class="line">    total_visitors: "我的第 %s 位朋友，"</span><br></pre></td></tr></table></figure>
<p>刷新浏览器即可生效。</p>
<h2 id="wen-zhang-fang-wen-liang-tong-ji">文章访问量统计</h2>
<p>该功能基于 <a href="https://leancloud.cn/" target="_blank" rel="noopener">LeanCloud</a> 提供后端数据服务，效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511201401441.png" alt></p>
<p>在 LeanCloud 上注册账号并创建应用，新建一个名为 Counter 的 Class，ACL 权限设置为 <strong>无限制</strong>：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511201501222.png" alt></p>
<p>在 LeanCloud 中的 Class 可以理解为数据库中的数据表。Counter 用于存储记录文章访问量，记录是以 url 作为唯一依据的，所以根据默认的 permalink 组成结构，如果你更改了文章的发布日期和标题中的任意一个，都会造成文章阅读数值的清零重计。</p>
<p>在控制台的 <strong>设置</strong> -&gt; <strong>应用 Key</strong> 中获取 App ID 和 App Key 填入到主题配置文件中：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">leancloud_visitors:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">app_id:</span> <span class="string">***&lt;app_id***</span></span><br><span class="line">  <span class="attr">app_key:</span> <span class="string">***&lt;app_key&gt;***</span></span><br><span class="line">  <span class="attr">security:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">betterPerformance:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p>站点上线后可以在 <strong>设置</strong> -&gt; <strong>安全中心</strong> 中添加博客域名到 Web 安全域名中，设置后仅可在该域名下通过 JavaScript SDK 调用服务器资源，借以保护 LeanCloud 应用的数据安全。</p>
<p>如果想要自定义 PV 表述文案，可以修改文章布局模板中的相关代码：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout_macro\post.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">  &#123;# LeanCould PageView #&#125;</span><br><span class="line">    ...</span><br><span class="line">    &#123;% if theme.post_meta.item_text %&#125;</span><br><span class="line"><span class="deletion">-     &lt;span class="post-meta-item-text"&gt;&#123;&#123;__('post.views') + __('symbol.colon') &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="addition">+     &lt;span class="post-meta-item-text"&gt;&#123;&#123;__('post.views')&#125;&#125; &lt;/span&gt;</span></span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line">    &lt;span class="leancloud-visitors-count"&gt;&lt;/span&gt;</span><br><span class="line"><span class="addition">+   &lt;span&gt;℃&lt;/span&gt;</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>修改统计表述文案：</p>
<figure class="highlight yml"><figcaption><span>themes/next/languages/zh-CN.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">post:</span></span><br><span class="line">  <span class="attr">views:</span> <span class="string">阅读次数</span></span><br></pre></td></tr></table></figure>
<p>刷新浏览器即可生效。</p>
<p>如果遇到如下报错，可能是你配置了 <code>security: true</code> 但又没有做好安全策略配置。</p>
<blockquote>
<p>阅读次数： Counter not initialized! See more at console err msg.</p>
</blockquote>
<p>有以下两种解决方案：</p>
<ul>
<li>下载安装 <a href="https://github.com/theme-next/hexo-leancloud-counter-security" target="_blank" rel="noopener">hexo-leancloud-counter-security</a> 插件</li>
<li>在主题配置中设置 <code>security: false</code></li>
</ul>
<p>个人推荐第二种，简单粗暴。</p>
<p>除了 LeanCloud，不蒜子也能提供文章阅读次数统计，但是不蒜子的统计结果只会在文章页显示，而不会显示在首页列表中，相关讨论可以参见 <a href="https://github.com/iissnan/hexo-theme-next/issues/801" target="_blank" rel="noopener">阅读计数。对比 LeanCloud 和不蒜子</a></p>
<h2 id="zhan-dian-ji-wen-zhang-zi-shu-tong-ji">站点及文章字数统计</h2>
<p>该功能由 <a href="https://github.com/theme-next/hexo-symbols-count-time" target="_blank" rel="noopener">hexo-symbols-count-time</a> 提供，效果如下图：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511201945103.png" alt></p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511202019299.png" alt></p>
<p>在根目录下执行如下命令安装相关依赖</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ npm <span class="keyword">install</span> hexo-symbols-<span class="keyword">count</span>-<span class="built_in">time</span> <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
<p>启用该功能需要同时修改站点配置文件和主题配置文件。</p>
<p>将如下配置项添加到<strong>站点配置文件</strong>中，这些配置项主要用于控制每项统计信息是否显示。</p>
<figure class="highlight yml"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">symbols_count_time:</span></span><br><span class="line">  <span class="attr">symbols:</span> <span class="literal">true</span> <span class="comment"># 统计单篇文章字数</span></span><br><span class="line">  <span class="attr">time:</span> <span class="literal">false</span> <span class="comment"># 取消估算单篇文章阅读时间</span></span><br><span class="line">  <span class="attr">total_symbols:</span> <span class="literal">true</span> <span class="comment"># 统计站点总字数</span></span><br><span class="line">  <span class="attr">total_time:</span> <span class="literal">false</span> <span class="comment"># 取消估算站点总阅读时间</span></span><br></pre></td></tr></table></figure>
<p>在<strong>主题配置文件</strong>中做如下修改，这些配置项主要用于控制统计信息的显示样式。</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">symbols_count_time:</span></span><br><span class="line">  <span class="attr">separated_meta:</span> <span class="literal">false</span> <span class="comment"># 统计信息不换行显示</span></span><br><span class="line">  <span class="attr">item_text_post:</span> <span class="literal">true</span> <span class="comment"># 文章统计信息中是否显示“本文字数/阅读时长”等描述文字</span></span><br><span class="line">  <span class="attr">item_text_total:</span> <span class="literal">true</span> <span class="comment"># 站点统计信息中是否显示“本文字数/阅读时长”等描述文字</span></span><br><span class="line">  <span class="attr">awl:</span> <span class="number">4</span> <span class="comment"># Average Word Length：平均字符长度</span></span><br><span class="line">  <span class="attr">wpm:</span> <span class="number">275</span> <span class="comment"># Words Per Minute：阅读速度</span></span><br></pre></td></tr></table></figure>
<p>汉字的平均字符长度为 1.5，如果在文章中使用纯中文进行写作（没有混杂英文），那么推荐设置 <code>awl: 2</code> 及 <code>wpm: 300</code>，但是如果文章中存在英文，建议设置 <code>awl: 4</code> 及 <code>wpm: 275</code>。</p>
<p>因为修改了站点配置文件，所以需要重新启动服务器才能生效。</p>
<h1 id="zhan-dian-ge-xing-hua-she-zhi">站点个性化设置</h1>
<h2 id="gao-guai-wang-ye-biao-ti">搞怪网页标题</h2>
<p>离开和进入页面时动态修改 Tab 标签中的标题。</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/title-trick.png" alt></p>
<p>在主题自定义布局文件中添加以下代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;# 搞怪网页标题 #&#125; &#123;% if theme.title_trick.enable %&#125;</span><br><span class="line">&lt;script&gt;</span><br><span class="line">  <span class="keyword">var</span> OriginTitile = <span class="built_in">document</span>.title;</span><br><span class="line">  <span class="keyword">var</span> titleTime;</span><br><span class="line">  <span class="built_in">document</span>.addEventListener(<span class="string">"visibilitychange"</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">document</span>.hidden) &#123;</span><br><span class="line">      <span class="built_in">document</span>.title = <span class="string">"&#123;&#123; theme.title_trick.leave &#125;&#125;"</span> + OriginTitile;</span><br><span class="line">      clearTimeout(titleTime);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="built_in">document</span>.title = <span class="string">"&#123;&#123; theme.title_trick.enter &#125;&#125;"</span> + OriginTitile;</span><br><span class="line">      titleTime = setTimeout(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">        <span class="built_in">document</span>.title = OriginTitile;</span><br><span class="line">      &#125;, <span class="number">2000</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">&#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure>
<p>如果 custom.swig 文件不存在，需要手动新建并在布局页面中 body 末尾引入：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout_layout.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">      ...</span><br><span class="line">      &#123;% include '_third-party/exturl.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/bookmark.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/copy-code.swig' %&#125;</span><br><span class="line"></span><br><span class="line"><span class="addition">+     &#123;% include '_custom/custom.swig' %&#125;</span></span><br><span class="line">    &lt;/body&gt;</span><br><span class="line">  &lt;/html&gt;</span><br></pre></td></tr></table></figure>
<p>在主题配置文件中添加以下代码：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># a trick on website title</span></span><br><span class="line"><span class="attr">title_trick:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">leave:</span> <span class="string">"(つェ⊂)我藏好了哦~"</span></span><br><span class="line">  <span class="attr">enter:</span> <span class="string">"(*´∇｀*) 被你发现啦~"</span></span><br></pre></td></tr></table></figure>
<h2 id="zhan-nei-sou-suo">站内搜索</h2>
<p>该功能由 <a href="https://github.com/theme-next/hexo-generator-searchdb" target="_blank" rel="noopener">hexo-generator-searchdb</a> 提供，效果如下图：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511202645563.png" alt></p>
<p>在根目录下执行以下命令安装相关依赖：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ npm <span class="keyword">install</span> hexo-generator-searchdb <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
<p>在<strong>主题配置</strong>文件中修改相关字段：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">local_search:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">trigger:</span> <span class="string">auto</span> <span class="comment"># 每次输入改变都执行搜索</span></span><br><span class="line">  <span class="attr">top_n_per_article:</span> <span class="number">3</span> <span class="comment"># 每篇文章显示的搜索结果数量</span></span><br><span class="line">  <span class="attr">unescape:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p>在站点配置文件中添加以下字段：</p>
<figure class="highlight yml"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">search:</span></span><br><span class="line">  <span class="attr">path:</span> <span class="string">search.xml</span></span><br><span class="line">  <span class="attr">field:</span> <span class="string">post</span> <span class="comment"># 指定搜索范围，可选 post | page | all</span></span><br><span class="line">  <span class="attr">format:</span> <span class="string">html</span> <span class="comment"># 指定页面内容形式，可选 html | raw (Markdown) | excerpt | more</span></span><br><span class="line">  <span class="attr">limit:</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure>
<p>在自定义样式文件中添加如下样式规则来增加搜索弹窗的页边距：</p>
<figure class="highlight css"><figcaption><span>themes\next\source\css_custom\custom.styl</span></figcaption><table><tr><td class="code"><pre><span class="line">//增加搜索弹窗的页边距</span><br><span class="line"><span class="selector-class">.local-search-popup</span> <span class="selector-id">#local-search-result</span> &#123;</span><br><span class="line">  <span class="attribute">padding</span>: <span class="number">25px</span> <span class="number">40px</span></span><br><span class="line">  height: <span class="built_in">calc</span>(<span class="number">100%</span> - <span class="number">95px</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果你同时在站点内启用了 wobblewindow 边缘摆动效果，则有可能会出现背景蒙版叠加在弹窗之前的问题，这种层级叠加异常的问题主要是因为 wobblewindow 中修改了弹窗父元素的 <code>position</code> 定位和 <code>z-index</code> 优先级，目前只能通过修改 localsearch 源码来修复该 Bug：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout_third-party\search\localsearch.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">  $.ajax(&#123;</span><br><span class="line">    url: path,</span><br><span class="line">    dataType: isXml ? <span class="string">"xml"</span> : <span class="string">"json"</span>,</span><br><span class="line">    <span class="keyword">async</span>: <span class="literal">true</span>,</span><br><span class="line">    success: <span class="function"><span class="keyword">function</span>(<span class="params">res</span>) </span>&#123;</span><br><span class="line">      <span class="comment">// get the contents from search data</span></span><br><span class="line">      isfetched = <span class="literal">true</span>;</span><br><span class="line">-     $(<span class="string">'.popup'</span>).detach().appendTo(<span class="string">'.header-inner'</span>);</span><br><span class="line">+     $(<span class="string">'.popup'</span>).detach().appendTo(<span class="string">'body'</span>);</span><br><span class="line">      <span class="keyword">var</span> datas = isXml ? $(<span class="string">"entry"</span>, res).map(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">          title: $(<span class="string">"title"</span>, <span class="keyword">this</span>).text(),</span><br><span class="line">          content: $(<span class="string">"content"</span>,<span class="keyword">this</span>).text(),</span><br><span class="line">          url: $(<span class="string">"url"</span> , <span class="keyword">this</span>).text()</span><br><span class="line">        &#125;;</span><br><span class="line">      &#125;).get() : res;</span><br><span class="line">      ...</span><br></pre></td></tr></table></figure>
<h2 id="re-men-wen-zhang-pai-xing-bang">热门文章排行榜</h2>
<p>本章节部分思路参考 <a href="https://thief.one/2017/03/03/Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">nMask | Hexo 搭建博客教程 #7.16</a>，自行进行了代码重构。</p>
<p>该功能同样是基于 LeanCloud 提供的后端服务支持。具体实现方案如下：</p>
<p>在站点目录下执行以下命令新建页面</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">$ hexo new<span class="built_in"> page </span>top</span><br></pre></td></tr></table></figure>
<p>在主题配置文件中新增一项菜单入口：</p>
<figure class="highlight diff"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">  menu:</span><br><span class="line">    home: / || home</span><br><span class="line"><span class="addition">+   top: /top/ || signal</span></span><br><span class="line">    tags: /tags/ || tags</span><br><span class="line">    categories: /categories/ || th</span><br><span class="line">    archives: /archives/ || archive</span><br><span class="line">    about: /about/ || user</span><br></pre></td></tr></table></figure>
<p>在语言包中新增菜单中文：</p>
<figure class="highlight diff"><figcaption><span>themes\next\languages\zh-CN.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">  menu:</span><br><span class="line">    home: 首页</span><br><span class="line">    archives: 归档</span><br><span class="line">    categories: 分类</span><br><span class="line">    tags: 标签</span><br><span class="line">    about: 关于</span><br><span class="line"><span class="addition">+   top: 排行榜</span></span><br></pre></td></tr></table></figure>
<p>然后在新增的排行榜页面内添加以下内容：</p>
<figure class="highlight js"><figcaption><span>source\top\index.md</span></figcaption><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: 热门文章Top <span class="number">10</span></span><br><span class="line">comments: <span class="literal">false</span></span><br><span class="line">date: <span class="number">2019</span><span class="number">-10</span><span class="number">-30</span> <span class="number">00</span>:<span class="number">54</span>:<span class="number">50</span></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">&lt;div id=<span class="string">"post-rank"</span>&gt;&lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">&lt;script src="/</span><span class="regexp">/cdn.jsdelivr.net/</span>npm/leancloud-storage@<span class="number">3.10</span><span class="number">.0</span>/dist/av-min.js<span class="string">"&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="string">&lt;script&gt;</span></span><br><span class="line"><span class="string">  var APP_ID = ******;  //输入个人LeanCloud账号AppID</span></span><br><span class="line"><span class="string">  var APP_KEY = ******;  //输入个人LeanCloud账号AppKey</span></span><br><span class="line"><span class="string">  AV.init(&#123;</span></span><br><span class="line"><span class="string">    appId: APP_ID,</span></span><br><span class="line"><span class="string">    appKey: APP_KEY</span></span><br><span class="line"><span class="string">  &#125;);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  var query = new AV.Query('Counter');//表名</span></span><br><span class="line"><span class="string">  query.descending('time'); //结果按阅读次数降序排序</span></span><br><span class="line"><span class="string">  query.limit(10);  //最终只返回10条结果</span></span><br><span class="line"><span class="string">  query.find().then( response =&gt; &#123;</span></span><br><span class="line"><span class="string">    var content = response.reduce( (accum, &#123;attributes&#125;) =&gt; &#123;</span></span><br><span class="line"><span class="string">      accum += `&lt;p&gt;&lt;div class="</span>prefix<span class="string">"&gt;热度 $&#123;attributes.time&#125; ℃&lt;/div&gt;&lt;div&gt;&lt;a href="</span>$&#123;attributes.url&#125;<span class="string">"&gt;$&#123;attributes.title&#125;&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;`</span></span><br><span class="line"><span class="string">      return accum;</span></span><br><span class="line"><span class="string">    &#125;,"</span><span class="string">")</span></span><br><span class="line">    document.querySelector("#post-rank").innerHTML = content;</span><br><span class="line">  &#125;)</span><br><span class="line">  .catch( <span class="function"><span class="params">error</span> =&gt;</span> &#123;</span><br><span class="line">    <span class="built_in">console</span>.log(error);</span><br><span class="line">  &#125;);</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">&lt;style type="text/</span>css<span class="string">"&gt;</span></span><br><span class="line"><span class="string">  #post-rank &#123;</span></span><br><span class="line"><span class="string">    text-align: center;</span></span><br><span class="line"><span class="string">  &#125;</span></span><br><span class="line"><span class="string">  #post-rank .prefix &#123;</span></span><br><span class="line"><span class="string">    color: #ff4d4f;</span></span><br><span class="line"><span class="string">  &#125;</span></span><br><span class="line"><span class="string">&lt;/style&gt;</span></span><br></pre></td></tr></table></figure>
<div class="note danger">
            <p>该功能控制台有bug，但是不影响使用</p>
          </div>
<h2 id="dou-ban-yue-du-dian-ying-you-xi">豆瓣阅读 / 电影 / 游戏</h2>
<p>本章节参考 <a href="https://github.com/mythsman/hexo-douban" target="_blank" rel="noopener">mythsman/hexo-douban README.md</a>。为站点添加豆瓣阅读 / 电影 / 游戏页面，效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511203614743.png" alt></p>
<p>在根目录下执行以下命令安装相关依赖：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ npm <span class="keyword">install</span> hexo-douban <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
<p>在站点配置文件中添加以下内容：</p>
<figure class="highlight yml"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">douban:</span></span><br><span class="line">  <span class="attr">user:</span> <span class="comment"># 个人豆瓣ID</span></span><br><span class="line">  <span class="attr">builtin:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">book:</span></span><br><span class="line">    <span class="attr">title:</span> <span class="string">"This is my book title"</span></span><br><span class="line">    <span class="attr">quote:</span> <span class="string">"This is my book quote"</span></span><br><span class="line">  <span class="attr">movie:</span></span><br><span class="line">    <span class="attr">title:</span> <span class="string">"This is my movie title"</span></span><br><span class="line">    <span class="attr">quote:</span> <span class="string">"This is my movie quote"</span></span><br><span class="line">  <span class="attr">game:</span></span><br><span class="line">    <span class="attr">title:</span> <span class="string">"This is my game title"</span></span><br><span class="line">    <span class="attr">quote:</span> <span class="string">"This is my game quote"</span></span><br><span class="line">  <span class="attr">timeout:</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure>
<ul>
<li>user: 填写豆瓣 ID。登陆豆瓣后点击<strong>个人主页</strong>，此时 url 中最后一段即是用户 ID，一般情况下会是一段数字，如果设置了个人域名的话，则个人域名即为 ID。</li>
<li>builtin: 是否将生成页面的功能嵌入 <code>hexo s</code> 和 <code>hexo g</code> 中。</li>
<li>timeout: 爬取数据的超时时间。</li>
</ul>
<p>如果只想生成某一个页面（比如只生成读书页面），把其他的配置项注释掉即可。</p>
<p>在主题配置文件中新增菜单入口：</p>
<figure class="highlight diff"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">  menu:</span><br><span class="line">    home: / || home</span><br><span class="line">    tags: /tags/ || tags</span><br><span class="line">    categories: /categories/ || th</span><br><span class="line">    archives: /archives/ || tasks</span><br><span class="line"><span class="addition">+   books: /books/ || book</span></span><br><span class="line"><span class="addition">+   movies: /movies/ || video-camera</span></span><br><span class="line"><span class="addition">+   games: /games/ || gamepad</span></span><br></pre></td></tr></table></figure>
<p>在语言包中新增菜单中文：</p>
<figure class="highlight diff"><figcaption><span>themes\next\language\zh_CN.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">  menu:</span><br><span class="line">    home: 首页</span><br><span class="line">    archives: 归档</span><br><span class="line">    categories: 分类</span><br><span class="line">    tags: 标签</span><br><span class="line"><span class="addition">+   movies: 电影</span></span><br><span class="line"><span class="addition">+   books: 读书</span></span><br><span class="line"><span class="addition">+   games: 游戏</span></span><br></pre></td></tr></table></figure>
<p>然后在根目录下执行以下命令生成豆瓣阅读 / 电影 / 游戏页面：</p>
<p>可选参数:</p>
<ul>
<li>-b | --books: 只生成豆瓣读书页面</li>
<li>-m | --movies: 只生成豆瓣电影页面</li>
<li>-g | --games: 只生成豆瓣游戏页面</li>
</ul>
<p>执行命令后，插件会根据用户提供的 ID 爬取豆瓣中的数据信息并在 <code>public</code> 目录下生成对应的页面，当服务器启动或部署后会将页面显示在对应的菜单路由下。</p>
<p>如果在站点配置中设置了 <code>douban.builtin: false</code>，则每次豆瓣数据变动后需要手动执行一次 <code>hexo douban</code> 来刷新页面数据。如果设置了 <code>douban.builtin: true</code>，则每次执行 <code>hexo s</code> 和 <code>hexo g</code> 的时候将会自动同时执行 <code>hexo douban</code> 命令，但这样可能会增加打包编译的时间。建议如果豆瓣数据变动不频繁的情况下该项设为 <code>false</code> 即可。</p>
<p>通常大家都喜欢用 <code>hexo d</code> 来作为 <code>hexo deploy</code> 命令的简化，但是当安装了 <code>hexo douban</code> 之后， <code>hexo d</code> 就会有歧义而无法执行，因为 <code>hexo douban</code> 跟 <code>hexo deploy</code> 的 Alias 都是 <code>hexo d</code>。</p>
<h2 id="zai-xian-liao-tian">在线聊天</h2>
<p>在线聊天算是一个比较成熟的 SaaS 商业应用了，业内产品如 <a href="https://www.tidiochat.com/" target="_blank" rel="noopener">Tidio</a>、 <a href="https://talkjs.com/" target="_blank" rel="noopener">TalkJS</a>、<a href="https://www.intercom.com/" target="_blank" rel="noopener">Intercom</a>、<a href="https://www.tawk.to/" target="_blank" rel="noopener">tawk.to</a> 等，使用体验都很好，交互界面也很干净别致。经过比较，本站最终选择了 Tidio：</p>
<ul>
<li>
<p>在个人博客这种业务场景中，几乎用不到它的收费功能，可以算是终身免费了。</p>
</li>
<li>
<p>Tidio 提供了多种消息回复渠道，包括网页、桌面应用、iOS/Android APP（需要 Google play 服务支持）。</p>
</li>
<li>
<p>除了在线聊天，Tidio 还可以在线发送邮件，以及关联接收 Fackbook 消息。</p>
</li>
<li>
<p>在几款产品的界面风格中，还是 Tidio 看起来更加优雅一些，深得我爱。</p>
</li>
</ul>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511204929049.png" alt></p>
<p>首先需要<a href="https://www.tidiochat.com/panel/login" target="_blank" rel="noopener">注册 Tidio 账号</a>，根据引导填写应用信息。进入控制台后，在 <strong>SETTINGS</strong> -&gt; <strong>Developer</strong> -&gt; <strong>Project data</strong> 中获取到 Public Key：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511205128090.png" alt></p>
<p>在主题配置文件下添加以下代码并补全 Public Key：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Tidio online chat</span></span><br><span class="line"><span class="comment"># see: https://www.tidiochat.com</span></span><br><span class="line"><span class="attr">tidio:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">key:</span> <span class="comment"># Public_Key</span></span><br></pre></td></tr></table></figure>
<p>在主题自定义布局文件中添加以下代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;# Tidio 在线联系功能 #&#125; &#123;% if theme.tidio.enable %&#125;</span><br><span class="line">&lt;script <span class="keyword">async</span> src=<span class="string">"//code.tidio.co/&#123;&#123; theme.tidio.key &#125;&#125;.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">&#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure>
<p>为避免代码加载阻塞页面渲染，需要为脚本添加 <code>async</code> 属性使其异步加载。</p>
<p>刷新页面即可在右下角看到 Tidio 的会话标志了。接下来可以在 Tidio 控制台的 <strong>Channel</strong> -&gt; <strong>Live chat</strong> -&gt; <strong>Appearance</strong> 中根据提示定制聊天对话框的主题外观和语言包：</p>
<h2 id="xing-wei-jian-ce-yu-fan-kui">行为监测与反馈</h2>
<p><a href="https://www.hotjar.com/" target="_blank" rel="noopener">Hotjar</a> 是一款轻量级的监测分析工具，能够提供用户行为监测和用户反馈分析，相比 Google Analysis 而言，它没有复杂的监测指标与分析报表，更加的简单实用，并且为免费用户提供 2000pv/day 的数据采集服务，适用于小型网站或个人博客的监测分析。</p>
<p>Hotjar 主要提供 <strong>ANALYTICS</strong> 和 <strong>FEEDBACK</strong> 两大类服务。</p>
<p>ANALYTICS 主要用于用户交互行为的监测分析，属于客观分析，包括以下四项具体功能：</p>
<ul>
<li>Heatmaps: 通过热力图可视化用户的鼠标交互行为，帮助理解用户动机和需求。</li>
<li>Recording: 记录用户在站点的行为轨迹，了解应用的可用性以及用户遭遇的问题。</li>
<li>Funnels: 记录每个页面或者步骤的用户流失率。</li>
<li>Forms: 记录表单中每一项输入的完成率，完成时间以及用户流失率。</li>
</ul>
<p>FEEDBACK 主要为用户提供反馈渠道，收集用户观点与数据，属于主观分析，包括以下四项具体功能：</p>
<ul>
<li>Incoming: 即时反馈，了解用户对页面的评价。</li>
<li>Polls: 投票反馈，获取某个问题的用户答案。</li>
<li>Surveys: 问卷调查，以问卷形式获取用户反馈。</li>
<li>Recruiters: 获取用户信息，招募用户用于用户调查或测试反馈。</li>
</ul>
<p>Hotjar 通过以上八项具体而实用的功能为用户提供主客观相结合的监测分析服务，可以说它是所有轻量级分析工具中唯一做到了主客观相结合的，同时也是所有主客观分析工具中，做的最轻量的。</p>
<p>读者可以通过该渠道评价页面或者提交勘误，点击悬挂在屏幕右侧的 Feedback 按钮弹出对话框，点击人物头像评价后将会跳转到如下界面</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/hotjar-feedback.png" alt></p>
<p>你可以在此页面输入反馈内容，并通过点击左下角的按钮在当前页面上标识目标元素，之后 hotjar 会将反馈内容连同带有高亮标识的页面截图一起提交到后台。</p>
<p>在站点中集成 Hotjar 的各项功能，需要先 <a href="https://insights.hotjar.com/register" target="_blank" rel="noopener">注册 Hotjar 账号</a>，根据指引一步步填写站点信息，然后在控制面板首页中获取 site ID，在主题配置文件下添加以下代码并补全 site ID：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Hotjar</span></span><br><span class="line"><span class="comment"># see: https://www.hotjar.com/</span></span><br><span class="line"><span class="attr">hotjar:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">siteID:</span> <span class="comment"># site ID</span></span><br></pre></td></tr></table></figure>
<p>在主题自定义布局文件中添加以下代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;# hotjar 页面反馈 #&#125; &#123;% if theme.hotjar.enable %&#125;</span><br><span class="line">&lt;script&gt;</span><br><span class="line">  (<span class="function"><span class="keyword">function</span>(<span class="params">h,o,t,j,a,r</span>)</span>&#123;</span><br><span class="line">    h.hj=h.hj||<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;(h.hj.q=h.hj.q||[]).push(<span class="built_in">arguments</span>)&#125;;</span><br><span class="line">    h._hjSettings=&#123;<span class="attr">hjid</span>:&#123;&#123; theme.hotjar.siteID &#125;&#125;,<span class="attr">hjsv</span>:<span class="number">6</span>&#125;;</span><br><span class="line">    a=o.getElementsByTagName(<span class="string">'head'</span>)[<span class="number">0</span>];</span><br><span class="line">    r=o.createElement(<span class="string">'script'</span>);r.async=<span class="number">1</span>;</span><br><span class="line">    r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;</span><br><span class="line">    a.appendChild(r);</span><br><span class="line">  &#125;)(<span class="built_in">window</span>,<span class="built_in">document</span>,<span class="string">'https://static.hotjar.com/c/hotjar-'</span>,<span class="string">'.js?sv='</span>);</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">&#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure>
<p>如此即可将 Hotjar 嵌入到站内，接下来在 Hotjar 控制台菜单中点击 Incoming，然后根据引导一步步配置即时反馈服务即可。</p>
<h1 id="wen-zhang-ye-mian-ge-xing-hua-she-zhi">文章页面个性化设置</h1>
<h2 id="tian-jia-ping-lun-gong-neng">添加评论功能</h2>
<p>Next 支持多款评论系统：</p>
<ul>
<li><a href="https://disqus.com/" target="_blank" rel="noopener">Disqus</a>：欧美 UI 风格，支持 Tweet、Facebook 等国外社交软件的三方登陆和一键分享。 <a href="https://blog.disqus.com/disqus-welcomes-the-spruce" target="_blank" rel="noopener">Demo</a></li>
<li><a href="https://github.com/imsun/gitment" target="_blank" rel="noopener">gitment</a>：必须用 github 账号登陆才能评论，支持 Markdown 语法，与 github issues 页面风格一致 <a href="https://imsun.github.io/gitment/" target="_blank" rel="noopener">Demo</a></li>
<li><a href="https://valine.js.org/" target="_blank" rel="noopener">Valine</a>：支持匿名评论，支持 Markdown 语法，界面简洁美观</li>
<li><a href="http://changyan.kuaizhan.com/" target="_blank" rel="noopener">畅言</a>：国产评论系统，可区分热评和最新评论，论坛贴吧风</li>
<li><a href="https://www.livere.com/" target="_blank" rel="noopener">来必力</a>：支持插入图片和 GIF，支持国内外多种社交媒体的三方登陆 <a href="https://www.livere.com/city-demo" target="_blank" rel="noopener">Demo</a></li>
</ul>
<p>博客的评论系统不需要太过复杂的功能，我的要求是一定要轻量级，足够简洁美观，并且支持 Markdown 语法，因此我首选 Valine 和 gitment，这两个评论系统都是由国内个人开发的，在此向开发者致敬。</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/valine-comments.png" alt></p>
<p>Next 已经内置了 Valine 组件，在主题配置文件中开启评论功能即可，同时，由于 Valine 是基于 Leancloud 提供后端服务的，所以需要填写 LeanCloud 的 App ID 和 App Key。</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">valine:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">appid:</span>  <span class="string">***&lt;app_id***</span></span><br><span class="line">  <span class="attr">appkey:</span> <span class="string">***&lt;app_key&gt;***</span></span><br><span class="line">  <span class="attr">notify:</span> <span class="literal">false</span>  <span class="comment"># 收到新评论是否邮件通知</span></span><br><span class="line">  <span class="attr">verify:</span> <span class="literal">false</span>  <span class="comment"># 是否开启验证码</span></span><br><span class="line">  <span class="attr">placeholder:</span>  <span class="comment"># 默认填充文字</span></span><br><span class="line">  <span class="attr">avatar:</span> <span class="string">mm</span>  <span class="comment"># 设置默认评论列表</span></span><br><span class="line">  <span class="attr">guest_info:</span> <span class="string">nick,mail</span>  <span class="comment"># 评论区头部表单</span></span><br><span class="line">  <span class="attr">pageSize:</span> <span class="number">10</span>  <span class="comment"># 每页评论数</span></span><br><span class="line">  <span class="attr">visitor:</span> <span class="literal">true</span>  <span class="comment"># 同时开启文章阅读次数统计</span></span><br></pre></td></tr></table></figure>
<p>Valine 也附带了阅读统计功能，可以在 Valine 配置项中设置 <code>visitor: true</code> 开启该功能。为避免后端服务冲突，建议不要同时启用 Valine 的阅读统计功能和 <code>leancloud_visitors</code>。Next 暂时不支持通过配置的方式隐藏文章标题下的评论数量，如要隐藏，可在自定义样式文件中添加如下代码：</p>
<figure class="highlight yml"><figcaption><span>themes/next/source/css/_custom/custom.styl</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="string">//屏蔽标题下的评论数量</span></span><br><span class="line"><span class="string">.post-comments-count</span> <span class="string">&#123;</span></span><br><span class="line">  <span class="attr">display:</span> <span class="string">none;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<p>如果你是轻度洁癖患者，想要隐藏评论区的浏览器和操作系统版本号以拥有更加干净的评论界面，可在自定义样式文件中添加如下代码：</p>
<figure class="highlight yml"><figcaption><span>themes/next/source/css/_custom/custom.styl</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="string">//屏蔽评论组件的多余信息</span></span><br><span class="line"><span class="comment">#comments .vsys &#123;</span></span><br><span class="line">  <span class="attr">display:</span> <span class="string">none;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="wen-mo-ban-quan-sheng-ming">文末版权声明</h2>
<p>在主题配置文件中开启文章底部的版权声明，版权声明默认使用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议，用户可以根据自身需要修改 <code>licence</code> 字段变更协议。</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">post_copyright:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">license:</span> <span class="string">&lt;a</span> <span class="string">href="https://creativecommons.org/licenses/by-nc-sa/4.0/"</span> <span class="string">rel="external</span> <span class="string">nofollow"</span> <span class="string">target="_blank"&gt;CC</span> <span class="string">BY-NC-SA</span> <span class="number">4.0</span><span class="string">&lt;/a&gt;</span></span><br></pre></td></tr></table></figure>
<p>默认版权声明中只有 <strong>本文作者</strong>、<strong>本文链接</strong>、<strong>版权声明</strong> 三项，如果你想添加更多内容，如 <strong>创建时间</strong>、<strong>修改时间</strong>、<strong>引用链接</strong> 等，需要修改版权声明的相关代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout_macro\post-copyright.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&lt;!-- JS库 clipboard 拷贝内容到粘贴板--&gt;</span><br><span class="line">&lt;script src=<span class="string">"https://cdn.bootcss.com/clipboard.js/2.0.1/clipboard.min.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">&lt;!-- JS库 sweetalert 显示提示信息--&gt;</span></span><br><span class="line"><span class="regexp">&lt;script src="https:/</span><span class="regexp">/unpkg.com/</span>sweetalert/dist/sweetalert.min.js<span class="string">"&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&lt;ul class="</span>post-copyright<span class="string">"&gt;</span></span><br><span class="line"><span class="string">  &lt;!-- 本文标题 --&gt;</span></span><br><span class="line"><span class="string">  &lt;li&gt;</span></span><br><span class="line"><span class="string">    &lt;strong&gt;&#123;&#123; __('post.copyright.title') + __('symbol.colon') &#125;&#125; &lt;/strong&gt;</span></span><br><span class="line"><span class="string">    &#123;&#123; post.title &#125;&#125;</span></span><br><span class="line"><span class="string">  &lt;/li&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &lt;!-- 本文作者 --&gt;</span></span><br><span class="line"><span class="string">  &lt;li class="</span>post-copyright-author<span class="string">"&gt;</span></span><br><span class="line"><span class="string">    &lt;strong&gt;&#123;&#123; __('post.copyright.author') + __('symbol.colon') &#125;&#125; &lt;/strong&gt;</span></span><br><span class="line"><span class="string">    &#123;&#123; post.author | default(author) &#125;&#125;</span></span><br><span class="line"><span class="string">  &lt;/li&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &lt;!-- 创建时间 --&gt;</span></span><br><span class="line"><span class="string">  &lt;li&gt;</span></span><br><span class="line"><span class="string">    &lt;strong&gt;&#123;&#123; __('post.created') + __('symbol.colon') &#125;&#125; &lt;/strong&gt;</span></span><br><span class="line"><span class="string">    &#123;&#123; post.date.format("</span>YYYY年MM月DD日 - HH时MM分<span class="string">") &#125;&#125;</span></span><br><span class="line"><span class="string">  &lt;/li&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &lt;!-- 修改时间 --&gt;</span></span><br><span class="line"><span class="string">  &lt;li&gt;</span></span><br><span class="line"><span class="string">    &lt;strong&gt;&#123;&#123; __('post.modified') + __('symbol.colon') &#125;&#125; &lt;/strong&gt;</span></span><br><span class="line"><span class="string">    &#123;&#123; post.updated.format("</span>YYYY年MM月DD日 - HH时MM分<span class="string">") &#125;&#125;</span></span><br><span class="line"><span class="string">  &lt;/li&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &lt;!-- 引用链接 --&gt;</span></span><br><span class="line"><span class="string">  &lt;li class="</span>post-copyright-link<span class="string">"&gt;</span></span><br><span class="line"><span class="string">    &lt;strong&gt;&#123;&#123; __('post.copyright.link') + __('symbol.colon') &#125;&#125;&lt;/strong&gt;</span></span><br><span class="line"><span class="string">    &lt;a href="</span>&#123;&#123; post.url | <span class="keyword">default</span>(post.permalink) &#125;&#125;<span class="string">" title="</span>&#123;&#123; post.title &#125;&#125;<span class="string">"</span></span><br><span class="line"><span class="string">      &gt;&#123;&#123; post.url | default(post.permalink) &#125;&#125;&lt;/a</span></span><br><span class="line"><span class="string">    &gt;</span></span><br><span class="line"><span class="string">    &lt;span class="</span>copy-path<span class="string">" title="</span>点击复制引用链接<span class="string">"</span></span><br><span class="line"><span class="string">      &gt;&lt;i</span></span><br><span class="line"><span class="string">        style="</span>cursor: pointer<span class="string">"</span></span><br><span class="line"><span class="string">        class="</span>fa fa-clipboard<span class="string">"</span></span><br><span class="line"><span class="string">        data-clipboard-text="</span>[&#123;&#123; post.author | <span class="keyword">default</span>(author) &#125;&#125;<span class="string">'s Blog | &#123;&#123; post.title &#125;&#125;](&#123;&#123; post.permalink &#125;&#125;)"</span></span><br><span class="line"><span class="string">        aria-label="&#123;&#123; __('</span>post.copy_success<span class="string">') &#125;&#125;"</span></span><br><span class="line"><span class="string">      &gt;&lt;/i</span></span><br><span class="line"><span class="string">    &gt;&lt;/span&gt;</span></span><br><span class="line"><span class="string">  &lt;/li&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &lt;!-- 版权声明 --&gt;</span></span><br><span class="line"><span class="string">  &lt;li class="post-copyright-license"&gt;</span></span><br><span class="line"><span class="string">    &lt;strong</span></span><br><span class="line"><span class="string">      &gt;&#123;&#123; __('</span>post.copyright.license_title<span class="string">') + __('</span>symbol.colon<span class="string">') &#125;&#125;</span></span><br><span class="line"><span class="string">    &lt;/strong&gt;</span></span><br><span class="line"><span class="string">    &#123;&#123; __('</span>post.copyright.license_content<span class="string">', theme.post_copyright.license) &#125;&#125;</span></span><br><span class="line"><span class="string">  &lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;/ul&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&lt;script&gt;</span></span><br><span class="line"><span class="string">  var clipboard = new ClipboardJS(".fa-clipboard");</span></span><br><span class="line"><span class="string">  clipboard.on("success", function(target) &#123;</span></span><br><span class="line"><span class="string">    var message = document.createElement("div");</span></span><br><span class="line"><span class="string">    message.innerHTML =</span></span><br><span class="line"><span class="string">      '</span>&lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-check-circle message-icon"</span>&gt;&lt;<span class="regexp">/i&gt;&lt;span class="message-content"&gt;' +</span></span><br><span class="line"><span class="regexp">      target.trigger.getAttribute("aria-label") +</span></span><br><span class="line"><span class="regexp">      "&lt;/</span>span&gt;<span class="string">";</span></span><br><span class="line"><span class="string">    swal(&#123;</span></span><br><span class="line"><span class="string">      content: message,</span></span><br><span class="line"><span class="string">      className: "</span>copy-success-message<span class="string">",</span></span><br><span class="line"><span class="string">      timer: 1000,</span></span><br><span class="line"><span class="string">      button: false</span></span><br><span class="line"><span class="string">    &#125;);</span></span><br><span class="line"><span class="string">  &#125;);</span></span><br><span class="line"><span class="string">&lt;/script&gt;</span></span><br></pre></td></tr></table></figure>
<p>在版权样式文件中添加如下样式：</p>
<figure class="highlight yml"><figcaption><span>themes\next\source\css_common\components\post\post-copyright.styl</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="string">.swal-overlay</span> <span class="string">&#123;</span></span><br><span class="line">  <span class="attr">background-color:</span> <span class="string">transparent;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">.copy-success-message</span> <span class="string">&#123;</span></span><br><span class="line">  <span class="attr">box-shadow:</span> <span class="string">0px</span> <span class="string">4px</span> <span class="string">12px</span> <span class="string">rgba(0,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0.15</span><span class="string">);</span></span><br><span class="line">  <span class="attr">border-radius:</span> <span class="string">4px;</span></span><br><span class="line">  <span class="attr">width:</span> <span class="string">auto;</span></span><br><span class="line">  <span class="attr">margin:</span> <span class="string">16x</span> <span class="string">0px;</span></span><br><span class="line">  <span class="attr">vertical-align:</span> <span class="string">top;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">.copy-success-message</span> <span class="string">.swal-content</span> <span class="string">&#123;</span></span><br><span class="line">  <span class="attr">margin:</span> <span class="string">0px</span> <span class="string">0px</span> <span class="type">!important</span><span class="string">;</span></span><br><span class="line">  <span class="attr">padding:</span> <span class="string">10px</span> <span class="string">16px;</span></span><br><span class="line">  <span class="attr">line-height:</span> <span class="string">1em;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">.copy-success-message</span> <span class="string">.message-icon</span> <span class="string">&#123;</span></span><br><span class="line">  <span class="attr">color:</span> <span class="comment">#52c41a;</span></span><br><span class="line">  <span class="attr">margin-right:</span> <span class="string">8px;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">.copy-success-message</span> <span class="string">.message-content</span> <span class="string">&#123;</span></span><br><span class="line">  <span class="attr">font-size:</span> <span class="string">14px;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<p>然后补全版权信息文案字段：</p>
<figure class="highlight yml"><figcaption><span>themes/next/languages/zh-CN.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">post:</span></span><br><span class="line">  <span class="attr">created:</span> <span class="string">创建时间</span></span><br><span class="line">  <span class="attr">modified:</span> <span class="string">修改时间</span></span><br><span class="line">  <span class="attr">copy_success:</span> <span class="string">复制成功</span></span><br><span class="line">  <span class="attr">copyright:</span></span><br><span class="line">    <span class="attr">title:</span> <span class="string">本文标题</span></span><br><span class="line">    <span class="attr">author:</span> <span class="string">本文作者</span></span><br><span class="line">    <span class="attr">link:</span> <span class="string">引用链接</span></span><br><span class="line">    <span class="attr">license_title:</span> <span class="string">版权声明</span></span><br><span class="line">    <span class="attr">license_content:</span> <span class="string">"本博客所有文章除特别声明外，均采用 %s 许可协议。转载请注明出处！"</span></span><br></pre></td></tr></table></figure>
<p>在实际使用过程中，并非每篇文章都需要版权声明，如果转载了别人的文章，文末再出现个人版权声明就不太合适。此时可在 Front-Matter 中设定变量 <code>copyright</code> 用于控制是否显示版权信息。修改文章布局模板中相关代码，使得只有当主题配置文件中 <code>post_copyright.enable</code> 字段和 <code>page.copyright</code> 字段同时为 <code>true</code> 时才会插入版权声明：</p>
<figure class="highlight diff"><figcaption><span>themes/next/layout/_macro/post.swig</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="deletion">- &#123;% if theme.post_copyright.enable and not is_index %&#125;</span></span><br><span class="line"><span class="addition">+ &#123;% if theme.post_copyright.enable and page.copyright and not is_index %&#125;</span></span><br><span class="line">    &lt;div&gt;</span><br><span class="line">      &#123;% include 'post-copyright.swig' with &#123; post: post &#125; %&#125;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">  &#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<p>为了批量为每篇新文章设定该变量并赋默认值，可以修改草稿模板内容，这样以来每篇草稿发布为正文后都会默认显示底部版权信息：</p>
<figure class="highlight diff"><figcaption><span>scaffolds\draft.md</span></figcaption><table><tr><td class="code"><pre><span class="line"> title: &#123;&#123; title &#125;&#125;</span><br><span class="line">  tags:</span><br><span class="line">  categories:</span><br><span class="line"><span class="addition">+ copyright: true</span></span><br></pre></td></tr></table></figure>
<h2 id="tian-jia-da-shang-gong-neng">添加打赏功能</h2>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511212128532.png" alt></p>
<p>启用主题配置文件中的打赏相关字段，并将个人收款码图片置于 themes\next\source\images\ 目录下，注意保持图片命名与配置文件中一致：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">reward_comment:</span></span><br><span class="line"><span class="attr">wechatpay:</span> <span class="string">/images/wechatpay.png</span></span><br><span class="line"><span class="attr">alipay:</span> <span class="string">/images/alipay.jpg</span></span><br></pre></td></tr></table></figure>
<p>如果要关闭悬停收款码上的文字抖动效果，可以在自定义样式文件中添加以下代码：</p>
<figure class="highlight yml"><figcaption><span>themes/next/source/css/_custom/custom.styl</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="string">//关闭打赏收款码的文字抖动效果</span></span><br><span class="line"><span class="comment">#QR &gt; div:hover p &#123;</span></span><br><span class="line">  <span class="attr">animation:</span> <span class="string">none;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<p>并非每个页面都需要开启打赏功能，可以在 Front-Matter 中添加 <code>reward</code> 字段来控制是否在本文章中添加打赏信息，然后修改文章布局模板中相关的判定条件：</p>
<figure class="highlight diff"><figcaption><span>themes/next/layout/_macro/post.swig</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="deletion">- &#123;% if (theme.alipay or theme.wechatpay or theme.bitcoin) and not is_index %&#125;</span></span><br><span class="line"><span class="addition">+ &#123;% if ( post.reward and (theme.alipay or theme.wechatpay or theme.bitcoin) and not is_index %&#125;</span></span><br><span class="line">    &lt;div&gt;</span><br><span class="line">      &#123;% include 'reward.swig' %&#125;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">  &#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<p>为了方便可在草稿模板 scaffolds\<a href="http://draft.md" target="_blank" rel="noopener">draft.md</a> 中统一添加 <code>reward</code> 字段默认值：</p>
<figure class="highlight diff"><figcaption><span>scaffolds\draft.md</span></figcaption><table><tr><td class="code"><pre><span class="line">  title: &#123;&#123; title &#125;&#125;</span><br><span class="line">  tags:</span><br><span class="line">  categories:</span><br><span class="line"><span class="addition">+ reward: true</span></span><br></pre></td></tr></table></figure>
<h2 id="tian-jia-tu-pian-deng-xiang">添加图片灯箱</h2>
<p>添加灯箱功能，实现点击图片后放大聚焦图片，并支持幻灯片播放、全屏播放、缩略图、快速分享到社交媒体等，该功能由 <a href="https://github.com/fancyapps/fancybox" target="_blank" rel="noopener">fancyBox</a> 提供，效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511212409792.png" alt></p>
<p>在根目录下执行以下命令安装相关依赖：</p>
<figure class="highlight crystal"><table><tr><td class="code"><pre><span class="line">$ git clone <span class="symbol">https:</span>/<span class="regexp">/github.com/theme</span>-<span class="keyword">next</span>/theme-<span class="keyword">next</span>-fancybox3 themes/<span class="keyword">next</span>/source/<span class="class"><span class="keyword">lib</span>/<span class="title">fancybox</span></span></span><br></pre></td></tr></table></figure>
<p>在主题配置文件中设置 <code>fancybox: true</code>：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">fancybox:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h2 id="xiang-guan-wen-zhang-tui-jian">相关文章推荐</h2>
<p>该功能由 <a href="https://github.com/tea3/hexo-related-popular-posts" target="_blank" rel="noopener">hexo-related-popular-posts</a> 插件提供，效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/related-post.png" alt></p>
<p>在站点根目录中执行以下命令安装依赖：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ npm <span class="keyword">install</span> hexo-related-popular-posts <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
<p>在主题配置文件中开启相关文章推荐功能：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">related_posts:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">title:</span> <span class="comment"># custom header, leave empty to use the default one</span></span><br><span class="line">  <span class="attr">display_in_home:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="attr">maxCount:</span> <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>此时会在每篇文章结尾根据标签相关性和内容相关性来推荐相关文章。</p>
<p>事实上并非每篇文章都需要开启该功能，可在文章 Front-Matter 中设置 <code>related_posts</code> 字段来控制是否在文末显示相关文章，然后修改文章布局模板中相关的判定条件：</p>
<figure class="highlight diff"><figcaption><span>themes/next/layout/_macro/post.swig</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="deletion">- &#123;% if theme.related_posts.enable and (theme.related_posts.display_in_home or not is_index) %&#125;</span></span><br><span class="line"><span class="addition">+ &#123;% if theme.related_posts.enable and (theme.related_posts.display_in_home or not is_index) and post.related_posts %&#125;</span></span><br><span class="line">    &#123;% include 'post-related.swig' with &#123; post: post &#125; %&#125;</span><br><span class="line">  &#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<p>为了方便可在草稿模板 scaffolds\<a href="http://draft.md" target="_blank" rel="noopener">draft.md</a> 中统一添加 <code>related_posts</code> 字段默认值：</p>
<figure class="highlight yml"><figcaption><span>scaffolds\draft.md</span></figcaption><table><tr><td class="code"><pre><span class="line">  <span class="attr">title:</span> <span class="string">&#123;&#123;</span> <span class="string">title</span> <span class="string">&#125;&#125;</span></span><br><span class="line">  <span class="attr">tags:</span></span><br><span class="line">  <span class="attr">categories:</span></span><br><span class="line"><span class="string">+</span> <span class="attr">related_posts:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h2 id="wen-zhang-ping-fen">文章评分</h2>
<p><a href="https://widgetpack.com/" target="_blank" rel="noopener">widgetpack</a> 是一款轻量级的插件，提供四项具体的功能：</p>
<ul>
<li>Comments: 评论系统，类似于留言板</li>
<li>Reviews: 评价系统，类似于商品评价</li>
<li>Rating: 星级评分系统</li>
<li>Google Reviews: 关联展示 Google Rating</li>
</ul>
<p>Next 主题中已经集成了 widgetpack 的星级评分系统，用户无须再安装或引入插件脚本，只需在 widgetpack 中注册账号并修改主题配置即可，应用效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/rating.png" alt></p>
<p>在 <a href="https://widgetpack.com/" target="_blank" rel="noopener">widgetpack</a> 中注册账号，根据引导填写应用名称和域名创建应用，创建后可在页面左上角看到应用 id。</p>
<p>在主题配置文件中开启评分功能，填写应用 id，并设置评分颜色：</p>
<figure class="highlight yml"><figcaption><span>themes\next_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Star rating support to each article.</span></span><br><span class="line"><span class="comment"># To get your ID visit https://widgetpack.com</span></span><br><span class="line"><span class="attr">rating:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">id:</span> <span class="comment">#&lt;app_id&gt;</span></span><br><span class="line">  <span class="attr">color:</span> <span class="string">fadb14</span></span><br></pre></td></tr></table></figure>
<p>此时刷新浏览器即可在文章末尾看到空的评分栏了。点击评分发现需要以社交账号登陆，而这些社交账号基本都是 facebook、twitter 等墙外的社交软件，限制了评分系统可用性，我们可以在 widgetpack 控制台中修改评分认证机制。</p>
<p>在控制台中点击左上角展开菜单，在 <strong>Rating</strong> -&gt; <strong>Setting</strong> 中将 Vote via 选项改为 Device(cookie) 以开启匿名评分，该选项将基于设备认证访问者身份：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/rate-vote-via.png" alt></p>
<p>用户还可以在该页面设定 star 数量和大小。修改后记得勾选右下角的 SAVE SETTING 才会生效。</p>
<p>在实际使用过程中，并非每篇文章都需要开启评分。此时可在 Front-Matter 中设定变量 rating 用于控制是否开启评分。修改文章布局模板中相关代码，使得只有当主题配置文件中 <code>rating.enable</code> 字段和 <code>page.rating</code> 字段同时为 <code>true</code> 才会插入评分组件：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout_macro\post.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">  &#123;% if not is_index %&#125;</span><br><span class="line"><span class="deletion">-  &#123;% if theme.rating.enable or (theme.vkontakte_api.enable and theme.vkontakte_api.like) or (theme.facebook_sdk.enable and theme.facebook_sdk.like_button) or (theme.needmoreshare2.enable and theme.needmoreshare2.postbottom.enable) or (theme.baidushare and theme.baidushare.type === "button" )%&#125;</span></span><br><span class="line"><span class="addition">+  &#123;% if (theme.rating.enable and post.rating) or (theme.vkontakte_api.enable and theme.vkontakte_api.like) or (theme.facebook_sdk.enable and theme.facebook_sdk.like_button) or (theme.needmoreshare2.enable and theme.needmoreshare2.postbottom.enable) or (theme.baidushare and theme.baidushare.type === "button" )%&#125;</span></span><br><span class="line">    &lt;div class="post-widgets"&gt;</span><br><span class="line">    &#123;% if theme.rating.enable %&#125;</span><br><span class="line">      &lt;div class="wp_rating"&gt;</span><br><span class="line">        &lt;div id="wpac-rating"&gt;&lt;/div&gt;</span><br><span class="line">      &lt;/div&gt;</span><br><span class="line">    &#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<p>为了批量为每篇新文章设定该变量并赋默认值，可以修改草稿模板内容，这样以来每篇草稿发布后都会默认开启评分：</p>
<figure class="highlight diff"><figcaption><span>scaffolds\draft.md</span></figcaption><table><tr><td class="code"><pre><span class="line">  title: &#123;&#123; title &#125;&#125;</span><br><span class="line">  tags:</span><br><span class="line">  categories:</span><br><span class="line"><span class="addition">+ rating: true</span></span><br></pre></td></tr></table></figure>
<p>站点上线后，可以在控制台菜单的 <strong>Site</strong> -&gt; <strong>Setting</strong> 中勾选 Private，使得组件只对应用内指定的域名上生效，这样以来即时别人错填了你的 id 也不会将评分数据误提交到你的应用中了。</p>
<p>widgetpack 与前文提到的 hotjar 在评价反馈功能上的侧重点不一样，widgetpack 更侧重于对文章的评分，而 hotjar 侧重于对整个页面的评分，并提供了文字和截图反馈的渠道。</p>
<h2 id="wen-zhang-jia-mi-fang-wen">文章加密访问</h2>
<p>该功能由 <a href="https://github.com/MikeCoder/hexo-blog-encrypt" target="_blank" rel="noopener">hexo-blog-encrypt</a> 插件提供，效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-advanced-settings/image-20200511213059680.png" alt></p>
<p>在站点根目录中执行以下命令安装依赖：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">npm <span class="keyword">install</span> hexo-blog-<span class="keyword">encrypt</span> <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
<p>在站点配置文件中添加如下字段：</p>
<figure class="highlight yml"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">encrypt:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">default_abstract:</span> <span class="string">此文章已被加密，需要输入密码访问。</span>  <span class="string">//首页文章列表中加密文章的默认描述文案</span></span><br><span class="line">  <span class="attr">default_message:</span> <span class="string">请输入密码以阅读这篇私密文章。</span>  <span class="string">//文章详情页的密码输入框上的默认描述文案</span></span><br></pre></td></tr></table></figure>
<p>然后在文章 Front-Matter 中添加 <code>password</code> 字段用于设置文章访问密码。重启服务器，这个时候可能需要经历较长一段时间的加密过程，请耐心等待，加密完成后刷新页面将会显示密码输入框，输入密码后才能继续访问文章内容。</p>
<p>该功能只会加密文章正文，其他内容如打赏、版权信息、标签等则不会被加密隐藏，这样看起来有点奇怪，所以建议加密文章隐藏掉打赏和版权信息内容。</p>
<figure class="highlight js"><figcaption><span>themes\next\layout_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&lt;script src=<span class="string">"https://unpkg.com/sweetalert/dist/sweetalert.min.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br></pre></td></tr></table></figure>
<p>在 node_modules 依赖库中修改 hexo-blog-encrypt 源码：</p>
<figure class="highlight js"><figcaption><span>node_modules\hexo-blog-encrypt\lib\blog.encrypt.js</span></figcaption><table><tr><td class="code"><pre><span class="line">  ...</span><br><span class="line">  &#125; <span class="keyword">catch</span> (e) &#123;</span><br><span class="line">-   alert(decryptionError);</span><br><span class="line">+   swal(&#123;</span><br><span class="line">+     text: <span class="string">"密码错误!"</span>,</span><br><span class="line">+     icon: <span class="string">"error"</span>,</span><br><span class="line">+     className: <span class="string">"password-error"</span>,</span><br><span class="line">+     timer: <span class="number">1000</span>,</span><br><span class="line">+     button: <span class="literal">false</span></span><br><span class="line">+   &#125;);</span><br><span class="line">    <span class="built_in">console</span>.log(e);</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<p>在自定义样式文件中添加如下代码：</p>
<figure class="highlight css"><figcaption><span>themes/next/source/css/_custom/custom.styl</span></figcaption><table><tr><td class="code"><pre><span class="line">//密码错误sweetalert弹框样式修改</span><br><span class="line"><span class="selector-class">.swal-overlay</span> &#123;</span><br><span class="line">  <span class="attribute">background-color</span>: transparent;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.password-error</span> &#123;</span><br><span class="line">  <span class="attribute">box-shadow</span>: <span class="number">0px</span> <span class="number">4px</span> <span class="number">12px</span> <span class="built_in">rgba</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.15</span>);</span><br><span class="line">  <span class="attribute">border-radius</span>: <span class="number">4px</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由于是在 node_module 中修改的依赖文件，一旦更新或者重装依赖都会覆盖修改，需要重新修改一遍。</p>
]]></content>
      <categories>
        <category>技术/博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo主题美化</title>
    <url>/2018/02/10/hexo/hexo-theme-beautify/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2018/02/10/hexo/hexo-theme-beautify/coast.jpg" alt></p>
<a id="more"></a>
<h1 id="xiu-gai-bo-ke-zi-ti">修改博客字体</h1>
<p>在 <a href="https://www.google.com/fonts" target="_blank" rel="noopener">Google Fonts</a> 上找到心仪的字体，然后在主题配置文件中为不同的应用场景配置字体：</p>
<figure class="highlight python"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">font:</span><br><span class="line">  enable: true</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 外链字体库地址，例如 //fonts.googleapis.com (默认值)</span></span><br><span class="line">  host:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 全局字体，应用在 body 元素上</span></span><br><span class="line">  <span class="keyword">global</span>:</span><br><span class="line">    external: true</span><br><span class="line">    family: Monda</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 标题字体 (h1, h2, h3, h4, h5, h6)</span></span><br><span class="line">  headings:</span><br><span class="line">    external: true</span><br><span class="line">    family: Roboto Slab</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 文章字体</span></span><br><span class="line">  posts:</span><br><span class="line">    external: true</span><br><span class="line">    family:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Logo 字体</span></span><br><span class="line">  logo:</span><br><span class="line">    external: true</span><br><span class="line">    family:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 代码字体，应用于 code 以及代码块</span></span><br><span class="line">  codes:</span><br><span class="line">    external: true</span><br><span class="line">    family:</span><br></pre></td></tr></table></figure>
<h1 id="wen-zhang-ye-mo-mei-hua">文章页末美化</h1>
<h2 id="wei-biao-qian-tian-jia-tu-biao">为标签添加图标</h2>
<p>默认情况下标签前缀是 <code>#</code> 字符，用户可以通过修改主题源码将标签的字符前缀改为图标前缀，更改后效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-theme-beautify/screenshot.png" alt></p>
<p>在文章布局模板中找到文末标签相关代码段，将 <code>#</code> 换成 <code>&lt;i class=&quot;fa fa-tags&quot;&gt;&lt;/i&gt;</code> 即可：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout\_macro\post.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&lt;footer class="post-footer"&gt;</span><br><span class="line">    &#123;% if post.tags and post.tags.length and not is_index %&#125;</span><br><span class="line">      &lt;div class="post-tags"&gt;</span><br><span class="line">        &#123;% for tag in post.tags %&#125;</span><br><span class="line"><span class="deletion">-          &lt;a href="&#123;&#123; url_for(tag.path) &#125;&#125;" rel="tag"&gt;# &#123;&#123; tag.name &#125;&#125;&lt;/a&gt;</span></span><br><span class="line"><span class="addition">+          &lt;a href="&#123;&#123; url_for(tag.path) &#125;&#125;" rel="tag"&gt;&lt;i class="fa fa-tags"&gt;&lt;/i&gt; &#123;&#123; tag.name &#125;&#125;&lt;/a&gt;</span></span><br><span class="line">        &#123;% endfor %&#125;</span><br><span class="line">      &lt;/div&gt;</span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line">    ...</span><br><span class="line">  &lt;/footer&gt;</span><br></pre></td></tr></table></figure>
<p>Next 中使用 <a href="https://fontawesome.com/v4.7.0/icons/" target="_blank" rel="noopener">FontAwesome</a> 作为图标库，用户可以在 FontAwesome 上找到心仪的图标来替换标签的字符前缀。</p>
<h2 id="tian-jia-jie-shu-biao-ji">添加结束标记</h2>
<p>在文末添加结束标记，效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-theme-beautify/20181113045252399.png" alt></p>
<p>修改主题配置文件：取消对 <code>postBodyEnd: source/_data/post-body-end.swig</code>的注释</p>
<figure class="highlight diff"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">custom_file_path:</span><br><span class="line">      #head: source/_data/head.swig</span><br><span class="line">      #header: source/_data/header.swig</span><br><span class="line">      #sidebar: source/_data/sidebar.swig</span><br><span class="line">      #postMeta: source/_data/post-meta.swig     </span><br><span class="line"><span class="addition">+     postBodyEnd: source/_data/post-body-end.swig</span></span><br><span class="line"><span class="deletion">-     #postBodyEnd: source/_data/post-body-end.swig</span></span><br><span class="line">      footer: source/_data/footer.swig</span><br><span class="line">      #bodyEnd: source/_data/body-end.swig</span><br><span class="line">      #variable: source/_data/variables.styl</span><br><span class="line">      #mixin: source/_data/mixins.styl</span><br><span class="line">      style: source/_data/styles.styl</span><br></pre></td></tr></table></figure>
<p>在<code>source/_data/post-body-end.swig</code>中添加如下代码：</p>
<figure class="highlight html"><figcaption><span>source/_data/post-body-end.swig</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">  &#123;% if not is_index %&#125;</span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">"text-align:center;color:#bfbfbf;font-size:16px;"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">span</span>&gt;</span>-------- 本文结束 <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"fa fa-coffee"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">span</span>&gt;</span> 感谢阅读 --------<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  &#123;% endif %&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h1 id="ye-mian-jia-zai-jin-du-tiao">页面加载进度条</h1>
<p>当网络不好的时候可能会在打开站点或跳转文章时出现短暂的白屏，此时如果能有加载进度提示将会提高用户操作体验。</p>
<p>在根目录下执行以下命令安装相关依赖：</p>
<figure class="highlight crystal"><table><tr><td class="code"><pre><span class="line">$ git clone <span class="symbol">https:</span>/<span class="regexp">/github.com/theme</span>-<span class="keyword">next</span>/theme-<span class="keyword">next</span>-pace themes/<span class="keyword">next</span>/source/<span class="class"><span class="keyword">lib</span>/<span class="title">pace</span></span></span><br></pre></td></tr></table></figure>
<p>在主题配置文件中设置 <code>pace: true</code>。</p>
<p>默认提供了多种主题的进度条加载样式，有顶部提示的，有中间提示的，还有全页面遮挡提示的，个人认为默认的进度条效果就恰如其当，既能够在页面空白的时候起到加载作用，也不会因为太过花里胡哨而喧宾夺主，尤其是当你如果使用了不蒜子的站点访问统计的功能的时候，常常会遇到所有资源都加载完毕而不蒜子还在等待响应，如果这个时候在页面较显眼的位置出现一个停滞不前的进度条，很让人抓狂。</p>
<h1 id="ce-bian-lan-fang-zuo-bian">侧边栏放左边</h1>
<p>Next 主题各系列中只有 Pisces 和 Gemini 支持通过主题配置文件来将侧边栏置于左侧或右侧，而 Muse 和 Mist 则需要深度修改源码才能实现改变侧边栏位置。</p>
<p>在自定义样式文件中添加如下规则：</p>
<figure class="highlight css"><figcaption><span>source/_data/styles.styl</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="selector-class">.sidebar-toggle</span> &#123;</span><br><span class="line">  <span class="attribute">left</span>: <span class="number">30px</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.sidebar</span> &#123;</span><br><span class="line">  <span class="attribute">left</span>: <span class="number">0px</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>若<code>source/_data/styles.styl</code>没有开启，需要在主题配置文件中开启</p>
<figure class="highlight diff"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">custom_file_path:</span><br><span class="line">      #head: source/_data/head.swig</span><br><span class="line">      #header: source/_data/header.swig</span><br><span class="line">      #sidebar: source/_data/sidebar.swig</span><br><span class="line">      #postMeta: source/_data/post-meta.swig     </span><br><span class="line">      postBodyEnd: source/_data/post-body-end.swig</span><br><span class="line">      #postBodyEnd: source/_data/post-body-end.swig</span><br><span class="line">      footer: source/_data/footer.swig</span><br><span class="line">      #bodyEnd: source/_data/body-end.swig</span><br><span class="line">      #variable: source/_data/variables.styl</span><br><span class="line">      #mixin: source/_data/mixins.styl</span><br><span class="line"><span class="addition">+      style: source/_data/styles.styl</span></span><br></pre></td></tr></table></figure>
<figure class="highlight js"><figcaption><span>themes\next\source\js\src\motion.js</span></figcaption><table><tr><td class="code"><pre><span class="line">$(<span class="built_in">document</span>)</span><br><span class="line">  .on(<span class="string">'sidebar.isShowing'</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    NexT.utils.isDesktop() &amp;&amp; $(<span class="string">'body'</span>).velocity(<span class="string">'stop'</span>).velocity(</span><br><span class="line">-     &#123;<span class="attr">paddingRight</span>: SIDEBAR_WIDTH&#125;,</span><br><span class="line">+     &#123;<span class="attr">paddingLeft</span>: SIDEBAR_WIDTH&#125;,</span><br><span class="line">      SIDEBAR_DISPLAY_DURATION</span><br><span class="line">    );</span><br><span class="line">  &#125;)</span><br><span class="line">  .on(<span class="string">'sidebar.isHiding'</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  &#125;);</span><br><span class="line">  ...</span><br><span class="line">  hideSidebar: <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">-   NexT.utils.isDesktop() &amp;&amp; $(<span class="string">'body'</span>).velocity(<span class="string">'stop'</span>).velocity(&#123;<span class="attr">paddingRight</span>: <span class="number">0</span>&#125;);</span><br><span class="line">+   NexT.utils.isDesktop() &amp;&amp; $(<span class="string">'body'</span>).velocity(<span class="string">'stop'</span>).velocity(&#123;<span class="attr">paddingLeft</span>: <span class="number">0</span>&#125;);</span><br><span class="line">    <span class="keyword">this</span>.sidebarEl.find(<span class="string">'.motion-element'</span>).velocity(<span class="string">'stop'</span>).css(<span class="string">'display'</span>, <span class="string">'none'</span>);</span><br><span class="line">    <span class="keyword">this</span>.sidebarEl.velocity(<span class="string">'stop'</span>).velocity(&#123;<span class="attr">width</span>: <span class="number">0</span>&#125;, &#123;<span class="attr">display</span>: <span class="string">'none'</span>&#125;);</span><br><span class="line"></span><br><span class="line">    sidebarToggleLines.init();</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如此以来就可以将侧边栏放置在左边了，但当窗口宽度缩小到 991px 之后会出现样式错误：侧边栏收缩消失但是页面左侧仍留有空白间距，此时修改如下代码即可：</p>
<figure class="highlight diff"><figcaption><span>themes\next\source\css\_common\scaffolding\base.styl</span></figcaption><table><tr><td class="code"><pre><span class="line">body &#123;</span><br><span class="line">  position: relative; // Required by scrollspy</span><br><span class="line">  font-family: $font-family-base;</span><br><span class="line">  font-size: $font-size-base;</span><br><span class="line">  line-height: $line-height-base;</span><br><span class="line">  color: $text-color;</span><br><span class="line">  background: $body-bg-color;</span><br><span class="line"></span><br><span class="line"><span class="deletion">- +mobile() &#123; padding-left: 0 !important; &#125;</span></span><br><span class="line"><span class="deletion">- +tablet() &#123; padding-left: 0 !important; &#125;  </span></span><br><span class="line"><span class="addition">+ +mobile() &#123; padding-right: 0 !important; &#125;</span></span><br><span class="line"><span class="addition">+ +tablet() &#123; padding-right: 0 !important; &#125;</span></span><br><span class="line">  +desktop-large() &#123; font-size: $font-size-large; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="tian-jia-dong-tai-bei-jing">添加动态背景</h1>
<p>Next 主题可以通过安装插件快速为站点添加不同效果的动态背景。</p>
<h2 id="li-zi-piao-fu-ju-he">粒子漂浮聚合</h2>
<p>应用效果如下图：</p>
<p><img src="/2018/02/10/hexo/hexo-theme-beautify/image-20200511162449080.png" alt></p>
<p>该功能由 <a href="https://github.com/theme-next/theme-next-canvas-nest" target="_blank" rel="noopener">theme-next-canvas-nest</a> 插件提供，在根目录下执行如下命令：</p>
<figure class="highlight crystal"><table><tr><td class="code"><pre><span class="line">$ git clone <span class="symbol">https:</span>/<span class="regexp">/github.com/theme</span>-<span class="keyword">next</span>/theme-<span class="keyword">next</span>-canvas-nest themes/<span class="keyword">next</span>/source/<span class="class"><span class="keyword">lib</span>/<span class="title">canvas</span>-<span class="title">nest</span></span></span><br></pre></td></tr></table></figure>
<p>然后在主题配置文件中设置 <code>canvas_nest: true</code> 即可。</p>
<p>Next v6.5.0 及以上版本支持更多的自定义选项：</p>
<figure class="highlight yml"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="attr">canvas_nest:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">onmobile:</span> <span class="literal">true</span> <span class="comment"># 是否在移动端显示</span></span><br><span class="line">  <span class="attr">color:</span> <span class="string">'0,0,255'</span> <span class="comment"># 动态背景中线条的 RGB 颜色</span></span><br><span class="line">  <span class="attr">opacity:</span> <span class="number">0.5</span> <span class="comment"># 动态背景中线条透明度</span></span><br><span class="line">  <span class="attr">zIndex:</span> <span class="number">-1</span> <span class="comment"># 动态背景的 z-index 属性值</span></span><br><span class="line">  <span class="attr">count:</span> <span class="number">99</span> <span class="comment"># 动态背景中线条数量</span></span><br></pre></td></tr></table></figure>
<h2 id="three-san-wei-dong-xiao">Three 三维动效</h2>
<p><a href="https://github.com/theme-next/theme-next-three" target="_blank" rel="noopener">theme-next-three</a> 插件提供了三个类型的背景动效，应用效果如下：</p>
<p><code><div class="tabs" id="三维动效"><ul class="nav-tabs"><li class="tab active"><a href="#三维动效-1">three-waves</a></li><li class="tab"><a href="#三维动效-2">canvas-lines</a></li><li class="tab"><a href="#三维动效-3">canvas-sphere</a></li></ul><div class="tab-content"><div class="tab-pane active" id="三维动效-1"><p><img src="/2018/02/10/hexo/hexo-theme-beautify/screenshot-9186344.png" alt></p></div><div class="tab-pane" id="三维动效-2"><p><img src="/2018/02/10/hexo/hexo-theme-beautify/image-20200511164017378.png" alt></p></div><div class="tab-pane" id="三维动效-3"><p><img src="/2018/02/10/hexo/hexo-theme-beautify/image-20200511164038461.png" alt></p></div></div></div></code></p>
<p>在根目录下执行如下命令安装相关依赖：</p>
<figure class="highlight crystal"><table><tr><td class="code"><pre><span class="line">$ git clone <span class="symbol">https:</span>/<span class="regexp">/github.com/theme</span>-<span class="keyword">next</span>/theme-<span class="keyword">next</span>-three themes/<span class="keyword">next</span>/source/<span class="class"><span class="keyword">lib</span>/<span class="title">three</span></span></span><br></pre></td></tr></table></figure>
<p>然后在主题配置文件中设置开启对应的动效选项即可。</p>
<figure class="highlight yml"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># JavaScript 3D library.</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/theme-next-three</span></span><br><span class="line"><span class="comment"># three_waves</span></span><br><span class="line"><span class="attr">three_waves:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># canvas_lines</span></span><br><span class="line"><span class="attr">canvas_lines:</span> <span class="literal">false</span></span><br><span class="line"><span class="comment"># canvas_sphere</span></span><br><span class="line"><span class="attr">canvas_sphere:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p>个人认为在站点中添加动态背景并没有实际的意义，只会凭空增加页面内存占用及 CPU 消耗，所以本站没有添加任何动态背景。</p>
<h2 id="sui-ji-san-jiao-si-dai">随机三角丝带</h2>
<p><img src="/2018/02/10/hexo/hexo-theme-beautify/evan-you.png" alt></p>
<p>点击下方按钮下载相应的脚本，并置<code>evan-you.js</code>于<code> themes\next\source\js\</code> 目录下：</p>
<p><code><a class="btn" href="js.zip">
            <i class="fa fa-download"></i>下载
          </a></code></p>
<p>在主题自定义布局文件中添加以下代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout\_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;# 随机三角丝带背景 #&#125;</span><br><span class="line">&#123;% <span class="keyword">if</span> theme.evanyou %&#125;</span><br><span class="line">  &lt;canvas id=<span class="string">"evanyou"</span>&gt;&lt;<span class="regexp">/canvas&gt;</span></span><br><span class="line"><span class="regexp">  &lt;style&gt;</span></span><br><span class="line"><span class="regexp">    #evanyou &#123;</span></span><br><span class="line"><span class="regexp">      position: fixed;</span></span><br><span class="line"><span class="regexp">      width: 100%;</span></span><br><span class="line"><span class="regexp">      height: 100%;</span></span><br><span class="line"><span class="regexp">      top: 0;</span></span><br><span class="line"><span class="regexp">      left: 0;</span></span><br><span class="line"><span class="regexp">      z-index: -1;</span></span><br><span class="line"><span class="regexp">    &#125;</span></span><br><span class="line"><span class="regexp">  &lt;/</span>style&gt;</span><br><span class="line">  &lt;script src=<span class="string">"/js/evan-you.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">&#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure>
<p>如果 custom.swig 文件不存在，需要手动新建并在布局页面中 body 末尾引入：</p>
<figure class="highlight html"><figcaption><span>themes\next\layout\_layout.swig</span></figcaption><table><tr><td class="code"><pre><span class="line"> ...</span><br><span class="line">      &#123;% include '_third-party/exturl.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/bookmark.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/copy-code.swig' %&#125;</span><br><span class="line"></span><br><span class="line">+     &#123;% include '_custom/custom.swig' %&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>在主题配置文件中添加以下代码：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># colorful trilateral riband background</span></span><br><span class="line"><span class="attr">evanyou:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h1 id="tian-jia-kan-ban-niang">添加看板娘</h1>
<p>该功能由 <a href="https://github.com/EYHN/hexo-helper-live2d" target="_blank" rel="noopener">hexo-helper-live2d</a> 插件支持，效果如下图：</p>
<p><img src="/2018/02/10/hexo/hexo-theme-beautify/image-20200511170001505.png" alt></p>
<p>在站点根目录下执行以下命令安装依赖：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ npm <span class="keyword">install</span> <span class="comment">--save hexo-helper-live2d</span></span><br></pre></td></tr></table></figure>
<p>在站点配置文件中添加以下下配置项</p>
<figure class="highlight yml"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Live2D</span></span><br><span class="line"><span class="comment"># https://github.com/EYHN/hexo-helper-live2d</span></span><br><span class="line"><span class="attr">live2d:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">pluginRootPath:</span> <span class="string">live2dw/</span></span><br><span class="line">  <span class="attr">pluginJsPath:</span> <span class="string">lib/</span></span><br><span class="line">  <span class="attr">pluginModelPath:</span> <span class="string">assets/</span> <span class="string">Relative)</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 脚本加载源</span></span><br><span class="line">  <span class="attr">scriptFrom:</span> <span class="string">local</span> <span class="comment"># 默认从本地加载脚本</span></span><br><span class="line">  <span class="comment"># scriptFrom: jsdelivr # 从 jsdelivr CDN 加载脚本</span></span><br><span class="line">  <span class="comment"># scriptFrom: unpkg # 从 unpkg CDN 加载脚本</span></span><br><span class="line">  <span class="comment"># scriptFrom: https://cdn.jsdelivr.net/npm/live2d-widget@3.x/lib/L2Dwidget.min.js # 从自定义地址加载脚本</span></span><br><span class="line">  <span class="attr">tagMode:</span> <span class="literal">false</span> <span class="comment"># 只在有 &#123;&#123; live2d() &#125;&#125; 标签的页面上加载 / 在所有页面上加载</span></span><br><span class="line">  <span class="attr">log:</span> <span class="literal">false</span> <span class="comment"># 是否在控制台打印日志</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 选择看板娘模型</span></span><br><span class="line">  <span class="attr">model:</span></span><br><span class="line">    <span class="attr">use:</span> <span class="string">live2d-widget-model-shizuku</span>  <span class="comment"># npm package的名字</span></span><br><span class="line">    <span class="comment"># use: wanko # /live2d_models/ 目录下的模型文件夹名称</span></span><br><span class="line">    <span class="comment"># use: ./wives/wanko # 站点根目录下的模型文件夹名称</span></span><br><span class="line">    <span class="comment"># use: https://cdn.jsdelivr.net/npm/live2d-widget-model-wanko@1.0.5/assets/wanko.model.json # 自定义网络数据源</span></span><br><span class="line">  <span class="attr">display:</span></span><br><span class="line">    <span class="attr">position:</span> <span class="string">left</span> <span class="comment"># 显示在左边还是右边</span></span><br><span class="line">    <span class="attr">width:</span> <span class="number">100</span> <span class="comment"># 宽度</span></span><br><span class="line">    <span class="attr">height:</span> <span class="number">180</span> <span class="comment"># 高度</span></span><br><span class="line">  <span class="attr">mobile:</span></span><br><span class="line">    <span class="attr">show:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">react:</span></span><br><span class="line">    <span class="attr">opacityDefault:</span> <span class="number">0.7</span> <span class="comment"># 默认透明度</span></span><br></pre></td></tr></table></figure>
<p>更多配置参数请查看 <a href="https://l2dwidget.js.org/docs/class/src/index.js~L2Dwidget.html#instance-method-init" target="_blank" rel="noopener">L2Dwidget | live2d-widget.js</a></p>
<p>此时重启服务器暂时还看不到看板娘，需要手动下载或安装模型资源。可以从 <a href="https://huaji8.top/post/live2d-plugin-2.0/" target="_blank" rel="noopener">hexo live2d 模型预览</a> 里找到你喜欢的角色，然后根据 <a href="https://github.com/xiazeyu/live2d-widget-models" target="_blank" rel="noopener">live2d-widget-models</a> 中提供的方法来下载模型数据.</p>
<p>例如通过以下命令下载模型 shizuku：</p>
<figure class="highlight gams"><table><tr><td class="code"><pre><span class="line"><span class="symbol">$</span> npm install live2d-widget-<span class="keyword">model</span>-shizuku</span><br></pre></td></tr></table></figure>
<p>因为修改了站点配置文件，所以需要重启服务器才能预览模型效果。</p>
<p>如果设置了 <code>live2d.tagMode: true</code>，则可以在指定页面中插入以下标签：</p>
<figure class="highlight clojure"><table><tr><td class="code"><pre><span class="line">&#123;&#123; live2d() &#125;&#125;</span><br></pre></td></tr></table></figure>
<p>只有拥有该标签的页面才会渲染 live2d 模型，这样以来就可以精确控制在哪些页面上显示看板娘了。</p>
<p>如果只想在一级菜单页面上显示看板娘，可以在 Header 模板中添加以下代码：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout\_partials\header\index.swig</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="addition">+ &#123;% if is_index %&#125;</span></span><br><span class="line"><span class="addition">+   &#123;&#123; live2d() &#125;&#125;</span></span><br><span class="line"><span class="addition">+ &#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure>
<p>个人认为在文章内出现看板娘将会影响读者注意力的集中，毕竟一篇博客里最重要的是内容，而不是这些花里胡哨转移注意力的东西。所以本站只在一级菜单页面添加了看板娘，文章页面则保持极致精简的阅读体验。</p>
<h1 id="bian-yuan-bai-dong-xiao-guo">边缘摆动效果</h1>
<p>点击下方按钮下载脚本，并置<code> wobblewindow.js</code>于 themes\next\source\js\ 目录下：</p>
<p><code><a class="btn" href="js.zip">
            <i class="fa fa-download"></i>wobblewindow.js
          </a></code></p>
<p>在主题自定义布局文件中添加以下代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout\_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;# wobble窗口摆动特效 #&#125;</span><br><span class="line">&#123;% <span class="keyword">if</span> theme.wobble %&#125;</span><br><span class="line">  &lt;script src=<span class="string">"/js/wobblewindow.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">  &lt;script&gt;</span></span><br><span class="line"><span class="regexp">    /</span><span class="regexp">/只在桌面版网页启用特效</span></span><br><span class="line"><span class="regexp">    if( window.innerWidth &gt; 768  )&#123;</span></span><br><span class="line"><span class="regexp">      $(document).ready(function () &#123;</span></span><br><span class="line"><span class="regexp">        &#123;% if theme.wobble.header %&#125;</span></span><br><span class="line"><span class="regexp">          $('#header').wobbleWindow(&#123;</span></span><br><span class="line"><span class="regexp">            radius: &#123;&#123; theme.wobble.radius &#125;&#125;,</span></span><br><span class="line"><span class="regexp">            movementTop: false,</span></span><br><span class="line"><span class="regexp">            movementLeft: false,</span></span><br><span class="line"><span class="regexp">            movementRight: false,</span></span><br><span class="line"><span class="regexp">            debug: false,</span></span><br><span class="line"><span class="regexp">          &#125;);</span></span><br><span class="line"><span class="regexp">        &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">        &#123;% if theme.wobble.sidebar %&#125;</span></span><br><span class="line"><span class="regexp">          $('#sidebar').wobbleWindow(&#123;</span></span><br><span class="line"><span class="regexp">            radius: &#123;&#123; theme.wobble.radius &#125;&#125;,</span></span><br><span class="line"><span class="regexp">            movementLeft: false,</span></span><br><span class="line"><span class="regexp">            movementTop: false,</span></span><br><span class="line"><span class="regexp">            movementBottom: false,</span></span><br><span class="line"><span class="regexp">            position: 'fixed',</span></span><br><span class="line"><span class="regexp">            debug: false,</span></span><br><span class="line"><span class="regexp">          &#125;);</span></span><br><span class="line"><span class="regexp">        &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">        &#123;% if theme.wobble.footer %&#125;</span></span><br><span class="line"><span class="regexp">          $('#footer').wobbleWindow(&#123;</span></span><br><span class="line"><span class="regexp">            radius: &#123;&#123; theme.wobble.radius &#125;&#125;,</span></span><br><span class="line"><span class="regexp">            movementBottom: false,</span></span><br><span class="line"><span class="regexp">            movementLeft: false,</span></span><br><span class="line"><span class="regexp">            movementRight: false,</span></span><br><span class="line"><span class="regexp">            offsetX: &#123;&#123; theme.wobble.offset &#125;&#125;,</span></span><br><span class="line"><span class="regexp">            position: 'absolute',</span></span><br><span class="line"><span class="regexp">            debug: false,</span></span><br><span class="line"><span class="regexp">          &#125;);</span></span><br><span class="line"><span class="regexp">        &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp">      &#125;);</span></span><br><span class="line"><span class="regexp">    &#125;</span></span><br><span class="line"><span class="regexp">  &lt;/</span>script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<p>在自定义样式文件中添加以下样式：</p>
<figure class="highlight js"><figcaption><span>source/_data/style.styl</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment">//窗口波动效果相关样式</span></span><br><span class="line"><span class="keyword">if</span> hexo-config(<span class="string">'wobble'</span>)  &#123;</span><br><span class="line">  .sidebar &#123;</span><br><span class="line">    box-shadow: none;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  .wobbleTransparentBK&#123;</span><br><span class="line">    background-color: rgba(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) !important;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  .wobbleTransparentLine&#123;</span><br><span class="line">    border-color: rgba(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) !important;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//Next.Muse中为Header和Footer添加背景色</span></span><br><span class="line">  #header, #footer &#123;</span><br><span class="line">    background-color: rgb(<span class="number">245</span>, <span class="number">245</span>, <span class="number">245</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//防止sidebar和footer同时开启动效时堆叠异常</span></span><br><span class="line">  #sidebar, header &#123;</span><br><span class="line">    z-index: <span class="number">1</span> !important;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//防止挡住页末文章的阅读全文按钮</span></span><br><span class="line">  .main &#123;</span><br><span class="line">    padding-bottom: <span class="number">200</span>px;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Next.Muse 主题方案中 Header 和 Footer 是没有背景色的，所以需要添加背景色后才能看出边缘摆动效果。另外，实现边缘摆动效果所需的 <code>z-index</code> 属性可能会导致元素堆叠异常，需要添加以上样式来矫正。</p>
<p>在主题配置文件中添加以下代码：</p>
<figure class="highlight yml"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># window woblle</span></span><br><span class="line"><span class="attr">wobble:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span>  <span class="comment"># 是否开启边缘波动效果</span></span><br><span class="line">  <span class="attr">radius:</span> <span class="number">50</span>  <span class="comment"># 波动半径</span></span><br><span class="line">  <span class="attr">sidebar:</span> <span class="literal">true</span>  <span class="comment"># 开启侧边栏边缘摆动</span></span><br><span class="line">  <span class="attr">header:</span> <span class="literal">true</span>  <span class="comment"># 开启头部边缘摆动</span></span><br><span class="line">  <span class="attr">footer:</span> <span class="literal">true</span>  <span class="comment"># 开启脚部边缘摆动</span></span><br></pre></td></tr></table></figure>
<p>用户可以根据需要在配置文件中为选择开启边缘摆动效果的布局元素。刷新浏览器，然后将鼠标移动到布局边缘上尽情的挑逗它吧。如果从本地加载 JS 脚本速度较慢，可以考虑将脚本放到 CDN 上再引入。</p>
<h1 id="ge-xing-hua-hui-dao-ding-bu">个性化回到顶部</h1>
<p>效果如下：</p>
<p><img src="/2018/02/10/hexo/hexo-theme-beautify/image-20200511170913805.png" alt></p>
<p>原理很简单，将 back-to-top 按钮添加图片背景，并添加 CSS3 动效即可。首先，找到自己喜欢的图片素材放到 source\images\ 目录下。你可以点击下方按钮下载本站所使用的小猫上吊素材（ 小猫咪这么可爱，当然要多放点孜然啦…）</p>
<p><code><a class="btn" href="scroll.png">
            <i class="fa fa-download"></i>下载图片
          </a></code></p>
<p>然后在自定义样式文件中添加如下代码：</p>
<figure class="highlight css"><figcaption><span>source/_data/style.styl</span></figcaption><table><tr><td class="code"><pre><span class="line">//自定义回到顶部样式</span><br><span class="line"><span class="selector-class">.back-to-top</span> &#123;</span><br><span class="line">  <span class="attribute">right</span>: <span class="number">60px</span>;</span><br><span class="line">  width: 70px;  //图片素材宽度</span><br><span class="line">  height: 900px;  //图片素材高度</span><br><span class="line">  <span class="selector-tag">top</span>: <span class="selector-tag">-900px</span>;</span><br><span class="line">  <span class="selector-tag">bottom</span>: <span class="selector-tag">unset</span>;</span><br><span class="line">  <span class="selector-tag">transition</span>: <span class="selector-tag">all</span> <span class="selector-class">.5s</span> <span class="selector-tag">ease-in-out</span>;</span><br><span class="line">  background: url("/images/scroll.png");</span><br><span class="line"></span><br><span class="line">  //隐藏箭头图标</span><br><span class="line">  &gt; <span class="selector-tag">i</span> &#123;</span><br><span class="line">    <span class="attribute">display</span>: none;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &amp;<span class="selector-class">.back-to-top-on</span> &#123;</span><br><span class="line">    <span class="attribute">bottom</span>: unset;</span><br><span class="line">    <span class="attribute">top</span>: <span class="number">100vh</span> &lt; (<span class="number">900px</span> + <span class="number">200px</span>) ? <span class="built_in">calc</span>( <span class="number">100vh</span> - <span class="number">900px</span> - <span class="number">200px</span> ) : <span class="number">0px</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>刷新浏览器即可预览效果。</p>
<h1 id="shu-biao-dian-ji-te-xiao">鼠标点击特效</h1>
<p>从各个站点里搜罗了以下四个比较常用的鼠标点击特效：</p>
<p><code><div class="tabs" id="点击特效"><ul class="nav-tabs"><li class="tab active"><a href="#点击特效-1">礼花特效</a></li><li class="tab"><a href="#点击特效-2">爆炸特效</a></li><li class="tab"><a href="#点击特效-3">浮出爱心</a></li><li class="tab"><a href="#点击特效-4">浮出文字</a></li></ul><div class="tab-content"><div class="tab-pane active" id="点击特效-1"><p><img src="/2018/02/10/hexo/hexo-theme-beautify/cursor-fireworks.gif" alt></p></div><div class="tab-pane" id="点击特效-2"><p><img src="/2018/02/10/hexo/hexo-theme-beautify/cursor-explosion.gif" alt></p></div><div class="tab-pane" id="点击特效-3"><p><img src="/2018/02/10/hexo/hexo-theme-beautify/cursor-love.gif" alt></p></div><div class="tab-pane" id="点击特效-4"><p><img src="/2018/02/10/hexo/hexo-theme-beautify/cursor-text.gif" alt></p></div></div></div></code></p>
<p>点击下方按钮下载相应的脚本，并置<code>fireworks.js.css</code>,<code>explosion.min.js</code>,<code>love.min.js</code>,<code>text.js</code>于 themes\next\source\js\cursor\ 目录下：</p>
<p><code><a class="btn" href="js.zip">
            <i class="fa fa-download"></i>礼花特效
          </a></code></p>
<p><code><a class="btn" href="js.zip">
            <i class="fa fa-download"></i>爆炸特效
          </a></code></p>
<p><code><a class="btn" href="js.zip">
            <i class="fa fa-download"></i>浮出爱心
          </a></code></p>
<p><code><a class="btn" href="js.zip">
            <i class="fa fa-download"></i>浮出文字
          </a></code></p>
<p>在主题自定义布局文件中添加以下代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout\_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;# 鼠标点击特效 #&#125;</span><br><span class="line">&#123;% <span class="keyword">if</span> theme.cursor_effect == <span class="string">"fireworks"</span> %&#125;</span><br><span class="line">  &lt;script <span class="keyword">async</span> src=<span class="string">"/js/cursor/fireworks.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">&#123;% elseif theme.cursor_effect == "explosion" %&#125;</span></span><br><span class="line"><span class="regexp">  &lt;canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" &gt;&lt;/</span>canvas&gt;</span><br><span class="line">  &lt;script src=<span class="string">"//cdn.bootcss.com/animejs/2.2.0/anime.min.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">  &lt;script async src="/</span>js/cursor/explosion.min.js<span class="string">"&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="string">&#123;% elseif theme.cursor_effect == "</span>love<span class="string">" %&#125;</span></span><br><span class="line"><span class="string">  &lt;script async src="</span>/js/cursor/love.min.js<span class="string">"&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="string">&#123;% elseif theme.cursor_effect == "</span>text<span class="string">" %&#125;</span></span><br><span class="line"><span class="string">  &lt;script async src="</span>/js/cursor/text.js<span class="string">"&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure>
<p>如果 custom.swig 文件不存在，需要手动新建并在布局页面中 body 末尾引入：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout\_layout.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">      ...</span><br><span class="line">      &#123;% include '_third-party/exturl.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/bookmark.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/copy-code.swig' %&#125;</span><br><span class="line"></span><br><span class="line"><span class="addition">+     &#123;% include '_custom/custom.swig' %&#125;</span></span><br><span class="line">    &lt;/body&gt;</span><br><span class="line">  &lt;/html&gt;</span><br></pre></td></tr></table></figure>
<p>在主题配置文件中添加以下代码：</p>
<figure class="highlight yml"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># mouse click effect: fireworks | explosion | love | text</span></span><br><span class="line"><span class="attr">cursor_effect:</span> <span class="string">fireworks</span></span><br></pre></td></tr></table></figure>
<p>这样即可在配置文件中一键快速切换鼠标点击特效。</p>
<h1 id="da-zi-te-xiao">打字特效</h1>
<p><img src="/2018/02/10/hexo/hexo-theme-beautify/typing-effect.gif" alt></p>
<p>点击下方按钮下载相应的脚本，并置<code>activate-power-mode.min.js.css</code>于 themes\next\source\js\ 目录下：</p>
<p><code><a class="btn" href="js.zip">
            <i class="fa fa-download"></i>打字特效
          </a></code></p>
<p>在主题自定义布局文件中添加以下代码：</p>
<figure class="highlight js"><figcaption><span>themes\next\layout\_custom\custom.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;# 打字特效 #&#125;</span><br><span class="line">&#123;% <span class="keyword">if</span> theme.typing_effect %&#125;</span><br><span class="line">  &lt;script src=<span class="string">"/js/activate-power-mode.min.js"</span>&gt;&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">  &lt;script&gt;</span></span><br><span class="line"><span class="regexp">    POWERMODE.colorful = &#123;&#123; theme.typing_effect.colorful &#125;&#125;;</span></span><br><span class="line"><span class="regexp">    POWERMODE.shake = &#123;&#123; theme.typing_effect.shake &#125;&#125;;</span></span><br><span class="line"><span class="regexp">    document.body.addEventListener('input', POWERMODE);</span></span><br><span class="line"><span class="regexp">  &lt;/</span>script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<p>如果 custom.swig 文件不存在，需要手动新建并在布局页面中 body 末尾引入：</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout\_layout.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">      ...</span><br><span class="line">      &#123;% include '_third-party/exturl.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/bookmark.swig' %&#125;</span><br><span class="line">      &#123;% include '_third-party/copy-code.swig' %&#125;</span><br><span class="line"></span><br><span class="line"><span class="addition">+     &#123;% include '_custom/custom.swig' %&#125;</span></span><br><span class="line">    &lt;/body&gt;</span><br><span class="line">  &lt;/html&gt;</span><br></pre></td></tr></table></figure>
<p>在主题配置文件中添加以下代码：</p>
<figure class="highlight yml"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># typing effect</span></span><br><span class="line"><span class="attr">typing_effect:</span></span><br><span class="line">  <span class="attr">colorful:</span> <span class="literal">true</span>  <span class="comment"># 礼花特效</span></span><br><span class="line">  <span class="attr">shake:</span> <span class="literal">false</span>  <span class="comment"># 震动特效</span></span><br></pre></td></tr></table></figure>
<p>如果从本地加载 JS 脚本速度较慢，可以考虑将脚本放到 CDN 上再引入。</p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://asdfv1929.github.io/2018/01/28/add-the-end/" target="_blank" rel="noopener">asdfv1929 | Hexo NexT 主题内给每篇文章后添加结束标语</a></li>
<li><a href="https://fjkang.github.io/2017/12/08/%E6%B7%BB%E5%8A%A0%E4%B8%80%E4%B8%AA%E8%90%8C%E7%89%A9/" target="_blank" rel="noopener">FJKang | 添加一个萌物</a></li>
<li><a href="http://evanyou.me/" target="_blank" rel="noopener">尤雨溪的个人主页</a></li>
<li><a href="https://diygod.me/" target="_blank" rel="noopener">DIYgod 的博客</a></li>
<li><a href="https://www.ofind.cn/" target="_blank" rel="noopener">猪猪侠的博客</a></li>
<li><a href="https://qianling.pw/hexo-optimization/" target="_blank" rel="noopener">千灵夙赋 | Hexo 优化汇总</a></li>
<li><a href="https://www.ilxtx.com/comment-input-effects.html" target="_blank" rel="noopener">龙笑天下 | 给 WordPress 博客网站添加评论输入打字礼花及震动特效</a></li>
<li><a href="https://sunhwee.com/posts/6e8839eb.html#toc-heading-26" target="_blank" rel="noopener">Hexo+Github博客搭建完全教程</a></li>
<li><a href="https://www.aomanhao.top/2019/11/04/hexo_clock/" target="_blank" rel="noopener">粒子时钟特效</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo写作技巧</title>
    <url>/2018/02/09/hexo/hexo-writing-skills/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2018/02/09/hexo/hexo-writing-skills/writing.jpg" alt></p>
<p>本文介绍 Hexo 博客的写作技巧。</p>
<a id="more"></a>
<h1 id="kai-shi-xie-zuo">开始写作</h1>
<p>在博客目录下执行如下命令新建一篇文章</p>
<figure class="highlight gauss"><table><tr><td class="code"><pre><span class="line">$ hexo <span class="keyword">new</span> [layout] &lt;<span class="built_in">title</span>&gt;</span><br></pre></td></tr></table></figure>
<p>如果未指定文章的布局（layout），则默认使用 <code>post</code> 布局，生成的文档存放于 <code>source\_posts\</code> 目录下，打开后使用 Markdown 语法进行写作，保存后刷新浏览器即可看到文章。</p>
<h2 id="bu-ju">布局</h2>
<p>布局是什么概念呢，你可以理解为新建文档时的一个模板，基于布局生成的文档将会继承布局的样式。</p>
<p>Hexo 默认有三种布局：<code>post</code>、 <code>page</code> 和 <code>draft</code>，用户可以在 <code>scaffolds</code> 目录下新建文档来自定义布局格式，还可以修改站点配置文件中的 <code>default_layout</code>参数来指定生成文档时的默认布局。</p>
<h3 id="wen-zhang-post">文章(post)</h3>
<p>基于 <code>post</code> 布局生成的文档存在于 <code>source\_posts\</code> 目录下，该目录下的文档会作为博客正文显示在网站中。</p>
<h3 id="ye-mian-page">页面（page）</h3>
<p><code>page</code> 布局用于生成类似 <strong>首页</strong> 和 <strong>归档</strong> 这样的页面。默认的 Next 主题样式中只包含首页和归档这两个链接，可以通过修改主题配置文件中的 <code>menu</code> 字段来新增更多页面菜单。</p>
<figure class="highlight diff"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: / || home</span><br><span class="line">  about: /about/ || user</span><br><span class="line"><span class="addition">+ tags: /tags/ || tags</span></span><br><span class="line"><span class="addition">+ categories: /categories/ || th</span></span><br><span class="line"><span class="addition">+ archives: /archives/ || archive</span></span><br></pre></td></tr></table></figure>
<p>其中，<code>||</code> 之前的值表示菜单链接，之后的值表示所用的 <code>FontAwesome</code> 图标名称。</p>
<p>刷新页面后即可看到页面内多了几项菜单。</p>
<p><img src="/2018/02/09/hexo/hexo-writing-skills/add.png" alt></p>
<p>此时点击 <strong>关于</strong>、<strong>标签</strong> 和 <strong>分类</strong> 都会跳转到 404 页面，原因是我们只开放了页面入口，却没有创造对应于链接的页面视图，此时就需要通过 <code>hexo new page </code> 命令来新建页面。</p>
<p>基于 <code>page</code> 布局的新建命令将会在 <code>source</code> 目录下新建一个 <code>title</code> 文件夹，并在该文件夹下创建一个 <code>index.md</code> 文件，编辑该文件即可修改页面内容。</p>
<p>例如，通过 <code>hexo new page tags</code> 命令将会生成如下目录。</p>
<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">└──  <span class="keyword">source</span>             </span><br><span class="line">  ├── _posts          </span><br><span class="line">  └── <span class="keyword">tags</span></span><br><span class="line">    └── <span class="built_in">index</span>.md</span><br></pre></td></tr></table></figure>
<p>编辑 <code>index.md</code> 文件，在 Front-Matter 中添加 <code>type: tags</code> ，Next 主题将自动在该页面内显示标签云。</p>
<figure class="highlight diff"><figcaption><span>source\tags\index.md</span></figcaption><table><tr><td class="code"><pre><span class="line">  title: 标签</span><br><span class="line">  date: 2018-10-19 22:57:00</span><br><span class="line"><span class="addition">+ type: tags</span></span><br></pre></td></tr></table></figure>
<div class="note info">
            <p>Front-Matter 是文件最上方以 <code>---</code> 分隔的区域，用于指定本文件的各种参数值</p>
          </div>
<p>在菜单中点击 <strong>标签</strong> 跳转到刚创建的标签页面。</p>
<p><img src="/2018/02/09/hexo/hexo-writing-skills/tags.png" alt></p>
<p>同理可通过 <code>page</code> 布局生成 <strong>关于</strong> 和 <strong>分类</strong> 两个页面。</p>
<h3 id="cao-gao-draft">草稿（draft）</h3>
<p><code>draft</code> 布局用于创建草稿，生成的文档存在于 source_drafts\ 目录中，默认配置下将不会把该目录下的文档渲染到网站中。</p>
<p>通过以下命令将草稿发布为正式文章：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">$ hexo publish <span class="tag">&lt;<span class="name">title</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>该命令会将 source_drafts\ 目录下,考虑到一些文章可能需要数天才能完成，建议将新建文档时的默认布局设置为 <code>draft</code>：</p>
<figure class="highlight diff"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="deletion">- default_layout: post</span></span><br><span class="line"><span class="addition">+ default_layout: draft</span></span><br></pre></td></tr></table></figure>
<h2 id="biao-qian-yu-fen-lei">标签与分类</h2>
<p>我们刚提到了标签，也提到了分类，那么标签和分类是什么，其区别是什么。</p>
<p>标签和分类都是用于对文章进行归档的一种方式，标签是一种列表结构，而分类是一种树结构。我们以人作为例子，从标签的角度考虑，我可以拥有程序员、高颜值、幽默等标签，这些标签之间没有层级关系；从分类的角度考虑，我是亚洲人、中国人、河南人，这些分类之间是有明确的包含关系的。</p>
<p>可以在 Front-Matter 中添加 <code>catergories</code> 和 <code>tags</code> 字段为文章添加标签和分类，如我为本文添加了 <strong>Hexo</strong> 和 <strong>Markdown</strong> 两个标签，并将其归类到了 <strong>技术 / 博客</strong> 类别，对应的 Front-Matter 结构如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">title:</span> <span class="string">写作技巧</span></span><br><span class="line"><span class="attr">mathjax:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">tags:</span> <span class="string">[hexo,</span> <span class="string">markdown]</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2020</span><span class="number">-05</span><span class="number">-10</span> <span class="number">10</span><span class="string">:35:32</span></span><br><span class="line"><span class="attr">categories:</span> <span class="string">技术/博客</span></span><br><span class="line"><span class="attr">password:</span> </span><br><span class="line"><span class="attr">top:</span></span><br></pre></td></tr></table></figure>
<h2 id="markdown-ji-ben-yu-fa">Markdown 基本语法</h2>
<p>Markdown 是一种标记语言，语法简单，易阅读易编写，可以让用户完全脱离鼠标写出样式丰富的文档，广受程序员喜爱，目前许多网站都已经支持通过 Markdown 语法来写文章或者发表评论。</p>
<table>
<thead>
<tr>
<th style="text-align:left">元素</th>
<th style="text-align:left">Markdown 语法</th>
<th style="text-align:left">效果预览</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">标题</td>
<td style="text-align:left"><code># 标题1</code> <code>## h2</code> <code>### h3</code></td>
<td style="text-align:left">标题一标题二标题三</td>
</tr>
<tr>
<td style="text-align:left">加粗</td>
<td style="text-align:left"><code>**文字加粗**</code></td>
<td style="text-align:left"><strong>文字加粗</strong></td>
</tr>
<tr>
<td style="text-align:left">引用</td>
<td style="text-align:left"><code>&gt; 引用文字</code></td>
<td style="text-align:left">引用文字</td>
</tr>
<tr>
<td style="text-align:left">有序列表</td>
<td style="text-align:left"><code>1. 第一项</code> <code>2. 第二项</code> <code>3. 第三项</code></td>
<td style="text-align:left">第一项第二项第三项</td>
</tr>
<tr>
<td style="text-align:left">无序列表</td>
<td style="text-align:left"><code>- 第一项</code> <code>- 第二项</code> <code>- 第三项</code></td>
<td style="text-align:left">第一项第二项第三项</td>
</tr>
<tr>
<td style="text-align:left">链接</td>
<td style="text-align:left"><code>[链接](url)</code></td>
<td style="text-align:left"><a href="http://yearito.cn/posts/url" target="_blank" rel="noopener">链接</a></td>
</tr>
<tr>
<td style="text-align:left">图片</td>
<td style="text-align:left"><code>![图片](image.jpg)</code></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">水平线</td>
<td style="text-align:left"><code>---</code></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">代码</td>
<td style="text-align:left"><code>code</code></td>
<td style="text-align:left"><code>code</code></td>
</tr>
<tr>
<td style="text-align:left">代码块</td>
<td style="text-align:left"><code>code snippet</code></td>
<td style="text-align:left"><code>code snippet</code></td>
</tr>
</tbody>
</table>
<div class="note info">
            <p>更多语法请参考 <a href="https://www.markdownguide.org/basic-syntax" target="_blank" rel="noopener">基础语法 | Markdown Guide</a> 和 <a href="https://www.markdownguide.org/extended-syntax" target="_blank" rel="noopener">扩展语法 | Markdown Guide</a></p>
          </div>
<h1 id="hexo-nei-zhi-biao-qian">Hexo 内置标签</h1>
<h2 id="wen-ben-ju-zhong-biao-qian">文本居中标签</h2>
<p>居中标签效果如下：</p>
<blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p>我不去想是否能够成功，既然选择了远方，便只顾风雨兼程。</p>

            <i class="fa fa-quote-right"></i>
          </blockquote>
<p>一般在引用单行文本时使用，如作为文章开篇题词。可以通过以下几种方式使用该标签：</p>
<figure class="highlight django"><table><tr><td class="code"><pre><span class="line"><span class="xml"><span class="comment">&lt;!-- HTML方式: 直接在 Markdown 文件中编写 HTML 来调用 --&gt;</span></span></span><br><span class="line"><span class="xml"><span class="comment">&lt;!-- 其中 class="blockquote-center" 是必须的 --&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">blockquote</span> <span class="attr">class</span>=<span class="string">"blockquote-center"</span>&gt;</span>blah blah blah<span class="tag">&lt;/<span class="name">blockquote</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="comment">&lt;!-- 标签方式 --&gt;</span></span></span><br><span class="line"><span class="template-tag">&#123;% <span class="name">centerquote</span> %&#125;</span><span class="xml">blah blah blah</span><span class="template-tag">&#123;% <span class="name">endcenterquote</span> %&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="comment">&lt;!-- 标签别名 --&gt;</span></span></span><br><span class="line"><span class="template-tag">&#123;% <span class="name">cq</span> %&#125;</span><span class="xml"> blah blah blah </span><span class="template-tag">&#123;% <span class="name">endcq</span> %&#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="dai-ma-kuai-jin-jie-yong-fa">代码块进阶用法</h2>
<p>可以通过为代码块附加参数的形式为其添加更丰富的信息提示，效果如下：</p>
<figure class="highlight python"><figcaption><span>hello world</span><a href="https://jeffery0628.github.io/" target="_blank" rel="noopener">我的主页</a></figcaption><table><tr><td class="code"><pre><span class="line">print(<span class="string">"Hello world!"</span>);</span><br></pre></td></tr></table></figure>
<p>代码块进阶语法规则：</p>
<div class="note ">
            <p>``` [language] [title] [url] [link text]<br>code snippet<br>```</p>
          </div>
<p>其中，各参数意义如下：</p>
<ul>
<li>langugae：语言名称，引导渲染引擎正确解析并高亮显示关键字</li>
<li>title：代码块标题，将会显示在左上角</li>
<li>url：链接地址，如果没有指定 link text 则会在右上角显示 link</li>
<li>link text：链接名称，指定 url 后有效，将会显示在右上角</li>
</ul>
<p>url 必须为有效链接地址才会以链接的形式显示在右上角，否则将作为标题显示在左上角。以 url 为分界，左侧除了第一个单词会被解析为 language，其他所有单词都会被解析为 title，而右侧的所有单词都会被解析为 link text。如果不想填写 title，可以在 language 和 url 之间添加至少三个空格。</p>
<p>可以在站点配置文件中设置 <code>highlight.auto_detect: true</code> 来开启自动语言检测高亮。</p>
<figure class="highlight diff"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">highlight:</span><br><span class="line">   enable: true</span><br><span class="line">   line_number: false</span><br><span class="line"><span class="deletion">-  auto_detect: false</span></span><br><span class="line"><span class="addition">+  auto_detect: true</span></span><br><span class="line">   tab_replace:</span><br></pre></td></tr></table></figure>
<p>如果设置语言为 diff，可以在代码前添加 <code>+</code> 和 <code>-</code> 来使用如上所示的高亮增删行提示效果，在展示代码改动痕迹时比较实用。</p>
<h2 id="note-biao-qian">note 标签</h2>
<p>通过 note 标签可以为段落添加背景色，语法如下：</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">&#123;%<span class="built_in"> note </span>[class] %&#125;</span><br><span class="line">文本内容 (支持行内标签)</span><br><span class="line">&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure>
<p>支持的 class 种类包括 <code>default</code> <code>primary</code> <code>success</code> <code>info</code> <code>warning</code> <code>danger</code>，也可以不指定 class。</p>
<p>各种 class 种类的效果如下：</p>
<div class="note primary">
            <p><strong>primary</strong> note tag</p>
          </div>
<div class="note success">
            <p><strong>success</strong> note tag</p>
          </div>
<div class="note info">
            <p><strong>info</strong> note tag</p>
          </div>
<div class="note warning">
            <p><strong>warning</strong> note tag</p>
          </div>
<div class="note danger">
            <p><strong>danger</strong> note tag</p>
          </div>
<div class="note ">
            <p>undefined class note tag</p>
          </div>
<p>更多配置可在主题配置文件中设置</p>
<figure class="highlight css"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">note</span>:</span><br><span class="line">  # <span class="selector-tag">Note</span> 标签样式预设</span><br><span class="line">  style: modern  # simple | modern | flat | disabled</span><br><span class="line">  <span class="selector-tag">icons</span>: <span class="selector-tag">false</span>  # 是否显示图标</span><br><span class="line">  <span class="selector-tag">border_radius</span>: 3  # 圆角半径</span><br><span class="line">  <span class="selector-tag">light_bg_offset</span>: 0  # 默认背景减淡效果，以百分比计算</span><br></pre></td></tr></table></figure>
<h2 id="label-biao-qian">label 标签</h2>
<p>通过 label 标签可以为文字添加背景色，语法如下：</p>
<figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line">&#123;% label [<span class="class"><span class="keyword">class</span>]<span class="meta">@text</span>  %&#125;</span></span><br></pre></td></tr></table></figure>
<p>支持的 class 种类包括 <code>default</code> <code>primary</code> <code>success</code> <code>info</code> <code>warning</code> <code>danger</code>，默认使用 <code>default</code> 作为缺省。</p>
<figure class="highlight puppet"><table><tr><td class="code"><pre><span class="line">I heard the echo, &#123;% label default@from the valleys and the heart %&#125;</span><br><span class="line">Open to the lonely soul <span class="keyword">of</span> &#123;% label info@sickle harvesting %&#125;</span><br><span class="line"><span class="keyword">Repeat</span> <span class="keyword">outrightly</span>, but also repeat the well-being of</span><br><span class="line"><span class="keyword">Eventually</span> &#123;% label warning@swaying <span class="keyword">in</span> the desert oasis %&#125;</span><br><span class="line">&#123;% label success@I believe %&#125; <span class="keyword">I</span> <span class="keyword">am</span></span><br><span class="line">&#123;% label primary@Born as the bright summer flowers %&#125;</span><br><span class="line"><span class="keyword">Do</span> <span class="keyword">not</span> <span class="keyword">withered</span> <span class="keyword">undefeated</span> <span class="keyword">fiery</span> <span class="keyword">demon</span> <span class="keyword">rule</span></span><br><span class="line"><span class="keyword">Heart</span> <span class="keyword">rate</span> <span class="keyword">and</span> <span class="keyword">breathing</span> <span class="keyword">to</span> <span class="keyword">bear</span> &#123;% label danger@the load of the cumbersome %&#125;</span><br><span class="line"><span class="keyword">Bored</span></span><br></pre></td></tr></table></figure>
<p>I heard the echo, <span class="label default">from the valleys and the heart</span><br>
Open to the lonely soul of <span class="label info">sickle harvesting</span><br>
Repeat outrightly, but also repeat the well-being of<br>
Eventually <span class="label warning">swaying in the desert oasis</span></p>
<span class="label success">I believe</span> I am
<span class="label primary">Born as the bright summer flowers</span>
<p>Do not withered undefeated fiery demon rule<br>
Heart rate and breathing to bear <span class="label danger">the load of the cumbersome</span><br>
Bored</p>
<p>可在主题配置文件中设置 <code>label: false</code> 来取消 label 标签默认 CSS 样式。</p>
<h2 id="button-an-niu">button 按钮</h2>
<p>通过 button 标签可以快速添加带有主题样式的按钮，语法如下：</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line">&#123;% button /<span class="type">path</span>/<span class="keyword">to</span>/url/, <span class="type">text</span>, icon [<span class="keyword">class</span>], title %&#125;</span><br></pre></td></tr></table></figure>
<p>也可简写为：</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line">&#123;% btn /<span class="type">path</span>/<span class="keyword">to</span>/url/, <span class="type">text</span>, icon [<span class="keyword">class</span>], title %&#125;</span><br></pre></td></tr></table></figure>
<p>其中， 图标 ID 来源于 <a href="https://fontawesome.com/v4.7.0/icons/" target="_blank" rel="noopener">FontAwesome</a> 。</p>
<p>使用示例如下：</p>
<figure class="highlight django"><table><tr><td class="code"><pre><span class="line"><span class="template-tag">&#123;% <span class="name">btn</span> #, 文本 %&#125;</span></span><br><span class="line"><span class="template-tag">&#123;% <span class="name">btn</span> #, 文本 &amp; 标题,, 标题 %&#125;</span></span><br><span class="line"><span class="template-tag">&#123;% <span class="name">btn</span> #, 文本 &amp; 图标, home %&#125;</span></span><br><span class="line"><span class="template-tag">&#123;% <span class="name">btn</span> #, 文本 &amp; 大图标 (固定宽度), home fa-fw fa-lg %&#125;</span></span><br></pre></td></tr></table></figure>
<a class="btn" href="#">
            <i class="fa fa-"></i>文本
          </a>
<a class="btn" href="#" title="标题">
            <i class="fa fa-"></i>文本 & 标题
          </a>
<a class="btn" href="#">
            <i class="fa fa-home"></i>文本 & 图标
          </a>
<a class="btn" href="#">
            <i class="fa fa-home fa-fw fa-lg"></i>文本 & 大图标 (固定宽度)
          </a>
<h2 id="tab-biao-qian">tab 标签</h2>
<p>tab 标签用于快速创建 tab 选项卡，语法如下</p>
<figure class="highlight django"><table><tr><td class="code"><pre><span class="line"><span class="template-tag">&#123;% <span class="name">tabs</span> [Unique name], [index] %&#125;</span></span><br><span class="line"><span class="xml">  <span class="comment">&lt;!-- tab [Tab caption]@[icon] --&gt;</span></span></span><br><span class="line"><span class="xml">  标签页内容（支持行内标签）</span></span><br><span class="line"><span class="xml">  <span class="comment">&lt;!-- endtab --&gt;</span></span></span><br><span class="line"><span class="template-tag">&#123;% <span class="name">endtabs</span> %&#125;</span></span><br></pre></td></tr></table></figure>
<p>其中，各参数意义如下：</p>
<ul>
<li>Unique name: 全局唯一的 Tab 名称，将作为各个标签页的 id 属性前缀</li>
<li>index: 当前激活的标签页索引，如果未定义则默认选中显示第一个标签页，如果设为 - 1 则默认隐藏所有标签页</li>
<li>Tab caption: 当前标签页的标题，如果不指定则会以 Unique name 加上索引作为标题</li>
<li>icon: 在标签页标题中添加 Font awesome 图标</li>
</ul>
<p>使用示例如下：</p>
<figure class="highlight django"><table><tr><td class="code"><pre><span class="line"><span class="template-tag">&#123;% <span class="name">tabs</span> Tab标签列表 %&#125;</span></span><br><span class="line"><span class="xml">  <span class="comment">&lt;!-- tab 标签页1 --&gt;</span></span></span><br><span class="line"><span class="xml">    标签页1文本内容</span></span><br><span class="line"><span class="xml">  <span class="comment">&lt;!-- endtab --&gt;</span></span></span><br><span class="line"><span class="xml">  <span class="comment">&lt;!-- tab 标签页2 --&gt;</span></span></span><br><span class="line"><span class="xml">    标签页2文本内容</span></span><br><span class="line"><span class="xml">  <span class="comment">&lt;!-- endtab --&gt;</span></span></span><br><span class="line"><span class="xml">  <span class="comment">&lt;!-- tab 标签页3 --&gt;</span></span></span><br><span class="line"><span class="xml">    标签页3文本内容</span></span><br><span class="line"><span class="xml">  <span class="comment">&lt;!-- endtab --&gt;</span></span></span><br><span class="line"><span class="template-tag">&#123;% <span class="name">endtabs</span> %&#125;</span></span><br></pre></td></tr></table></figure>
<div class="tabs" id="tab标签列表"><ul class="nav-tabs"><li class="tab active"><a href="#tab标签列表-1">标签页1</a></li><li class="tab"><a href="#tab标签列表-2">标签页2</a></li><li class="tab"><a href="#tab标签列表-3">标签页3</a></li></ul><div class="tab-content"><div class="tab-pane active" id="tab标签列表-1"><p>标签页1文本内容</p></div><div class="tab-pane" id="tab标签列表-2"><p>标签页2文本内容</p></div><div class="tab-pane" id="tab标签列表-3"><p>标签页3文本内容</p></div></div></div>
<h2 id="yin-yong-zhan-nei-lian-jie">引用站内链接</h2>
<p>可以通过如下语法引入站内文章的地址或链接：</p>
<figure class="highlight django"><table><tr><td class="code"><pre><span class="line"><span class="template-tag">&#123;% <span class="name">post_path</span> slug %&#125;</span></span><br><span class="line"><span class="template-tag">&#123;% <span class="name">post_link</span> slug [title] %&#125;</span></span><br></pre></td></tr></table></figure>
<p>其中，<code>slug</code> 表示 <code>_post</code> 目录下的 Markdown 文件名。</p>
<p><code>post_path</code> 标签将会渲染为文章的地址，即 <code>permalink</code>；而 <code>post_link</code> 标签将会渲染为链接，可以通过 <code>title</code> 指定链接标题。</p>
<p>如以下标签将会生成 </p>
<figure class="highlight django"><table><tr><td class="code"><pre><span class="line"><span class="template-tag">&#123;% <span class="name">post_path</span> hexo-writing-skills %&#125;</span></span><br></pre></td></tr></table></figure>
<p>而以下标签则会生成 </p>
<figure class="highlight django"><table><tr><td class="code"><pre><span class="line"><span class="template-tag">&#123;% <span class="name">post_link</span> hexo-writing-skills 链接标题 %&#125;</span></span><br></pre></td></tr></table></figure>
<p>这种站内引用方式比直接使用 url 引用的形式更为可靠，因为即使修改了 <code>permalink</code> 格式，或者修改了文章的路由地址，只要 Markdown 文件名没有发生改变，引用链接都不会失效。</p>
<h2 id="cha-ru-swig-dai-ma">插入 Swig 代码</h2>
<p>如果需要在页面内插入 Swig 代码，包括原生 HTML 代码，JavaScript 脚本等，可以通过 raw 标签来禁止 Markdown 引擎渲染标签内的内容。语法如下：</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">&#123;%<span class="built_in"> raw </span>%&#125;</span><br><span class="line">content</span><br><span class="line">&#123;% endraw %&#125;</span><br></pre></td></tr></table></figure>
<p>该标签通常用于在页面内引入三方脚本实现特殊功能，尤其是当该三方脚本尚无相关 hexo 插件支持的时候，可以通过写原生 Web 页面的形式引入脚本并编写实现逻辑。</p>
<h1 id="cha-ru-duo-mei-ti">插入多媒体</h1>
<h2 id="cha-ru-tu-pian">插入图片</h2>
<p>Markdown 并不会保存插入的图片资源本身，只是记录了获取资源的链接。因此我们需要选择一款合适的图床来支持博客写作，目前各大云服务商都提供了对象存储服务，如七牛云 KODO、又拍云 USS、腾讯云 COS、阿里云 OSS 等。</p>
<p>所以在 Markdown 中插入一张图片要分为以下几步来进行：</p>
<ol>
<li>将图片资源上传到图床中</li>
<li>获取图片外链</li>
<li>插入到 Markdown 文档中</li>
</ol>
<p>对于博客这种低频访问的应用场景，各大服务商的服务其实并没有显著的差异，并且前期的使用都提供了免费的流量，所以我认为图床的选择主要参考以下几个方面：</p>
<ul>
<li>
<p>图床是否提供了便捷的图形化管理工具用于图片的上传下载？</p>
<p>如阿里云有 ossbrowser，腾讯云有 cosbrowser，七牛云有 QsunSync 等，但就本人使用体验来说，七牛云 QsunSync 的 UI 界面确实很拙劣，功能较为单一，而腾讯云 cosbrowser 的界面就相对美观优雅的多，并以 Windows 资源管理器的交互方式为用户提供资源的上传、下载和管理服务。</p>
<p><img src="/2018/02/09/hexo/hexo-writing-skills/cos_client.png" alt></p>
</li>
<li>
<p>是否能够方便的插入到 Markdown 文档中？</p>
</li>
</ul>
<h2 id="wang-yi-yun-yin-le">网易云音乐</h2>
<p>在网页版云音乐中找到歌曲，点击生成外链播放器：</p>
<p><img src="/2018/02/09/hexo/hexo-writing-skills/music.png" alt></p>
<p>根据个人喜好选择播放器尺寸和播放模式：</p>
<p><img src="/2018/02/09/hexo/hexo-writing-skills/music_size.png" alt></p>
<p>将获取到的 <code>iframe</code> 代码添加到页面中，默认样式如下：</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=31165363&auto=1&height=66"></iframe>
<p>播放器宽度将会被拉长占满整个页宽，看起来有点别扭。查看控制台之后发现 <code>iframe</code> 在渲染的时候被处理过，外层包了一个类名为 <code>fluid-vids</code> 的 <code>div</code> 元素。顺藤摸瓜，找到了相关代码，原来是为了让嵌入的视频支持自适应布局，恰好也将 <code>music.163.com</code> 域名包含在了处理逻辑内，只需要将该行删除即可。</p>
<figure class="highlight js"><figcaption><span>themes\next\source\js\src\utils.js</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> SUPPORTED_PLAYERS = [</span><br><span class="line">  <span class="string">'www.youtube.com'</span>,</span><br><span class="line">  <span class="string">'player.vimeo.com'</span>,</span><br><span class="line">  <span class="string">'player.youku.com'</span>,</span><br><span class="line">  <span class="comment">//'music.163.com',</span></span><br><span class="line">  <span class="string">'www.tudou.com'</span></span><br><span class="line">];</span><br></pre></td></tr></table></figure>
<p>这样播放器样式就变成左对齐固定宽度了，如果你还想让播放器居中，可以将 <code>iframe</code> 包在 <code>&lt;center&gt;</code> 标签内。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">center</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">iframe</span> <span class="attr">frameborder</span>=<span class="string">"no"</span> <span class="attr">border</span>=<span class="string">"0"</span> <span class="attr">marginwidth</span>=<span class="string">"0"</span> <span class="attr">marginheight</span>=<span class="string">"0"</span> <span class="attr">width</span>=<span class="string">329</span> <span class="attr">height</span>=<span class="string">86</span> <span class="attr">src</span>=<span class="string">"//music.163.com/outchain/player?type=2&amp;id=34613621&amp;auto=0&amp;height=66"</span>&gt;</span><span class="tag">&lt;/<span class="name">iframe</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">center</span>&gt;</span></span><br></pre></td></tr></table></figure>
<div class="note warning">
            <p>网易云音乐中部分歌曲因版权保护已经无法生成外链了，即使是通过控制台强行拿到外链地址，嵌入网页后也无法播放。</p>
          </div>
<h2 id="aplayer-yin-pin-bo-fang-qi">Aplayer 音频播放器</h2>
<p><a href="https://aplayer.js.org/#/" target="_blank" rel="noopener">APlayer</a> 是 HTML5 音频播放器，提供了另一种音频播放方案。借助 <a href="https://github.com/MoePlayer/hexo-tag-aplayer" target="_blank" rel="noopener">hexo-tag-aplayer</a> 插件，可以通过标签的形式方便快捷的插入音频组件。在站点根目录下执行以下命令：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ npm <span class="keyword">install</span> hexo-tag-aplayer <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
<p>然后在页面中按照以下标签格式插入歌曲链接和相关信息：</p>
<figure class="highlight clojure"><table><tr><td class="code"><pre><span class="line">&#123;% aplayer title author url [picture_url, narrow, autoplay, width:xxx, lrc:xxx] %&#125;</span><br></pre></td></tr></table></figure>
<p>其中，各参数意义如下：</p>
<ul>
<li>title: 曲目标题</li>
<li>author: 曲目作者</li>
<li>url: 音乐文件 URL 地址</li>
<li>picture_url: (可选) 音乐对应的图片地址</li>
<li>narrow: （可选）播放器袖珍风格</li>
<li>autoplay: (可选) 自动播放，移动端浏览器暂时不支持此功能</li>
<li>width:xxx: (可选) 播放器宽度 (默认: 100%)</li>
<li>lrc:xxx: （可选）歌词文件 URL 地址</li>
</ul>
<p>当开启 Hexo 的 <a href="https://hexo.io/zh-cn/docs/asset-folders.html#%E6%96%87%E7%AB%A0%E8%B5%84%E6%BA%90%E6%96%87%E4%BB%B6%E5%A4%B9" target="_blank" rel="noopener">文章资源文件夹</a> 功能时，可以将图片、音乐文件、歌词文件放入与文章对应的资源文件夹中，然后直接引用，示例如下：</p>
<figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">&#123;<span class="meta">%</span> aplayer <span class="string">"Caffeine"</span> <span class="string">"Jeff Williams"</span> <span class="string">"caffeine.mp3"</span> <span class="string">"picture.jpg"</span> <span class="string">"lrc:caffeine.txt"</span> <span class="meta">%</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight js"><figcaption><span>themes\next\source\css\_custom\custom.styl</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment">//Aplayer 播放器居中</span></span><br><span class="line">div.aplayer &#123;</span><br><span class="line">  margin: <span class="number">5</span>px auto;</span><br><span class="line">  max-width: <span class="number">500</span>px;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<div class="note info">
            <p>插入播放列表功能请参考： <a href="https://github.com/MoePlayer/hexo-tag-aplayer#with-playlist" target="_blank" rel="noopener">hexo-tag-aplayer | With playlist</a></p>
          </div>
<h2 id="dpalyer-shi-pin-bo-fang-qi">Dpalyer 视频播放器</h2>
<p><a href="http://dplayer.js.org/#/" target="_blank" rel="noopener">DPlayer</a> 是一款简洁美观的 HTML5 视频播放器，支持弹幕互动。借助 <a href="https://github.com/MoePlayer/hexo-tag-dplayer" target="_blank" rel="noopener">hexo-tag-dplayer</a> 插件，可以通过标签的形式方便快捷的插入视频组件。在站点根目录下执行以下命令：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ npm <span class="keyword">install</span> hexo-tag-dplayer <span class="comment">--save</span></span><br></pre></td></tr></table></figure>
<p>然后在页面中按照以下标签格式插入歌曲链接和相关信息：</p>
<figure class="highlight clojure"><table><tr><td class="code"><pre><span class="line">&#123;% dplayer <span class="string">"url=video-url"</span> <span class="string">"pic=image-url"</span> ... [<span class="string">"key=value"</span>] %&#125;</span><br></pre></td></tr></table></figure>
<p>此处列举部分重要 <code>key</code> 的参数意义:</p>
<div class="tabs" id="tab标签列表"><ul class="nav-tabs"><li class="tab active"><a href="#tab标签列表-1">播放器</a></li><li class="tab"><a href="#tab标签列表-2">视频</a></li><li class="tab"><a href="#tab标签列表-3">字幕</a></li><li class="tab"><a href="#tab标签列表-4">弹幕</a></li></ul><div class="tab-content"><div class="tab-pane active" id="tab标签列表-1"><ul>
<li>
<p>autoplay：是否开启视频自动播放，默认为 fasle</p>
</li>
<li>
<p>loop：是否开启视频循环播放，默认为 false</p>
</li>
<li>
<p>screenshot：是否开启截图，默认为 false</p>
</li>
<li>
<p>mutex：是否禁止多个播放器同时播放，默认为 true</p>
</li>
<li>
<p>dmunlimited：是否开启海量弹幕模式，默认为 false</p>
</li>
<li>
<p>preload：预加载模式，可选 note metadata auto</p>
</li>
<li>
<p>theme：主题色</p>
</li>
<li>
<p>lang：语言，可选 en zh-cn zh-tw</p>
</li>
<li>
<p>logo：左上角的 Logo</p>
</li>
<li>
<p>volume：默认音量，默认为 0.7</p>
</li>
<li>
<p>width：播放器宽度</p>
</li>
<li>
<p>height：播放器长度</p>
</li>
</ul></div><div class="tab-pane" id="tab标签列表-2"><ul>
<li>
<p>url：视频链接</p>
</li>
<li>
<p>pic：视频封面</p>
</li>
<li>
<p>thumbnails：视频缩略图，可以使用 DPlayer-thumbnails 生成</p>
</li>
<li>
<p>vidtype：视频类型，可选 auto hls flv dash 或其他自定义类型</p>
</li>
</ul></div><div class="tab-pane" id="tab标签列表-3"><ul>
<li>
<p>suburl：字幕链接</p>
</li>
<li>
<p>subtype：字幕类型，可选 webvtt ass，目前只支持 webvtt</p>
</li>
<li>
<p>subbottom：字幕距离播放器底部的距离，如 10px 10%</p>
</li>
<li>
<p>subcolor：字幕颜色</p>
</li>
</ul></div><div class="tab-pane" id="tab标签列表-4"><ul>
<li>
<p>id：弹幕 id</p>
</li>
<li>
<p>api：弹幕 api</p>
</li>
<li>
<p>token：弹幕后端验证 token</p>
</li>
<li>
<p>addition：额外外挂弹幕</p>
</li>
<li>
<p>dmuser：弹幕用户名</p>
</li>
<li>
<p>maximum：弹幕最大数量</p>
</li>
</ul></div></div></div>
<h1 id="jie-shu-yu">结束语</h1>
<p>本文介绍了 Hexo 博客的几项关键写作技巧，包括 Markdown 的基本语法，Hexo 主题的内置标签等，本文还介绍了如何在文章中利用图床外链插入图片，如何利用 Aplayer / Dplayer 等音视频播放器插件在页面内插入多媒体元素等。</p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://qcmoke.netlify.app/blog/hexo_code.html" target="_blank" rel="noopener">hexo代码块进阶写法</a></li>
<li><a href="https://www.ofind.cn/blog/HEXO/HEXO%E4%B8%8B%E7%9A%84%E8%AF%AD%E6%B3%95%E9%AB%98%E4%BA%AE%E6%8B%93%E5%B1%95%E4%BF%AE%E6%94%B9.html#%E6%98%AF%E5%90%A6%E6%98%BE%E7%A4%BA%E8%A1%8C%E5%8F%B7" target="_blank" rel="noopener">HEXO下的语法高亮拓展修改</a></li>
<li><a href="http://dplayer.js.org/#/zh-Hans/" target="_blank" rel="noopener">Dplayer 官方中文文档</a></li>
<li><a href="https://github.com/MoePlayer/hexo-tag-aplayer/blob/master/docs/README-zh_cn.md" target="_blank" rel="noopener">hexo-tag-aplayer | 中文文档</a></li>
<li><a href="https://theme-next.iissnan.com/tag-plugins.html" target="_blank" rel="noopener">NexT 使用文档 | 内置标签</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>writing skills</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo基础建站</title>
    <url>/2018/02/09/hexo/hexo-get-start/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/2018/02/09/hexo/hexo-get-start/hexo.png" alt></p>
<p>Hexo 是一个高效简洁的静态博客框架，支持 Markdown 写作语法，插件丰富，主题优雅，部署方便。目前已成为多数人博客建站的选择。</p>
<p>本文为 Hexo 搭建个人博客系列中的第一篇。第一章中介绍了如何在本地搭建 Hexo 博客，第二章中介绍了如何安装使用 Next 主题，第三章和第四章分别介绍了针对于站点和文章详情页的一些基础优化方案。</p>
<a id="more"></a>
<h1 id="kai-shi-shi-yong">开始使用</h1>
<p>在命令行中通过 npm 来安装 Hexo：</p>
<figure class="highlight avrasm"><table><tr><td class="code"><pre><span class="line">$ npm install -g hexo-<span class="keyword">cli</span></span><br></pre></td></tr></table></figure>
<p><code>-g</code> 表示全局安装，会将 Hexo 命令加入环境变量中，以使其在 cmd 下有效。</p>
<p>Hexo 依赖于 <a href="https://nodejs.org/zh-cn/" target="_blank" rel="noopener">Node.js</a> 和 <a href="https://git-scm.com/download/" target="_blank" rel="noopener">git</a>，所以在安装 Hexo 之前先确保已安装了这两项应用。Hexo 依赖于 <a href="https://nodejs.org/zh-cn/" target="_blank" rel="noopener">Node.js</a> 和 <a href="https://git-scm.com/download/" target="_blank" rel="noopener">git</a>，所以在安装 Hexo 之前先确保已安装了这两项应用。</p>
<p>新建博客目录，然后在该路径下执行初始化命令：</p>
<figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line"><span class="variable">$ </span>hexo init</span><br></pre></td></tr></table></figure>
<p>官方教程中提到要在项目目录下执行 <code>npm install</code> 命令，事实上不必如此，在执行 <code>hexo init</code> 的过程中就已经自动安装好了项目依赖。</p>
<p>执行完毕后，将会生成以下文件结构：</p>
<figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── node_modules       <span class="comment">//依赖安装目录</span></span><br><span class="line">├── scaffolds          <span class="comment">//模板文件夹，新建的文章将会从此目录下的文件中继承格式</span></span><br><span class="line">|   ├── draft.md         <span class="comment">//草稿模板</span></span><br><span class="line">|   ├── page.md          <span class="comment">//页面模板</span></span><br><span class="line">|   └── post.md          <span class="comment">//文章模板</span></span><br><span class="line">├── <span class="keyword">source</span>             <span class="comment">//资源文件夹，用于放置图片、数据、文章等资源</span></span><br><span class="line">|   └── _posts           <span class="comment">//文章目录</span></span><br><span class="line">├── themes             <span class="comment">//主题文件夹</span></span><br><span class="line">|   └── landscape        <span class="comment">//默认主题</span></span><br><span class="line">├── .gitignore         <span class="comment">//指定不纳入git版本控制的文件</span></span><br><span class="line">├── _config.yml        <span class="comment">//站点配置文件</span></span><br><span class="line">├── db.json            </span><br><span class="line">├── <span class="keyword">package</span>.json</span><br><span class="line">└── <span class="keyword">package</span>-lock.json</span><br></pre></td></tr></table></figure>
<p>在根目录下执行如下命令启动 hexo 的内置 Web 服务器</p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line">$ hexo <span class="keyword">server</span></span><br></pre></td></tr></table></figure>
<p>该命令将会调用 Markdown 引擎解析项目中的博客内容生成网页资源，资源将会存于内存中，所以用户执行完命令之后在项目文件夹中是找不到相关的 Web 资源目录的。该命令还会启动一个简易的 Web 服务器用于提供对内存中网页资源的访问（工作机制类似于 webpack-dev-server），Web 服务器默认监听 4000 端口，用户可在浏览器中通过地址 <code>localhost:4000</code> 访问博客。</p>
<p><img src="/2018/02/09/hexo/hexo-get-start/hexo_default.png" alt></p>
<p>此外，可以通过添加命令行参数来支持高级用法：</p>
<ul>
<li>当 4000 端口已被其他应用占用时，可以添加 <code>-p</code> / <code>--port</code> 参数来设置 Web 服务监听的端口号，如<code>hexo s -p 8000</code></li>
<li>默认情况下，hexo 监听项目目录的文件变化，用户对于项目文件的任何改动都会触发实时解析编译并更新内存中的网页资源，也就是说，用户在本地修改后刷新浏览器就可以看到改动效果。如果不希望 hexo 监听项目目录的文件变化，可以添加 <code>-s</code> / <code>--static</code> 参数，这样本地改动就不会触发 hexo 实时解析更新。</li>
</ul>
<h1 id="geng-huan-next-zhu-ti">更换 Next 主题</h1>
<p>Next 作为一款符合广大程序员审美的主题，还是有着较高的出场率的。Hexo 中切换主题的方式非常简单，只需要将主题文件拷贝至根目录下的 <code>themes</code> 文件夹中， 然后修改 <code>_config.yml</code> 文件中的 <code>theme</code> 字段即可。</p>
<p>在根目录下执行以下命令下载主题文件：</p>
<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line">$ git clone <span class="symbol">https:</span>/<span class="regexp">/github.com/theme</span>-<span class="keyword">next</span>/hexo-theme-<span class="keyword">next</span>.git themes/<span class="keyword">next</span></span><br></pre></td></tr></table></figure>
<p>也可以在 <a href="https://github.com/theme-next/hexo-theme-next/releases" target="_blank" rel="noopener">NexT 版本发布页面</a> 手动下载然后解压到根目录下的 <code>theme</code> 文件夹下，并将文件夹命名为 <code>next</code> 。这里可以看到 <code>theme</code> 文件夹下已经有一个名为 <code>landscape</code> 的文件夹了，这就是默认主题了。</p>
<p><img src="/2018/02/09/hexo/hexo-get-start/next_download.png" alt></p>
<p>打开站点配置文件，将 <code>theme</code> 字段的值修改为 <code>next</code>。</p>
<figure class="highlight xml"><figcaption><span>_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">theme: next</span><br></pre></td></tr></table></figure>
<p>这个时候刷新浏览器页面并不会发生变化，需要重启服务器并刷新才能使主题生效。如果重启服务器仍无效，尝试使用 <code>hexo clean</code> 清除缓存.</p>
<p>Next 默认主题风格为 Muse，用户可以在主题配置文件中修改 <code>scheme</code> 字段以选择自己喜欢的主题风格：</p>
<figure class="highlight xml"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"># Schemes</span><br><span class="line">#scheme: Muse</span><br><span class="line">#scheme: Mist</span><br><span class="line">#scheme: Pisces</span><br><span class="line">scheme: Gemini</span><br></pre></td></tr></table></figure>
<h1 id="zhan-dian-you-hua">站点优化</h1>
<p>根目录下的 _config.yml 文件负责站点的相关配置，用户可以通过修改该文件来自定义站点内容或功能，修改后需要重启服务器才能看到效果。</p>
<p>本节通过修改站点配置文件完善了网站标题、网站描述、社交链接、站点版权信息、友情链接等，效果如下图：</p>
<p><img src="/2018/02/09/hexo/hexo-get-start/info.png" alt></p>
<h2 id="wan-shan-zhan-dian-ji-chu-xin-xi">完善站点基础信息</h2>
<p>在站点配置文件中完善网站基本信息：</p>
<figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line"><span class="string">title:</span> 火种<span class="number">2</span>号</span><br><span class="line"><span class="string">subtitle:</span> <span class="string">'火种计划'</span></span><br><span class="line"><span class="string">description:</span> <span class="string">'但行好事，莫问前程！'</span></span><br><span class="line"><span class="string">keywords:</span></span><br><span class="line"><span class="string">author:</span> Li Zhen</span><br><span class="line"><span class="string">language:</span> zh-CN</span><br><span class="line"><span class="string">timezone:</span> Asia/Shanghai</span><br></pre></td></tr></table></figure>
<p>每个字段的冒号与值之间需要<strong>间隔一个空格</strong>。</p>
<h2 id="shou-ye-xian-shi-wen-zhang-zhai-yao">首页显示文章摘要</h2>
<p>根据默认的主题配置，首页将会显示每一篇文章的全文，如果想要只显示文章摘要，可以在主题配置文件中做出如下更改：</p>
<figure class="highlight python"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">auto_excerpt:</span><br><span class="line">  enable: true  <span class="comment"># 开启自动摘要提取</span></span><br><span class="line">  length: <span class="number">150</span></span><br></pre></td></tr></table></figure>
<p>此时将会从文章中提取 150 个字符作为摘要。</p>
<p>用户可以在文章中通过 <code>&lt;!--more--&gt;</code> 标记来精确划分摘要信息，标记之前的段落将作为摘要显示在首页。</p>
<p>如果在文章的 Front-Matter 中有非空的 <code>description</code> 字段，则该字段的内容会被作为摘要显示在首页。</p>
<h2 id="xiu-gai-zhan-dian-ye-jiao">修改站点页脚</h2>
<p>在主题配置文件中修改网站页脚信息：</p>
<figure class="highlight python"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">footer:  <span class="comment"># 底部信息区</span></span><br><span class="line">  <span class="comment"># Specify the date when the site was setup. If not defined, current year will be used.</span></span><br><span class="line">  since: <span class="number">2018</span> <span class="comment"># 建站时间</span></span><br><span class="line"></span><br><span class="line">  ages:</span><br><span class="line">    <span class="comment"># site running time</span></span><br><span class="line">    enable: true</span><br><span class="line">    <span class="comment"># birthday of your site</span></span><br><span class="line">    birthday: <span class="number">201810628</span></span><br><span class="line">    <span class="comment"># color of number</span></span><br><span class="line">    color: <span class="string">"#1890ff"</span></span><br><span class="line">  <span class="comment"># Icon between year and copyright info.</span></span><br><span class="line">  icon:</span><br><span class="line">    <span class="comment"># Icon name in Font Awesome. See: https://fontawesome.com/icons</span></span><br><span class="line">    name: fa fa-heart <span class="comment"># 图标名称</span></span><br><span class="line">    <span class="comment"># If you want to animate the icon, set it to true.</span></span><br><span class="line">    animated: true <span class="comment"># 开启动画</span></span><br><span class="line">    <span class="comment"># Change the color of icon, using Hex Code.</span></span><br><span class="line">    color: <span class="string">"#ff0000"</span>  <span class="comment"># 图标颜色</span></span><br></pre></td></tr></table></figure>
<p>更改后效果如下：</p>
<p><img src="/2018/02/09/hexo/hexo-get-start/footer.png" alt></p>
<h2 id="xiu-gai-wang-zhan-favicon">修改网站 Favicon</h2>
<p>Favicon 即浏览器标签左侧的图标。下载自己喜欢的图标置于 <code>themes\next\source\images\</code> 目录下，命名方式参考主题配置文件中的 <code>favicon</code> 字段。</p>
<figure class="highlight python"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">favicon:</span><br><span class="line">  small: /images/snoppy.jpeg</span><br><span class="line">  medium: /images/snoppy.jpeg</span><br><span class="line">  apple_touch_icon: /images/snoppy.jpeg</span><br><span class="line">  safari_pinned_tab: /images/logo.svg</span><br><span class="line">  <span class="comment">#android_manifest: /images/manifest.json</span></span><br><span class="line">  <span class="comment">#ms_browserconfig: /images/browserconfig.xml</span></span><br></pre></td></tr></table></figure>
<h2 id="tian-jia-you-lian">添加友链</h2>
<p>在主题配置文件中修改相应字段：</p>
<figure class="highlight less"><table><tr><td class="code"><pre><span class="line"><span class="attribute">links</span>:</span><br><span class="line">  # <span class="attribute">Title</span>: <span class="attribute">https</span>:<span class="comment">//github.com/jeffery0628</span></span><br></pre></td></tr></table></figure>
<h2 id="tian-jia-she-jiao-lian-jie">添加社交链接</h2>
<p>用户可以在主题配置文件中根据样例提示添加个人社交软件链接：</p>
<figure class="highlight python"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">social:</span><br><span class="line">  GitHub: hhttps://github.com/jeffery0628 || fab fa-github</span><br><span class="line">  邮箱: mailto:jeffery.lee<span class="number">.0628</span>@gmail.com || fa fa-envelope</span><br><span class="line">  <span class="comment">#Weibo: https://weibo.com/yourname || fab fa-weibo</span></span><br><span class="line">  <span class="comment">#Google: https://plus.google.com/yourname || fab fa-google</span></span><br><span class="line">  <span class="comment">#Twitter: https://twitter.com/yourname || fab fa-twitter</span></span><br><span class="line">  <span class="comment">#FB Page: https://www.facebook.com/yourname || fab fa-facebook</span></span><br><span class="line">  <span class="comment">#StackOverflow: https://stackoverflow.com/yourname || fab fa-stack-overflow</span></span><br><span class="line">  <span class="comment">#YouTube: https://youtube.com/yourname || fab fa-youtube</span></span><br><span class="line">  <span class="comment">#Instagram: https://instagram.com/yourname || fab fa-instagram</span></span><br><span class="line">  <span class="comment">#Skype: skype:yourname?call|chat || fab fa-skype</span></span><br><span class="line"></span><br><span class="line">social_icons:</span><br><span class="line">  enable: true <span class="comment"># 显示社交软件图标</span></span><br><span class="line">  icons_only: false <span class="comment"># 显示图标的同时显示文字</span></span><br><span class="line">  transition: false</span><br></pre></td></tr></table></figure>
<h2 id="tian-jia-ban-quan-xie-yi">添加版权协议</h2>
<p>在主题配置文件中开启相关字段并选择知识共享协议：</p>
<figure class="highlight python"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">creative_commons: by-nc-sa</span><br></pre></td></tr></table></figure>
<h2 id="dian-ji-tou-xiang-hui-dao-shou-ye">点击头像回到首页</h2>
<p>修改侧边栏模板代码:</p>
<figure class="highlight diff"><figcaption><span>themes\next\layout\_partials\sidebar\site-overview.swig</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;% if theme.avatar.url %&#125;</span><br><span class="line"><span class="addition">+   &lt;a href="/"&gt;</span></span><br><span class="line">      &lt;img class="site-author-image" itemprop="image"</span><br><span class="line">        src="&#123;&#123; url_for( theme.avatar.url | default(theme.images + '/avatar.gif') ) &#125;&#125;"</span><br><span class="line">        alt="&#123;&#123; author &#125;&#125;" /&gt;</span><br><span class="line"><span class="addition">+   &lt;/a&gt;</span></span><br><span class="line">  &#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<h1 id="wen-zhang-ye-you-hua">文章页优化</h1>
<p>主题目录下的 themes\next_config.yml 文件负责与主题相关的配置，用户可以通过修改该文件来自定义与主题相关的内容或功能，修改后刷新浏览器即可即时生效。</p>
<h2 id="xiu-gai-wen-zhang-mu-lu-dao-hang">修改文章目录导航</h2>
<p>默认情况下文章的多级目录是折叠的，点击才会触发下级菜单的展开，并且并且同时只能展开一个目录分支，这会造成在点击不同目录标题的时候目录跳来跳去。如果你想实现默认展开全部目录的功能，可以在自定义样式文件中添加以下代码：</p>
<figure class="highlight python"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line">toc:</span><br><span class="line">  enable: true</span><br><span class="line">  <span class="comment"># Automatically add list number to toc.</span></span><br><span class="line">  number: true</span><br><span class="line">  <span class="comment"># If true, all words will placed on next lines if header width longer then sidebar width.</span></span><br><span class="line">  wrap: false</span><br><span class="line">  <span class="comment"># If true, all level of TOC in a post will be displayed, rather than the activated part of it.</span></span><br><span class="line">  expand_all: true <span class="comment"># 自动展开所有</span></span><br><span class="line">  <span class="comment"># Maximum heading depth of generated toc.</span></span><br><span class="line">  max_depth: <span class="number">4</span> <span class="comment"># 展开深度</span></span><br></pre></td></tr></table></figure>
<h2 id="xiu-gai-wen-zhang-meta-xin-xi">修改文章 meta 信息</h2>
<p>默认主题配置中，标题下方会显示文章的创建时间、文章的修改时间、文章分类信息等元数据，用户可以在主题配置文件中自定义设置需要显示的 meta 元信息：</p>
<figure class="highlight python"><figcaption><span>themes\next\_config.yml</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># 显示在 home 页的文章创建于、更新于、阅读次数之类的数据</span></span><br><span class="line">post_meta:</span><br><span class="line">  item_text: true <span class="comment"># 显示文字说明</span></span><br><span class="line">  created_at: false <span class="comment"># 显示文章创建时间</span></span><br><span class="line">  updated_at:</span><br><span class="line">    enable: true <span class="comment"># 文章修改时间</span></span><br><span class="line">    another_day: true <span class="comment"># 更新日期显示规则，只有更新日期与创建日期不同时，才会显示</span></span><br><span class="line">  categories: true <span class="comment"># post分类信息</span></span><br></pre></td></tr></table></figure>
<h2 id="zhong-ying-wen-zhi-jian-zi-dong-tian-jia-kong-ge">中英文之间自动添加空格</h2>
<p>该功能由 <a href="https://github.com/vinta/pangu.js" target="_blank" rel="noopener">pangu</a> 提供，在根目录下执行如下命令克隆插件到项目中：</p>
<figure class="highlight crystal"><table><tr><td class="code"><pre><span class="line">$ git clone <span class="symbol">https:</span>/<span class="regexp">/github.com/theme</span>-<span class="keyword">next</span>/theme-<span class="keyword">next</span>-pangu.git themes/<span class="keyword">next</span>/source/<span class="class"><span class="keyword">lib</span>/<span class="title">pangu</span></span></span><br></pre></td></tr></table></figure>
<p>在主题配置文件中设置 <code>pangu: true</code> 即可启用该动能。</p>
<h1 id="jie-shu-yu">结束语</h1>
<div class="note warning">
            <p>不同版本的 Hexo 和 Next 主题之间配置项可能存在差异，本系列文章中的配置有效性以 Hexo v3.7.1 和 Next v7.8.0 为准。</p>
          </div>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">Hexo 官方文档</a></li>
<li><a href="https://theme-next.iissnan.com/getting-started.html" target="_blank" rel="noopener">NexT 使用文档</a></li>
<li><a href="https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html" target="_blank" rel="noopener">reuixiy | 打造个性超赞博客 Hexo+NexT+GitHubPages 的超深度优化</a></li>
<li><a href="http://yearito.cn/posts/hexo-get-started.html" target="_blank" rel="noopener">Hexo 搭建个人博客系列：基础建站篇</a></li>
</ol>
]]></content>
      <categories>
        <category>技术/博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
</search>
