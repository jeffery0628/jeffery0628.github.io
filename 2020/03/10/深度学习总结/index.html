<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/snoppy.jpeg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open Sans:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jeffery.ink","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":15,"offset":12,"onmobile":true,"dimmer":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":true,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":true,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="知识点 神经元(感知器)  感知器是一种人工神经元.它接受几个二进制输出并产生一个二进制输入.如果引入权重和阈值,那么感知器的参数可以表示为:\(f(x)&#x3D;sign(wx+b)\) 感知器是单输出的,但这个单输出可以被用于多个其它感知器的输入. 感知器可以很容易地计算基本的逻辑功能,如与,或,与非.所以感知器网络可以计算任何逻辑功能 使感知器能够自动调整权重和偏置的学习算法是神经网络有别于传统逻辑">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习总结">
<meta property="og:url" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="火种2号">
<meta property="og:description" content="知识点 神经元(感知器)  感知器是一种人工神经元.它接受几个二进制输出并产生一个二进制输入.如果引入权重和阈值,那么感知器的参数可以表示为:\(f(x)&#x3D;sign(wx+b)\) 感知器是单输出的,但这个单输出可以被用于多个其它感知器的输入. 感知器可以很容易地计算基本的逻辑功能,如与,或,与非.所以感知器网络可以计算任何逻辑功能 使感知器能够自动调整权重和偏置的学习算法是神经网络有别于传统逻辑">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/bp.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/rnn.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/birnn.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/lstm_c.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/lstm_forget_gate.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/input_gate.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/output_gate.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/lstm_all.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/GRU.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/word_embedding.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/CBOW.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/skipgram.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/svm_hierarchical.gif">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/cbow_hierarchical.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/glove.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/EMLO.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/emlo_embedding.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/gpt_1.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/gpt_2.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/bert_gpt_elmo.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/bert_tokenize.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/transformer.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention-1.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention-2.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention-3.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention-4.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/multi-head.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/multi-head-1.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/transformer-xl.png">
<meta property="article:published_time" content="2020-03-10T12:05:35.000Z">
<meta property="article:modified_time" content="2020-05-08T09:15:42.155Z">
<meta property="article:author" content="Li Zhen">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="自然语言处理">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/bp.png">

<link rel="canonical" href="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>深度学习总结 | 火种2号</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">火种2号</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">火种计划</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>简历</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>读书</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-video fa-fw"></i>电影</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>



</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jeffery.ink/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/snoppy.jpeg">
      <meta itemprop="name" content="Li Zhen">
      <meta itemprop="description" content="他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="火种2号">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习总结
        </h1>

        <div class="post-meta">
          
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-08 17:15:42" itemprop="dateModified" datetime="2020-05-08T17:15:42+08:00">2020-05-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          
            <span id="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习总结" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论：</span>
    
    <a title="valine" href="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>23k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>21 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="zhi-shi-dian">知识点</h3>
<h4 id="shen-jing-yuan-gan-zhi-qi">神经元(感知器)</h4>
<ol>
<li>感知器是一种人工<strong>神经元</strong>.它接受几个<strong>二进制输出</strong>并产生一个<strong>二进制输入</strong>.如果引入<strong>权重</strong>和<strong>阈值</strong>,那么感知器的参数可以表示为:\(f(x)=sign(wx+b)\)</li>
<li>感知器是<strong>单输出</strong>的,但这个单输出可以被用于<strong>多个</strong>其它感知器的输入.</li>
<li>感知器可以很容易地计算基本的<strong>逻辑功能</strong>,如<strong>与</strong>,<strong>或</strong>,<strong>与非</strong>.所以感知器网络可以计算任何逻辑功能</li>
<li>使感知器能够自动调整权重和偏置的<strong>学习算法</strong>是神经网络有别于传统逻辑门的关键.</li>
</ol>
<h4 id="s-xing-shen-jing-yuan">S型神经元</h4>
<ol>
<li>网络中单个感知器上权重或偏置的<strong>微小改动</strong>可能会引起<strong>输出翻转</strong>,从而导致其余网络的行为改变.所以<strong>逐步修改</strong>权重和偏置来让输出接近期望很困难,所以引入了<strong>S型神经元(逻辑神经元)</strong></li>
<li>S型神经元和感知器类似,但是权重和偏置的微小改动只引起输出的<strong>微小变化</strong>.S型神经元的输入可以是<strong>0和1中的任意值</strong>,输出是\(σ(wx+b)\),其中\(σ\)被称为s型函数(逻辑函数).</li>
<li>σ函数是阶跃函数的<strong>平滑</strong>版本.这意味着权重和偏置的微小变化会产生一个微小的输出变化,\(\Delta output \approx \sum_{j} \frac{\partial \text { output }}{\partial w_{j}} \Delta w_{j}+\frac{\partial \text { output }}{\partial b} \Delta b\),这意味着输出的变化是权重和偏置的变化的<strong>线性函数</strong>.</li>
</ol>
<a id="more"></a>
<h4 id="ti-du-xia-jiang-suan-fa">梯度下降算法</h4>
<ol>
<li>
<p>神经网络中经常需要<strong>大量</strong>变量,因此采用<strong>梯度下降法</strong>来计算代价函数最小值:重复计算梯度,然后沿着<strong>相反</strong>的方向移动\(v \rightarrow v^{\prime}=v-\eta \nabla C\)</p>
</li>
<li>
<p>假如需要计算每个训练输入的梯度值,训练速度会相当缓慢.因此引入了<strong>随机梯度下降</strong>,只计算小批量数据(mini-batch)的梯度值来求平均值<br>
\[
\begin{aligned}
w_{k} \rightarrow w_{k}^{\prime} &amp;=w_{k}-\frac{\eta}{m} \sum_{j} \frac{\partial C_{X_{j}}}{\partial w_{k}} \\
b_{l} \rightarrow b_{l}^{\prime} &amp;=b_{l}-\frac{\eta}{m} \sum_{j} \frac{\partial C_{X_{j}}}{\partial b_{l}}
\end{aligned}
\]</p>
</li>
<li>
<p>随机机梯度下降不断地选定小批量数据来进行训练,直到用完了所有的训练输入,就称为完成了一个<strong>迭代期(epoch)</strong>,然后会开始一个新的迭代期.</p>
</li>
</ol>
<h4 id="fan-xiang-chuan-bo-bp-suan-fa">反向传播(BP)算法</h4>
<h5 id="shen-jing-wang-luo-zhong-shi-yong-ju-zhen-kuai-su-ji-suan-shu-chu-de-fang-fa">神经网络中使用矩阵快速计算输出的方法</h5>
<ol>
<li>\(w^l_{jk}\)表示从第\((l-1)\)层的第\(k\)个神经元到第\(l\)层的第\(j\)个神经元的链接上的<strong>权重</strong>,\(b^l_j\)表示在第l层第j个神经元的<strong>偏置</strong>.\(a^l_j\)表示第l层第j个神经元的<strong>激活值</strong>.由此得到了激活值之间的关系\(a_{j}^{l}=\sigma\left(\sum_{k} w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\right)\)</li>
<li>对每一层l定义<strong>权重矩阵</strong>(其中第j行第k列的元素是\(w^l_{jk}\),<strong>偏置向量</strong>(每个元素是\(b^l_j\))和<strong>激活向量</strong>(每个元素是\(a^l_j\)).所以上式可以改写为\(a^{l}=\sigma\left(w^{l} a^{l-1}+b^{l}\right)\),其中σ是<strong>向量化函数</strong>(作用σ到向量中的每个元素).中间量\(z^{l} \equiv w^{l} a^{l-1}+b^{l}\)称为<strong>带权输</strong>入(每个元素是第l层第j个神经元的激活函数的带权输入**)**.</li>
</ol>
<h5 id="guan-yu-dai-jie-han-shu-de-liang-ge-jia-she">关于代价函数的两个假设</h5>
<p><strong>反向传播</strong>的目标是计算代价函数关于w和b的<strong>偏导数</strong>.为了让反向传播可行,需要作出两个主要假设</p>
<ol>
<li>代价函数可以被写成一个在每个训练样本x上的代价函数的<strong>均值</strong>\(C=\frac{1}{n} \sum_{x} C_{x}\)</li>
<li>代价可以写成神经网络输出的函数,以二次代价函数为例\(C=\frac{1}{2}\left\|y-a^{L}\right\|^{2}=\frac{1}{2} \sum_{j}\left(y_{j}-a_{j}^{L}\right)^{2}\)</li>
</ol>
<h5 id="hadamard-cheng-ji-s-t">Hadamard乘积,s⊙t</h5>
<p>假设s和t是两个同样维度的向量,那么s⊙t表示<strong>按元素的乘积</strong>,称为<strong>Hadamard乘积</strong></p>
<h5 id="fan-xiang-chuan-bo-de-si-ge-ji-ben-fang-cheng">反向传播的四个基本方程</h5>
<p>在第l层第j个神经元上的<strong>误差</strong>,被定义为\(\delta_{j}^{l} \equiv \frac{\partial C}{\partial z_{j}^{l}}\)</p>
<p><strong>四个基本方程</strong></p>
<ol>
<li>
<p>输出层误差的方程: \(\delta_{j}^{L} = \frac{\partial C}{\partial a^L_j}\frac{\partial a^L_j}{\partial z^l_j} = \frac{\partial C}{\partial a_{j}^{L}} \sigma^{\prime}\left(z_{j}^{L}\right)\)</p>
<p>右式第⼀个项 \(\frac{\partial C}{\partial a^L_j}\) 表⽰代价随着 \(j^{th}\)输出激活值的变化⽽ 变化的速度。假如 \(C\) 不太依赖⼀个特定的输出神经元 \(j\)，那么\(\delta^L_j\)就会很⼩，这也是我们想要的效果。右式第⼆项 \(\sigma^\prime(Z^L_j)\)刻画了在\(z^L_j\) 处激活函数 \(\sigma\) 变化的速度。</p>
</li>
<li>
<p>使用下一层的误差来表示当前层的误差:\(\delta^{l}=\left(\left(w^{l+1}\right)^{T} \delta^{l+1}\right) \odot \sigma^{\prime}\left(z^{l}\right)\)</p>
<p>由递推公式<br>
\[
\begin{aligned}
\frac{\partial  C}{\partial  z^l_j} &amp;=\delta^l_j \\
&amp; = \sum_i{\frac{\partial  C}{\partial  z^{l+1}_i}\frac{\partial  z^{l+1}_i}{\partial  z^l_j}} \\ 
&amp;=\sum_i{\delta ^{l+1}_i \frac{\partial  z^{l+1}_i}{\partial  a^l_j}\frac{\partial  a^l_j}{\partial  z^l_j}} \\ 
&amp;=\sum_i{\delta^{l+1}_i w^{l+1}_{ji} \sigma \prime (z^l_j)} \\
&amp;=(w^{l+1}_j)^T\delta^{l+1}\sigma \prime(z^l_j) \\
&amp;=(w^{l+1})^T \sigma^{l+1}\bigodot \sigma \prime (z^l)
\end{aligned}
\]</p>
</li>
<li>
<p>代价函数关于网络中任意偏置的改变率:\(\frac{\partial C}{\partial b_{j}^{l}}=\delta_{j}^{l}\)</p>
</li>
<li>
<p>代价函数关于任何一个权重的改变率:\(\frac{\partial C}{\partial w_{j k}^{l}}=a_{k}^{l-1} \delta_{j}^{l}\)（因为\(z^l = w^la^{l-1}+b^l\)）</p>
</li>
</ol>
<h5 id="suan-fa-miao-shu">算法描述</h5>
<p>反向传播方程给出了一种计算代价函数梯度的方法:</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/bp.png" alt="avatar"></p>
<ol>
<li><strong>输入x</strong>:为输入层设置对应的激活值\(a1\)</li>
<li><strong>前向传播</strong>:对从前往后对每层计算相应的\(z=wa+b\)和\(a=σ(z)\)</li>
<li><strong>输出层误差</strong>:根据BP1计算误差向量</li>
<li><strong>反向误差传播</strong>:对后往前根据BP2对每层计算误差向量</li>
<li><strong>输出</strong>:根据BP3和BP4计算代价函数的梯度.</li>
</ol>
<h5 id="dai-ma">代码</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">    <span class="string">"""Return a tuple "(nabla_b, nabla_w)" representing the</span></span><br><span class="line"><span class="string">    gradient for the cost function C_x.</span></span><br><span class="line"><span class="string">    "nabla_b" and "nabla_w" are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">    to "self.biases" and "self.weights"."""</span></span><br><span class="line"></span><br><span class="line">    nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">    nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># feedforward</span></span><br><span class="line">    activation = x</span><br><span class="line">    activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">    zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">      z = np.dot(w, activation)+b</span><br><span class="line">      zs.append(z)</span><br><span class="line">      activation = sigmoid(z)</span><br><span class="line">      activations.append(activation)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward pass</span></span><br><span class="line">    delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * sigmoid_prime(zs[<span class="number">-1</span>])</span><br><span class="line">    nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">    nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line"><span class="comment"># differently to the notation in Chapter 2 of the book.</span></span><br><span class="line"><span class="comment"># Here,l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line"><span class="comment"># second-last layer, and so on.It's a renumbering of the</span></span><br><span class="line"><span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line"><span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">    	z = zs[-l]</span><br><span class="line">      sp = sigmoid_prime(z)</span><br><span class="line">    	delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">    	nabla_b[-l] = delta</span><br><span class="line">    	nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span><br><span class="line">    <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></span><br><span class="line">  <span class="string">"""Return the vector of partial derivatives \partial C_x /</span></span><br><span class="line"><span class="string">  \partial a for the output activations."""</span></span><br><span class="line">  	<span class="keyword">return</span> (output_activations-y)</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line"><span class="string">"""The sigmoid function."""</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line"><span class="string">"""Derivative of the sigmoid function."""</span></span><br><span class="line">  <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure>
<h4 id="jiao-cha-shang-dai-jie-han-shu">交叉熵代价函数</h4>
<p>人类通常在<strong>犯错比较明显</strong>的时候学习的<strong>速度最快</strong>.而神经元在这种<strong>饱和</strong>情况下学习很有<strong>难度</strong>(也就是偏导数很小),这是因为<strong>二次代价函数</strong>关于权重和偏置的<strong>偏导数</strong>是\(\frac{\partial C}{\partial w}=(a-y) \sigma^{\prime}(z) x=a \sigma^{\prime}(z)\)和\(\frac{\partial C}{\partial b}=(a-y) \sigma^{\prime}(z)=a \sigma^{\prime}(z)\)而σ函数的<strong>导数</strong>在接近0和1时都很小。</p>
<p>解决上述问题的方法是引入<strong>交叉熵代价函数</strong>:\(C=-\frac{1}{n} \sum_{x}[y \ln a+(1-y) \ln (1-a)]\)</p>
<ol>
<li>它是<strong>非负</strong>的,并且当实际输出接近目标值时它<strong>接近0</strong>,因此可以作为代价函数.</li>
<li>它关于权重的<strong>偏导数</strong>是\(\frac{\partial C}{\partial w_{j}}=\frac{1}{n} \sum_{x} x_{j}(\sigma(z)-y)\),也就是误差越大,学习速度越快.</li>
<li>如果输出层是<strong>线性神经元</strong>,那么<strong>二次代价函数</strong>不再会导致学习速度下降的问题,可以选用.如果输出神经元是<strong>S型神经元</strong>,<strong>交叉熵</strong>一般都是更好的选择.</li>
<li>对于多分类交叉熵代价函数为：\(C=-\sum_{j=1}^{T} y_{j} \log {p_j}\),</li>
</ol>
<h4 id="guo-ni-he">过拟合</h4>
<ol>
<li>最好的降低过拟合的方式就是<strong>增加训练样本量</strong>,但训练数据其实是很<strong>难得</strong>的资源.</li>
<li><strong>规范化</strong>也是一种缓解过拟合的技术.效果是让网络倾向于学习<strong>小一点的权重</strong>,它是寻找小的权重和最小化原始代价函数之间的折中,相对重要性由λ控制.
<ol>
<li><strong>L2规范化(权重衰减)<strong>的想法是增加一个额外的</strong>规范化项</strong>：\(\frac{\lambda}{2 n} \sum_{w} w^{2}\)到代价函数上.其中λ是<strong>规范化参数</strong>,注意规范化项里<strong>不包含偏置</strong>.</li>
<li><strong>L1规范化</strong>的规范化项为：\(\frac{\lambda}{n} \sum_{w}|w|\)在L1规范化中,权重通过一个常量<strong>向0</strong>进行缩小,在L2规范化中,权重通过一个和w成<strong>比例</strong>的量进行缩小.所以L1规范化倾向于<strong>聚集</strong>网络的权重在<strong>相对少量</strong>的高重要度连接上.</li>
</ol>
</li>
<li><strong>dropout</strong></li>
<li><strong>人为扩展训练数据</strong></li>
</ol>
<h4 id="diao-can">调参</h4>
<ol>
<li><strong>学习速率η</strong></li>
<li><strong>早停</strong></li>
<li><strong>规范化参数λ</strong></li>
<li><strong>小批量数据大小</strong></li>
<li><strong>网格搜索(grid search)</strong></li>
</ol>
<h3 id="xun-huan-shen-jing-wang-luo">循环神经网络</h3>
<h4 id="rnn">RNN</h4>
<p>对于前馈神经网络来说其公式：<br>
\[
O=f(W*X+b)
\]<br>
其中\(W\)和\(b\)是模型的参数，\(X\)是当前的输入，\(f(·)\)是激活函数，\(*\)是矩阵乘法，\(O\)是当前的输出。即输出等于输入经过线性与非线性映射后的结果。</p>
<p>而RNN通过将前一时刻的运算结果添加到当前的运算中，从而实现了“考虑上文信息”的功能。<br>
\[
\mathrm{O}_{\mathrm{t}}=\mathrm{f}\left(\mathrm{X} * \mathrm{W}+\mathrm{O}_{\mathrm{t}-1} * \mathrm{V}+\mathrm{b}\right)
\]<br>
其中\(W,V,b\)是模型的参数，下标\(t\)代表当前的序列位置／时间点，\(t-1\)代表上个位置/上个时间点，\(X\)是当前的输入，\(f(·)\)是激活函数，\(*\)是矩阵乘法，\(O\)是模型输出。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/rnn.png" alt="avatar"></p>
<p>BiRnn:RNN可以考虑上文的信息，那么如何将下文的信息也添加进去呢？这就是BiRNN要做的事情。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/birnn.png" alt="avatar"></p>
<h5 id="rnn-cun-zai-de-wen-ti">RNN存在的问题</h5>
<p>首先，从RNN的前向过程来看，可以认为它只有一层权重矩阵W（先不管V）。由此可见从深层的角度去看RNN的前向过程，就可以认为RNN是<strong>各个层的权重矩阵相同</strong>的深层网络。忽略V和激活函数，就可以近似的认为网络一共有T层（T等于序列的长度），那么第t层的输出就是连乘t次W，也就是\(W^t\)！,由于<strong>矩阵可以用它的特征值矩阵和特征向量矩阵去近似</strong>，即<br>
\[
W = V diag(\lambda) V^{-1}  \\
W^t = (V diag(\lambda) V^{-1})(V diag(\lambda) V^{-1})\cdots(V diag(\lambda) V^{-1})=V{ diag(\lambda)}^t V^{-1}
\]<br>
也就是说，**特征值矩阵中的每个特征值都会随着t的增大发生指数级变化！**所以某个特征值大于1时，就容易导致这个维度的值爆炸性增长；当特征值小于1时，会就会导致这个维度的值指数级速度衰减为0！</p>
<p>前向过程如此，误差反向传播的过程也必然近似为输出层的误差会乘以\(W^t\)来得到倒数第t层的梯度，然而由于刚各个维度不是指数级衰减就是指数级爆炸，很容易看出当更新RNN的靠前的层的时候（即离着输出层较远的那些层，或者说与序列末端离得远的位置），计算出的梯度要么大的吓人，要么小的忽略。小的忽略的值不会对W的更新有任何指导作用，大的吓人的值一下子就把W中的某些值弄的大的吓人了，害得前面的优化都白做了。</p>
<p>一个很简单的想法是进行<strong>梯度截断</strong>，在优化RNN的参数的时候，给梯度值设置一个上限。但是这样显然就会导致模型难以再顾及很靠前的历史信息了，因此理论上RNN可以保存任意长的历史信息来辅助当前时间点的决策，然而由于在优化时（训练RNN时），梯度无法准确合理的传到很靠前的时间点，因此RNN实际上只能记住并不长的序列信息（在NLP中，经验上认为序列大于30的时候，RNN基本记不住多于30的部分，而从多于10的位置开始就变得记性很差了），因此RNN相比前馈网络，可以记住的历史信息要长一些，但是无法记住长距离的信息（比如段落级的序列甚至篇章级的序列，用RNN就鸡肋了）。</p>
<h4 id="lstm">LSTM</h4>
<p>因为RNN存在梯度弥散和梯度爆炸的问题，所以RNN很难完美地处理具有长期依赖的信息。既然仅仅依靠一条线连接后面的神经元不足以解决问题，那么就再加一条线好了，这就是LSTM。</p>
<p>LSTM的关键在于细胞的状态和穿过细胞的线，细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流动保持不变会变得容易。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/lstm_c.png" alt="avatar"></p>
<p>在LSTM中，门可以实现选择性的让信息通过，主要通过一个sigmoid的神经层和一个逐点相乘的操作来实现。LSTM通过三个这样的门结构来实现信息的保护和控制，分别是遗忘门（forget gate）、输入门（input gate）与输出门（output gate）。</p>
<h5 id="strong-yi-wang-men-strong"><strong>遗忘门</strong></h5>
<p>在LSTM中的第一步是决定从细胞状态中丢弃什么信息。这个决定通过一个称为遗忘门的结构来完成。遗忘门会读取\(h_{t-1}\)和\(x_t\)，输出一个0到1之间的数值给细胞的状态\(c_{t-1}\)中的数字。1表示完全保留，0表示完全舍弃。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/lstm_forget_gate.png" alt="avatar"></p>
<h5 id="strong-shu-ru-men-strong"><strong>输入门</strong></h5>
<p>遗忘门决定让多少新的信息加入到cell的状态中来。实现这个需要两个步骤：</p>
<ol>
<li>首先一个叫“input gate layer”的sigmoid层决定哪些信息需要更新；一个tanh层生成一个向量，用来更新状态C。</li>
</ol>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/input_gate.png" alt="avatar"></p>
<ol start="2">
<li>把 1 中的两部分联合起来，对cell的状态进行更新，我们把旧的状态与\(f_t\)相乘，丢弃掉我们确定需要丢弃的信息，接着加上\(i_t * \tilde{C}_{t}\)</li>
</ol>
<h5 id="shu-chu-men">输出门</h5>
<p>最终，我们需要确定输出什么值，这个输出将会基于我们的细胞的状态，但是也是一个过滤后的版本。首先，我们通过一个sigmoid层来确定细胞状态的哪些部分将输出出去。接着，我们把细胞状态通过tanh进行处理（得到一个-1到1之间的值）并将它和sigmoid门的输出相乘，最终我们仅仅会输出我们确定输出的部分。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/output_gate.png" alt="avatar"></p>
<h5 id="gong-shi">公式</h5>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/lstm_all.png" alt="avatar"></p>
<h4 id="gru">GRU</h4>
<p>在LSTM中引入了三个门函数：输入门、遗忘门和输出门来控制输入值、记忆值和输出值。而在GRU模型中只有两个门：分别是更新门和重置门。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/GRU.png" alt="avatar"></p>
<p>图中的\(z_t\)和\(r_t\)分别表示更新门和重置门。更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多。重置门控制前一状态有多少信息被写入到当前的候选集 \(\tilde{h}_{t}\)上，重置门越小，前一状态的信息被写入的越少。</p>
<p>LSTM和GRU都是通过各种门函数来将重要特征保留下来，这样就保证了在long-term传播的时候也不会丢失。此外GRU相对于LSTM少了一个门函数，因此在参数的数量上也是要少于LSTM的，所以整体上GRU的训练速度要快于LSTM的。</p>
<h3 id="self-attention">Self-Attention</h3>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention.png" alt="avatar"></p>
<p><strong>核心思想</strong>：在decoding阶段对input中的信息赋予不同权重。</p>
<ol>
<li>Encode所有输入序列,得到对应的\(h_1,h_2, \cdots ,h_T\)(T为输入序列长度)</li>
<li>Decode输出目标\(y_t\)之前，会将上一步输出的隐藏状态\(S_{t-1}\)与之前encode好的\(h_1,h_2,\cdots,h_T\)进行比对，计算相似度（\(e_{t,j}=a(s_{t-1},h_j)\)）,\(h_j\)为之前第j个输入encode得到的隐藏向量，a为任意一种计算相似度的方式</li>
<li>然后通过softmax，即\(a_{t,j}=\frac{exp(e_{t,j})}{\sum^{T_x}_{k=1}exp(e_{t,k})}\)将之前得到的各个部分的相关系数进行归一化，得到\(a_{t,1},a_{t,2},\cdots,a_{t,T}\)</li>
<li>在对输入序列的隐藏层进行相关性加权求和得到此时decode需要的context vector ：\(c_j=\sum^{T_x}_{j=1}a_{i,j}h_j\)</li>
</ol>
<h3 id="ci-xiang-liang-mo-xing">词向量模型</h3>
<p>onehot编码方式的缺点：</p>
<ol>
<li>它是一个词袋模型，不考虑词与词之间的顺序（文本中词的顺序信息也是很重要的）</li>
<li>它假设词与词相互独立（在大多数情况下，词与词是相互影响的）</li>
<li>它得到的特征是离散稀疏的，实际应用中，面临着巨大的维度灾难问题</li>
</ol>
<p>将高维词向量嵌入到一个低维空间。word2vec是词嵌入方式的一种。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/word_embedding.png" alt="avatar"></p>
<p>主要解决两个问题：</p>
<ol>
<li>一个是统计语言模型里关注的条件概率\(𝑝(𝑤𝑡|𝑐𝑜𝑛𝑡𝑒𝑥𝑡)\)的计算</li>
<li>一个是向量空间模型里关注的词向量的表达</li>
</ol>
<h4 id="word-2-vec">word2vec</h4>
<h5 id="strong-c-bo-w-mo-xing-strong"><strong>CBoW模型</strong></h5>
<p>CBOW是已知上下文，估算当前词语的语言模型。其学习目标是最大化对数似然函数：<br>
\[
\mathcal{L}=\sum_{w \in \mathcal{C}} \log p(w | \text {Context}(w))
\]<br>
其中，w表示语料库C中任意一个词。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/CBOW.png" alt="avatar"></p>
<ol>
<li><strong>输入层</strong>是上下文的词语的词向量（在训练CBOW模型，词向量只是个副产品，确切来说，是CBOW模型的一个参数。训练开始的时候，词向量是个随机值，随着训练的进行不断被更新）。</li>
<li><strong>投影层</strong>对其求和，所谓求和，就是简单的向量加法。</li>
<li><strong>输出层</strong>输出最可能的w。由于语料库中词汇量是固定的|C|个，所以上述过程其实可以看做一个多分类问题。给定特征，从|C|个分类中挑一个。</li>
</ol>
<p>对于神经网络模型多分类，最朴素的做法是softmax回归：<br>
\[
h_{\theta}\left(x^{(i)}\right)=\left[\begin{array}{c}
p\left(y^{(i)}=1 | x^{(i)} ; \theta\right) \\
p\left(y^{(i)}=2 | x^{(i)} ; \theta\right) \\
\vdots \\
p\left(y^{(i)}=k | x^{(i)} ; \theta\right)
\end{array}\right]=\frac{1}{\sum_{j=1}^{k} e^{\theta_{j}^{T} x^{(i)}}}\left[\begin{array}{c}
e^{\theta_{1}^{T} x^{(i)}} \\
e^{\theta_{2}^{T} x^{(i)}} \\
\vdots \\
e^{\theta_{k}^{T} x^{(i)}}
\end{array}\right]
\]</p>
<h5 id="skip-gram-mo-xing">skip-gram模型</h5>
<p><strong>target word对context的预测中学习word vector</strong><br>
\[
p(\text {Context}(w) | w)=\prod_{u \in \text {Context}(w)} p(u | w)
\]</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/skipgram.png" alt="avatar"></p>
<p>如何将Skip-gram模型的前向计算过程写成数学形式，我们得到：<br>
\[
p\left(w_{o} | w_{i}\right)=\frac{e^{U_{o} \cdot V_{i}}}{\sum_{j} e^{U_{j} V_{i}}}
\]</p>
<p>其中，其中，\(V_i\)是Embedding层矩阵里的列向量，也被称为\(w_i\)的input vector。\(U_j\)是softmax层矩阵里的行向量，也被称为\(w_i\)的output vector。因此，Skip-gram模型的本质是<strong>计算输入word的input vector与目标word的output vector之间的余弦相似度，并进行softmax归一化</strong>。我们要学习的模型参数正是这两类词向量。</p>
<p>然而，直接对词典里的V个词计算相似度并归一化，显然是一件极其耗时的impossible mission。引入了两种优化算法：<strong>层次Softmax（Hierarchical Softmax）<strong>和</strong>负采样（Negative Sampling）</strong>。</p>
<h5 id="hierarchical-softmax">Hierarchical Softmax</h5>
<p>softmax回归需要对语料库中每个词语（类）都计算一遍输出概率并进行归一化，在几十万词汇量的语料上无疑是令人头疼的。在SVM中的多分类，其多分类是由二分类组合而来的：</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/svm_hierarchical.gif" alt="avatar"></p>
<p>这是一种二叉树结构，应用到word2vec中被作者称为Hierarchical Softmax：</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/cbow_hierarchical.jpg" alt="avatar"></p>
<p>上图输出层的树形结构即为Hierarchical Softmax。非叶子节点相当于一个神经元（感知机，逻辑斯谛回归就是感知机的输出代入\(f(x)=\frac{1}{1+e^x}\)，二分类决策输出1或0，分别代表向下左转或向下右转；每个叶子节点代表语料库中的一个词语，于是每个词语都可以被01唯一地编码，并且其编码序列对应一个事件序列，于是我们可以计算条件概率\(p(w|Context(x))\)。</p>
<p>在开始计算之前，还是得引入一些符号：</p>
<ol>
<li>\(p^w\)从根结点出发到达w对应叶子结点的路径.</li>
<li>\(l^w\)路径中包含结点的个数</li>
<li>\(p^w_1,p^w_2,\cdots,p^w_{l^w}\)路径\(p^w\)中的各个节点</li>
<li>\(d^w_2,d^w_3,\cdots,d^w_{l^w} \in {0,1}\)词w的编码，\(d^w_j\)表示路径\(p^w\)第j个节点对应的编码（根节点无编码）</li>
<li>\(\theta^w_1,\theta^w_2,\cdots,\theta^w_{l^w-1} \in R^m\)路径\(p^w\)中非叶节点对应的参数向量</li>
</ol>
<p>于是可以给出\(w\)的条件概率：<br>
\[
p(w | \text { Context }(w))=\prod_{j=2}^{l^{w}} p\left(d_{j}^{w} | \mathbf{x}_{w}, \theta_{j-1}^{w}\right)
\]</p>
<p>这是个简单明了的式子，从根节点到叶节点经过了\(l^w-1\)个节点，编码从下标2开始（根节点无编码），对应的参数向量下标从1开始（根节点为1）。其中，每一项是一个逻辑斯谛回归：</p>
<p>\[
p\left(d_{j}^{w} | \mathbf{x}_{w}, \theta_{j-1}^{w}\right)=\left\{\begin{array}{ll}
\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right), &amp; d_{j}^{w}=0 \\
1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right), &amp; d_{j}^{w}=1
\end{array}\right.
\]<br>
考虑到d只有0和1两种取值，我们可以用指数形式方便地将其写到一起：<br>
\[
p\left(d_{j}^{w} | \mathbf{x}_{w}, \theta_{j-1}^{w}\right)=\left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{1-d^w_{j}} \cdot\left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{d^w_{j}}
\]<br>
我们的目标函数取对数似然：<br>
\[
\mathcal{L}=\sum_{w \in \mathcal{C}} \log p(w | \text { Context }(w))
\]<br>
将\(p(w|Context(w))\)代入上式，有<br>
\[
\begin{aligned}
\mathcal{L} &amp;=\sum_{w \in \mathcal{C}} \log \prod_{j=2}^{l^{w}}\left\{\left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{1-d_{j}^{v}} \cdot\left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{d_{j}}\right\} \\
&amp;=\sum_{w \in \mathcal{C}} \sum_{j=2}^{l^{w}}\left\{\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]+d_{j}^{w} \cdot \log \left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]\right\}
\end{aligned}
\]<br>
这也很直白，连乘的对数换成求和。不过还是有点长，我们把每一项简记为：<br>
\[
\mathcal{L}(w, j)=\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]+d_{j}^{w} \cdot \log \left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]
\]<br>
怎么最大化对数似然函数呢？分别最大化每一项即可（这应该是一种近似，最大化某一项不一定使整体增大，具体收敛的证明还不清楚）。怎么最大化每一项呢？先求函数对每个变量的偏导数，对每一个样本，代入偏导数表达式得到函数在该维度的增长梯度，然后让对应参数加上这个梯度，函数在这个维度上就增长了。</p>
<p>每一项有两个参数，一个是每个节点的参数向量\(\theta^w_{j-1}\)，另一个是输出层的输入\(X_w\)，我们分别对其求偏导数：<br>
\[
\frac{\partial \mathcal{L}(w, j)}{\partial \theta_{j-1}^{w}}=\frac{\partial}{\partial \theta_{j-1}^{w}}\left\{\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]+d_{j}^{w} \cdot \log \left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]\right\}
\]<br>
因为sigmoid函数的导数有个很棒的形式：<br>
\[
\sigma^{\prime}(x)=\sigma(x)[1-\sigma(x)]
\]<br>
于是代入上上式得到：<br>
\[
\left(1-d_{j}^{w}\right)\left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right] \mathbf{x}_{w}-d_{j}^{w} \sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right) \mathbf{x}_{w}
\]<br>
合并同类项得到：<br>
\[
\left[1-d_{j}^{w}-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right] \mathbf{x}_{w}
\]<br>
于是\(\theta^w_{j-1}\)的更新表达式就得到了：<br>
\[
\theta_{j-1}^{w}:=\theta_{j-1}^{w}+\eta\left[1-d_{j}^{w}-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right] \mathbf{x}_{w}
\]<br>
再来\(X_w\)的偏导数，注意到\(\mathcal{L}(w, j)=\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]+d_{j}^{w} \cdot \log \left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]\)中\(X_w\)和\(\theta^w_{j-1}\)是对称的，所有直接将\(\theta^w_{j-1}\)的偏导数中的\(\theta^w_{j-1}\)替换为\(X_w\)，得到关于\(X_w\)的偏导数：</p>
<p>\[
\frac{\partial \mathcal{L}(w, j)}{\partial \mathbf{x}_{w}}=\left[1-d_{j}^{w}-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right] \theta_{j-1}^{w}
\]</p>
<p>不过\(X_w\)是上下文的词向量的和，不是上下文单个词的词向量。怎么把这个更新量应用到单个词的词向量上去呢？word2vec采取的是直接将\(X_w\)的更新量整个应用到每个单词的词向量上去：<br>
\[
\mathbf{v}(\tilde{w}):=\mathbf{v}(\tilde{w})+\eta \sum_{j=2}^{l^{w}} \frac{\partial \mathcal{L}(w, j)}{\partial \mathbf{x}_{w}}, \quad \tilde{w} \in \text { Context }(w)
\]<br>
其中，\(V(\tilde w)\)代表上下文中某一个单词的词向量。</p>
<h4 id="glove">glove</h4>
<p>CBOW和skip-gram虽然可以很好地进行词汇类比，但是因为这两种方法是基于一个个局部的上下文窗口方法，因此，没有有效地利用全局的词汇共现统计信息。为了克服全局矩阵分解和局部上下文窗口的缺陷，GloVe方法基于全局词汇共现的统计信息来学习词向量，从而将统计信息与局部上下文窗口方法的优点都结合起来，并发现其效果确实得到了提升。</p>
<h5 id="li-zi">例子</h5>
<p>假设i=ice,j=steam并对k取不同的词汇，如“solid”，“gas”，“water”，“fashion”，根据上面的定义，我们分别计算他们的概率\(P(k∣ice)\)、\(P(k∣steam)\)，并计算两者的比率\(P(k∣ice)/P(k∣steam)\)，可以发现，对于“solid”，其出现在“ice”上下文的概率应该比较大，出现在“steam”上下文的概率应该比较小，因此，他们的比值应该是一个比较大的数，在下表中是8.9，而对于“gas”，出现在“ice”上下文的概率应该比较小，而出现在“steam”上下文的概率应该比较大，因此，两者的比值应该是一个比较小的数，在下表中是\(8.5×10^{-2}\) ，而对于“water、fashion”这两个词汇，他们与“ice”和steam“的相关性应该比较小，因此，他们的比值应该都是接近1。因此，这样来看可以发现，比值\(P(k∣ice)/P(k∣steam)\)在一定程度上可以反映词汇之间的相关性，当相关性比较低时，其值应该在1附近，当相关性比较高时，比值应该偏离1比较远。<br>
<img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/glove.png" alt="avatar"></p>
<h5 id="yuan-li">原理</h5>
<p>基于这样的思想，作者提出了这样一种猜想，能不能通过训练词向量，使得词向量经过某种函数计算之后可以得到上面的比值，具体如下：<br>
\[
F\left(w_{i}, w_{j}, \tilde{w}_{k}\right)=\frac{P_{i k}}{P_{j k}}
\]</p>
<p>其中，\(w_i\),\(w_j\),\(\tilde{w}_{k}\) 为词汇i,j,k对应的词向量，其维度都为d，而\(P_{ik}\)，\(P_{jk}\) 则可以直接通过语料计算得到，这里F为一个未知的函数。由于词向量都是在一个线性向量空间，因此，可以对\(w_i\),\(w_j\)进行差分，将上式转变为如下：<br>
\[
F\left(w_{i}-w_{j}, \tilde{w}_{k}\right)=\frac{P_{i k}}{P_{j k}}
\]<br>
由于上式中左侧括号中是两个维度为d的词向量，而右侧是一个标量，因此，很容易会想到向量的内积，因此，上式可以进一步改变为:<br>
\[
F\left(\left(w_{i}-w_{j}\right)^{T} \tilde{w}_{k}\right)=F\left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\right)=\frac{P_{i k}}{P_{j k}}
\]<br>
由于上式中左侧是一种减法，而右侧是一种除法，很容易联想到指数计算，因此，可以把F限定为指数函数，此时有：<br>
\[
\exp \left(w_{i}^{T} w_{k}-w_{j}^{T} w_{k}\right)=\frac{\exp \left(w_{i}^{T} w_{k}\right)}{\exp \left(w_{j}^{T} w_{k}\right)}=\frac{P_{i k}}{P_{j k}}
\]<br>
因此，此时只要确保等式两边分子分母相等即可，即：<br>
\[
\exp \left(w_{i}^{T} w_{k}\right)=P_{i k}, \exp \left(w_{j}^{T} w_{k}\right)=P_{j k}
\]<br>
进一步的，可以转化为对语料中的所有词汇，考察\(exp(w^T_iw_k)\)=\(P_{ik}\)=\(\frac{X_{ik}}{X_i}\) ，即：<br>
\[
w_{i}^{T} w_{k}=\log \left(\frac{X_{i k}}{X_{i}}\right)=\log X_{i k}-\log X_{i}
\]<br>
由于上式左侧\(w^T_iw_k\)中，调换i和k的值不会改变其结果，即具有对称性，因此，为了确保等式右侧也具备对称性，引入了两个偏置项，即<br>
\[
w_{i}^{T} w_{k}=\log X_{i k}-b_{i}-b_{k}
\]<br>
此时，\(logX_i\)已经包含在\(bi\)当中。因此，此时模型的目标就转化为通过学习词向量的表示，使得上式两边尽量接近，因此，可以通过计算两者之间的平方差来作为目标函数，即：<br>
\[
J=\sum_{i, k=1}^{V}\left(w_{i}^{T} \tilde{w}_{k}+b_{i}+b_{k}-\log X_{i k}\right)^{2}
\]<br>
但是这样的目标函数有一个缺点，就是对所有的共现词汇都是采用同样的权重，因此，作者对目标函数进行了进一步的修正，通过语料中的词汇共现统计信息来改变他们在目标函数中的权重，具体如下：<br>
\[
J=\sum_{i, k=1}^{V} f\left(X_{i k}\right)\left(w_{i}^{T} \tilde{w}_{k}+b_{i}+b_{k}-\log X_{i k}\right)^{2}
\]<br>
这里\(V\)表示词汇的数量，并且权重函数\(f\)必须具备以下的特性</p>
<ul>
<li>\(f(0)=0\)，当词汇共现的次数为0时，此时对应的权重应该为0。</li>
<li>f(x)必须是一个非减函数，这样才能保证当词汇共现的次数越大时，其权重不会出现下降的情况。</li>
<li>对于那些太频繁的词，\(f(x)\)应该能给予他们一个相对小的数值，这样才不会出现过度加权。</li>
</ul>
<p>综合以上三点特性，作者提出了下面的权重函数：<br>
\[
f(x)=\left\{\begin{array}{cc}
\left(x / x_{\max }\right)^{\alpha} &amp; \text { if } x&lt;x_{\max } \\
1 &amp; \text { otherwise }
\end{array}\right.
\]</p>
<h5 id="zong-jie">总结</h5>
<ol>
<li>Glove综合了全局词汇共现的统计信息和局部窗口上下文方法的优点，可以说是两个主流方法的一种综合，但是相比于全局矩阵分解方法，由于GloVe不需要计算那些共现次数为0的词汇，因此，可以极大的减少计算量和数据的存储空间。</li>
<li>但是GloVe把语料中的词频共现次数作为词向量学习逼近的目标，当语料比较少时，有些词汇共现的次数可能比较少，笔者觉得可能会出现一种误导词向量训练方向的现象。</li>
</ol>
<h4 id="elmo">ELMO</h4>
<h5 id="zhen-dui-de-wen-ti">针对的问题</h5>
<p>word2vec中，只有apple一个词向量，无法对一词多义进行建模。</p>
<h5 id="yuan-li-1">原理</h5>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/EMLO.png" alt="avatar"></p>
<p>ELMo的主要做法是先训练一个完整的语言模型，再用这个语言模型去处理需要训练的文本，生成相应的词向量，所以在文中一直强调ELMo的模型对同一个字在不同句子中能生成不同的词向量。</p>
<p>在进行有监督的NLP任务时，可以将ELMo直接当做特征拼接到具体任务模型的词向量输入或者是模型的最高层表示上。不像传统的词向量，每一个词只对应一个词向量，ELMo利用预训练好的双向语言模型，然后根据具体输入从该语言模型中可以得到上下文依赖的当前词表示（<strong>对于不同上下文的同一个词的表示是不一样的</strong>），再当成特征加入到具体的NLP有监督模型里。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/emlo_embedding.png" alt="avatar"></p>
<p>EMLO使用的是一个<strong>双向的LSTM语言模型</strong>，由一个前向和一个后向语言模型构成，目标函数就是取这两个方向语言模型的最大似然。</p>
<p>前向LSTM结构：<br>
\[
p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} | t_{1}, t_{2}, \ldots, t_{k-1}\right)
\]<br>
反向LSTM结构：<br>
\[
p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} | t_{k+1}, t_{k+2}, \ldots, t_{N}\right)
\]<br>
最大似然函数：<br>
\[
\sum_{k=1}^{N}\left(\log p\left(t_{k} | t_{1}, t_{2}, \ldots, t_{k-1}\right)+\log p\left(t_{k} | t_{k+1}, t_{k+2}, \ldots, t_{N}\right)\right)
\]</p>
<h5 id="zong-jie-1">总结</h5>
<ol>
<li>ELMo的假设前提一个词的词向量不应该是固定的，所以在一词多意方面ELMo的效果一定比word2vec要好。</li>
<li>word2vec的学习词向量的过程是通过中心词的上下窗口去学习，学习的范围太小了，而ELMo在学习语言模型的时候是从整个语料库去学习的，而后再通过语言模型生成的词向量就相当于基于整个语料库学习的词向量，更加准确代表一个词的意思。</li>
</ol>
<h4 id="gpt">GPT</h4>
<p>用Transformer网络代替了LSTM作为语言模型来更好的捕获长距离语言结构。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/gpt_1.png" alt="avatar"></p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/gpt_2.png" alt="avatar"></p>
<h5 id="masked-attention">Masked Attention</h5>
<p>在transformer中，Encoder因为要编码整个句子，所以每个词都需要考虑上下文的关系。所以每个词在计算的过程中都是可以看到句子中所有的词的。但是Decoder与Seq2Seq中的解码器类似，每个词都只能看到前面词的状态，所以是一个单向的Self-Attention结构。Masked Attention的实现也非常简单，只要在普通的Self Attention的Softmax步骤之前，与按位乘上一个下三角矩阵M就好了</p>
<p>attention：<br>
\[
\begin{aligned}
Q &amp;=X W_{Q} \\
K &amp;=X W_{K} \\
V &amp;=X W_{V} \\
\text {Attention}(Q, K, V) &amp;=\operatorname{Softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
\end{aligned}
\]<br>
masked-attention<br>
\[
\text { Attention }(Q, K, V)=\operatorname{Softmax}\left(\frac{Q K^{T} \Theta M}{\sqrt{d_{k}}}\right) V
\]</p>
<h4 id="bert">BERT</h4>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/bert_gpt_elmo.png" alt="avatar"></p>
<h5 id="task-1-mlm">Task 1: MLM</h5>
<p>由于BERT需要通过上下文信息，来预测中心词的信息，同时又不希望模型提前看见中心词的信息，因此提出了一种 Masked Language Model 的预训练方式，即随机从输入预料上 mask 掉一些单词，然后通过的上下文预测该单词，类似于一个完形填空任务。</p>
<p>在预训练任务中，15%的 Word Piece 会被mask，这15%的 Word Piece 中，80%的时候会直接替换为 [Mask] ，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token</p>
<ul>
<li>没有100%mask的原因
<ul>
<li>如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词</li>
</ul>
</li>
<li>加入10%随机token的原因
<ul>
<li>Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘</li>
<li>另外编码器不知道哪些词需要预测的，哪些词是错误的，因此被迫需要学习每一个token的表示向量</li>
</ul>
</li>
<li>另外，每个batchsize只有15%的单词被mask的原因，是因为性能开销的问题，双向编码器比单项编码器训练要更慢</li>
</ul>
<h5 id="task-2-nsp">Task 2: NSP</h5>
<p>仅仅一个MLM任务是不足以让 BERT 解决阅读理解等句子关系判断任务的，因此添加了额外的一个预训练任务，即 Next Sequence Prediction。</p>
<p>具体任务即为一个句子关系判断任务，即判断句子B是否是句子A的下文，如果是的话输出’IsNext‘，否则输出’NotNext‘。</p>
<p>训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在图4中的[CLS]符号中</p>
<h5 id="shu-ru">输入</h5>
<ul>
<li>Token Embeddings：即传统的词向量层，每个输入样本的首字符需要设置为[CLS]，可以用于之后的分类任务，若有两个不同的句子，需要用[SEP]分隔，且最后一个字符需要用[SEP]表示终止</li>
<li>Segment Embeddings：为[0,1]序列，用来在NSP任务中区别两个句子，便于做句子关系判断任务</li>
<li>Position Embeddings：与Transformer中的位置向量不同，BERT中的位置向量是直接训练出来的</li>
</ul>
<h5 id="fine-tunninng">Fine-tunninng</h5>
<p>对于不同的下游任务，我们仅需要对BERT不同位置的输出进行处理即可，或者直接将BERT不同位置的输出直接输入到下游模型当中。具体的如下所示：</p>
<ul>
<li>对于情感分析等单句分类任务，可以直接输入单个句子（不需要[SEP]分隔双句），将[CLS]的输出直接输入到分类器进行分类</li>
<li>对于句子对任务（句子关系判断任务），需要用[SEP]分隔两个句子输入到模型中，然后同样仅须将[CLS]的输出送到分类器进行分类</li>
<li>对于问答任务，将问题与答案拼接输入到BERT模型中，然后将答案位置的输出向量进行二分类并在句子方向上进行softmax（只需预测开始和结束位置即可）</li>
<li>对于命名实体识别任务，对每个位置的输出进行分类即可，如果将每个位置的输出作为特征输入到CRF将取得更好的效果。</li>
</ul>
<h5 id="que-dian">缺点</h5>
<ul>
<li>BERT的预训练任务MLM使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务</li>
<li>另外，BERT没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计</li>
<li>由于最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）</li>
<li>适合处理自然语义理解类任务(NLU)，而不适合自然语言生成类任务(NLG)</li>
</ul>
<h5 id="torch-shi-yong">torch使用</h5>
<ol>
<li>导入相关库</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> transformers <span class="keyword">as</span> ppb <span class="comment"># pytorch transformers</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>导入预训练好的 DistilBERT 模型与分词器</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, <span class="string">'distilbert-base-uncased'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Want BERT instead of distilBERT? Uncomment the following line:</span></span><br><span class="line"><span class="comment">#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')# </span></span><br><span class="line"></span><br><span class="line">Load pretrained model/tokenizertokenizer = tokenizer_class.from_pretrained(pretrained_weights)</span><br><span class="line">model = model_class.from_pretrained(pretrained_weights)</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>tokenize：分词+[cls]+[sep]<br>
<img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/bert_tokenize.png" alt="avatar"></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.encode(<span class="string">"a visually stunning rumination on love"</span>, add_special_tokens=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>padding</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   max_len = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tokenized.values:</span><br><span class="line">    <span class="keyword">if</span> len(i) &gt; max_len:</span><br><span class="line">        max_len = len(i)</span><br><span class="line">padded = np.array([i + [<span class="number">0</span>]*(max_len-len(i)) <span class="keyword">for</span> i <span class="keyword">in</span> tokenized.values])</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>masking</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">attention_mask = np.where(padded != <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ol start="6">
<li>使用bert进行编码</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># last_hidden_states = [batch_size, max_sentence_len, 768]</span></span><br><span class="line">    last_hidden_states = model(input_ids, attention_mask=attention_mask)</span><br></pre></td></tr></table></figure>
<h3 id="transformer">transformer</h3>
<p>无论是RNN还是CNN，在处理NLP任务时都有缺陷。CNN是其先天的卷积操作不很适合序列化的文本，RNN是其没有并行化，很容易超出内存限制（比如50tokens长度的句子就会占据很大的内存）。</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/transformer.png" alt="avatar"></p>
<h4 id="wen-ti">问题</h4>
<ol>
<li>如何实现并行计算同时缩短依赖距离？采用自注意机制</li>
<li>如何向CNN一样考虑多通道信息？采用多头注意力</li>
<li>自注意力机制损失了位置信息，如何补偿? 位置嵌入</li>
<li>后面的层中位置信息消散？ 残差连接</li>
</ol>
<h4 id="self-attention-1">self-attention</h4>
<img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention-1.png" alt="avatar">
<h5 id="bing-xing-hua">并行化</h5>
<img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention-2.png" alt="avatar" style="zoom:67%;">
<img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention-3.png" alt="avatar" style="zoom: 67%;">
<img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/self-attention-4.png" alt="avatar" style="zoom: 67%;">
<h5 id="multi-head-self-attention">multi-head self-attention</h5>
<img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/multi-head.png" alt="avatar" style="zoom:67%;">
<img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/multi-head-1.png" alt="avatar">
<h4 id="encoder">encoder</h4>
<p>Encoder由N=6个相同的layer组成，layer指的就是上图左侧的单元，最左边有个“Nx”，这里是x6个。每个Layer由两个sub-layer组成，分别是multi-head self-attention mechanism和fully connected feed-forward network。其中每个sub-layer都加了residual connection和normalisation，因此可以将sub-layer的输出表示为：<br>
\[
sub_layer_output=LayerNorm(x+(\text {SubLayer}(x)))
\]</p>
<h5 id="multi-head-self-attention-1">Multi-head self-attention</h5>
<p>由于attention：<br>
\[
attention_output=Attention(Q, K, V)
\]<br>
multi-head attention则是通过h个不同的<strong>线性变换</strong>对Q，K，V进行投影，最后将不同的attention结果拼接起来：<br>
\[
\begin{array}{l}\text { MultiHead(Q,K,V)}= Concat(head_1,\ldots,head_h)W^{O}  \\ head_i = Attention(QW_i\left.^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \\ \text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V\end{array}
\]</p>
<p>多个head带来的优势是：不同的head，所attention的内容不同，比如：\(head_1\) attention 全局的内容，\(head_2\) attention local的内容。</p>
<h5 id="position-wise-feed-forward-networks">Position-wise feed-forward networks</h5>
<p>这层主要是提供非线性变换。Attention输出的维度是[bsz<em>seq_len, num_heads</em>head_size]，第二个sub-layer是个全连接层，之所以是position-wise是因为过线性层时每个位置i的变换参数是一样的.</p>
<h4 id="strong-decoder-strong"><strong>Decoder</strong></h4>
<p>Decoder和Encoder的结构差不多，但是多了一个attention的sub-layer,decoder的输入输出和解码过程:</p>
<ul>
<li>输出：对应i位置的输出词的概率分布</li>
<li>输入：encoder的输出 &amp; 对应i-1位置decoder的输出。所以中间的attention不是self-attention，它的K，V来自encoder，Q来自上一位置decoder的输出</li>
<li>解码：<strong>编码可以并行计算，一次性全部encoding出来，但解码不是一次把所有序列解出来的，而是像rnn一样一个一个解出来的</strong>，因为要用上一个位置的输入当作attention的query</li>
</ul>
<h4 id="que-dian-1">缺点</h4>
<ol>
<li>
<p>不能一次对很长文本进行建模（假如文本长度为\(10^5\),在一个forward的模块中需要\(10^5 * 10^5 = 10^{10}\)大小的矩阵来保存score，很容易导致 out of memory 问题）。解决方法：divide long sequence into smaller sequence. 但是这样导致smaller sequence 之间是没有联系的。（使用rnn的方式来建立smaller sequence 之间的联系–&gt; transformer XL）</p>
<p><img src="/2020/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/transformer-xl.png" alt="avatar"></p>
<p>在引入rnn的时候，遇到了absolute position的问题，用relative position进行了解决。</p>
</li>
</ol>
<h5 id="zong-jie-2">总结</h5>
<p>引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</p>
<p>但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。</p>
<h3 id="can-kao">参考</h3>
<ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247484682&amp;idx=1&amp;sn=51520138eed826ec891ac8154ee550f9&amp;chksm=970c2ddca07ba4ca33ee14542cff0457601bb16236f8edc1ff0e617051d1f063354dda0f8893&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">从前馈到反馈：解析循环神经网络（RNN）及其tricks</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2018-07-06-7" target="_blank" rel="noopener">Step-by-step to LSTM: 解析LSTM神经网络设计原理</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/43493999" target="_blank" rel="noopener">Attention原理和源码解析</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2019-12-28?from=synced&amp;keyword=bert" target="_blank" rel="noopener">BERT模型超酷炫，上手又太难？请查收这份BERT快速入门指南！</a></li>
</ol>

    </div>

    
    
    <div>
  
    <div style="text-align:center;color:#bfbfbf;font-size:16px;">
      <span>-------- 本文结束 </span>
      <i class="fa fa-coffee"></i>
      <span> 感谢阅读 --------</span>
    </div>
  
</div>
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    鼓励一下
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat.png" alt="Li Zhen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/ali.png" alt="Li Zhen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a>
              <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" rel="tag"><i class="fa fa-tag"></i> 自然语言处理</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%80%BB%E7%BB%93/" rel="prev" title="机器学习大总结">
      <i class="fa fa-chevron-left"></i> 机器学习大总结
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/03/20/life/past_present_future/" rel="next" title="过去、现在、未来">
      过去、现在、未来 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#zhi-shi-dian"><span class="nav-number">1.</span> <span class="nav-text">知识点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#shen-jing-yuan-gan-zhi-qi"><span class="nav-number">1.1.</span> <span class="nav-text">神经元(感知器)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#s-xing-shen-jing-yuan"><span class="nav-number">1.2.</span> <span class="nav-text">S型神经元</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ti-du-xia-jiang-suan-fa"><span class="nav-number">1.3.</span> <span class="nav-text">梯度下降算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#fan-xiang-chuan-bo-bp-suan-fa"><span class="nav-number">1.4.</span> <span class="nav-text">反向传播(BP)算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#shen-jing-wang-luo-zhong-shi-yong-ju-zhen-kuai-su-ji-suan-shu-chu-de-fang-fa"><span class="nav-number">1.4.1.</span> <span class="nav-text">神经网络中使用矩阵快速计算输出的方法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#guan-yu-dai-jie-han-shu-de-liang-ge-jia-she"><span class="nav-number">1.4.2.</span> <span class="nav-text">关于代价函数的两个假设</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#hadamard-cheng-ji-s-t"><span class="nav-number">1.4.3.</span> <span class="nav-text">Hadamard乘积,s⊙t</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#fan-xiang-chuan-bo-de-si-ge-ji-ben-fang-cheng"><span class="nav-number">1.4.4.</span> <span class="nav-text">反向传播的四个基本方程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#suan-fa-miao-shu"><span class="nav-number">1.4.5.</span> <span class="nav-text">算法描述</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#dai-ma"><span class="nav-number">1.4.6.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#jiao-cha-shang-dai-jie-han-shu"><span class="nav-number">1.5.</span> <span class="nav-text">交叉熵代价函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#guo-ni-he"><span class="nav-number">1.6.</span> <span class="nav-text">过拟合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#diao-can"><span class="nav-number">1.7.</span> <span class="nav-text">调参</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xun-huan-shen-jing-wang-luo"><span class="nav-number">2.</span> <span class="nav-text">循环神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#rnn"><span class="nav-number">2.1.</span> <span class="nav-text">RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#rnn-cun-zai-de-wen-ti"><span class="nav-number">2.1.1.</span> <span class="nav-text">RNN存在的问题</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lstm"><span class="nav-number">2.2.</span> <span class="nav-text">LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#strong-yi-wang-men-strong"><span class="nav-number">2.2.1.</span> <span class="nav-text">遗忘门</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#strong-shu-ru-men-strong"><span class="nav-number">2.2.2.</span> <span class="nav-text">输入门</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#shu-chu-men"><span class="nav-number">2.2.3.</span> <span class="nav-text">输出门</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#gong-shi"><span class="nav-number">2.2.4.</span> <span class="nav-text">公式</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#gru"><span class="nav-number">2.3.</span> <span class="nav-text">GRU</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#self-attention"><span class="nav-number">3.</span> <span class="nav-text">Self-Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ci-xiang-liang-mo-xing"><span class="nav-number">4.</span> <span class="nav-text">词向量模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#word-2-vec"><span class="nav-number">4.1.</span> <span class="nav-text">word2vec</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#strong-c-bo-w-mo-xing-strong"><span class="nav-number">4.1.1.</span> <span class="nav-text">CBoW模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#skip-gram-mo-xing"><span class="nav-number">4.1.2.</span> <span class="nav-text">skip-gram模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#hierarchical-softmax"><span class="nav-number">4.1.3.</span> <span class="nav-text">Hierarchical Softmax</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#glove"><span class="nav-number">4.2.</span> <span class="nav-text">glove</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#li-zi"><span class="nav-number">4.2.1.</span> <span class="nav-text">例子</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#yuan-li"><span class="nav-number">4.2.2.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#zong-jie"><span class="nav-number">4.2.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#elmo"><span class="nav-number">4.3.</span> <span class="nav-text">ELMO</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#zhen-dui-de-wen-ti"><span class="nav-number">4.3.1.</span> <span class="nav-text">针对的问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#yuan-li-1"><span class="nav-number">4.3.2.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#zong-jie-1"><span class="nav-number">4.3.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#gpt"><span class="nav-number">4.4.</span> <span class="nav-text">GPT</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#masked-attention"><span class="nav-number">4.4.1.</span> <span class="nav-text">Masked Attention</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bert"><span class="nav-number">4.5.</span> <span class="nav-text">BERT</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#task-1-mlm"><span class="nav-number">4.5.1.</span> <span class="nav-text">Task 1: MLM</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#task-2-nsp"><span class="nav-number">4.5.2.</span> <span class="nav-text">Task 2: NSP</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#shu-ru"><span class="nav-number">4.5.3.</span> <span class="nav-text">输入</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#fine-tunninng"><span class="nav-number">4.5.4.</span> <span class="nav-text">Fine-tunninng</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#que-dian"><span class="nav-number">4.5.5.</span> <span class="nav-text">缺点</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#torch-shi-yong"><span class="nav-number">4.5.6.</span> <span class="nav-text">torch使用</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transformer"><span class="nav-number">5.</span> <span class="nav-text">transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#wen-ti"><span class="nav-number">5.1.</span> <span class="nav-text">问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#self-attention-1"><span class="nav-number">5.2.</span> <span class="nav-text">self-attention</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#bing-xing-hua"><span class="nav-number">5.2.1.</span> <span class="nav-text">并行化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#multi-head-self-attention"><span class="nav-number">5.2.2.</span> <span class="nav-text">multi-head self-attention</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#encoder"><span class="nav-number">5.3.</span> <span class="nav-text">encoder</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#multi-head-self-attention-1"><span class="nav-number">5.3.1.</span> <span class="nav-text">Multi-head self-attention</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#position-wise-feed-forward-networks"><span class="nav-number">5.3.2.</span> <span class="nav-text">Position-wise feed-forward networks</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#strong-decoder-strong"><span class="nav-number">5.4.</span> <span class="nav-text">Decoder</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#que-dian-1"><span class="nav-number">5.5.</span> <span class="nav-text">缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#zong-jie-2"><span class="nav-number">5.5.1.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#can-kao"><span class="nav-number">6.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <a href='/'>
    <img class="site-author-image" itemprop="image" alt="Li Zhen"
      src="/images/snoppy.jpeg">
  </a>
  <p class="site-author-name" itemprop="name">Li Zhen</p>
  <div class="site-description" itemprop="description">他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">51</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">59</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jeffery0628" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jeffery0628" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jeffery.lee.0628@gmail.com" title="邮箱 → mailto:jeffery.lee.0628@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮箱</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Zhen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">570k</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  






  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>


 

<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180101,"YYYYMMDD"));
    ages = ages.replace(/years?/, "年");
    ages = ages.replace(/months?/, "月");
    ages = ages.replace(/days?/, "天");
    ages = ages.replace(/hours?/, "小时");
    ages = ages.replace(/minutes?/, "分");
    ages = ages.replace(/seconds?/, "秒");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `我已在此等候你 ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


<script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>

<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdnjs.cloudflare.com/ajax/libs/valine/1.3.10/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'tChTbEIggDSVcdhPBUf0MFxW-gzGzoHsz',
      appKey     : 'sJ6xUiFnifEDIIfnj3Ut9zy1',
      placeholder: "留下你的小脚印吧！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '8' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
