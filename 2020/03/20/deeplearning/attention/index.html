<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/snoppy.jpeg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open Sans:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css">
  <script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jeffery.ink","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":15,"offset":12,"onmobile":true,"dimmer":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":true,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":true,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="注意力机制">
<meta property="og:url" content="https://jeffery.ink/2020/03/20/deeplearning/attention/index.html">
<meta property="og:site_name" content="火种2号">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jeffery.ink/2020/03/20/deeplearning/attention/v2-24927f5c33083c1322bc16fa9feb38fd_r.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/03/20/deeplearning/attention/10.11_attention.svg">
<meta property="og:image" content="https://jeffery.ink/2020/03/20/deeplearning/attention/v2-24927f5c33083c1322bc16fa9feb38fd_r.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/03/20/deeplearning/attention/v2-07c4c02a9bdecb23d9664992f142eaa5_1440w.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/03/20/deeplearning/attention/v2-1b7a38bc0bd8a46b52753ece64f665ad_r.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/03/20/deeplearning/attention/v2-c19809fef8cd32115ad69d6946ec6564_r.jpg">
<meta property="article:published_time" content="2020-03-20T14:54:37.000Z">
<meta property="article:modified_time" content="2020-08-13T23:39:12.960Z">
<meta property="article:author" content="Li Zhen">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jeffery.ink/2020/03/20/deeplearning/attention/v2-24927f5c33083c1322bc16fa9feb38fd_r.jpg">

<link rel="canonical" href="https://jeffery.ink/2020/03/20/deeplearning/attention/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>注意力机制 | 火种2号</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">火种2号</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">火种计划</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>简历</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>读书</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-video fa-fw"></i>电影</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>



</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jeffery.ink/2020/03/20/deeplearning/attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/snoppy.jpeg">
      <meta itemprop="name" content="Li Zhen">
      <meta itemprop="description" content="他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="火种2号">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          注意力机制
        </h1>

        <div class="post-meta">
          
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-08-14 07:39:12" itemprop="dateModified" datetime="2020-08-14T07:39:12+08:00">2020-08-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">技术/深度学习</span></a>
                </span>
            </span>

          
            <span id="/2020/03/20/deeplearning/attention/" class="post-meta-item leancloud_visitors" data-flag-title="注意力机制" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论：</span>
    
    <a title="valine" href="/2020/03/20/deeplearning/attention/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/03/20/deeplearning/attention/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>10k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>9 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/2020/03/20/deeplearning/attention/v2-24927f5c33083c1322bc16fa9feb38fd_r.jpg" alt></p>
<a id="more"></a>
<h1 id="rnn-jie-gou-de-ju-xian">RNN结构的局限</h1>
<p>机器翻译解决的是输入是一串在某种语言中的一句话，输出是目标语言相对应的话的问题，如将英语中的一段话翻译成合适的法语。以前的Neural Machine Translation模型，通常的配置是encoder-decoder结构，即encoder读取输入的句子将其转换为定长的一个向量，然后decoder再将这个向量翻译成对应的目标语言的文字。通常encoder及decoder均采用RNN结构如LSTM或GRU等（encoder在各个时间步依赖相同的背景变量来获取输入序列信息，decoder在各个时间步依赖相同的背景变量来获取输入序列信息）。但是这个结构有些问题，尤其是RNN机制实际中存在长程梯度消失的问题，对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息，所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降。</p>
<h1 id="attention-ji-zhi-de-yin-ru">Attention机制的引入</h1>
<p>为了解决这一由长序列到定长向量转化而造成的信息损失的瓶颈，Attention注意力机制被引入了。Attention机制跟人类翻译文章时候的思路有些类似，即将注意力关注于我们翻译部分对应的上下文。在Attention模型中，当我们翻译当前词语时，我们会寻找源语句中相对应的几个词语，并结合之前的已经翻译的部分作出相应的翻译。</p>
<p>一个翻译例子：输入为英语序列“They”“are”“watching”“.”，输出为法语列“Ils”“regardent”“.”。不难想到，解码器在生成输出序列中的每一个词时可能只需利用输入序列某一部分的信息。例如，在输出序列的时间步1，解码器可以主要依赖“They”“are”的信息来生成“Ils”，在时间步2则主要使用来自“watching”的编码信息生成“regardent”，最后在时间步3则直接映射句号“.”。这看上去就像是在解码器的每一时间步对输入序列中不同时间步的表征或编码信息分配不同的注意力一样。</p>
<p>仍然以循环神经网络为例，注意力机制通过对编码器所有时间步的隐藏状态做加权平均来得到背景变量。解码器在每一时间步调整这些权重，即注意力权重，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量。在seq2seq里区分了输入序列（编码器）的索引\(t\)与输出序列（解码器）的索引\(t'\)。解码器在时间步\(t'\)的隐藏状态</p>
<p>\[
\boldsymbol{s}_{t'} = g(\boldsymbol{y}_{t'-1}, \boldsymbol{c},\boldsymbol{s}_{t'-1})
\]<br>
其中，\(\boldsymbol{y}_{t'-1}\)是上一时间步\((t'-1)\)的输出\(y_{t'-1}\)的表征，\(s_{t^\prime-1}\)是上一时间步\((t^\prime-1)\)的隐藏状态，且任一时间步\(t'\)使用相同的背景变量\(\boldsymbol{c}\)。</p>
<p>但在注意力机制中，解码器的每一时间步将使用可变的背景变量。记\(\boldsymbol{c}_{t'}\)是解码器在时间步\(t'\)的背景变量，那么解码器在该时间步的隐藏状态可以改写为：<br>
\[
\boldsymbol{s}_{t'} = g(\boldsymbol{y}_{t'-1}, \boldsymbol{c}_{t'}, \boldsymbol{s}_{t'-1})
\]</p>
<p>这里的关键是如何计算背景变量\(\boldsymbol{c}_{t'}\)和如何利用它来更新隐藏状态\(\boldsymbol{s}_{t'}\)。下面将分别描述这两个关键点。</p>
<h2 id="bian-ma-qi-zhong-de-attention">编码器中的attention</h2>
<p>下图描绘了注意力机制如何为解码器在时间步2计算背景变量。首先，函数\(a\)根据解码器在时间步1的隐藏状态和编码器在各个时间步的隐藏状态计算softmax运算的输入。softmax运算输出概率分布并对编码器各个时间步的隐藏状态做加权平均，从而得到背景变量。</p>
<p><img src="/2020/03/20/deeplearning/attention/10.11_attention.svg" alt="编码器—解码器上的注意力机制"></p>
<p>具体来说，令编码器在时间步\(t\)的隐藏状态为\(\boldsymbol{h}_t\)，且总时间步数为\(T\)。那么解码器在时间步\(t'\)的背景变量为所有编码器隐藏状态的加权平均：</p>
<p>\[
\boldsymbol{c}_{t'} = \sum_{t=1}^T \alpha_{t' t} \boldsymbol{h}_t,
\]</p>
<p>其中给定\(t'\)时，权重\(\alpha_{t' t}\)在\(t=1,\ldots,T\)的值是一个概率分布。为了得到概率分布，可以使用softmax运算:</p>
<p>\[
\alpha_{t' t} = \frac{\exp(e_{t' t})}{ \sum_{k=1}^T \exp(e_{t' k}) },\quad t=1,\ldots,T.
\]</p>
<p>现在，需要定义如何计算上式中softmax运算的输入\(e_{t' t}\)。由于\(e_{t' t}\)同时取决于解码器的时间步\(t'\)和编码器的时间步\(t\)，不妨以解码器在时间步\(t'-1\)的隐藏状态\(\boldsymbol{s}_{t' - 1}\)与编码器在时间步\(t\)的隐藏状态\(\boldsymbol{h}_t\)为输入，并通过函数\(a\)计算\(e_{t' t}\)：</p>
<p>\[
e_{t' t} = a(\boldsymbol{s}_{t' - 1}, \boldsymbol{h}_t).
\]</p>
<p>这里函数\(a\)有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积\(a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}\)。而最早提出注意力机制的论文则将输入连结后通过含单隐藏层的多层感知机变换：</p>
<p>\[
a(\boldsymbol{s}, \boldsymbol{h}) = \boldsymbol{v}^\top \tanh(\boldsymbol{W}_s \boldsymbol{s} + \boldsymbol{W}_h \boldsymbol{h}),
\]</p>
<p>其中\(\boldsymbol{v}\)、\(\boldsymbol{W}_s\)、\(\boldsymbol{W}_h\)都是可以学习的模型参数。</p>
<h2 id="ji-suan-bei-jing-xiang-liang">计算背景向量</h2>
<p>还可以对注意力机制采用更高效的矢量化计算。广义上，注意力机制的输入包括查询项以及一一对应的键项和值项，其中值项是需要加权平均的一组项。在加权平均中，值项的权重来自查询项以及与该值项对应的键项的计算。</p>
<p>在上面的例子中，查询项为解码器的隐藏状态，键项和值项均为编码器的隐藏状态。<br>
考虑一个常见的简单情形，即编码器和解码器的隐藏单元个数均为\(h\)，且函数\(a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}\)。假设我们希望根据解码器单个隐藏状态\(\boldsymbol{s}_{t' - 1} \in \mathbb{R}^{h}\)和编码器所有隐藏状态\(\boldsymbol{h}_t \in \mathbb{R}^{h}, t = 1,\ldots,T\)来计算背景向量\(\boldsymbol{c}_{t'}\in \mathbb{R}^{h}\)。<br>
我们可以将查询项矩阵\(\boldsymbol{Q} \in \mathbb{R}^{1 \times h}\)设为\(\boldsymbol{s}_{t' - 1}^\top\)，并令键项矩阵\(\boldsymbol{K} \in \mathbb{R}^{T \times h}\)和值项矩阵\(\boldsymbol{V} \in \mathbb{R}^{T \times h}\)相同且第\(t\)行均为\(\boldsymbol{h}_t^\top\)。此时，我们只需要通过矢量化计算<br>
\[
\boldsymbol{c}_{t'}^\top =\text{softmax}(\boldsymbol{Q}\boldsymbol{K}^\top)\boldsymbol{V}
\]</p>
<p>即可算出转置后的背景向量\(\boldsymbol{c}_{t'}^\top\)。当查询项矩阵\(\boldsymbol{Q}\)的行数为\(n\)时，上式将得到\(n\)行的输出矩阵。输出矩阵与查询项矩阵在相同行上一一对应。</p>
<h1 id="attention-ji-zhi-de-ben-zhi-si-xiang">Attention机制的本质思想</h1>
<p>如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。</p>
<p><img src="/2020/03/20/deeplearning/attention/v2-24927f5c33083c1322bc16fa9feb38fd_r.jpg" alt></p>
<p>可以这样来看待Attention机制：将Source中的构成元素想象成是由一系列的&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：<br>
\[
Attention(Query,source) = \sum_{i=1}^{L_x} similarity(query,key)*value_i
\]<br>
其中，\(L_x=\|Source\|\)代表Source的长度。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的隐藏状态，所以可能不容易看出这种能够体现本质思想的结构。</p>
<p>从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p>
<p>从上图还可以引出另外一种理解，也可以将Attention机制看作一种<strong>软寻址</strong>:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。</p>
<h1 id="attention-ji-zhi-de-ji-suan-guo-cheng">Attention机制的计算过程</h1>
<p>Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为<strong>两个过程</strong>：</p>
<ol>
<li>
<p>第一个过程是根据Query和Key计算权重系数</p>
<ol>
<li>
<p>第一个阶段：根据Query和Key计算两者的相似性或者相关性</p>
<ul>
<li>点积：\(score(h_t,h_s) = h_t^T \cdot h_s\)</li>
<li>Cosine相似度：\(score(h_t,h_s) = \frac{h_t^T \cdot h_s}{\|h_t\|\cdot\|h_s\|}\)</li>
<li>MLP:\(score(h_t,h_s) = MLP(h_t , h_s)\)</li>
<li>Scaled Dot Product : \(score(h_t,h_s) = \frac{h_t^T \cdot h_s}{\sqrt{H}}\) ,其中<code>H</code>是RNN encoder隐藏状态的数量</li>
<li>Concat：\(v_{a}^{\top} \cdot \tanh \left(W_{a}\left[h_{t}: h_{s}\right]\right)\)</li>
<li>Location: \(W_a \cdot h_t\)</li>
<li>General: \(h_t^T\cdot W_a\cdot h_s\)</li>
</ul>
</li>
<li>
<p>第二个阶段：对第一阶段的原始分值进行归一化处理<br>
\[
\alpha_i = softmax(sim_i) = \frac{e^{sim_i}}{\sum_{j=1}^{L_x}{e^{sim_j}}}
\]</p>
</li>
</ol>
</li>
<li>
<p>第二个过程根据权重系数对Value进行加权求和<br>
\[
attention(Query,Source) = \sum_{i=1}^{L_x} \alpha_i \cdot Value_i
\]</p>
</li>
</ol>
<p><img src="/2020/03/20/deeplearning/attention/v2-07c4c02a9bdecb23d9664992f142eaa5_1440w.jpg" alt></p>
<h1 id="self-attention">Self Attention</h1>
<p>Self Attention也经常被称为intra Attention（内部Attention）。在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。而Self Attention是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已。</p>
<p>##Self Attention学到了哪些规律或者抽取出了哪些特征？</p>
<p>以机器翻译中的Self Attention来说明，图11和图12是可视化地表示Self Attention在同一个英语句子内单词间产生的联系。以机器翻译中的Self Attention来说明，下面两张图是可视化地表示Self Attention在同一个英语句子内单词间产生的联系。</p>
<p><img src="/2020/03/20/deeplearning/attention/v2-1b7a38bc0bd8a46b52753ece64f665ad_r.jpg" alt></p>
<p><img src="/2020/03/20/deeplearning/attention/v2-c19809fef8cd32115ad69d6946ec6564_r.jpg" alt></p>
<p>从两张图可以看出，Self Attention可以捕获同一个句子中单词之间的一些句法特征（有一定距离的短语结构）或者语义特征（its的指代对象Law）。</p>
<p>很明显，引入Self Attention后会<strong>更容易捕获句子中长距离的相互依赖的特征</strong>，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</p>
<p>但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。</p>
<h2 id="dai-ma">代码</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SelfAttention</span><span class="params">(Layer)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Layer for implementing self-attention mechanism. Weight variables were preferred over Dense()</span></span><br><span class="line"><span class="string">    layers in implementation because they allow easier identification of shapes. Softmax activation</span></span><br><span class="line"><span class="string">    ensures that all weights sum up to 1.</span></span><br><span class="line"><span class="string">    @param (int) size: a.k.a attention length, number of hidden units to decode the attention before</span></span><br><span class="line"><span class="string">           the softmax activation and becoming annotation weights</span></span><br><span class="line"><span class="string">    @param (int) num_hops: number of hops of attention, or number of distinct components to be</span></span><br><span class="line"><span class="string">           extracted from each sentence.</span></span><br><span class="line"><span class="string">    @param (bool) use_penalization: set True to use penalization, otherwise set False</span></span><br><span class="line"><span class="string">    @param (int) penalty_coefficient: the weight of the extra loss</span></span><br><span class="line"><span class="string">    @param (str) model_api: specify to use TF's Sequential OR Functional API, note that attention</span></span><br><span class="line"><span class="string">           weights are not outputted with the former as it only accepts single-output layers</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, num_hops=<span class="number">8</span>, use_penalization=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                 penalty_coefficient=<span class="number">0.1</span>, model_api=<span class="string">'functional'</span>, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> model_api <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'sequential'</span>, <span class="string">'functional'</span>]:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Argument for param @model_api is not recognized"</span>)</span><br><span class="line">        self.size = size</span><br><span class="line">        self.num_hops = num_hops</span><br><span class="line">        self.use_penalization = use_penalization</span><br><span class="line">        self.penalty_coefficient = penalty_coefficient</span><br><span class="line">        self.model_api = model_api</span><br><span class="line">        super(SelfAttention, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span></span><br><span class="line">        base_config = super(SelfAttention, self).get_config()</span><br><span class="line">        base_config[<span class="string">'size'</span>] = self.size</span><br><span class="line">        base_config[<span class="string">'num_hops'</span>] = self.num_hops</span><br><span class="line">        base_config[<span class="string">'use_penalization'</span>] = self.use_penalization</span><br><span class="line">        base_config[<span class="string">'penalty_coefficient'</span>] = self.penalty_coefficient</span><br><span class="line">        base_config[<span class="string">'model_api'</span>] = self.model_api</span><br><span class="line">        <span class="keyword">return</span> base_config</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.W1 = self.add_weight(name=<span class="string">'W1'</span>,</span><br><span class="line">                                  shape=(self.size, input_shape[<span class="number">2</span>]),  <span class="comment"># (size, H)</span></span><br><span class="line">                                  initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line">        self.W2 = self.add_weight(name=<span class="string">'W2'</span>,</span><br><span class="line">                                  shape=(self.num_hops, self.size),  <span class="comment"># (num_hops, size)</span></span><br><span class="line">                                  initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line">        super(SelfAttention, self).build(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span>  <span class="comment"># (B, S, H)</span></span><br><span class="line">        <span class="comment"># Expand weights to include batch size through implicit broadcasting</span></span><br><span class="line">        W1, W2 = self.W1[<span class="literal">None</span>, :, :], self.W2[<span class="literal">None</span>, :, :]</span><br><span class="line">        hidden_states_transposed = Permute(dims=(<span class="number">2</span>, <span class="number">1</span>))(inputs) <span class="comment"># (B, H, S)</span></span><br><span class="line">        attention_score = tf.matmul(W1, hidden_states_transposed) <span class="comment"># (B, size, S)</span></span><br><span class="line">        attention_score = Activation(<span class="string">'tanh'</span>)(attention_score) <span class="comment"># (B, size, S)</span></span><br><span class="line">        attention_weights = tf.matmul(W2, attention_score)<span class="comment"># (B, num_hops, S)</span></span><br><span class="line">        attention_weights = Activation(<span class="string">'softmax'</span>)(attention_weights)  <span class="comment"># (B, num_hops, S)</span></span><br><span class="line">        embedding_matrix = tf.matmul(attention_weights, inputs)  <span class="comment"># (B, num_hops, H)</span></span><br><span class="line">        embedding_matrix_flattened = Flatten()(embedding_matrix)  <span class="comment"># (B, num_hops*H)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.use_penalization:</span><br><span class="line">            attention_weights_transposed = Permute(dims=(<span class="number">2</span>, <span class="number">1</span>))(attention_weights)  <span class="comment"># (B, S, num_hops)</span></span><br><span class="line">            product = tf.matmul(attention_weights, attention_weights_transposed)  <span class="comment"># (B, num_hops, num_hops)</span></span><br><span class="line">            identity = tf.eye(self.num_hops, batch_shape=(inputs.shape[<span class="number">0</span>],)) <span class="comment"># (B, num_hops, num_hops)</span></span><br><span class="line">            frobenius_norm = tf.sqrt(tf.reduce_sum(tf.square(product - identity)))  <span class="comment"># distance</span></span><br><span class="line">            self.add_loss(self.penalty_coefficient * frobenius_norm)  <span class="comment"># loss</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.model_api == <span class="string">'functional'</span>:</span><br><span class="line">            <span class="keyword">return</span> embedding_matrix_flattened, attention_weights</span><br><span class="line">        <span class="keyword">elif</span> self.model_api == <span class="string">'sequential'</span>:</span><br><span class="line">            <span class="keyword">return</span> embedding_matrix_flattened</span><br></pre></td></tr></table></figure>
<h1 id="attention-bian-chong">Attention变种</h1>
<h2 id="ying-xing-zhu-yi-li">硬性注意力</h2>
<p>之前提到的注意力是软性注意力，其选择的信息是所有输入信息<strong>在注意力分布下的期望</strong>。还有一种注意力是只关注到<strong>某一个位置上的信息</strong>，叫做<strong>hard attention</strong>。硬性注意力有两种实现方式：</p>
<ol>
<li>一种是选取<strong>最高概率</strong>的输入信息；</li>
<li>另一种硬性注意力可以通过在注意力分布式上<strong>随机采样</strong>的方式实现。</li>
</ol>
<p>硬性注意力模型的<strong>缺点</strong>：</p>
<p>硬性注意力的一个缺点是基于最大采样或随机采样的方式来选择信息。<strong>因此最终的损失函数与注意力分布之间的函数关系不可导</strong>，因此<strong>无法使用在反向传播算法进行训练</strong>。为了使用反向传播算法，一般使用软性注意力来代替硬性注意力。硬性注意力需要通过强化学习来进行训练。</p>
<h2 id="jian-zhi-dui-zhu-yi-li">键值对注意力</h2>
<p>Key！=Value，注意力函数变成：<br>
\[
attention((K,V),Q) = \sum_{i=1}^{N}{\alpha_i v_i} = \sum_{i=1}^{N}{\frac{e^{sim_i}}{\sum_{j=1}^{L_x}{e^{sim_j}}} v_i}
\]</p>
<h2 id="duo-tou-zhu-yi-li">多头注意力</h2>
<p>多头注意力是利用多个查询\(Q = [q_1, \ldots, q_M]\)，来平行地计算从输入信息中选取多个信息。<strong>每个注意力关注输入信息的不同部分</strong>，然后再进行拼接：<br>
\[
\operatorname{att}((K, V), Q)=\operatorname{att}\left((K, V), \mathbf{q}_{1}\right) \oplus \cdots \oplus \mathbf{a t t}\left((K, V), \mathbf{q}_{M}\right)
\]</p>
<h1 id="zong-jie">总结</h1>
<ul>
<li>可以在解码器的每个时间步使用不同的背景变量，并对输入序列中不同时间步编码的信息分配不同的注意力。</li>
<li>广义上，注意力机制的输入包括查询项以及一一对应的键项和值项。</li>
<li>注意力机制可以采用更为高效的矢量化计算。</li>
</ul>
<h1 id="can-kao-wen-xian">参考文献</h1>
<p>[1] Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</p>
<p>[2] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems (pp. 5998-6008).</p>
<p>[3] Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>[4] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever I. (2019). Language Models are Unsupervised Multitask Learners. OpenAI.</p>

    </div>

    
    
    <div>
  
    <div style="text-align:center;color:#bfbfbf;font-size:16px;">
      <span>-------- 本文结束 </span>
      <i class="fa fa-coffee"></i>
      <span> 感谢阅读 --------</span>
    </div>
  
</div>
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    鼓励一下
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat.png" alt="Li Zhen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/ali.png" alt="Li Zhen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/03/20/life/past_present_future/" rel="prev" title="过去、现在、未来">
      <i class="fa fa-chevron-left"></i> 过去、现在、未来
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/03/28/data_preprocessing/" rel="next" title="数据预处理">
      数据预处理 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#rnn-jie-gou-de-ju-xian"><span class="nav-number">1.</span> <span class="nav-text">RNN结构的局限</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#attention-ji-zhi-de-yin-ru"><span class="nav-number">2.</span> <span class="nav-text">Attention机制的引入</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#bian-ma-qi-zhong-de-attention"><span class="nav-number">2.1.</span> <span class="nav-text">编码器中的attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ji-suan-bei-jing-xiang-liang"><span class="nav-number">2.2.</span> <span class="nav-text">计算背景向量</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#attention-ji-zhi-de-ben-zhi-si-xiang"><span class="nav-number">3.</span> <span class="nav-text">Attention机制的本质思想</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#attention-ji-zhi-de-ji-suan-guo-cheng"><span class="nav-number">4.</span> <span class="nav-text">Attention机制的计算过程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#self-attention"><span class="nav-number">5.</span> <span class="nav-text">Self Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#dai-ma"><span class="nav-number">5.1.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#attention-bian-chong"><span class="nav-number">6.</span> <span class="nav-text">Attention变种</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ying-xing-zhu-yi-li"><span class="nav-number">6.1.</span> <span class="nav-text">硬性注意力</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#jian-zhi-dui-zhu-yi-li"><span class="nav-number">6.2.</span> <span class="nav-text">键值对注意力</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#duo-tou-zhu-yi-li"><span class="nav-number">6.3.</span> <span class="nav-text">多头注意力</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#zong-jie"><span class="nav-number">7.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#can-kao-wen-xian"><span class="nav-number">8.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <a href='/'>
    <img class="site-author-image" itemprop="image" alt="Li Zhen"
      src="/images/snoppy.jpeg">
  </a>
  <p class="site-author-name" itemprop="name">Li Zhen</p>
  <div class="site-description" itemprop="description">他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">89</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">67</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jeffery0628" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jeffery0628" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jeffery.lee.0628@gmail.com" title="邮箱 → mailto:jeffery.lee.0628@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮箱</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Zhen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">1.1m</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  






  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>


 

<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180101,"YYYYMMDD"));
    ages = ages.replace(/years?/, "年");
    ages = ages.replace(/months?/, "月");
    ages = ages.replace(/days?/, "天");
    ages = ages.replace(/hours?/, "小时");
    ages = ages.replace(/minutes?/, "分");
    ages = ages.replace(/seconds?/, "秒");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `我已在此等候你 ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


<script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>

<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'tChTbEIggDSVcdhPBUf0MFxW-gzGzoHsz',
      appKey     : 'sJ6xUiFnifEDIIfnj3Ut9zy1',
      placeholder: "留下你的小脚印吧！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '8' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
