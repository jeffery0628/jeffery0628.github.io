<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/snoppy.jpeg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jeffery0628.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":300,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":true,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="应用文本分类可以用于新闻分类、简历分类、邮件分类、办公文档分类、区域分类等诸多方面，还能够实现文本过滤，从大量文本中快速识别和过滤出符合特殊要求的信息。 数据集准备了长短文本，二分类十分类进行比较 情感二分类weibo_senti_100k：共119988条数据，正例：59993,负例59995    句子最大长度：260，最小长度：3，平均长度：66.04 部分样例     label revi">
<meta property="og:type" content="article">
<meta property="og:title" content="文本分类模型及比较">
<meta property="og:url" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/index.html">
<meta property="og:site_name" content="火种2号">
<meta property="og:description" content="应用文本分类可以用于新闻分类、简历分类、邮件分类、办公文档分类、区域分类等诸多方面，还能够实现文本过滤，从大量文本中快速识别和过滤出符合特殊要求的信息。 数据集准备了长短文本，二分类十分类进行比较 情感二分类weibo_senti_100k：共119988条数据，正例：59993,负例59995    句子最大长度：260，最小长度：3，平均长度：66.04 部分样例     label revi">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/cbow.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/fasttext.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/textcnn.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/textcnndetail.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/HAN.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/rnn.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/birnn.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/lstm_c.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/lstm_forget_gate.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/input_gate.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/output_gate.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/lstm_all.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/GRU.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/self-attention.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/rcnn.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/DPCNN.jpg">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/equal_cnn.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/dpcnn_pooling.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/dpcnn_resnet.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/bert_gpt_elmo.png">
<meta property="article:published_time" content="2020-03-29T04:52:20.000Z">
<meta property="article:modified_time" content="2020-04-08T00:39:39.617Z">
<meta property="article:author" content="Li Zhen">
<meta property="article:tag" content="自然语言处理">
<meta property="article:tag" content="FastText">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/cbow.png">

<link rel="canonical" href="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>文本分类模型及比较 | 火种2号</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">火种2号</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">火种计划</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>读书</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-video fa-fw"></i>电影</a>

  </li>
        <li class="menu-item menu-item-games">

    <a href="/games/" rel="section"><i class="fa fa-gamepad fa-fw"></i>游戏</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>



</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jeffery0628.github.io/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/snoppy.jpeg">
      <meta itemprop="name" content="Li Zhen">
      <meta itemprop="description" content="但行好事，莫问前程！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="火种2号">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          文本分类模型及比较
        </h1>

        <div class="post-meta">
          
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-08 08:39:39" itemprop="dateModified" datetime="2020-04-08T08:39:39+08:00">2020-04-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">文本分类</span></a>
                </span>
            </span>

          
            <span id="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/" class="post-meta-item leancloud_visitors" data-flag-title="文本分类模型及比较" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>45k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>41 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>文本分类可以用于新闻分类、简历分类、邮件分类、办公文档分类、区域分类等诸多方面，还能够实现文本过滤，从大量文本中快速识别和过滤出符合特殊要求的信息。</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>准备了长短文本，二分类十分类进行比较</p>
<h4 id="情感二分类"><a href="#情感二分类" class="headerlink" title="情感二分类"></a>情感二分类</h4><p>weibo_senti_100k：共119988条数据，正例：59993,负例59995   </p>
<p>句子最大长度：260，最小长度：3，平均长度：66.04</p>
<p>部分样例</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>label</th>
<th>review</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]</td>
</tr>
<tr>
<td>1</td>
<td>@张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心，酒店都全部OK啦。</td>
</tr>
<tr>
<td>1</td>
<td>姑娘都羡慕你呢…还有招财猫高兴……//@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢//@李欣芸SharonLee:大佬范儿[书呆子]</td>
</tr>
<tr>
<td>1</td>
<td>美<del>~</del>[爱你]</td>
</tr>
<tr>
<td>1</td>
<td>梦想有多大，舞台就有多大![鼓掌]</td>
</tr>
<tr>
<td>0</td>
<td>今天真冷啊，难道又要穿棉袄了[晕]？今年的春天真的是百变莫测啊[抓狂]</td>
</tr>
<tr>
<td>0</td>
<td>[衰][衰][衰]像给剥了皮的蛇</td>
</tr>
<tr>
<td>0</td>
<td>酒驾的危害，这回是潜水艇。//@上海译文丁丽洁:[泪]</td>
</tr>
<tr>
<td>0</td>
<td>积压了这么多的枕边书，没一本看完了的，现在我读书的最佳地点尽然是公交车[晕]</td>
</tr>
<tr>
<td>0</td>
<td>[泪]错过了……</td>
</tr>
</tbody>
</table>
</div>
<p><a href="https://github.com/Embedding/Chinese-Word-Vectors" target="_blank" rel="noopener">词向量下载</a></p>
<p><a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">中文预训练bert模型下载</a></p>
<p><a href="https://github.com/jeffery0628/text_classification" target="_blank" rel="noopener">情感二分类github仓库代码</a></p>
<a id="more"></a>
<h4 id="新闻十分类"><a href="#新闻十分类" class="headerlink" title="新闻十分类"></a>新闻十分类</h4><p>类别：’体育’ ‘娱乐’ ‘家居’ ‘房产’ ‘教育’ ‘时尚’ ‘时政’ ‘游戏’ ‘科技’ ‘财经’<br>训练集：50000 条数据，最大长度：27467，最小长度：8，类别个数：10,平均长度：913.31<br>验证集：5000 条数据，最大长度：10919，最小长度：15，类别个数：10<br>测试集：10000 条数据，最大长度：14720，最小长度：13，类别个数：10</p>
<p>使用数据集<a href="https://pan.baidu.com/s/1OOTK374IZ1DHnz5COUcfbw" target="_blank" rel="noopener">cnews</a>  密码:hf6o</p>
<p>训练集部分样例及每个类别的统计：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>label</th>
<th>text</th>
<th>数量</th>
</tr>
</thead>
<tbody>
<tr>
<td>体育</td>
<td>黄蜂vs湖人首发：科比带伤战保罗 加索尔救赎之战 新浪体育讯北京时间4月27日，NBA季后赛首轮洛杉矶湖人主场迎战新奥尔良黄蜂，此前的比赛中，双方战成2-2平，因此本场比赛对于两支球队来说都非常重要，赛前双方也公布了首发阵容：湖人队：费舍尔、科比、阿泰斯特、加索尔、拜纳姆黄蜂队：保罗、贝里内利、阿里扎、兰德里、奥卡福[新浪NBA官方微博][新浪NBA湖人新闻动态微博][新浪NBA专题]<a href="新浪体育">黄蜂vs湖人图文直播室</a></td>
<td>5000</td>
</tr>
<tr>
<td>娱乐</td>
<td>皮克斯首部3D动画《飞屋历险记》预告发布(图)视频：动画片《飞屋历险记》先行版43秒预告新浪娱乐讯 迪士尼、皮克斯2009暑期3D动画力作《飞屋历险记》(Up)发布预告片，虽然这款预告片仅有43秒，并且只出现了被汽球吊起来的房屋，但门前老爷爷卡尔的一声“下午好”着实让人忍俊不禁。该片由《怪兽电力公司》导演彼特·道格特(Pete Docter)执导，曾在《海底总动员》、《料理鼠王》担任编剧的皮克斯老班底鲍勃-派特森(Bob Peterson)亦将在本片担任共同导演，献出自己的导演处女作。《飞屋历险记》同时会是皮克斯有史以来第一部以3-D电影呈现的里程碑作品，之后皮克斯的所有影片都将制作成立体电影。《飞屋历险记》讲述了一老一少的冒险旅程。78岁的老翁卡尔·弗雷德里克森(Carl Fredricksen)一生中都梦想着能环游世界、出没于异境险地体验，却平淡地渡过了一生。在他生活的最后阶段，卡而仿佛在命运的安排下，带着8岁的亚裔小鬼头Russell一同踏上了冒险的旅程。这对一老一小的奇特组合肩并肩闯荡江湖，共同经历了荒野跋涉、丛林野兽与反派坏蛋的狙击。田野/文</td>
<td>5000</td>
</tr>
<tr>
<td>家居</td>
<td>橱柜价格关键在计价方式 教你如何挑选买过橱柜的人都知道，橱柜的计价很复杂，商家的报价方式也不尽相同，那么哪种计价方式对消费者更有利？在计价过程中应该注意哪些问题？消费者在购买橱柜之前一定要了解清楚。 橱柜的主要计价方式——延米计价和单元柜体计价 现在市场上橱柜主要有两种计价方式——延米计价和单元柜体计价。 延米计价是指地柜和吊柜各一米的总价(有些还包含台面)。在此基础上，如果有局部区域只要地柜不要吊柜，就会按就会按“2/8”或“4/6”的比例折算。如某橱柜材料的延米价为2000元/延米，某顾客做2米的吊柜、4米的地柜，则吊柜价=2000X0.4X2=1600元，地柜价=2000X0.6X4=4800元(此吊柜、地柜价按4/6的比例计算)，再加上所选台面、配件、电器等附加费用即为整套橱柜的价格。 延米报价有许多不合理之处，水槽、燃气灶、嵌入式电器等部分所需门板很少，但仍按延米来算价，对消费者来说很不划算。例如一款1000元/延米的橱柜，一个水槽约0.8米长，但消费者还是要按1000元的单价乘以0.8米付费，这个实际上只是几块材料简单组合的水槽柜需要消费者花800元，而同样材质、同样大小的水槽柜仅需400元左右，二者价格相差数百元。 按延米计价，所有的配件费用都是在原有的基础上增加，虽然有些厂家宣称抽屉、拉篮不加钱，但其实那是最基本的配置，一旦顾客要求调整方案，就会要多加钱，此外不足一米的部分要按一米计价，因此对顾客来说，如此计价会多花不少冤枉钱。 “单元柜体计价”是国际惯例的橱柜计价方式，是按每一个组成厨柜的单元柜价格进行计算，然后加出总价。具体为：某吊柜单价×个数+某地柜单价×个数……。利用单元柜体计价，更为合理。举个例子说，外观相同的柜体，抽屉数量、五金件、托架数量如果不同，在以延米计价时，商家往往只给消费者最简单、最省成本的产品。而按单元柜体计价，一款尺寸相同的抽屉柜可按不同配置报出不同价格：同样是一款30cm宽、66cm高的单体柜，如果门改成弧型，是多少钱；如果抽屉里加上可拆装的金属篮，是多少钱；如果抽屉的侧板是木质的多少钱……把橱柜的每个细节都分解开来，消费者可以在预算之内把可有可无的配置省掉，把钱花在自己更需要的功能上。 两全其美报价方式——延米计价和单元柜体计价相结合 现在中国橱柜市场上仍普遍采用延米计价，但进口品牌及国内一些大品牌橱柜都采用单元柜体计价方式，如德宝·西克曼、海尔等品牌即是采用单元柜体计价方式。不过德宝·西克曼厨柜的工作人员介绍到，如果一开始就用单元柜体计价来进行报价，不够直观，同时为了便于顾客进行比较，他们会用延米计价给顾客所选定的材料进行一个初始报价，让顾客对自己的厨房装修要花多少钱心里大概有个底。在对厨房进行量尺后，设计师会按照顾客的需求，设计出厨房效果图。这时，销售人员会按单元柜体计价给顾客进行一个报价。对于每一种标准柜体都有相应的报价，顾客实际用到几组柜子，将这些柜子价格累加，再加上台面及其他相关费用，便是整个橱柜的价格。</td>
<td>5000</td>
</tr>
<tr>
<td>房产</td>
<td>冯仑：继续增持高GDP城市商业地产确立商业地产投资战略不久的万通地产(企业专区,旗下楼盘)(600246)，今年上半年遭遇了业绩下滑。公司昨日公布的半年报显示，其商业物业在报告期内实现的营业收入同比下降33.71%，营业利润率比上年同期下降47.29个百分点。不过，公司董事长冯仑日前表示，依然看好人均GDP8000美元以上城市的商业地产，万通将继续增加高GDP城市的商业地产；计划用5-10年，商业物业收入占比达到30%-50%。逆向运作地产投资冯仑指出，根据历史经验，GDP的增长、城市化的增长，和房地产物业形态有一定关系，即人均GDP在8000美元以下时，住宅是市场的核心，主流产品都将围绕住宅展开。目前，在中国的城市中，人均GDP8000美元的城市大约有十个，大部分省会城市依然在3000美元至5000美元之间，因此，未来5-10年，中国房地产市场的产品结构仍然是以住宅为主。 冯仑认为，万通地产从现在开始扩大商业地产的比重，在目前的市场中，是一种逆向运作的思维，但符合长期趋势。他指出，在人均GDP达到8000美元的经济实体中，商用不动产会成为地产业的主角。以美国为例，商业地产的市场规模大约是住宅的两倍。中国商业地产未来的市场空间很大。根据万通地产的发展战略，除了在环渤海区域内发展住宅以外，还会重点发展商业不动产。未来，公司业务结构将逐步调整，商用地产的收入会逐年增加；今后，公司商业物业收入将占到整体营业收入的一半左右。对于目前商业地产面临的不景气局面，万通地产董事会秘书程晓?指出，公司战略不会因市场的短期波动而改变，公司将继续加大商用物业项目的投资力度，以营运带动开发，以财务安排的多样化实施商用物业投资。改变商业模式冯仑表示，就房地产开发模式而言，过去两百年主要经历了三次变化，即从“地主加工头”到“厂长加资本家”，再到“导演加制片”。目前，国内多数地产商的开发模式属于“地主加工头”和“厂长加资本家”的阶段；而商业地产的开发模式，不能停留在这两个阶段。所谓“导演加制片”模式，即由专业的房地产投资和资产管理公司负责运营商业地产项目，实现收入的多元化。而这种模式需要相应的金融创新产品支持。业内人士指出，房地产金融领域内的REITS、抵押贷款等金融产品体系的完善，将支持商用地产在一个多元化的不动产经营环境中快速的成长。而商业模式的改变需要较长一段时间。数据显示，香港主流房地产企业在人均GDP10000美元的时候开始逐步发展商业地产，先后经过13-15年确立起新的商业模式。其中，长江实业经过13年的发展，商业地产在业务机构的比重占到30%，新鸿基则经过15年的调整，商业地产比重占到50%。SOHO中国(企业专区,旗下楼盘)董事长潘石屹也指出，现在的市场虽然在调整，不过也给从事商业地产开发的企业提供了良好机会和平台，应及时在地域、开发物业的品种、品牌的建设、销售和持有物业的比重四个方面做出调整。 我要评论</td>
<td>5000</td>
</tr>
<tr>
<td>教育</td>
<td>2010年6月英语六级考试考后难度调查2010年6月大学英语六级考试将于19日下午举行，欢迎各位考生在考试后参加难度调查，发表你对这次考试的看法。点击进入论坛，参与考后大讨论</td>
<td>5000</td>
</tr>
<tr>
<td>时尚</td>
<td>组图：萝莉潮人示范春季复古实穿导语：萝莉潮人示范春季复古实穿，在乍暖还寒的初春，有的甜美、有的优雅、有的性感，但无论是哪种风格都给人强烈的视觉冲击力，在这个缤纷的春季更加脱俗动人。</td>
<td>5000</td>
</tr>
<tr>
<td>时政</td>
<td>香港特区行政长官曾荫权将离港休假中新社香港八月七日电 香港特区行政长官曾荫权将于八月九日至十五日离港休假。特区政府发言人七日透露，曾荫权离港期间，八月九日由特区财政司司长曾俊华署理行政长官职务；八月十日至十五日由政务司司长唐英年署理行政长官职务。(完)</td>
<td>5000</td>
</tr>
<tr>
<td>游戏</td>
<td>全国人大常委会将对59件法律相关条文作出修改新华社快讯：全国人大常委会27日表决通过了关于修改部分法律的决定，对59件法律的相关条文作出修改。</td>
<td>5000</td>
</tr>
<tr>
<td>科技</td>
<td>入门级时尚卡片机 尼康S220套装仅1150尼康S220延续了S系列纤巧超薄的机身设计，采用铝合金材质打造，表面质地细腻，不易沾染指纹。S220拥有紫色、深蓝、洋红、水晶绿和柔银五款靓丽颜色可供选择。</td>
<td>5000</td>
</tr>
</tbody>
</table>
</div>
<p>使用搜狗新闻词向量和字向量，300d：<a href="https://pan.baidu.com/s/1svFOwFBKnnlsqrF1t99Lnw" target="_blank" rel="noopener">词向量</a>,<a href="https://pan.baidu.com/s/1tUghuTno5yOvOx4LXA9-wg" target="_blank" rel="noopener">字向量</a></p>
<h3 id="数据处理代码（微博）"><a href="#数据处理代码（微博）" class="headerlink" title="数据处理代码（微博）"></a>数据处理代码（微博）</h3><h4 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WeiboDataset</span><span class="params">(NLPDataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,data_path,test=False,stop_words_path=None,batch_first=False,include_lengths=False,tokenizer_language=<span class="string">'cn'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param data_path:</span></span><br><span class="line"><span class="string">        :param test: 如果为测试集，则不加载label</span></span><br><span class="line"><span class="string">        :param stop_words_path:</span></span><br><span class="line"><span class="string">        :param batch_first:</span></span><br><span class="line"><span class="string">        :param include_lengths:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.data = pd.read_csv(data_path)</span><br><span class="line">        print(<span class="string">'read data from &#123;&#125;'</span>.format(data_path))</span><br><span class="line">        self.text_field = <span class="string">"review"</span></span><br><span class="line">        self.label_field = <span class="string">"label"</span></span><br><span class="line">        self.test = test</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> stop_words_path:</span><br><span class="line">            stop_words = read_stop_words(stop_words_path)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            stop_words = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.LABEL = LabelField(sequential=<span class="literal">False</span>, use_vocab=<span class="literal">False</span>, dtype=torch.float)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># lambda x: [y for y in x]</span></span><br><span class="line">        self.TEXT = Field(sequential=<span class="literal">True</span>,stop_words=stop_words, tokenize=<span class="keyword">lambda</span> x: [y <span class="keyword">for</span> y <span class="keyword">in</span> x], batch_first=batch_first,tokenizer_language=tokenizer_language,</span><br><span class="line">                          include_lengths=include_lengths)  <span class="comment"># include_lengths=True for LSTM</span></span><br><span class="line"></span><br><span class="line">        self.fields = [(<span class="string">"text"</span>, self.TEXT), (<span class="string">"label"</span>, self.LABEL)]</span><br><span class="line"></span><br><span class="line">        self.examples = self.build_examples()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_examples</span><span class="params">(self)</span>:</span></span><br><span class="line">        examples = []</span><br><span class="line">        <span class="keyword">if</span> self.test:</span><br><span class="line">            <span class="comment"># 如果为测试集，则不加载label</span></span><br><span class="line">            <span class="keyword">for</span> text <span class="keyword">in</span> tqdm(self.data[self.text_field]):</span><br><span class="line">                examples.append(Example.fromlist([text, <span class="literal">None</span>], self.fields))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> text, label <span class="keyword">in</span> tqdm(zip(self.data[self.text_field], self.data[self.label_field])):</span><br><span class="line">                <span class="comment"># Example: Defines a single training or test example.</span></span><br><span class="line">                <span class="comment"># Stores each column of the example as an attribute.</span></span><br><span class="line">                examples.append(Example.fromlist([text, label], self.fields))</span><br><span class="line">        <span class="keyword">return</span> examples</span><br></pre></td></tr></table></figure>
<h4 id="dataloader"><a href="#dataloader" class="headerlink" title="dataloader"></a>dataloader</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WeiboDataLoader</span><span class="params">(NLPDataLoader)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,dataset,split_ratio=None, batch_size=<span class="number">64</span>, sort_key=None, device=None,train=True, repeat=False, shuffle=None, sort=None,sort_within_batch=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 batch_size_fn=None,  use_pretrained_word_embedding=False, word_embedding_path=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        微博二分类数据集加载器</span></span><br><span class="line"><span class="string">        :param dataset:</span></span><br><span class="line"><span class="string">        :param split_ratio:</span></span><br><span class="line"><span class="string">        :param batch_size:</span></span><br><span class="line"><span class="string">        :param device:</span></span><br><span class="line"><span class="string">        :param train: Whether the iterator represents a train set</span></span><br><span class="line"><span class="string">        :param repeat:</span></span><br><span class="line"><span class="string">        :param shuffle:</span></span><br><span class="line"><span class="string">        :param sort:</span></span><br><span class="line"><span class="string">        :param sort_within_batch:</span></span><br><span class="line"><span class="string">        :param use_pretrained_word_embedding:</span></span><br><span class="line"><span class="string">        :param word_embedding_path:</span></span><br><span class="line"><span class="string">        :param vocab_size:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 构建数据集</span></span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        <span class="comment"># 迭代器参数</span></span><br><span class="line">        self.split_ratio = split_ratio</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.sort_key = sort_key</span><br><span class="line">        self.device = device</span><br><span class="line">        self.batch_size_fn = batch_size_fn</span><br><span class="line">        self.train = train</span><br><span class="line">        self.repeat = repeat</span><br><span class="line">        self.shuffle = shuffle</span><br><span class="line">        self.sort = sort</span><br><span class="line">        self.sort_within_batch = sort_within_batch</span><br><span class="line">        self.use_pretrained_word_embedding = use_pretrained_word_embedding</span><br><span class="line">        self.word_embedding_path = word_embedding_path</span><br><span class="line">        self.train_iter, self.valid_iter, self.test_iter = self.get_iterators()</span><br><span class="line">        self.vocab = self.dataset.TEXT.vocab</span><br><span class="line">        self.vocab.pad_index = self.dataset.TEXT.vocab.stoi[self.dataset.TEXT.pad_token]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_iterators</span><span class="params">(self)</span>:</span></span><br><span class="line">        train_iter, valid_iter, test_iter = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.split_ratio:</span><br><span class="line">            <span class="comment"># 构建词汇表</span></span><br><span class="line">            <span class="keyword">if</span> self.use_pretrained_word_embedding:</span><br><span class="line">                vectors = Vectors(name=self.word_embedding_path)  <span class="comment"># 使用预训练的词向量</span></span><br><span class="line">                self.dataset.TEXT.build_vocab(self.dataset, vectors=vectors,</span><br><span class="line">                                              unk_init=torch.Tensor.normal_)</span><br><span class="line">                self.dataset.LABEL.build_vocab(self.dataset)</span><br><span class="line">                print(<span class="string">'label types:&#123;&#125;'</span>.format(self.dataset.LABEL.vocab.stoi))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.dataset.TEXT.build_vocab(self.dataset)  <span class="comment"># 不使用预训练的词向量</span></span><br><span class="line">                self.dataset.LABEL.build_vocab(self.dataset)</span><br><span class="line">                print(<span class="string">'label types:&#123;&#125;'</span>.format(self.dataset.LABEL.vocab.stoi))</span><br><span class="line"></span><br><span class="line">            train_iter = Iterator(self.dataset, batch_size=self.batch_size, device=self.device, sort_key=self.sort_key,</span><br><span class="line">                                  sort_within_batch=self.sort_within_batch, repeat=self.repeat,</span><br><span class="line">                                  batch_size_fn=self.batch_size_fn</span><br><span class="line">                                  , train=self.train, shuffle=self.shuffle, sort=self.sort)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            train_data, valid_data, test_data = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">            <span class="keyword">if</span> isinstance(self.split_ratio, list):</span><br><span class="line">                <span class="keyword">if</span> len(self.split_ratio) == <span class="number">3</span>:</span><br><span class="line">                    train_data, valid_data, test_data = self.dataset.split(split_ratio=self.split_ratio)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    train_data, valid_data = self.dataset.split(split_ratio=self.split_ratio)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                train_data, valid_data = self.dataset.split(split_ratio=self.split_ratio)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 构建词汇表</span></span><br><span class="line">            <span class="keyword">if</span> self.use_pretrained_word_embedding:</span><br><span class="line">                vectors = Vectors(name=self.word_embedding_path)  <span class="comment"># 使用预训练的词向量</span></span><br><span class="line">                self.dataset.TEXT.build_vocab(train_data, vectors=vectors,</span><br><span class="line">                                              unk_init=torch.Tensor.normal_)</span><br><span class="line">                self.dataset.LABEL.build_vocab(train_data)</span><br><span class="line">                print(<span class="string">'label types:&#123;&#125;'</span>.format(self.dataset.LABEL.vocab.stoi))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.dataset.TEXT.build_vocab(train_data)  <span class="comment"># 不使用预训练的词向量</span></span><br><span class="line">                self.dataset.LABEL.build_vocab(train_data)</span><br><span class="line">                print(<span class="string">'label types:&#123;&#125;'</span>.format(self.dataset.LABEL.vocab.stoi))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 构建迭代器</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> train_data:</span><br><span class="line">                train_iter = Iterator(train_data, batch_size=self.batch_size, device=self.device,</span><br><span class="line">                                      sort_key=self.sort_key,</span><br><span class="line">                                      sort_within_batch=self.sort_within_batch, repeat=self.repeat,</span><br><span class="line">                                      batch_size_fn=self.batch_size_fn</span><br><span class="line">                                      , train=self.train, shuffle=self.shuffle, sort=self.sort)</span><br><span class="line">            <span class="keyword">if</span> valid_data:</span><br><span class="line">                valid_iter = Iterator(valid_data, batch_size=self.batch_size, device=self.device,</span><br><span class="line">                                      sort_key=self.sort_key,</span><br><span class="line">                                      sort_within_batch=self.sort_within_batch, repeat=self.repeat,</span><br><span class="line">                                      batch_size_fn=self.batch_size_fn</span><br><span class="line">                                      , train=self.train, shuffle=self.shuffle, sort=self.sort)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                test_iter = Iterator(test_data, batch_size=self.batch_size, device=self.device, sort_key=self.sort_key,</span><br><span class="line">                                     sort_within_batch=self.sort_within_batch, repeat=self.repeat,</span><br><span class="line">                                     batch_size_fn=self.batch_size_fn</span><br><span class="line">                                     , train=self.train, shuffle=self.shuffle, sort=self.sort)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> train_iter, valid_iter, test_iter</span><br></pre></td></tr></table></figure>
<h4 id="Vocab属性："><a href="#Vocab属性：" class="headerlink" title="Vocab属性："></a>Vocab属性：</h4><ol>
<li>freqs</li>
<li>itos</li>
<li>stoi</li>
<li>vectors</li>
</ol>
<h4 id="Bert-Dataprocess"><a href="#Bert-Dataprocess" class="headerlink" title="Bert Dataprocess"></a>Bert Dataprocess</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertDataProcess</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data_path, batch_size, device, processed_data_path=None, bert_model_path=None)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.bert_model_path = bert_model_path</span><br><span class="line">        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_path)</span><br><span class="line">        self.CLS = self.bert_tokenizer.cls_token</span><br><span class="line">        self.data_path = data_path</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.device = device</span><br><span class="line">        self.processed_data_path = processed_data_path  <span class="comment"># 经过load_data 处理过的数据保存到该路径下。</span></span><br><span class="line">        self.train_data,self.val_data = self.built_dataset()</span><br><span class="line">        self.train_iter = self.built_iterater(self.train_data)</span><br><span class="line">        self.val_iter = self.built_iterater(self.val_data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">(self, path)</span>:</span></span><br><span class="line">        contents = []</span><br><span class="line">        <span class="keyword">with</span> open(path, <span class="string">'r'</span>, encoding=<span class="string">'UTF-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(f):</span><br><span class="line">                lin = line.strip()</span><br><span class="line">                <span class="keyword">if</span> <span class="string">'label,review'</span> <span class="keyword">in</span> lin:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> lin:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                label, content = lin.split(<span class="string">','</span>,<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># print("content&gt;&gt;&gt;:", content)</span></span><br><span class="line">                token = self.bert_tokenizer.tokenize(content)</span><br><span class="line">                token = [self.CLS] + token</span><br><span class="line">                seq_len = len(token)</span><br><span class="line">                token_ids = self.bert_tokenizer.convert_tokens_to_ids(token)</span><br><span class="line">                contents.append((token_ids, int(label), seq_len))</span><br><span class="line">        <span class="keyword">return</span> contents</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">built_dataset</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(self.processed_data_path):</span><br><span class="line">            all_data = self.load_dataset(self.data_path)</span><br><span class="line">            pickle.dump(all_data, open(self.processed_data_path,<span class="string">'wb'</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            all_data = pickle.load(open(self.processed_data_path,<span class="string">'rb'</span>))</span><br><span class="line"></span><br><span class="line">        train_data, val_data = train_test_split(all_data, test_size=<span class="number">0.3</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> train_data, val_data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">built_iterater</span><span class="params">(self, dataset)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> BertDatasetIterater(dataset, self.batch_size, self.device,self.bert_model_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertDatasetIterater</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, batches, batch_size, device,bert_model_path)</span>:</span></span><br><span class="line">        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_path)</span><br><span class="line"></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.batches = batches</span><br><span class="line">        self.n_batches = len(batches) // batch_size</span><br><span class="line">        self.residue = <span class="literal">False</span>  <span class="comment"># 记录batch数量是否为整数</span></span><br><span class="line">        <span class="keyword">if</span> len(batches) % self.n_batches != <span class="number">0</span>:</span><br><span class="line">            self.residue = <span class="literal">True</span></span><br><span class="line">        self.index = <span class="number">0</span></span><br><span class="line">        self.device = device</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_mask_sentence_len</span><span class="params">(self,content,max_sent_len)</span>:</span></span><br><span class="line">        pad_index = self.bert_tokenizer.pad_token_id</span><br><span class="line">        sent_len = len(content)</span><br><span class="line">        mask = [<span class="number">1</span>]*sent_len + ([<span class="number">0</span>] * (max_sent_len-sent_len))</span><br><span class="line">        content = content.extend([pad_index] * (max_sent_len-sent_len))</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_to_tensor</span><span class="params">(self, datas)</span>:</span></span><br><span class="line">        datas = pd.DataFrame(datas,columns=[<span class="string">'content'</span>,<span class="string">'label'</span>,<span class="string">'sent len'</span>])</span><br><span class="line">        max_sent_len = max(datas[<span class="string">'sent len'</span>])</span><br><span class="line">        datas[<span class="string">"mask"</span>] = datas[<span class="string">"content"</span>].apply(self._mask_sentence_len,args=&#123;max_sent_len&#125;)</span><br><span class="line"></span><br><span class="line">        x = torch.LongTensor(pd.DataFrame(datas[<span class="string">'content'</span>].to_list()).values).to(self.device)</span><br><span class="line">        y = torch.FloatTensor(datas[<span class="string">'label'</span>].values).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pad前的长度(超过pad_size的设为pad_size)</span></span><br><span class="line">        seq_len = torch.LongTensor(datas[<span class="string">'sent len'</span>]).to(self.device)</span><br><span class="line">        mask = torch.LongTensor(pd.DataFrame(datas[<span class="string">'mask'</span>].to_list()).values).to(self.device)</span><br><span class="line">        <span class="keyword">return</span> (x, seq_len, mask), y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.residue <span class="keyword">and</span> self.index == self.n_batches:</span><br><span class="line">            batches = self.batches[self.index * self.batch_size: len(self.batches)]</span><br><span class="line">            self.index += <span class="number">1</span></span><br><span class="line">            batches = self._to_tensor(batches)</span><br><span class="line">            <span class="keyword">return</span> batches</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> self.index &gt; self.n_batches:</span><br><span class="line">            self.index = <span class="number">0</span></span><br><span class="line">            <span class="keyword">raise</span> StopIteration</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            batches = self.batches[self.index * self.batch_size: (self.index + <span class="number">1</span>) * self.batch_size]</span><br><span class="line">            self.index += <span class="number">1</span></span><br><span class="line">            batches = self._to_tensor(batches)</span><br><span class="line">            <span class="keyword">return</span> batches</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.residue:</span><br><span class="line">            <span class="keyword">return</span> self.n_batches + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.n_batches</span><br></pre></td></tr></table></figure>
<h3 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>fastText是Facebook于2016年开源的一个词向量计算和文本分类工具。在文本分类任务中，fastText（浅层网络）往往能取得和深度网络相媲美的精度，却在训练时间上比深度网络快许多数量级。在标准的多核CPU上， 在10分钟之内能够训练10亿词级别语料库的词向量，在1分钟之内能够分类有着30万多类别的50多万句子。</p>
<p>fastText是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：<br>1、fastText在保持高精度的情况下加快了训练速度和测试速度<br>2、fastText不需要预训练好的词向量，fastText会自己训练词向量<br>3、fastText两个重要的优化：Hierarchical Softmax、N-gram</p>
<h4 id="fastText模型架构"><a href="#fastText模型架构" class="headerlink" title="fastText模型架构"></a>fastText模型架构</h4><p>fastText模型架构和word2vec中的CBOW很相似， 不同之处是fastText预测标签而CBOW预测的是中间词，即模型架构类似但是模型的任务不同。下面我们先看一下CBOW的架构：</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/cbow.png" alt="avatar"></p>
<p>word2vec将上下文关系转化为多分类任务，进而训练逻辑回归模型，这里的类别数量|V|词库大小。通常的文本数据中，词库少则数万，多则百万，在训练中直接训练多分类逻辑回归并不现实。word2vec中提供了两种针对大规模多分类问题的优化手段， negative sampling 和hierarchical softmax。在优化中，negative sampling 只更新少量负面类，从而减轻了计算量。hierarchical softmax 将词库表示成前缀树，从树根到叶子的路径可以表示为一系列二分类器，一次多分类计算的复杂度从|V|降低到了树的高度<br>fastText模型架构:其中x1,x2,…,xN−1,xN表示一个文本中的n-gram向量，每个特征是词向量的平均值。这和前文中提到的cbow相似，cbow用上下文去预测中心词，而此处用全部的n-gram去预测指定类别</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/fasttext.png" alt="avatar"></p>
<h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h4><blockquote>
<p>我不喜欢这类电影，但是喜欢这一个。</p>
<p>我喜欢这类电影，但是不喜欢这一个。</p>
</blockquote>
<p><strong>这样的两句句子经过词向量平均以后已经送入单层神经网络的时候已经完全一模一样了，分类器不可能分辨出这两句话的区别</strong>，只有添加n-gram特征以后才可能有区别。因此，在实际应用的时候需要对数据有足够的了解,然后在选择模型。</p>
<h4 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FastText</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab, embedding_dim, output_dim, use_pretrain_embedding=False)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        <span class="keyword">if</span> use_pretrain_embedding:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(vocab.vectors)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.embedding = nn.Embedding(len(vocab), embedding_dim, padding_idx=vocab.pad_index)</span><br><span class="line">        <span class="comment"># 把unknown 和 pad 向量设置为零</span></span><br><span class="line">        self.embedding.weight.data[vocab.unk_index] = torch.zeros(embedding_dim)</span><br><span class="line">        self.embedding.weight.data[vocab.pad_index] = torch.zeros(embedding_dim)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(embedding_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [sent len, batch size]</span></span><br><span class="line"></span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedded = [sent len, batch size, emb dim]</span></span><br><span class="line"></span><br><span class="line">        embedded = embedded.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line"></span><br><span class="line">        pooled = F.avg_pool2d(embedded, (embedded.shape[<span class="number">1</span>], <span class="number">1</span>)).squeeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pooled = [batch size, embedding_dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.fc(pooled)</span><br></pre></td></tr></table></figure>
<h3 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h3><h4 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h4><p><strong>Yoon Kim</strong>在论文<a href="https://arxiv.org/abs/1408.5882" target="_blank" rel="noopener">(2014 EMNLP) Convolutional Neural Networks for Sentence Classification</a>提出TextCNN。</p>
<p>将<strong>卷积神经网络CNN</strong>应用到<strong>文本分类</strong>任务，利用<strong>多个不同size的kernel</strong>来提取句子中的关键信息（类似于多窗口大小的ngram)，从而能够更好地捕捉局部相关性。</p>
<h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/textcnn.png" alt="avatar"></p>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>TextCNN的详细过程原理图如下：</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/textcnndetail.png" alt="avatar"></p>
<p>TextCNN详细过程：</p>
<ul>
<li><strong>Embedding</strong>：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点。</li>
<li><strong>Convolution</strong>：然后经过 kernel_sizes=(2,3,4) 的一维卷积层，每个kernel_size 有两个输出 channel。</li>
<li><strong>MaxPolling</strong>：第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示。</li>
<li><p><strong>FullConnection and Softmax</strong>：最后接一层全连接的 softmax 层，输出每个类别的概率。</p>
<p>通道（Channels）：</p>
</li>
<li><p>图像中可以利用 (R, G, B) 作为不同channel；</p>
</li>
<li>文本的输入的channel通常是不同方式的embedding方式（比如 word2vec或Glove），实践中也有利用静态词向量和fine-tunning词向量作为不同channel的做法。</li>
</ul>
<p>一维卷积（conv-1d）：</p>
<ul>
<li>图像是二维数据；</li>
<li><strong>文本是一维数据，因此在TextCNN卷积用的是一维卷积</strong>（在<strong>word-level</strong>上是一维卷积；虽然文本经过词向量表达后是二维数据，但是在embedding-level上的二维卷积没有意义）。一维卷积带来的问题是需要<strong>通过设计不同 kernel_size 的 filter 获取不同宽度的视野</strong>。</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><p>TextCNN模型最大的问题也是这个全局的max pooling丢失了结构信息，因此很难去发现文本中的转折关系等复杂模式。针对这个问题，可以尝试k-max pooling做一些优化，k-max pooling针对每个卷积核都不只保留最大的值，他保留前k个最大值，并且保留这些值出现的顺序，也即按照文本中的位置顺序来排列这k个最大值。在某些比较复杂的文本上相对于1-max pooling会有提升。</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab, embedding_dim, n_filters, filter_sizes, output_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout, use_pretrain_embedding=False)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        <span class="keyword">if</span> use_pretrain_embedding:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(vocab.vectors)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.embedding = nn.Embedding(len(vocab), embedding_dim, padding_idx=vocab.pad_index)</span><br><span class="line">        <span class="comment"># 把unknown 和 pad 向量设置为零</span></span><br><span class="line">        self.embedding.weight.data[vocab.unk_index] = torch.zeros(embedding_dim)</span><br><span class="line">        self.embedding.weight.data[vocab.pad_index] = torch.zeros(embedding_dim)</span><br><span class="line"></span><br><span class="line">        self.convs = nn.ModuleList([</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">1</span>,</span><br><span class="line">                      out_channels=n_filters,</span><br><span class="line">                      kernel_size=(fs, embedding_dim))</span><br><span class="line">            <span class="keyword">for</span> fs <span class="keyword">in</span> filter_sizes</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [batch size, sent len]</span></span><br><span class="line"></span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line"></span><br><span class="line">        embedded = embedded.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedded = [batch size, 1, sent len, emb dim]</span></span><br><span class="line"></span><br><span class="line">        conved = [F.relu(conv(embedded)).squeeze(<span class="number">3</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]</span></span><br><span class="line"></span><br><span class="line">        pooled = [F.max_pool1d(conv, conv.shape[<span class="number">2</span>]).squeeze(<span class="number">2</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> conved]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pooled_n = [batch size, n_filters]</span></span><br><span class="line"></span><br><span class="line">        cat = self.dropout(torch.cat(pooled, dim=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># cat = [batch size, n_filters * len(filter_sizes)]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.fc(cat)</span><br></pre></td></tr></table></figure>
<h3 id="HAN（Hierarchy-Attention-Network）"><a href="#HAN（Hierarchy-Attention-Network）" class="headerlink" title="HAN（Hierarchy Attention Network）"></a>HAN（Hierarchy Attention Network）</h3><p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/HAN.png" alt="avatar"></p>
<p>整个网络结构包括五个部分：</p>
<ol>
<li>词序列编码器</li>
<li>基于词级的注意力层</li>
<li>句子编码器</li>
<li>基于句子级的注意力层</li>
<li>分类</li>
</ol>
<p>整个网络结构由双向GRU网络和注意力机制组合而成。</p>
<h4 id="词序列编码器"><a href="#词序列编码器" class="headerlink" title="词序列编码器"></a>词序列编码器</h4><p>给定一个句子中的单词$w_{it}$，其中 $i$ 表示第$i$ 个句子，$t$ 表示第 $t$ 个词。通过一个词嵌入矩阵 $W_e$ 将单词转换成向量表示，具体如下所示：</p>
<script type="math/tex; mode=display">
x_{it}=W_e w_{it}</script><p>利用双向GRU实现的整个编码流程：</p>
<script type="math/tex; mode=display">
\begin{aligned}
x_{i t} &=W_{e} w_{i t}, t \in[1, T] \\
\overrightarrow{h}_{i t} &=\overrightarrow{\operatorname{GRU}}\left(x_{i t}\right), t \in[1, T] \\
\overleftarrow{h}_{i t} &=\overleftarrow{\operatorname{GRU}}\left(x_{i t}\right), t \in[T, 1] \\
{h}_{i t} &= [\overrightarrow{h}_{i t},\overleftarrow{h}_{i t} ]
\end{aligned}</script><h4 id="词级的注意力层"><a href="#词级的注意力层" class="headerlink" title="词级的注意力层"></a>词级的注意力层</h4><p>但是对于一句话中的单词，并不是每一个单词对分类任务都是有用的，比如在做文本的情绪分类时，可能我们就会比较关注“很好”、“伤感”这些词。为了能使循环神经网络也能自动将“注意力”放在这些词汇上，作者设计了基于单词的注意力层的具体流程如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
u_{i t} &=\tanh \left(W_{w} h_{i t}+b_{w}\right) \\
\alpha_{i t} &=\frac{\exp \left(u_{i t}^{\top} u_{w}\right)}{\sum_{t} \exp \left(u_{i t}^{\top} u_{w}\right)} \\
s_{i} &=\sum_{t} \alpha_{i t} h_{i t}
\end{aligned}</script><p>上面式子中，$u_{it}$ 是 $h_{it}$ 的隐层表示，$a_{it}$ 是经 softmax 函数处理后的归一化权重系数，$u_w$是一个随机初始化的向量，之后会作为模型的参数一起被训练，$s_i$ 就是我们得到的第 i 个句子的向量表示。</p>
<h4 id="句子编码器"><a href="#句子编码器" class="headerlink" title="句子编码器"></a>句子编码器</h4><p>句子编码器也是基于双向GRU实现编码的，</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\overrightarrow{h}_{i}=\overrightarrow{\operatorname{GRU}}\left(s_{i}\right), i \in[1, L]\\
&\overleftarrow{h}_{i}=\overleftarrow{\operatorname{GRU}}\left(s_{i}\right), t \in[L, 1]
\end{aligned}</script><p>公式和词编码类似，最后的 $h_i$ 也是通过拼接得到的.</p>
<h4 id="句子级注意力层"><a href="#句子级注意力层" class="headerlink" title="句子级注意力层"></a>句子级注意力层</h4><p>注意力层的流程如下，和词级的一致:</p>
<script type="math/tex; mode=display">
\begin{aligned}
u_{i} &=\tanh \left(W_{s} h_{i}+b_{s}\right) \\
\alpha_{i} &=\frac{\exp \left(u_{i}^{\top} u_{s}\right)}{\sum_{i} \exp \left(u_{i}^{\top} u_{s}\right)} \\
v &=\sum_{i} \alpha_{i} h_{i}
\end{aligned}</script><p>最后得到的向量$v$ 就是文档的向量表示，这是文档的高层表示。接下来就可以用可以用这个向量表示作为文档的特征。</p>
<h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><p>使用最常用的softmax分类器对整个文本进行分类了</p>
<script type="math/tex; mode=display">
p=\operatorname{softmax}\left(W_{c} v+b_{c}\right)</script><p>损失函数</p>
<script type="math/tex; mode=display">
L=-\sum_{d} \log p_{d j}</script><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><h3 id="RNN系列"><a href="#RNN系列" class="headerlink" title="RNN系列"></a>RNN系列</h3><p>通过将前一时刻的运算结果添加到当前的运算中，从而实现了“考虑上文信息”的功能。</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/rnn.png" alt="avatar"></p>
<p>RNN可以考虑上文的信息，那么如何将下文的信息也添加进去呢？这就是BiRNN要做的事情。</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/birnn.png" alt="avatar"></p>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>因为RNN存在梯度弥散和梯度爆炸的问题，所以RNN很难完美地处理具有长期依赖的信息。既然仅仅依靠一条线连接后面的神经元不足以解决问题，那么就再加一条线好了，这就是LSTM。</p>
<p>LSTM的关键在于细胞的状态和穿过细胞的线，细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流动保持不变会变得容易。</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/lstm_c.png" alt="avatar"></p>
<p>在LSTM中，门可以实现选择性的让信息通过，主要通过一个sigmoid的神经层和一个逐点相乘的操作来实现。LSTM通过三个这样的门结构来实现信息的保护和控制，分别是遗忘门（forget gate）、输入门（input gate）与输出门（output gate）。</p>
<h5 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a><strong>遗忘门</strong></h5><p>在LSTM中的第一步是决定从细胞状态中丢弃什么信息。这个决定通过一个称为遗忘门的结构来完成。遗忘门会读取$h_{t-1}$和$x_t$，输出一个0到1之间的数值给细胞的状态$c_{t-1}$中的数字。1表示完全保留，0表示完全舍弃。</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/lstm_forget_gate.png" alt="avatar"></p>
<h5 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a><strong>输入门</strong></h5><p>遗忘门决定让多少新的信息加入到cell的状态中来。实现这个需要两个步骤：</p>
<ol>
<li><p>首先一个叫“input gate layer”的sigmoid层决定哪些信息需要更新；一个tanh层生成一个向量，用来更新状态C。</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/input_gate.png" alt="avatar"></p>
</li>
<li><p>把 1 中的两部分联合起来，对cell的状态进行更新，我们把旧的状态与$f_t$相乘，丢弃掉我们确定需要丢弃的信息，接着加上$i_t * \tilde{C}_{t}$</p>
</li>
</ol>
<h5 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h5><p>最终，我们需要确定输出什么值，这个输出将会基于我们的细胞的状态，但是也是一个过滤后的版本。首先，我们通过一个sigmoid层来确定细胞状态的哪些部分将输出出去。接着，我们把细胞状态通过tanh进行处理（得到一个-1到1之间的值）并将它和sigmoid门的输出相乘，最终我们仅仅会输出我们确定输出的部分。</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/output_gate.png" alt="avatar"></p>
<h5 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h5><p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/lstm_all.png" alt="avatar"></p>
<h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><p>在LSTM中引入了三个门函数：输入门、遗忘门和输出门来控制输入值、记忆值和输出值。而在GRU模型中只有两个门：分别是更新门和重置门。</p>
<p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/GRU.png" alt="avatar"></p>
<p>图中的$z_t$和$r_t$分别表示更新门和重置门。更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多。重置门控制前一状态有多少信息被写入到当前的候选集 $\tilde{h}_{t}$上，重置门越小，前一状态的信息被写入的越少。</p>
<p>LSTM和CRU都是通过各种门函数来将重要特征保留下来，这样就保证了在long-term传播的时候也不会丢失。此外GRU相对于LSTM少了一个门函数，因此在参数的数量上也是要少于LSTM的，所以整体上GRU的训练速度要快于LSTM的。</p>
<h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h4><p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/self-attention.png" alt="avatar"></p>
<ol>
<li>Encode所有输入序列,得到对应的$h_1,h_2, \cdots ,h_T$(T为输入序列长度)</li>
<li>Decode输出目标$y_t$之前，会将上一步输出的隐藏状态$S_{t-1}$与之前encode好的$h_1,h_2,\cdots,h_T$进行比对，计算相似度（$e_{t,j}=a(s_{t-1},h_j)$）,$h_j$为之前第j个输入encode得到的隐藏向量，a为任意一种计算相似度的方式</li>
<li>然后通过softmax，即$a_{t,j}=\frac{exp(e_{t,j})}{\sum^{T_x}_{k=1}exp(e_{t,k})}$将之前得到的各个部分的相关系数进行归一化，得到$a_{t,1},a_{t,2},\cdots,a_{t,T}$</li>
<li>在对输入序列的隐藏层进行相关性加权求和得到此时decode需要的context vector ：$</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RnnModel</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, vocab, embedding_dim, hidden_dim, output_dim, n_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 bidirectional, dropout, batch_first=False,use_pretrain_embedding=False)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        <span class="keyword">if</span> use_pretrain_embedding:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(vocab.vectors)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.embedding = nn.Embedding(len(vocab), embedding_dim, padding_idx=vocab.pad_index)</span><br><span class="line">        <span class="comment"># 把unknown 和 pad 向量设置为零</span></span><br><span class="line">        self.embedding.weight.data[vocab.unk_index] = torch.zeros(embedding_dim)</span><br><span class="line">        self.embedding.weight.data[vocab.pad_index] = torch.zeros(embedding_dim)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(embedding_dim,</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(embedding_dim,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(embedding_dim,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(hidden_dim * n_layers, output_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [sent len, batch size]</span></span><br><span class="line"></span><br><span class="line">        embedded = self.dropout(self.embedding(text))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedded = [sent len, batch size, emb dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.bidirectional:</span><br><span class="line">            hidden = torch.reshape(hidden,(hidden.shape[<span class="number">1</span>],self.hidden_dim * self.n_layers))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hidden = torch.reshape(hidden, (<span class="number">-1</span>,hidden.shape[<span class="number">1</span>], self.hidden_dim * self.n_layers))</span><br><span class="line">            hidden = torch.mean(hidden,dim=<span class="number">0</span>)</span><br><span class="line">        output = torch.sum(output,dim=<span class="number">0</span>)</span><br><span class="line">        fc_input = self.dropout(output+hidden)</span><br><span class="line">        out = self.fc(fc_input)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h4 id="Rnn-Attenton"><a href="#Rnn-Attenton" class="headerlink" title="Rnn-Attenton"></a>Rnn-Attenton</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RnnAttentionModel</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, vocab, embedding_dim, hidden_dim, output_dim, n_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 bidirectional, dropout, batch_first=False,use_pretrain_embedding=False)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_pretrain_embedding:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(vocab.vectors)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.embedding = nn.Embedding(len(vocab), embedding_dim, padding_idx=vocab.pad_index)</span><br><span class="line">        <span class="comment"># 把unknown 和 pad 向量设置为零</span></span><br><span class="line">        self.embedding.weight.data[vocab.unk_index] = torch.zeros(embedding_dim)</span><br><span class="line">        self.embedding.weight.data[vocab.pad_index] = torch.zeros(embedding_dim)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(embedding_dim,</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(embedding_dim,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(embedding_dim,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.tanh1 = nn.Tanh()</span><br><span class="line">        self.tanh2 = nn.Tanh()</span><br><span class="line">        <span class="comment"># self.u = nn.Parameter(torch.Tensor(self.hidden_dim * 2,self.hidden_dim*2))</span></span><br><span class="line">        self.w = nn.Parameter(torch.randn(hidden_dim),requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="keyword">if</span> bidirectional:</span><br><span class="line">            self.w = nn.Parameter(torch.randn(hidden_dim * <span class="number">2</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">            self.fc = nn.Linear(hidden_dim * <span class="number">2</span>, output_dim)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.w = nn.Parameter(torch.randn(hidden_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line">            self.fc = nn.Linear(hidden_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [sent len, batch size]</span></span><br><span class="line"></span><br><span class="line">        embedded = self.dropout(self.embedding(text))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedded = [sent len, batch size, emb dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        <span class="comment"># output = [sent len, batch size, hidden dim * num_direction]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># attention</span></span><br><span class="line">        <span class="comment"># M = [sent len, batch size, hidden dim * num_direction]</span></span><br><span class="line">        <span class="comment"># M = self.tanh1(output)</span></span><br><span class="line"></span><br><span class="line">        alpha = F.softmax(torch.matmul(self.tanh1(output), self.w), dim=<span class="number">0</span>).unsqueeze(<span class="number">-1</span>) <span class="comment"># dim=0表示针对文本中的每个词的输出softmax</span></span><br><span class="line">        output_attention = output * alpha</span><br><span class="line"></span><br><span class="line">        <span class="comment"># hidden = [n_layers * num_direction,batch_size, hidden_dim]</span></span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            hidden = torch.mean(torch.reshape(hidden, (<span class="number">-1</span>,hidden.shape[<span class="number">1</span>], self.hidden_dim * <span class="number">2</span>)),dim=<span class="number">0</span>)  <span class="comment"># hidden = [batch_size, hidden_dim * num_direction]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hidden = torch.mean(torch.reshape(hidden, (<span class="number">-1</span>, hidden.shape[<span class="number">1</span>], self.hidden_dim)), dim=<span class="number">0</span>)   <span class="comment"># hidden = [batch_size, hidden_dim]</span></span><br><span class="line"></span><br><span class="line">        output_attention = torch.sum(output_attention,dim=<span class="number">0</span>)</span><br><span class="line">        output = torch.sum(output,dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        fc_input = self.dropout(output+output_attention+hidden)</span><br><span class="line">        <span class="comment"># fc_input = self.dropout(output_attention)</span></span><br><span class="line">        out = self.fc(fc_input)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h3 id="TextRCNN"><a href="#TextRCNN" class="headerlink" title="TextRCNN"></a>TextRCNN</h3><h4 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h4><p>就深度学习领域来说，RNN和CNN作为文本分类问题的主要模型架构，都存在各自的优点及局限性。RNN擅长处理序列结构，能够考虑到句子的上下文信息，但RNN属于“biased model”，一个句子中越往后的词重要性越高，这有可能影响最后的分类结果，因为对句子分类影响最大的词可能处在句子任何位置。CNN属于无偏模型，能够通过最大池化获得最重要的特征，但是CNN的滑动窗口大小不容易确定，选的过小容易造成重要信息丢失，选的过大会造成巨大参数空间。为了解决二者的局限性，这篇文章提出了一种新的网络架构，用双向循环结构获取上下文信息，这比传统的基于窗口的神经网络更能减少噪声，而且在学习文本表达时可以大范围的保留词序。其次使用最大池化层获取文本的重要部分，自动判断哪个特征在文本分类过程中起更重要的作用。</p>
<p><a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552" target="_blank" rel="noopener">论文</a></p>
<h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/rcnn.png" alt="avatar"></p>
<h4 id="Word-Representation-Learning"><a href="#Word-Representation-Learning" class="headerlink" title="Word Representation Learning"></a>Word Representation Learning</h4><p>作者提出将单词的左上下文、右上下文、单词本身结合起来作为单词表示。作者使用了双向RNN来分别提取句子的上下文信息。公式如下:</p>
<script type="math/tex; mode=display">
\begin{array}{l}
c_{l}\left(w_{i}\right)=f\left(W^{(l)} c_{l}\left(w_{i-1}\right)+W^{(s l)} e\left(w_{i-1}\right)\right)  \\
c_{r}\left(w_{i}\right)=f\left(W^{(r)} c_{r}\left(w_{i+1}\right)+W^{(s r)} e\left(w_{i+1}\right)\right)
\end{array}</script><p>其中，$c_l(w_i)$代表单词$w_i$的左上下文，$c_l(w_i)$由上一个单词的左上下文$c_l$和$c_l(w_{i-1})$上一个单词的词嵌入向量 $e(w_{i-1})$计算得到，如公式（1）所示，所有句子第一个单词的左侧上下文使用相同的共享参数$c_l(w_1)$。 $W^{(l)},W^{(sl)}$用于将上一个单词的左上下文语义和上一个单词的语义结合到单词 $w_i$的左上下文表示中。右上下文的处理与左上下文完全相同，同样所有句子最后一个单词的右侧上下文使用相同的共享参数$c_r(w_n)$。 得到句子中每个单词的左上下文表示和右上下文表示后，就可以定义单词  $w_i$的表示如下</p>
<script type="math/tex; mode=display">
\boldsymbol{x}_{i}=\left[\boldsymbol{c}_{l}\left(w_{i}\right) ; \boldsymbol{e}\left(w_{i}\right) ; \boldsymbol{c}_{r}\left(w_{i}\right)\right]</script><p>实际就是单词$w_i$，单词的词嵌入表示向量 $e(w_i)$以及单词的右上下文向量$c_e(w_i)$ 的拼接后的结果。得到$w_i$的表示$x_i$后，就可以输入激活函数得到$w_i$的潜在语义向量 $y_i^{(2)}$ 。</p>
<script type="math/tex; mode=display">
\boldsymbol{y}_{i}^{(2)}=\tanh \left(W^{(2)} \boldsymbol{x}_{i}+\boldsymbol{b}^{(2)}\right)</script><h4 id="Text-Representation-Learning"><a href="#Text-Representation-Learning" class="headerlink" title="Text Representation Learning"></a>Text Representation Learning</h4><p>经过卷积层后，获得了所有词的表示，首先对其进行最大池化操作，最大池化可以帮助找到句子中最重要的潜在语义信息。</p>
<script type="math/tex; mode=display">
\boldsymbol{y}^{(3)}=\max _{i=1}^{n} \boldsymbol{y}_{i}^{(2)}</script><p>然后经过全连接层得到文本的表示，最后通过softmax层进行分类。</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol{y}^{(4)}=W^{(4)} \boldsymbol{y}^{(3)}+\boldsymbol{b}^{(4)}\\
&p_{i}=\frac{\exp \left(\boldsymbol{y}_{i}^{(4)}\right)}{\sum_{k=1}^{n} \exp \left(\boldsymbol{y}_{k}^{(4)}\right)}
\end{aligned}</script><h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RCNNModel</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, vocab, embedding_dim, hidden_dim, output_dim, n_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 bidirectional, dropout, pad_size=<span class="number">32</span>,batch_first=False,use_pretrain_embedding=False)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.pad_size = pad_size</span><br><span class="line">        <span class="keyword">if</span> use_pretrain_embedding:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(vocab.vectors)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.embedding = nn.Embedding(len(vocab), embedding_dim, padding_idx=vocab.pad_index)</span><br><span class="line">        <span class="comment"># 把unknown 和 pad 向量设置为零</span></span><br><span class="line">        self.embedding.weight.data[vocab.unk_index] = torch.zeros(embedding_dim)</span><br><span class="line">        self.embedding.weight.data[vocab.pad_index] = torch.zeros(embedding_dim)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(embedding_dim,</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(embedding_dim,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(embedding_dim,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.maxpool = nn.MaxPool1d()</span></span><br><span class="line">        self.fc = nn.Linear(hidden_dim * n_layers + embedding_dim, output_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [sent len, batch size]</span></span><br><span class="line"></span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedded = [sent len, batch size, emb dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># packed_output</span></span><br><span class="line">        <span class="comment"># hidden [n_layers * bi_direction,batch_size,hidden_dim]</span></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        <span class="comment"># output [sent len, batch_size * n_layers * bi_direction]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if not self.bidirectional:</span></span><br><span class="line">        <span class="comment">#     hidden = torch.reshape(hidden,(hidden.shape[1],self.hidden_dim * self.n_layers))</span></span><br><span class="line">        <span class="comment"># else:</span></span><br><span class="line">        <span class="comment">#     hidden = torch.reshape(hidden, (-1,hidden.shape[1], self.hidden_dim * self.n_layers))</span></span><br><span class="line">        <span class="comment">#     hidden = torch.mean(hidden,dim=0)</span></span><br><span class="line"></span><br><span class="line">        output = torch.cat((output,embedded),<span class="number">2</span>)</span><br><span class="line">        out = output.relu().permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)</span><br><span class="line">        max_sentence_len = output_lengths[<span class="number">0</span>].item()</span><br><span class="line">        out = nn.MaxPool1d(max_sentence_len)(out).squeeze()</span><br><span class="line">        out = self.fc(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h3 id="DPCNN"><a href="#DPCNN" class="headerlink" title="DPCNN"></a>DPCNN</h3><h4 id="简介："><a href="#简介：" class="headerlink" title="简介："></a>简介：</h4><p>ACL2017年中，腾讯AI-lab提出了Deep Pyramid Convolutional Neural Networks for Text Categorization(DPCNN)。论文中提出了一种基于word-level级别的网络-DPCNN，由于TextCNN 不能通过卷积获得文本的长距离依赖关系，而论文中DPCNN通过不断加深网络，可以抽取长距离的文本依赖关系。实验证明在不增加太多计算成本的情况下，增加网络深度就可以获得最佳的准确率。‍</p>
<h4 id="网络结构-1"><a href="#网络结构-1" class="headerlink" title="网络结构"></a>网络结构</h4><p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/DPCNN.jpg" alt="avatar"></p>
<h5 id="Region-embedding"><a href="#Region-embedding" class="headerlink" title="Region embedding"></a>Region embedding</h5><p>作者将TextCNN的包含多尺寸卷积滤波器的卷积层的卷积结果称之为Region embedding，意思就是对一个文本区域/片段（比如3gram）进行一组卷积操作后生成的embedding。<br>卷积操作有两种选择：</p>
<ol>
<li>保留词序：也就是设置一组size=3*D的二维卷积核对3gram进行卷积（其中D是word embedding维度）</li>
<li>不保留词序（即使用词袋模型），即首先对3gram中的3个词的embedding取均值得到一个size=D的向量，然后设置一组size=D的一维卷积核对该3gram进行卷积。</li>
</ol>
<p>TextCNN里使用的是保留词序的做法，而DPCNN使用的是词袋模型的做法，DPCNN作者argue前者做法更容易造成过拟合，后者的性能却跟前者差不多。</p>
<h4 id="卷积和全连接的权衡"><a href="#卷积和全连接的权衡" class="headerlink" title="卷积和全连接的权衡"></a>卷积和全连接的权衡</h4><p>产生region embedding后，按照经典的TextCNN的做法的话，就是从每个特征图中挑选出最有代表性的特征，也就是直接应用全局最大池化层（max-over-time-pooling layer），这样就生成了这段文本的特征向量（假如卷积滤波器的size有3，4，5这三种，每种size包含100个卷积核，那么当然就会产生3<em>100幅特征图，然后将max-over-time-pooling操作应用到每个特征图上，于是文本的特征向量即3</em>100=300维）。<br>TextCNN这样做的意义本质上与词袋模型（含ngram）+weighting+NB/MaxEnt/SVM的经典文本分类模型没本质区别，只不过one-hot表示到word embedding表示的转变避免了词袋模型遭遇的数据稀疏问题。可以说，TextCNN本质上收益于词向量的引入带来的“近义词有相近向量表示”的bonus，同时TextCNN恰好可以较好的利用词向量中的知识（近义关系）。这意味着，经典模型里难以学习的远距离信息（如12gram）在TextCNN中依然难以学习。</p>
<h5 id="等长卷积"><a href="#等长卷积" class="headerlink" title="等长卷积"></a>等长卷积</h5><p>假设输入的序列长度为n，卷积核大小为m，步长(stride)为s,输入序列两端各填补p个零(zero padding),那么该卷积层的输出序列为(n-m+2p)/s+1。</p>
<ol>
<li>窄卷积(narrow convolution):步长s=1,两端不补零，即p=0，卷积后输出长度为n-m+1。</li>
<li>宽卷积(wide onvolution) ：步长s=1,两端补零p=m-1，卷积后输出长度 n+m-1。</li>
<li>等长卷积(equal-width convolution): 步长s=1,两端补零p=(m-1)/2，卷积后输出长度为n。</li>
</ol>
<p>那么对文本，或者说对word embedding序列进行等长卷积的意义是什么呢？<br>既然输入输出序列的位置数一样多，我们将输入输出序列的第n个embedding称为第n个词位，那么这时size为n的卷积核产生的等长卷积的意义就很明显了，那就是将输入序列的每个词位及其左右((n-1)/2)个词的上下文信息压缩为该词位的embedding，也就是说，产生了每个词位的被上下文信息修饰过的更高level更加准确的语义。</p>
<p>回到DPCNN上来，我们想要克服TextCNN的缺点，捕获长距离模式，显然就要用到深层CNN啦。那么直接等长卷积堆等长卷积可不可以呢？<br>显然这样会让每个词位包含进去越来越多，越来越长的上下文信息，但是这样效率也太低了，显然会让网络层数变得非常非常非常深，但是这种方式太笨重。不过，既然等长卷积堆等长卷积会让每个词位的embedding描述语义描述的更加丰富准确，那么当然我们可以适当的堆两层来提高词位embedding的表示的丰富性。<br><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/equal_cnn.png" alt="avatar"></p>
<h5 id="固定feature-map的数量"><a href="#固定feature-map的数量" class="headerlink" title="固定feature map的数量"></a>固定feature map的数量</h5><p>在表示好每个词位的语义后，其实很多邻接词或者邻接ngram的词义是可以合并的，例如“小明 人 不要 太好”中的“不要”和“太好”虽然语义本来离得很远，但是作为邻接词“不要太好”出现时其语义基本等价为“很好”，这样完全可以把“不要”和“太好”的语义进行合并。同时，合并的过程完全可以在原始的embedding space中进行的，毕竟原文中直接把“不要太好”合并为“很好”是很可以的，完全没有必要动整个语义空间。<br>实际上，相比图像中这种从“点、线、弧”这种low-level特征到“眼睛、鼻子、嘴”这种high-level特征的明显层次性的特征区分，文本中的特征进阶明显要扁平的多，即从单词（1gram）到短语再到3gram、4gram的升级，其实很大程度上均满足“语义取代”的特性。而图像中就很难发生这种”语义取代“现象<br>因此，DPCNN与ResNet很大一个不同就是，<strong>在DPCNN中固定死了feature map的数量</strong>，也就是固定住了embedding space的维度（为了方便理解，以下简称语义空间），使得网络有可能让整个邻接词（邻接ngram）的合并操作在原始空间或者与原始空间相似的空间中进行（当然，网络在实际中会不会这样做是不一定的，只是提供了这么一种条件）。也就是说，整个网络虽然形状上来看是深层的，但是从语义空间上来看完全可以是扁平的。而ResNet则是不断的改变语义空间，使得图像的语义随着网络层的加深也不断的跳向更高level的语义空间。</p>
<h5 id="池化"><a href="#池化" class="headerlink" title="池化"></a><strong>池化</strong></h5><p>所以提供了这么好的合并条件后，我们就可以用pooling layer进行合并啦。每经过一个size=3, stride=2（大小为3，步长为2）的池化层（以下简称1/2池化层），序列的长度就被压缩成了原来的一半。这样同样是size=3的卷积核，每经过一个1/2池化层后，其能感知到的文本片段就比之前长了一倍。例如之前是只能感知3个词位长度的信息，经过1/2池化层后就能感知6个词位长度的信息，这时把1/2池化层和size=3的卷积层组合起来如图：<br><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/dpcnn_pooling.png" alt="avatar"></p>
<h5 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h5><p>于我们在初始化深度CNN时，往往各层权重都是初始化为一个很小的值，这就导致最开始的网络中，后续几乎每层的输入都是接近0，这时网络的输出自然是没意义的，而这些小权重同时也阻碍了梯度的传播，使得网络的初始训练阶段往往要迭代好久才能启动。<br>同时，就算网络启动完成，由于深度网络中仿射矩阵（每两层间的连接边）近似连乘，训练过程中网络也非常容易发生梯度爆炸或弥散问题（虽然由于非共享权重，深度CNN网络比RNN网络要好点）。<br>那么如何解决深度CNN网络的梯度弥散问题呢？<br>ResNet中提出的shortcut-connection/skip-connection/residual-connection（残差连接）就是一种非常简单、合理、有效的解决方案。<br><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/dpcnn_resnet.png" alt="avatar"><br>既然每个block的输入在初始阶段容易是0而无法激活，那么直接用一条线把region embedding层连接到每个block的输入乃至最终的池化层/输出层不就可以。有了shortcut后，梯度就可以忽略卷积层权重的削弱，从shortcut一路无损的传递到各个block手里，直至网络前端，从而极大的缓解了梯度消失问题。</p>
<h4 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DPCNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab, embedding_dim, num_filters, use_pretrain_embedding,num_classes)</span>:</span></span><br><span class="line">        super(DPCNN, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> use_pretrain_embedding:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(vocab.vectors)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.embedding = nn.Embedding(len(vocab), embedding_dim, padding_idx=vocab.pad_index)</span><br><span class="line">        self.conv_region = nn.Conv2d(<span class="number">1</span>, num_filters, (<span class="number">3</span>, embedding_dim), stride=<span class="number">1</span>)</span><br><span class="line">        self.conv = nn.Conv2d(num_filters, num_filters, (<span class="number">3</span>, <span class="number">1</span>), stride=<span class="number">1</span>)</span><br><span class="line">        self.max_pool = nn.MaxPool2d(kernel_size=(<span class="number">3</span>, <span class="number">1</span>), stride=<span class="number">2</span>)</span><br><span class="line">        self.padding1 = nn.ZeroPad2d((<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># top bottom</span></span><br><span class="line">        self.padding2 = nn.ZeroPad2d((<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>))  <span class="comment"># bottom</span></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.fc = nn.Linear(num_filters, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text [batch_size,seq_len]</span></span><br><span class="line">        x = self.embedding(text) <span class="comment"># x=[batch_size,seq_len,embedding_dim]</span></span><br><span class="line">        x = x.unsqueeze(<span class="number">1</span>)  <span class="comment"># [batch_size, 1, seq_len, embedding_dim]</span></span><br><span class="line">        x = self.conv_region(x)  <span class="comment"># x = [batch_size, num_filters, seq_len-3+1, 1]</span></span><br><span class="line"></span><br><span class="line">        x = self.padding1(x)  <span class="comment"># [batch_size, num_filters, seq_len, 1]</span></span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv(x)  <span class="comment"># [batch_size, num_filters, seq_len-3+1, 1]</span></span><br><span class="line">        x = self.padding1(x)  <span class="comment"># [batch_size, num_filters, seq_len, 1]</span></span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv(x)  <span class="comment"># [batch_size, num_filters, seq_len-3+1, 1]</span></span><br><span class="line">        <span class="keyword">while</span> x.size()[<span class="number">2</span>] &gt;= <span class="number">2</span>:</span><br><span class="line">            x = self._block(x) <span class="comment"># [batch_size, num_filters,1,1]</span></span><br><span class="line">        x = x.squeeze()  <span class="comment"># [batch_size, num_filters]</span></span><br><span class="line">        x = self.fc(x) <span class="comment"># [batch_size, 1]</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_block</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.padding2(x)</span><br><span class="line">        px = self.max_pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.padding1(px)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line"></span><br><span class="line">        x = self.padding1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Short Cut</span></span><br><span class="line">        x = x + px</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h3><h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h4><p><img src="/2020/03/29/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%AF%94%E8%BE%83/bert_gpt_elmo.png" alt="avatar"></p>
<h5 id="Task-1-MLM"><a href="#Task-1-MLM" class="headerlink" title="Task 1: MLM"></a>Task 1: MLM</h5><p>由于BERT需要通过上下文信息，来预测中心词的信息，同时又不希望模型提前看见中心词的信息，因此提出了一种 Masked Language Model 的预训练方式，即随机从输入预料上 mask 掉一些单词，然后通过的上下文预测该单词，类似于一个完形填空任务。</p>
<p>在预训练任务中，15%的 Word Piece 会被mask，这15%的 Word Piece 中，80%的时候会直接替换为 [Mask] ，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token</p>
<ul>
<li>没有100%mask的原因<ul>
<li>如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词</li>
</ul>
</li>
<li>加入10%随机token的原因<ul>
<li>Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘</li>
<li>另外编码器不知道哪些词需要预测的，哪些词是错误的，因此被迫需要学习每一个token的表示向量</li>
</ul>
</li>
<li>另外，每个batchsize只有15%的单词被mask的原因，是因为性能开销的问题，双向编码器比单项编码器训练要更慢</li>
</ul>
<h5 id="Task-2-NSP"><a href="#Task-2-NSP" class="headerlink" title="Task 2: NSP"></a>Task 2: NSP</h5><p>仅仅一个MLM任务是不足以让 BERT 解决阅读理解等句子关系判断任务的，因此添加了额外的一个预训练任务，即 Next Sequence Prediction。</p>
<p>具体任务即为一个句子关系判断任务，即判断句子B是否是句子A的下文，如果是的话输出’IsNext‘，否则输出’NotNext‘。</p>
<p>训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在图4中的[CLS]符号中</p>
<h5 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h5><ul>
<li>Token Embeddings：即传统的词向量层，每个输入样本的首字符需要设置为[CLS]，可以用于之后的分类任务，若有两个不同的句子，需要用[SEP]分隔，且最后一个字符需要用[SEP]表示终止</li>
<li>Segment Embeddings：为[0,1][0,1]序列，用来在NSP任务中区别两个句子，便于做句子关系判断任务</li>
<li>Position Embeddings：与Transformer中的位置向量不同，BERT中的位置向量是直接训练出来的</li>
</ul>
<h5 id="Fine-tunninng"><a href="#Fine-tunninng" class="headerlink" title="Fine-tunninng"></a>Fine-tunninng</h5><p>对于不同的下游任务，我们仅需要对BERT不同位置的输出进行处理即可，或者直接将BERT不同位置的输出直接输入到下游模型当中。具体的如下所示：</p>
<ul>
<li>对于情感分析等单句分类任务，可以直接输入单个句子（不需要[SEP]分隔双句），将[CLS]的输出直接输入到分类器进行分类</li>
<li>对于句子对任务（句子关系判断任务），需要用[SEP]分隔两个句子输入到模型中，然后同样仅须将[CLS]的输出送到分类器进行分类</li>
<li>对于问答任务，将问题与答案拼接输入到BERT模型中，然后将答案位置的输出向量进行二分类并在句子方向上进行softmax（只需预测开始和结束位置即可）</li>
<li>对于命名实体识别任务，对每个位置的输出进行分类即可，如果将每个位置的输出作为特征输入到CRF将取得更好的效果。</li>
</ul>
<h5 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>BERT的预训练任务MLM使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务</li>
<li>另外，BERT没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计</li>
<li>由于最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）</li>
<li>适合处理自然语义理解类任务(NLU)，而不适合自然语言生成类任务(NLG)</li>
</ul>
<h5 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bert</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bert_model_path,num_classes)</span>:</span></span><br><span class="line">        super(Bert, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_model_path)</span><br><span class="line">        <span class="comment"># 不对bert进行训练</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.bert.parameters():</span><br><span class="line">            param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text [ batch_size,senten_len]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># context = x[0]  # 输入的句子</span></span><br><span class="line">        <span class="comment"># mask = x[2]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># cls [batch_size, 768]</span></span><br><span class="line">        <span class="comment"># sentence [batch size,sen len,  768]</span></span><br><span class="line">        sentence, cls = self.bert(text)</span><br><span class="line"></span><br><span class="line">        out = self.fc(cls)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h4 id="bert-config-json"><a href="#bert-config-json" class="headerlink" title="bert_config.json"></a>bert_config.json</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "attention_probs_dropout_prob": 0.1, #乘法attention时，softmax后dropout概率 </span><br><span class="line">  "directionality": "bidi", </span><br><span class="line">  "hidden_act": "gelu",   #激活函数 </span><br><span class="line">  "hidden_dropout_prob": 0.1, #隐藏层dropout概率 </span><br><span class="line">  "hidden_size": 768, #隐藏单元数 </span><br><span class="line">  "initializer_range": 0.02, #初始化范围 </span><br><span class="line">  "intermediate_size": 3072, #升维维度</span><br><span class="line">  "max_position_embeddings": 512, #一个大于seq_length的参数，用于生成position_embedding</span><br><span class="line">  "num_attention_heads": 12,#每个隐藏层中的attention head数 </span><br><span class="line">  "num_hidden_layers": 2, #隐藏层数 </span><br><span class="line">  "pooler_fc_size": 768, </span><br><span class="line">  "pooler_num_attention_heads": 12, </span><br><span class="line">  "pooler_num_fc_layers": 3, </span><br><span class="line">  "pooler_size_per_head": 128, </span><br><span class="line">  "pooler_type": "first_token_transform", </span><br><span class="line">  "type_vocab_size": 2, #segment_ids类别 [0,1] </span><br><span class="line">  "vocab_size": 21128#词典中词数</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="微博情感分类结果比较"><a href="#微博情感分类结果比较" class="headerlink" title="微博情感分类结果比较"></a>微博情感分类结果比较</h3><div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>速度</th>
<th style="text-align:center">效果(best)：</th>
<th>效果：</th>
<th>参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>FastText</td>
<td>4.43s/epoch</td>
<td style="text-align:center">字向量,训练集：97.30%,验证集：96.39%，测试集：96.00%</td>
<td>词向量,速度：13.97s/epoch，训练集：95.92% 验证集：93.31%，测试集：92.19%</td>
<td>“embedding_dim”: 300, “output_dim”: 1</td>
</tr>
<tr>
<td>Textcnn</td>
<td>18.99s/epoch</td>
<td style="text-align:center">字向量，训练集：97.28%,验证集：97.68%，测试集：97.12%</td>
<td>词向量,速度：36.79s/epoch,训练集：95.63，验证集：94.08%，测试集：94.58%</td>
<td>embedding_dim: 300, n_filters: 100, filter_sizes: [3,4,5], dropout: 0.5, output_dim: 1</td>
</tr>
<tr>
<td>Rnn</td>
<td>23.33s/epoch</td>
<td style="text-align:center">字向量：训练集：97.81%,验证集：97.67%，测试集：97.66%</td>
<td>词向量：速度：22.44s/epoch,训练集：96.03%，验证集：95.46%，测试集：95.12%</td>
<td>“embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 1, “n_layers”:1, “bidirectional”: false, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>Rnn</td>
<td>46.78s/epoch</td>
<td style="text-align:center">字向量：训练集：97.76%，验证集：97.71%，测试集：97.65%</td>
<td>词向量：速度：32.34s/epoch,训练集：92.49%，验证集：95.43%，测试集：92.20%</td>
<td>“embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 1, “<strong>n_layers</strong>“:2, “<strong>bidirectional</strong>“: true, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>Lstm</td>
<td>27.19s/epoch</td>
<td style="text-align:center">字向量：训练集：98.00%，验证集：97.76%，测试集：97.68%</td>
<td>词向量：速度：24.14s/epoch,训练集：93.99%，验证集：96.16%，测试集：96.20%</td>
<td>“embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 1, “n_layers”:1, “bidirectional”: false, “dropout”: 0, “batch_first”: false</td>
</tr>
<tr>
<td>Lstm</td>
<td>65.50s/epoch</td>
<td style="text-align:center">字向量：训练集：98.03%，验证集：97.95%，测试集：97.88%</td>
<td>词向量：速度：40.78s/epoch,训练集：96.93%，验证集：97.03%，测试集：96.80%</td>
<td>“embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 1, “<strong>n_layers</strong>“:2, “<strong>bidirectional</strong>“: true, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>Rnn-attention</td>
<td>24.27s/epoch</td>
<td style="text-align:center">字向量：训练集：97.68%，验证集：97.87%，测试集：97.66%</td>
<td>词向量：速度：22.53s/epoch,训练集：96.18%，验证集：95.61%，测试集：95.53%</td>
<td>“embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 1, “n_layers”: 1, “bidirectional”: false, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>Rnn-attention</td>
<td>48.73s/epoch</td>
<td style="text-align:center">字向量：训练集：98.24%，验证集：97.89%，测试集：97.76%</td>
<td>词向量：速度：33.19s/epoch,训练集：96.53%，验证集：95.78%，测试集：95.50%</td>
<td>“embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 1, “<strong>n_layers</strong>“:2, “<strong>bidirectional</strong>“: true, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>Rcnn</td>
<td>27.51s/epoch</td>
<td style="text-align:center">字向量：训练集：98.36%，验证集：98.36%，测试集：98.30%</td>
<td>词向量：速度：23.61s/epoch,训练集：97.66%，验证集：97.29%，测试集：97.20%</td>
<td>“embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 1, “n_layers”:1, “bidirectional”: false, “dropout”: 0, “batch_first”: false</td>
</tr>
<tr>
<td>Rcnn</td>
<td>54.01s/epoch</td>
<td style="text-align:center">字向量：训练集：98.34%，验证集：98.31%，测试集：98.30%</td>
<td>词向量：速度：34.92s/epoch,训练集：95.77%，验证集：97.57%，测试集：97.49%</td>
<td>“embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 1, “<strong>n_layers</strong>“:2, “<strong>bidirectional</strong>“: true, “dropout”: 0, “batch_first”: false</td>
</tr>
<tr>
<td>Bert</td>
<td>143.17s/epoch</td>
<td style="text-align:center">训练集：97.47%，验证集：97.78%，测试集：97.66%</td>
<td></td>
<td>“num_classes”: 1</td>
</tr>
<tr>
<td>Bert-cnn</td>
<td>254.66s/epoch</td>
<td style="text-align:center">训练集：98.01，验证集：98.32%，测试集：98.21%</td>
<td></td>
<td>“num_filters”: 100, “hidden_size”: 768, “filter_sizes”: [3,4,5], “dropout”: 0.1, “num_classes”: 1</td>
</tr>
<tr>
<td>Bert-rnn</td>
<td>195.59s/epoch</td>
<td style="text-align:center">训练集：97.53%，验证集：97.83%,测试集：97.55%</td>
<td></td>
<td>“rnn_type”: “rnn”, “bert_embedding_dim”: 768, “hidden_dim”: 256, “n_layers”: 2, “bidirectional”: true, “batch_first”: true, “dropout”: 0.1, “num_classes”: 1</td>
</tr>
<tr>
<td>Bert-rcnn</td>
<td>200.17s/epoch</td>
<td style="text-align:center">训练集：98.17%，验证集：98.15%,测试集：98.13%</td>
<td></td>
<td>“rnn_type”: “rnn”, “bert_embedding_dim”: 768, “hidden_dim”: 256, “num_classes”: 1, “n_layers”:2, “bidirectional”: true, “dropout”: 0, “batch_first”: false</td>
</tr>
<tr>
<td>DPCNN</td>
<td>9.76s/epoch</td>
<td style="text-align:center">字向量：训练集：97.93%，验证集：97.72%,测试集：97.62%</td>
<td>词向量：速度：7.62s/epoch,训练集：93.16%，验证集：91.70%，测试集：92.00%</td>
<td>“embedding_dim”: 300, “num_filters”: 100, “num_classes”: 1</td>
</tr>
</tbody>
</table>
</div>
<h3 id="新闻十分类结果比较"><a href="#新闻十分类结果比较" class="headerlink" title="新闻十分类结果比较"></a>新闻十分类结果比较</h3><div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>速度</th>
<th>效果：</th>
<th>效果：</th>
<th>参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>Fast_Text</td>
<td>17.68s/epoch</td>
<td>字向量,训练集：96.57%,验证集：92.87%</td>
<td>18.26s/epoch,词向量，训练集：96.7%，验证集：92.61%</td>
<td>“embedding_dim”: 300, “output_dim”: 10</td>
</tr>
<tr>
<td>TextCNN</td>
<td>143.61s/epoch</td>
<td>字向量,训练集：98.13%,验证集：96.42%</td>
<td>142.95s/epoch,词向量，训练集：98.32%，验证集：95.7%</td>
<td>“embedding_dim”: 300, “n_filters”: 50, “filter_sizes”: [3,4,5], “dropout”: 0.5, “output_dim”: 10</td>
</tr>
<tr>
<td>TextCNN1d</td>
<td>58.45s/epoch</td>
<td>字向量,训练集：99.09%,验证集：96.42%</td>
<td>57.66s/epoch,词向量，训练集：98.97%，验证集：96.69%</td>
<td>“embedding_dim”: 300, “n_filters”: 100, “filter_sizes”: [3,4,5], “dropout”: 0.5, “output_dim”: 10</td>
</tr>
<tr>
<td>DPCNN</td>
<td>58.19s/epoch</td>
<td>字向量,训练集：97.12%,验证集：93.98%</td>
<td>57.87s/epoch,词向量，训练集：96.97%，验证集：95.04%</td>
<td>“use_pretrain_embedding”: true, “embedding_dim”: 300, “num_filters”: 100, “num_classes”: 10</td>
</tr>
<tr>
<td>rcnn</td>
<td>461s/epoch</td>
<td>字向量,训练集：97.85%,验证集：95.41%</td>
<td>461.04s/epoch,词向量，训练集：98.5%，验证集：95.9%</td>
<td>“rnn_type”: “lstm”, “embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 10, “n_layers”:2, “bidirectional”: true, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>rnn（1 layer）</td>
<td>131.58s/epoch</td>
<td>字向量,训练集：93.43%,验证集：90.06%</td>
<td>131.04s/epoch,词向量，训练集：98.06%，验证集：93.19</td>
<td>“rnn_type”: “rnn”, “embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 10, “n_layers”:1, “bidirectional”: false, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>rnn（2，layer）</td>
<td>304.57s/epoch</td>
<td>字向量,训练集：96.5%,验证集：92.6%</td>
<td>304s/epoch,词向量，训练集：94.60%，验证集：93.39%</td>
<td>“rnn_type”: “rnn”, “embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 10, “n_layers”:2, “bidirectional”: true, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>lstm</td>
<td>437.34s/epoch</td>
<td>字向量,训练集：98.45%,验证集：96.01%</td>
<td>439.16s/epoch,词向量，训练集：97.73%，验证集：94.30%</td>
<td>“rnn_type”: “lstm”, “embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 10, “n_layers”:2, “bidirectional”: true, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>Lstm-attention</td>
<td>381.35s/epoch</td>
<td>字向量,训练集：97.09%,验证集：95.00%</td>
<td>380.58s/epoch,词向量，训练集：99.15%，验证集：96.32</td>
<td>“rnn_type”: “lstm”, “embedding_dim”: 300, “hidden_dim”: 256, “output_dim”: 10, “n_layers”: 2, “bidirectional”: true, “dropout”: 0.5, “batch_first”: false</td>
</tr>
<tr>
<td>Bert</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Bert-cnn</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Bert-rnn</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Bert-rcnn</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>HAN</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>使用bert进行训练，机子跑步起来了，而且即使跑起来也是阉割版的，似乎没什么意义。</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><h4 id="长短文本分类的比较"><a href="#长短文本分类的比较" class="headerlink" title="长短文本分类的比较"></a>长短文本分类的比较</h4><p>对于词嵌入技术的文本表示，短文本和长文本表示上没有差别，此时分类效果的优劣主要在分类模型和训练数据上。</p>
<p>对于数据而言：随着文本越长，语义的重要性就越高，在文本很短的情况下，语义的重要性就很小，比如：“今天 天气 怎么样”，“今天 怎么样 天气”，“怎么样 天气 今天”。你甚至不必要考虑句子是否通顺，基本上可以当一句话处理，没有第二个意思。但是随着文本越来越长，比如512个字符，颠倒一下可能就要归为两类了。</p>
<p>对于模型而言：对于短文本，CNN配合Max-pooling池化(如TextCNN模型)速度快，而且效果也很好。因为短文本上的关键词比较容易找到，而且Max-pooling会直接过滤掉模型认为不重要特征。具体工作机制是：卷积窗口沿着长度为n的文本一个个滑动，类似于n-gram机制对文本切词，然后和文本中的每个词进行相似度计算，因为后面接了个Max-pooling，因此只会保留和卷积核最相近的词。微博数据集属于情感分类，为了判断句子的情感极性，只需要让分类器能识别出“不开心”这类词是个负极性的词，“高兴”、“开心”等这类词是正极性的词，其他词是偏中性词就可以了。因此，当我们把该句子中的各个词条输入给模型去分类时，并不需要去“瞻前顾后”，因此使用一个关注局部的前馈神经网络往往表现更佳。虽然Attention也突出了重点特征，但是难以过滤掉所有低分特征。但是对于长文本直接用CNN就不行了，TextCNN会比HAN模型泛化能力差很多。如果在TextCNN前加一层LSTM，这样效果可以提升很大。</p>
<h4 id="为什么长文本分类的实验中，cnn-和-rnn-没有拉开差距？"><a href="#为什么长文本分类的实验中，cnn-和-rnn-没有拉开差距？" class="headerlink" title="为什么长文本分类的实验中，cnn 和 rnn 没有拉开差距？"></a>为什么长文本分类的实验中，cnn 和 rnn 没有拉开差距？</h4><p>cnn和rnn的精度都很高，分析主要还是分类的文章规则性比较强，且属于特定领域，词量不多，类别差异可能比较明显。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>复杂的模型未必会有很好的结果，简单模型效果未必不理想，没必要一味追求深度学习、复杂模型什么的。选什么样的模型还是要根据数据来的。同一类问题，不同的数据效果差异很大，不要小看任何一类问题，例如分类，我们通常觉得它很简单，但有些数据并非你所想。</p>
<h3 id="文本分类tricks"><a href="#文本分类tricks" class="headerlink" title="文本分类tricks"></a>文本分类tricks</h3><h4 id="分词器"><a href="#分词器" class="headerlink" title="分词器"></a>分词器</h4><p><strong>分词器所分出的词与词向量表中的token粒度match是更重要的事情</strong></p>
<h5 id="已知预训练词向量的分词器"><a href="#已知预训练词向量的分词器" class="headerlink" title="已知预训练词向量的分词器"></a>已知预训练词向量的分词器</h5><p>像word2vec、glove、fasttext这些官方release的预训练词向量都会公布相应训练语料的信息，包括预处理策略如分词，这种情况下直接使用官方的训练该词向量所使用的分词器，此分词器在下游任务的表现十之八九会比其他花里胡哨的分词器好用。</p>
<h5 id="不知道预训练词向量的分词器"><a href="#不知道预训练词向量的分词器" class="headerlink" title="不知道预训练词向量的分词器"></a>不知道预训练词向量的分词器</h5><p>这时就需要去“猜”一下分词器。怎么猜呢？<br>首先，拿到预训练词向量表后，去里面search一些特定词汇比如一些网站、邮箱、成语、人名等，英文里还有n’t等，看看训练词向量使用的分词器是把它们分成什么粒度。<br>然后跑几个分词器，看看哪个分词器的粒度跟他最接近就用哪个，如果不放心，就放到下游任务里跑跑看。</p>
<p>最理想的情况是：先确定最适合当前任务数据集的分词器，再使用同分词器产出的预训练词向量。如果无法满足理想情况，则需要自己在下游任务训练集或者大量同分布无监督语料上训练的词向量更有利于进一步压榨模型的性能。</p>
<h4 id="关于中文字向量"><a href="#关于中文字向量" class="headerlink" title="关于中文字向量"></a>关于中文字向量</h4><p>预训练中文字向量的时候，把窗口开大一些，不要直接使用word-level的窗口大小，效果会比随机初始化的字向量明显的好。</p>
<h4 id="数据集噪声很严重"><a href="#数据集噪声很严重" class="headerlink" title="数据集噪声很严重"></a>数据集噪声很严重</h4><p>里噪声严重有两种情况。对于数据集D(X, Y)，一种是X内部噪声很大（比如文本为口语化表述或由互联网用户生成），一种是Y的噪声很大（一些样本被明显的错误标注，一些样本人也很难定义是属于哪一类，甚至具备类别二义性）。</p>
<h5 id="X内部噪声很大"><a href="#X内部噪声很大" class="headerlink" title="X内部噪声很大"></a>X内部噪声很大</h5><p>法一：直接将模型的输入变成char-level（中文中就是字的粒度），然后train from scratch（不使用预训练词向量）去跟word-level的对比一下，如果char-level的明显的效果好，那么短时间之内就直接基于char-level去做模型。</p>
<p>法二：使用特殊超参的FastText去训练一份词向量：<br>一般来说fasttext在英文中的char ngram的窗口大小一般取值3～6，但是在处理中文时，如果我们的目的是为了去除输入中的噪声，那么我们可以把这个窗口限制为1～2，这种小窗口有利于模型去捕获错别字（比如，我们打一个错误词的时候，一般都是将其中的一个字达成同音异形的另一个字），比如word2vec学出来的“似乎”的最近词可能是“好像”，然而小ngram窗口fasttext学出来的“似乎”最近词则很有可能是“是乎”等内部包含错别字的词，这样就一下子让不太过分的错别字构成的词们又重新回到了一起，甚至可以一定程度上对抗分词器产生的噪声（把一个词切分成多个字）。</p>
<h5 id="Y的噪声很大"><a href="#Y的噪声很大" class="headerlink" title="Y的噪声很大"></a>Y的噪声很大</h5><p>首先忽略这个噪声，强行的把模型尽可能好的训出来。然后让训练好的模型去跑训练集和开发集，取出训练集中的错误样本和开发集中那些以很高的置信度做出错误决策的样本（比如以99%的把握把一个标签为0的样本预测为1），然后去做这些bad cases的分析，如果发现错误标注有很强的规律性，则直接撸一个脚本批量纠正一下（只要确保纠正后的标注正确率比纠正前明显高就行）。<br>如果没有什么规律，但是发现模型高置信度做错的这些样本大部分都是标注错误的话，就直接把这些样本都删掉，常常也可以换来性能的小幅提升，毕竟测试集都是人工标注的，困难样本和错标样本不会太多。</p>
<h4 id="baseline选用CNN还是RNN？"><a href="#baseline选用CNN还是RNN？" class="headerlink" title="baseline选用CNN还是RNN？"></a>baseline选用CNN还是RNN？</h4><p>看数据集，如果感觉数据集里很多很强的ngram可以直接帮助生成正确决策，那就CNN。<br>如果感觉很多case都是那种需要把一个句子看完甚至看两三遍才容易得出正确tag，那就RNN。<br>还可以CNN、RNN的模型都跑出来简单集成一下。</p>
<h4 id="Dropout加在哪里"><a href="#Dropout加在哪里" class="headerlink" title="Dropout加在哪里"></a>Dropout加在哪里</h4><p>word embedding层后、pooling层后、FC层（全联接层）后。</p>
<h4 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h4><p>二分类问题不一定要用sigmoid作为输出层的激活函数，尝试一下包含俩类别的softmax。可能多一条分支就多一点信息，实践中常常带来零点几个点的提升，也是比较玄学了。</p>
<h4 id="多标签分类"><a href="#多标签分类" class="headerlink" title="多标签分类"></a>多标签分类</h4><p>如果一个样本同时拥有多个标签，甚至标签同时还构成了DAG（有向无环图），先用binary-cross-entropy训出个baseline来（即把每个类别变成一个二分类问题，这样N个类别的多标签分类问题就变成了N个二分类问题），这个baseline做好后好像多标签问题不大了，DAG问题自己也基本解决了（虽然模型层并没有专门针对这个问题作处理），然后就可以安心做模型辣。</p>
<h4 id="样本类别不均衡问题"><a href="#样本类别不均衡问题" class="headerlink" title="样本类别不均衡问题"></a>样本类别不均衡问题</h4><p>如果正负样本比小于9:1的话，继续做深度模型调超参，模型做好后会发现这点不均衡对模型来说不值一提，决策阈值也完全不用手调。<br>但是，如果经常一个batch中完全就是同一个类别的样本，或者一些类别的样本经过好多batch都难遇到一个的话，均衡就非常非常有必要了。<a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247484993&amp;idx=1&amp;sn=0bd32089a638e5a1b48239656febb6e0&amp;chksm=970c2e97a07ba7818d63dddbb469486dccb369ecc11f38ffdea596452b9e5bf65772820a8ac9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何优雅而时髦的解决不均衡分类问题</a></p>
<h4 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h4><ol>
<li>别太纠结文本截断长度使用120还是150</li>
<li>别太纠结对性能不敏感的超参数带来的开发集性能的微小提升</li>
<li>别太纠结未登陆词的embedding是初始化成全0还是随机初始化，别跟PAD共享embedding就行</li>
<li>别太纠结优化器用Adam还是MomentumSGD，如果跟SGD的感情还不深，就无脑Adam，最后再用MomentumSGD跑几遍</li>
<li>还是不会用tricks但是就是想跑出个好结果，bert大力出奇迹。</li>
</ol>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a href="https://blog.csdn.net/feilong_csdn/article/details/88655927" target="_blank" rel="noopener">fastText原理和文本分类实战，看这一篇就够了</a></li>
<li><a href="https://blog.csdn.net/asialee_bird/article/details/88813385#一、论文笔记" target="_blank" rel="noopener">TextCNN文本分类（keras实现）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/55263066" target="_blank" rel="noopener">浅谈基于深度学习的文本分类问题</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2018-06-22-4" target="_blank" rel="noopener">从DPCNN出发，撩一下深层word-level文本分类模型</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2019-01-24-5" target="_blank" rel="noopener">文本分类有哪些论文中很少提及却对性能有重要影响的tricks？</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247484682&amp;idx=1&amp;sn=51520138eed826ec891ac8154ee550f9&amp;chksm=970c2ddca07ba4ca33ee14542cff0457601bb16236f8edc1ff0e617051d1f063354dda0f8893&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">从前馈到反馈：解析循环神经网络（RNN）及其tricks</a></li>
<li><a href="http://www.52nlp.cn/tag/长文本分类" target="_blank" rel="noopener"><a href="http://www.52nlp.cn/如何用深度学习做好长文本分类与法律文书智能化" target="_blank" rel="noopener">达观数据曾彦能：如何用深度学习做好长文本分类与法律文书智能化处理</a></a></li>
<li><a href="https://www.zhihu.com/question/326770917/answer/698646465" target="_blank" rel="noopener">短文本分类和长文本分类的模型如何进行选择？</a></li>
<li><a href="https://www.pianshen.com/article/4319299677/" target="_blank" rel="noopener">NLP实践九：HAN原理与文本分类实践</a></li>
</ol>

    </div>

    
    
    <div>
  
    <div style="text-align:center;color:#bfbfbf;font-size:16px;">
      <span>-------- 本文结束 </span>
      <i class="fa fa-coffee"></i>
      <span> 感谢阅读 --------</span>
    </div>
  
</div>
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    鼓励一下
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat.png" alt="Li Zhen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/ali.png" alt="Li Zhen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" rel="tag"><i class="fa fa-tag"></i> 自然语言处理</a>
              <a href="/tags/FastText/" rel="tag"><i class="fa fa-tag"></i> FastText</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/03/28/BatchNormalization/" rel="prev" title="BatchNormalization">
      <i class="fa fa-chevron-left"></i> BatchNormalization
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/" rel="next" title="序列标注">
      序列标注 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#应用"><span class="nav-text">应用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集"><span class="nav-text">数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#情感二分类"><span class="nav-text">情感二分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#新闻十分类"><span class="nav-text">新闻十分类</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据处理代码（微博）"><span class="nav-text">数据处理代码（微博）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DataSet"><span class="nav-text">DataSet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dataloader"><span class="nav-text">dataloader</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Vocab属性："><span class="nav-text">Vocab属性：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bert-Dataprocess"><span class="nav-text">Bert Dataprocess</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FastText"><span class="nav-text">FastText</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简介"><span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#fastText模型架构"><span class="nav-text">fastText模型架构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#缺点："><span class="nav-text">缺点：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模型代码"><span class="nav-text">模型代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TextCNN"><span class="nav-text">TextCNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简介-1"><span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#网络结构"><span class="nav-text">网络结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#原理"><span class="nav-text">原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#缺点"><span class="nav-text">缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代码"><span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HAN（Hierarchy-Attention-Network）"><span class="nav-text">HAN（Hierarchy Attention Network）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#词序列编码器"><span class="nav-text">词序列编码器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#词级的注意力层"><span class="nav-text">词级的注意力层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#句子编码器"><span class="nav-text">句子编码器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#句子级注意力层"><span class="nav-text">句子级注意力层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#分类"><span class="nav-text">分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代码-1"><span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN系列"><span class="nav-text">RNN系列</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#LSTM"><span class="nav-text">LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#遗忘门"><span class="nav-text">遗忘门</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#输入门"><span class="nav-text">输入门</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#输出门"><span class="nav-text">输出门</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#公式"><span class="nav-text">公式</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GRU"><span class="nav-text">GRU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Self-Attention"><span class="nav-text">Self-Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Rnn-Attenton"><span class="nav-text">Rnn-Attenton</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TextRCNN"><span class="nav-text">TextRCNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简介-2"><span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模型结构"><span class="nav-text">模型结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Word-Representation-Learning"><span class="nav-text">Word Representation Learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Text-Representation-Learning"><span class="nav-text">Text Representation Learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代码-2"><span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DPCNN"><span class="nav-text">DPCNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简介："><span class="nav-text">简介：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#网络结构-1"><span class="nav-text">网络结构</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Region-embedding"><span class="nav-text">Region embedding</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积和全连接的权衡"><span class="nav-text">卷积和全连接的权衡</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#等长卷积"><span class="nav-text">等长卷积</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#固定feature-map的数量"><span class="nav-text">固定feature map的数量</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#池化"><span class="nav-text">池化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#残差连接"><span class="nav-text">残差连接</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代码-3"><span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bert"><span class="nav-text">Bert</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#BERT"><span class="nav-text">BERT</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Task-1-MLM"><span class="nav-text">Task 1: MLM</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Task-2-NSP"><span class="nav-text">Task 2: NSP</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#输入"><span class="nav-text">输入</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Fine-tunninng"><span class="nav-text">Fine-tunninng</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#缺点-1"><span class="nav-text">缺点</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#代码-4"><span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bert-config-json"><span class="nav-text">bert_config.json</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#微博情感分类结果比较"><span class="nav-text">微博情感分类结果比较</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#新闻十分类结果比较"><span class="nav-text">新闻十分类结果比较</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分析"><span class="nav-text">分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#长短文本分类的比较"><span class="nav-text">长短文本分类的比较</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么长文本分类的实验中，cnn-和-rnn-没有拉开差距？"><span class="nav-text">为什么长文本分类的实验中，cnn 和 rnn 没有拉开差距？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#总结"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#文本分类tricks"><span class="nav-text">文本分类tricks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#分词器"><span class="nav-text">分词器</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#已知预训练词向量的分词器"><span class="nav-text">已知预训练词向量的分词器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#不知道预训练词向量的分词器"><span class="nav-text">不知道预训练词向量的分词器</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#关于中文字向量"><span class="nav-text">关于中文字向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#数据集噪声很严重"><span class="nav-text">数据集噪声很严重</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#X内部噪声很大"><span class="nav-text">X内部噪声很大</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Y的噪声很大"><span class="nav-text">Y的噪声很大</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#baseline选用CNN还是RNN？"><span class="nav-text">baseline选用CNN还是RNN？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dropout加在哪里"><span class="nav-text">Dropout加在哪里</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#二分类"><span class="nav-text">二分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多标签分类"><span class="nav-text">多标签分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#样本类别不均衡问题"><span class="nav-text">样本类别不均衡问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#最后"><span class="nav-text">最后</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考"><span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <a href='/'>
    <img class="site-author-image" itemprop="image" alt="Li Zhen"
      src="/images/snoppy.jpeg">
  </a>
  <p class="site-author-name" itemprop="name">Li Zhen</p>
  <div class="site-description" itemprop="description">但行好事，莫问前程！</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">38</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jeffery0628" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jeffery0628" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jeffery.lee.0628@gmail.com" title="邮箱 → mailto:jeffery.lee.0628@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮箱</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Zhen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">441k</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
</div>






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"bu0jcrISneKfTwssc7P792xE-gzGzoHsz","app_key":"3y7nYJuTGp6zIHSfRBQlMQnB","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  

  <canvas id="evanyou"></canvas>
  <style>
    #evanyou {
      position: fixed;
      width: 100%;
      height: 100%;
      top: 0;
      left: 0;
      z-index: -1;
    }
  </style>
  <script src="/js/evan-you.js"></script>




  <canvas id="evanyou"></canvas>
  <style>
    #evanyou {
      position: fixed;
      width: 100%;
      height: 100%;
      top: 0;
      left: 0;
      z-index: -1;
    }
  </style>
  <script src="/js/evan-you.js"></script>



  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>


 

<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180101,"YYYYMMDD"));
    ages = ages.replace(/years?/, "年");
    ages = ages.replace(/months?/, "月");
    ages = ages.replace(/days?/, "天");
    ages = ages.replace(/hours?/, "小时");
    ages = ages.replace(/minutes?/, "分");
    ages = ages.replace(/seconds?/, "秒");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `我已在此等候你 ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


<script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>

<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdnjs.cloudflare.com/ajax/libs/valine/1.3.10/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'wocKAhd8E1IRPHNgYbfFVsKf-gzGzoHsz',
      appKey     : '0SDY7WADR3m02c4hBcUv3T0B',
      placeholder: "留下你的小脚印吧！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '8' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
