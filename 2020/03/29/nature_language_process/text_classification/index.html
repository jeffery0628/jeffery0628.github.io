<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/snoppy.jpeg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open Sans:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-big-counter.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jeffery.ink","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":270,"display":"hide","padding":15,"offset":12,"onmobile":true,"dimmer":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":true,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":true,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="文本分类">
<meta property="og:url" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/index.html">
<meta property="og:site_name" content="火种2号">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/QQ%E6%88%AA%E5%9B%BE20180116105758_%E5%89%AF%E6%9C%AC.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/595c46f937d92.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/image-20200520093625468.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/image-20200520093324964.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/image-20200520093359191.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/FastText_network_structure.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/textcnn.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/textcnndetail.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/TextCNN_network_structure.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/DPCNN.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/equal_cnn.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/dpcnn_pooling.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/dpcnn_resnet.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/rnn.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/birnn.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/lstm_c.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/lstm_forget_gate.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/input_gate.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/output_gate.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/lstm_all.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/GRU.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/TextBiRNN_network_structure.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/self-attention.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/TextAttBiRNN_network_structure.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/rcnn.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/RCNN_network_structure.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/HAN.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/HAN_network_structure.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/bert_gpt_elmo.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/image-20200520140510805.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/image-20200520140848340.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/image-20200520135717244.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/image-20200520140931559.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/image-20200520144027325.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/image-20200520144139492.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/image-20200521150254593.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/image-20200526204848210.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/image-20200526205351509.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/image-20200526205849399.png">
<meta property="og:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/image-20200526210508245.png">
<meta property="article:published_time" content="2020-03-29T04:52:20.000Z">
<meta property="article:modified_time" content="2020-08-22T05:45:25.364Z">
<meta property="article:author" content="Li Zhen">
<meta property="article:tag" content="FastText">
<meta property="article:tag" content="自然语言处理">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/QQ%E6%88%AA%E5%9B%BE20180116105758_%E5%89%AF%E6%9C%AC.png">

<link rel="canonical" href="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>文本分类 | 火种2号</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">火种2号</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">火种计划</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>简历</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>读书</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-video fa-fw"></i>电影</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>



</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jeffery.ink/2020/03/29/nature_language_process/text_classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/snoppy.jpeg">
      <meta itemprop="name" content="Li Zhen">
      <meta itemprop="description" content="他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="火种2号">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          文本分类
        </h1>

        <div class="post-meta">
          
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-08-22 13:45:25" itemprop="dateModified" datetime="2020-08-22T13:45:25+08:00">2020-08-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">技术</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">自然语言处理</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">深度学习自然语言处理</span></a>
                </span>
            </span>

          
            <span id="/2020/03/29/nature_language_process/text_classification/" class="post-meta-item leancloud_visitors" data-flag-title="文本分类" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论：</span>
    
    <a title="valine" href="/2020/03/29/nature_language_process/text_classification/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/03/29/nature_language_process/text_classification/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>43k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>39 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/2020/03/29/nature_language_process/text_classification/QQ%E6%88%AA%E5%9B%BE20180116105758_%E5%89%AF%E6%9C%AC.png" alt></p>
<a id="more"></a>
<h1 id="xiang-mu-jie-shao">项目介绍</h1>
<ol>
<li>
<p>项目使用深度学习模型进行文本分类，所使用的模型主要包括：FastText，TextCNN，DPCNN，RNN系列(RNN，LSTM，GRU)，RNN-Attention，TextRCNN，HAN，Bert，BertCNN，BertRNN，BertRCNN,XLNet。</p>
</li>
<li>
<p>方法部分对每个模型及其结构给出简要介绍，并附上pytorch代码实现。</p>
</li>
<li>
<p>实验部分所采用的的数据集：weibo_senti_100k情感分类(二分类)，cnews新闻十分类，____文本多标签分类。</p>
</li>
</ol>
<p><strong>数据下载</strong>：微博情感分类数据在<a href="https://github.com/jeffery0628/text_classification" target="_blank" rel="noopener">github仓库</a>中给出, <a href="https://pan.baidu.com/s/1OOTK374IZ1DHnz5COUcfbw" target="_blank" rel="noopener">cnews新闻数据</a>  密码:hf6o, <a href>____文本多标签数据</a></p>
<p><strong>词向量下载</strong>：<a href="https://github.com/Embedding/Chinese-Word-Vectors" target="_blank" rel="noopener">词向量</a></p>
<p><strong>预训练模型下载</strong>：<a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">中文预训练bert模型下载</a>,<a href="https://github.com/ymcui/Chinese-XLNet" target="_blank" rel="noopener">中文预训练XLNet下载</a></p>
<p><strong>项目仓库地址</strong>：<a href="https://github.com/jeffery0628/text_classification" target="_blank" rel="noopener">中文文本分类</a></p>
<p>如出现数学公式乱码以及图片问题，请移步<a href="https://jeffery0628.github.io/" target="_blank" rel="noopener">github.io</a>来获得更好的阅读体验。</p>
<p>最后，欢迎star！</p>
<h1 id="jian-jie">简介</h1>
<p>文本分类在文本处理中是很重要的一个模块，它的应用也非常广泛，比如：新闻分类、简历分类、邮件分类、办公文档分类、区域分类等诸多方面，还能够实现文本过滤，从大量文本中快速识别和过滤出符合特殊要求的信息。。它和其他的分类没有本质的区别，核心方法为首先提取分类数据的特征，然后选择最优的匹配，从而分类。但是文本也有自己的特点，根据文本的特点，文本分类的一般流程为：1.预处理；2.文本表示及特征选择；3.构造分类器；4.分类。</p>
<p>通常来讲，文本分类任务是指在给定的分类体系中，将文本指定分到某个或某几个类别中。被分类的对象有短文本，例如句子、标题、商品评论等等，长文本，如文章等。分类体系一般人工划分，例如：1）政治、体育、军事 2）正能量、负能量 3）好评、中性、差评。此外，还有文本多标签分类，比如一篇博客的标签可以同时是：自然语言处理，文本分类等。因此，对应的分类模式可以分为：二分类、多分类以及多标签分类问题。</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/595c46f937d92.png" alt></p>
<ol>
<li>对文本分类的研究可以追溯到二十世纪五十年代，当时主要依据特定的人工规则进行文本分类。</li>
<li>到二十世纪九十年代，统计机器学习 (Statistical machine learning) 成为主流，一些统计机器学习方法，比如支持向量机和朴素贝叶斯等分类方法在文本分类中取得了非常高的分类准确率。然而，统计机器学习方法首先需要进行特征工程工作，该工作需要深入理解业务需求，并且非常耗时耗力。</li>
<li>随着大数据量和图形处理单元强计算力的支持，深度学习近年来发展迅速，与统计机器学习方法相比，深度学习方法可以自动提取特征，使得人们将注意力更多地集中在数据和模型上。</li>
</ol>
<h1 id="fang-fa">方法</h1>
<p>这里我们根据是否使用深度学习方法将文本分类主要分为一下两个大类：</p>
<ul>
<li>基于机器学习的文本分类（不涉及实现以及实验结果的比较）。</li>
<li>基于深度学习的文本分类。</li>
</ul>
<h2 id="ji-yu-ji-qi-xue-xi-de-wen-ben-fen-lei">基于机器学习的文本分类</h2>
<p>90年代后互联网在线文本数量增长和机器学习学科的兴起，逐渐形成了一套解决大规模文本分类问题的经典玩法，这个阶段的主要套路是人工特征工程+浅层分类模型。整个文本分类问题就拆分成了<strong>特征工程</strong>和<strong>分类器</strong>两部分。</p>
<h3 id="te-zheng-gong-cheng">特征工程</h3>
<p>特征工程也就是将文本表示为计算机可以识别的、能够代表该文档特征的特征矩阵的过程。在基于传统机器学习的文本分类中，通常将特征工程分为<strong>文本预处理、特征提取、文本表示</strong>等三个部分。</p>
<h4 id="wen-ben-yu-chu-li">文本预处理</h4>
<p>文本预处理过程是提取文本中的关键词来表示文本的过程。中文文本预处理主要包括文本分词和去停用词两个阶段。文本分词，是因为很多研究表明特征粒度为词粒度远好于字粒度（因为大部分分类算法不考虑词序信息，基于字粒度显然损失了过多<code>n-gram</code>信息）。具体到中文分词，不同于英文有天然的空格间隔，需要设计复杂的分词算法。传统分词算法主要有基于字符串匹配的正向/逆向/双向最大匹配；基于理解的句法和语义分析消歧；基于统计的互信息/CRF方法(<code>WordEmbedding+Bi-LSTM+CRF</code>方法逐渐成为主流)。 而停用词是文本中一些高频的代词、连词、介词等对文本分类无意义的词，通常维护一个停用词表，特征提取过程中删除停用表中出现的词，本质上属于特征选择的一部分。</p>
<h4 id="te-zheng-ti-qu">特征提取</h4>
<p>特征提取包括<strong>特征选择</strong>和<strong>特征权重计算</strong>两部分。 特征选择的基本思路是根据某个评价指标独立的对原始特征项（词项）进行评分排序，从中选择得分最高的一些特征项，过滤掉其余的特征项。常用的评价有：文档频率、互信息、信息增益、χ²统计量等。特征权重计算主要是经典的TF-IDF方法及其扩展方法。</p>
<h4 id="wen-ben-biao-shi">文本表示</h4>
<p>文本表示的目的是把文本预处理后的转换成计算机可理解的方式，是决定文本分类质量最重要的部分。</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/image-20200520093625468.png" alt></p>
<h5 id="ci-dai-fa">词袋法</h5>
<p>忽略其词序和语法，句法，将文本仅仅看做是一个词集合。若词集合共有NN个词，每个文本表示为一个<code>N</code>维向量，元素为<code>0/1</code>，表示该文本是否包含对应的词。<code>( 0, 0, 0, 0, .... , 1, ... 0, 0, 0, 0)</code>。一般来说词库量至少都是百万级别，因此词袋模型有个两个最大的问题：高纬度、高稀疏性。</p>
<h5 id="n-gram-ci-dai-mo-xing">n-gram 词袋模型</h5>
<p>与词袋模型类似，考虑了局部的顺序信息，但是向量的维度过大，基本不采用。如果词集合大小为<code>N</code>，则bi-gram的单词总数为\(n^2\)向量空间模型。</p>
<h5 id="xiang-liang-kong-jian-mo-xing">向量空间模型</h5>
<p>以词袋模型为基础，向量空间模型通过特征选择降低维度，通过特征权重计算增加稠密性。</p>
<h3 id="fen-lei-qi">分类器</h3>
<p>大部分机器学习方法都在文本分类领域有所应用，比如朴素贝叶斯分类算法、KNN、SVM、最大熵、GBDT/XGBoost等等。</p>
<h2 id="ji-yu-shen-du-xue-xi-de-wen-ben-fen-lei">基于深度学习的文本分类</h2>
<h3 id="fast-text">FastText</h3>
<h4 id="jian-jie-1">简介</h4>
<p>fastText是Facebook于2016年开源的一个词向量计算和文本分类工具,<a href="https://arxiv.org/pdf/1607.01759.pdf" target="_blank" rel="noopener">论文地址</a>,其<strong>特点</strong>就是<strong>fast</strong>。在文本分类任务中，fastText（浅层网络）往往能取得和深度网络相媲美的精度，却在训练时间上比深度网络快许多数量级。在标准的多核CPU上， 在10分钟之内能够训练10亿词级别语料库的词向量，在1分钟之内能够分类有着30万多类别的50多万句子。</p>
<p>fastText是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：</p>
<ol>
<li>fastText在保持高精度的情况下加快了训练速度和测试速度</li>
<li>fastText不需要预训练好的词向量，fastText会自己训练词向量</li>
<li>fastText两个重要的优化：Hierarchical Softmax、N-gram</li>
</ol>
<h4 id="fast-text-mo-xing-jia-gou">fastText模型架构</h4>
<p>fastText模型架构和word2vec中的CBOW很相似， 不同之处是fastText预测标签而CBOW预测的是中间词，即模型架构类似但是模型的任务不同：</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/image-20200520093324964.png" alt></p>
<p>word2vec将上下文关系转化为多分类任务，进而训练逻辑回归模型。通常的文本数据中，词库少则数万，多则百万，在训练中直接训练多分类逻辑回归并不现实。word2vec中提供了两种针对大规模多分类问题的优化手段， negative sampling 和hierarchical softmax。在优化中，negative sampling 只更新少量负面类，从而减轻了计算量。hierarchical softmax 将词库表示成前缀树，从树根到叶子的路径可以表示为一系列二分类器，一次多分类计算的复杂度从|V|降低到了树的高度。<br>
fastText模型架构:其中\(x_1,x_2,\ldots,x_{N−1},x_N\)表示一个文本中的n-gram向量，每个特征是词向量的平均值。</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/image-20200520093359191.png" alt></p>
<h4 id="que-dian">缺点：</h4>
<blockquote>
<p>我不喜欢这类电影，但是喜欢这一个。</p>
<p>我喜欢这类电影，但是不喜欢这一个。</p>
</blockquote>
<p><strong>这样的两句句子经过词向量平均以后已经送入单层神经网络的时候已经完全一模一样了，分类器不可能分辨出这两句话的区别</strong>，只有添加n-gram特征以后才可能有区别。因此，在实际应用的时候需要对数据有足够的了解,然后在选择模型。</p>
<h4 id="mo-xing-dai-ma">模型代码</h4>
<p><img src="/2020/03/29/nature_language_process/text_classification/FastText_network_structure.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FastText</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, output_dim, word_embedding, freeze)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(self.embedding_size, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,_, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line">        pooled = F.avg_pool2d(embedded, (embedded.shape[<span class="number">1</span>], <span class="number">1</span>)).squeeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># pooled = [batch size, embedding_dim]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(pooled)</span><br></pre></td></tr></table></figure>
<h3 id="text-cnn">TextCNN</h3>
<h4 id="jian-jie-2">简介</h4>
<p><strong>Yoon Kim</strong>在论文<a href="https://arxiv.org/abs/1408.5882" target="_blank" rel="noopener">(2014 EMNLP) Convolutional Neural Networks for Sentence Classification</a>提出TextCNN。将卷积神经网络CNN应用到文本分类任务，利用多个不同size的kernel来提取句子中的关键信息，从而能够更好地捕捉局部相关性。</p>
<h4 id="wang-luo-jie-gou">网络结构</h4>
<p><img src="/2020/03/29/nature_language_process/text_classification/textcnn.png" alt></p>
<h4 id="yuan-li">原理</h4>
<p>TextCNN的详细过程原理图如下：</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/textcnndetail.png" alt></p>
<p>TextCNN详细过程：</p>
<ul>
<li>Embedding：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点。</li>
<li>Convolution：然后经过 <code>kernel_sizes=(2,3,4) </code>的一维卷积层，每个kernel_size 有两个输出 channel。</li>
<li>MaxPolling：第三层是一个<code>1-max pooling</code>层，这样不同长度句子经过pooling层之后都能变成定长的表示。</li>
<li>FullConnection and Softmax：最后接一层全连接的 softmax 层，输出每个类别的概率。</li>
</ul>
<h4 id="que-dian-1">缺点</h4>
<p>TextCNN模型最大的问题也是这个全局的max pooling丢失了结构信息，因此很难去发现文本中的转折关系等复杂模式。针对这个问题，可以尝试k-max pooling做一些优化，k-max pooling针对每个卷积核都不只保留最大的值，他保留前k个最大值，并且保留这些值出现的顺序，也即按照文本中的位置顺序来排列这k个最大值。在某些比较复杂的文本上相对于1-max pooling会有提升。</p>
<h4 id="dai-ma">代码</h4>
<p><img src="/2020/03/29/nature_language_process/text_classification/TextCNN_network_structure.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_filters, filter_sizes, output_dim, dropout, word_embedding, freeze)</span>:</span></span><br><span class="line">        <span class="comment"># n_filter 每个卷积核的个数</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line">        self.convs = nn.ModuleList(</span><br><span class="line">            [nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=n_filters, kernel_size=(fs, self.embedding_size)) <span class="keyword">for</span> fs <span class="keyword">in</span></span><br><span class="line">             filter_sizes])</span><br><span class="line">        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, _, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [batch size, sent len]</span></span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line">        embedded = embedded.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># embedded = [batch size, 1, sent len, emb dim]</span></span><br><span class="line">        conved = [F.relu(conv(embedded)).squeeze(<span class="number">3</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs]</span><br><span class="line">        <span class="comment"># conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]</span></span><br><span class="line">        pooled = [F.max_pool1d(conv, conv.shape[<span class="number">2</span>]).squeeze(<span class="number">2</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> conved]</span><br><span class="line">        <span class="comment"># pooled_n = [batch size, n_filters]</span></span><br><span class="line">        cat = self.dropout(torch.cat(pooled, dim=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># cat = [batch size, n_filters * len(filter_sizes)]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(cat)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN1d</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_filters, filter_sizes, output_dim, dropout, word_embedding, freeze)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line">        self.convs = nn.ModuleList(</span><br><span class="line">            [nn.Conv1d(in_channels=self.embedding_size, out_channels=n_filters, kernel_size=fs) <span class="keyword">for</span> fs <span class="keyword">in</span> filter_sizes])</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, _, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text = [batch size, sent len]</span></span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line">        embedded = embedded.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># embedded = [batch size, emb dim, sent len]</span></span><br><span class="line">        conved = [F.relu(conv(embedded)) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs]</span><br><span class="line">        <span class="comment"># conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]</span></span><br><span class="line">        pooled = [F.max_pool1d(conv, conv.shape[<span class="number">2</span>]).squeeze(<span class="number">2</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> conved]</span><br><span class="line">        <span class="comment"># pooled_n = [batch size, n_filters]</span></span><br><span class="line">        cat = self.dropout(torch.cat(pooled, dim=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># cat = [batch size, n_filters * len(filter_sizes)]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(cat)</span><br></pre></td></tr></table></figure>
<h3 id="dpcnn">DPCNN</h3>
<h4 id="jian-jie-3">简介：</h4>
<p>ACL2017年中，腾讯AI-lab提出了Deep Pyramid Convolutional Neural Networks for Text Categorization(<a href="https://ai.tencent.com/ailab/media/publications/ACL3-Brady.pdf" target="_blank" rel="noopener">DPCNN</a>)。论文中提出了一种基于word-level级别的网络-DPCNN，由于TextCNN 不能通过卷积获得文本的长距离依赖关系，而论文中DPCNN通过不断加深网络，可以抽取长距离的文本依赖关系。实验证明在不增加太多计算成本的情况下，增加网络深度就可以获得最佳的准确率。‍</p>
<h4 id="wang-luo-jie-gou-1">网络结构</h4>
<p><img src="/2020/03/29/nature_language_process/text_classification/DPCNN.jpg" alt></p>
<h5 id="region-embedding">Region embedding</h5>
<p>作者将TextCNN的包含多尺寸卷积滤波器的卷积层的卷积结果称之为<code>Region embedding</code>，意思就是对一个文本区域/片段（比如<code>3-gram</code>）进行一组卷积操作后生成的embedding。<br>
卷积操作有两种选择：</p>
<ol>
<li>保留词序：也就是设置一组<code>size=3*D</code>的二维卷积核对<code>3-gram</code>进行卷积（其中D是word embedding维度）</li>
<li>不保留词序（即使用词袋模型），即首先对<code>3-gram</code>中的3个词的embedding取均值得到一个size=D的向量，然后设置一组size=D的一维卷积核对该<code>3-gram</code>进行卷积。</li>
</ol>
<p>TextCNN里使用的是保留词序的做法，而DPCNN使用的是词袋模型的做法，DPCNN作者认为前者做法更容易造成过拟合，后者的性能却跟前者差不多。</p>
<h4 id="juan-ji-he-quan-lian-jie-de-quan-heng">卷积和全连接的权衡</h4>
<p>产生<code>region embedding</code>后，按照经典的TextCNN的做法的话，就是从每个特征图中挑选出最有代表性的特征，也就是直接应用全局最大池化层，这样就生成了这段文本的特征向量,假如卷积滤波器的size有3，4，5这三种，每种size包含100个卷积核，那么当然就会产生3<em>100幅特征图，然后将max-over-time-pooling操作应用到每个特征图上，于是文本的特征向量即3</em>100=300维。<br>
TextCNN这样做的意义本质上与<code>词袋模型(n-gram)+weighting+NB/MaxEnt/SVM</code>的经典文本分类模型没本质区别，只不过one-hot表示到word embedding表示的转变避免了词袋模型遭遇的数据稀疏问题。TextCNN本质上收益于词向量的引入带来的近义词有相近向量表示的bonus，同时TextCNN可以较好的利用词向量中近义关系。<strong>经典模型里难以学习的远距离信息在TextCNN中依然难以学习</strong>。</p>
<h5 id="deng-chang-juan-ji">等长卷积</h5>
<p>假设输入的序列长度为\(n\)，卷积核大小为\(m\)，步长为\(s\),输入序列两端各填补\(p\)个零,那么该卷积层的输出序列为\(\frac{(n-m+2p)}{s}+1\)。</p>
<ol>
<li>窄卷积:步长\(s=1\),两端不补零，即\(p=0\)，卷积后输出长度为\(n-m+1\)。</li>
<li>宽卷积:步长\(s=1\),两端补零\(p=m-1\)，卷积后输出长度\(n+m-1\)。</li>
<li>等长卷积: 步长\(s=1\),两端补零\(p=(m-1)/2\)，卷积后输出长度为\(n\)。</li>
</ol>
<p>将输入输出序列的第n个embedding称为第n个词位，那么这时size为n的卷积核产生的等长卷积的意义就是将输入序列的每个词位及其左右\(\frac{n-1}{2}\)个词的上下文信息压缩为该词位的embedding，产生了每个词位的被上下文信息修饰过的更高level更加准确的语义。想要克服TextCNN的缺点，捕获长距离模式，显然就要用到深层CNN。</p>
<p>直接等长卷积堆等长卷积会让每个词位包含进去越来越多，越来越长的上下文信息，这种方式会让网络层数变得非常非常非常深，但是这种方式太笨重。不过，既然等长卷积堆等长卷积会让每个词位的embedding描述语义描述的更加丰富准确，可以适当的堆两层来提高词位embedding的表示的丰富性。<br>
<img src="/2020/03/29/nature_language_process/text_classification/equal_cnn.png" alt></p>
<h5 id="gu-ding-feature-map-de-shu-liang">固定feature map的数量</h5>
<p>在表示好每个词位的语义后，很多邻接词或者邻接<code>ngram</code>的词义是可以合并，例如“小明 人 不要 太好”中的“不要”和“太好”虽然语义本来离得很远，但是作为邻接词“不要太好”出现时其语义基本等价为“很好”，完全可以把“不要”和“太好”的语义进行合并。同时，合并的过程完全可以在原始的embedding space中进行的，原文中直接把“不要太好”合并为“很好”是很可以的，完全没有必要动整个语义空间。<br>
实际上，相比图像中这种从“点、线、弧”这种low-level特征到“眼睛、鼻子、嘴”这种high-level特征的明显层次性的特征区分，文本中的特征进阶明显要扁平的多，即从单词（1gram）到短语再到3gram、4gram的升级，其实很大程度上均满足“语义取代”的特性。而图像中就很难发生这种“语义取代”现象。因此，DPCNN与ResNet很大一个不同就是，<strong>在DPCNN中固定死了feature map的数量</strong>，也就是固定住了embedding space的维度（为了方便理解，以下简称语义空间），使得网络有可能让整个邻接词（邻接ngram）的合并操作在原始空间或者与原始空间相似的空间中进行（当然，网络在实际中会不会这样做是不一定的，只是提供了这么一种条件）。也就是说，整个网络虽然形状上来看是深层的，但是从语义空间上来看完全可以是扁平的。而ResNet则是不断的改变语义空间，使得图像的语义随着网络层的加深也不断的跳向更高level的语义空间。</p>
<h5 id="strong-chi-hua-strong"><strong>池化</strong></h5>
<p>每经过一个\(size=3,stride=2\)的池化层(简称\(1/2\)池化层)，序列的长度就被压缩成了原来的一半。这样同样是\(size=3\)的卷积核，每经过一个\(1/2\)池化层后，其能感知到的文本片段就比之前长了一倍。例如之前是只能感知3个词位长度的信息，经过1/2池化层后就能感知6个词位长度的信息，这时把1/2池化层和size=3的卷积层组合起来如图：<br>
<img src="/2020/03/29/nature_language_process/text_classification/dpcnn_pooling.png" alt></p>
<h5 id="can-chai-lian-jie">残差连接</h5>
<p>在初始化深度CNN时，往往各层权重都是初始化为一个很小的值，这就导致最开始的网络中，后续几乎每层的输入都是接近0，这时网络的输出自然是没意义的，而这些小权重同时也阻碍了梯度的传播，使得网络的初始训练阶段往往要迭代好久才能启动。同时，就算网络启动完成，由于深度网络中仿射矩阵近似连乘，训练过程中网络也非常容易发生梯度爆炸或弥散问题（虽然由于非共享权重，深度CNN网络比RNN网络要好点）。<br>
针对深度CNN网络的梯度弥散问题ResNet中提出的<code>shortcut-connection/skip-connection/residual-connection</code>（残差连接）就是一种非常简单、合理、有效的解决方案。<br>
<img src="/2020/03/29/nature_language_process/text_classification/dpcnn_resnet.png" alt><br>
既然每个block的输入在初始阶段容易是0而无法激活，那么直接用一条线把region embedding层连接到每个block的输入乃至最终的池化层/输出层。有了shortcut后，梯度就可以忽略卷积层权重的削弱，从shortcut一路无损的传递到各个block，直至网络前端，从而极大的缓解了梯度消失问题。</p>
<h4 id="dai-ma-1">代码</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DPCNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,num_filters, num_classes,word_embedding, freeze)</span>:</span></span><br><span class="line">        super(DPCNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line"></span><br><span class="line">        self.conv_region = nn.Conv2d(<span class="number">1</span>, num_filters, (<span class="number">3</span>, self.embedding_size), stride=<span class="number">1</span>)</span><br><span class="line">        self.conv = nn.Conv2d(num_filters, num_filters, (<span class="number">3</span>, <span class="number">1</span>), stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.max_pool = nn.MaxPool2d(kernel_size=(<span class="number">3</span>, <span class="number">1</span>), stride=<span class="number">2</span>)</span><br><span class="line">        self.padding1 = nn.ZeroPad2d((<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># top bottom</span></span><br><span class="line">        self.padding2 = nn.ZeroPad2d((<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>))  <span class="comment"># bottom</span></span><br><span class="line"></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.fc = nn.Linear(num_filters, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,_, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># text [batch_size,seq_len]</span></span><br><span class="line">        x = self.embedding(text)  <span class="comment"># x=[batch_size,seq_len,embedding_dim]</span></span><br><span class="line">        x = x.unsqueeze(<span class="number">1</span>)  <span class="comment"># [batch_size, 1, seq_len, embedding_dim]</span></span><br><span class="line">        x = self.conv_region(x)  <span class="comment"># x = [batch_size, num_filters, seq_len-3+1, 1]</span></span><br><span class="line"></span><br><span class="line">        x = self.padding1(x)  <span class="comment"># [batch_size, num_filters, seq_len, 1]</span></span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv(x)  <span class="comment"># [batch_size, num_filters, seq_len-3+1, 1]</span></span><br><span class="line">        x = self.padding1(x)  <span class="comment"># [batch_size, num_filters, seq_len, 1]</span></span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv(x)  <span class="comment"># [batch_size, num_filters, seq_len-3+1, 1]</span></span><br><span class="line">        <span class="keyword">while</span> x.size()[<span class="number">2</span>] &gt;= <span class="number">2</span>:</span><br><span class="line">            x = self._block(x)  <span class="comment"># [batch_size, num_filters,1,1]</span></span><br><span class="line">        x = x.squeeze()  <span class="comment"># [batch_size, num_filters]</span></span><br><span class="line">        x = self.fc(x)  <span class="comment"># [batch_size, 1]</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_block</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.padding2(x)</span><br><span class="line">        px = self.max_pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.padding1(px)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line"></span><br><span class="line">        x = self.padding1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Short Cut</span></span><br><span class="line">        x = x + px</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="rnn-xi-lie">RNN系列</h3>
<h4 id="rnn">RNN</h4>
<p>通过将前一时刻的运算结果添加到当前的运算中，从而实现了“考虑上文信息”的功能。</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/rnn.png" alt></p>
<p>RNN可以考虑上文的信息，那么如何将下文的信息也添加进去呢？这就是BiRNN要做的事情。</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/birnn.png" alt></p>
<h4 id="lstm">LSTM</h4>
<p>因为RNN存在梯度弥散和梯度爆炸的问题，所以RNN很难完美地处理具有长期依赖的信息。既然仅仅依靠一条线连接后面的神经元不足以解决问题，那么就再加一条线(这条线实现的功能是把rnn中的累乘变成了累加)，这就是LSTM。</p>
<p>LSTM的关键在于细胞的状态和穿过细胞的线，细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流动保持不变会变得容易。</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/lstm_c.png" alt></p>
<p>在LSTM中，门可以实现选择性的让信息通过，主要通过一个sigmoid的神经层和一个逐点相乘的操作来实现。LSTM通过三个这样的门结构来实现信息的保护和控制，分别是遗忘门（forget gate）、输入门（input gate）与输出门（output gate）。</p>
<h5 id="strong-yi-wang-men-strong"><strong>遗忘门</strong></h5>
<p>在LSTM中的第一步是决定从细胞状态中丢弃什么信息。这个决定通过一个称为遗忘门的结构来完成。遗忘门会读取\(h_{t-1}\)和\(x_t\)，输出一个0到1之间的数值给细胞的状态\(c_{t-1}\)中的数字。1表示完全保留，0表示完全舍弃。</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/lstm_forget_gate.png" alt></p>
<h5 id="strong-shu-ru-men-strong"><strong>输入门</strong></h5>
<p>遗忘门决定让多少新的信息加入到cell的状态中来。实现这个需要两个步骤：</p>
<ol>
<li>
<p>首先一个叫“input gate layer”的sigmoid层决定哪些信息需要更新；一个tanh层生成一个向量，用来更新状态C。</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/input_gate.png" alt></p>
</li>
<li>
<p>把 1 中的两部分联合起来，对cell的状态进行更新，我们把旧的状态与\(f_t\)相乘，丢弃掉我们确定需要丢弃的信息，接着加上\(i_t * \tilde{C}_{t}\)</p>
</li>
</ol>
<h5 id="shu-chu-men">输出门</h5>
<p>最终，我们需要确定输出什么值，这个输出将会基于我们的细胞的状态，但是也是一个过滤后的版本。首先，我们通过一个sigmoid层来确定细胞状态的哪些部分将输出出去。接着，我们把细胞状态通过tanh进行处理（得到一个-1到1之间的值）并将它和sigmoid门的输出相乘，最终我们仅仅会输出我们确定输出的部分。</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/output_gate.png" alt></p>
<h5 id="gong-shi">公式</h5>
<p><img src="/2020/03/29/nature_language_process/text_classification/lstm_all.png" alt></p>
<h4 id="gru">GRU</h4>
<p>在LSTM中引入了三个门函数：输入门、遗忘门和输出门来控制输入值、记忆值和输出值。而在GRU模型中只有两个门：分别是更新门和重置门。</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/GRU.png" alt></p>
<p>图中的\(z_t\)和\(r_t\)分别表示更新门和重置门。更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多。重置门控制前一状态有多少信息被写入到当前的候选集 \(\tilde{h}_{t}\)上，重置门越小，前一状态的信息被写入的越少。</p>
<p>LSTM和CRU都是通过各种门函数来将重要特征保留下来，这样就保证了在long-term传播的时候也不会丢失。此外GRU相对于LSTM少了一个门函数，因此在参数的数量上也是要少于LSTM的，所以整体上GRU的训练速度要快于LSTM的。</p>
<h4 id="dai-ma-2">代码</h4>
<p><img src="/2020/03/29/nature_language_process/text_classification/TextBiRNN_network_structure.png" alt="TextBiRNN_network_structure"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RnnModel</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, hidden_dim, output_dim, n_layers,bidirectional, dropout,word_embedding, freeze,batch_first=True)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.embedding_size,</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.embedding_size,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.embedding_size,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(hidden_dim * n_layers, output_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,_, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># 按照句子长度从大到小排序</span></span><br><span class="line">        text, sorted_seq_lengths, desorted_indices = prepare_pack_padded_sequence(text, text_lengths)</span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        embedded = self.dropout(self.embedding(text))</span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_seq_lengths, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># output (seq_len, batch, num_directions * hidden_size)</span></span><br><span class="line">            <span class="comment"># hidden (num_layers * num_directions, batch, hidden_size)</span></span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line">        <span class="comment"># 把句子序列再调整成输入时的顺序</span></span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        <span class="comment"># output = [batch_size,seq_len,hidden_dim * num_directionns ]</span></span><br><span class="line">        batch_size, max_seq_len,hidden_dim = output.shape</span><br><span class="line">        hidden = torch.mean(torch.reshape(hidden,[batch_size,<span class="number">-1</span>,hidden_dim]),dim=<span class="number">1</span>)</span><br><span class="line">        output = torch.sum(output,dim=<span class="number">1</span>)</span><br><span class="line">        fc_input = self.dropout(output + hidden)</span><br><span class="line">        out = self.fc(fc_input)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h4 id="self-attention">Self-Attention</h4>
<p><img src="/2020/03/29/nature_language_process/text_classification/self-attention.png" alt></p>
<ol>
<li>Encode所有输入序列,得到对应的\(h_1,h_2, \cdots ,h_T\)(T为输入序列长度)</li>
<li>Decode输出目标\(y_t\)之前，会将上一步输出的隐藏状态\(S_{t-1}\)与之前encode好的\(h_1,h_2,\cdots,h_T\)进行比对，计算相似度（\(e_{t,j}=a(s_{t-1},h_j)\)）,\(h_j\)为之前第j个输入encode得到的隐藏向量，a为任意一种计算相似度的方式</li>
<li>然后通过softmax，即\(a_{t,j}=\frac{exp(e_{t,j})}{\sum^{T_x}_{k=1}exp(e_{t,k})}\)将之前得到的各个部分的相关系数进行归一化，得到\(a_{t,1},a_{t,2},\cdots,a_{t,T}\)</li>
<li>在对输入序列的隐藏层进行相关性加权求和得到此时decode需要的context vector ：</li>
</ol>
<h4 id="rnn-attenton">Rnn-Attenton</h4>
<p><img src="/2020/03/29/nature_language_process/text_classification/TextAttBiRNN_network_structure.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RnnAttentionModel</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, hidden_dim, output_dim, n_layers,bidirectional, dropout,word_embedding, freeze, batch_first=True)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.embedding_size,</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.embedding_size,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.embedding_size,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        self.tanh1 = nn.Tanh()</span><br><span class="line">        self.tanh2 = nn.Tanh()</span><br><span class="line">        <span class="comment"># self.u = nn.Parameter(torch.Tensor(self.hidden_dim * 2,self.hidden_dim*2))</span></span><br><span class="line">        self.w = nn.Parameter(torch.randn(hidden_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="keyword">if</span> bidirectional:</span><br><span class="line">            self.w = nn.Parameter(torch.randn(hidden_dim * <span class="number">2</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">            self.fc = nn.Linear(hidden_dim * <span class="number">2</span>, output_dim)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.w = nn.Parameter(torch.randn(hidden_dim), requires_grad=<span class="literal">True</span>)</span><br><span class="line">            self.fc = nn.Linear(hidden_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,_, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># 按照句子长度从大到小排序</span></span><br><span class="line">        text, sorted_seq_lengths, desorted_indices = prepare_pack_padded_sequence(text, text_lengths)</span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        embedded = self.dropout(self.embedding(text))</span><br><span class="line">        <span class="comment"># embedded = [batch size,sent len,  emb dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_seq_lengths, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        <span class="comment"># output = [sent len, batch size, hidden dim * num_direction]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line">        <span class="comment"># 把句子序列再调整成输入时的顺序</span></span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        <span class="comment"># attention</span></span><br><span class="line">        <span class="comment"># M = [sent len, batch size, hidden dim * num_direction]</span></span><br><span class="line">        <span class="comment"># M = self.tanh1(output)</span></span><br><span class="line">        alpha = F.softmax(torch.matmul(self.tanh1(output), self.w), dim=<span class="number">0</span>).unsqueeze(<span class="number">-1</span>)  <span class="comment"># dim=0表示针对文本中的每个词的输出softmax</span></span><br><span class="line">        output_attention = output * alpha</span><br><span class="line"></span><br><span class="line">        batch_size, max_seq_len,hidden_dim = output.shape</span><br><span class="line">        hidden = torch.mean(torch.reshape(hidden,[batch_size,<span class="number">-1</span>,hidden_dim]),dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        output_attention = torch.sum(output_attention, dim=<span class="number">1</span>)</span><br><span class="line">        output = torch.sum(output, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        fc_input = self.dropout(output + output_attention + hidden)</span><br><span class="line">        <span class="comment"># fc_input = self.dropout(output_attention)</span></span><br><span class="line">        out = self.fc(fc_input)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h3 id="text-rcnn">TextRCNN</h3>
<h4 id="jian-jie-4">简介</h4>
<p>RNN和CNN作为文本分类问题的主要模型架构，都存在各自的优点及局限性。RNN擅长处理序列结构，能够考虑到句子的上下文信息，但RNN属于“biased model”，一个句子中越往后的词重要性越高，这有可能影响最后的分类结果，因为对句子分类影响最大的词可能处在句子任何位置。CNN属于无偏模型，能够通过最大池化获得最重要的特征，但是CNN的滑动窗口大小不容易确定，选的过小容易造成重要信息丢失，选的过大会造成巨大参数空间。为了解决二者的局限性，<a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552" target="_blank" rel="noopener">RCNN</a>这篇文章提出了一种新的网络架构，用双向循环结构获取上下文信息，这比传统的基于窗口的神经网络更能减少噪声，而且在学习文本表达时可以大范围的保留词序。其次使用最大池化层获取文本的重要部分，自动判断哪个特征在文本分类过程中起更重要的作用。</p>
<h4 id="mo-xing-jie-gou">模型结构</h4>
<p><img src="/2020/03/29/nature_language_process/text_classification/rcnn.png" alt></p>
<h4 id="word-representation-learning">Word Representation Learning</h4>
<p>作者提出将单词的左上下文、右上下文、单词本身结合起来作为单词表示。作者使用了双向RNN来分别提取句子的上下文信息。公式如下:<br>
\[
\begin{array}{l}
c_{l}\left(w_{i}\right)=f\left(W^{(l)} c_{l}\left(w_{i-1}\right)+W^{(s l)} e\left(w_{i-1}\right)\right)  \\
c_{r}\left(w_{i}\right)=f\left(W^{(r)} c_{r}\left(w_{i+1}\right)+W^{(s r)} e\left(w_{i+1}\right)\right)
\end{array}
\]<br>
其中，\(c_l(w_i)\)代表单词\(w_i\)的左上下文，\(c_l(w_i)\)由上一个单词的左上下文\(c_l\)和\(c_l(w_{i-1})\)上一个单词的词嵌入向量 \(e(w_{i-1})\)计算得到，如公式（1）所示，所有句子第一个单词的左侧上下文使用相同的共享参数\(c_l(w_1)\)。 \(W^{(l)},W^{(sl)}\)用于将上一个单词的左上下文语义和上一个单词的语义结合到单词 \(w_i\)的左上下文表示中。右上下文的处理与左上下文完全相同，同样所有句子最后一个单词的右侧上下文使用相同的共享参数\(c_r(w_n)\)。 得到句子中每个单词的左上下文表示和右上下文表示后，就可以定义单词  \(w_i\)的表示如下<br>
\[
\boldsymbol{x}_{i}=\left[\boldsymbol{c}_{l}\left(w_{i}\right) ; \boldsymbol{e}\left(w_{i}\right) ; \boldsymbol{c}_{r}\left(w_{i}\right)\right]
\]</p>
<p>实际就是单词\(w_i\)，单词的词嵌入表示向量 \(e(w_i)\)以及单词的右上下文向量\(c_e(w_i)\) 的拼接后的结果。得到\(w_i\)的表示\(x_i\)后，就可以输入激活函数得到\(w_i\)的潜在语义向量 \(y_i^{(2)}\) 。<br>
\[
\boldsymbol{y}_{i}^{(2)}=\tanh \left(W^{(2)} \boldsymbol{x}_{i}+\boldsymbol{b}^{(2)}\right)
\]</p>
<h4 id="text-representation-learning">Text Representation Learning</h4>
<p>经过卷积层后，获得了所有词的表示，首先对其进行最大池化操作，最大池化可以帮助找到句子中最重要的潜在语义信息。<br>
\[
\boldsymbol{y}^{(3)}=\max _{i=1}^{n} \boldsymbol{y}_{i}^{(2)}
\]<br>
然后经过全连接层得到文本的表示，最后通过softmax层进行分类。<br>
\[
\begin{aligned}
&amp;\boldsymbol{y}^{(4)}=W^{(4)} \boldsymbol{y}^{(3)}+\boldsymbol{b}^{(4)}\\
&amp;p_{i}=\frac{\exp \left(\boldsymbol{y}_{i}^{(4)}\right)}{\sum_{k=1}^{n} \exp \left(\boldsymbol{y}_{k}^{(4)}\right)}
\end{aligned}
\]</p>
<h4 id="dai-ma-3">代码</h4>
<p><img src="/2020/03/29/nature_language_process/text_classification/RCNN_network_structure.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RCNNModel</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, hidden_dim, output_dim, n_layers,bidirectional, dropout,word_embedding, freeze, batch_first=True)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.embedding_size = word_embedding.vectors.shape[<span class="number">1</span>]</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding.vectors), freeze=freeze)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.embedding_size ,</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.embedding_size ,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.embedding_size ,</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.fc_cat = nn.Linear(hidden_dim * n_layers + self.embedding_size,self.embedding_size)</span><br><span class="line">        self.fc = nn.Linear(self.embedding_size, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text,_, text_lengths)</span>:</span></span><br><span class="line">        <span class="comment"># 按照句子长度从大到小排序</span></span><br><span class="line">        text, sorted_seq_lengths, desorted_indices = prepare_pack_padded_sequence(text, text_lengths)</span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        embedded = self.dropout(self.embedding(text))</span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_seq_lengths, batch_first=self.batch_first)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># packed_output</span></span><br><span class="line">        <span class="comment"># hidden [n_layers * bi_direction,batch_size,hidden_dim]</span></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        <span class="comment"># output [sent len, batch_size * n_layers * bi_direction]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line">        <span class="comment"># 把句子序列再调整成输入时的顺序</span></span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        <span class="comment"># output = [batch_size,seq_len,hidden_dim * num_directionns ]</span></span><br><span class="line">        batch_size, max_seq_len,hidden_dim = output.shape</span><br><span class="line">        <span class="comment"># 拼接左右上下文信息</span></span><br><span class="line">        output = torch.tanh(self.fc_cat(torch.cat((output, embedded), dim=<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">        output = torch.transpose(output,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">        output = F.max_pool1d(output,max_seq_len).squeeze().contiguous()</span><br><span class="line">        output = self.fc(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h3 id="han">HAN</h3>
<p><img src="/2020/03/29/nature_language_process/text_classification/HAN.png" alt></p>
<p>整个网络结构包括五个部分：</p>
<ol>
<li>词序列编码器</li>
<li>基于词级的注意力层</li>
<li>句子编码器</li>
<li>基于句子级的注意力层</li>
<li>分类</li>
</ol>
<p>整个网络结构由双向GRU网络和注意力机制组合而成。</p>
<h4 id="ci-xu-lie-bian-ma-qi">词序列编码器</h4>
<p>给定一个句子中的单词\(w_{it}\)，其中 \(i\) 表示第\(i\) 个句子，\(t\) 表示第 \(t\) 个词。通过一个词嵌入矩阵 \(W_e\) 将单词转换成向量表示，具体如下所示：<br>
\[
x_{it}=W_e w_{it}
\]</p>
<p>利用双向GRU实现的整个编码流程：<br>
\[
\begin{aligned}
x_{i t} &amp;=W_{e} w_{i t}, t \in[1, T] \\
\overrightarrow{h}_{i t} &amp;=\overrightarrow{\operatorname{GRU}}\left(x_{i t}\right), t \in[1, T] \\
\overleftarrow{h}_{i t} &amp;=\overleftarrow{\operatorname{GRU}}\left(x_{i t}\right), t \in[T, 1] \\
{h}_{i t} &amp;= [\overrightarrow{h}_{i t},\overleftarrow{h}_{i t} ]
\end{aligned}
\]</p>
<h4 id="ci-ji-de-zhu-yi-li-ceng">词级的注意力层</h4>
<p>但是对于一句话中的单词，并不是每一个单词对分类任务都是有用的，比如在做文本的情绪分类时，可能我们就会比较关注“很好”、“伤感”这些词。为了能使循环神经网络也能自动将“注意力”放在这些词汇上，作者设计了基于单词的注意力层的具体流程如下：<br>
\[
\begin{aligned}
u_{i t} &amp;=\tanh \left(W_{w} h_{i t}+b_{w}\right) \\
\alpha_{i t} &amp;=\frac{\exp \left(u_{i t}^{\top} u_{w}\right)}{\sum_{t} \exp \left(u_{i t}^{\top} u_{w}\right)} \\
s_{i} &amp;=\sum_{t} \alpha_{i t} h_{i t}
\end{aligned}
\]<br>
上面式子中，\(u_{it}\) 是 \(h_{it}\) 的隐层表示，\(a_{it}\) 是经 softmax 函数处理后的归一化权重系数，\(u_w\)是一个随机初始化的向量，之后会作为模型的参数一起被训练，\(s_i\) 就是我们得到的第 i 个句子的向量表示。</p>
<h4 id="ju-zi-bian-ma-qi">句子编码器</h4>
<p>句子编码器也是基于双向GRU实现编码的，<br>
\[
\begin{aligned}
&amp;\overrightarrow{h}_{i}=\overrightarrow{\operatorname{GRU}}\left(s_{i}\right), i \in[1, L]\\
&amp;\overleftarrow{h}_{i}=\overleftarrow{\operatorname{GRU}}\left(s_{i}\right), t \in[L, 1]
\end{aligned}
\]<br>
公式和词编码类似，最后的 \(h_i\) 也是通过拼接得到的.</p>
<h4 id="ju-zi-ji-zhu-yi-li-ceng">句子级注意力层</h4>
<p>注意力层的流程如下，和词级的一致:<br>
\[
\begin{aligned}
u_{i} &amp;=\tanh \left(W_{s} h_{i}+b_{s}\right) \\
\alpha_{i} &amp;=\frac{\exp \left(u_{i}^{\top} u_{s}\right)}{\sum_{i} \exp \left(u_{i}^{\top} u_{s}\right)} \\
v &amp;=\sum_{i} \alpha_{i} h_{i}
\end{aligned}
\]<br>
最后得到的向量\(v\) 就是文档的向量表示，这是文档的高层表示。接下来就可以用可以用这个向量表示作为文档的特征。</p>
<h4 id="fen-lei">分类</h4>
<p>使用最常用的softmax分类器对整个文本进行分类了<br>
\[
p=\operatorname{softmax}\left(W_{c} v+b_{c}\right)
\]<br>
损失函数<br>
\[
L=-\sum_{d} \log p_{d j}
\]</p>
<h4 id="dai-ma-4">代码</h4>
<p><img src="/2020/03/29/nature_language_process/text_classification/HAN_network_structure.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HierAttNet</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,rnn_type, word_hidden_size, sent_hidden_size, num_classes, word_embedding,n_layers,bidirectional,batch_first,freeze,dropout)</span>:</span></span><br><span class="line">        super(HierAttNet, self).__init__()</span><br><span class="line">        self.word_embedding = word_embedding</span><br><span class="line">        self.word_hidden_size = word_hidden_size</span><br><span class="line">        self.sent_hidden_size = sent_hidden_size</span><br><span class="line">        self.word_att_net = WordAttNet(rnn_type,word_embedding, word_hidden_size,n_layers,bidirectional,batch_first,dropout,freeze)</span><br><span class="line">        self.sent_att_net = SentAttNet(rnn_type,sent_hidden_size, word_hidden_size,n_layers,bidirectional,batch_first,dropout, num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, batch_doc, _, text_lengths)</span>:</span></span><br><span class="line">        output_list = []</span><br><span class="line">        <span class="comment"># ############################ 词级 #########################################</span></span><br><span class="line">        <span class="keyword">for</span> idx,doc <span class="keyword">in</span> enumerate(batch_doc):</span><br><span class="line">            <span class="comment"># 把一篇文档拆成多个句子</span></span><br><span class="line">            doc = doc[:text_lengths[idx]]</span><br><span class="line">            doc_list = doc.cpu().numpy().tolist()</span><br><span class="line">            sep_index = [i <span class="keyword">for</span> i, num <span class="keyword">in</span> enumerate(doc_list) <span class="keyword">if</span> num == self.word_embedding.stoi[<span class="string">'[SEP]'</span>]]</span><br><span class="line">            sentence_list = []</span><br><span class="line">            <span class="keyword">if</span> sep_index:</span><br><span class="line">                pre = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> cur <span class="keyword">in</span> sep_index:</span><br><span class="line">                    sentence_list.append(doc_list[pre:cur])</span><br><span class="line">                    pre = cur</span><br><span class="line"></span><br><span class="line">                sentence_list.append(doc_list[cur:])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sentence_list.append(doc_list)</span><br><span class="line">            max_sentence_len = len(max(sentence_list,key=<span class="keyword">lambda</span> x:len(x)))</span><br><span class="line">            seq_lens = []</span><br><span class="line">            input_token_ids = []</span><br><span class="line">            <span class="keyword">for</span> sent <span class="keyword">in</span> sentence_list:</span><br><span class="line">                cur_sent_len = len(sent)</span><br><span class="line">                seq_lens.append(cur_sent_len)</span><br><span class="line">                input_token_ids.append(sent+[self.word_embedding.stoi[<span class="string">'PAD'</span>]]*(max_sentence_len-cur_sent_len))</span><br><span class="line">            input_token_ids = torch.LongTensor(np.array(input_token_ids)).to(batch_doc.device)</span><br><span class="line">            seq_lens = torch.LongTensor(np.array(seq_lens)).to(batch_doc.device)</span><br><span class="line">            word_output, hidden = self.word_att_net(input_token_ids,seq_lens)</span><br><span class="line">            <span class="comment"># word_output = [bs,hidden_size]</span></span><br><span class="line">            output_list.append(word_output)</span><br><span class="line"></span><br><span class="line">        max_doc_sent_num = len(max(output_list,key=<span class="keyword">lambda</span> x: len(x)))</span><br><span class="line">        batch_sent_lens = []</span><br><span class="line">        batch_sent_inputs = []</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ############################ 句子级 #########################################</span></span><br><span class="line">        <span class="keyword">for</span> doc <span class="keyword">in</span> output_list:</span><br><span class="line">            cur_doc_sent_len = len(doc)</span><br><span class="line">            batch_sent_lens.append(cur_doc_sent_len)</span><br><span class="line">            expand_doc = torch.cat([doc,torch.zeros(size=((max_doc_sent_num-cur_doc_sent_len),len(doc[<span class="number">0</span>]))).to(doc.device)],dim=<span class="number">0</span>)</span><br><span class="line">            batch_sent_inputs.append(expand_doc.unsqueeze(dim=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        batch_sent_inputs = torch.cat(batch_sent_inputs, <span class="number">0</span>)</span><br><span class="line">        batch_sent_lens = torch.LongTensor(np.array(batch_sent_lens)).to(doc.device)</span><br><span class="line">        output = self.sent_att_net(batch_sent_inputs,batch_sent_lens)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h3 id="bert">Bert</h3>
<h4 id="bert-1">BERT</h4>
<p><img src="/2020/03/29/nature_language_process/text_classification/bert_gpt_elmo.png" alt></p>
<h5 id="task-1-mlm">Task 1: MLM</h5>
<p>由于BERT需要通过上下文信息，来预测中心词的信息，同时又不希望模型提前看见中心词的信息，因此提出了一种 Masked Language Model 的预训练方式，即随机从输入预料上 mask 掉一些单词，然后通过的上下文预测该单词，类似于一个完形填空任务。</p>
<p>在预训练任务中，15%的 Word Piece 会被mask，这15%的 Word Piece 中，80%的时候会直接替换为 [Mask] ，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token</p>
<ul>
<li>没有100%mask的原因
<ul>
<li>如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词</li>
</ul>
</li>
<li>加入10%随机token的原因
<ul>
<li>Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘</li>
<li>另外编码器不知道哪些词需要预测的，哪些词是错误的，因此被迫需要学习每一个token的表示向量</li>
</ul>
</li>
<li>另外，每个batchsize只有15%的单词被mask的原因，是因为性能开销的问题，双向编码器比单项编码器训练要更慢</li>
</ul>
<h5 id="task-2-nsp">Task 2: NSP</h5>
<p>仅仅一个MLM任务是不足以让 BERT 解决阅读理解等句子关系判断任务的，因此添加了额外的一个预训练任务，即 Next Sequence Prediction。</p>
<p>具体任务即为一个句子关系判断任务，即判断句子B是否是句子A的下文，如果是的话输出’IsNext‘，否则输出’NotNext‘。</p>
<p>训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在图4中的[CLS]符号中</p>
<h5 id="shu-ru">输入</h5>
<ul>
<li>Token Embeddings：即传统的词向量层，每个输入样本的首字符需要设置为[CLS]，可以用于之后的分类任务，若有两个不同的句子，需要用[SEP]分隔，且最后一个字符需要用[SEP]表示终止</li>
<li>Segment Embeddings：为[0,1][0,1]序列，用来在NSP任务中区别两个句子，便于做句子关系判断任务</li>
<li>Position Embeddings：与Transformer中的位置向量不同，BERT中的位置向量是直接训练出来的</li>
</ul>
<h5 id="fine-tunninng">Fine-tunninng</h5>
<p>对于不同的下游任务，我们仅需要对BERT不同位置的输出进行处理即可，或者直接将BERT不同位置的输出直接输入到下游模型当中。具体的如下所示：</p>
<ul>
<li>对于情感分析等单句分类任务，可以直接输入单个句子（不需要[SEP]分隔双句），将[CLS]的输出直接输入到分类器进行分类</li>
<li>对于句子对任务（句子关系判断任务），需要用[SEP]分隔两个句子输入到模型中，然后同样仅须将[CLS]的输出送到分类器进行分类</li>
<li>对于问答任务，将问题与答案拼接输入到BERT模型中，然后将答案位置的输出向量进行二分类并在句子方向上进行softmax（只需预测开始和结束位置即可）</li>
<li>对于命名实体识别任务，对每个位置的输出进行分类即可，如果将每个位置的输出作为特征输入到CRF将取得更好的效果。</li>
</ul>
<h5 id="que-dian-2">缺点</h5>
<ul>
<li>BERT的预训练任务MLM使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务</li>
<li>另外，BERT没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计</li>
<li>由于最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）</li>
<li>适合处理自然语义理解类任务(NLU)，而不适合自然语言生成类任务(NLG)</li>
</ul>
<h5 id="dai-ma-5">代码</h5>
<h6 id="bert-2">Bert</h6>
<figure class="highlight python"><figcaption><span>bert</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bert</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bert_path, num_classes, word_embedding, trained=True)</span>:</span></span><br><span class="line">        super(Bert, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="comment"># 不对bert进行训练</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.bert.parameters():</span><br><span class="line">            param.requires_grad = trained</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>], num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, context, bert_masks, seq_lens)</span>:</span></span><br><span class="line">        <span class="comment"># context  输入的句子序列</span></span><br><span class="line">        <span class="comment"># seq_len  句子长度</span></span><br><span class="line">        <span class="comment"># mask     对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        <span class="comment"># cls [batch_size, 768]</span></span><br><span class="line">        <span class="comment"># sentence [batch size,sen len,  768]</span></span><br><span class="line">        sentence, cls = self.bert(context, attention_mask=bert_masks)</span><br><span class="line">        sentence = torch.sum(sentence,dim=<span class="number">1</span>)</span><br><span class="line">        out = self.fc(sentence)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h6 id="bert-cnn">BertCNN</h6>
<figure class="highlight python"><figcaption><span>BertCNN</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertCNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bert_path, num_filters, hidden_size, filter_sizes, dropout, num_classes, word_embedding,</span></span></span><br><span class="line"><span class="function"><span class="params">                 trained=True)</span>:</span></span><br><span class="line">        super(BertCNN, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.bert.parameters():</span><br><span class="line">            param.requires_grad = trained</span><br><span class="line">        self.convs = nn.ModuleList(</span><br><span class="line">            [nn.Conv2d(<span class="number">1</span>, num_filters, (k, hidden_size)) <span class="keyword">for</span> k <span class="keyword">in</span> filter_sizes])</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.fc_cnn = nn.Linear(num_filters * len(filter_sizes), num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conv_and_pool</span><span class="params">(self, x, conv)</span>:</span></span><br><span class="line">        x = F.relu(conv(x)).squeeze(<span class="number">3</span>)</span><br><span class="line">        x = F.max_pool1d(x, x.size(<span class="number">2</span>)).squeeze(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, context, bert_masks, seq_len)</span>:</span></span><br><span class="line">        <span class="comment"># context    输入的句子</span></span><br><span class="line">        <span class="comment"># mask  对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        encoder_out, text_cls = self.bert(context, attention_mask=bert_masks)</span><br><span class="line">        out = encoder_out.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        out = torch.cat([self.conv_and_pool(out, conv) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs], <span class="number">1</span>)</span><br><span class="line">        out = self.dropout(out)</span><br><span class="line">        out = self.fc_cnn(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h6 id="bert-rnn">BertRNN</h6>
<figure class="highlight python"><figcaption><span>BertRNN</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, bert_path, hidden_dim, n_layers, bidirectional, batch_first, word_embedding,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout, num_classes, trained)</span>:</span></span><br><span class="line">        super(BertRNN, self).__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.bert.parameters():</span><br><span class="line">            param.requires_grad = trained</span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                               hidden_size=hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.fc_rnn = nn.Linear(hidden_dim * <span class="number">2</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, bert_masks, seq_lens)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        <span class="comment"># context    输入的句子</span></span><br><span class="line">        <span class="comment"># mask  对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        bert_sentence, bert_cls = self.bert(text, attention_mask=bert_masks)</span><br><span class="line">        sentence_len = bert_sentence.shape[<span class="number">1</span>]</span><br><span class="line">        bert_cls = bert_cls.unsqueeze(dim=<span class="number">1</span>).repeat(<span class="number">1</span>, sentence_len, <span class="number">1</span>)</span><br><span class="line">        bert_sentence = bert_sentence + bert_cls</span><br><span class="line">        <span class="comment"># 按照句子长度从大到小排序</span></span><br><span class="line">        bert_sentence, sorted_seq_lengths, desorted_indices = prepare_pack_padded_sequence(bert_sentence, seq_lens)</span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(bert_sentence, sorted_seq_lengths,</span><br><span class="line">                                                            batch_first=self.batch_first)</span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output = [ batch size,sent len, hidden_dim * bidirectional]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        batch_size, max_seq_len, hidden_dim = output.shape</span><br><span class="line">        hidden = torch.mean(torch.reshape(hidden, [batch_size, <span class="number">-1</span>, hidden_dim]), dim=<span class="number">1</span>)</span><br><span class="line">        output = torch.sum(output, dim=<span class="number">1</span>)</span><br><span class="line">        fc_input = self.dropout(output + hidden)</span><br><span class="line">        out = self.fc_rnn(fc_input)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h6 id="bert-rcnn">BertRCNN</h6>
<figure class="highlight python"><figcaption><span>BertRCNN</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertRCNN</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, bert_path, hidden_dim, n_layers, bidirectional, dropout, num_classes, word_embedding,</span></span></span><br><span class="line"><span class="function"><span class="params">                 trained, batch_first=True)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.bert.parameters():</span><br><span class="line">            param.requires_grad = trained</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                               hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.maxpool = nn.MaxPool1d()</span></span><br><span class="line">        self.fc = nn.Linear(hidden_dim * n_layers, num_classes)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, bert_masks, seq_lens)</span>:</span></span><br><span class="line">        <span class="comment"># text = [batch size,sent len]</span></span><br><span class="line">        <span class="comment"># context    输入的句子</span></span><br><span class="line">        <span class="comment"># mask  对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        bert_sentence, bert_cls = self.bert(text, attention_mask=bert_masks)</span><br><span class="line">        sentence_len = bert_sentence.shape[<span class="number">1</span>]</span><br><span class="line">        bert_cls = bert_cls.unsqueeze(dim=<span class="number">1</span>).repeat(<span class="number">1</span>, sentence_len, <span class="number">1</span>)</span><br><span class="line">        bert_sentence = bert_sentence + bert_cls</span><br><span class="line">        <span class="comment"># 按照句子长度从大到小排序</span></span><br><span class="line">        bert_sentence, sorted_seq_lengths, desorted_indices = prepare_pack_padded_sequence(bert_sentence, seq_lens)</span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(bert_sentence, sorted_seq_lengths,</span><br><span class="line">                                                            batch_first=self.batch_first)</span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        batch_size, max_seq_len, hidden_dim = output.shape</span><br><span class="line">        out = torch.transpose(output.relu(), <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        out = F.max_pool1d(out, max_seq_len).squeeze()</span><br><span class="line">        out = self.fc(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h6 id="xlnet">xlnet</h6>
<figure class="highlight python"><figcaption><span>xlnet</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XLNet</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, xlnet_path, num_classes, word_embedding, trained=True)</span>:</span></span><br><span class="line">        super(XLNet, self).__init__()</span><br><span class="line">        self.xlnet = XLNetModel.from_pretrained(xlnet_path)</span><br><span class="line">        <span class="comment"># 不对bert进行训练</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.xlnet.parameters():</span><br><span class="line">            param.requires_grad = trained</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(self.xlnet.d_model, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, context, xlnet_masks, seq_lens)</span>:</span></span><br><span class="line">        <span class="comment"># context  输入的句子序列</span></span><br><span class="line">        <span class="comment"># seq_len  句子长度</span></span><br><span class="line">        <span class="comment"># mask     对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        <span class="comment"># cls [batch_size, 768]</span></span><br><span class="line">        <span class="comment"># sentence [batch size,sen len,  768]</span></span><br><span class="line">        sentence_encoder = self.xlnet(context, attention_mask=xlnet_masks)</span><br><span class="line">        sentence_encoder = torch.sum(sentence_encoder[<span class="number">0</span>], dim=<span class="number">1</span>)</span><br><span class="line">        out = self.fc(sentence_encoder)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h4 id="bert-config-json">bert_config.json</h4>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "attention_probs_dropout_prob": 0.1, #乘法attention时，softmax后dropout概率 </span><br><span class="line">  "directionality": "bidi", </span><br><span class="line">  "hidden_act": "gelu",   #激活函数 </span><br><span class="line">  "hidden_dropout_prob": 0.1, #隐藏层dropout概率 </span><br><span class="line">  "hidden_size": 768, #隐藏单元数 </span><br><span class="line">  "initializer_range": 0.02, #初始化范围 </span><br><span class="line">  "intermediate_size": 3072, #升维维度</span><br><span class="line">  "max_position_embeddings": 512, #一个大于seq_length的参数，用于生成position_embedding</span><br><span class="line">  "num_attention_heads": 12,#每个隐藏层中的attention head数 </span><br><span class="line">  "num_hidden_layers": 2, #隐藏层数 </span><br><span class="line">  "pooler_fc_size": 768, </span><br><span class="line">  "pooler_num_attention_heads": 12, </span><br><span class="line">  "pooler_num_fc_layers": 3, </span><br><span class="line">  "pooler_size_per_head": 128, </span><br><span class="line">  "pooler_type": "first_token_transform", </span><br><span class="line">  "type_vocab_size": 2, #segment_ids类别 [0,1] </span><br><span class="line">  "vocab_size": 21128#词典中词数</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="dui-bi">对比</h1>
<h2 id="wei-bo-qing-gan-fen-lei">微博情感分类</h2>
<h3 id="shu-ju">数据</h3>
<p>weibo_senti_100k：共119988条数据，正例：59993,负例59995</p>
<p>句子最大长度：260，最小长度：3，平均长度：66.04</p>
<p>部分样例:</p>
<table>
<thead>
<tr>
<th>label</th>
<th>review</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]</td>
</tr>
<tr>
<td>1</td>
<td>@张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心，酒店都全部OK啦。</td>
</tr>
<tr>
<td>1</td>
<td>姑娘都羡慕你呢…还有招财猫高兴……//@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢//@李欣芸SharonLee:大佬范儿[书呆子]</td>
</tr>
<tr>
<td>1</td>
<td>美~~~~~[爱你]</td>
</tr>
<tr>
<td>1</td>
<td>梦想有多大，舞台就有多大![鼓掌]</td>
</tr>
<tr>
<td>0</td>
<td>今天真冷啊，难道又要穿棉袄了[晕]？今年的春天真的是百变莫测啊[抓狂]</td>
</tr>
<tr>
<td>0</td>
<td>[衰][衰][衰]像给剥了皮的蛇</td>
</tr>
<tr>
<td>0</td>
<td>酒驾的危害，这回是潜水艇。//@上海译文丁丽洁:[泪]</td>
</tr>
<tr>
<td>0</td>
<td>积压了这么多的枕边书，没一本看完了的，现在我读书的最佳地点尽然是公交车[晕]</td>
</tr>
<tr>
<td>0</td>
<td>[泪]错过了……</td>
</tr>
</tbody>
</table>
<p><a href="https://github.com/Embedding/Chinese-Word-Vectors" target="_blank" rel="noopener">词向量下载</a></p>
<p><a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">中文预训练bert模型下载</a></p>
<h3 id="fen-xi-ji-bi-jiao">分析及比较</h3>
<p>分成三类进行比较：</p>
<ol>
<li>FastText，TextCNN，DPCNN，RNN，RNN-Attention，TextRCNN(词向量/字向量)</li>
<li>RNN，LSTM，GRU，RNN-Attention</li>
<li>Bert，BertCNN，BertRNN，BertRCNN</li>
</ol>
<h4 id="fast-text-text-cnn-dpcnn-rnn-rnn-attention-text-rcnn">FastText，TextCNN，DPCNN，RNN，RNN-Attention，TextRCNN</h4>
<p>训练集上（词向量）的表现：</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/image-20200520140510805.png" alt></p>
<p>训练集上的速度：</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/image-20200520140848340.png" alt></p>
<p>验证集上的表现：</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/image-20200520135717244.png" alt></p>
<p>验证集上的速度</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/image-20200520140931559.png" alt="image-20200520140931559"></p>
<ol>
<li>从验证集上的表现来看，rcnn的表现比较稳定，0.985，但是其训练以及预测速度却是最慢的一个。FastText是速度最快的一个，在验证集上也能取得0.961的表现。可以根据任务的需求进行取舍。</li>
<li>对于FastText 来说，embedding的训练一定要打开。</li>
<li>对于过拟合现象，可以通过freeze word embedding 来缓解。</li>
<li>是使用字向量还是词向量？
<ul>
<li>对于FastText来说，使用词向量会比使用字向量精确度高出1%左右。原因就向上面FastText的缺点所述部分，使用词向量的时候会添加额外的<code>n-gram</code>信息。</li>
<li>对于TextCNN来说，网络自身就能够提取<code>n-gram</code>特征，如果再使用词向量，对于短文本来说，句子信息被压缩，容易出现过拟合现象（DPCNN同样出现过拟合）。在短文本的数据集上，TextCNN还是使用字向量比较好。</li>
<li>对于rnn来说，本身就存在梯度弥散和梯度爆炸的问题，所以使用词向量，使得句子序列会变长，会加剧这个问题。对于lstm来说也是同样的。</li>
<li>对于rcnn来说，使用词向量还是字向量基本没有任何区别。</li>
<li>对于加attention的rnn，每个时间步会attention到整个序列的word embedding，所以词向量或者字向量带来的影响并不明显。</li>
</ul>
</li>
</ol>
<h4 id="rnn-lstm-gru">RNN，LSTM，GRU</h4>
<p>训练集和验证集上的表现：</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/image-20200520144027325.png" alt></p>
<p>速度比较：</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/image-20200520144139492.png" alt></p>
<ol>
<li>速度上相差无几，能用lstm就用lstm把。</li>
<li>不要仅仅使用rnn最后输出的hidden来做分类。（如果只使用hidden来做分类，准确度50%.）</li>
<li>是使用sum求和来获取整句话的语义还是使用mean来获取整句话的语义其实影响不大。</li>
<li>在rnn上使用attention 精度上会略有提升，但是相比于速度的下降，感觉有些得不偿失，如果追求精度可以加上attention。</li>
</ol>
<h4 id="bert-bert-cnn-bert-rnn-bert-rcnn">Bert，BertCNN，BertRNN，BertRCNN</h4>
<p><img src="/2020/03/29/nature_language_process/text_classification/image-20200521150254593.png" alt></p>
<ol>
<li>通常来说bert的模型的train不用打开，如果打开，在bert后面接的层的学习率应大于bert学习率一两个数量级，使得后面的层得到充分的训练。</li>
<li>bert模型本身就可以达到98.3%左右的精确度，在后面添加其他模型看不出效果。</li>
</ol>
<h2 id="cnews-xin-wen-shi-fen-lei-jie-guo-bi-jiao">Cnews新闻十分类结果比较</h2>
<h3 id="shu-ju-1">数据</h3>
<p>类别：‘体育’ ‘娱乐’ ‘家居’ ‘房产’ ‘教育’ ‘时尚’ ‘时政’ ‘游戏’ ‘科技’ ‘财经’<br>
训练集：50000 条数据，最大长度：27467，最小长度：8，类别个数：10,平均长度：913.31<br>
验证集：5000 条数据，最大长度：10919，最小长度：15，类别个数：10<br>
测试集：10000 条数据，最大长度：14720，最小长度：13，类别个数：10</p>
<p>使用数据集<a href="https://pan.baidu.com/s/1OOTK374IZ1DHnz5COUcfbw" target="_blank" rel="noopener">cnews</a>  密码:hf6o</p>
<p>训练集部分样例及每个类别的统计：</p>
<table>
<thead>
<tr>
<th>label</th>
<th>text</th>
<th>数量</th>
</tr>
</thead>
<tbody>
<tr>
<td>体育</td>
<td>黄蜂vs湖人首发：科比带伤战保罗 加索尔救赎之战 新浪体育讯北京时间4月27日，NBA季后赛首轮洛杉矶湖人主场迎战新奥尔良黄蜂，此前的比赛中，双方战成2-2平，因此本场比赛对于两支球队来说都非常重要，赛前双方也公布了首发阵容：湖人队：费舍尔、科比、阿泰斯特、加索尔、拜纳姆黄蜂队：保罗、贝里内利、阿里扎、兰德里、奥卡福[新浪NBA官方微博][新浪NBA湖人新闻动态微博][新浪NBA专题]<a href="%E6%96%B0%E6%B5%AA%E4%BD%93%E8%82%B2">黄蜂vs湖人图文直播室</a></td>
<td>5000</td>
</tr>
<tr>
<td>娱乐</td>
<td>皮克斯首部3D动画《飞屋历险记》预告发布(图)视频：动画片《飞屋历险记》先行版43秒预告新浪娱乐讯 迪士尼、皮克斯2009暑期3D动画力作《飞屋历险记》(Up)发布预告片，虽然这款预告片仅有43秒，并且只出现了被汽球吊起来的房屋，但门前老爷爷卡尔的一声“下午好”着实让人忍俊不禁。该片由《怪兽电力公司》导演彼特·道格特(Pete Docter)执导，曾在《海底总动员》、《料理鼠王》担任编剧的皮克斯老班底鲍勃-派特森(Bob Peterson)亦将在本片担任共同导演，献出自己的导演处女作。《飞屋历险记》同时会是皮克斯有史以来第一部以3-D电影呈现的里程碑作品，之后皮克斯的所有影片都将制作成立体电影。《飞屋历险记》讲述了一老一少的冒险旅程。78岁的老翁卡尔·弗雷德里克森(Carl Fredricksen)一生中都梦想着能环游世界、出没于异境险地体验，却平淡地渡过了一生。在他生活的最后阶段，卡而仿佛在命运的安排下，带着8岁的亚裔小鬼头Russell一同踏上了冒险的旅程。这对一老一小的奇特组合肩并肩闯荡江湖，共同经历了荒野跋涉、丛林野兽与反派坏蛋的狙击。田野/文</td>
<td>5000</td>
</tr>
<tr>
<td>家居</td>
<td>橱柜价格关键在计价方式 教你如何挑选买过橱柜的人都知道，橱柜的计价很复杂，商家的报价方式也不尽相同，那么哪种计价方式对消费者更有利？在计价过程中应该注意哪些问题？消费者在购买橱柜之前一定要了解清楚。 橱柜的主要计价方式——延米计价和单元柜体计价 现在市场上橱柜主要有两种计价方式——延米计价和单元柜体计价。 延米计价是指地柜和吊柜各一米的总价(有些还包含台面)。在此基础上，如果有局部区域只要地柜不要吊柜，就会按就会按“2/8”或“4/6”的比例折算。如某橱柜材料的延米价为2000元/延米，某顾客做2米的吊柜、4米的地柜，则吊柜价=2000X0.4X2=1600元，地柜价=2000X0.6X4=4800元(此吊柜、地柜价按4/6的比例计算)，再加上所选台面、配件、电器等附加费用即为整套橱柜的价格。 延米报价有许多不合理之处，水槽、燃气灶、嵌入式电器等部分所需门板很少，但仍按延米来算价，对消费者来说很不划算。例如一款1000元/延米的橱柜，一个水槽约0.8米长，但消费者还是要按1000元的单价乘以0.8米付费，这个实际上只是几块材料简单组合的水槽柜需要消费者花800元，而同样材质、同样大小的水槽柜仅需400元左右，二者价格相差数百元。 按延米计价，所有的配件费用都是在原有的基础上增加，虽然有些厂家宣称抽屉、拉篮不加钱，但其实那是最基本的配置，一旦顾客要求调整方案，就会要多加钱，此外不足一米的部分要按一米计价，因此对顾客来说，如此计价会多花不少冤枉钱。 “单元柜体计价”是国际惯例的橱柜计价方式，是按每一个组成厨柜的单元柜价格进行计算，然后加出总价。具体为：某吊柜单价×个数+某地柜单价×个数……。利用单元柜体计价，更为合理。举个例子说，外观相同的柜体，抽屉数量、五金件、托架数量如果不同，在以延米计价时，商家往往只给消费者最简单、最省成本的产品。而按单元柜体计价，一款尺寸相同的抽屉柜可按不同配置报出不同价格：同样是一款30cm宽、66cm高的单体柜，如果门改成弧型，是多少钱；如果抽屉里加上可拆装的金属篮，是多少钱；如果抽屉的侧板是木质的多少钱……把橱柜的每个细节都分解开来，消费者可以在预算之内把可有可无的配置省掉，把钱花在自己更需要的功能上。 两全其美报价方式——延米计价和单元柜体计价相结合 现在中国橱柜市场上仍普遍采用延米计价，但进口品牌及国内一些大品牌橱柜都采用单元柜体计价方式，如德宝·西克曼、海尔等品牌即是采用单元柜体计价方式。不过德宝·西克曼厨柜的工作人员介绍到，如果一开始就用单元柜体计价来进行报价，不够直观，同时为了便于顾客进行比较，他们会用延米计价给顾客所选定的材料进行一个初始报价，让顾客对自己的厨房装修要花多少钱心里大概有个底。在对厨房进行量尺后，设计师会按照顾客的需求，设计出厨房效果图。这时，销售人员会按单元柜体计价给顾客进行一个报价。对于每一种标准柜体都有相应的报价，顾客实际用到几组柜子，将这些柜子价格累加，再加上台面及其他相关费用，便是整个橱柜的价格。</td>
<td>5000</td>
</tr>
<tr>
<td>房产</td>
<td>冯仑：继续增持高GDP城市商业地产确立商业地产投资战略不久的万通地产(企业专区,旗下楼盘)(600246)，今年上半年遭遇了业绩下滑。公司昨日公布的半年报显示，其商业物业在报告期内实现的营业收入同比下降33.71%，营业利润率比上年同期下降47.29个百分点。不过，公司董事长冯仑日前表示，依然看好人均GDP8000美元以上城市的商业地产，万通将继续增加高GDP城市的商业地产；计划用5-10年，商业物业收入占比达到30%-50%。逆向运作地产投资冯仑指出，根据历史经验，GDP的增长、城市化的增长，和房地产物业形态有一定关系，即人均GDP在8000美元以下时，住宅是市场的核心，主流产品都将围绕住宅展开。目前，在中国的城市中，人均GDP8000美元的城市大约有十个，大部分省会城市依然在3000美元至5000美元之间，因此，未来5-10年，中国房地产市场的产品结构仍然是以住宅为主。 冯仑认为，万通地产从现在开始扩大商业地产的比重，在目前的市场中，是一种逆向运作的思维，但符合长期趋势。他指出，在人均GDP达到8000美元的经济实体中，商用不动产会成为地产业的主角。以美国为例，商业地产的市场规模大约是住宅的两倍。中国商业地产未来的市场空间很大。根据万通地产的发展战略，除了在环渤海区域内发展住宅以外，还会重点发展商业不动产。未来，公司业务结构将逐步调整，商用地产的收入会逐年增加；今后，公司商业物业收入将占到整体营业收入的一半左右。对于目前商业地产面临的不景气局面，万通地产董事会秘书程晓?指出，公司战略不会因市场的短期波动而改变，公司将继续加大商用物业项目的投资力度，以营运带动开发，以财务安排的多样化实施商用物业投资。改变商业模式冯仑表示，就房地产开发模式而言，过去两百年主要经历了三次变化，即从“地主加工头”到“厂长加资本家”，再到“导演加制片”。目前，国内多数地产商的开发模式属于“地主加工头”和“厂长加资本家”的阶段；而商业地产的开发模式，不能停留在这两个阶段。所谓“导演加制片”模式，即由专业的房地产投资和资产管理公司负责运营商业地产项目，实现收入的多元化。而这种模式需要相应的金融创新产品支持。业内人士指出，房地产金融领域内的REITS、抵押贷款等金融产品体系的完善，将支持商用地产在一个多元化的不动产经营环境中快速的成长。而商业模式的改变需要较长一段时间。数据显示，香港主流房地产企业在人均GDP10000美元的时候开始逐步发展商业地产，先后经过13-15年确立起新的商业模式。其中，长江实业经过13年的发展，商业地产在业务机构的比重占到30%，新鸿基则经过15年的调整，商业地产比重占到50%。SOHO中国(企业专区,旗下楼盘)董事长潘石屹也指出，现在的市场虽然在调整，不过也给从事商业地产开发的企业提供了良好机会和平台，应及时在地域、开发物业的品种、品牌的建设、销售和持有物业的比重四个方面做出调整。 我要评论</td>
<td>5000</td>
</tr>
<tr>
<td>教育</td>
<td>2010年6月英语六级考试考后难度调查2010年6月大学英语六级考试将于19日下午举行，欢迎各位考生在考试后参加难度调查，发表你对这次考试的看法。点击进入论坛，参与考后大讨论</td>
<td>5000</td>
</tr>
<tr>
<td>时尚</td>
<td>组图：萝莉潮人示范春季复古实穿导语：萝莉潮人示范春季复古实穿，在乍暖还寒的初春，有的甜美、有的优雅、有的性感，但无论是哪种风格都给人强烈的视觉冲击力，在这个缤纷的春季更加脱俗动人。</td>
<td>5000</td>
</tr>
<tr>
<td>时政</td>
<td>香港特区行政长官曾荫权将离港休假中新社香港八月七日电 香港特区行政长官曾荫权将于八月九日至十五日离港休假。特区政府发言人七日透露，曾荫权离港期间，八月九日由特区财政司司长曾俊华署理行政长官职务；八月十日至十五日由政务司司长唐英年署理行政长官职务。(完)</td>
<td>5000</td>
</tr>
<tr>
<td>游戏</td>
<td>全国人大常委会将对59件法律相关条文作出修改新华社快讯：全国人大常委会27日表决通过了关于修改部分法律的决定，对59件法律的相关条文作出修改。</td>
<td>5000</td>
</tr>
<tr>
<td>科技</td>
<td>入门级时尚卡片机 尼康S220套装仅1150尼康S220延续了S系列纤巧超薄的机身设计，采用铝合金材质打造，表面质地细腻，不易沾染指纹。S220拥有紫色、深蓝、洋红、水晶绿和柔银五款靓丽颜色可供选择。</td>
<td>5000</td>
</tr>
</tbody>
</table>
<h3 id="fen-xi-bi-jiao">分析比较</h3>
<p>训练集上的表现（字向量）（序列长度：2000）：</p>
<p><img src="/2020/03/29/nature_language_process/text_classification/image-20200526204848210.png" alt="各种模型"></p>
<p><img src="/2020/03/29/nature_language_process/text_classification/image-20200526205351509.png" alt="rnn/cnn-attention/fast_text"></p>
<p><img src="/2020/03/29/nature_language_process/text_classification/image-20200526205849399.png" alt="speed"></p>
<p><img src="/2020/03/29/nature_language_process/text_classification/image-20200526210508245.png" alt="蓝色为字向量/绿色为词向量"></p>
<ol>
<li>rnn在面对长文本时直接崩溃了（感觉自己说了一个废话）。再给rnn加上attention之后，rnn得到了救赎，但是效果和FastText基本持平。由此推断lstm+attention 能够获得一个相对较好的结果（如果不考虑速度的话）。</li>
<li>从speed图来看，最快的当然是FastText，可以并行的CNN处于第二梯队，最后的是RNN系列的模型。</li>
<li>词向量还是字向量：
<ol>
<li>对于FastText来说，在长文本上，词向量的表现要远远好于字向量。</li>
<li>可能是文本长度过长了吧，实验结果表明：在长文本数据上，词向量的表现要好于字向量，对于长文本分类来说，整句话的语义要高于某些特定的词吧。</li>
<li>对于rnn系列来说，毫无疑问，词向量的表现远好于字向量，因为句子序列长度变短了。</li>
<li>rcnn模型一般来说不会比rnn，或者cnn表现差。</li>
</ol>
</li>
<li>xlnet由于显存的原因，序列长度只取到1500（只要你显存够，在一定意义上来说是解决了bert的长度限制的问题）。但是在11g单卡上，batch_size=2，跑完一步，FastText可以跑一个epoch。不知道知识蒸馏效果会怎样。</li>
</ol>
<h2 id="wen-ben-duo-biao-qian-fen-lei">文本多标签分类</h2>
<p>（先挖一个坑）</p>
<h2 id="fen-xi">分析</h2>
<h4 id="chang-duan-wen-ben-fen-lei-de-bi-jiao">长短文本分类的比较</h4>
<p>对于词嵌入技术的文本表示，短文本和长文本表示上没有差别，此时分类效果的优劣主要在分类模型和训练数据上。</p>
<p>对于数据而言：随着文本越长，语义的重要性就越高，在文本很短的情况下，语义的重要性就很小，比如：“今天 天气 怎么样”，“今天 怎么样 天气”，“怎么样 天气 今天”。你甚至不必要考虑句子是否通顺，基本上可以当一句话处理，没有第二个意思。但是随着文本越来越长，比如512个字符，颠倒一下可能就要归为两类了。</p>
<p>对于模型而言：对于短文本，CNN配合Max-pooling池化(如TextCNN模型)速度快，而且效果也很好。因为短文本上的关键词比较容易找到，而且Max-pooling会直接过滤掉模型认为不重要特征。具体工作机制是：卷积窗口沿着长度为n的文本一个个滑动，类似于n-gram机制对文本切词，然后和文本中的每个词进行相似度计算，因为后面接了个Max-pooling，因此只会保留和卷积核最相近的词。微博数据集属于情感分类，为了判断句子的情感极性，只需要让分类器能识别出“不开心”这类词是个负极性的词，“高兴”、“开心”等这类词是正极性的词，其他词是偏中性词就可以了。因此，当把该句子中的各个词条输入给模型去分类时，并不需要去“瞻前顾后”，因此使用一个关注局部的前馈神经网络往往表现更佳。虽然Attention也突出了重点特征，但是难以过滤掉所有低分特征。但是对于长文本直接用CNN就不行了，TextCNN会比HAN模型泛化能力差很多。<strong>如果在TextCNN前加一层LSTM，这样效果可以提升很大</strong>。</p>
<h4 id="wei-shi-yao-chang-wen-ben-fen-lei-de-shi-yan-zhong-cnn-he-rnn-mei-you-la-kai-chai-ju">为什么长文本分类的实验中，cnn 和 rnn 没有拉开差距？</h4>
<p>cnn和rnn的精度都很高，分析主要还是分类的文章规则性比较强，且属于特定领域，词量不多，类别差异可能比较明显。</p>
<h1 id="wen-ben-fen-lei-tricks">文本分类tricks</h1>
<h2 id="fen-ci-qi">分词器</h2>
<p><strong>分词器所分出的词与词向量表中的token粒度match是更重要的事情</strong></p>
<h2 id="yi-zhi-yu-xun-lian-ci-xiang-liang-de-fen-ci-qi">已知预训练词向量的分词器</h2>
<p>像word2vec、glove、fasttext这些官方release的预训练词向量都会公布相应训练语料的信息，包括预处理策略如分词，这种情况下直接使用官方的训练该词向量所使用的分词器，此分词器在下游任务的表现十之八九会比其他花里胡哨的分词器好用。</p>
<h2 id="bu-zhi-dao-yu-xun-lian-ci-xiang-liang-de-fen-ci-qi">不知道预训练词向量的分词器</h2>
<p>这时就需要去“猜”一下分词器。怎么猜呢？<br>
首先，拿到预训练词向量表后，去里面search一些特定词汇比如一些网站、邮箱、成语、人名等，英文里还有n’t等，看看训练词向量使用的分词器是把它们分成什么粒度。<br>
然后跑几个分词器，看看哪个分词器的粒度跟他最接近就用哪个，如果不放心，就放到下游任务里跑跑看。</p>
<p>最理想的情况是：先确定最适合当前任务数据集的分词器，再使用同分词器产出的预训练词向量。如果无法满足理想情况，则需要自己在下游任务训练集或者大量同分布无监督语料上训练的词向量更有利于进一步压榨模型的性能。</p>
<h2 id="guan-yu-zhong-wen-zi-xiang-liang">关于中文字向量</h2>
<p>预训练中文字向量的时候，把窗口开大一些，不要直接使用word-level的窗口大小，效果会比随机初始化的字向量明显的好。</p>
<h2 id="shu-ju-ji-zao-sheng-hen-yan-zhong">数据集噪声很严重</h2>
<p>里噪声严重有两种情况。对于数据集D(X, Y)，一种是X内部噪声很大（比如文本为口语化表述或由互联网用户生成），一种是Y的噪声很大（一些样本被明显的错误标注，一些样本人也很难定义是属于哪一类，甚至具备类别二义性）。</p>
<h2 id="x-nei-bu-zao-sheng-hen-da">X内部噪声很大</h2>
<p>法一：直接将模型的输入变成char-level（中文中就是字的粒度），然后train from scratch（不使用预训练词向量）去跟word-level的对比一下，如果char-level的明显的效果好，那么短时间之内就直接基于char-level去做模型。</p>
<p>法二：使用特殊超参的FastText去训练一份词向量：<br>
一般来说fasttext在英文中的char ngram的窗口大小一般取值3～6，但是在处理中文时，如果我们的目的是为了去除输入中的噪声，那么我们可以把这个窗口限制为1～2，这种小窗口有利于模型去捕获错别字（比如，我们打一个错误词的时候，一般都是将其中的一个字达成同音异形的另一个字），比如word2vec学出来的“似乎”的最近词可能是“好像”，然而小ngram窗口fasttext学出来的“似乎”最近词则很有可能是“是乎”等内部包含错别字的词，这样就一下子让不太过分的错别字构成的词们又重新回到了一起，甚至可以一定程度上对抗分词器产生的噪声（把一个词切分成多个字）。</p>
<h2 id="y-de-zao-sheng-hen-da">Y的噪声很大</h2>
<p>首先忽略这个噪声，强行的把模型尽可能好的训出来。然后让训练好的模型去跑训练集和开发集，取出训练集中的错误样本和开发集中那些以很高的置信度做出错误决策的样本（比如以99%的把握把一个标签为0的样本预测为1），然后去做这些bad cases的分析，如果发现错误标注有很强的规律性，则直接撸一个脚本批量纠正一下（只要确保纠正后的标注正确率比纠正前明显高就行）。<br>
如果没有什么规律，但是发现模型高置信度做错的这些样本大部分都是标注错误的话，就直接把这些样本都删掉，常常也可以换来性能的小幅提升，毕竟测试集都是人工标注的，困难样本和错标样本不会太多。</p>
<h2 id="baseline-xuan-yong-cnn-huan-shi-rnn">baseline选用CNN还是RNN？</h2>
<p>看数据集，如果感觉数据集里很多很强的ngram可以直接帮助生成正确决策，那就CNN。<br>
如果感觉很多case都是那种需要把一个句子看完甚至看两三遍才容易得出正确tag，那就RNN。<br>
还可以CNN、RNN的模型都跑出来简单集成一下。</p>
<h2 id="dropout-jia-zai-na-li">Dropout加在哪里</h2>
<p>word embedding层后、pooling层后、FC层（全联接层）后。</p>
<h2 id="er-fen-lei">二分类</h2>
<p>二分类问题不一定要用sigmoid作为输出层的激活函数，尝试一下包含俩类别的softmax。可能多一条分支就多一点信息，实践中常常带来零点几个点的提升。</p>
<h2 id="yang-ben-lei-bie-bu-jun-heng-wen-ti">样本类别不均衡问题</h2>
<p>如果正负样本比小于9:1的话，继续做深度模型调超参，决策阈值也完全不用手调。但是，如果经常一个batch中完全就是同一个类别的样本，或者一些类别的样本经过好多batch都难遇到一个的话，均衡就非常非常有必要了。</p>
<h2 id="zui-hou">最后</h2>
<ol>
<li>别太纠结文本截断长度使用120还是150</li>
<li>别太纠结对性能不敏感的超参数带来的开发集性能的微小提升</li>
<li>别太纠结未登陆词的embedding是初始化成全0还是随机初始化，别跟PAD共享embedding就行</li>
<li>别太纠结优化器用Adam还是MomentumSGD，如果跟SGD的感情还不深，无脑Adam，最后再用MomentumSGD跑几遍</li>
<li>还是不会用tricks但是就是想跑出个好结果，bert大力出奇迹。</li>
</ol>
<h1 id="zong-jie">总结</h1>
<p>复杂的模型未必会有很好的结果，简单模型效果未必不理想，没必要一味追求深度学习、复杂模型什么的。选什么样的模型还是要根据数据来的。同一类问题，不同的数据效果差异很大，不要小看任何一类问题，例如分类，我们通常觉得它很简单，但有些数据并非你所想。</p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://blog.csdn.net/feilong_csdn/article/details/88655927" target="_blank" rel="noopener">fastText原理和文本分类实战，看这一篇就够了</a></li>
<li><a href="https://blog.csdn.net/asialee_bird/article/details/88813385#%E4%B8%80%E3%80%81%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0" target="_blank" rel="noopener">TextCNN文本分类（keras实现）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/55263066" target="_blank" rel="noopener">浅谈基于深度学习的文本分类问题</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2018-06-22-4" target="_blank" rel="noopener">从DPCNN出发，撩一下深层word-level文本分类模型</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2019-01-24-5" target="_blank" rel="noopener">文本分类有哪些论文中很少提及却对性能有重要影响的tricks？</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247484682&amp;idx=1&amp;sn=51520138eed826ec891ac8154ee550f9&amp;chksm=970c2ddca07ba4ca33ee14542cff0457601bb16236f8edc1ff0e617051d1f063354dda0f8893&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">从前馈到反馈：解析循环神经网络（RNN）及其tricks</a></li>
<li>[<a href="http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%81%9A%E5%A5%BD%E9%95%BF%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%B3%95%E5%BE%8B%E6%96%87%E4%B9%A6%E6%99%BA%E8%83%BD%E5%8C%96" target="_blank" rel="noopener">达观数据曾彦能：如何用深度学习做好长文本分类与法律文书智能化处理</a>](<a href="http://www.52nlp.cn/tag/%E9%95%BF%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB" target="_blank" rel="noopener">http://www.52nlp.cn/tag/长文本分类</a>)</li>
<li><a href="https://www.zhihu.com/question/326770917/answer/698646465" target="_blank" rel="noopener">短文本分类和长文本分类的模型如何进行选择？</a></li>
<li><a href="https://www.pianshen.com/article/4319299677/" target="_blank" rel="noopener">NLP实践九：HAN原理与文本分类实践</a></li>
<li><a href="https://www.jianshu.com/p/56061b8f463a" target="_blank" rel="noopener">NLP之文本分类</a></li>
</ol>

    </div>

    
    
    <div>
  
    <div style="text-align:center;color:#bfbfbf;font-size:16px;">
      <span>-------- 本文结束 </span>
      <i class="fa fa-coffee"></i>
      <span> 感谢阅读 --------</span>
    </div>
  
</div>
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    鼓励一下
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat.png" alt="Li Zhen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/ali.png" alt="Li Zhen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/FastText/" rel="tag"><i class="fa fa-tag"></i> FastText</a>
              <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" rel="tag"><i class="fa fa-tag"></i> 自然语言处理</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/03/28/machine_learning/data_preprocessing/" rel="prev" title="数据预处理">
      <i class="fa fa-chevron-left"></i> 数据预处理
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/04/01/machine_learning/exploratory_data_analysis/" rel="next" title="数据探索性分析(EDA)">
      数据探索性分析(EDA) <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#xiang-mu-jie-shao"><span class="nav-number">1.</span> <span class="nav-text">项目介绍</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#jian-jie"><span class="nav-number">2.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#fang-fa"><span class="nav-number">3.</span> <span class="nav-text">方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ji-yu-ji-qi-xue-xi-de-wen-ben-fen-lei"><span class="nav-number">3.1.</span> <span class="nav-text">基于机器学习的文本分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#te-zheng-gong-cheng"><span class="nav-number">3.1.1.</span> <span class="nav-text">特征工程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#wen-ben-yu-chu-li"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">文本预处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#te-zheng-ti-qu"><span class="nav-number">3.1.1.2.</span> <span class="nav-text">特征提取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#wen-ben-biao-shi"><span class="nav-number">3.1.1.3.</span> <span class="nav-text">文本表示</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ci-dai-fa"><span class="nav-number">3.1.1.3.1.</span> <span class="nav-text">词袋法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#n-gram-ci-dai-mo-xing"><span class="nav-number">3.1.1.3.2.</span> <span class="nav-text">n-gram 词袋模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#xiang-liang-kong-jian-mo-xing"><span class="nav-number">3.1.1.3.3.</span> <span class="nav-text">向量空间模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fen-lei-qi"><span class="nav-number">3.1.2.</span> <span class="nav-text">分类器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ji-yu-shen-du-xue-xi-de-wen-ben-fen-lei"><span class="nav-number">3.2.</span> <span class="nav-text">基于深度学习的文本分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#fast-text"><span class="nav-number">3.2.1.</span> <span class="nav-text">FastText</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#jian-jie-1"><span class="nav-number">3.2.1.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#fast-text-mo-xing-jia-gou"><span class="nav-number">3.2.1.2.</span> <span class="nav-text">fastText模型架构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#que-dian"><span class="nav-number">3.2.1.3.</span> <span class="nav-text">缺点：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mo-xing-dai-ma"><span class="nav-number">3.2.1.4.</span> <span class="nav-text">模型代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#text-cnn"><span class="nav-number">3.2.2.</span> <span class="nav-text">TextCNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#jian-jie-2"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#wang-luo-jie-gou"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">网络结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#yuan-li"><span class="nav-number">3.2.2.3.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#que-dian-1"><span class="nav-number">3.2.2.4.</span> <span class="nav-text">缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dai-ma"><span class="nav-number">3.2.2.5.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dpcnn"><span class="nav-number">3.2.3.</span> <span class="nav-text">DPCNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#jian-jie-3"><span class="nav-number">3.2.3.1.</span> <span class="nav-text">简介：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#wang-luo-jie-gou-1"><span class="nav-number">3.2.3.2.</span> <span class="nav-text">网络结构</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#region-embedding"><span class="nav-number">3.2.3.2.1.</span> <span class="nav-text">Region embedding</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#juan-ji-he-quan-lian-jie-de-quan-heng"><span class="nav-number">3.2.3.3.</span> <span class="nav-text">卷积和全连接的权衡</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#deng-chang-juan-ji"><span class="nav-number">3.2.3.3.1.</span> <span class="nav-text">等长卷积</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#gu-ding-feature-map-de-shu-liang"><span class="nav-number">3.2.3.3.2.</span> <span class="nav-text">固定feature map的数量</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#strong-chi-hua-strong"><span class="nav-number">3.2.3.3.3.</span> <span class="nav-text">池化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#can-chai-lian-jie"><span class="nav-number">3.2.3.3.4.</span> <span class="nav-text">残差连接</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dai-ma-1"><span class="nav-number">3.2.3.4.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rnn-xi-lie"><span class="nav-number">3.2.4.</span> <span class="nav-text">RNN系列</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#rnn"><span class="nav-number">3.2.4.1.</span> <span class="nav-text">RNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lstm"><span class="nav-number">3.2.4.2.</span> <span class="nav-text">LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#strong-yi-wang-men-strong"><span class="nav-number">3.2.4.2.1.</span> <span class="nav-text">遗忘门</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#strong-shu-ru-men-strong"><span class="nav-number">3.2.4.2.2.</span> <span class="nav-text">输入门</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#shu-chu-men"><span class="nav-number">3.2.4.2.3.</span> <span class="nav-text">输出门</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#gong-shi"><span class="nav-number">3.2.4.2.4.</span> <span class="nav-text">公式</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#gru"><span class="nav-number">3.2.4.3.</span> <span class="nav-text">GRU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dai-ma-2"><span class="nav-number">3.2.4.4.</span> <span class="nav-text">代码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#self-attention"><span class="nav-number">3.2.4.5.</span> <span class="nav-text">Self-Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#rnn-attenton"><span class="nav-number">3.2.4.6.</span> <span class="nav-text">Rnn-Attenton</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#text-rcnn"><span class="nav-number">3.2.5.</span> <span class="nav-text">TextRCNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#jian-jie-4"><span class="nav-number">3.2.5.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mo-xing-jie-gou"><span class="nav-number">3.2.5.2.</span> <span class="nav-text">模型结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#word-representation-learning"><span class="nav-number">3.2.5.3.</span> <span class="nav-text">Word Representation Learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#text-representation-learning"><span class="nav-number">3.2.5.4.</span> <span class="nav-text">Text Representation Learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dai-ma-3"><span class="nav-number">3.2.5.5.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#han"><span class="nav-number">3.2.6.</span> <span class="nav-text">HAN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ci-xu-lie-bian-ma-qi"><span class="nav-number">3.2.6.1.</span> <span class="nav-text">词序列编码器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ci-ji-de-zhu-yi-li-ceng"><span class="nav-number">3.2.6.2.</span> <span class="nav-text">词级的注意力层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ju-zi-bian-ma-qi"><span class="nav-number">3.2.6.3.</span> <span class="nav-text">句子编码器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ju-zi-ji-zhu-yi-li-ceng"><span class="nav-number">3.2.6.4.</span> <span class="nav-text">句子级注意力层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#fen-lei"><span class="nav-number">3.2.6.5.</span> <span class="nav-text">分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dai-ma-4"><span class="nav-number">3.2.6.6.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bert"><span class="nav-number">3.2.7.</span> <span class="nav-text">Bert</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#bert-1"><span class="nav-number">3.2.7.1.</span> <span class="nav-text">BERT</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#task-1-mlm"><span class="nav-number">3.2.7.1.1.</span> <span class="nav-text">Task 1: MLM</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#task-2-nsp"><span class="nav-number">3.2.7.1.2.</span> <span class="nav-text">Task 2: NSP</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#shu-ru"><span class="nav-number">3.2.7.1.3.</span> <span class="nav-text">输入</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#fine-tunninng"><span class="nav-number">3.2.7.1.4.</span> <span class="nav-text">Fine-tunninng</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#que-dian-2"><span class="nav-number">3.2.7.1.5.</span> <span class="nav-text">缺点</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#dai-ma-5"><span class="nav-number">3.2.7.1.6.</span> <span class="nav-text">代码</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#bert-2"><span class="nav-number">3.2.7.1.6.1.</span> <span class="nav-text">Bert</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#bert-cnn"><span class="nav-number">3.2.7.1.6.2.</span> <span class="nav-text">BertCNN</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#bert-rnn"><span class="nav-number">3.2.7.1.6.3.</span> <span class="nav-text">BertRNN</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#bert-rcnn"><span class="nav-number">3.2.7.1.6.4.</span> <span class="nav-text">BertRCNN</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#xlnet"><span class="nav-number">3.2.7.1.6.5.</span> <span class="nav-text">xlnet</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bert-config-json"><span class="nav-number">3.2.7.2.</span> <span class="nav-text">bert_config.json</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dui-bi"><span class="nav-number">4.</span> <span class="nav-text">对比</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#wei-bo-qing-gan-fen-lei"><span class="nav-number">4.1.</span> <span class="nav-text">微博情感分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#shu-ju"><span class="nav-number">4.1.1.</span> <span class="nav-text">数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fen-xi-ji-bi-jiao"><span class="nav-number">4.1.2.</span> <span class="nav-text">分析及比较</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#fast-text-text-cnn-dpcnn-rnn-rnn-attention-text-rcnn"><span class="nav-number">4.1.2.1.</span> <span class="nav-text">FastText，TextCNN，DPCNN，RNN，RNN-Attention，TextRCNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#rnn-lstm-gru"><span class="nav-number">4.1.2.2.</span> <span class="nav-text">RNN，LSTM，GRU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bert-bert-cnn-bert-rnn-bert-rcnn"><span class="nav-number">4.1.2.3.</span> <span class="nav-text">Bert，BertCNN，BertRNN，BertRCNN</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cnews-xin-wen-shi-fen-lei-jie-guo-bi-jiao"><span class="nav-number">4.2.</span> <span class="nav-text">Cnews新闻十分类结果比较</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#shu-ju-1"><span class="nav-number">4.2.1.</span> <span class="nav-text">数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fen-xi-bi-jiao"><span class="nav-number">4.2.2.</span> <span class="nav-text">分析比较</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#wen-ben-duo-biao-qian-fen-lei"><span class="nav-number">4.3.</span> <span class="nav-text">文本多标签分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fen-xi"><span class="nav-number">4.4.</span> <span class="nav-text">分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#chang-duan-wen-ben-fen-lei-de-bi-jiao"><span class="nav-number">4.4.0.1.</span> <span class="nav-text">长短文本分类的比较</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#wei-shi-yao-chang-wen-ben-fen-lei-de-shi-yan-zhong-cnn-he-rnn-mei-you-la-kai-chai-ju"><span class="nav-number">4.4.0.2.</span> <span class="nav-text">为什么长文本分类的实验中，cnn 和 rnn 没有拉开差距？</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#wen-ben-fen-lei-tricks"><span class="nav-number">5.</span> <span class="nav-text">文本分类tricks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#fen-ci-qi"><span class="nav-number">5.1.</span> <span class="nav-text">分词器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#yi-zhi-yu-xun-lian-ci-xiang-liang-de-fen-ci-qi"><span class="nav-number">5.2.</span> <span class="nav-text">已知预训练词向量的分词器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bu-zhi-dao-yu-xun-lian-ci-xiang-liang-de-fen-ci-qi"><span class="nav-number">5.3.</span> <span class="nav-text">不知道预训练词向量的分词器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#guan-yu-zhong-wen-zi-xiang-liang"><span class="nav-number">5.4.</span> <span class="nav-text">关于中文字向量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shu-ju-ji-zao-sheng-hen-yan-zhong"><span class="nav-number">5.5.</span> <span class="nav-text">数据集噪声很严重</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#x-nei-bu-zao-sheng-hen-da"><span class="nav-number">5.6.</span> <span class="nav-text">X内部噪声很大</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#y-de-zao-sheng-hen-da"><span class="nav-number">5.7.</span> <span class="nav-text">Y的噪声很大</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#baseline-xuan-yong-cnn-huan-shi-rnn"><span class="nav-number">5.8.</span> <span class="nav-text">baseline选用CNN还是RNN？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dropout-jia-zai-na-li"><span class="nav-number">5.9.</span> <span class="nav-text">Dropout加在哪里</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#er-fen-lei"><span class="nav-number">5.10.</span> <span class="nav-text">二分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#yang-ben-lei-bie-bu-jun-heng-wen-ti"><span class="nav-number">5.11.</span> <span class="nav-text">样本类别不均衡问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#zui-hou"><span class="nav-number">5.12.</span> <span class="nav-text">最后</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#zong-jie"><span class="nav-number">6.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#can-kao"><span class="nav-number">7.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <a href='/'>
    <img class="site-author-image" itemprop="image" alt="Li Zhen"
      src="/images/snoppy.jpeg">
  </a>
  <p class="site-author-name" itemprop="name">Li Zhen</p>
  <div class="site-description" itemprop="description">他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">90</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">66</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jeffery0628" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jeffery0628" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jeffery.lee.0628@gmail.com" title="邮箱 → mailto:jeffery.lee.0628@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮箱</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Zhen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">1.2m</span>
</div>

        
<div class="busuanzi-count">
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.1.0/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  






  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>


 

<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180101,"YYYYMMDD"));
    ages = ages.replace(/years?/, "年");
    ages = ages.replace(/months?/, "月");
    ages = ages.replace(/days?/, "天");
    ages = ages.replace(/hours?/, "小时");
    ages = ages.replace(/minutes?/, "分");
    ages = ages.replace(/seconds?/, "秒");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `我已在此等候你 ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


<script src="https://cdn.jsdelivr.net/npm/sweetalert@2.1.2/dist/sweetalert.min.js"></script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'tChTbEIggDSVcdhPBUf0MFxW-gzGzoHsz',
      appKey     : 'sJ6xUiFnifEDIIfnj3Ut9zy1',
      placeholder: "留下你的小脚印吧！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '8' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
