<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/snoppy.jpeg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open Sans:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-big-counter.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jeffery.ink","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":15,"offset":12,"onmobile":true,"dimmer":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":true,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":true,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="tensorflow计算性能">
<meta property="og:url" content="https://jeffery.ink/2020/07/19/deeplearning/tf_computational_performance/index.html">
<meta property="og:site_name" content="火种2号">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jeffery.ink/2020/07/19/deeplearning/tf_computational_performance/201510141003.jpg">
<meta property="article:published_time" content="2020-07-19T02:01:37.000Z">
<meta property="article:modified_time" content="2020-07-19T04:52:41.228Z">
<meta property="article:author" content="Li Zhen">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jeffery.ink/2020/07/19/deeplearning/tf_computational_performance/201510141003.jpg">

<link rel="canonical" href="https://jeffery.ink/2020/07/19/deeplearning/tf_computational_performance/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>tensorflow计算性能 | 火种2号</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">火种2号</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">火种计划</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>简历</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>读书</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-video fa-fw"></i>电影</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>



</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jeffery.ink/2020/07/19/deeplearning/tf_computational_performance/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/snoppy.jpeg">
      <meta itemprop="name" content="Li Zhen">
      <meta itemprop="description" content="他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="火种2号">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          tensorflow计算性能
        </h1>

        <div class="post-meta">
          
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-07-19 12:52:41" itemprop="dateModified" datetime="2020-07-19T12:52:41+08:00">2020-07-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF-tensorflow/" itemprop="url" rel="index"><span itemprop="name">技术/tensorflow</span></a>
                </span>
            </span>

          
            <span id="/2020/07/19/deeplearning/tf_computational_performance/" class="post-meta-item leancloud_visitors" data-flag-title="tensorflow计算性能" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论：</span>
    
    <a title="valine" href="/2020/07/19/deeplearning/tf_computational_performance/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/07/19/deeplearning/tf_computational_performance/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>23k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>21 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/2020/07/19/deeplearning/tf_computational_performance/201510141003.jpg" alt></p>
<a id="more"></a>
<h1 id="ming-ling-shi-he-fu-hao-shi-hun-he-bian-cheng">命令式和符号式混合编程</h1>
<p>考虑下面这段简单的命令式程序。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fancy_func</span><span class="params">(a, b, c, d)</span>:</span></span><br><span class="line">    e = add(a, b)</span><br><span class="line">    f = add(c, d)</span><br><span class="line">    g = add(e, f)</span><br><span class="line">    <span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line">fancy_func(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>) <span class="comment"># 10</span></span><br></pre></td></tr></table></figure>
<p>和预期的一样，在运行语句<code>e = add(a, b)</code>时，Python会做加法运算并将结果存储在变量<code>e</code>中，从而令程序的状态发生改变。类似地，后面的两条语句<code>f = add(c, d)</code>和<code>g = add(e, f)</code>会依次做加法运算并存储变量。</p>
<p>虽然使用命令式编程很方便，但它的运行可能很慢。一方面，即使<code>fancy_func</code>函数中的<code>add</code>是被重复调用的函数，Python也会逐一执行这3条函数调用语句。另一方面，我们需要保存变量<code>e</code>和<code>f</code>的值直到<code>fancy_func</code>中所有语句执行结束。这是因为在执行<code>e = add(a, b)</code>和<code>f = add(c, d)</code>这2条语句之后我们并不知道变量<code>e</code>和<code>f</code>是否会被程序的其他部分使用。</p>
<p>与命令式编程不同，符号式编程通常在计算流程完全定义好后才被执行。多个深度学习框架，如<strong>Theano和TensorFlow，都使用了符号式编程</strong>。通常，符号式编程的程序需要下面3个步骤：</p>
<ol>
<li>定义计算流程；</li>
<li>把计算流程编译成可执行的程序；</li>
<li>给定输入，调用编译好的程序执行。</li>
</ol>
<p>下面我们用符号式编程重新实现本节开头给出的命令式编程代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_str</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'''</span></span><br><span class="line"><span class="string">def add(a, b):</span></span><br><span class="line"><span class="string">    return a + b</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fancy_func_str</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'''</span></span><br><span class="line"><span class="string">def fancy_func(a, b, c, d):</span></span><br><span class="line"><span class="string">    e = add(a, b)</span></span><br><span class="line"><span class="string">    f = add(c, d)</span></span><br><span class="line"><span class="string">    g = add(e, f)</span></span><br><span class="line"><span class="string">    return g</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evoke_str</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> add_str() + fancy_func_str() + <span class="string">'''</span></span><br><span class="line"><span class="string">print(fancy_func(1, 2, 3, 4))</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">prog = evoke_str()</span><br><span class="line">print(prog)</span><br><span class="line">y = compile(prog, <span class="string">''</span>, <span class="string">'exec'</span>)</span><br><span class="line">exec(y)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def <span class="built_in">add</span>(<span class="keyword">a</span>, b):</span><br><span class="line">    <span class="literal">return</span> <span class="keyword">a</span> + b</span><br><span class="line"></span><br><span class="line">def fancy_func(<span class="keyword">a</span>, b, c, d):</span><br><span class="line">    e = <span class="built_in">add</span>(<span class="keyword">a</span>, b)</span><br><span class="line">    f = <span class="built_in">add</span>(c, d)</span><br><span class="line">    g = <span class="built_in">add</span>(e, f)</span><br><span class="line">    <span class="literal">return</span> g</span><br><span class="line"></span><br><span class="line">print(fancy_func(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>以上定义的3个函数都仅以字符串的形式返回计算流程。最后，通过<code>compile</code>函数编译完整的计算流程并运行。由于在编译时系统能够完整地获取整个程序，因此有更多空间优化计算。例如，编译的时候可以将程序改写成<code>print((1 + 2) + (3 + 4))</code>，甚至直接改写成<code>print(10)</code>。这样不仅减少了函数调用，还节省了内存。</p>
<p>对比这两种编程方式，可以看到以下两点。</p>
<ul>
<li>
<p>命令式编程更方便。当在Python里使用命令式编程时，大部分代码编写起来都很直观。同时，命令式编程更容易调试。这是因为可以很方便地获取并打印所有的中间变量值，或者使用Python的调试工具。</p>
</li>
<li>
<p>符号式编程更高效并更容易移植。一方面，在编译的时候系统容易做更多优化；另一方面，符号式编程可以将程序变成一个与Python无关的格式，从而可以使程序在非Python环境下运行，以避开Python解释器的性能问题。</p>
</li>
</ul>
<h2 id="hun-he-shi-bian-cheng-qu-liang-zhe-zhi-chang">混合式编程取两者之长</h2>
<p>大部分深度学习框架在命令式编程和符号式编程之间二选一。例如，Theano和受其启发的后来者<code>TensorFlow1.x</code>使用了符号式编程，<code>Chainer</code>和它的追随者<code>PyTorch</code>使用了命令式编程。开发人员在设计<code>Tensorflow2.x</code>时思考了这个问题：有没有可能既得到命令式编程的好处，又享受符号式编程的优势？开发者们认为，用户应该用纯命令式编程进行开发和调试；当需要产品级别的计算性能和部署时，用户可以将大部分命令式程序转换成符号式程序来运行。Tensorflow通过提供静态图转换器<code>tf.function</code>,实现对两种编程方式的支持。在不使用静态图转换器<code>tf.function</code>时，用户编写的<code>python</code>函数默认会采用命令式编程<strong>逐行执行</strong>，符合<code>python</code>编程的直觉，便于调试，但因为框架不能获得完整的静态运算图，不能进行优化，且动态图不能序列化。官方由此推出了静态图转换器<code>tf.function</code>，其作用在<code>python_function</code>后会将这个函数&quot;编译&quot;成一个运算图，接受<code>input_tensors</code>为输入并用图执行的方式计算结果，可以加速函数执行，并且可以被序列化后供任何其他语言（如C++和Java）调用，这样用户只需要在使用动态图运算编写和测试完毕函数之后使用tf.function装饰一下就能够获得静态图的所有优点。</p>
<h2 id="tf-function-de-shi-yong">tf.function 的使用</h2>
<h3 id="ji-chu">基础</h3>
<p><code>tf.function</code>可以定义一个<code>Tensorflow</code>操作，既可以命令式的执行它，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> a+b</span><br><span class="line"></span><br><span class="line">add(tf.ones([<span class="number">2</span>, <span class="number">2</span>]), tf.ones([<span class="number">2</span>, <span class="number">2</span>]))  <span class="comment">#  [[2., 2.], [2., 2.]]</span></span><br></pre></td></tr></table></figure>
<pre><code>&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[2., 2.],
       [2., 2.]], dtype=float32)&gt;
</code></pre>
<p>也可以在图中执行它，并求其梯度，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    result = add(v, <span class="number">1.0</span>)</span><br><span class="line">tape.gradient(result, v)</span><br></pre></td></tr></table></figure>
<pre><code>  &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt;
</code></pre>
<p>也可以定义嵌套定义（当然，在实际使用中，可以直接在顶层定义，会自动对子图进行转换），</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_layer</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> add(tf.matmul(x, w), b)</span><br><span class="line"></span><br><span class="line">dense_layer(tf.ones([<span class="number">3</span>, <span class="number">2</span>]), tf.ones([<span class="number">2</span>, <span class="number">2</span>]), tf.ones([<span class="number">2</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=
array([[3., 3.],
       [3., 3.],
       [3., 3.]], dtype=float32)&gt;
</code></pre>
<h3 id="zhui-zong-yu-duo-tai">追踪与多态</h3>
<p><code>Python</code> 的动态类型意味着用户可以传递多种类型的参数，这也许会导致函数产生不同的行为。</p>
<p>在另一方面，<code>Tensorflow</code>的静态图需要确定的数据类型和维度。<code>tf.function</code> 通过<code>retracing</code>函数调用来弥补这一差距，并在必要的时候产生正确的计算图。<code>tf.function</code>大多数微妙的行为都产生自<code>retracing</code>行为。</p>
<p>可以用不同的参数调用同一函数来观察<code>retracing</code>行为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Functions are polymorphic</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">double</span><span class="params">(a)</span>:</span></span><br><span class="line">  print(<span class="string">"Tracing with"</span>, a)</span><br><span class="line">  <span class="keyword">return</span> a + a</span><br><span class="line"></span><br><span class="line">print(double(tf.constant(<span class="number">1</span>)))</span><br><span class="line">print()</span><br><span class="line">print(double(tf.constant(<span class="number">1.1</span>)))</span><br><span class="line">print()</span><br><span class="line">print(double(tf.constant(<span class="string">"a"</span>)))</span><br><span class="line">print()</span><br></pre></td></tr></table></figure>
<pre><code>Tracing with Tensor(&quot;a:0&quot;, shape=(), dtype=int32)
tf.Tensor(2, shape=(), dtype=int32)

Tracing with Tensor(&quot;a:0&quot;, shape=(), dtype=float32)
tf.Tensor(2.2, shape=(), dtype=float32)

Tracing with Tensor(&quot;a:0&quot;, shape=(), dtype=string)
tf.Tensor(b'aa', shape=(), dtype=string)
</code></pre>
<p>如果希望控制 <code>tracing</code> 行为，可以用如下方式操作：</p>
<ul>
<li>创建新的 <code>tf.function</code>。分离 <code>tf.fucntion</code> 对象，保证没有共享的计算图引用。</li>
<li>使用 <code>get_concrete_function</code> 方法，得到特定的计算图。</li>
<li>声明 <code>input_signature</code> 当调用 <code>tf.function</code> 时，仅跟踪与输入签名一致的调用。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Obtaining concrete trace"</span>)</span><br><span class="line">double_strings = double.get_concrete_function(tf.TensorSpec(shape=<span class="literal">None</span>, dtype=tf.string))</span><br><span class="line">print(<span class="string">"Executing traced function"</span>)</span><br><span class="line">print(double_strings(tf.constant(<span class="string">"a"</span>)))</span><br><span class="line">print(double_strings(a=tf.constant(<span class="string">"b"</span>)))</span><br><span class="line">print(<span class="string">"Using a concrete trace with incompatible types will throw an error"</span>)</span><br><span class="line"><span class="keyword">with</span> assert_raises(tf.errors.InvalidArgumentError):</span><br><span class="line">  double_strings(tf.constant(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Obtaining concrete trace</span><br><span class="line">Tracing <span class="keyword">with</span> <span class="constructor">Tensor(<span class="string">"a:0"</span>, <span class="params">dtype</span>=<span class="params">string</span>)</span></span><br><span class="line">Executing traced <span class="keyword">function</span></span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">aa</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">bb</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line"></span><br><span class="line">Using a concrete trace <span class="keyword">with</span> incompatible types will throw an error</span><br><span class="line">Caught expected <span class="keyword">exception</span> </span><br><span class="line">  &lt;<span class="keyword">class</span> 'tensorflow.python.framework.errors_impl.InvalidArgumentError'&gt;:</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;ipython-input-3-73d0ca52e838&gt;"</span>, line <span class="number">8</span>, <span class="keyword">in</span> assert_raises</span><br><span class="line">    yield</span><br><span class="line">  File <span class="string">"&lt;ipython-input-8-5351d0a2eda2&gt;"</span>, line <span class="number">8</span>, <span class="keyword">in</span> &lt;<span class="keyword">module</span>&gt;</span><br><span class="line">    double<span class="constructor">_strings(<span class="params">tf</span>.<span class="params">constant</span>(1)</span>)</span><br><span class="line">tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute __inference_double_87 <span class="keyword">as</span> input #<span class="number">0</span>(zero-based) was expected <span class="keyword">to</span> be a <span class="built_in">string</span> tensor but is a <span class="built_in">int32</span> tensor <span class="literal">[O<span class="identifier">p</span>:<span class="identifier">__inference_double_87</span>]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32),))</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_collatz</span><span class="params">(x)</span>:</span></span><br><span class="line">  print(<span class="string">"Tracing with"</span>, x)</span><br><span class="line">  <span class="keyword">return</span> tf.where(x % <span class="number">2</span> == <span class="number">0</span>, x // <span class="number">2</span>, <span class="number">3</span> * x + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(next_collatz(tf.constant([<span class="number">1</span>, <span class="number">2</span>])))</span><br><span class="line"><span class="comment"># We specified a 1-D tensor in the input signature, so this should fail.</span></span><br><span class="line"><span class="keyword">with</span> assert_raises(ValueError):</span><br><span class="line">  next_collatz(tf.constant([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]))</span><br></pre></td></tr></table></figure>
<pre><code>Tracing with Tensor(&quot;x:0&quot;, shape=(None,), dtype=int32)
tf.Tensor([4 1], shape=(2,), dtype=int32)

Caught expected exception 
  &lt;class 'ValueError'&gt;:

Traceback (most recent call last):
  File &quot;&lt;ipython-input-3-73d0ca52e838&gt;&quot;, line 8, in assert_raises
    yield
  File &quot;&lt;ipython-input-9-9939c82c1507&gt;&quot;, line 9, in &lt;module&gt;
    next_collatz(tf.constant([[1, 2], [3, 4]]))
ValueError: Python inputs incompatible with input_signature:
  inputs: (
    tf.Tensor(
[[1 2]
 [3 4]], shape=(2, 2), dtype=int32))
  input_signature: (
    TensorSpec(shape=(None,), dtype=tf.int32, name=None))
</code></pre>
<h3 id="zhui-zong-hong-fa-de-shi-ji">追踪触发的时机</h3>
<p>多态函数 <code>tf.function</code> 会缓存之前追踪行为触发生成过的具体函数。缓存的键由传入的参数确定，对于 <code>tf.Tensor</code> 参数而言，是其维度和类型，而对于 <code>Python</code> 元语，是其值。对于其它 Python类型，使用对象 id，即对每个不同的类实例都会触发独立的追踪行为，并生成相应的静态图。</p>
<h3 id="shu-ru-can-shu-de-xuan-ze-python-or-tensor">输入参数的选择 Python or Tensor</h3>
<p>通常，<code>Python</code> 参数被用作超参数，如 <code>num_layers=10</code>、<code>training=True</code>以及<code>nonlinearity='relu'</code>。此时，<code>Python</code> 参数的改变触发 <code>retrace</code> 行为来构建新的计算图是合理的。然而，在另一些情况下，<code>Python</code> 参数并不改变计算图，是不需要触发 <code>retrace</code> 重新构建计算图的。例如，在训练过程中控制步数，AutoGraph 会自动动态展开，因此传入不同的步数，其生成图是一致的，这时如果触发多个 <code>trace</code> 生成同样的计算图，是很低效的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_one_step</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(num_steps)</span>:</span></span><br><span class="line">  print(<span class="string">"Tracing with num_steps = &#123;&#125;"</span>.format(num_steps))</span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> tf.range(num_steps):</span><br><span class="line">    train_one_step()</span><br><span class="line"></span><br><span class="line">train(num_steps=<span class="number">10</span>)</span><br><span class="line">train(num_steps=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Tracing with num_steps = 10
Tracing with num_steps = 20
</code></pre>
<p>一种简单的绕过方式是，将参数转换为 Tensor，这样不改变 shape，就不会触发生成计算图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train(num_steps=tf.constant(<span class="number">10</span>))</span><br><span class="line">train(num_steps=tf.constant(<span class="number">20</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Tracing with num_steps = Tensor(&quot;num_steps:0&quot;, shape=(), dtype=int32)
</code></pre>
<h3 id="tf-function-de-fu-dai-xiao-ying">tf.function 的附带效应</h3>
<p>通常，<code>Python</code> 的附带效应（<code>print</code> 或改变对象）仅发生在 <code>tracing</code> 行为中。何时应该触发 <code>tf.function</code> 的附带效应呢？</p>
<p>经验上推荐仅使用附带效应来 debug <code>trace</code> 行为。其它情况则建议使用 <code>Tensorflow</code> 运算如 <code>tf.Variable.assign</code>、<code>tf.print</code>和<code>tf.summary</code>来在 <code>Tensorflow runtime</code> 保证代码跟踪和执行。帮助调试的最佳实践是使用函数式风格。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">  print(<span class="string">"Traced with"</span>, x)</span><br><span class="line">  tf.print(<span class="string">"Executed with"</span>, x)</span><br><span class="line"></span><br><span class="line">f(<span class="number">1</span>)</span><br><span class="line">f(<span class="number">1</span>)</span><br><span class="line">f(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Traced with 1
Executed with 1
Executed with 1
Traced with 2
Executed with 2
</code></pre>
<p>如果想要在每次调用 <code>tf.function</code> 时执行 <code>Python</code> 代码，<code>tf.py_function</code> 提供了这种方式的支持。使用 <code>tf.py_function</code> 的一个缺点是性能，不能很好的工作在分布式环境下（多GPU/TPU）。由于 <code>tf.py_function</code> 需要被可微的连入计算图，输入和输出都会被转换为 <code>tf.Tensor</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">external_list = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">side_effect</span><span class="params">(x)</span>:</span></span><br><span class="line">  print(<span class="string">'Python side effect'</span>)</span><br><span class="line">  external_list.append(x)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">  tf.py_function(side_effect, inp=[x], Tout=[])</span><br><span class="line"></span><br><span class="line">f(<span class="number">1</span>)</span><br><span class="line">f(<span class="number">1</span>)</span><br><span class="line">f(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> len(external_list) == <span class="number">3</span></span><br><span class="line"><span class="comment"># .numpy() call required because py_function casts 1 to tf.constant(1)</span></span><br><span class="line"><span class="keyword">assert</span> external_list[<span class="number">0</span>].numpy() == <span class="number">1</span></span><br></pre></td></tr></table></figure>
<pre><code>Python side effect
Python side effect
Python side effect
</code></pre>
<h3 id="zhu-yi-python-de-zhuang-tai">注意 Python 的状态</h3>
<p>许多 Python 特性，如生成器和迭代器，依赖于 Python 运行时跟踪其状态。通常，这些构件在动态图模式下工作正常，但由于 <code>tracing</code> 行为，在 <code>tf.function</code> 内会发生预期外行为。</p>
<p>例如，迭代器状态被视为一种 Python 附带效应，因此只在 <code>tracing</code> 时触发一次。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">external_var = tf.Variable(<span class="number">0</span>)</span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">buggy_consume_next</span><span class="params">(iterator)</span>:</span></span><br><span class="line">  external_var.assign_add(next(iterator))</span><br><span class="line">  tf.print(<span class="string">"Value of external_var:"</span>, external_var)</span><br><span class="line"></span><br><span class="line">iterator = iter([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">buggy_consume_next(iterator)</span><br><span class="line"><span class="comment"># This reuses the first value from the iterator, rather than consuming the next value.</span></span><br><span class="line">buggy_consume_next(iterator)</span><br><span class="line">buggy_consume_next(iterator)</span><br></pre></td></tr></table></figure>
<pre><code>Value of external_var: 0
Value of external_var: 0
Value of external_var: 0
</code></pre>
<p>如果一个迭代器生成和消费完全在 <code>tf.function</code> 中，它会正确地工作，但会产生巨大地计算图，这也许不符合你的预期，更严重的是，对于以 Python List 表示的内存中大型数据集，相应的大型计算图也并不能带来性能提升。</p>
<p>如果想要在 <code>Python</code> 数据集上迭代，最安全的方式是使用 <code>tf.data.Dataset</code> 封装，并以 <code>for x in y</code> 方式遍历。AutoGraph 对 <code>for</code> 循环中 <code>y</code> 是一个 <code>tf.Tensor</code> 或 <code>tf.data.Dataset</code> 有特殊支持。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">measure_graph_size</span><span class="params">(f, *args)</span>:</span></span><br><span class="line">  g = f.get_concrete_function(*args).graph</span><br><span class="line">  print(<span class="string">"&#123;&#125;(&#123;&#125;) contains &#123;&#125; nodes in its graph"</span>.format(</span><br><span class="line">      f.__name__, <span class="string">', '</span>.join(map(str, args)), len(g.as_graph_def().node)))</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(dataset)</span>:</span></span><br><span class="line">  loss = tf.constant(<span class="number">0</span>)</span><br><span class="line">  <span class="keyword">for</span> x, y <span class="keyword">in</span> dataset:</span><br><span class="line">    loss += tf.abs(y - x) <span class="comment"># Some dummy computation.</span></span><br><span class="line">  <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">small_data = [(<span class="number">1</span>, <span class="number">1</span>)] * <span class="number">2</span></span><br><span class="line">big_data = [(<span class="number">1</span>, <span class="number">1</span>)] * <span class="number">10</span></span><br><span class="line">measure_graph_size(train, small_data)</span><br><span class="line">measure_graph_size(train, big_data)</span><br><span class="line"></span><br><span class="line">measure_graph_size(train, tf.data.Dataset.from_generator(</span><br><span class="line">    <span class="keyword">lambda</span>: small_data, (tf.int32, tf.int32)))</span><br><span class="line">measure_graph_size(train, tf.data.Dataset.from_generator(</span><br><span class="line">    <span class="keyword">lambda</span>: big_data, (tf.int32, tf.int32)))</span><br></pre></td></tr></table></figure>
<pre><code>train([(1, 1), (1, 1)]) contains 8 nodes in its graph
train([(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]) contains 32 nodes in its graph
train(&lt;FlatMapDataset shapes: (&lt;unknown&gt;, &lt;unknown&gt;), types: (tf.int32, tf.int32)&gt;) contains 9 nodes in its graph
train(&lt;FlatMapDataset shapes: (&lt;unknown&gt;, &lt;unknown&gt;), types: (tf.int32, tf.int32)&gt;) contains 9 nodes in its graph
</code></pre>
<p>当封装 <code>Python/Numpy</code> 数据为 <code>tf.data.Dataset</code> 时，要注意 <code>tf.data.Dataset.from_generator</code> 与 <code>tf.data.Dataset.from_tensors</code> 的区别。前者保持数据在 <code>Python runtime</code> 中，通过 <code>tf.py_function</code> 取数，会造成一定的性能瓶颈。而后者会复制一份到 <code>Tensorflow runtime</code> 成为计算图的一个 <code>tf.constant()</code> 节点，会占用更多的内存。</p>
<p>从文件中读数据，如 <code>TFRecordDataset/CsvDataset</code> 等，是读取数据最高效的方式，<code>Tensorflow</code> 可以自行管理异步读取和预加载数据，而不需要 <code>Python</code> 的参与。</p>
<h3 id="zi-dong-kong-zhi-yi-lai">自动控制依赖</h3>
<p>作为编程模型，<code>tf.function</code> 一个非常吸引人的特性是，在通常的数据流图之上，为运行时环境提供了更多关于代码预期行为的信息。</p>
<p>例如，当写代码是多次读写同一变量，数据流图也许不会编码原本预期的操作顺序。在 <code>tf.function</code> 中，则会依照在 <code>Python</code> 代码中的声明顺序消除执行顺序的歧义。这使得 <code>tf.function</code> 支持状态操作，可以复制动态图模式的语义。</p>
<p>这意味着不再需要手动添加控制依赖，而可以交由 <code>tf.function</code> 自动添加控制依赖。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Automatic control dependencies</span></span><br><span class="line"></span><br><span class="line">a = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line">b = tf.Variable(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x, y)</span>:</span></span><br><span class="line">  a.assign(y * b)</span><br><span class="line">  b.assign_add(x * a)</span><br><span class="line">  <span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line">f(<span class="number">1.0</span>, <span class="number">2.0</span>)  <span class="comment"># 10.0</span></span><br><span class="line">&lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">10.0</span>&gt;</span><br></pre></td></tr></table></figure>
<h3 id="bian-liang">变量</h3>
<p>变量也可能使动态图模式的执行结果和静态图模式产生差异。当每次调用创建一个新变量时，由于 <code>tracing</code> 语义，<code>tf.function</code> 会在每次调用时重用同一变量，但动态图时会在每次调用时新建相应的变量。为了避免类似的问题，<code>tf.function</code> 将在监测到危险的变量创建行为时抛出异常。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">  v = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line">  v.assign_add(x)</span><br><span class="line">  <span class="keyword">return</span> v</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> assert_raises(ValueError):</span><br><span class="line">  f(<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;ipython-input-17-73e410646579&gt;:3 f  *
    v = tf.Variable(1.0)
/tmpfs/src/tf_docs_env/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:260 __call__
    return cls._variable_v2_call(*args, **kwargs)
/tmpfs/src/tf_docs_env/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:254 _variable_v2_call
    shape=shape)
/tmpfs/src/tf_docs_env/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:65 getter
    return captured_getter(captured_previous, **kwargs)
/tmpfs/src/tf_docs_env/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py:502 invalid_creator_scope
    &quot;tf.function-decorated function tried to create &quot;

ValueError: tf.function-decorated function tried to create variables on non-first call.
</code></pre>
<p>而无歧义的代码就不会触发相应的行为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">v = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> v.assign_add(x)</span><br><span class="line"></span><br><span class="line">print(f(<span class="number">1.0</span>))  <span class="comment"># 2.0</span></span><br><span class="line">print(f(<span class="number">2.0</span>))  <span class="comment"># 4.0</span></span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(2.0, shape=(), dtype=float32)
tf.Tensor(4.0, shape=(), dtype=float32)
</code></pre>
<p>只要可以证明<code>tf.function</code>中的变量只在函数初次执行时被创建，也是可以通过检查的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span>:</span></span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">obj = C()</span><br><span class="line">obj.v = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> obj.v <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    obj.v = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line">  <span class="keyword">return</span> obj.v.assign_add(x)</span><br><span class="line"></span><br><span class="line">print(g(<span class="number">1.0</span>))  <span class="comment"># 2.0</span></span><br><span class="line">print(g(<span class="number">2.0</span>))  <span class="comment"># 4.0</span></span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(2.0, shape=(), dtype=float32)
tf.Tensor(4.0, shape=(), dtype=float32)
</code></pre>
<h2 id="auto-graph">AutoGraph</h2>
<p><code>AutoGraph</code> 集成在 <code>tf.function</code> 中，用来重写依赖与张量的条件分支和循环，以支持计算图动态改变结构。</p>
<p><code>tf.cond</code> 和 <code>tf.while_loop</code> 依旧与 <code>tf.function</code> 兼容，但以命令式写控制流更简单直观。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Simple loop</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">while</span> tf.reduce_sum(x) &gt; <span class="number">1</span>:</span><br><span class="line">    tf.print(x)</span><br><span class="line">    x = tf.tanh(x)</span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">f(tf.random.uniform([<span class="number">5</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>[0.654489756 0.378447413 0.843570352 0.706569076 0.899703264]
[0.57468468 0.361358374 0.687695503 0.608520865 0.716153383]
[0.518791437 0.346409947 0.596499443 0.543085039 0.614520967]
[0.476766676 0.333187848 0.534554 0.495319664 0.54730171]
[0.443650424 0.321382284 0.488854468 0.458428353 0.498495191]
[0.416665703 0.310756236 0.453306764 0.428802431 0.460932881]
[0.394117773 0.301124901 0.424613386 0.40432 0.430844247]
[0.374904692 0.292341709 0.400809854 0.383639246 0.406026632]
[0.358274311 0.284288675 0.380641699 0.36586377 0.385093749]
[0.343693078 0.276869684 0.36326462 0.3503685 0.367122918]
[0.330770403 0.270005435 0.348086357 0.336702287 0.351472586]
[0.319212824 0.263629884 0.334677339 0.324530154 0.337680846]
[0.308794975 0.257687539 0.322717279 0.313597322 0.325405359]
[0.299340427 0.252131343 0.31196183 0.303706169 0.314386249]
[0.290708899 0.246921107 0.302220792 0.294700593 0.30442214]
[0.282787144 0.242022276 0.293343604 0.286455452 0.295354247]
[0.275482684 0.237404957 0.285209358 0.278869152 0.287055373]
[0.268719077 0.233043134 0.277719557 0.271858126 0.279422313]
[0.262432516 0.228914022 0.27079317 0.265352964 0.272370338]
[0.256569326 0.22499761 0.264362723 0.259295493 0.265829057]
[0.25108391 0.221276194 0.258371592 0.25363645 0.259739518]
[0.245937288 0.217734098 0.252771795 0.248333916 0.254051864]
[0.241095871 0.214357331 0.247522414 0.243351877 0.248723671]
[0.236530572 0.211133406 0.242588282 0.238659233 0.24371852]
[0.23221606 0.2080511 0.237939 0.234228939 0.239004955]
[0.228130147 0.205100328 0.233548105 0.230037391 0.234555662]
[0.224253282 0.202271983 0.229392484 0.226063833 0.230346799]
[0.220568195 0.199557811 0.225451797 0.222289979 0.226357415]
[0.217059553 0.196950331 0.221708104 0.218699604 0.222569034]
[0.213713691 0.194442704 0.21814549 0.215278283 0.218965292]
[0.210518375 0.192028716 0.214749783 0.212013125 0.215531647]
[0.207462624 0.189702675 0.211508334 0.208892584 0.21225509]
[0.204536542 0.18745935 0.208409771 0.205906287 0.209123984]
[0.201731205 0.185293958 0.205443889 0.203044847 0.206127867]

&lt;tf.Tensor: shape=(5,), dtype=float32, numpy=
array([0.19903852, 0.18320207, 0.20260146, 0.20029978, 0.20325728],
      dtype=float32)&gt;
</code></pre>
<p>如下代码可以观察 <code>autograph</code> 生成的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">while</span> tf.reduce_sum(x) &gt; <span class="number">1</span>:</span><br><span class="line">    tf.print(x)</span><br><span class="line">    x = tf.tanh(x)</span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">print(tf.autograph.to_code(f))</span><br></pre></td></tr></table></figure>
<pre><code>def tf__f(x):
  do_return = False
  retval_ = ag__.UndefinedReturnValue()
  with ag__.FunctionScope('f', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:

    def get_state():
      return ()

    def set_state(_):
      pass

    def loop_body(x):
      ag__.converted_call(tf.print, (x,), None, fscope)
      x = ag__.converted_call(tf.tanh, (x,), None, fscope)
      return x,

    def loop_test(x):
      return ag__.converted_call(tf.reduce_sum, (x,), None, fscope) &gt; 1
    x, = ag__.while_stmt(loop_test, loop_body, get_state, set_state, (x,), ('x',), ())
    do_return = True
    retval_ = fscope.mark_return_value(x)
  do_return,
  return ag__.retval(retval_)
</code></pre>
<h3 id="auto-graph-tiao-jian-fen-zhi">AutoGraph：条件分支</h3>
<p><code>AutoGraph</code> 将 <code>if</code> 语句转换为等效的 <code>tf.cond</code> 调用。这一替换发生在条件变量为张量时，除此之外的条件，在 <code>tracing</code> 时确定。<code>test_tf_cond</code> 函数用来检查函数中是否使用了 <code>tf.cond</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_tf_cond</span><span class="params">(f, *args)</span>:</span></span><br><span class="line">  g = f.get_concrete_function(*args).graph</span><br><span class="line">  <span class="keyword">if</span> any(node.name == <span class="string">'cond'</span> <span class="keyword">for</span> node <span class="keyword">in</span> g.as_graph_def().node):</span><br><span class="line">    print(<span class="string">"&#123;&#125;(&#123;&#125;) uses tf.cond."</span>.format(</span><br><span class="line">        f.__name__, <span class="string">', '</span>.join(map(str, args))))</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"&#123;&#125;(&#123;&#125;) executes normally."</span>.format(</span><br><span class="line">        f.__name__, <span class="string">', '</span>.join(map(str, args))))</span><br><span class="line"></span><br><span class="line">  print(<span class="string">"  result: "</span>,f(*args).numpy())</span><br></pre></td></tr></table></figure>
<p>当参数为 python <code>True</code> 时，正常地执行条件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(x, training=True)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> training:</span><br><span class="line">    x = tf.nn.dropout(x, rate=<span class="number">0.5</span>)</span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">test_tf_cond(dropout, tf.ones([<span class="number">10</span>], dtype=tf.float32), <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>dropout(tf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(10,), dtype=float32), True) executes normally.
  result:  [2. 2. 2. 2. 0. 0. 2. 0. 0. 2.]
</code></pre>
<p>但传递一个张量则会使 python <code>if</code> 替换为 <code>tf.cond</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_tf_cond(dropout, tf.ones([<span class="number">10</span>], dtype=tf.float32), tf.constant(<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<pre><code>dropout(tf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(10,), dtype=float32), tf.Tensor(True, shape=(), dtype=bool)) uses tf.cond.
  result:  [0. 2. 2. 2. 0. 0. 0. 0. 2. 2.]
</code></pre>
<h3 id="auto-graph-yu-xun-huan">AutoGraph 与循环</h3>
<p><code>AutoGraph</code> 有一些转换循环的简单规则。</p>
<ul>
<li><code>for</code>：迭代器是张量时转换</li>
<li><code>while</code>：循环条件与张量有关时转换</li>
</ul>
<p>如果一个循环被转换，它将由 <code>tf.while</code> 动态展开，或在 <code>for x in tf.data.Dataset</code> 情况下，将循环转换为 <code>tf.data.Dataset.reduce</code>。</p>
<p>如果循环没有被转换，则静态展开。</p>
<p><code>test_dynamically_unrolled(f, *args)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_dynamically_unrolled</span><span class="params">(f, *args)</span>:</span></span><br><span class="line">  g = f.get_concrete_function(*args).graph</span><br><span class="line">  <span class="keyword">if</span> any(node.name == <span class="string">'while'</span> <span class="keyword">for</span> node <span class="keyword">in</span> g.as_graph_def().node):</span><br><span class="line">    print(<span class="string">"&#123;&#125;(&#123;&#125;) uses tf.while_loop."</span>.format(</span><br><span class="line">        f.__name__, <span class="string">', '</span>.join(map(str, args))))</span><br><span class="line">  <span class="keyword">elif</span> any(node.name == <span class="string">'ReduceDataset'</span> <span class="keyword">for</span> node <span class="keyword">in</span> g.as_graph_def().node):</span><br><span class="line">    print(<span class="string">"&#123;&#125;(&#123;&#125;) uses tf.data.Dataset.reduce."</span>.format(</span><br><span class="line">        f.__name__, <span class="string">', '</span>.join(map(str, args))))</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"&#123;&#125;(&#123;&#125;) gets unrolled."</span>.format(</span><br><span class="line">        f.__name__, <span class="string">', '</span>.join(map(str, args))))</span><br></pre></td></tr></table></figure>
<h4 id="for-xun-huan">For 循环</h4>
<p><code>tf.function</code> 的静态展开</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">for_in_range</span><span class="params">()</span>:</span></span><br><span class="line">  x = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    x += i</span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">test_dynamically_unrolled(for_in_range)</span><br></pre></td></tr></table></figure>
<pre><code>for_in_range() gets unrolled.
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">for_in_tfrange</span><span class="params">()</span>:</span></span><br><span class="line">  x = tf.constant(<span class="number">0</span>, dtype=tf.int32)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> tf.range(<span class="number">5</span>):</span><br><span class="line">    x += i</span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">test_dynamically_unrolled(for_in_tfrange)</span><br></pre></td></tr></table></figure>
<pre><code>for_in_tfrange() uses tf.while_loop.
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">for_in_tfdataset</span><span class="params">()</span>:</span></span><br><span class="line">  x = tf.constant(<span class="number">0</span>, dtype=tf.int64)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> tf.data.Dataset.range(<span class="number">5</span>):</span><br><span class="line">    x += i</span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">test_dynamically_unrolled(for_in_tfdataset)</span><br></pre></td></tr></table></figure>
<pre><code>for_in_tfdataset() uses tf.data.Dataset.reduce.
</code></pre>
<h4 id="while-xun-huan">While 循环</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">while_py_cond</span><span class="params">()</span>:</span></span><br><span class="line">  x = <span class="number">5</span></span><br><span class="line">  <span class="keyword">while</span> x &gt; <span class="number">0</span>:</span><br><span class="line">    x -= <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">test_dynamically_unrolled(while_py_cond)</span><br></pre></td></tr></table></figure>
<pre><code>while_py_cond() gets unrolled.
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">while_tf_cond</span><span class="params">()</span>:</span></span><br><span class="line">  x = tf.constant(<span class="number">5</span>)</span><br><span class="line">  <span class="keyword">while</span> x &gt; <span class="number">0</span>:</span><br><span class="line">    x -= <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">test_dynamically_unrolled(while_tf_cond)</span><br></pre></td></tr></table></figure>
<pre><code>while_tf_cond() uses tf.while_loop.
</code></pre>
<h1 id="yi-bu-ji-suan">异步计算</h1>
<p>Tensorflow使用异步计算来提升计算性能。理解它的工作原理既有助于开发更高效的程序，又有助于在内存资源有限的情况下主动降低计算性能从而减小内存开销。我们先导入本节中实验需要的包或模块。</p>
<h2 id="tensorflow-zhong-de-yi-bu-ji-suan">Tensorflow 中的异步计算</h2>
<p>广义上讲，<code>Tensorflow</code>包括用户直接用来交互的前端和系统用来执行计算的后端。例如，用户可以使用不同的前端语言编写<code>Tensorflow</code>程序，如<code>Python</code>、<code>C++</code>和<code>Javascript</code>。无论使用何种前端编程语言，<code>Tensorflow</code>程序的执行主要都发生在<code>C++</code>实现的后端。换句话说，用户写好的前端<code>Tensorflow</code>程序会传给后端执行计算。后端有自己的线程在队列中不断收集任务并执行它们。</p>
<p><code>Tensorflow</code>通过前端线程和后端线程的交互实现异步计算。异步计算指，前端线程无须等待当前指令从后端线程返回结果就继续执行后面的指令。为了便于解释，假设<code>Python</code>前端线程调用以下4条指令。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.ones((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">b = tf.ones((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">c = a * b + <span class="number">2</span></span><br><span class="line">c <span class="comment">#   &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[3., 3.]], dtype=float32)&gt;</span></span><br></pre></td></tr></table></figure>
<p>在异步计算中，<code>Python</code>前端线程执行前3条语句的时候，仅仅是把任务放进后端的队列里就返回了。当最后一条语句需要打印计算结果时，<code>Python</code>前端线程会等待<code>C++</code>后端线程把变量<code>c</code>的结果计算完。此设计的一个好处是，这里的Python前端线程不需要做实际计算。因此，无论<code>Python</code>的性能如何，它对整个程序性能的影响很小。只要<code>C++</code>后端足够高效，那么不管前端编程语言性能如何，<code>Tensorflow</code>都可以提供一致的高性能。</p>
<p>为了演示异步计算的性能，先实现一个简单的计时类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Benchmark</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, prefix=None)</span>:</span></span><br><span class="line">    self.prefix = prefix + <span class="string">' '</span> <span class="keyword">if</span> prefix <span class="keyword">else</span> <span class="string">''</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.start = time.time()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">    print(<span class="string">'%stime: %.4f sec'</span> % (self.prefix, time.time() - self.start))</span><br></pre></td></tr></table></figure>
<p>下面的例子通过计时来展示<code>Tensorflow2.x</code>的计算行为。可以看到，当<code>y = tf.keras.backend.sum(tf.transpose(x) * x)</code>返回的时候需等待变量y真正被计算完，以便<code>pdb</code>在命令模式下调试。这里的行为不同于<code>MXNet</code>。在<code>MXNet</code>中，计算行为发送到C++后端，由<code>print</code>触发同步行为，等待完成计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'Workloads are queued.'</span>):</span><br><span class="line">  x = tf.random.uniform(shape=(<span class="number">2000</span>, <span class="number">2000</span>))</span><br><span class="line">  y = tf.keras.backend.sum(tf.transpose(x) * x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'Workloads are finished.'</span>):</span><br><span class="line">  print(<span class="string">'sum ='</span>, y)</span><br></pre></td></tr></table></figure>
<pre><code>    Workloads are queued. time: 0.0808 sec
    sum = tf.Tensor(999325.0, shape=(), dtype=float32)
    Workloads are finished. time: 0.0001 sec
</code></pre>
<p>的确，除非需要打印或者保存计算结果，否则基本无须关心目前结果在内存中是否已经计算好了。<code>Tensorflow</code>默认使用命令模式，如果需要提高性能，需要利用<code>tf.function</code>和<code>AutoGraph</code>创建比一行命令对应的单独命令节点更大的计算图，使<code>C++</code>后端更少和前端交互，从而获得更好的性能。</p>
<h2 id="yong-tong-bu-han-shu-rang-qian-duan-deng-dai-ji-suan-jie-guo">用同步函数让前端等待计算结果</h2>
<p>除了刚刚介绍的<code>print</code>函数外，<code>MXNet</code>还有其他方法让前端线程等待后端的计算结果完成。我们可以使用  <code>wait_to_read</code>函数让前端等待某个的<code>NDArray</code>的计算结果完成，再执行前端中后面的语句。或者，我们可以用<code>waitall</code>函数令前端等待前面所有计算结果完成。后者是性能测试中常用的方法。</p>
<h2 id="shi-yong-yi-bu-ji-suan-ti-sheng-ji-suan-xing-neng">使用异步计算提升计算性能</h2>
<p>在下面的例子中，我们用<code>for</code>循环不断对变量<code>y</code>赋值。当在<code>for</code>循环内执行<code>y = x + 1</code>时，每次赋值不使用异步计算；当在<code>for</code>循环外使用<code>tf.function</code>装饰时，则使用异步计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'synchronous.'</span>):</span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    y = x + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    y = x + <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'asynchronous.'</span>):</span><br><span class="line">  y = loop()</span><br></pre></td></tr></table></figure>
<pre><code>    synchronous. time: 3.5589 sec
    asynchronous. time: 1.0457 sec
</code></pre>
<p>观察到，使用异步计算能提升一定的计算性能。为了解释这一现象，对Python前端线程和C++后端线程的交互稍作简化。在每一次循环中，前端和后端的交互大约可以分为3个阶段：</p>
<ol>
<li>前端令后端将计算任务y = x + 1放进队列；</li>
<li>后端从队列中获取计算任务并执行真正的计算；</li>
<li>后端将计算结果返回给前端。</li>
</ol>
<p>将这3个阶段的耗时分别设为 t1,t2,t3 。如果不使用异步计算，执行1000次计算的总耗时大约为 1000(t1+t2+t3) ；如果使用异步计算，由于每次循环中前端都无须等待后端返回计算结果，执行1000次计算的总耗时可以降为 t1+1000t2+t3 （假设 1000t2&gt;999t1 ）。</p>
<h2 id="yi-bu-ji-suan-dui-nei-cun-de-ying-xiang">异步计算对内存的影响</h2>
<p>在实现的模型训练过程中，通常会在每个小批量上评测一下模型，如模型的损失或者精度。细心的读者也许已经发现了，而<code>keras model</code>的<code>compile</code>方法会隐式调用<code>tf.function</code>，触发<code>AutoGraph</code>，前端会在极短的时间内使后端生成完整的计算图，从而可能导致占用更多内存。当我们使用命令执行模式时，前端在每次迭代时仅会将一个小批量的任务丢给后端执行计算，并通常会减小内存占用。</p>
<p>由于深度学习模型通常比较大，而内存资源通常有限，建议在训练模型时对每个小批量操作使用<code>tf.function</code>函数，而不是整个训练过程。类似地，在使用模型预测时，为了减小内存的占用，也建议对每个小批量预测时都使用同步函数。</p>
<p>下面演示异步计算对内存的影响。先定义一个数据获取函数<code>data_iter</code>，它会从被调用时开始计时，并定期打印到目前为止获取数据批量的总耗时。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">()</span>:</span></span><br><span class="line">  start = time.time()</span><br><span class="line">  num_batches, batch_size = <span class="number">100</span>, <span class="number">1024</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(num_batches):</span><br><span class="line">    X = tf.random.normal(shape=(batch_size, <span class="number">512</span>))</span><br><span class="line">    y = tf.ones((batch_size,))</span><br><span class="line">    <span class="keyword">yield</span> X, y</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">      print(<span class="string">'batch %d, time %f sec'</span> % (i+<span class="number">1</span>, time.time()-start))</span><br></pre></td></tr></table></figure>
<p>下面定义多层感知机、优化算法和损失函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = keras.Sequential()</span><br><span class="line">net.add(keras.layers.Dense(<span class="number">2048</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">net.add(keras.layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">net.add(keras.layers.Dense(<span class="number">1</span>))</span><br><span class="line">optimizer=keras.optimizers.SGD(<span class="number">0.05</span>)</span><br><span class="line">loss = keras.losses.MeanSquaredError()</span><br></pre></td></tr></table></figure>
<p>这里定义辅助函数来监测内存的使用。</p>
<blockquote>
<p>这个函数只能在Linux或macOS上运行。需要支持<code>ps</code>指令</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_mem</span><span class="params">()</span>:</span></span><br><span class="line">  res = subprocess.check_output([<span class="string">'ps'</span>, <span class="string">'u'</span>, <span class="string">'-p'</span>, str(os.getpid())])</span><br><span class="line">  <span class="keyword">return</span> int(str(res).split()[<span class="number">15</span>]) / <span class="number">1e3</span></span><br></pre></td></tr></table></figure>
<p>对于训练模型<code>net</code>来说，可以自然地使用命令式方式实现。此时，每个小批量的生成间隔较长，不过内存开销较小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">l_sum, mem = <span class="number">0</span>, get_mem()</span><br><span class="line">dense_1 = keras.layers.Dense(<span class="number">2048</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">dense_2 = keras.layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">dense_3 = keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line">trainable_variables = (dense_1.trainable_variables + </span><br><span class="line">                       dense_2.trainable_variables +</span><br><span class="line">                       dense_3.trainable_variables)</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter():</span><br><span class="line">  <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    logits = net(X)</span><br><span class="line">    loss_value = loss(y, logits)</span><br><span class="line"></span><br><span class="line">  grads = tape.gradient(loss_value, trainable_variables)</span><br><span class="line">  optimizer.apply_gradients(zip(grads, trainable_variables))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'increased memory: %f MB'</span> % (get_mem() - mem))</span><br></pre></td></tr></table></figure>
<pre><code>  batch 50, time 7.880550 sec
  batch 100, time 15.700529 sec
  increased memory: 14.336000 MB
</code></pre>
<p>如果转而使用预生成计算图，虽然每个小批量的生成间隔较短，但训练过程中可能会导致内存占用较高。这是因为在默认异步计算下，前端会将所有计算图在短时间内由后端完整生成。这使得在内存保存大量中间计算节点无法释放，从而占用额外内存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">l_sum, mem = <span class="number">0</span>, get_mem()</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter():</span><br><span class="line">  <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    logits = net(X)</span><br><span class="line">    loss_value = loss(y, logits)</span><br><span class="line"></span><br><span class="line">  grads = tape.gradient(loss_value, net.trainable_weights)</span><br><span class="line">  optimizer.apply_gradients(zip(grads, net.trainable_weights))</span><br></pre></td></tr></table></figure>
<pre><code>  batch 50, time 7.976524 sec
  batch 100, time 15.683179 sec
  increased memory: 12.268000 MB
</code></pre>
<h1 id="zi-dong-bing-xing-ji-suan">自动并行计算</h1>
<p>Tensorflow后端会自动构建计算图。通过计算图，系统可以知道所有计算的依赖关系，并可以选择将没有依赖关系的多个任务并行执行来获得计算性能的提升。例如“异步计算”第一个例子里依次执行了a = tf.ones((1, 2))和b = tf.ones((1, 2))。这两步计算之间并没有依赖关系，因此系统可以选择并行执行它们。</p>
<p>通常，一个运算符会用到所有CPU或单块GPU上全部的计算资源。例如，dot运算符会用到所有CPU（即使是一台机器上有多个CPU处理器）或单块GPU上所有的线程。如果每个运算符的计算量足够大，只在CPU上或者单块GPU上并行运行多个运算符时，每个运算符的运行只分到CPU或单块GPU上部分计算资源。即使这些计算可以并行，最终计算性能的提升可能也并不明显。</p>
<h2 id="cpu-he-gpu-de-bing-xing-ji-suan">CPU和GPU的并行计算</h2>
<p>程序中的计算既发生在CPU上，又发生在GPU上。先定义<code>run</code>函数，令它做10次矩阵乘法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> [tf.matmul(x, x) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br></pre></td></tr></table></figure>
<p>接下来，分别在CPU和GPU上创建<code>Tensor</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/CPU:0'</span>):</span><br><span class="line">  x_cpu = tf.random.uniform(shape=(<span class="number">2000</span>, <span class="number">2000</span>))</span><br><span class="line">  </span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/GPU:0'</span>):</span><br><span class="line">  x_gpu = tf.random.uniform(shape=(<span class="number">6000</span>, <span class="number">6000</span>))</span><br></pre></td></tr></table></figure>
<p>然后，分别使用它们在CPU和GPU上运行<code>run</code>函数并打印运行所需时间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">run(x_cpu)</span><br><span class="line">run(x_gpu)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'Run on CPU.'</span>):</span><br><span class="line">  run(x_cpu)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'Then Run on GPU.'</span>):</span><br><span class="line">  run(x_gpu)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Run <span class="keyword">on</span> CPU. <span class="built_in">time</span>: <span class="number">1.2657</span> sec</span><br><span class="line">Then Run <span class="keyword">on</span> GPU. <span class="built_in">time</span>: <span class="number">0.0005</span> sec</span><br></pre></td></tr></table></figure>
<p>尝试系统能自动并行这两个任务：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'Run on both CPU and GPU in parallel.'</span>):</span><br><span class="line">  run(x_cpu)</span><br><span class="line">  run(x_gpu)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">Run on both CPU <span class="keyword">and</span> GPU <span class="keyword">in</span> parallel. time: <span class="number">1.2364</span> sec</span><br></pre></td></tr></table></figure>
<p>可以看到，当两个计算任务一起执行时，执行总时间小于它们分开执行的总和。这表明，Tensorflow能有效地在CPU和GPU上自动并行计算。</p>
<h2 id="ji-suan-he-tong-xin-de-bing-xing-ji-suan">计算和通信的并行计算</h2>
<p>在同时使用CPU和GPU的计算中，经常需要在内存和显存之间复制数据，造成数据的通信。在下面的例子中，在GPU上计算，然后将结果复制回CPU使用的内存。分别打印GPU上计算时间和显存到内存的通信时间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">copy_to_cpu</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">with</span> tf.device(<span class="string">'/CPU:0'</span>):</span><br><span class="line">    <span class="keyword">return</span> [y <span class="keyword">for</span> y <span class="keyword">in</span> x]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'Run on GPU.'</span>):</span><br><span class="line">  y = run(x_gpu)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'Then copy to CPU.'</span>):</span><br><span class="line">  copy_to_cpu(y)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">Run on GPU. time: <span class="number">0.0047</span> sec</span><br><span class="line">Then copy to CPU. time: <span class="number">0.0007</span> sec</span><br></pre></td></tr></table></figure>
<p>打印这两个任务完成的总时间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> Benchmark(<span class="string">'Run and copy in parallel.'</span>):</span><br><span class="line">    y = run(x_gpu)</span><br><span class="line">    copy_to_cpu(y)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">Run <span class="keyword">and</span> copy <span class="keyword">in</span> parallel. time: <span class="number">0.0024</span> sec</span><br></pre></td></tr></table></figure>
<h1 id="zong-jie">总结</h1>
<ul>
<li>Tensorflow包括用户直接用来交互的前端和系统用来执行计算的后端。</li>
<li>Tensorflow能够通过生成更大规模的计算图，使后端异步计算时间更长，更少被打断，从而提升计算性能。</li>
<li>建议使用每个小批量训练或预测时以<code>batch</code>为单位生成计算图，从而避免在短时间内将过多计算任务丢给后端</li>
</ul>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://zh.d2l.ai/chapter_computational-performance/hybridize.html" target="_blank" rel="noopener">dive into deeplearning</a></li>
<li><a href="https://www.tensorflow.org/tutorials/customization/performance" target="_blank" rel="noopener">Better performance with tf.function</a></li>
<li><a href="https://github.com/tensorflow/community/blob/master/rfcs/20180918-functions-not-sessions-20.md" target="_blank" rel="noopener">Functions, not Sessions</a></li>
<li><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md" target="_blank" rel="noopener">AutoGraph Reference</a></li>
<li><a href="https://github.com/tensorflow/community/blob/master/rfcs/20190117-tf-module.md" target="_blank" rel="noopener">tf.Module</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/74441082" target="_blank" rel="noopener">Tensorflow2.0 学习笔记之静态图转换器</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/73575776" target="_blank" rel="noopener">Tensorflow2.0 学习笔记之状态容器</a></li>
</ol>

    </div>

    
    
    <div>
  
    <div style="text-align:center;color:#bfbfbf;font-size:16px;">
      <span>-------- 本文结束 </span>
      <i class="fa fa-coffee"></i>
      <span> 感谢阅读 --------</span>
    </div>
  
</div>
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    鼓励一下
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat.png" alt="Li Zhen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/ali.png" alt="Li Zhen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/07/16/deeplearning/numberical_stability_and_init/" rel="prev" title="数值稳定性和模型初始化">
      <i class="fa fa-chevron-left"></i> 数值稳定性和模型初始化
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/07/20/deeplearning/beam_search/" rel="next" title="Beam Search">
      Beam Search <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#ming-ling-shi-he-fu-hao-shi-hun-he-bian-cheng"><span class="nav-number">1.</span> <span class="nav-text">命令式和符号式混合编程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#hun-he-shi-bian-cheng-qu-liang-zhe-zhi-chang"><span class="nav-number">1.1.</span> <span class="nav-text">混合式编程取两者之长</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-function-de-shi-yong"><span class="nav-number">1.2.</span> <span class="nav-text">tf.function 的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ji-chu"><span class="nav-number">1.2.1.</span> <span class="nav-text">基础</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zhui-zong-yu-duo-tai"><span class="nav-number">1.2.2.</span> <span class="nav-text">追踪与多态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zhui-zong-hong-fa-de-shi-ji"><span class="nav-number">1.2.3.</span> <span class="nav-text">追踪触发的时机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#shu-ru-can-shu-de-xuan-ze-python-or-tensor"><span class="nav-number">1.2.4.</span> <span class="nav-text">输入参数的选择 Python or Tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-function-de-fu-dai-xiao-ying"><span class="nav-number">1.2.5.</span> <span class="nav-text">tf.function 的附带效应</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zhu-yi-python-de-zhuang-tai"><span class="nav-number">1.2.6.</span> <span class="nav-text">注意 Python 的状态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zi-dong-kong-zhi-yi-lai"><span class="nav-number">1.2.7.</span> <span class="nav-text">自动控制依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bian-liang"><span class="nav-number">1.2.8.</span> <span class="nav-text">变量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#auto-graph"><span class="nav-number">1.3.</span> <span class="nav-text">AutoGraph</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#auto-graph-tiao-jian-fen-zhi"><span class="nav-number">1.3.1.</span> <span class="nav-text">AutoGraph：条件分支</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#auto-graph-yu-xun-huan"><span class="nav-number">1.3.2.</span> <span class="nav-text">AutoGraph 与循环</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#for-xun-huan"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">For 循环</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#while-xun-huan"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">While 循环</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#yi-bu-ji-suan"><span class="nav-number">2.</span> <span class="nav-text">异步计算</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#tensorflow-zhong-de-yi-bu-ji-suan"><span class="nav-number">2.1.</span> <span class="nav-text">Tensorflow 中的异步计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#yong-tong-bu-han-shu-rang-qian-duan-deng-dai-ji-suan-jie-guo"><span class="nav-number">2.2.</span> <span class="nav-text">用同步函数让前端等待计算结果</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shi-yong-yi-bu-ji-suan-ti-sheng-ji-suan-xing-neng"><span class="nav-number">2.3.</span> <span class="nav-text">使用异步计算提升计算性能</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#yi-bu-ji-suan-dui-nei-cun-de-ying-xiang"><span class="nav-number">2.4.</span> <span class="nav-text">异步计算对内存的影响</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#zi-dong-bing-xing-ji-suan"><span class="nav-number">3.</span> <span class="nav-text">自动并行计算</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#cpu-he-gpu-de-bing-xing-ji-suan"><span class="nav-number">3.1.</span> <span class="nav-text">CPU和GPU的并行计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ji-suan-he-tong-xin-de-bing-xing-ji-suan"><span class="nav-number">3.2.</span> <span class="nav-text">计算和通信的并行计算</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#zong-jie"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#can-kao"><span class="nav-number">5.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <a href='/'>
    <img class="site-author-image" itemprop="image" alt="Li Zhen"
      src="/images/snoppy.jpeg">
  </a>
  <p class="site-author-name" itemprop="name">Li Zhen</p>
  <div class="site-description" itemprop="description">他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">83</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">69</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jeffery0628" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jeffery0628" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jeffery.lee.0628@gmail.com" title="邮箱 → mailto:jeffery.lee.0628@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮箱</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Zhen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">1.1m</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  






  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>


 

<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180101,"YYYYMMDD"));
    ages = ages.replace(/years?/, "年");
    ages = ages.replace(/months?/, "月");
    ages = ages.replace(/days?/, "天");
    ages = ages.replace(/hours?/, "小时");
    ages = ages.replace(/minutes?/, "分");
    ages = ages.replace(/seconds?/, "秒");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `我已在此等候你 ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


<script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>

<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'tChTbEIggDSVcdhPBUf0MFxW-gzGzoHsz',
      appKey     : 'sJ6xUiFnifEDIIfnj3Ut9zy1',
      placeholder: "留下你的小脚印吧！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '8' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
