<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/snoppy.jpeg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open Sans:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-big-counter.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jeffery.ink","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":270,"display":"hide","padding":15,"offset":12,"onmobile":true,"dimmer":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":true,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":true,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="tensorflow2">
<meta property="og:url" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/index.html">
<meta property="og:site_name" content="火种2号">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/tensorflow_2-1200x600.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-1-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B1%95%E7%A4%BA.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-1-Label%E5%88%86%E5%B8%83.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-1-%E5%B9%B4%E9%BE%84%E5%88%86%E5%B8%83.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-1-%E5%B9%B4%E9%BE%84%E7%9B%B8%E5%85%B3%E6%80%A7.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-1-Loss%E6%9B%B2%E7%BA%BF.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-1-AUC%E6%9B%B2%E7%BA%BF.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/cifar2.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-2-%E5%9B%BE%E7%89%87%E9%A2%84%E8%A7%88.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-2-tensorboard.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-2-dfhistory.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-2-Loss%E6%9B%B2%E7%BA%BF.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-2-Accuracy%E6%9B%B2%E7%BA%BF.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/%E7%94%B5%E5%BD%B1%E8%AF%84%E8%AE%BA.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-4-%E6%96%B0%E5%A2%9E%E4%BA%BA%E6%95%B0.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-4-%E7%B4%AF%E7%A7%AF%E6%9B%B2%E7%BA%BF.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-4-%E6%96%B0%E5%A2%9E%E6%9B%B2%E7%BA%BF.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-4-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%9B%B2%E7%BA%BF.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-4-%E6%97%A5%E6%9C%9F3%E6%9C%8810.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-4-%E9%A2%84%E6%B5%8B%E7%A1%AE%E8%AF%8A.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-4-%E9%A2%84%E6%B5%8B%E6%B2%BB%E6%84%88.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/1-4-%E9%A2%84%E6%B5%8B%E6%AD%BB%E4%BA%A1.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/strjoin_graph.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/2-2-tensorboard%E8%AE%A1%E7%AE%97%E5%9B%BE.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/tensorflow_structure.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/3-1-01-%E5%9B%9E%E5%BD%92%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/3-1-2-%E5%9B%9E%E5%BD%92%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/3-1-03-%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/3-1-04-%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/3-2-01-%E5%9B%9E%E5%BD%92%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/3-2-02-%E5%9B%9E%E5%BD%92%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/3-1-03-%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96-4135196.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/3-2-04-%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/3-3-01-%E5%9B%9E%E5%BD%92%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/3-3-02-%E5%9B%9E%E5%BD%92%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/3-3-03-%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/3-3-04-%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/%E6%9F%A5%E7%9C%8B%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E4%BF%A1%E6%81%AF.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/demomodule%E7%9A%84%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%BB%93%E6%9E%84.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/5-1-cifar2%E9%A2%84%E8%A7%88.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/5-1-car2.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/5-1-car9.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/%E7%89%B9%E5%BE%81%E5%88%979%E7%A7%8D.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/5-2-01-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/sigmoid.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/softmax%E8%AF%B4%E6%98%8E.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/tanh.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/relu.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/leaky_relu.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/elu.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/selu.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/swish.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/gelu.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/KS_curve.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/Sequential%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/6-1-fit%E6%A8%A1%E5%9E%8B.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/FunctionalAPI%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/6-1-2-train.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/Model%E5%AD%90%E7%B1%BB%E5%8C%96%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/6-1-3-fit%E6%A8%A1%E5%9E%8B.jpg">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E4%BF%A1%E6%81%AF.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/TfDriver.png">
<meta property="og:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/TfRDD.png">
<meta property="article:published_time" content="2020-05-06T23:31:43.000Z">
<meta property="article:modified_time" content="2020-08-22T05:20:17.950Z">
<meta property="article:author" content="Li Zhen">
<meta property="article:tag" content="建模流程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/tensorflow_2-1200x600.png">

<link rel="canonical" href="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>tensorflow2 | 火种2号</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">火种2号</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">火种计划</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>简历</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>读书</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-video fa-fw"></i>电影</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>



</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jeffery.ink/2020/05/07/tensorflow/tensorflow2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/snoppy.jpeg">
      <meta itemprop="name" content="Li Zhen">
      <meta itemprop="description" content="他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="火种2号">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          tensorflow2
        </h1>

        <div class="post-meta">
          
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-08-22 13:20:17" itemprop="dateModified" datetime="2020-08-22T13:20:17+08:00">2020-08-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">技术</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/TensorFlow/" itemprop="url" rel="index"><span itemprop="name">TensorFlow</span></a>
                </span>
            </span>

          
            <span id="/2020/05/07/tensorflow/tensorflow2/" class="post-meta-item leancloud_visitors" data-flag-title="tensorflow2" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论：</span>
    
    <a title="valine" href="/2020/05/07/tensorflow/tensorflow2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/05/07/tensorflow/tensorflow2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>222k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3:22</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/2020/05/07/tensorflow/tensorflow2/tensorflow_2-1200x600.png" alt></p>
<a id="more"></a>
<h1 id="tensor-flow-jian-mo-liu-cheng">TensorFlow建模流程</h1>
<p>尽管TensorFlow设计上足够灵活，可以用于进行各种复杂的数值计算。但通常人们使用TensorFlow来实现机器学习模型，尤其常用于实现神经网络模型。从原理上说可以使用张量构建计算图来定义神经网络，并通过自动微分机制训练模型。但为简洁起见，一般推荐使用TensorFlow的高层次keras接口来实现神经网络网模型。<br>
使用TensorFlow实现神经网络模型的一般流程包括：</p>
<ol>
<li>准备数据</li>
<li>定义模型</li>
<li>训练模型</li>
<li>评估模型</li>
<li>使用模型</li>
<li>保存模型。</li>
</ol>
<p>在实践中通常会遇到的数据类型包括结构化数据，图片数据，文本数据，时间序列数据。</p>
<h2 id="jie-gou-hua-shu-ju-jian-mo-liu-cheng">结构化数据建模流程</h2>
<h3 id="zhun-bei-shu-ju">准备数据</h3>
<p>titanic数据集的目标是根据乘客信息预测他们在Titanic号撞击冰山沉没后能否生存。</p>
<p>结构化数据一般会使用Pandas中的DataFrame进行预处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers</span><br><span class="line"></span><br><span class="line">dftrain_raw = pd.read_csv(<span class="string">'./data/titanic/train.csv'</span>)</span><br><span class="line">dftest_raw = pd.read_csv(<span class="string">'./data/titanic/test.csv'</span>)</span><br><span class="line">dftrain_raw.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-1-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B1%95%E7%A4%BA.jpg" alt></p>
<p>字段说明：</p>
<ul>
<li>Survived:0代表死亡，1代表存活【y标签】</li>
<li>Pclass:乘客所持票类，有三种值(1,2,3) 【转换成onehot编码】</li>
<li>Name:乘客姓名 【舍去】</li>
<li>Sex:乘客性别 【转换成bool特征】</li>
<li>Age:乘客年龄(有缺失) 【数值特征，添加“年龄是否缺失”作为辅助特征】</li>
<li>SibSp:乘客兄弟姐妹/配偶的个数(整数值) 【数值特征】</li>
<li>Parch:乘客父母/孩子的个数(整数值)【数值特征】</li>
<li>Ticket:票号(字符串)【舍去】</li>
<li>Fare:乘客所持票的价格(浮点数，0-500不等) 【数值特征】</li>
<li>Cabin:乘客所在船舱(有缺失) 【添加“所在船舱是否缺失”作为辅助特征】</li>
<li>Embarked:乘客登船港口:S、C、Q(有缺失)【转换成onehot编码，四维度 S,C,Q,nan】</li>
</ul>
<p>利用Pandas的数据可视化功能简单地进行探索性数据分析EDA（Exploratory Data Analysis）。</p>
<p>label分布情况</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'png'</span></span><br><span class="line">ax = dftrain_raw[<span class="string">'Survived'</span>].value_counts().plot(kind = <span class="string">'bar'</span>,</span><br><span class="line">     figsize = (<span class="number">12</span>,<span class="number">8</span>),fontsize=<span class="number">15</span>,rot = <span class="number">0</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Counts'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Survived'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-1-Label%E5%88%86%E5%B8%83.jpg" alt></p>
<p>年龄分布情况</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'png'</span></span><br><span class="line">ax = dftrain_raw[<span class="string">'Age'</span>].plot(kind = <span class="string">'hist'</span>,bins = <span class="number">20</span>,color= <span class="string">'purple'</span>,</span><br><span class="line">                    figsize = (<span class="number">12</span>,<span class="number">8</span>),fontsize=<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Age'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-1-%E5%B9%B4%E9%BE%84%E5%88%86%E5%B8%83.jpg" alt></p>
<p>年龄和label的相关性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'png'</span></span><br><span class="line">ax = dftrain_raw.query(<span class="string">'Survived == 0'</span>)[<span class="string">'Age'</span>].plot(kind = <span class="string">'density'</span>,</span><br><span class="line">                      figsize = (<span class="number">12</span>,<span class="number">8</span>),fontsize=<span class="number">15</span>)</span><br><span class="line">dftrain_raw.query(<span class="string">'Survived == 1'</span>)[<span class="string">'Age'</span>].plot(kind = <span class="string">'density'</span>,</span><br><span class="line">                      figsize = (<span class="number">12</span>,<span class="number">8</span>),fontsize=<span class="number">15</span>)</span><br><span class="line">ax.legend([<span class="string">'Survived==0'</span>,<span class="string">'Survived==1'</span>],fontsize = <span class="number">12</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Density'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Age'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-1-%E5%B9%B4%E9%BE%84%E7%9B%B8%E5%85%B3%E6%80%A7.jpg" alt></p>
<p>下面为正式的数据预处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocessing</span><span class="params">(dfdata)</span>:</span></span><br><span class="line"></span><br><span class="line">    dfresult= pd.DataFrame()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Pclass</span></span><br><span class="line">    dfPclass = pd.get_dummies(dfdata[<span class="string">'Pclass'</span>])</span><br><span class="line">    dfPclass.columns = [<span class="string">'Pclass_'</span> +str(x) <span class="keyword">for</span> x <span class="keyword">in</span> dfPclass.columns ]</span><br><span class="line">    dfresult = pd.concat([dfresult,dfPclass],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Sex</span></span><br><span class="line">    dfSex = pd.get_dummies(dfdata[<span class="string">'Sex'</span>])</span><br><span class="line">    dfresult = pd.concat([dfresult,dfSex],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Age</span></span><br><span class="line">    dfresult[<span class="string">'Age'</span>] = dfdata[<span class="string">'Age'</span>].fillna(<span class="number">0</span>)</span><br><span class="line">    dfresult[<span class="string">'Age_null'</span>] = pd.isna(dfdata[<span class="string">'Age'</span>]).astype(<span class="string">'int32'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#SibSp,Parch,Fare</span></span><br><span class="line">    dfresult[<span class="string">'SibSp'</span>] = dfdata[<span class="string">'SibSp'</span>]</span><br><span class="line">    dfresult[<span class="string">'Parch'</span>] = dfdata[<span class="string">'Parch'</span>]</span><br><span class="line">    dfresult[<span class="string">'Fare'</span>] = dfdata[<span class="string">'Fare'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Carbin</span></span><br><span class="line">    dfresult[<span class="string">'Cabin_null'</span>] =  pd.isna(dfdata[<span class="string">'Cabin'</span>]).astype(<span class="string">'int32'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Embarked</span></span><br><span class="line">    dfEmbarked = pd.get_dummies(dfdata[<span class="string">'Embarked'</span>],dummy_na=<span class="literal">True</span>)</span><br><span class="line">    dfEmbarked.columns = [<span class="string">'Embarked_'</span> + str(x) <span class="keyword">for</span> x <span class="keyword">in</span> dfEmbarked.columns]</span><br><span class="line">    dfresult = pd.concat([dfresult,dfEmbarked],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>(dfresult)</span><br><span class="line"></span><br><span class="line">x_train = preprocessing(dftrain_raw)</span><br><span class="line">y_train = dftrain_raw[<span class="string">'Survived'</span>].values</span><br><span class="line"></span><br><span class="line">x_test = preprocessing(dftest_raw)</span><br><span class="line">y_test = dftest_raw[<span class="string">'Survived'</span>].values</span><br><span class="line"></span><br><span class="line">print(<span class="string">"x_train.shape ="</span>, x_train.shape )</span><br><span class="line">print(<span class="string">"x_test.shape ="</span>, x_test.shape )</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">x_train.shape = (<span class="number">712</span>, <span class="number">15</span>)</span><br><span class="line">x_test.shape = (<span class="number">179</span>, <span class="number">15</span>)</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing">定义模型</h3>
<p>使用Keras接口有以下3种方式构建模型：</p>
<ol>
<li>使用Sequential按层顺序构建模型</li>
<li>使用函数式API构建任意结构模型</li>
<li>继承Model基类构建自定义模型。</li>
</ol>
<p>此处选择使用最简单的Sequential，按层顺序模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">20</span>,activation = <span class="string">'relu'</span>,input_shape=(<span class="number">15</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>,activation = <span class="string">'relu'</span> ))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>,activation = <span class="string">'sigmoid'</span> ))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">dense (Dense)                (None, 20)                320       </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_1 (Dense)              (None, 10)                210       </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_2 (Dense)              (None, 1)                 11        </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 541</span><br><span class="line">Trainable params: 541</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<h3 id="xun-lian-mo-xing">训练模型</h3>
<p>训练模型通常有3种方法:</p>
<ol>
<li>内置fit方法</li>
<li>内置train_on_batch方法</li>
<li>以及自定义训练循环</li>
</ol>
<p>此处选择最常用也最简单的内置fit方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二分类问题选择二元交叉熵损失函数</span></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">            loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'AUC'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train,y_train,</span><br><span class="line">                    batch_size= <span class="number">64</span>,</span><br><span class="line">                    epochs= <span class="number">30</span>,</span><br><span class="line">                    validation_split=<span class="number">0.2</span> <span class="comment">#分割一部分训练数据用于验证</span></span><br><span class="line">                   )</span><br></pre></td></tr></table></figure>
<h3 id="ping-gu-mo-xing">评估模型</h3>
<p>首先评估模型在训练集和验证集上的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">'val_'</span>+metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span>+ metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span>+metric, <span class="string">'val_'</span>+metric])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"loss"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-1-Loss%E6%9B%B2%E7%BA%BF.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"AUC"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-1-AUC%E6%9B%B2%E7%BA%BF.jpg" alt></p>
<p>再看一下模型在测试集上的效果.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate(x = x_test,y = y_test)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[<span class="number">0.5191367897907448</span>, <span class="number">0.8122605</span>]</span><br></pre></td></tr></table></figure>
<h3 id="shi-yong-mo-xing">使用模型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预测概率</span></span><br><span class="line">model.predict(x_test[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"><span class="comment">#model(tf.constant(x_test[0:10].values,dtype = tf.float32)) #等价写法</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">array([[<span class="number">0.26501188</span>],</span><br><span class="line">       [<span class="number">0.40970832</span>],</span><br><span class="line">       [<span class="number">0.44285864</span>],</span><br><span class="line">       [<span class="number">0.78408605</span>],</span><br><span class="line">       [<span class="number">0.47650957</span>],</span><br><span class="line">       [<span class="number">0.43849158</span>],</span><br><span class="line">       [<span class="number">0.27426785</span>],</span><br><span class="line">       [<span class="number">0.5962582</span> ],</span><br><span class="line">       [<span class="number">0.59476686</span>],</span><br><span class="line">       [<span class="number">0.17882936</span>]], dtype=float32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预测类别</span></span><br><span class="line">model.predict_classes(x_test[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">array([[<span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>],</span><br><span class="line">       [<span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>],</span><br><span class="line">       [<span class="number">0</span>]], dtype=int32)</span><br></pre></td></tr></table></figure>
<h3 id="bao-cun-mo-xing">保存模型</h3>
<p>可以使用Keras方式保存模型，也可以使用TensorFlow原生方式保存。前者仅仅适合使用Python环境恢复模型，后者则可以跨平台进行模型部署。</p>
<p>推荐使用后一种方式进行保存。</p>
<h4 id="keras-fang-shi-bao-cun">Keras方式保存</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构及权重</span></span><br><span class="line"></span><br><span class="line">model.save(<span class="string">'./data/keras_model.h5'</span>)  </span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> model  <span class="comment">#删除现有模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># identical to the previous one</span></span><br><span class="line">model = models.load_model(<span class="string">'./data/keras_model.h5'</span>)</span><br><span class="line">model.evaluate(x_test,y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[<span class="number">0.5191367897907448</span>, <span class="number">0.8122605</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构</span></span><br><span class="line">json_str = model.to_json()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复模型结构</span></span><br><span class="line">model_json = models.model_from_json(json_str)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#保存模型权重</span></span><br><span class="line">model.save_weights(<span class="string">'./data/keras_model_weight.h5'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复模型结构</span></span><br><span class="line">model_json = models.model_from_json(json_str)</span><br><span class="line">model_json.compile(</span><br><span class="line">        optimizer=<span class="string">'adam'</span>,</span><br><span class="line">        loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">        metrics=[<span class="string">'AUC'</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载权重</span></span><br><span class="line">model_json.load_weights(<span class="string">'./data/keras_model_weight.h5'</span>)</span><br><span class="line">model_json.evaluate(x_test,y_test)</span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">[<span class="number">0.5191367897907448</span>, <span class="number">0.8122605</span>]</span><br></pre></td></tr></table></figure>
<h4 id="tensor-flow-yuan-sheng-fang-shi-bao-cun">TensorFlow原生方式保存</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存权重，该方式仅仅保存权重张量</span></span><br><span class="line">model.save_weights(<span class="string">'./data/tf_model_weights.ckpt'</span>,save_format = <span class="string">"tf"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构与模型参数到文件,该方式保存的模型具有跨平台性便于部署</span></span><br><span class="line">model.save(<span class="string">'./data/tf_model_savedmodel'</span>, save_format=<span class="string">"tf"</span>)</span><br><span class="line">print(<span class="string">'export saved model.'</span>)</span><br><span class="line"></span><br><span class="line">model_loaded = tf.keras.models.load_model(<span class="string">'./data/tf_model_savedmodel'</span>)</span><br><span class="line">model_loaded.evaluate(x_test,y_test)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[<span class="number">0.5191365896656527</span>, <span class="number">0.8122605</span>]</span><br></pre></td></tr></table></figure>
<h2 id="tu-pian-shu-ju-jian-mo-liu-cheng">图片数据建模流程</h2>
<h3 id="zhun-bei-shu-ju-1">准备数据</h3>
<p>cifar2数据集为cifar10数据集的子集，只包括前两种类别airplane和automobile。</p>
<p>训练集有airplane和automobile图片各5000张，测试集有airplane和automobile图片各1000张。</p>
<p>cifar2任务的目标是训练一个模型来对飞机airplane和机动车automobile两种图片进行分类。</p>
<p><img src="/2020/05/07/tensorflow/tensorflow2/cifar2.jpg" alt></p>
<p>在tensorflow中准备图片数据的常用方案有两种</p>
<ol>
<li>
<p>第一种是使用tf.keras中的ImageDataGenerator工具构建图片数据生成器。</p>
</li>
<li>
<p>第二种是使用tf.data.Dataset搭配tf.image中的一些图片处理方法构建数据管道。</p>
</li>
</ol>
<p>第一种方法更为简单。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line">train_dir = <span class="string">'cifar2_datasets/train'</span></span><br><span class="line">test_dir = <span class="string">'cifar2_datasets/test'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对训练集数据设置数据增强</span></span><br><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">            rescale = <span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">            rotation_range=<span class="number">40</span>,</span><br><span class="line">            width_shift_range=<span class="number">0.2</span>,</span><br><span class="line">            height_shift_range=<span class="number">0.2</span>,</span><br><span class="line">            shear_range=<span class="number">0.2</span>,</span><br><span class="line">            zoom_range=<span class="number">0.2</span>,</span><br><span class="line">            horizontal_flip=<span class="literal">True</span>,</span><br><span class="line">            fill_mode=<span class="string">'nearest'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对测试集数据无需使用数据增强</span></span><br><span class="line">test_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">                    train_dir,</span><br><span class="line">                    target_size=(<span class="number">32</span>, <span class="number">32</span>),</span><br><span class="line">                    batch_size=<span class="number">32</span>,</span><br><span class="line">                    shuffle = <span class="literal">True</span>,</span><br><span class="line">                    class_mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">test_generator = test_datagen.flow_from_directory(</span><br><span class="line">                    test_dir,</span><br><span class="line">                    target_size=(<span class="number">32</span>, <span class="number">32</span>),</span><br><span class="line">                    batch_size=<span class="number">32</span>,</span><br><span class="line">                    shuffle = <span class="literal">False</span>,</span><br><span class="line">                    class_mode=<span class="string">'binary'</span>)</span><br></pre></td></tr></table></figure>
<p>第二种方法是TensorFlow的原生方法，更加灵活，使用得当的话也可以获得更好的性能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets,layers,models</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(img_path,size = <span class="params">(<span class="number">32</span>,<span class="number">32</span>)</span>)</span>:</span></span><br><span class="line">    label = tf.constant(<span class="number">1</span>,tf.int8) <span class="keyword">if</span> tf.strings.regex_full_match(img_path,<span class="string">".*automobile.*"</span>) \</span><br><span class="line">            <span class="keyword">else</span> tf.constant(<span class="number">0</span>,tf.int8)</span><br><span class="line">    img = tf.io.read_file(img_path)</span><br><span class="line">    img = tf.image.decode_jpeg(img) <span class="comment">#注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img,size)/<span class="number">255.0</span></span><br><span class="line">    <span class="keyword">return</span>(img,label)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用并行化预处理num_parallel_calls 和预存数据prefetch来提升性能</span></span><br><span class="line">ds_train = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/train/*/*.jpg"</span>) \</span><br><span class="line">           .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">           .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">           .prefetch(tf.data.experimental.AUTOTUNE)  </span><br><span class="line"></span><br><span class="line">ds_test = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/test/*/*.jpg"</span>) \</span><br><span class="line">           .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">           .batch(BATCH_SIZE) \</span><br><span class="line">           .prefetch(tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看部分样本</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>)) </span><br><span class="line"><span class="keyword">for</span> i,(img,label) <span class="keyword">in</span> enumerate(ds_train.unbatch().take(<span class="number">9</span>)):</span><br><span class="line">    ax=plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    ax.imshow(img.numpy())</span><br><span class="line">    ax.set_title(<span class="string">"label = %d"</span>%label)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([]) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-2-%E5%9B%BE%E7%89%87%E9%A2%84%E8%A7%88.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> ds_train.take(<span class="number">1</span>):</span><br><span class="line">    print(x.shape,y.shape)</span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">(<span class="number">100</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>) (<span class="number">100</span>,)</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing-1">定义模型</h3>
<p>使用Keras接口有以下3种方式构建模型：</p>
<ol>
<li>使用Sequential按层顺序构建模型</li>
<li>使用函数式API构建任意结构模型</li>
<li>继承Model基类构建自定义模型。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session() <span class="comment">#清空会话</span></span><br><span class="line"></span><br><span class="line">inputs = layers.Input(shape=(<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>))</span><br><span class="line">x = layers.Conv2D(<span class="number">32</span>,kernel_size=(<span class="number">3</span>,<span class="number">3</span>))(inputs)</span><br><span class="line">x = layers.MaxPool2D()(x)</span><br><span class="line">x = layers.Conv2D(<span class="number">64</span>,kernel_size=(<span class="number">5</span>,<span class="number">5</span>))(x)</span><br><span class="line">x = layers.MaxPool2D()(x)</span><br><span class="line">x = layers.Dropout(rate=<span class="number">0.1</span>)(x)</span><br><span class="line">x = layers.Flatten()(x)</span><br><span class="line">x = layers.Dense(<span class="number">32</span>,activation=<span class="string">'relu'</span>)(x)</span><br><span class="line">outputs = layers.Dense(<span class="number">1</span>,activation = <span class="string">'sigmoid'</span>)(x)</span><br><span class="line"></span><br><span class="line">model = models.Model(inputs = inputs,outputs = outputs)</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Model: "model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">input_1 (InputLayer)         [(None, 32, 32, 3)]       0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv2d (Conv2D)              (None, 30, 30, 32)        896       </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv2d_1 (Conv2D)            (None, 11, 11, 64)        51264     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max<span class="emphasis">_pooling2d_</span>1 (MaxPooling2 (None, 5, 5, 64)          0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dropout (Dropout)            (None, 5, 5, 64)          0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">flatten (Flatten)            (None, 1600)              0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 32)                51232     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_1 (Dense)              (None, 1)                 33        </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 103,425</span><br><span class="line">Trainable params: 103,425</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<h3 id="xun-lian-mo-xing-1">训练模型</h3>
<p>训练模型通常有3种方法，内置fit方法，内置train_on_batch方法，以及自定义训练循环。此处我们选择最常用也最简单的内置fit方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">stamp = datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">logdir = os.path.join(<span class="string">'data'</span>, <span class="string">'autograph'</span>, stamp)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 在 Python3 下建议使用 pathlib 修正各操作系统的路径</span></span><br><span class="line"><span class="comment"># from pathlib import Path</span></span><br><span class="line"><span class="comment"># stamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")</span></span><br><span class="line"><span class="comment"># logdir = str(Path('./data/autograph/' + stamp))</span></span><br><span class="line"></span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">model.compile(</span><br><span class="line">        optimizer=tf.keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>),</span><br><span class="line">        loss=tf.keras.losses.binary_crossentropy,</span><br><span class="line">        metrics=[<span class="string">"accuracy"</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">history = model.fit(ds_train,epochs= <span class="number">10</span>,validation_data=ds_test,</span><br><span class="line">                    callbacks = [tensorboard_callback],workers = <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h3 id="ping-gu-mo-xing-1">评估模型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%load_ext tensorboard</span><br><span class="line"><span class="comment">#%tensorboard --logdir ./data/keras_model</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorboard <span class="keyword">import</span> notebook</span><br><span class="line">notebook.list()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在tensorboard中查看模型</span></span><br><span class="line">notebook.start(<span class="string">"--logdir ./data/keras_model"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-2-tensorboard.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line">dfhistory = pd.DataFrame(history.history)</span><br><span class="line">dfhistory.index = range(<span class="number">1</span>,len(dfhistory) + <span class="number">1</span>)</span><br><span class="line">dfhistory.index.name = <span class="string">'epoch'</span></span><br><span class="line">dfhistory</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-2-dfhistory.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">'val_'</span>+metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span>+ metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span>+metric, <span class="string">'val_'</span>+metric])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"loss"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-2-Loss%E6%9B%B2%E7%BA%BF.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-2-Accuracy%E6%9B%B2%E7%BA%BF.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#可以使用evaluate对数据进行评估</span></span><br><span class="line">val_loss,val_accuracy = model.evaluate(ds_test,workers=<span class="number">4</span>)</span><br><span class="line">print(val_loss,val_accuracy)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="number">0.16139143370091916</span> <span class="number">0.9345</span></span><br></pre></td></tr></table></figure>
<h3 id="shi-yong-mo-xing-1">使用模型</h3>
<p>可以使用model.predict(ds_test)进行预测。</p>
<p>也可以使用model.predict_on_batch(x_test)对一个批量进行预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.predict(ds_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">array</span>([[<span class="number">9.9996173e-01</span>],</span><br><span class="line">       [<span class="number">9.5104784e-01</span>],</span><br><span class="line">       [<span class="number">2.8648047e-04</span>],</span><br><span class="line">       ...,</span><br><span class="line">       [<span class="number">1.1484033e-03</span>],</span><br><span class="line">       [<span class="number">3.5589080e-02</span>],</span><br><span class="line">       [<span class="number">9.8537153e-01</span>]], dtype=<span class="built_in">float</span>32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> ds_test.take(<span class="number">1</span>):</span><br><span class="line">    print(model.predict_on_batch(x[<span class="number">0</span>:<span class="number">20</span>]))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">3.8065155e-05</span>]</span><br><span class="line"> [<span class="number">8.8236779e-01</span>]</span><br><span class="line"> [<span class="number">9.1433197e-01</span>]</span><br><span class="line"> [<span class="number">9.9921846e-01</span>]</span><br><span class="line"> [<span class="number">6.4052093e-01</span>]</span><br><span class="line"> [<span class="number">4.9970779e-03</span>]</span><br><span class="line"> [<span class="number">2.6735585e-04</span>]</span><br><span class="line"> [<span class="number">9.9842811e-01</span>]</span><br><span class="line"> [<span class="number">7.9198682e-01</span>]</span><br><span class="line"> [<span class="number">7.4823302e-01</span>]</span><br><span class="line"> [<span class="number">8.7208226e-03</span>]</span><br><span class="line"> [<span class="number">9.3951421e-03</span>]</span><br><span class="line"> [<span class="number">9.9790359e-01</span>]</span><br><span class="line"> [<span class="number">9.9998581e-01</span>]</span><br><span class="line"> [<span class="number">2.1642199e-05</span>]</span><br><span class="line"> [<span class="number">1.7915063e-02</span>]</span><br><span class="line"> [<span class="number">2.5839690e-02</span>]</span><br><span class="line"> [<span class="number">9.7538447e-01</span>]</span><br><span class="line"> [<span class="number">9.7393811e-01</span>]</span><br><span class="line"> [<span class="number">9.7333014e-01</span>]], shape=(<span class="number">20</span>, <span class="number">1</span>), dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="bao-cun-mo-xing-1">保存模型</h3>
<p>推荐使用TensorFlow原生方式保存模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存权重，该方式仅仅保存权重张量</span></span><br><span class="line">model.save_weights(<span class="string">'./data/tf_model_weights.ckpt'</span>,save_format = <span class="string">"tf"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构与模型参数到文件,该方式保存的模型具有跨平台性便于部署</span></span><br><span class="line">model.save(<span class="string">'./data/tf_model_savedmodel'</span>, save_format=<span class="string">"tf"</span>)</span><br><span class="line">print(<span class="string">'export saved model.'</span>)</span><br><span class="line"></span><br><span class="line">model_loaded = tf.keras.models.load_model(<span class="string">'./data/tf_model_savedmodel'</span>)</span><br><span class="line">model_loaded.evaluate(ds_test)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[<span class="number">0.16139124035835267</span>, <span class="number">0.9345</span>]</span><br></pre></td></tr></table></figure>
<h2 id="wen-ben-shu-ju-jian-mo-liu-cheng">文本数据建模流程</h2>
<h3 id="zhun-bei-shu-ju-2">准备数据</h3>
<p>imdb数据集的目标是根据电影评论的文本内容预测评论的情感标签。</p>
<p>训练集有20000条电影评论文本，测试集有5000条电影评论文本，其中正面评论和负面评论都各占一半。</p>
<p>文本数据预处理较为繁琐，包括中文切词，构建词典，编码转换，序列填充，构建数据管道等等。</p>
<p>在tensorflow中完成文本数据预处理的常用方案有两种</p>
<ol>
<li>第一种是利用tf.keras.preprocessing中的Tokenizer词典构建工具和tf.keras.utils.Sequence构建文本数据生成器管道。</li>
<li>第二种是使用tf.data.Dataset搭配.keras.layers.experimental.preprocessing.TextVectorization预处理层。</li>
</ol>
<p>第一种方法较为复杂，其使用范例可以参考以下代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm </span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集路径</span></span><br><span class="line">train_data_path = <span class="string">'imdb_datasets/xx_train_imdb'</span></span><br><span class="line">test_data_path = <span class="string">'imdb_datasets/xx_test_imdb'</span></span><br><span class="line"></span><br><span class="line">train_samples = <span class="number">20000</span> <span class="comment">#训练集样本数量</span></span><br><span class="line">test_samples = <span class="number">5000</span> <span class="comment">#测试集样本数量</span></span><br><span class="line"></span><br><span class="line">max_words = <span class="number">10000</span>  <span class="comment"># 保留词频最高的前10000个词</span></span><br><span class="line">maxlen = <span class="number">500</span>       <span class="comment"># 每个样本文本内容最多保留500个词</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建训练集文本生成器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">texts_gen</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(train_data_path,<span class="string">'r'</span>,encoding = <span class="string">'utf-8'</span>) <span class="keyword">as</span> f,\</span><br><span class="line">    tqdm(total = train_samples) <span class="keyword">as</span> pbar:      </span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            text = (f.readline().rstrip(<span class="string">'\n'</span>).split(<span class="string">'\t'</span>)[<span class="number">-1</span>])</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> text:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> len(text) &gt; maxlen:</span><br><span class="line">                text = text[<span class="number">0</span>:maxlen]</span><br><span class="line">            pbar.update(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">yield</span> text</span><br><span class="line"></span><br><span class="line">texts = texts_gen()</span><br><span class="line">tokenizer = Tokenizer(num_words=max_words)</span><br><span class="line">tokenizer.fit_on_texts(texts)</span><br></pre></td></tr></table></figure>
<p>分割样本</p>
<p>为了能够像ImageDataGenerator那样用数据管道多进程并行地读取数据，需要将数据集按样本分割成<strong>多个文件</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">scatter_train_data_path = <span class="string">'imdb_datasets/train/'</span></span><br><span class="line">scatter_test_data_path = <span class="string">'imdb_datasets/test/'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据按样本打散到多个文件</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scatter_data</span><span class="params">(data_file, scatter_data_path)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(scatter_data_path):</span><br><span class="line">        os.makedirs(scatter_data_path)</span><br><span class="line">    <span class="keyword">for</span> idx,line <span class="keyword">in</span> tqdm(enumerate(open(data_file,<span class="string">'r'</span>,encoding = <span class="string">'utf-8'</span>))):</span><br><span class="line">        <span class="keyword">with</span> open(scatter_data_path + str(idx) + <span class="string">'.txt'</span>,<span class="string">'w'</span>,encoding = <span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">             f.write(line)</span><br><span class="line"></span><br><span class="line">scatter_data(train_data_path,scatter_train_data_path)</span><br><span class="line">scatter_data(test_data_path,scatter_test_data_path)</span><br></pre></td></tr></table></figure>
<p>第二种方法为TensorFlow原生方式，相对也更加简单一些。</p>
<p><img src="/2020/05/07/tensorflow/tensorflow2/%E7%94%B5%E5%BD%B1%E8%AF%84%E8%AE%BA.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,preprocessing,optimizers,losses,metrics</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers.experimental.preprocessing <span class="keyword">import</span> TextVectorization</span><br><span class="line"><span class="keyword">import</span> re,string</span><br><span class="line"></span><br><span class="line">train_data_path = <span class="string">"./data/imdb/train.csv"</span></span><br><span class="line">test_data_path =  <span class="string">"./data/imdb/test.csv"</span></span><br><span class="line"></span><br><span class="line">MAX_WORDS = <span class="number">10000</span>  <span class="comment"># 仅考虑最高频的10000个词</span></span><br><span class="line">MAX_LEN = <span class="number">200</span>  <span class="comment"># 每个样本保留200个词的长度</span></span><br><span class="line">BATCH_SIZE = <span class="number">20</span> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#构建管道</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_line</span><span class="params">(line)</span>:</span></span><br><span class="line">    arr = tf.strings.split(line,<span class="string">"\t"</span>)</span><br><span class="line">    label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[<span class="number">0</span>]),tf.int32),axis = <span class="number">0</span>)</span><br><span class="line">    text = tf.expand_dims(arr[<span class="number">1</span>],axis = <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> (text,label)</span><br><span class="line"></span><br><span class="line">ds_train_raw =  tf.data.TextLineDataset(filenames = [train_data_path]) \</span><br><span class="line">   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \</span><br><span class="line">   .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">   .prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">ds_test_raw = tf.data.TextLineDataset(filenames = [test_data_path]) \</span><br><span class="line">   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \</span><br><span class="line">   .batch(BATCH_SIZE) \</span><br><span class="line">   .prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#构建词典</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_text</span><span class="params">(text)</span>:</span></span><br><span class="line">    lowercase = tf.strings.lower(text)</span><br><span class="line">    stripped_html = tf.strings.regex_replace(lowercase, <span class="string">'&lt;br /&gt;'</span>, <span class="string">' '</span>)</span><br><span class="line">    cleaned_punctuation = tf.strings.regex_replace(stripped_html,<span class="string">'[%s]'</span> % re.escape(string.punctuation),<span class="string">''</span>)</span><br><span class="line">    <span class="keyword">return</span> cleaned_punctuation</span><br><span class="line"></span><br><span class="line">vectorize_layer = TextVectorization(</span><br><span class="line">    standardize=clean_text,</span><br><span class="line">    split = <span class="string">'whitespace'</span>,</span><br><span class="line">    max_tokens=MAX_WORDS<span class="number">-1</span>, <span class="comment">#有一个留给占位符</span></span><br><span class="line">    output_mode=<span class="string">'int'</span>,</span><br><span class="line">    output_sequence_length=MAX_LEN)</span><br><span class="line"></span><br><span class="line">ds_text = ds_train_raw.map(<span class="keyword">lambda</span> text,label: text)</span><br><span class="line">vectorize_layer.adapt(ds_text)</span><br><span class="line">print(vectorize_layer.get_vocabulary()[<span class="number">0</span>:<span class="number">100</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#单词编码</span></span><br><span class="line">ds_train = ds_train_raw.map(<span class="keyword">lambda</span> text,label:(vectorize_layer(text),label)) \</span><br><span class="line">    .prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line">ds_test = ds_test_raw.map(<span class="keyword">lambda</span> text,label:(vectorize_layer(text),label)) \</span><br><span class="line">    .prefetch(tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="name">b</span><span class="symbol">'the</span>', b<span class="symbol">'and</span>', b<span class="symbol">'a</span>', b<span class="symbol">'of</span>', b<span class="symbol">'to</span>', b<span class="symbol">'is</span>', b<span class="symbol">'in</span>', b<span class="symbol">'it</span>', b<span class="symbol">'i</span>', b<span class="symbol">'this</span>', b<span class="symbol">'that</span>', b<span class="symbol">'was</span>', b<span class="symbol">'as</span>', b<span class="symbol">'for</span>', b<span class="symbol">'with</span>', b<span class="symbol">'movie</span>', b<span class="symbol">'but</span>', b<span class="symbol">'film</span>', b<span class="symbol">'on</span>', b<span class="symbol">'not</span>', b<span class="symbol">'you</span>', b<span class="symbol">'his</span>', b<span class="symbol">'are</span>', b<span class="symbol">'have</span>', b<span class="symbol">'be</span>', b<span class="symbol">'he</span>', b<span class="symbol">'one</span>', b<span class="symbol">'its</span>', b<span class="symbol">'at</span>', b<span class="symbol">'all</span>', b<span class="symbol">'by</span>', b<span class="symbol">'an</span>', b<span class="symbol">'they</span>', b<span class="symbol">'from</span>', b<span class="symbol">'who</span>', b<span class="symbol">'so</span>', b<span class="symbol">'like</span>', b<span class="symbol">'her</span>', b<span class="symbol">'just</span>', b<span class="symbol">'or</span>', b<span class="symbol">'about</span>', b<span class="symbol">'has</span>', b<span class="symbol">'if</span>', b<span class="symbol">'out</span>', b<span class="symbol">'some</span>', b<span class="symbol">'there</span>', b<span class="symbol">'what</span>', b<span class="symbol">'good</span>', b<span class="symbol">'more</span>', b<span class="symbol">'when</span>', b<span class="symbol">'very</span>', b<span class="symbol">'she</span>', b<span class="symbol">'even</span>', b<span class="symbol">'my</span>', b<span class="symbol">'no</span>', b<span class="symbol">'would</span>', b<span class="symbol">'up</span>', b<span class="symbol">'time</span>', b<span class="symbol">'only</span>', b<span class="symbol">'which</span>', b<span class="symbol">'story</span>', b<span class="symbol">'really</span>', b<span class="symbol">'their</span>', b<span class="symbol">'were</span>', b<span class="symbol">'had</span>', b<span class="symbol">'see</span>', b<span class="symbol">'can</span>', b<span class="symbol">'me</span>', b<span class="symbol">'than</span>', b<span class="symbol">'we</span>', b<span class="symbol">'much</span>', b<span class="symbol">'well</span>', b<span class="symbol">'get</span>', b<span class="symbol">'been</span>', b<span class="symbol">'will</span>', b<span class="symbol">'into</span>', b<span class="symbol">'people</span>', b<span class="symbol">'also</span>', b<span class="symbol">'other</span>', b<span class="symbol">'do</span>', b<span class="symbol">'bad</span>', b<span class="symbol">'because</span>', b<span class="symbol">'great</span>', b<span class="symbol">'first</span>', b<span class="symbol">'how</span>', b<span class="symbol">'him</span>', b<span class="symbol">'most</span>', b<span class="symbol">'dont</span>', b<span class="symbol">'made</span>', b<span class="symbol">'then</span>', b<span class="symbol">'them</span>', b<span class="symbol">'films</span>', b<span class="symbol">'movies</span>', b<span class="symbol">'way</span>', b<span class="symbol">'make</span>', b<span class="symbol">'could</span>', b<span class="symbol">'too</span>', b<span class="symbol">'any</span>', b<span class="symbol">'after</span>', b<span class="symbol">'characters</span>']</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing-2">定义模型</h3>
<p>使用Keras接口有以下3种方式构建模型：</p>
<ol>
<li>使用Sequential按层顺序构建模型</li>
<li>使用函数式API构建任意结构模型</li>
<li>继承Model基类构建自定义模型。</li>
</ol>
<p>此处选择使用继承Model基类构建自定义模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 演示自定义模型范例，实际上应该优先使用Sequential或者函数式API</span></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CnnModel</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(CnnModel, self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self,input_shape)</span>:</span></span><br><span class="line">        self.embedding = layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN)</span><br><span class="line">        self.conv_1 = layers.Conv1D(<span class="number">16</span>, kernel_size= <span class="number">5</span>,name = <span class="string">"conv_1"</span>,activation = <span class="string">"relu"</span>)</span><br><span class="line">        self.pool_1 = layers.MaxPool1D(name = <span class="string">"pool_1"</span>)</span><br><span class="line">        self.conv_2 = layers.Conv1D(<span class="number">128</span>, kernel_size=<span class="number">2</span>,name = <span class="string">"conv_2"</span>,activation = <span class="string">"relu"</span>)</span><br><span class="line">        self.pool_2 = layers.MaxPool1D(name = <span class="string">"pool_2"</span>)</span><br><span class="line">        self.flatten = layers.Flatten()</span><br><span class="line">        self.dense = layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">        super(CnnModel,self).build(input_shape)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x = self.conv_1(x)</span><br><span class="line">        x = self.pool_1(x)</span><br><span class="line">        x = self.conv_2(x)</span><br><span class="line">        x = self.pool_2(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.dense(x)</span><br><span class="line">        <span class="keyword">return</span>(x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用于显示Output Shape</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">summary</span><span class="params">(self)</span>:</span></span><br><span class="line">        x_input = layers.Input(shape = MAX_LEN)</span><br><span class="line">        output = self.call(x_input)</span><br><span class="line">        model = tf.keras.Model(inputs = x_input,outputs = output)</span><br><span class="line">        model.summary()</span><br><span class="line">    </span><br><span class="line">model = CnnModel()</span><br><span class="line">model.build(input_shape =(<span class="literal">None</span>,MAX_LEN))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Model: "model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">input_1 (InputLayer)         [(None, 200)]             0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">embedding (Embedding)        (None, 200, 7)            70000     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv_1 (Conv1D)              (None, 196, 16)           576       </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">pool_1 (MaxPooling1D)        (None, 98, 16)            0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv_2 (Conv1D)              (None, 97, 128)           4224      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">pool_2 (MaxPooling1D)        (None, 48, 128)           0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">flatten (Flatten)            (None, 6144)              0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 1)                 6145      </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 80,945</span><br><span class="line">Trainable params: 80,945</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<h3 id="xun-lian-mo-xing-2">训练模型</h3>
<p>训练模型通常有3种方法：</p>
<ol>
<li>内置fit方法</li>
<li>内置train_on_batch方法</li>
<li>以及自定义训练循环</li>
</ol>
<p>此处通过自定义训练循环训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line">    </span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optimizers.Nadam()</span><br><span class="line">loss_func = losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">train_loss = metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_metric = metrics.BinaryAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">valid_loss = metrics.Mean(name=<span class="string">'valid_loss'</span>)</span><br><span class="line">valid_metric = metrics.BinaryAccuracy(name=<span class="string">'valid_accuracy'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features,training = <span class="literal">True</span>)</span><br><span class="line">        loss = loss_func(labels, predictions)</span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss.update_state(loss)</span><br><span class="line">    train_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">valid_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    predictions = model(features,training = <span class="literal">False</span>)</span><br><span class="line">    batch_loss = loss_func(labels, predictions)</span><br><span class="line">    valid_loss.update_state(batch_loss)</span><br><span class="line">    valid_metric.update_state(labels, predictions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,ds_train,ds_valid,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_train:</span><br><span class="line">            train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_valid:</span><br><span class="line">            valid_step(model,features,labels)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#此处logs模板需要根据metric具体情况修改</span></span><br><span class="line">        logs = <span class="string">'Epoch=&#123;&#125;,Loss:&#123;&#125;,Accuracy:&#123;&#125;,Valid Loss:&#123;&#125;,Valid Accuracy:&#123;&#125;'</span> </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">1</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(tf.strings.format(logs,</span><br><span class="line">            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))</span><br><span class="line">            tf.print(<span class="string">""</span>)</span><br><span class="line">        </span><br><span class="line">        train_loss.reset_states()</span><br><span class="line">        valid_loss.reset_states()</span><br><span class="line">        train_metric.reset_states()</span><br><span class="line">        valid_metric.reset_states()</span><br><span class="line"></span><br><span class="line">train_model(model,ds_train,ds_test,epochs = <span class="number">6</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">13</span>:<span class="number">54</span>:<span class="number">08</span></span><br><span class="line">Epoch=<span class="number">1</span>,Loss:<span class="number">0.442317516</span>,Accuracy:<span class="number">0.7695</span>,Valid Loss:<span class="number">0.323672801</span>,Valid Accuracy:<span class="number">0.8614</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">54</span>:<span class="number">20</span></span><br><span class="line">Epoch=<span class="number">2</span>,Loss:<span class="number">0.245737702</span>,Accuracy:<span class="number">0.90215</span>,Valid Loss:<span class="number">0.356488883</span>,Valid Accuracy:<span class="number">0.8554</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">54</span>:<span class="number">32</span></span><br><span class="line">Epoch=<span class="number">3</span>,Loss:<span class="number">0.17360799</span>,Accuracy:<span class="number">0.93455</span>,Valid Loss:<span class="number">0.361132562</span>,Valid Accuracy:<span class="number">0.8674</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">54</span>:<span class="number">44</span></span><br><span class="line">Epoch=<span class="number">4</span>,Loss:<span class="number">0.113476314</span>,Accuracy:<span class="number">0.95975</span>,Valid Loss:<span class="number">0.483677238</span>,Valid Accuracy:<span class="number">0.856</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">54</span>:<span class="number">57</span></span><br><span class="line">Epoch=<span class="number">5</span>,Loss:<span class="number">0.0698405355</span>,Accuracy:<span class="number">0.9768</span>,Valid Loss:<span class="number">0.607856631</span>,Valid Accuracy:<span class="number">0.857</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">55</span>:<span class="number">15</span></span><br><span class="line">Epoch=<span class="number">6</span>,Loss:<span class="number">0.0366807655</span>,Accuracy:<span class="number">0.98825</span>,Valid Loss:<span class="number">0.745884955</span>,Valid Accuracy:<span class="number">0.854</span></span><br></pre></td></tr></table></figure>
<h3 id="ping-gu-mo-xing-2">评估模型</h3>
<p>通过自定义训练循环训练的模型没有经过编译，无法直接使用model.evaluate(ds_valid)方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_model</span><span class="params">(model,ds_valid)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_valid:</span><br><span class="line">         valid_step(model,features,labels)</span><br><span class="line">    logs = <span class="string">'Valid Loss:&#123;&#125;,Valid Accuracy:&#123;&#125;'</span> </span><br><span class="line">    tf.print(tf.strings.format(logs,(valid_loss.result(),valid_metric.result())))</span><br><span class="line">    </span><br><span class="line">    valid_loss.reset_states()</span><br><span class="line">    train_metric.reset_states()</span><br><span class="line">    valid_metric.reset_states()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">evaluate_model(model,ds_test)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">Valid Loss:<span class="number">0.745884418</span>,Valid Accuracy:<span class="number">0.854</span></span><br></pre></td></tr></table></figure>
<h3 id="shi-yong-mo-xing-2">使用模型</h3>
<p>可以使用以下方法:</p>
<ul>
<li>model.predict(ds_test)</li>
<li>model(x_test)</li>
<li>model.call(x_test)</li>
<li>model.predict_on_batch(x_test)</li>
</ul>
<p>推荐优先使用model.predict(ds_test)方法，既可以对Dataset，也可以对Tensor使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.predict(ds_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">array</span>([[<span class="number">0.7864823</span> ],</span><br><span class="line">       [<span class="number">0.9999901</span> ],</span><br><span class="line">       [<span class="number">0.99944776</span>],</span><br><span class="line">       ...,</span><br><span class="line">       [<span class="number">0.8498302</span> ],</span><br><span class="line">       [<span class="number">0.13382755</span>],</span><br><span class="line">       [<span class="number">1.</span>        ]], dtype=<span class="built_in">float</span>32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x_test,_ <span class="keyword">in</span> ds_test.take(<span class="number">1</span>):</span><br><span class="line">    print(model(x_test))</span><br><span class="line">    <span class="comment">#以下方法等价：</span></span><br><span class="line">    <span class="comment">#print(model.call(x_test))</span></span><br><span class="line">    <span class="comment">#print(model.predict_on_batch(x_test))</span></span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">7.8648227e-01</span>]</span><br><span class="line"> [<span class="number">9.9999011e-01</span>]</span><br><span class="line"> [<span class="number">9.9944776e-01</span>]</span><br><span class="line"> [<span class="number">3.7153201e-09</span>]</span><br><span class="line"> [<span class="number">9.4462049e-01</span>]</span><br><span class="line"> [<span class="number">2.3522753e-04</span>]</span><br><span class="line"> [<span class="number">1.2044354e-04</span>]</span><br><span class="line"> [<span class="number">9.3752089e-07</span>]</span><br><span class="line"> [<span class="number">9.9996352e-01</span>]</span><br><span class="line"> [<span class="number">9.3435925e-01</span>]</span><br><span class="line"> [<span class="number">9.8746723e-01</span>]</span><br><span class="line"> [<span class="number">9.9908626e-01</span>]</span><br><span class="line"> [<span class="number">4.1563155e-08</span>]</span><br><span class="line"> [<span class="number">4.1808244e-03</span>]</span><br><span class="line"> [<span class="number">8.0184749e-05</span>]</span><br><span class="line"> [<span class="number">8.3910513e-01</span>]</span><br><span class="line"> [<span class="number">3.5167937e-05</span>]</span><br><span class="line"> [<span class="number">7.2113985e-01</span>]</span><br><span class="line"> [<span class="number">4.5228912e-03</span>]</span><br><span class="line"> [<span class="number">9.9942589e-01</span>]], shape=(<span class="number">20</span>, <span class="number">1</span>), dtype=<span class="built_in">float</span>32)</span><br></pre></td></tr></table></figure>
<h3 id="bao-cun-mo-xing-2">保存模型</h3>
<p>推荐使用TensorFlow原生方式保存模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model.save(<span class="string">'./data/tf_model_savedmodel'</span>, save_format=<span class="string">"tf"</span>)</span><br><span class="line">print(<span class="string">'export saved model.'</span>)</span><br><span class="line"></span><br><span class="line">model_loaded = tf.keras.models.load_model(<span class="string">'./data/tf_model_savedmodel'</span>)</span><br><span class="line">model_loaded.predict(ds_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">array([[<span class="number">0.7864823</span> ],</span><br><span class="line">       [<span class="number">0.9999901</span> ],</span><br><span class="line">       [<span class="number">0.99944776</span>],</span><br><span class="line">       ...,</span><br><span class="line">       [<span class="number">0.8498302</span> ],</span><br><span class="line">       [<span class="number">0.13382755</span>],</span><br><span class="line">       [<span class="number">1.</span>        ]], dtype=float32)</span><br></pre></td></tr></table></figure>
<h2 id="shi-jian-xu-lie-shu-ju-jian-mo-liu-cheng">时间序列数据建模流程</h2>
<h3 id="zhun-bei-shu-ju-3">准备数据</h3>
<p>数据<a href="https://github.com/BlankerL/DXY-COVID-19-Data/tree/master/json" target="_blank" rel="noopener">获取</a></p>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-4-%E6%96%B0%E5%A2%9E%E4%BA%BA%E6%95%B0.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,losses,metrics,callbacks</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">"./data/covid-19.csv"</span>,sep = <span class="string">"\t"</span>)</span><br><span class="line">df.plot(x = <span class="string">"date"</span>,y = [<span class="string">"confirmed_num"</span>,<span class="string">"cured_num"</span>,<span class="string">"dead_num"</span>],figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">plt.xticks(rotation=<span class="number">60</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-4-%E7%B4%AF%E7%A7%AF%E6%9B%B2%E7%BA%BF.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dfdata = df.set_index(<span class="string">"date"</span>)</span><br><span class="line">dfdiff = dfdata.diff(periods=<span class="number">1</span>).dropna()</span><br><span class="line">dfdiff = dfdiff.reset_index(<span class="string">"date"</span>)</span><br><span class="line"></span><br><span class="line">dfdiff.plot(x = <span class="string">"date"</span>,y = [<span class="string">"confirmed_num"</span>,<span class="string">"cured_num"</span>,<span class="string">"dead_num"</span>],figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">plt.xticks(rotation=<span class="number">60</span>)</span><br><span class="line">dfdiff = dfdiff.drop(<span class="string">"date"</span>,axis = <span class="number">1</span>).astype(<span class="string">"float32"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-4-%E6%96%B0%E5%A2%9E%E6%9B%B2%E7%BA%BF.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#用某日前8天窗口数据作为输入预测该日数据</span></span><br><span class="line">WINDOW_SIZE = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_dataset</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    dataset_batched = dataset.batch(WINDOW_SIZE,drop_remainder=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> dataset_batched</span><br><span class="line"></span><br><span class="line">ds_data = tf.data.Dataset.from_tensor_slices(tf.constant(dfdiff.values,dtype = tf.float32)) \</span><br><span class="line">   .window(WINDOW_SIZE,shift=<span class="number">1</span>).flat_map(batch_dataset)</span><br><span class="line"></span><br><span class="line">ds_label = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    tf.constant(dfdiff.values[WINDOW_SIZE:],dtype = tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据较小，可以将全部训练数据放入到一个batch中，提升性能</span></span><br><span class="line">ds_train = tf.data.Dataset.zip((ds_data,ds_label)).batch(<span class="number">38</span>).cache()</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing-3">定义模型</h3>
<p>使用Keras接口有以下3种方式构建模型：</p>
<ol>
<li>使用Sequential按层顺序构建模型</li>
<li>使用函数式API构建任意结构模型</li>
<li>继承Model基类构建自定义模型</li>
</ol>
<p>此处选择使用函数式API构建任意结构模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#考虑到新增确诊，新增治愈，新增死亡人数数据不可能小于0，设计如下结构</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span><span class="params">(layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(Block, self).__init__(**kwargs)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x_input,x)</span>:</span></span><br><span class="line">        x_out = tf.maximum((<span class="number">1</span>+x)*x_input[:,<span class="number">-1</span>,:],<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">return</span> x_out</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        config = super(Block, self).get_config()</span><br><span class="line">        <span class="keyword">return</span> config</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line">x_input = layers.Input(shape = (<span class="literal">None</span>,<span class="number">3</span>),dtype = tf.float32)</span><br><span class="line">x = layers.LSTM(<span class="number">3</span>,return_sequences = <span class="literal">True</span>,input_shape=(<span class="literal">None</span>,<span class="number">3</span>))(x_input)</span><br><span class="line">x = layers.LSTM(<span class="number">3</span>,return_sequences = <span class="literal">True</span>,input_shape=(<span class="literal">None</span>,<span class="number">3</span>))(x)</span><br><span class="line">x = layers.LSTM(<span class="number">3</span>,return_sequences = <span class="literal">True</span>,input_shape=(<span class="literal">None</span>,<span class="number">3</span>))(x)</span><br><span class="line">x = layers.LSTM(<span class="number">3</span>,input_shape=(<span class="literal">None</span>,<span class="number">3</span>))(x)</span><br><span class="line">x = layers.Dense(<span class="number">3</span>)(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#考虑到新增确诊，新增治愈，新增死亡人数数据不可能小于0，设计如下结构</span></span><br><span class="line"><span class="comment">#x = tf.maximum((1+x)*x_input[:,-1,:],0.0)</span></span><br><span class="line">x = Block()(x_input,x)</span><br><span class="line">model = models.Model(inputs = [x_input],outputs = [x])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Model: "model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">input_1 (InputLayer)         [(None, None, 3)]         0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">lstm (LSTM)                  (None, None, 3)           84        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">lstm_1 (LSTM)                (None, None, 3)           84        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">lstm_2 (LSTM)                (None, None, 3)           84        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">lstm_3 (LSTM)                (None, 3)                 84        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 3)                 12        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">block (Block)                (None, 3)                 0         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 348</span><br><span class="line">Trainable params: 348</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<h3 id="xun-lian-mo-xing-3">训练模型</h3>
<p>训练模型通常有3种方法</p>
<ol>
<li>内置fit方法</li>
<li>内置train_on_batch方法</li>
<li>以及自定义训练循环</li>
</ol>
<p>此处我们选择最常用也最简单的内置fit方法。</p>
<blockquote>
<p>注：循环神经网络调试较为困难，需要设置多个不同的学习率多次尝试，以取得较好的效果。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#自定义损失函数，考虑平方差和预测目标的比值</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MSPE</span><span class="params">(losses.Loss)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self,y_true,y_pred)</span>:</span></span><br><span class="line">        err_percent = (y_true - y_pred)**<span class="number">2</span>/(tf.maximum(y_true**<span class="number">2</span>,<span class="number">1e-7</span>))</span><br><span class="line">        mean_err_percent = tf.reduce_mean(err_percent)</span><br><span class="line">        <span class="keyword">return</span> mean_err_percent</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span></span><br><span class="line">        config = super(MSPE, self).get_config()</span><br><span class="line">        <span class="keyword">return</span> config</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.Adam(learning_rate=<span class="number">0.01</span>)</span><br><span class="line">model.compile(optimizer=optimizer,loss=MSPE(name = <span class="string">"MSPE"</span>))</span><br><span class="line"></span><br><span class="line">stamp = datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">logdir = os.path.join(<span class="string">'data'</span>, <span class="string">'autograph'</span>, stamp)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 在 Python3 下建议使用 pathlib 修正各操作系统的路径</span></span><br><span class="line"><span class="comment"># from pathlib import Path</span></span><br><span class="line"><span class="comment"># stamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")</span></span><br><span class="line"><span class="comment"># logdir = str(Path('./data/autograph/' + stamp))</span></span><br><span class="line"></span><br><span class="line">tb_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#如果loss在100个epoch后没有提升，学习率减半。</span></span><br><span class="line">lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">"loss"</span>,factor = <span class="number">0.5</span>, patience = <span class="number">100</span>)</span><br><span class="line"><span class="comment">#当loss在200个epoch后没有提升，则提前终止训练。</span></span><br><span class="line">stop_callback = tf.keras.callbacks.EarlyStopping(monitor = <span class="string">"loss"</span>, patience= <span class="number">200</span>)</span><br><span class="line">callbacks_list = [tb_callback,lr_callback,stop_callback]</span><br><span class="line"></span><br><span class="line">history = model.fit(ds_train,epochs=<span class="number">500</span>,callbacks = callbacks_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">371</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">61</span>ms/step - loss: <span class="number">0.1184</span></span><br><span class="line">Epoch <span class="number">372</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">64</span>ms/step - loss: <span class="number">0.1177</span></span><br><span class="line">Epoch <span class="number">373</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.1169</span></span><br><span class="line">Epoch <span class="number">374</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">50</span>ms/step - loss: <span class="number">0.1161</span></span><br><span class="line">Epoch <span class="number">375</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.1154</span></span><br><span class="line">Epoch <span class="number">376</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.1147</span></span><br><span class="line">Epoch <span class="number">377</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">62</span>ms/step - loss: <span class="number">0.1140</span></span><br><span class="line">Epoch <span class="number">378</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">93</span>ms/step - loss: <span class="number">0.1133</span></span><br><span class="line">Epoch <span class="number">379</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">85</span>ms/step - loss: <span class="number">0.1126</span></span><br><span class="line">Epoch <span class="number">380</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">68</span>ms/step - loss: <span class="number">0.1119</span></span><br><span class="line">Epoch <span class="number">381</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">52</span>ms/step - loss: <span class="number">0.1113</span></span><br><span class="line">Epoch <span class="number">382</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">54</span>ms/step - loss: <span class="number">0.1107</span></span><br><span class="line">Epoch <span class="number">383</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.1100</span></span><br><span class="line">Epoch <span class="number">384</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.1094</span></span><br><span class="line">Epoch <span class="number">385</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">54</span>ms/step - loss: <span class="number">0.1088</span></span><br><span class="line">Epoch <span class="number">386</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">74</span>ms/step - loss: <span class="number">0.1082</span></span><br><span class="line">Epoch <span class="number">387</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.1077</span></span><br><span class="line">Epoch <span class="number">388</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">52</span>ms/step - loss: <span class="number">0.1071</span></span><br><span class="line">Epoch <span class="number">389</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">52</span>ms/step - loss: <span class="number">0.1066</span></span><br><span class="line">Epoch <span class="number">390</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.1060</span></span><br><span class="line">Epoch <span class="number">391</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">61</span>ms/step - loss: <span class="number">0.1055</span></span><br><span class="line">Epoch <span class="number">392</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.1050</span></span><br><span class="line">Epoch <span class="number">393</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">59</span>ms/step - loss: <span class="number">0.1045</span></span><br><span class="line">Epoch <span class="number">394</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">65</span>ms/step - loss: <span class="number">0.1040</span></span><br><span class="line">Epoch <span class="number">395</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">58</span>ms/step - loss: <span class="number">0.1035</span></span><br><span class="line">Epoch <span class="number">396</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">52</span>ms/step - loss: <span class="number">0.1031</span></span><br><span class="line">Epoch <span class="number">397</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">58</span>ms/step - loss: <span class="number">0.1026</span></span><br><span class="line">Epoch <span class="number">398</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.1022</span></span><br><span class="line">Epoch <span class="number">399</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.1017</span></span><br><span class="line">Epoch <span class="number">400</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">63</span>ms/step - loss: <span class="number">0.1013</span></span><br><span class="line">Epoch <span class="number">401</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">59</span>ms/step - loss: <span class="number">0.1009</span></span><br><span class="line">Epoch <span class="number">402</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">53</span>ms/step - loss: <span class="number">0.1005</span></span><br><span class="line">Epoch <span class="number">403</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.1001</span></span><br><span class="line">Epoch <span class="number">404</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0997</span></span><br><span class="line">Epoch <span class="number">405</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">58</span>ms/step - loss: <span class="number">0.0993</span></span><br><span class="line">Epoch <span class="number">406</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">53</span>ms/step - loss: <span class="number">0.0990</span></span><br><span class="line">Epoch <span class="number">407</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">59</span>ms/step - loss: <span class="number">0.0986</span></span><br><span class="line">Epoch <span class="number">408</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">63</span>ms/step - loss: <span class="number">0.0982</span></span><br><span class="line">Epoch <span class="number">409</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">67</span>ms/step - loss: <span class="number">0.0979</span></span><br><span class="line">Epoch <span class="number">410</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0976</span></span><br><span class="line">Epoch <span class="number">411</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">54</span>ms/step - loss: <span class="number">0.0972</span></span><br><span class="line">Epoch <span class="number">412</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0969</span></span><br><span class="line">Epoch <span class="number">413</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0966</span></span><br><span class="line">Epoch <span class="number">414</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">59</span>ms/step - loss: <span class="number">0.0963</span></span><br><span class="line">Epoch <span class="number">415</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0960</span></span><br><span class="line">Epoch <span class="number">416</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">62</span>ms/step - loss: <span class="number">0.0957</span></span><br><span class="line">Epoch <span class="number">417</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">69</span>ms/step - loss: <span class="number">0.0954</span></span><br><span class="line">Epoch <span class="number">418</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0951</span></span><br><span class="line">Epoch <span class="number">419</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">50</span>ms/step - loss: <span class="number">0.0948</span></span><br><span class="line">Epoch <span class="number">420</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0946</span></span><br><span class="line">Epoch <span class="number">421</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0943</span></span><br><span class="line">Epoch <span class="number">422</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0941</span></span><br><span class="line">Epoch <span class="number">423</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">62</span>ms/step - loss: <span class="number">0.0938</span></span><br><span class="line">Epoch <span class="number">424</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0936</span></span><br><span class="line">Epoch <span class="number">425</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">100</span>ms/step - loss: <span class="number">0.0933</span></span><br><span class="line">Epoch <span class="number">426</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">68</span>ms/step - loss: <span class="number">0.0931</span></span><br><span class="line">Epoch <span class="number">427</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0929</span></span><br><span class="line">Epoch <span class="number">428</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">50</span>ms/step - loss: <span class="number">0.0926</span></span><br><span class="line">Epoch <span class="number">429</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0924</span></span><br><span class="line">Epoch <span class="number">430</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0922</span></span><br><span class="line">Epoch <span class="number">431</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">75</span>ms/step - loss: <span class="number">0.0920</span></span><br><span class="line">Epoch <span class="number">432</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0918</span></span><br><span class="line">Epoch <span class="number">433</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">77</span>ms/step - loss: <span class="number">0.0916</span></span><br><span class="line">Epoch <span class="number">434</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">50</span>ms/step - loss: <span class="number">0.0914</span></span><br><span class="line">Epoch <span class="number">435</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0912</span></span><br><span class="line">Epoch <span class="number">436</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0911</span></span><br><span class="line">Epoch <span class="number">437</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0909</span></span><br><span class="line">Epoch <span class="number">438</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0907</span></span><br><span class="line">Epoch <span class="number">439</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">59</span>ms/step - loss: <span class="number">0.0905</span></span><br><span class="line">Epoch <span class="number">440</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0904</span></span><br><span class="line">Epoch <span class="number">441</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">68</span>ms/step - loss: <span class="number">0.0902</span></span><br><span class="line">Epoch <span class="number">442</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">73</span>ms/step - loss: <span class="number">0.0901</span></span><br><span class="line">Epoch <span class="number">443</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">50</span>ms/step - loss: <span class="number">0.0899</span></span><br><span class="line">Epoch <span class="number">444</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">58</span>ms/step - loss: <span class="number">0.0898</span></span><br><span class="line">Epoch <span class="number">445</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0896</span></span><br><span class="line">Epoch <span class="number">446</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">52</span>ms/step - loss: <span class="number">0.0895</span></span><br><span class="line">Epoch <span class="number">447</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0893</span></span><br><span class="line">Epoch <span class="number">448</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">64</span>ms/step - loss: <span class="number">0.0892</span></span><br><span class="line">Epoch <span class="number">449</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">70</span>ms/step - loss: <span class="number">0.0891</span></span><br><span class="line">Epoch <span class="number">450</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0889</span></span><br><span class="line">Epoch <span class="number">451</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">53</span>ms/step - loss: <span class="number">0.0888</span></span><br><span class="line">Epoch <span class="number">452</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">51</span>ms/step - loss: <span class="number">0.0887</span></span><br><span class="line">Epoch <span class="number">453</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0886</span></span><br><span class="line">Epoch <span class="number">454</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">58</span>ms/step - loss: <span class="number">0.0885</span></span><br><span class="line">Epoch <span class="number">455</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0883</span></span><br><span class="line">Epoch <span class="number">456</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">71</span>ms/step - loss: <span class="number">0.0882</span></span><br><span class="line">Epoch <span class="number">457</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">50</span>ms/step - loss: <span class="number">0.0881</span></span><br><span class="line">Epoch <span class="number">458</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0880</span></span><br><span class="line">Epoch <span class="number">459</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0879</span></span><br><span class="line">Epoch <span class="number">460</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0878</span></span><br><span class="line">Epoch <span class="number">461</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0878</span></span><br><span class="line">Epoch <span class="number">462</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0879</span></span><br><span class="line">Epoch <span class="number">463</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0879</span></span><br><span class="line">Epoch <span class="number">464</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">68</span>ms/step - loss: <span class="number">0.0888</span></span><br><span class="line">Epoch <span class="number">465</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">62</span>ms/step - loss: <span class="number">0.0875</span></span><br><span class="line">Epoch <span class="number">466</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0873</span></span><br><span class="line">Epoch <span class="number">467</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">49</span>ms/step - loss: <span class="number">0.0872</span></span><br><span class="line">Epoch <span class="number">468</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0872</span></span><br><span class="line">Epoch <span class="number">469</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0871</span></span><br><span class="line">Epoch <span class="number">470</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0871</span></span><br><span class="line">Epoch <span class="number">471</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">59</span>ms/step - loss: <span class="number">0.0870</span></span><br><span class="line">Epoch <span class="number">472</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">68</span>ms/step - loss: <span class="number">0.0871</span></span><br><span class="line">Epoch <span class="number">473</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0869</span></span><br><span class="line">Epoch <span class="number">474</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">61</span>ms/step - loss: <span class="number">0.0870</span></span><br><span class="line">Epoch <span class="number">475</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">47</span>ms/step - loss: <span class="number">0.0868</span></span><br><span class="line">Epoch <span class="number">476</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0868</span></span><br><span class="line">Epoch <span class="number">477</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">62</span>ms/step - loss: <span class="number">0.0866</span></span><br><span class="line">Epoch <span class="number">478</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">58</span>ms/step - loss: <span class="number">0.0867</span></span><br><span class="line">Epoch <span class="number">479</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">60</span>ms/step - loss: <span class="number">0.0865</span></span><br><span class="line">Epoch <span class="number">480</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">65</span>ms/step - loss: <span class="number">0.0866</span></span><br><span class="line">Epoch <span class="number">481</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">58</span>ms/step - loss: <span class="number">0.0864</span></span><br><span class="line">Epoch <span class="number">482</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0865</span></span><br><span class="line">Epoch <span class="number">483</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">53</span>ms/step - loss: <span class="number">0.0863</span></span><br><span class="line">Epoch <span class="number">484</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0864</span></span><br><span class="line">Epoch <span class="number">485</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0862</span></span><br><span class="line">Epoch <span class="number">486</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">55</span>ms/step - loss: <span class="number">0.0863</span></span><br><span class="line">Epoch <span class="number">487</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">52</span>ms/step - loss: <span class="number">0.0861</span></span><br><span class="line">Epoch <span class="number">488</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">68</span>ms/step - loss: <span class="number">0.0862</span></span><br><span class="line">Epoch <span class="number">489</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">62</span>ms/step - loss: <span class="number">0.0860</span></span><br><span class="line">Epoch <span class="number">490</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0861</span></span><br><span class="line">Epoch <span class="number">491</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">51</span>ms/step - loss: <span class="number">0.0859</span></span><br><span class="line">Epoch <span class="number">492</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">54</span>ms/step - loss: <span class="number">0.0860</span></span><br><span class="line">Epoch <span class="number">493</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">51</span>ms/step - loss: <span class="number">0.0859</span></span><br><span class="line">Epoch <span class="number">494</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">54</span>ms/step - loss: <span class="number">0.0860</span></span><br><span class="line">Epoch <span class="number">495</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">50</span>ms/step - loss: <span class="number">0.0858</span></span><br><span class="line">Epoch <span class="number">496</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">69</span>ms/step - loss: <span class="number">0.0859</span></span><br><span class="line">Epoch <span class="number">497</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">63</span>ms/step - loss: <span class="number">0.0857</span></span><br><span class="line">Epoch <span class="number">498</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">56</span>ms/step - loss: <span class="number">0.0858</span></span><br><span class="line">Epoch <span class="number">499</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">54</span>ms/step - loss: <span class="number">0.0857</span></span><br><span class="line">Epoch <span class="number">500</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">57</span>ms/step - loss: <span class="number">0.0858</span></span><br></pre></td></tr></table></figure>
<h3 id="ping-gu-mo-xing-3">评估模型</h3>
<p>评估模型一般要设置验证集或者测试集，由于此例数据较少，仅仅可视化损失函数在训练集上的迭代情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training '</span>+ metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span>+metric])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"loss"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-4-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%9B%B2%E7%BA%BF.png" alt></p>
<h3 id="shi-yong-mo-xing-3">使用模型</h3>
<p>此处我们使用模型预测疫情结束时间，即 新增确诊病例为0 的时间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用dfresult记录现有数据以及此后预测的疫情数据</span></span><br><span class="line">dfresult = dfdiff[[<span class="string">"confirmed_num"</span>,<span class="string">"cured_num"</span>,<span class="string">"dead_num"</span>]].copy()</span><br><span class="line">dfresult.tail()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-4-%E6%97%A5%E6%9C%9F3%E6%9C%8810.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预测此后100天的新增走势,将其结果添加到dfresult中</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    arr_predict = model.predict(tf.constant(tf.expand_dims(dfresult.values[<span class="number">-38</span>:,:],axis = <span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">    dfpredict = pd.DataFrame(tf.cast(tf.floor(arr_predict),tf.float32).numpy(),</span><br><span class="line">                columns = dfresult.columns)</span><br><span class="line">    dfresult = dfresult.append(dfpredict,ignore_index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dfresult.query(<span class="string">"confirmed_num==0"</span>).head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第55天开始新增确诊降为0，第45天对应3月10日，也就是10天后，即预计3月20日新增确诊降为0</span></span><br><span class="line"><span class="comment"># 注：该预测偏乐观</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-4-%E9%A2%84%E6%B5%8B%E7%A1%AE%E8%AF%8A.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dfresult.query(<span class="string">"cured_num==0"</span>).head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第164天开始新增治愈降为0，第45天对应3月10日，也就是大概4个月后，即7月10日左右全部治愈。</span></span><br><span class="line"><span class="comment"># 注: 该预测偏悲观，并且存在问题，如果将每天新增治愈人数加起来，将超过累计确诊人数。</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-4-%E9%A2%84%E6%B5%8B%E6%B2%BB%E6%84%88.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dfresult.query(<span class="string">"dead_num==0"</span>).head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第60天开始，新增死亡降为0，第45天对应3月10日，也就是大概15天后，即20200325</span></span><br><span class="line"><span class="comment"># 该预测较为合理</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/1-4-%E9%A2%84%E6%B5%8B%E6%AD%BB%E4%BA%A1.png" alt></p>
<h3 id="bao-cun-mo-xing-3">保存模型</h3>
<p>推荐使用TensorFlow原生方式保存模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.save(<span class="string">'./data/tf_model_savedmodel'</span>, save_format=<span class="string">"tf"</span>)</span><br><span class="line">print(<span class="string">'export saved model.'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model_loaded = tf.keras.models.load_model(<span class="string">'./data/tf_model_savedmodel'</span>,compile=<span class="literal">False</span>)</span><br><span class="line">optimizer = tf.keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_loaded.compile(optimizer=optimizer,loss=MSPE(name = <span class="string">"MSPE"</span>))</span><br><span class="line">model_loaded.predict(ds_train)</span><br></pre></td></tr></table></figure>
<h1 id="tensor-flow-de-he-xin-gai-nian">TensorFlow的核心概念</h1>
<p>TensorFlow™ 是一个采用 <strong>数据流图</strong>（data flow graphs），用于数值计算的开源软件库。节点（Nodes）在图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。它灵活的架构让你可以<strong>在多种平台上展开计算</strong>，例如台式计算机中的一个或多个CPU（或GPU），服务器，移动设备等等。TensorFlow 最初由Google AI小组（隶属于Google机器智能研究机构）的研究员和工程师们开发出来，<strong>用于机器学习和深度神经网络</strong>方面的研究，但这个系统的通用性使其也可<strong>广泛用于其他计算领域</strong>。</p>
<p>TensorFlow的主要优点：</p>
<ul>
<li>
<p>灵活性：支持底层数值计算，C++自定义操作符</p>
</li>
<li>
<p>可移植性：从服务器到PC到手机，从CPU到GPU到TPU</p>
</li>
<li>
<p>分布式计算：分布式并行计算，可指定操作符对应计算设备</p>
</li>
</ul>
<p>俗话说，万丈高楼平地起，TensorFlow这座大厦也有它的地基。</p>
<p>Tensorflow底层最核心的概念是张量，计算图以及自动微分。</p>
<h2 id="zhang-liang-shu-ju-jie-gou">张量数据结构</h2>
<p>程序 = 数据结构+算法，TensorFlow程序 = 张量数据结构 + 计算图算法语言，张量和计算图是 TensorFlow的核心概念。</p>
<p>Tensorflow的基本数据结构是张量Tensor。张量即多维数组。Tensorflow的张量和numpy中的array很类似。</p>
<p>从行为特性来看，有两种类型的张量，常量constant和变量Variable，常量的值在计算图中不可以被重新赋值，变量可以在计算图中用assign等算子重新赋值。</p>
<h3 id="chang-liang-zhang-liang">常量张量</h3>
<p>张量的数据类型和numpy.array基本一一对应。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">i = tf.constant(<span class="number">1</span>) <span class="comment"># tf.int32 类型常量</span></span><br><span class="line">l = tf.constant(<span class="number">1</span>,dtype = tf.int64) <span class="comment"># tf.int64 类型常量</span></span><br><span class="line">f = tf.constant(<span class="number">1.23</span>) <span class="comment">#tf.float32 类型常量</span></span><br><span class="line">d = tf.constant(<span class="number">3.14</span>,dtype = tf.double) <span class="comment"># tf.double 类型常量</span></span><br><span class="line">s = tf.constant(<span class="string">"hello world"</span>) <span class="comment"># tf.string类型常量</span></span><br><span class="line">b = tf.constant(<span class="literal">True</span>) <span class="comment">#tf.bool类型常量</span></span><br><span class="line"></span><br><span class="line">print(tf.int64 == np.int64) </span><br><span class="line">print(tf.bool == np.bool)</span><br><span class="line">print(tf.double == np.float64)</span><br><span class="line">print(tf.string == np.unicode) <span class="comment"># tf.string类型和np.unicode类型不等价</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>不同类型的数据可以用不同维度(rank)的张量来表示，标量是0维张量，向量为1维张量，矩阵为2维张量。</p>
<p>彩色图像有rgb三个通道，可以表示为3维张量。视频还有时间维，可以表示为4维张量，可以简单地总结为：有几层中括号，就是多少维的张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scalar = tf.constant(<span class="literal">True</span>)  <span class="comment">#标量，0维张量</span></span><br><span class="line"></span><br><span class="line">print(tf.rank(scalar))</span><br><span class="line">print(scalar.numpy().ndim)  <span class="comment"># tf.rank的作用和numpy的ndim方法相同</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="number">0</span>, shape=(), dtype=int32)</span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vector = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>]) <span class="comment">#向量，1维张量</span></span><br><span class="line"></span><br><span class="line">print(tf.rank(vector)) <span class="comment"># tf.Tensor(1, shape=(), dtype=int32)</span></span><br><span class="line">print(np.ndim(vector.numpy())) <span class="comment"># 1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">matrix = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]]) <span class="comment">#矩阵, 2维张量</span></span><br><span class="line"></span><br><span class="line">print(tf.rank(matrix).numpy()) <span class="comment"># 2</span></span><br><span class="line">print(np.ndim(matrix)) <span class="comment"># 2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor3 = tf.constant([[[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]],[[<span class="number">5.0</span>,<span class="number">6.0</span>],[<span class="number">7.0</span>,<span class="number">8.0</span>]]])  <span class="comment"># 3维张量</span></span><br><span class="line">print(tensor3)</span><br><span class="line">print(tf.rank(tensor3))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[[<span class="number">1.</span> <span class="number">2.</span>]</span><br><span class="line">  [<span class="number">3.</span> <span class="number">4.</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">5.</span> <span class="number">6.</span>]</span><br><span class="line">  [<span class="number">7.</span> <span class="number">8.</span>]]], shape=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">3</span>, shape=(), dtype=int32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">tensor4 = tf.constant([[[[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">2.0</span>,<span class="number">2.0</span>]],[[<span class="number">3.0</span>,<span class="number">3.0</span>],[<span class="number">4.0</span>,<span class="number">4.0</span>]]],</span><br><span class="line">                        [[[<span class="number">5.0</span>,<span class="number">5.0</span>],[<span class="number">6.0</span>,<span class="number">6.0</span>]],[[<span class="number">7.0</span>,<span class="number">7.0</span>],[<span class="number">8.0</span>,<span class="number">8.0</span>]]]])  <span class="comment"># 4维张量</span></span><br><span class="line">print(tensor4)</span><br><span class="line">print(tf.rank(tensor4))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line">   [<span class="number">2.</span> <span class="number">2.</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">3.</span> <span class="number">3.</span>]</span><br><span class="line">   [<span class="number">4.</span> <span class="number">4.</span>]]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> [[[<span class="number">5.</span> <span class="number">5.</span>]</span><br><span class="line">   [<span class="number">6.</span> <span class="number">6.</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">7.</span> <span class="number">7.</span>]</span><br><span class="line">   [<span class="number">8.</span> <span class="number">8.</span>]]]], shape=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">4</span>, shape=(), dtype=int32)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以用tf.cast改变张量的数据类型。</p>
<p>可以用numpy方法将tensorflow中的张量转化成numpy中的张量。</p>
<p>可以用shape方法查看张量的尺寸。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">h = tf.constant([<span class="number">123</span>,<span class="number">456</span>],dtype = tf.int32)</span><br><span class="line">f = tf.cast(h,tf.float32)</span><br><span class="line">print(h.dtype, f.dtype) <span class="comment"># &lt;dtype: 'int32'&gt; &lt;dtype: 'float32'&gt;</span></span><br><span class="line"></span><br><span class="line">y = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]])</span><br><span class="line">print(y.numpy()) <span class="comment">#转换成np.array</span></span><br><span class="line">print(y.shape)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">1.</span> <span class="number">2.</span>]</span><br><span class="line"> [<span class="number">3.</span> <span class="number">4.</span>]]</span><br><span class="line">(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">u = tf.constant(<span class="string">u"你好 世界"</span>)</span><br><span class="line">print(u.numpy())  <span class="comment"># b'\xe4\xbd\xa0\xe5\xa5\xbd \xe4\xb8\x96\xe7\x95\x8c'</span></span><br><span class="line">print(u.numpy().decode(<span class="string">"utf-8"</span>)) <span class="comment"># 你好 世界</span></span><br></pre></td></tr></table></figure>
<h3 id="bian-liang-zhang-liang">变量张量</h3>
<p>模型中需要被训练的参数一般被设置成变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 常量值不可以改变，常量的重新赋值相当于创造新的内存空间</span></span><br><span class="line">c = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>])</span><br><span class="line">print(c) <span class="comment"># tf.Tensor([1. 2.], shape=(2,), dtype=float32)</span></span><br><span class="line">print(id(c)) <span class="comment"># 5276289568</span></span><br><span class="line">c = c + tf.constant([<span class="number">1.0</span>,<span class="number">1.0</span>])</span><br><span class="line">print(c) <span class="comment"># tf.Tensor([2. 3.], shape=(2,), dtype=float32)</span></span><br><span class="line">print(id(c)) <span class="comment"># 5276290240</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 变量的值可以改变，可以通过assign, assign_add等方法给变量重新赋值</span></span><br><span class="line">v = tf.Variable([<span class="number">1.0</span>,<span class="number">2.0</span>],name = <span class="string">"v"</span>)</span><br><span class="line">print(v) <span class="comment"># &lt;tf.Variable 'v:0' shape=(2,) dtype=float32, numpy=array([1., 2.], dtype=float32)&gt;</span></span><br><span class="line">print(id(v)) <span class="comment"># 5276259888</span></span><br><span class="line">v.assign_add([<span class="number">1.0</span>,<span class="number">1.0</span>]) </span><br><span class="line">print(v) <span class="comment"># &lt;tf.Variable 'v:0' shape=(2,) dtype=float32, numpy=array([2., 3.], dtype=float32)&gt;</span></span><br><span class="line">print(id(v)) <span class="comment"># 5276259888</span></span><br></pre></td></tr></table></figure>
<h2 id="san-chong-ji-suan-tu">三种计算图</h2>
<p>有三种计算图的构建方式：</p>
<ol>
<li>静态计算图</li>
<li>动态计算图</li>
<li>Autograph</li>
</ol>
<p>在TensorFlow1.0时代，采用的是静态计算图，需要先使用TensorFlow的各种算子创建计算图，然后再开启一个会话Session，显式执行计算图。</p>
<p>而在TensorFlow2.0时代，采用的是动态计算图，即每使用一个算子后，该算子会被动态加入到隐含的默认计算图中立即执行得到结果，而无需开启Session。</p>
<p>使用动态计算图即Eager Excution的好处是方便调试程序，它会让TensorFlow代码的表现和Python原生代码的表现一样，写起来就像写numpy一样，各种日志打印，控制流全部都是可以使用的。</p>
<p>使用动态计算图的缺点是运行效率相对会低一些。因为使用动态图会有许多次Python进程和TensorFlow的C<ins>进程之间的通信。而静态计算图构建完成之后几乎全部在TensorFlow内核上使用C</ins>代码执行，效率更高。此外静态图会对计算步骤进行一定的优化，剪去和结果无关的计算步骤。</p>
<p>如果需要在TensorFlow2.0中使用静态图，可以使用@tf.function装饰器将普通Python函数转换成对应的TensorFlow计算图构建代码。运行该函数就相当于在TensorFlow1.0中用Session执行代码。使用tf.function构建静态图的方式叫做 Autograph.</p>
<h3 id="ji-suan-tu-jian-jie">计算图简介</h3>
<p>计算图由节点(nodes)和线(edges)组成。节点表示操作符Operator，或者称之为算子，线表示计算间的依赖。</p>
<p>实线表示有数据传递依赖，传递的数据即张量。虚线通常可以表示控制依赖，即执行先后顺序。</p>
<p><img src="/2020/05/07/tensorflow/tensorflow2/strjoin_graph.png" alt></p>
<h3 id="jing-tai-ji-suan-tu">静态计算图</h3>
<p>在TensorFlow1.0中，使用静态计算图分两步，第一步定义计算图，第二步在会话中执行计算图。<br>
<strong>TensorFlow 1.0静态计算图范例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义计算图</span></span><br><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    <span class="comment">#placeholder为占位符，执行会话时候指定填充对象</span></span><br><span class="line">    x = tf.placeholder(name=<span class="string">'x'</span>, shape=[], dtype=tf.string)  </span><br><span class="line">    y = tf.placeholder(name=<span class="string">'y'</span>, shape=[], dtype=tf.string)</span><br><span class="line">    z = tf.string_join([x,y],name = <span class="string">'join'</span>,separator=<span class="string">' '</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph = g) <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(fetches = z,feed_dict = &#123;x:<span class="string">"hello"</span>,y:<span class="string">"world"</span>&#125;))</span><br></pre></td></tr></table></figure>
<p><strong>TensorFlow2.0 怀旧版静态计算图</strong></p>
<p>TensorFlow2.0为了确保对老版本tensorflow项目的兼容性，在tf.compat.v1子模块中保留了对TensorFlow1.0那种静态计算图构建风格的支持。(已经不推荐使用了)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">g = tf.compat.v1.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    x = tf.compat.v1.placeholder(name=<span class="string">'x'</span>, shape=[], dtype=tf.string)</span><br><span class="line">    y = tf.compat.v1.placeholder(name=<span class="string">'y'</span>, shape=[], dtype=tf.string)</span><br><span class="line">    z = tf.strings.join([x,y],name = <span class="string">"join"</span>,separator = <span class="string">" "</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session(graph = g) <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># fetches的结果非常像一个函数的返回值，而feed_dict中的占位符相当于函数的参数序列。</span></span><br><span class="line">    result = sess.run(fetches = z,feed_dict = &#123;x:<span class="string">"hello"</span>,y:<span class="string">"world"</span>&#125;)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure>
<figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b'hello world'</span><br></pre></td></tr></table></figure>
<h3 id="dong-tai-ji-suan-tu">动态计算图</h3>
<p>在TensorFlow2.0中，使用的是动态计算图和Autograph.</p>
<p>在TensorFlow1.0中，使用静态计算图分两步，第一步定义计算图，第二步在会话中执行计算图。</p>
<p>动态计算图已经不区分计算图的定义和执行了，而是定义后立即执行。因此称之为 Eager Excution.</p>
<p>Eager这个英文单词的原意是&quot;迫不及待的&quot;，也就是立即执行的意思。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 动态计算图在每个算子处都进行构建，构建后立即执行</span></span><br><span class="line"></span><br><span class="line">x = tf.constant(<span class="string">"hello"</span>)</span><br><span class="line">y = tf.constant(<span class="string">"world"</span>)</span><br><span class="line">z = tf.strings.join([x,y],separator=<span class="string">" "</span>)</span><br><span class="line"></span><br><span class="line">tf.print(z)</span><br></pre></td></tr></table></figure>
<figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hello world</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以将动态计算图代码的输入和输出关系封装成函数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strjoin</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    z =  tf.strings.join([x,y],separator = <span class="string">" "</span>)</span><br><span class="line">    tf.print(z)</span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">result = strjoin(tf.constant(<span class="string">"hello"</span>),tf.constant(<span class="string">"world"</span>))</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hello world</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">hello</span> <span class="params">world</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br></pre></td></tr></table></figure>
<h3 id="tensor-flow-2-0-de-autograph">TensorFlow2.0的Autograph</h3>
<p>动态计算图运行效率相对较低。</p>
<p>可以用@tf.function装饰器将普通Python函数转换成和TensorFlow1.0对应的静态计算图构建代码。</p>
<p>在TensorFlow1.0中，使用计算图分两步，第一步定义计算图，第二步在会话中执行计算图。</p>
<p>在TensorFlow2.0中，如果采用Autograph的方式使用计算图，第一步定义计算图变成了定义函数，第二步执行计算图变成了调用函数。</p>
<p>不需要使用会话了，一些都像原始的Python语法一样自然。</p>
<p>实践中，我们一般会先用动态计算图调试代码，然后在需要提高性能的的地方利用@tf.function切换成Autograph获得更高的效率。</p>
<p>当然，@tf.function的使用需要遵循一定的规范，后面章节将重点介绍。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用autograph构建静态图</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strjoin</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    z =  tf.strings.join([x,y],separator = <span class="string">" "</span>)</span><br><span class="line">    tf.print(z)</span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">result = strjoin(tf.constant(<span class="string">"hello"</span>),tf.constant(<span class="string">"world"</span>))</span><br><span class="line"></span><br><span class="line">print(result)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">hello world</span><br><span class="line">tf.Tensor(<span class="string">b'hello world'</span>, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建日志</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">stamp = datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">logdir = os.path.join(<span class="string">'data'</span>, <span class="string">'autograph'</span>, stamp)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 在 Python3 下建议使用 pathlib 修正各操作系统的路径</span></span><br><span class="line"><span class="comment"># from pathlib import Path</span></span><br><span class="line"><span class="comment"># stamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")</span></span><br><span class="line"><span class="comment"># logdir = str(Path('./data/autograph/' + stamp))</span></span><br><span class="line"></span><br><span class="line">writer = tf.summary.create_file_writer(logdir)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开启autograph跟踪</span></span><br><span class="line">tf.summary.trace_on(graph=<span class="literal">True</span>, profiler=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#执行autograph</span></span><br><span class="line">result = strjoin(<span class="string">"hello"</span>,<span class="string">"world"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将计算图信息写入日志</span></span><br><span class="line"><span class="keyword">with</span> writer.as_default():</span><br><span class="line">    tf.summary.trace_export(</span><br><span class="line">        name=<span class="string">"autograph"</span>,</span><br><span class="line">        step=<span class="number">0</span>,</span><br><span class="line">        profiler_outdir=logdir)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#启动 tensorboard在jupyter中的魔法命令</span></span><br><span class="line">%load_ext tensorboard</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#启动tensorboard</span></span><br><span class="line">%tensorboard --logdir ./data/autograph/</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/2-2-tensorboard%E8%AE%A1%E7%AE%97%E5%9B%BE.jpg" alt></p>
<h2 id="zi-dong-wei-fen-ji-zhi">自动微分机制</h2>
<p>神经网络通常依赖反向传播求梯度来更新网络参数，求梯度过程通常是一件非常复杂而容易出错的事情。</p>
<p>而深度学习框架可以帮助我们自动地完成这种求梯度运算。</p>
<p>Tensorflow一般使用梯度磁带tf.GradientTape来记录正向运算过程，然后反播磁带自动得到梯度值。</p>
<p>这种利用tf.GradientTape求微分的方法叫做Tensorflow的自动微分机制。</p>
<h3 id="li-yong-ti-du-ci-dai-qiu-dao-shu">利用梯度磁带求导数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的导数</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    </span><br><span class="line">dy_dx = tape.gradient(y,x)</span><br><span class="line">print(dy_dx) <span class="comment"># tf.Tensor(-2.0, shape=(), dtype=float32)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对常量张量也可以求导，需要增加watch</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    tape.watch([a,b,c])</span><br><span class="line">    y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    </span><br><span class="line">dy_dx,dy_da,dy_db,dy_dc = tape.gradient(y,[x,a,b,c])</span><br><span class="line">print(dy_da) <span class="comment"># tf.Tensor(0.0, shape=(), dtype=float32)</span></span><br><span class="line">print(dy_dc) <span class="comment"># tf.Tensor(1.0, shape=(), dtype=float32)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以求二阶导数</span></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape2:</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape1:   </span><br><span class="line">        y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    dy_dx = tape1.gradient(y,x)   </span><br><span class="line">dy2_dx2 = tape2.gradient(dy_dx,x)</span><br><span class="line"></span><br><span class="line">print(dy2_dx2) <span class="comment"># tf.Tensor(2.0, shape=(), dtype=float32)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以在autograph中使用</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span>   </span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 自变量转换成tf.float32</span></span><br><span class="line">    x = tf.cast(x,tf.float32)</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        tape.watch(x)</span><br><span class="line">        y = a*tf.pow(x,<span class="number">2</span>)+b*x+c</span><br><span class="line">    dy_dx = tape.gradient(y,x) </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span>((dy_dx,y))</span><br><span class="line"></span><br><span class="line">tf.print(f(tf.constant(<span class="number">0.0</span>))) <span class="comment"># (-2, 1)</span></span><br><span class="line">tf.print(f(tf.constant(<span class="number">1.0</span>))) <span class="comment"># (0, 0)</span></span><br></pre></td></tr></table></figure>
<h3 id="li-yong-ti-du-ci-dai-he-you-hua-qi-qiu-zui-xiao-zhi">利用梯度磁带和优化器求最小值</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"><span class="comment"># 使用optimizer.apply_gradients</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    dy_dx = tape.gradient(y,x)</span><br><span class="line">    optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])</span><br><span class="line">    </span><br><span class="line">tf.print(<span class="string">"y ="</span>,y,<span class="string">"; x ="</span>,x) <span class="comment"># y = 0 ; x = 0.999998569</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"><span class="comment"># 使用optimizer.minimize</span></span><br><span class="line"><span class="comment"># optimizer.minimize相当于先用tape求gradient,再apply_gradient</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment">#注意f()无参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">()</span>:</span>   </span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    y = a*tf.pow(x,<span class="number">2</span>)+b*x+c</span><br><span class="line">    <span class="keyword">return</span>(y)</span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)   </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    optimizer.minimize(f,[x])   </span><br><span class="line">    </span><br><span class="line">tf.print(<span class="string">"y ="</span>,f(),<span class="string">"; x ="</span>,x) <span class="comment"># y = 0 ; x = 0.999998569</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在autograph中完成最小值求解</span></span><br><span class="line"><span class="comment"># 使用optimizer.apply_gradients</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minimizef</span><span class="params">()</span>:</span></span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> tf.range(<span class="number">1000</span>): <span class="comment">#注意autograph时使用tf.range(1000)而不是range(1000)</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">        dy_dx = tape.gradient(y,x)</span><br><span class="line">        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])</span><br><span class="line">        </span><br><span class="line">    y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">tf.print(minimizef()) <span class="comment"># 0</span></span><br><span class="line">tf.print(x) <span class="comment"># 0.999998569</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在autograph中完成最小值求解</span></span><br><span class="line"><span class="comment"># 使用optimizer.minimize</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)   </span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">()</span>:</span>   </span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    y = a*tf.pow(x,<span class="number">2</span>)+b*x+c</span><br><span class="line">    <span class="keyword">return</span>(y)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epoch)</span>:</span>  </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> tf.range(epoch):  </span><br><span class="line">        optimizer.minimize(f,[x])</span><br><span class="line">    <span class="keyword">return</span>(f())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.print(train(<span class="number">1000</span>)) <span class="comment"># 0</span></span><br><span class="line">tf.print(x) <span class="comment"># 0.999998569</span></span><br></pre></td></tr></table></figure>
<h1 id="tensor-flow-de-ceng-ci-jie-gou">TensorFlow的层次结构</h1>
<p>本章介绍TensorFlow中5个不同的层次结构：</p>
<ol>
<li>硬件层</li>
<li>内核层</li>
<li>低阶API</li>
<li>中阶API</li>
<li>高阶API。</li>
</ol>
<p>并以线性回归和DNN二分类模型为例，直观对比展示在不同层级实现模型的特点。</p>
<p>TensorFlow的层次结构从低到高可以分成如下五层。</p>
<p>最底层为硬件层，TensorFlow支持CPU、GPU或TPU加入计算资源池。</p>
<p>第二层为C++实现的内核，kernel可以跨平台分布运行。</p>
<p>第三层为Python实现的操作符，提供了封装C++内核的低级API指令，主要包括各种张量操作算子、计算图、自动微分.如tf.Variable,tf.constant,tf.function,tf.GradientTape,tf.nn.softmax…如果把模型比作一个房子，那么第三层API就是【模型之砖】。</p>
<p>第四层为Python实现的模型组件，对低级API进行了函数封装，主要包括各种模型层，损失函数，优化器，数据管道，特征列等等。如tf.keras.layers,tf.keras.losses,tf.keras.metrics,tf.keras.optimizers,tf.data.DataSet,tf.feature_column…如果把模型比作一个房子，那么第四层API就是【模型之墙】。</p>
<p>第五层为Python实现的模型成品，一般为按照OOP方式封装的高级API，主要为tf.keras.models提供的模型的类接口。<br>
如果把模型比作一个房子，那么第五层API就是模型本身，即【模型之屋】。</p>
<p><img src="/2020/05/07/tensorflow/tensorflow2/tensorflow_structure.jpg" alt></p>
<h2 id="di-jie-api">低阶API</h2>
<p>使用低阶API实现线性回归模型和DNN二分类模型。</p>
<p>低阶API主要包括：</p>
<ol>
<li>张量操作</li>
<li>计算图</li>
<li>自动微分。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure>
<h3 id="xian-xing-hui-gui-mo-xing">线性回归模型</h3>
<h4 id="zhun-bei-shu-ju-4">准备数据</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#样本数量</span></span><br><span class="line">n = <span class="number">400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成测试用数据集</span></span><br><span class="line">X = tf.random.uniform([n,<span class="number">2</span>],minval=<span class="number">-10</span>,maxval=<span class="number">10</span>) </span><br><span class="line">w0 = tf.constant([[<span class="number">2.0</span>],[<span class="number">-3.0</span>]])</span><br><span class="line">b0 = tf.constant([[<span class="number">3.0</span>]])</span><br><span class="line">Y = X@w0 + b0 + tf.random.normal([n,<span class="number">1</span>],mean = <span class="number">0.0</span>,stddev= <span class="number">2.0</span>)  <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], c = <span class="string">"b"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], c = <span class="string">"g"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/3-1-01-%E5%9B%9E%E5%BD%92%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建数据管道迭代器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(features, labels, batch_size=<span class="number">8</span>)</span>:</span></span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    np.random.shuffle(indices)  <span class="comment">#样本的读取顺序是随机的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        indexs = indices[i: min(i + batch_size, num_examples)]</span><br><span class="line">        <span class="keyword">yield</span> tf.gather(features,indexs), tf.gather(labels,indexs)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 测试数据管道效果   </span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">(features,labels) = next(data_iter(X,Y,batch_size))</span><br><span class="line">print(features)</span><br><span class="line">print(labels)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[ <span class="number">2.6161194</span>   <span class="number">0.11071014</span>]</span><br><span class="line"> [ <span class="number">9.79207</span>    <span class="number">-0.70180416</span>]</span><br><span class="line"> [ <span class="number">9.792343</span>    <span class="number">6.9149055</span> ]</span><br><span class="line"> [<span class="number">-2.4186516</span>  <span class="number">-9.375019</span>  ]</span><br><span class="line"> [ <span class="number">9.83749</span>    <span class="number">-3.4637213</span> ]</span><br><span class="line"> [ <span class="number">7.3953056</span>   <span class="number">4.374569</span>  ]</span><br><span class="line"> [<span class="number">-0.14686584</span> <span class="number">-0.28063297</span>]</span><br><span class="line"> [ <span class="number">0.49001217</span> <span class="number">-9.739792</span>  ]], shape=(<span class="number">8</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[ <span class="number">9.334667</span> ]</span><br><span class="line"> [<span class="number">22.058844</span> ]</span><br><span class="line"> [ <span class="number">3.0695205</span>]</span><br><span class="line"> [<span class="number">26.736238</span> ]</span><br><span class="line"> [<span class="number">35.292133</span> ]</span><br><span class="line"> [ <span class="number">4.2943544</span>]</span><br><span class="line"> [ <span class="number">1.6713585</span>]</span><br><span class="line"> [<span class="number">34.826904</span> ]], shape=(<span class="number">8</span>, <span class="number">1</span>), dtype=float32)</span><br></pre></td></tr></table></figure>
<h4 id="ding-yi-mo-xing-4">定义模型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">w = tf.Variable(tf.random.normal(w0.shape))</span><br><span class="line">b = tf.Variable(tf.zeros_like(b0,dtype = tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span>:</span>     </span><br><span class="line">    <span class="comment">#正向传播</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self,x)</span>:</span> </span><br><span class="line">        <span class="keyword">return</span> x@w + b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 损失函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_func</span><span class="params">(self,y_true,y_pred)</span>:</span>  </span><br><span class="line">        <span class="keyword">return</span> tf.reduce_mean((y_true - y_pred)**<span class="number">2</span>/<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model = LinearRegression()</span><br></pre></td></tr></table></figure>
<h4 id="xun-lian-mo-xing-4">训练模型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用动态图调试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = model.loss_func(labels, predictions)</span><br><span class="line">    <span class="comment"># 反向传播求梯度</span></span><br><span class="line">    dloss_dw,dloss_db = tape.gradient(loss,[w,b])</span><br><span class="line">    <span class="comment"># 梯度下降法更新参数</span></span><br><span class="line">    w.assign(w - <span class="number">0.001</span>*dloss_dw)</span><br><span class="line">    b.assign(b - <span class="number">0.001</span>*dloss_db)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试train_step效果</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">(features,labels) = next(data_iter(X,Y,batch_size))</span><br><span class="line">train_step(model,features,labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">tf.Tensor:</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">211.09982</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> data_iter(X,Y,<span class="number">10</span>):</span><br><span class="line">            loss = train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">50</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">"loss = "</span>,loss)</span><br><span class="line">            tf.print(<span class="string">"w ="</span>,w)</span><br><span class="line">            tf.print(<span class="string">"b ="</span>,b)</span><br><span class="line"></span><br><span class="line">train_model(model,epochs = <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">16</span>:<span class="number">35</span>:<span class="number">56</span></span><br><span class="line">epoch = <span class="number">50</span> loss =  <span class="number">1.78806472</span></span><br><span class="line">w = <span class="string">[[1.97554708]</span></span><br><span class="line"><span class="string"> [-2.97719598]]</span></span><br><span class="line">b = <span class="string">[[2.60692883]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">00</span></span><br><span class="line">epoch = <span class="number">100</span> loss =  <span class="number">2.64588404</span></span><br><span class="line">w = <span class="string">[[1.97319281]</span></span><br><span class="line"><span class="string"> [-2.97810626]]</span></span><br><span class="line">b = <span class="string">[[2.95525956]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">04</span></span><br><span class="line">epoch = <span class="number">150</span> loss =  <span class="number">1.42576694</span></span><br><span class="line">w = <span class="string">[[1.96466208]</span></span><br><span class="line"><span class="string"> [-2.98337793]]</span></span><br><span class="line">b = <span class="string">[[3.00264144]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">08</span></span><br><span class="line">epoch = <span class="number">200</span> loss =  <span class="number">1.68992615</span></span><br><span class="line">w = <span class="string">[[1.97718477]</span></span><br><span class="line"><span class="string"> [-2.983814]]</span></span><br><span class="line">b = <span class="string">[[3.01013041]]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##使用autograph机制转换成静态图加速</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = model.loss_func(labels, predictions)</span><br><span class="line">    <span class="comment"># 反向传播求梯度</span></span><br><span class="line">    dloss_dw,dloss_db = tape.gradient(loss,[w,b])</span><br><span class="line">    <span class="comment"># 梯度下降法更新参数</span></span><br><span class="line">    w.assign(w - <span class="number">0.001</span>*dloss_dw)</span><br><span class="line">    b.assign(b - <span class="number">0.001</span>*dloss_db)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> data_iter(X,Y,<span class="number">10</span>):</span><br><span class="line">            loss = train_step(model,features,labels)</span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">50</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">"loss = "</span>,loss)</span><br><span class="line">            tf.print(<span class="string">"w ="</span>,w)</span><br><span class="line">            tf.print(<span class="string">"b ="</span>,b)</span><br><span class="line"></span><br><span class="line">train_model(model,epochs = <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">35</span></span><br><span class="line">epoch = <span class="number">50</span> loss =  <span class="number">0.894210339</span></span><br><span class="line">w = <span class="string">[[1.96927285]</span></span><br><span class="line"><span class="string"> [-2.98914337]]</span></span><br><span class="line">b = <span class="string">[[3.00987792]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">36</span></span><br><span class="line">epoch = <span class="number">100</span> loss =  <span class="number">1.58621466</span></span><br><span class="line">w = <span class="string">[[1.97566223]</span></span><br><span class="line"><span class="string"> [-2.98550248]]</span></span><br><span class="line">b = <span class="string">[[3.00998402]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">37</span></span><br><span class="line">epoch = <span class="number">150</span> loss =  <span class="number">2.2695992</span></span><br><span class="line">w = <span class="string">[[1.96664226]</span></span><br><span class="line"><span class="string"> [-2.99248481]]</span></span><br><span class="line">b = <span class="string">[[3.01028705]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">38</span></span><br><span class="line">epoch = <span class="number">200</span> loss =  <span class="number">1.90848124</span></span><br><span class="line">w = <span class="string">[[1.98000824]</span></span><br><span class="line"><span class="string"> [-2.98888135]]</span></span><br><span class="line">b = <span class="string">[[3.01085401]]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], c = <span class="string">"b"</span>,label = <span class="string">"samples"</span>)</span><br><span class="line">ax1.plot(X[:,<span class="number">0</span>],w[<span class="number">0</span>]*X[:,<span class="number">0</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">"model"</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], c = <span class="string">"g"</span>,label = <span class="string">"samples"</span>)</span><br><span class="line">ax2.plot(X[:,<span class="number">1</span>],w[<span class="number">1</span>]*X[:,<span class="number">1</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">"model"</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/3-1-2-%E5%9B%9E%E5%BD%92%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<h3 id="dnn-er-fen-lei-mo-xing">DNN二分类模型</h3>
<h4 id="zhun-bei-shu-ju-5">准备数据</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#正负样本数量</span></span><br><span class="line">n_positive,n_negative = <span class="number">2000</span>,<span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成正样本, 小圆环分布</span></span><br><span class="line">r_p = <span class="number">5.0</span> + tf.random.truncated_normal([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_p = tf.random.uniform([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xp = tf.concat([r_p*tf.cos(theta_p),r_p*tf.sin(theta_p)],axis = <span class="number">1</span>)</span><br><span class="line">Yp = tf.ones_like(r_p)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成负样本, 大圆环分布</span></span><br><span class="line">r_n = <span class="number">8.0</span> + tf.random.truncated_normal([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_n = tf.random.uniform([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xn = tf.concat([r_n*tf.cos(theta_n),r_n*tf.sin(theta_n)],axis = <span class="number">1</span>)</span><br><span class="line">Yn = tf.zeros_like(r_n)</span><br><span class="line"></span><br><span class="line"><span class="comment">#汇总样本</span></span><br><span class="line">X = tf.concat([Xp,Xn],axis = <span class="number">0</span>)</span><br><span class="line">Y = tf.concat([Yp,Yn],axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line">plt.figure(figsize = (<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">plt.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">plt.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/3-1-03-%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建数据管道迭代器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(features, labels, batch_size=<span class="number">8</span>)</span>:</span></span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    np.random.shuffle(indices)  <span class="comment">#样本的读取顺序是随机的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        indexs = indices[i: min(i + batch_size, num_examples)]</span><br><span class="line">        <span class="keyword">yield</span> tf.gather(features,indexs), tf.gather(labels,indexs)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 测试数据管道效果   </span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">(features,labels) = next(data_iter(X,Y,batch_size))</span><br><span class="line">print(features)</span><br><span class="line">print(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[ <span class="number">0.03732629</span>  <span class="number">3.5783494</span> ]</span><br><span class="line"> [ <span class="number">0.542919</span>    <span class="number">5.035079</span>  ]</span><br><span class="line"> [ <span class="number">5.860281</span>   <span class="number">-2.4476354</span> ]</span><br><span class="line"> [ <span class="number">0.63657564</span>  <span class="number">3.194231</span>  ]</span><br><span class="line"> [<span class="number">-3.5072308</span>   <span class="number">2.5578873</span> ]</span><br><span class="line"> [<span class="number">-2.4109735</span>  <span class="number">-3.6621518</span> ]</span><br><span class="line"> [ <span class="number">4.0975413</span>  <span class="number">-2.4172943</span> ]</span><br><span class="line"> [ <span class="number">1.9393908</span>  <span class="number">-6.782317</span>  ]</span><br><span class="line"> [<span class="number">-4.7453732</span>  <span class="number">-0.5176727</span> ]</span><br><span class="line"> [<span class="number">-1.4057113</span>  <span class="number">-7.9775257</span> ]], shape=(<span class="number">10</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span>]], shape=(<span class="number">10</span>, <span class="number">1</span>), dtype=float32)</span><br></pre></td></tr></table></figure>
<h4 id="ding-yi-mo-xing-5">定义模型</h4>
<p>利用tf.Module来组织模型变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DNNModel</span><span class="params">(tf.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name = None)</span>:</span></span><br><span class="line">        super(DNNModel, self).__init__(name=name)</span><br><span class="line">        self.w1 = tf.Variable(tf.random.truncated_normal([<span class="number">2</span>,<span class="number">4</span>]),dtype = tf.float32)</span><br><span class="line">        self.b1 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">4</span>]),dtype = tf.float32)</span><br><span class="line">        self.w2 = tf.Variable(tf.random.truncated_normal([<span class="number">4</span>,<span class="number">8</span>]),dtype = tf.float32)</span><br><span class="line">        self.b2 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">8</span>]),dtype = tf.float32)</span><br><span class="line">        self.w3 = tf.Variable(tf.random.truncated_normal([<span class="number">8</span>,<span class="number">1</span>]),dtype = tf.float32)</span><br><span class="line">        self.b3 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">1</span>]),dtype = tf.float32)</span><br><span class="line"></span><br><span class="line">     </span><br><span class="line">    <span class="comment"># 正向传播</span></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [None,2], dtype = tf.float32)])  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x = tf.nn.relu(x@self.w1 + self.b1)</span><br><span class="line">        x = tf.nn.relu(x@self.w2 + self.b2)</span><br><span class="line">        y = tf.nn.sigmoid(x@self.w3 + self.b3)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 损失函数(二元交叉熵)</span></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [None,1], dtype = tf.float32),</span></span><br><span class="line">                              tf.TensorSpec(shape = [<span class="literal">None</span>,<span class="number">1</span>], dtype = tf.float32)])  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_func</span><span class="params">(self,y_true,y_pred)</span>:</span>  </span><br><span class="line">        <span class="comment">#将预测值限制在 1e-7 以上, 1 - 1e-7 以下，避免log(0)错误</span></span><br><span class="line">        eps = <span class="number">1e-7</span></span><br><span class="line">        y_pred = tf.clip_by_value(y_pred,eps,<span class="number">1.0</span>-eps)</span><br><span class="line">        bce = - y_true*tf.math.log(y_pred) - (<span class="number">1</span>-y_true)*tf.math.log(<span class="number">1</span>-y_pred)</span><br><span class="line">        <span class="keyword">return</span>  tf.reduce_mean(bce)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 评估指标(准确率)</span></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [None,1], dtype = tf.float32),</span></span><br><span class="line">                              tf.TensorSpec(shape = [<span class="literal">None</span>,<span class="number">1</span>], dtype = tf.float32)]) </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">metric_func</span><span class="params">(self,y_true,y_pred)</span>:</span></span><br><span class="line">        y_pred = tf.where(y_pred&gt;<span class="number">0.5</span>,tf.ones_like(y_pred,dtype = tf.float32),</span><br><span class="line">                          tf.zeros_like(y_pred,dtype = tf.float32))</span><br><span class="line">        acc = tf.reduce_mean(<span class="number">1</span>-tf.abs(y_true-y_pred))</span><br><span class="line">        <span class="keyword">return</span> acc</span><br><span class="line">    </span><br><span class="line">model = DNNModel()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试模型结构</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">(features,labels) = next(data_iter(X,Y,batch_size))</span><br><span class="line"></span><br><span class="line">predictions = model(features)</span><br><span class="line"></span><br><span class="line">loss = model.loss_func(labels,predictions)</span><br><span class="line">metric = model.metric_func(labels,predictions)</span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"init loss:"</span>,loss) <span class="comment"># init loss: 1.76568353</span></span><br><span class="line">tf.print(<span class="string">"init metric"</span>,metric) <span class="comment"># init metric 0.6</span></span><br><span class="line">print(len(model.trainable_variables)) <span class="comment"># 6</span></span><br></pre></td></tr></table></figure>
<p><strong>3，训练模型</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##使用autograph机制转换成静态图加速</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 正向传播求损失</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = model.loss_func(labels, predictions) </span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 反向传播求梯度</span></span><br><span class="line">    grads = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 执行梯度下降</span></span><br><span class="line">    <span class="keyword">for</span> p, dloss_dp <span class="keyword">in</span> zip(model.trainable_variables,grads):</span><br><span class="line">        p.assign(p - <span class="number">0.001</span>*dloss_dp)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 计算评估指标</span></span><br><span class="line">    metric = model.metric_func(labels,predictions)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss, metric</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> data_iter(X,Y,<span class="number">100</span>):</span><br><span class="line">            loss,metric = train_step(model,features,labels)</span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">"loss = "</span>,loss, <span class="string">"accuracy = "</span>, metric)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">train_model(model,epochs = <span class="number">600</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">35</span></span><br><span class="line">epoch = <span class="number">100</span> loss =  <span class="number">0.567795336</span> accuracy =  <span class="number">0.71</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">39</span></span><br><span class="line">epoch = <span class="number">200</span> loss =  <span class="number">0.50955683</span> accuracy =  <span class="number">0.77</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">43</span></span><br><span class="line">epoch = <span class="number">300</span> loss =  <span class="number">0.421476126</span> accuracy =  <span class="number">0.84</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">47</span></span><br><span class="line">epoch = <span class="number">400</span> loss =  <span class="number">0.330618203</span> accuracy =  <span class="number">0.9</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">51</span></span><br><span class="line">epoch = <span class="number">500</span> loss =  <span class="number">0.308296859</span> accuracy =  <span class="number">0.89</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">55</span></span><br><span class="line">epoch = <span class="number">600</span> loss =  <span class="number">0.279367268</span> accuracy =  <span class="number">0.96</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line">fig, (ax1,ax2) = plt.subplots(nrows=<span class="number">1</span>,ncols=<span class="number">2</span>,figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1.scatter(Xp[:,<span class="number">0</span>],Xp[:,<span class="number">1</span>],c = <span class="string">"r"</span>)</span><br><span class="line">ax1.scatter(Xn[:,<span class="number">0</span>],Xn[:,<span class="number">1</span>],c = <span class="string">"g"</span>)</span><br><span class="line">ax1.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax1.set_title(<span class="string">"y_true"</span>);</span><br><span class="line"></span><br><span class="line">Xp_pred = tf.boolean_mask(X,tf.squeeze(model(X)&gt;=<span class="number">0.5</span>),axis = <span class="number">0</span>)</span><br><span class="line">Xn_pred = tf.boolean_mask(X,tf.squeeze(model(X)&lt;<span class="number">0.5</span>),axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2.scatter(Xp_pred[:,<span class="number">0</span>],Xp_pred[:,<span class="number">1</span>],c = <span class="string">"r"</span>)</span><br><span class="line">ax2.scatter(Xn_pred[:,<span class="number">0</span>],Xn_pred[:,<span class="number">1</span>],c = <span class="string">"g"</span>)</span><br><span class="line">ax2.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax2.set_title(<span class="string">"y_pred"</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/3-1-04-%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<h2 id="zhong-jie-api">中阶API</h2>
<p>使用TensorFlow的中阶API实现线性回归模型和和DNN二分类模型。</p>
<p>TensorFlow的中阶API主要包括：</p>
<ol>
<li>各种模型层</li>
<li>损失函数</li>
<li>优化器</li>
<li>数据管道</li>
<li>特征列</li>
<li>…</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure>
<h3 id="xian-xing-hui-gui-mo-xing-1">线性回归模型</h3>
<h4 id="zhun-bei-shu-ju-6">准备数据</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,losses,metrics,optimizers</span><br><span class="line"></span><br><span class="line"><span class="comment">#样本数量</span></span><br><span class="line">n = <span class="number">400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成测试用数据集</span></span><br><span class="line">X = tf.random.uniform([n,<span class="number">2</span>],minval=<span class="number">-10</span>,maxval=<span class="number">10</span>) </span><br><span class="line">w0 = tf.constant([[<span class="number">2.0</span>],[<span class="number">-3.0</span>]])</span><br><span class="line">b0 = tf.constant([[<span class="number">3.0</span>]])</span><br><span class="line">Y = X@w0 + b0 + tf.random.normal([n,<span class="number">1</span>],mean = <span class="number">0.0</span>,stddev= <span class="number">2.0</span>)  <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], c = <span class="string">"b"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], c = <span class="string">"g"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/3-2-01-%E5%9B%9E%E5%BD%92%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构建输入数据管道</span></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices((X,Y)) \</span><br><span class="line">     .shuffle(buffer_size = <span class="number">100</span>).batch(<span class="number">10</span>) \</span><br><span class="line">     .prefetch(tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure>
<h4 id="ding-yi-mo-xing-6">定义模型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.build(input_shape = (<span class="number">2</span>,)) <span class="comment">#用build方法创建variables</span></span><br><span class="line">model.loss_func = losses.mean_squared_error</span><br><span class="line">model.optimizer = optimizers.SGD(learning_rate=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>
<h4 id="xun-lian-mo-xing-5">训练模型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用autograph机制转换成静态图加速</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = model.loss_func(tf.reshape(labels,[<span class="number">-1</span>]), tf.reshape(predictions,[<span class="number">-1</span>]))</span><br><span class="line">    grads = tape.gradient(loss,model.variables)</span><br><span class="line">    model.optimizer.apply_gradients(zip(grads,model.variables))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试train_step效果</span></span><br><span class="line">features,labels = next(ds.as_numpy_iterator())</span><br><span class="line">train_step(model,features,labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        loss = tf.constant(<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds:</span><br><span class="line">            loss = train_step(model,features,labels)</span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">50</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">"loss = "</span>,loss)</span><br><span class="line">            tf.print(<span class="string">"w ="</span>,model.variables[<span class="number">0</span>])</span><br><span class="line">            tf.print(<span class="string">"b ="</span>,model.variables[<span class="number">1</span>])</span><br><span class="line">train_model(model,epochs = <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">01</span>:<span class="number">48</span></span><br><span class="line">epoch = <span class="number">50</span> loss =  <span class="number">2.56481647</span></span><br><span class="line">w = [[<span class="number">1.99355531</span>]</span><br><span class="line"> [<span class="number">-2.99061537</span>]]</span><br><span class="line">b = [<span class="number">3.09484935</span>]</span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">01</span>:<span class="number">51</span></span><br><span class="line">epoch = <span class="number">100</span> loss =  <span class="number">5.96198225</span></span><br><span class="line">w = [[<span class="number">1.98028314</span>]</span><br><span class="line"> [<span class="number">-2.96975136</span>]]</span><br><span class="line">b = [<span class="number">3.09501529</span>]</span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">01</span>:<span class="number">54</span></span><br><span class="line">epoch = <span class="number">150</span> loss =  <span class="number">4.79625702</span></span><br><span class="line">w = [[<span class="number">2.00056171</span>]</span><br><span class="line"> [<span class="number">-2.98774862</span>]]</span><br><span class="line">b = [<span class="number">3.09567738</span>]</span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">01</span>:<span class="number">58</span></span><br><span class="line">epoch = <span class="number">200</span> loss =  <span class="number">8.26704407</span></span><br><span class="line">w = [[<span class="number">2.00282311</span>]</span><br><span class="line"> [<span class="number">-2.99300027</span>]]</span><br><span class="line">b = [<span class="number">3.09406662</span>]</span><br></pre></td></tr></table></figure>
<h4 id="jie-guo-ke-shi-hua">结果可视化</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line">w,b = model.variables</span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], c = <span class="string">"b"</span>,label = <span class="string">"samples"</span>)</span><br><span class="line">ax1.plot(X[:,<span class="number">0</span>],w[<span class="number">0</span>]*X[:,<span class="number">0</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">"model"</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], c = <span class="string">"g"</span>,label = <span class="string">"samples"</span>)</span><br><span class="line">ax2.plot(X[:,<span class="number">1</span>],w[<span class="number">1</span>]*X[:,<span class="number">1</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">"model"</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/3-2-02-%E5%9B%9E%E5%BD%92%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<h3 id="dnn-er-fen-lei-mo-xing-1">DNN二分类模型</h3>
<h4 id="zhun-bei-shu-ju-7">准备数据</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,losses,metrics,optimizers</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#正负样本数量</span></span><br><span class="line">n_positive,n_negative = <span class="number">2000</span>,<span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成正样本, 小圆环分布</span></span><br><span class="line">r_p = <span class="number">5.0</span> + tf.random.truncated_normal([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_p = tf.random.uniform([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xp = tf.concat([r_p*tf.cos(theta_p),r_p*tf.sin(theta_p)],axis = <span class="number">1</span>)</span><br><span class="line">Yp = tf.ones_like(r_p)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成负样本, 大圆环分布</span></span><br><span class="line">r_n = <span class="number">8.0</span> + tf.random.truncated_normal([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_n = tf.random.uniform([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xn = tf.concat([r_n*tf.cos(theta_n),r_n*tf.sin(theta_n)],axis = <span class="number">1</span>)</span><br><span class="line">Yn = tf.zeros_like(r_n)</span><br><span class="line"></span><br><span class="line"><span class="comment">#汇总样本</span></span><br><span class="line">X = tf.concat([Xp,Xn],axis = <span class="number">0</span>)</span><br><span class="line">Y = tf.concat([Yp,Yn],axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line">plt.figure(figsize = (<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">plt.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">plt.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/3-1-03-%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96-4135196.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构建输入数据管道</span></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices((X,Y)) \</span><br><span class="line">     .shuffle(buffer_size = <span class="number">4000</span>).batch(<span class="number">100</span>) \</span><br><span class="line">     .prefetch(tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure>
<h4 id="ding-yi-mo-xing-7">定义模型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DNNModel</span><span class="params">(tf.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name = None)</span>:</span></span><br><span class="line">        super(DNNModel, self).__init__(name=name)</span><br><span class="line">        self.dense1 = layers.Dense(<span class="number">4</span>,activation = <span class="string">"relu"</span>) </span><br><span class="line">        self.dense2 = layers.Dense(<span class="number">8</span>,activation = <span class="string">"relu"</span>)</span><br><span class="line">        self.dense3 = layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>)</span><br><span class="line"></span><br><span class="line">     </span><br><span class="line">    <span class="comment"># 正向传播</span></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [None,2], dtype = tf.float32)])  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x = self.dense1(x)</span><br><span class="line">        x = self.dense2(x)</span><br><span class="line">        y = self.dense3(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line">model = DNNModel()</span><br><span class="line">model.loss_func = losses.binary_crossentropy</span><br><span class="line">model.metric_func = metrics.binary_accuracy</span><br><span class="line">model.optimizer = optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试模型结构</span></span><br><span class="line">(features,labels) = next(ds.as_numpy_iterator())</span><br><span class="line"></span><br><span class="line">predictions = model(features)</span><br><span class="line"></span><br><span class="line">loss = model.loss_func(tf.reshape(labels,[<span class="number">-1</span>]),tf.reshape(predictions,[<span class="number">-1</span>]))</span><br><span class="line">metric = model.metric_func(tf.reshape(labels,[<span class="number">-1</span>]),tf.reshape(predictions,[<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"init loss:"</span>,loss) <span class="comment"># init loss: 1.13653195</span></span><br><span class="line">tf.print(<span class="string">"init metric"</span>,metric) <span class="comment"># init metric 0.5</span></span><br></pre></td></tr></table></figure>
<h4 id="xun-lian-mo-xing-6">训练模型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用autograph机制转换成静态图加速</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = model.loss_func(tf.reshape(labels,[<span class="number">-1</span>]), tf.reshape(predictions,[<span class="number">-1</span>]))</span><br><span class="line">    grads = tape.gradient(loss,model.trainable_variables)</span><br><span class="line">    model.optimizer.apply_gradients(zip(grads,model.trainable_variables))</span><br><span class="line">    </span><br><span class="line">    metric = model.metric_func(tf.reshape(labels,[<span class="number">-1</span>]), tf.reshape(predictions,[<span class="number">-1</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss,metric</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试train_step效果</span></span><br><span class="line">features,labels = next(ds.as_numpy_iterator())</span><br><span class="line">train_step(model,features,labels)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">(&lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">1.2033114</span>&gt;,&lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">0.47</span>&gt;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        loss, metric = tf.constant(<span class="number">0.0</span>),tf.constant(<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds:</span><br><span class="line">            loss,metric = train_step(model,features,labels)</span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">"loss = "</span>,loss, <span class="string">"accuracy = "</span>,metric)</span><br><span class="line">train_model(model,epochs = <span class="number">60</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">36</span></span><br><span class="line">epoch = <span class="number">10</span> loss =  <span class="number">0.556449413</span> accuracy =  <span class="number">0.79</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">38</span></span><br><span class="line">epoch = <span class="number">20</span> loss =  <span class="number">0.439187407</span> accuracy =  <span class="number">0.86</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">40</span></span><br><span class="line">epoch = <span class="number">30</span> loss =  <span class="number">0.259921253</span> accuracy =  <span class="number">0.95</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">42</span></span><br><span class="line">epoch = <span class="number">40</span> loss =  <span class="number">0.244920313</span> accuracy =  <span class="number">0.9</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">43</span></span><br><span class="line">epoch = <span class="number">50</span> loss =  <span class="number">0.19839409</span> accuracy =  <span class="number">0.92</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">45</span></span><br><span class="line">epoch = <span class="number">60</span> loss =  <span class="number">0.126151696</span> accuracy =  <span class="number">0.95</span></span><br></pre></td></tr></table></figure>
<h4 id="jie-guo-ke-shi-hua-1">结果可视化</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">fig, (ax1,ax2) = plt.subplots(nrows=<span class="number">1</span>,ncols=<span class="number">2</span>,figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">ax1.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">ax1.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax1.set_title(<span class="string">"y_true"</span>);</span><br><span class="line"></span><br><span class="line">Xp_pred = tf.boolean_mask(X,tf.squeeze(model(X)&gt;=<span class="number">0.5</span>),axis = <span class="number">0</span>)</span><br><span class="line">Xn_pred = tf.boolean_mask(X,tf.squeeze(model(X)&lt;<span class="number">0.5</span>),axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2.scatter(Xp_pred[:,<span class="number">0</span>].numpy(),Xp_pred[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">ax2.scatter(Xn_pred[:,<span class="number">0</span>].numpy(),Xn_pred[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">ax2.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax2.set_title(<span class="string">"y_pred"</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/3-2-04-%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<h2 id="gao-jie-api">高阶API</h2>
<p>使用TensorFlow的高阶API实现线性回归模型和DNN二分类模型。TensorFlow的高阶API主要为tf.keras.models提供的模型的类接口。</p>
<p>使用Keras接口有以下3种方式构建模型：</p>
<ol>
<li>使用Sequential按层顺序构建模型</li>
<li>使用函数式API构建任意结构模型</li>
<li>继承Model基类构建自定义模型。</li>
</ol>
<p>此处分别使用Sequential按层顺序构建模型以及继承Model基类构建自定义模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure>
<h3 id="xian-xing-hui-gui-mo-xing-2">线性回归模型</h3>
<p>使用Sequential按层顺序构建模型，并使用内置model.fit方法训练模型。</p>
<h4 id="zhun-bei-shu-ju-8">准备数据</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,losses,metrics,optimizers</span><br><span class="line"></span><br><span class="line"><span class="comment">#样本数量</span></span><br><span class="line">n = <span class="number">400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成测试用数据集</span></span><br><span class="line">X = tf.random.uniform([n,<span class="number">2</span>],minval=<span class="number">-10</span>,maxval=<span class="number">10</span>) </span><br><span class="line">w0 = tf.constant([[<span class="number">2.0</span>],[<span class="number">-3.0</span>]])</span><br><span class="line">b0 = tf.constant([[<span class="number">3.0</span>]])</span><br><span class="line">Y = X@w0 + b0 + tf.random.normal([n,<span class="number">1</span>],mean = <span class="number">0.0</span>,stddev= <span class="number">2.0</span>)  <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], c = <span class="string">"b"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], c = <span class="string">"g"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/3-3-01-%E5%9B%9E%E5%BD%92%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<h4 id="ding-yi-mo-xing-8">定义模型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>,input_shape =(<span class="number">2</span>,)))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">dense (Dense)                (None, 1)                 3         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 3</span><br><span class="line">Trainable params: 3</span><br><span class="line">Non-trainable params: 0</span><br></pre></td></tr></table></figure>
<h4 id="xun-lian-mo-xing-7">训练模型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 使用fit方法进行训练</span></span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">"adam"</span>,loss=<span class="string">"mse"</span>,metrics=[<span class="string">"mae"</span>])</span><br><span class="line">model.fit(X,Y,batch_size = <span class="number">10</span>,epochs = <span class="number">200</span>)  </span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"w = "</span>,model.layers[<span class="number">0</span>].kernel)</span><br><span class="line">tf.print(<span class="string">"b = "</span>,model.layers[<span class="number">0</span>].bias)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">197</span>/<span class="number">200</span></span><br><span class="line"><span class="number">400</span>/<span class="number">400</span> [==============================] - <span class="number">0</span>s <span class="number">190</span>us/sample - loss: <span class="number">4.3977</span> - mae: <span class="number">1.7129</span></span><br><span class="line">Epoch <span class="number">198</span>/<span class="number">200</span></span><br><span class="line"><span class="number">400</span>/<span class="number">400</span> [==============================] - <span class="number">0</span>s <span class="number">172</span>us/sample - loss: <span class="number">4.3918</span> - mae: <span class="number">1.7117</span></span><br><span class="line">Epoch <span class="number">199</span>/<span class="number">200</span></span><br><span class="line"><span class="number">400</span>/<span class="number">400</span> [==============================] - <span class="number">0</span>s <span class="number">134</span>us/sample - loss: <span class="number">4.3861</span> - mae: <span class="number">1.7106</span></span><br><span class="line">Epoch <span class="number">200</span>/<span class="number">200</span></span><br><span class="line"><span class="number">400</span>/<span class="number">400</span> [==============================] - <span class="number">0</span>s <span class="number">166</span>us/sample - loss: <span class="number">4.3786</span> - mae: <span class="number">1.7092</span></span><br><span class="line">w =  [[<span class="number">1.99339032</span>]</span><br><span class="line"> [<span class="number">-3.00866461</span>]]</span><br><span class="line">b =  [<span class="number">2.67018795</span>]</span><br></pre></td></tr></table></figure>
<h4 id="jie-guo-ke-shi-hua-2">结果可视化</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line">w,b = model.variables</span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], c = <span class="string">"b"</span>,label = <span class="string">"samples"</span>)</span><br><span class="line">ax1.plot(X[:,<span class="number">0</span>],w[<span class="number">0</span>]*X[:,<span class="number">0</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">"model"</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], c = <span class="string">"g"</span>,label = <span class="string">"samples"</span>)</span><br><span class="line">ax2.plot(X[:,<span class="number">1</span>],w[<span class="number">1</span>]*X[:,<span class="number">1</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">"model"</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/3-3-02-%E5%9B%9E%E5%BD%92%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<h3 id="dnn-er-fen-lei-mo-xing-2">DNN二分类模型</h3>
<p>此范例我们使用继承Model基类构建自定义模型，并构建自定义训练循环【面向专家】</p>
<h4 id="zhun-bei-shu-ju-9">准备数据</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,losses,metrics,optimizers</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#正负样本数量</span></span><br><span class="line">n_positive,n_negative = <span class="number">2000</span>,<span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成正样本, 小圆环分布</span></span><br><span class="line">r_p = <span class="number">5.0</span> + tf.random.truncated_normal([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_p = tf.random.uniform([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xp = tf.concat([r_p*tf.cos(theta_p),r_p*tf.sin(theta_p)],axis = <span class="number">1</span>)</span><br><span class="line">Yp = tf.ones_like(r_p)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成负样本, 大圆环分布</span></span><br><span class="line">r_n = <span class="number">8.0</span> + tf.random.truncated_normal([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_n = tf.random.uniform([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xn = tf.concat([r_n*tf.cos(theta_n),r_n*tf.sin(theta_n)],axis = <span class="number">1</span>)</span><br><span class="line">Yn = tf.zeros_like(r_n)</span><br><span class="line"></span><br><span class="line"><span class="comment">#汇总样本</span></span><br><span class="line">X = tf.concat([Xp,Xn],axis = <span class="number">0</span>)</span><br><span class="line">Y = tf.concat([Yp,Yn],axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#样本洗牌</span></span><br><span class="line">data = tf.concat([X,Y],axis = <span class="number">1</span>)</span><br><span class="line">data = tf.random.shuffle(data)</span><br><span class="line">X = data[:,:<span class="number">2</span>]</span><br><span class="line">Y = data[:,<span class="number">2</span>:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line">plt.figure(figsize = (<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">plt.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">plt.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/3-3-03-%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ds_train = tf.data.Dataset.from_tensor_slices((X[<span class="number">0</span>:n*<span class="number">3</span>//<span class="number">4</span>,:],Y[<span class="number">0</span>:n*<span class="number">3</span>//<span class="number">4</span>,:])) \</span><br><span class="line">     .shuffle(buffer_size = <span class="number">1000</span>).batch(<span class="number">20</span>) \</span><br><span class="line">     .prefetch(tf.data.experimental.AUTOTUNE) \</span><br><span class="line">     .cache()</span><br><span class="line"></span><br><span class="line">ds_valid = tf.data.Dataset.from_tensor_slices((X[n*<span class="number">3</span>//<span class="number">4</span>:,:],Y[n*<span class="number">3</span>//<span class="number">4</span>:,:])) \</span><br><span class="line">     .batch(<span class="number">20</span>) \</span><br><span class="line">     .prefetch(tf.data.experimental.AUTOTUNE) \</span><br><span class="line">     .cache()</span><br></pre></td></tr></table></figure>
<h4 id="ding-yi-mo-xing-9">定义模型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DNNModel</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(DNNModel, self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self,input_shape)</span>:</span></span><br><span class="line">        self.dense1 = layers.Dense(<span class="number">4</span>,activation = <span class="string">"relu"</span>,name = <span class="string">"dense1"</span>) </span><br><span class="line">        self.dense2 = layers.Dense(<span class="number">8</span>,activation = <span class="string">"relu"</span>,name = <span class="string">"dense2"</span>)</span><br><span class="line">        self.dense3 = layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>,name = <span class="string">"dense3"</span>)</span><br><span class="line">        super(DNNModel,self).build(input_shape)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 正向传播</span></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [None,2], dtype = tf.float32)])  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x = self.dense1(x)</span><br><span class="line">        x = self.dense2(x)</span><br><span class="line">        y = self.dense3(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">model = DNNModel()</span><br><span class="line">model.build(input_shape =(<span class="literal">None</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Model: "dnn_model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">dense1 (Dense)               multiple                  12        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense2 (Dense)               multiple                  40        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense3 (Dense)               multiple                  9         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 61</span><br><span class="line">Trainable params: 61</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<h4 id="xun-lian-mo-xing-8">训练模型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 自定义训练循环</span></span><br><span class="line"></span><br><span class="line">optimizer = optimizers.Adam(learning_rate=<span class="number">0.01</span>)</span><br><span class="line">loss_func = tf.keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">train_loss = tf.keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_metric = tf.keras.metrics.BinaryAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">valid_loss = tf.keras.metrics.Mean(name=<span class="string">'valid_loss'</span>)</span><br><span class="line">valid_metric = tf.keras.metrics.BinaryAccuracy(name=<span class="string">'valid_accuracy'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = loss_func(labels, predictions)</span><br><span class="line">    grads = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(grads, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss.update_state(loss)</span><br><span class="line">    train_metric.update_state(labels, predictions)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">valid_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    predictions = model(features)</span><br><span class="line">    batch_loss = loss_func(labels, predictions)</span><br><span class="line">    valid_loss.update_state(batch_loss)</span><br><span class="line">    valid_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,ds_train,ds_valid,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_train:</span><br><span class="line">            train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_valid:</span><br><span class="line">            valid_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        logs = <span class="string">'Epoch=&#123;&#125;,Loss:&#123;&#125;,Accuracy:&#123;&#125;,Valid Loss:&#123;&#125;,Valid Accuracy:&#123;&#125;'</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>  epoch%<span class="number">100</span> ==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(tf.strings.format(logs,</span><br><span class="line">            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))</span><br><span class="line">        </span><br><span class="line">        train_loss.reset_states()</span><br><span class="line">        valid_loss.reset_states()</span><br><span class="line">        train_metric.reset_states()</span><br><span class="line">        valid_metric.reset_states()</span><br><span class="line"></span><br><span class="line">train_model(model,ds_train,ds_valid,<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">35</span>:<span class="number">02</span></span><br><span class="line">Epoch=<span class="number">100</span>,Loss:<span class="number">0.194088802</span>,Accuracy:<span class="number">0.923064</span>,Valid Loss:<span class="number">0.215538561</span>,Valid Accuracy:<span class="number">0.904368</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">35</span>:<span class="number">22</span></span><br><span class="line">Epoch=<span class="number">200</span>,Loss:<span class="number">0.151239693</span>,Accuracy:<span class="number">0.93768847</span>,Valid Loss:<span class="number">0.181166962</span>,Valid Accuracy:<span class="number">0.920664132</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">35</span>:<span class="number">43</span></span><br><span class="line">Epoch=<span class="number">300</span>,Loss:<span class="number">0.134556711</span>,Accuracy:<span class="number">0.944247484</span>,Valid Loss:<span class="number">0.171530813</span>,Valid Accuracy:<span class="number">0.926396072</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">36</span>:<span class="number">04</span></span><br><span class="line">Epoch=<span class="number">400</span>,Loss:<span class="number">0.125722557</span>,Accuracy:<span class="number">0.949172914</span>,Valid Loss:<span class="number">0.16731061</span>,Valid Accuracy:<span class="number">0.929318547</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">36</span>:<span class="number">24</span></span><br><span class="line">Epoch=<span class="number">500</span>,Loss:<span class="number">0.120216407</span>,Accuracy:<span class="number">0.952525079</span>,Valid Loss:<span class="number">0.164817035</span>,Valid Accuracy:<span class="number">0.931044817</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">36</span>:<span class="number">44</span></span><br><span class="line">Epoch=<span class="number">600</span>,Loss:<span class="number">0.116434008</span>,Accuracy:<span class="number">0.954830289</span>,Valid Loss:<span class="number">0.163089141</span>,Valid Accuracy:<span class="number">0.932202339</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">05</span></span><br><span class="line">Epoch=<span class="number">700</span>,Loss:<span class="number">0.113658346</span>,Accuracy:<span class="number">0.956433</span>,Valid Loss:<span class="number">0.161804497</span>,Valid Accuracy:<span class="number">0.933092058</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">25</span></span><br><span class="line">Epoch=<span class="number">800</span>,Loss:<span class="number">0.111522928</span>,Accuracy:<span class="number">0.957467675</span>,Valid Loss:<span class="number">0.160796657</span>,Valid Accuracy:<span class="number">0.93379426</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">46</span></span><br><span class="line">Epoch=<span class="number">900</span>,Loss:<span class="number">0.109816991</span>,Accuracy:<span class="number">0.958205402</span>,Valid Loss:<span class="number">0.159987748</span>,Valid Accuracy:<span class="number">0.934343576</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">38</span>:<span class="number">06</span></span><br><span class="line">Epoch=<span class="number">1000</span>,Loss:<span class="number">0.10841465</span>,Accuracy:<span class="number">0.958805501</span>,Valid Loss:<span class="number">0.159325734</span>,Valid Accuracy:<span class="number">0.934785843</span></span><br></pre></td></tr></table></figure>
<h4 id="jie-guo-ke-shi-hua-3">结果可视化</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">fig, (ax1,ax2) = plt.subplots(nrows=<span class="number">1</span>,ncols=<span class="number">2</span>,figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">ax1.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">ax1.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax1.set_title(<span class="string">"y_true"</span>);</span><br><span class="line"></span><br><span class="line">Xp_pred = tf.boolean_mask(X,tf.squeeze(model(X)&gt;=<span class="number">0.5</span>),axis = <span class="number">0</span>)</span><br><span class="line">Xn_pred = tf.boolean_mask(X,tf.squeeze(model(X)&lt;<span class="number">0.5</span>),axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2.scatter(Xp_pred[:,<span class="number">0</span>].numpy(),Xp_pred[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">ax2.scatter(Xn_pred[:,<span class="number">0</span>].numpy(),Xn_pred[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">ax2.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax2.set_title(<span class="string">"y_pred"</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/3-3-04-%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<h1 id="tensor-flow-de-di-jie-api">TensorFlow的低阶API</h1>
<p>TensorFlow的低阶API主要包括张量操作，计算图和自动微分。如果把模型比作一个房子，那么低阶API就是【模型之砖】。</p>
<p>在低阶API层次上，可以把TensorFlow当做一个增强版的numpy来使用。TensorFlow提供的方法比numpy更全面，运算速度更快，如果需要的话，还可以使用GPU进行加速。</p>
<p>前面几章对低阶API已经有了一个整体的认识，本章将重点详细介绍张量操作和Autograph计算图。</p>
<p>张量的操作主要包括张量的结构操作和张量的数学运算。</p>
<p>张量结构操作诸如：张量创建，索引切片，维度变换，合并分割。</p>
<p>张量数学运算主要有：标量运算，向量运算，矩阵运算，张量运算的广播机制。</p>
<p>Autograph计算图我们将介绍使用Autograph的规范建议，Autograph的机制原理，Autograph和tf.Module.</p>
<h2 id="zhang-liang-de-jie-gou-cao-zuo">张量的结构操作</h2>
<p>张量的操作主要包括张量的结构操作和张量的数学运算。</p>
<p>张量结构操作诸如：张量创建，索引切片，维度变换，合并分割。</p>
<p>张量数学运算主要有：标量运算，向量运算，矩阵运算。另外我们会介绍张量运算的广播机制。</p>
<p>本篇我们介绍张量的结构操作。</p>
<h3 id="chuang-jian-zhang-liang">创建张量</h3>
<p>张量创建的许多方法和numpy中创建array的方法很像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],dtype = tf.float32)</span><br><span class="line">tf.print(a) <span class="comment"># [1 2 3]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = tf.range(<span class="number">1</span>,<span class="number">10</span>,delta = <span class="number">2</span>)</span><br><span class="line">tf.print(b) <span class="comment"># [1 3 5 7 9]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = tf.linspace(<span class="number">0.0</span>,<span class="number">2</span>*<span class="number">3.14</span>,<span class="number">100</span>)</span><br><span class="line">tf.print(c) <span class="comment"># [0 0.0634343475 0.126868695 ... 6.15313148 6.21656609 6.28]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">d = tf.zeros([<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line">tf.print(d)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = tf.ones([<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line">b = tf.zeros_like(a,dtype= tf.float32)</span><br><span class="line">tf.print(a)</span><br><span class="line">tf.print(b)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]]</span><br><span class="line">[[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">b = tf.fill([<span class="number">3</span>,<span class="number">2</span>],<span class="number">5</span>)</span><br><span class="line">tf.print(b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">5</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">5</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#均匀分布随机</span></span><br><span class="line">tf.random.set_seed(<span class="number">1.0</span>)</span><br><span class="line">a = tf.random.uniform([<span class="number">5</span>],minval=<span class="number">0</span>,maxval=<span class="number">10</span>)</span><br><span class="line">tf.print(a) <span class="comment"># [1.65130854 9.01481247 6.30974197 4.34546089 2.9193902]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#正态分布随机</span></span><br><span class="line">b = tf.random.normal([<span class="number">3</span>,<span class="number">3</span>],mean=<span class="number">0.0</span>,stddev=<span class="number">1.0</span>)</span><br><span class="line">tf.print(b)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">0.403087884</span> <span class="number">-1.0880208</span> <span class="number">-0.0630953535</span>]</span><br><span class="line"> [<span class="number">1.33655667</span> <span class="number">0.711760104</span> <span class="number">-0.489286453</span>]</span><br><span class="line"> [<span class="number">-0.764221311</span> <span class="number">-1.03724861</span> <span class="number">-1.25193381</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#正态分布随机，剔除2倍方差以外数据重新生成</span></span><br><span class="line">c = tf.random.truncated_normal((<span class="number">5</span>,<span class="number">5</span>), mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, dtype=tf.float32)</span><br><span class="line">tf.print(c)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">-0.457012236</span> <span class="number">-0.406867266</span> <span class="number">0.728577733</span> <span class="number">-0.892977774</span> <span class="number">-0.369404584</span>]</span><br><span class="line"> [<span class="number">0.323488563</span> <span class="number">1.19383323</span> <span class="number">0.888299048</span> <span class="number">1.25985599</span> <span class="number">-1.95951891</span>]</span><br><span class="line"> [<span class="number">-0.202244401</span> <span class="number">0.294496894</span> <span class="number">-0.468728036</span> <span class="number">1.29494202</span> <span class="number">1.48142183</span>]</span><br><span class="line"> [<span class="number">0.0810953453</span> <span class="number">1.63843894</span> <span class="number">0.556645</span> <span class="number">0.977199793</span> <span class="number">-1.17777884</span>]</span><br><span class="line"> [<span class="number">1.67368948</span> <span class="number">0.0647980496</span> <span class="number">-0.705142677</span> <span class="number">-0.281972528</span> <span class="number">0.126546144</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特殊矩阵</span></span><br><span class="line">I = tf.eye(<span class="number">3</span>,<span class="number">3</span>) <span class="comment">#单位矩阵</span></span><br><span class="line">tf.print(I)</span><br><span class="line">tf.print(<span class="string">" "</span>)</span><br><span class="line">t = tf.linalg.diag([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) <span class="comment">#对角阵</span></span><br><span class="line">tf.print(t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">1</span>]]</span><br><span class="line"> </span><br><span class="line">[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">2</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">3</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="suo-yin-qie-pian">索引切片</h3>
<p>张量的索引切片方式和numpy几乎是一样的。切片时支持缺省参数和省略号。</p>
<p>对于tf.Variable,可以通过索引和切片对部分元素进行修改。</p>
<p>对于提取张量的连续子区域，也可以使用tf.slice.</p>
<p>此外，对于不规则的切片提取,可以使用tf.gather,tf.gather_nd,tf.boolean_mask。</p>
<p>tf.boolean_mask功能最为强大，它可以实现tf.gather,tf.gather_nd的功能，并且tf.boolean_mask还可以实现布尔索引。</p>
<p>如果要通过修改张量的某些元素得到新的张量，可以使用tf.where，tf.scatter_nd。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">3</span>)</span><br><span class="line">t = tf.random.uniform([<span class="number">5</span>,<span class="number">5</span>],minval=<span class="number">0</span>,maxval=<span class="number">10</span>,dtype=tf.int32)</span><br><span class="line">tf.print(t)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">4</span> <span class="number">7</span> <span class="number">4</span> <span class="number">2</span> <span class="number">9</span>]</span><br><span class="line"> [<span class="number">9</span> <span class="number">1</span> <span class="number">2</span> <span class="number">4</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">7</span> <span class="number">2</span> <span class="number">7</span> <span class="number">4</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">9</span> <span class="number">6</span> <span class="number">9</span> <span class="number">7</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">7</span> <span class="number">0</span> <span class="number">0</span> <span class="number">3</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#第0行</span></span><br><span class="line">tf.print(t[<span class="number">0</span>]) <span class="comment"># [4 7 4 2 9]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#倒数第一行</span></span><br><span class="line">tf.print(t[<span class="number">-1</span>]) <span class="comment"># [3 7 0 0 3]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#第1行第3列</span></span><br><span class="line">tf.print(t[<span class="number">1</span>,<span class="number">3</span>]) <span class="comment"># 4</span></span><br><span class="line">tf.print(t[<span class="number">1</span>][<span class="number">3</span>]) <span class="comment"># 4</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#第1行至第3行</span></span><br><span class="line">tf.print(t[<span class="number">1</span>:<span class="number">4</span>,:])</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">9</span> <span class="number">1</span> <span class="number">2</span> <span class="number">4</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">7</span> <span class="number">2</span> <span class="number">7</span> <span class="number">4</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">9</span> <span class="number">6</span> <span class="number">9</span> <span class="number">7</span> <span class="number">2</span>]]</span><br><span class="line"></span><br><span class="line">tf.print(tf.slice(t,[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">3</span>,<span class="number">5</span>])) <span class="comment">#tf.slice(input,begin_vector,size_vector)</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">9</span> <span class="number">1</span> <span class="number">2</span> <span class="number">4</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">7</span> <span class="number">2</span> <span class="number">7</span> <span class="number">4</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">9</span> <span class="number">6</span> <span class="number">9</span> <span class="number">7</span> <span class="number">2</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#第1行至最后一行，第0列到最后一列每隔两列取一列</span></span><br><span class="line">tf.print(t[<span class="number">1</span>:<span class="number">4</span>,:<span class="number">4</span>:<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">9</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">7</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">9</span> <span class="number">9</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对变量来说，还可以使用索引和切片修改部分元素</span></span><br><span class="line">x = tf.Variable([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]],dtype = tf.float32)</span><br><span class="line">x[<span class="number">1</span>,:].assign(tf.constant([<span class="number">0.0</span>,<span class="number">0.0</span>]))</span><br><span class="line">tf.print(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.uniform([<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>],minval=<span class="number">0</span>,maxval=<span class="number">10</span>,dtype=tf.int32)</span><br><span class="line">tf.print(a)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[[<span class="number">7</span> <span class="number">3</span> <span class="number">9</span>]</span><br><span class="line">  [<span class="number">9</span> <span class="number">0</span> <span class="number">7</span>]</span><br><span class="line">  [<span class="number">9</span> <span class="number">6</span> <span class="number">7</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">3</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">8</span> <span class="number">1</span>]</span><br><span class="line">  [<span class="number">3</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">4</span> <span class="number">0</span> <span class="number">6</span>]</span><br><span class="line">  [<span class="number">6</span> <span class="number">2</span> <span class="number">2</span>]</span><br><span class="line">  [<span class="number">7</span> <span class="number">9</span> <span class="number">5</span>]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#省略号可以表示多个冒号</span></span><br><span class="line">tf.print(a[...,<span class="number">1</span>])</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">3</span> <span class="number">0</span> <span class="number">6</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">8</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">2</span> <span class="number">9</span>]]</span><br></pre></td></tr></table></figure>
<p>以上切片方式相对规则，对于不规则的切片提取,可以使用tf.gather,tf.gather_nd,tf.boolean_mask。</p>
<p>考虑班级成绩册的例子，有4个班级，每个班级10个学生，每个学生7门科目成绩。可以用一个4×10×7的张量来表示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">scores = tf.random.uniform((<span class="number">4</span>,<span class="number">10</span>,<span class="number">7</span>),minval=<span class="number">0</span>,maxval=<span class="number">100</span>,dtype=tf.int32)</span><br><span class="line">tf.print(scores)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[[<span class="number">52</span> <span class="number">82</span> <span class="number">66</span> ... <span class="number">17</span> <span class="number">86</span> <span class="number">14</span>]</span><br><span class="line">  [<span class="number">8</span> <span class="number">36</span> <span class="number">94</span> ... <span class="number">13</span> <span class="number">78</span> <span class="number">41</span>]</span><br><span class="line">  [<span class="number">77</span> <span class="number">53</span> <span class="number">51</span> ... <span class="number">22</span> <span class="number">91</span> <span class="number">56</span>]</span><br><span class="line">  ...</span><br><span class="line">  [<span class="number">11</span> <span class="number">19</span> <span class="number">26</span> ... <span class="number">89</span> <span class="number">86</span> <span class="number">68</span>]</span><br><span class="line">  [<span class="number">60</span> <span class="number">72</span> <span class="number">0</span> ... <span class="number">11</span> <span class="number">26</span> <span class="number">15</span>]</span><br><span class="line">  [<span class="number">24</span> <span class="number">99</span> <span class="number">38</span> ... <span class="number">97</span> <span class="number">44</span> <span class="number">74</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">79</span> <span class="number">73</span> <span class="number">73</span> ... <span class="number">35</span> <span class="number">3</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">83</span> <span class="number">36</span> <span class="number">31</span> ... <span class="number">75</span> <span class="number">38</span> <span class="number">85</span>]</span><br><span class="line">  [<span class="number">54</span> <span class="number">26</span> <span class="number">67</span> ... <span class="number">60</span> <span class="number">68</span> <span class="number">98</span>]</span><br><span class="line">  ...</span><br><span class="line">  [<span class="number">20</span> <span class="number">5</span> <span class="number">18</span> ... <span class="number">32</span> <span class="number">45</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">72</span> <span class="number">52</span> <span class="number">81</span> ... <span class="number">88</span> <span class="number">41</span> <span class="number">20</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">21</span> <span class="number">89</span> ... <span class="number">53</span> <span class="number">10</span> <span class="number">90</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">52</span> <span class="number">80</span> <span class="number">22</span> ... <span class="number">29</span> <span class="number">25</span> <span class="number">60</span>]</span><br><span class="line">  [<span class="number">78</span> <span class="number">71</span> <span class="number">54</span> ... <span class="number">43</span> <span class="number">98</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">21</span> <span class="number">66</span> <span class="number">53</span> ... <span class="number">97</span> <span class="number">75</span> <span class="number">77</span>]</span><br><span class="line">  ...</span><br><span class="line">  [<span class="number">6</span> <span class="number">74</span> <span class="number">3</span> ... <span class="number">53</span> <span class="number">65</span> <span class="number">43</span>]</span><br><span class="line">  [<span class="number">98</span> <span class="number">36</span> <span class="number">72</span> ... <span class="number">33</span> <span class="number">36</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">61</span> <span class="number">78</span> <span class="number">70</span> ... <span class="number">7</span> <span class="number">59</span> <span class="number">21</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">56</span> <span class="number">57</span> <span class="number">45</span> ... <span class="number">23</span> <span class="number">15</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">35</span> <span class="number">8</span> <span class="number">82</span> ... <span class="number">11</span> <span class="number">59</span> <span class="number">97</span>]</span><br><span class="line">  [<span class="number">44</span> <span class="number">6</span> <span class="number">99</span> ... <span class="number">81</span> <span class="number">60</span> <span class="number">27</span>]</span><br><span class="line">  ...</span><br><span class="line">  [<span class="number">76</span> <span class="number">26</span> <span class="number">35</span> ... <span class="number">51</span> <span class="number">8</span> <span class="number">17</span>]</span><br><span class="line">  [<span class="number">33</span> <span class="number">52</span> <span class="number">53</span> ... <span class="number">78</span> <span class="number">37</span> <span class="number">31</span>]</span><br><span class="line">  [<span class="number">71</span> <span class="number">27</span> <span class="number">44</span> ... <span class="number">0</span> <span class="number">52</span> <span class="number">16</span>]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#抽取每个班级第0个学生，第5个学生，第9个学生的全部成绩</span></span><br><span class="line">p = tf.gather(scores,[<span class="number">0</span>,<span class="number">5</span>,<span class="number">9</span>],axis=<span class="number">1</span>)</span><br><span class="line">tf.print(p)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[[<span class="number">52</span> <span class="number">82</span> <span class="number">66</span> ... <span class="number">17</span> <span class="number">86</span> <span class="number">14</span>]</span><br><span class="line">  [<span class="number">24</span> <span class="number">80</span> <span class="number">70</span> ... <span class="number">72</span> <span class="number">63</span> <span class="number">96</span>]</span><br><span class="line">  [<span class="number">24</span> <span class="number">99</span> <span class="number">38</span> ... <span class="number">97</span> <span class="number">44</span> <span class="number">74</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">79</span> <span class="number">73</span> <span class="number">73</span> ... <span class="number">35</span> <span class="number">3</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">46</span> <span class="number">10</span> <span class="number">94</span> ... <span class="number">23</span> <span class="number">18</span> <span class="number">92</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">21</span> <span class="number">89</span> ... <span class="number">53</span> <span class="number">10</span> <span class="number">90</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">52</span> <span class="number">80</span> <span class="number">22</span> ... <span class="number">29</span> <span class="number">25</span> <span class="number">60</span>]</span><br><span class="line">  [<span class="number">19</span> <span class="number">12</span> <span class="number">23</span> ... <span class="number">87</span> <span class="number">86</span> <span class="number">25</span>]</span><br><span class="line">  [<span class="number">61</span> <span class="number">78</span> <span class="number">70</span> ... <span class="number">7</span> <span class="number">59</span> <span class="number">21</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">56</span> <span class="number">57</span> <span class="number">45</span> ... <span class="number">23</span> <span class="number">15</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">6</span> <span class="number">41</span> <span class="number">79</span> ... <span class="number">97</span> <span class="number">43</span> <span class="number">13</span>]</span><br><span class="line">  [<span class="number">71</span> <span class="number">27</span> <span class="number">44</span> ... <span class="number">0</span> <span class="number">52</span> <span class="number">16</span>]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#抽取每个班级第0个学生，第5个学生，第9个学生的第1门课程，第3门课程，第6门课程成绩</span></span><br><span class="line">q = tf.gather(tf.gather(scores,[<span class="number">0</span>,<span class="number">5</span>,<span class="number">9</span>],axis=<span class="number">1</span>),[<span class="number">1</span>,<span class="number">3</span>,<span class="number">6</span>],axis=<span class="number">2</span>)</span><br><span class="line">tf.print(q)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[[<span class="number">82</span> <span class="number">55</span> <span class="number">14</span>]</span><br><span class="line">  [<span class="number">80</span> <span class="number">46</span> <span class="number">96</span>]</span><br><span class="line">  [<span class="number">99</span> <span class="number">58</span> <span class="number">74</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">73</span> <span class="number">48</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">10</span> <span class="number">38</span> <span class="number">92</span>]</span><br><span class="line">  [<span class="number">21</span> <span class="number">86</span> <span class="number">90</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">80</span> <span class="number">57</span> <span class="number">60</span>]</span><br><span class="line">  [<span class="number">12</span> <span class="number">34</span> <span class="number">25</span>]</span><br><span class="line">  [<span class="number">78</span> <span class="number">71</span> <span class="number">21</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">57</span> <span class="number">75</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">41</span> <span class="number">47</span> <span class="number">13</span>]</span><br><span class="line">  [<span class="number">27</span> <span class="number">96</span> <span class="number">16</span>]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 抽取第0个班级第0个学生，第2个班级的第4个学生，第3个班级的第6个学生的全部成绩</span></span><br><span class="line"><span class="comment">#indices的长度为采样样本的个数，每个元素为采样位置的坐标</span></span><br><span class="line">s = tf.gather_nd(scores,indices = [(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">2</span>,<span class="number">4</span>),(<span class="number">3</span>,<span class="number">6</span>)])</span><br><span class="line">s</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">7</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">52</span>, <span class="number">82</span>, <span class="number">66</span>, <span class="number">55</span>, <span class="number">17</span>, <span class="number">86</span>, <span class="number">14</span>],</span><br><span class="line">       [<span class="number">99</span>, <span class="number">94</span>, <span class="number">46</span>, <span class="number">70</span>,  <span class="number">1</span>, <span class="number">63</span>, <span class="number">41</span>],</span><br><span class="line">       [<span class="number">46</span>, <span class="number">83</span>, <span class="number">70</span>, <span class="number">80</span>, <span class="number">90</span>, <span class="number">85</span>, <span class="number">17</span>]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<p>以上tf.gather和tf.gather_nd的功能也可以用tf.boolean_mask来实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#抽取每个班级第0个学生，第5个学生，第9个学生的全部成绩</span></span><br><span class="line">p = tf.boolean_mask(scores,[<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,</span><br><span class="line">                            <span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>],axis=<span class="number">1</span>)</span><br><span class="line">tf.print(p)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[[<span class="number">52</span> <span class="number">82</span> <span class="number">66</span> ... <span class="number">17</span> <span class="number">86</span> <span class="number">14</span>]</span><br><span class="line">  [<span class="number">24</span> <span class="number">80</span> <span class="number">70</span> ... <span class="number">72</span> <span class="number">63</span> <span class="number">96</span>]</span><br><span class="line">  [<span class="number">24</span> <span class="number">99</span> <span class="number">38</span> ... <span class="number">97</span> <span class="number">44</span> <span class="number">74</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">79</span> <span class="number">73</span> <span class="number">73</span> ... <span class="number">35</span> <span class="number">3</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">46</span> <span class="number">10</span> <span class="number">94</span> ... <span class="number">23</span> <span class="number">18</span> <span class="number">92</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">21</span> <span class="number">89</span> ... <span class="number">53</span> <span class="number">10</span> <span class="number">90</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">52</span> <span class="number">80</span> <span class="number">22</span> ... <span class="number">29</span> <span class="number">25</span> <span class="number">60</span>]</span><br><span class="line">  [<span class="number">19</span> <span class="number">12</span> <span class="number">23</span> ... <span class="number">87</span> <span class="number">86</span> <span class="number">25</span>]</span><br><span class="line">  [<span class="number">61</span> <span class="number">78</span> <span class="number">70</span> ... <span class="number">7</span> <span class="number">59</span> <span class="number">21</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">56</span> <span class="number">57</span> <span class="number">45</span> ... <span class="number">23</span> <span class="number">15</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">6</span> <span class="number">41</span> <span class="number">79</span> ... <span class="number">97</span> <span class="number">43</span> <span class="number">13</span>]</span><br><span class="line">  [<span class="number">71</span> <span class="number">27</span> <span class="number">44</span> ... <span class="number">0</span> <span class="number">52</span> <span class="number">16</span>]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#抽取第0个班级第0个学生，第2个班级的第4个学生，第3个班级的第6个学生的全部成绩</span></span><br><span class="line">s = tf.boolean_mask(scores,</span><br><span class="line">    [[<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>],</span><br><span class="line">     [<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>],</span><br><span class="line">     [<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>],</span><br><span class="line">     [<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>]])</span><br><span class="line">tf.print(s)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">52</span> <span class="number">82</span> <span class="number">66</span> ... <span class="number">17</span> <span class="number">86</span> <span class="number">14</span>]</span><br><span class="line"> [<span class="number">99</span> <span class="number">94</span> <span class="number">46</span> ... <span class="number">1</span> <span class="number">63</span> <span class="number">41</span>]</span><br><span class="line"> [<span class="number">46</span> <span class="number">83</span> <span class="number">70</span> ... <span class="number">90</span> <span class="number">85</span> <span class="number">17</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#利用tf.boolean_mask可以实现布尔索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#找到矩阵中小于0的元素</span></span><br><span class="line">c = tf.constant([[<span class="number">-1</span>,<span class="number">1</span>,<span class="number">-1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">-2</span>],[<span class="number">3</span>,<span class="number">-3</span>,<span class="number">3</span>]],dtype=tf.float32)</span><br><span class="line">tf.print(c,<span class="string">"\n"</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">-1</span> <span class="number">1</span> <span class="number">-1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">2</span> <span class="number">-2</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">-3</span> <span class="number">3</span>]] </span><br><span class="line"></span><br><span class="line">tf.print(tf.boolean_mask(c,c&lt;<span class="number">0</span>),<span class="string">"\n"</span>)  <span class="comment"># [-1 -1 -2 -3] </span></span><br><span class="line">tf.print(c[c&lt;<span class="number">0</span>]) <span class="comment">#布尔索引，为boolean_mask的语法糖形式  [-1 -1 -2 -3]</span></span><br></pre></td></tr></table></figure>
<p>以上这些方法仅能提取张量的部分元素值，但不能更改张量的部分元素值得到新的张量。如果要通过修改张量的部分元素值得到新的张量，可以使用tf.where和tf.scatter_nd。</p>
<p>tf.where可以理解为if的张量版本，此外它还可以用于找到满足条件的所有元素的位置坐标。</p>
<p>tf.scatter_nd的作用和tf.gather_nd有些相反，tf.gather_nd用于收集张量的给定位置的元素，而tf.scatter_nd可以将某些值插入到一个给定shape的全0的张量的指定位置处。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#找到张量中小于0的元素,将其换成np.nan得到新的张量</span></span><br><span class="line"><span class="comment">#tf.where和np.where作用类似，可以理解为if的张量版本</span></span><br><span class="line"></span><br><span class="line">c = tf.constant([[<span class="number">-1</span>,<span class="number">1</span>,<span class="number">-1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">-2</span>],[<span class="number">3</span>,<span class="number">-3</span>,<span class="number">3</span>]],dtype=tf.float32)</span><br><span class="line">d = tf.where(c&lt;<span class="number">0</span>,tf.fill(c.shape,np.nan),c) </span><br><span class="line">d</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[nan,  <span class="number">1.</span>, nan],</span><br><span class="line">       [ <span class="number">2.</span>,  <span class="number">2.</span>, nan],</span><br><span class="line">       [ <span class="number">3.</span>, nan,  <span class="number">3.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#如果where只有一个参数，将返回所有满足条件的位置坐标</span></span><br><span class="line">indices = tf.where(c&lt;<span class="number">0</span>)</span><br><span class="line">indices</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: shape=(4, 2), dtype=int64, numpy=</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">1</span>]])&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将张量的第[0,0]和[2,1]两个位置元素替换为0得到新的张量</span></span><br><span class="line">d = c - tf.scatter_nd([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">2</span>,<span class="number">1</span>]],[c[<span class="number">0</span>,<span class="number">0</span>],c[<span class="number">2</span>,<span class="number">1</span>]],c.shape)</span><br><span class="line">d</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">1.</span>, <span class="number">-1.</span>],</span><br><span class="line">       [ <span class="number">2.</span>,  <span class="number">2.</span>, <span class="number">-2.</span>],</span><br><span class="line">       [ <span class="number">3.</span>,  <span class="number">0.</span>,  <span class="number">3.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#scatter_nd的作用和gather_nd有些相反</span></span><br><span class="line"><span class="comment">#可以将某些值插入到一个给定shape的全0的张量的指定位置处。</span></span><br><span class="line">indices = tf.where(c&lt;<span class="number">0</span>)</span><br><span class="line">tf.scatter_nd(indices,tf.gather_nd(c,indices),c.shape)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=</span></span><br><span class="line">array([[<span class="number">-1.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-2.</span>],</span><br><span class="line">       [ <span class="number">0.</span>, <span class="number">-3.</span>,  <span class="number">0.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h3 id="wei-du-bian-huan">维度变换</h3>
<p>维度变换相关函数主要有 tf.reshape, tf.squeeze, tf.expand_dims, tf.transpose.</p>
<blockquote>
<p>tf.reshape 可以改变张量的形状。(tf.reshape可以改变张量的形状，但是其本质上不会改变张量元素的存储顺序，所以，该操作实际上非常迅速，并且是可逆的。)</p>
<p>tf.squeeze 可以减少维度。</p>
<p>tf.expand_dims 可以增加维度。</p>
<p>tf.transpose 可以交换维度。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.uniform(shape=[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">2</span>],minval=<span class="number">0</span>,maxval=<span class="number">255</span>,dtype=tf.int32)</span><br><span class="line">tf.print(a.shape) <span class="comment"># TensorShape([1, 3, 3, 2])</span></span><br><span class="line">tf.print(a)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[[[<span class="number">135</span> <span class="number">178</span>]</span><br><span class="line">   [<span class="number">26</span> <span class="number">116</span>]</span><br><span class="line">   [<span class="number">29</span> <span class="number">224</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">179</span> <span class="number">219</span>]</span><br><span class="line">   [<span class="number">153</span> <span class="number">209</span>]</span><br><span class="line">   [<span class="number">111</span> <span class="number">215</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">39</span> <span class="number">7</span>]</span><br><span class="line">   [<span class="number">138</span> <span class="number">129</span>]</span><br><span class="line">   [<span class="number">59</span> <span class="number">205</span>]]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 改成 （3,6）形状的张量</span></span><br><span class="line">b = tf.reshape(a,[<span class="number">3</span>,<span class="number">6</span>])</span><br><span class="line">tf.print(b.shape) <span class="comment"># TensorShape([3, 6])</span></span><br><span class="line">tf.print(b)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">135</span> <span class="number">178</span> <span class="number">26</span> <span class="number">116</span> <span class="number">29</span> <span class="number">224</span>]</span><br><span class="line"> [<span class="number">179</span> <span class="number">219</span> <span class="number">153</span> <span class="number">209</span> <span class="number">111</span> <span class="number">215</span>]</span><br><span class="line"> [<span class="number">39</span> <span class="number">7</span> <span class="number">138</span> <span class="number">129</span> <span class="number">59</span> <span class="number">205</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 改回成 [1,3,3,2] 形状的张量</span></span><br><span class="line">c = tf.reshape(b,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">2</span>])</span><br><span class="line">tf.print(c)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[[[<span class="number">135</span> <span class="number">178</span>]</span><br><span class="line">   [<span class="number">26</span> <span class="number">116</span>]</span><br><span class="line">   [<span class="number">29</span> <span class="number">224</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">179</span> <span class="number">219</span>]</span><br><span class="line">   [<span class="number">153</span> <span class="number">209</span>]</span><br><span class="line">   [<span class="number">111</span> <span class="number">215</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">39</span> <span class="number">7</span>]</span><br><span class="line">   [<span class="number">138</span> <span class="number">129</span>]</span><br><span class="line">   [<span class="number">59</span> <span class="number">205</span>]]]]</span><br></pre></td></tr></table></figure>
<p>如果张量在某个维度上只有一个元素，利用tf.squeeze可以消除这个维度。</p>
<p>和tf.reshape相似，它本质上不会改变张量元素的存储顺序。</p>
<p>张量的各个元素在内存中是线性存储的，其一般规律是，同一层级中的相邻元素的物理地址也相邻。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">s = tf.squeeze(a)</span><br><span class="line">tf.print(s.shape) <span class="comment"># TensorShape([3, 3, 2])</span></span><br><span class="line">tf.print(s)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[[<span class="number">135</span> <span class="number">178</span>]</span><br><span class="line">  [<span class="number">26</span> <span class="number">116</span>]</span><br><span class="line">  [<span class="number">29</span> <span class="number">224</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">179</span> <span class="number">219</span>]</span><br><span class="line">  [<span class="number">153</span> <span class="number">209</span>]</span><br><span class="line">  [<span class="number">111</span> <span class="number">215</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">39</span> <span class="number">7</span>]</span><br><span class="line">  [<span class="number">138</span> <span class="number">129</span>]</span><br><span class="line">  [<span class="number">59</span> <span class="number">205</span>]]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">d = tf.expand_dims(s,axis=<span class="number">0</span>) <span class="comment">#在第0维插入长度为1的一个维度</span></span><br><span class="line">d</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[[[<span class="number">135</span>, <span class="number">178</span>],</span><br><span class="line">         [ <span class="number">26</span>, <span class="number">116</span>],</span><br><span class="line">         [ <span class="number">29</span>, <span class="number">224</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">179</span>, <span class="number">219</span>],</span><br><span class="line">         [<span class="number">153</span>, <span class="number">209</span>],</span><br><span class="line">         [<span class="number">111</span>, <span class="number">215</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">39</span>,   <span class="number">7</span>],</span><br><span class="line">         [<span class="number">138</span>, <span class="number">129</span>],</span><br><span class="line">         [ <span class="number">59</span>, <span class="number">205</span>]]]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<p>tf.transpose可以交换张量的维度，与tf.reshape不同，它会改变张量元素的存储顺序。tf.transpose常用于图片存储格式的变换上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Batch,Height,Width,Channel</span></span><br><span class="line">a = tf.random.uniform(shape=[<span class="number">100</span>,<span class="number">600</span>,<span class="number">600</span>,<span class="number">4</span>],minval=<span class="number">0</span>,maxval=<span class="number">255</span>,dtype=tf.int32)</span><br><span class="line">tf.print(a.shape) <span class="comment"># TensorShape([100, 600, 600, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换成 Channel,Height,Width,Batch</span></span><br><span class="line">s= tf.transpose(a,perm=[<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>])</span><br><span class="line">tf.print(s.shape) <span class="comment"># TensorShape([4, 600, 600, 100])</span></span><br></pre></td></tr></table></figure>
<h3 id="he-bing-fen-ge">合并分割</h3>
<p>和numpy类似，可以用tf.concat和tf.stack方法对多个张量进行合并，可以用tf.split方法把一个张量分割成多个张量。</p>
<p>tf.concat和tf.stack有略微的区别，tf.concat是连接，不会增加维度，而tf.stack是堆叠，会增加维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">5.0</span>,<span class="number">6.0</span>],[<span class="number">7.0</span>,<span class="number">8.0</span>]])</span><br><span class="line">c = tf.constant([[<span class="number">9.0</span>,<span class="number">10.0</span>],[<span class="number">11.0</span>,<span class="number">12.0</span>]])</span><br><span class="line"></span><br><span class="line">tf.concat([a,b,c],axis = <span class="number">0</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">6</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">       [ <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">       [ <span class="number">5.</span>,  <span class="number">6.</span>],</span><br><span class="line">       [ <span class="number">7.</span>,  <span class="number">8.</span>],</span><br><span class="line">       [ <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">       [<span class="number">11.</span>, <span class="number">12.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.concat([a,b,c],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">6</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">       [ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>, <span class="number">11.</span>, <span class="number">12.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.stack([a,b,c])</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[[ <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">3.</span>,  <span class="number">4.</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">5.</span>,  <span class="number">6.</span>],</span><br><span class="line">        [ <span class="number">7.</span>,  <span class="number">8.</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">        [<span class="number">11.</span>, <span class="number">12.</span>]]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.stack([a,b,c],axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[[ <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">5.</span>,  <span class="number">6.</span>],</span><br><span class="line">        [ <span class="number">9.</span>, <span class="number">10.</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">7.</span>,  <span class="number">8.</span>],</span><br><span class="line">        [<span class="number">11.</span>, <span class="number">12.</span>]]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">5.0</span>,<span class="number">6.0</span>],[<span class="number">7.0</span>,<span class="number">8.0</span>]])</span><br><span class="line">c = tf.constant([[<span class="number">9.0</span>,<span class="number">10.0</span>],[<span class="number">11.0</span>,<span class="number">12.0</span>]])</span><br><span class="line"></span><br><span class="line">c = tf.concat([a,b,c],axis = <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>tf.split是tf.concat的逆运算，可以指定分割份数平均分割，也可以通过指定每份的记录数量进行分割。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#tf.split(value,num_or_size_splits,axis)</span></span><br><span class="line">tf.split(c,<span class="number">3</span>,axis = <span class="number">0</span>)  <span class="comment">#指定分割份数，平均分割</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">4.</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">5.</span>, <span class="number">6.</span>],</span><br><span class="line">        [<span class="number">7.</span>, <span class="number">8.</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[ <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">        [<span class="number">11.</span>, <span class="number">12.</span>]], dtype=float32)&gt;]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tf.split(c,[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],axis = <span class="number">0</span>) <span class="comment">#指定每份的记录数量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">4.</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">5.</span>, <span class="number">6.</span>],</span><br><span class="line">        [<span class="number">7.</span>, <span class="number">8.</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[ <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">        [<span class="number">11.</span>, <span class="number">12.</span>]], dtype=float32)&gt;]</span><br></pre></td></tr></table></figure>
<h2 id="zhang-liang-de-shu-xue-yun-suan">张量的数学运算</h2>
<p>张量的操作主要包括张量的结构操作和张量的数学运算。</p>
<p>张量结构操作诸如：张量创建，索引切片，维度变换，合并分割。</p>
<p>张量数学运算主要有：标量运算，向量运算，矩阵运算。</p>
<p>张量运算的广播机制。</p>
<h3 id="biao-liang-yun-suan">标量运算</h3>
<p>张量的数学运算符可以分为标量运算符、向量运算符、以及矩阵运算符。</p>
<p>加减乘除乘方，以及三角函数，指数，对数等常见函数，逻辑比较运算符等都是标量运算符。</p>
<p>标量运算符的特点是对张量实施逐元素运算。</p>
<p>有些标量运算符对常用的数学运算符进行了重载。并且支持类似numpy的广播特性。</p>
<p>许多标量运算符都在 tf.math模块下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">-3</span>,<span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">5.0</span>,<span class="number">6</span>],[<span class="number">7.0</span>,<span class="number">8.0</span>]])</span><br><span class="line">a+b  <span class="comment">#运算符重载</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">6.</span>,  <span class="number">8.</span>],</span><br><span class="line">       [ <span class="number">4.</span>, <span class="number">12.</span>]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line">a-b </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">-4.</span>,  <span class="number">-4.</span>],</span><br><span class="line">       [<span class="number">-10.</span>,  <span class="number">-4.</span>]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line">a*b </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[  <span class="number">5.</span>,  <span class="number">12.</span>],</span><br><span class="line">       [<span class="number">-21.</span>,  <span class="number">32.</span>]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line">a/b</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">0.2</span>       ,  <span class="number">0.33333334</span>],</span><br><span class="line">       [<span class="number">-0.42857143</span>,  <span class="number">0.5</span>       ]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line">a**<span class="number">2</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">4.</span>],</span><br><span class="line">       [ <span class="number">9.</span>, <span class="number">16.</span>]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line">a**(<span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">1.</span>       , <span class="number">1.4142135</span>],</span><br><span class="line">       [      nan, <span class="number">2.</span>       ]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line">a%<span class="number">3</span> <span class="comment">#mod的运算符重载，等价于m = tf.math.mod(a,3)</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>], dtype=int32)&gt;</span><br><span class="line">    </span><br><span class="line">a//<span class="number">3</span>  <span class="comment">#地板除法</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">-1.</span>,  <span class="number">1.</span>]], dtype=float32)&gt;</span><br><span class="line">(a&gt;=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=bool, numpy=</span><br><span class="line">array([[<span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">       [<span class="literal">False</span>,  <span class="literal">True</span>]])&gt;</span><br><span class="line"></span><br><span class="line">(a&gt;=<span class="number">2</span>)&amp;(a&lt;=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=bool, numpy=</span><br><span class="line">array([[<span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">       [<span class="literal">False</span>, <span class="literal">False</span>]])&gt;</span><br><span class="line"></span><br><span class="line">(a&gt;=<span class="number">2</span>)|(a&lt;=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=bool, numpy=</span><br><span class="line">array([[ <span class="literal">True</span>,  <span class="literal">True</span>],</span><br><span class="line">       [ <span class="literal">True</span>,  <span class="literal">True</span>]])&gt;</span><br><span class="line"></span><br><span class="line">a==<span class="number">5</span> <span class="comment">#tf.equal(a,5)</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=bool, numpy=array([<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>])&gt;</span><br><span class="line">    </span><br><span class="line">tf.sqrt(a)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">1.</span>       , <span class="number">1.4142135</span>],</span><br><span class="line">       [      nan, <span class="number">2.</span>       ]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1.0</span>,<span class="number">8.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">5.0</span>,<span class="number">6.0</span>])</span><br><span class="line">c = tf.constant([<span class="number">6.0</span>,<span class="number">7.0</span>])</span><br><span class="line">tf.add_n([a,b,c]) <span class="comment"># &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([12., 21.], dtype=float32)&gt;</span></span><br><span class="line">tf.print(tf.maximum(a,b)) <span class="comment"># [5 8]</span></span><br><span class="line">tf.print(tf.minimum(a,b)) <span class="comment"># [1 6]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">2.6</span>,<span class="number">-2.7</span>])</span><br><span class="line"></span><br><span class="line">tf.print(tf.math.round(x)) <span class="comment">#保留整数部分，四舍五入 [3 -3]</span></span><br><span class="line">tf.print(tf.math.floor(x)) <span class="comment">#保留整数部分，向下归整 [2 -3]</span></span><br><span class="line">tf.print(tf.math.ceil(x))  <span class="comment">#保留整数部分，向上归整 [3 -2]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 幅值裁剪</span></span><br><span class="line">x = tf.constant([<span class="number">0.9</span>,<span class="number">-0.8</span>,<span class="number">100.0</span>,<span class="number">-20.0</span>,<span class="number">0.7</span>])</span><br><span class="line">y = tf.clip_by_value(x,clip_value_min=<span class="number">-1</span>,clip_value_max=<span class="number">1</span>)</span><br><span class="line">z = tf.clip_by_norm(x,clip_norm = <span class="number">3</span>)</span><br><span class="line">tf.print(y) <span class="comment"># [0.9 -0.8 1 -1 0.7]</span></span><br><span class="line">tf.print(z) <span class="comment"># [0.0264732055 -0.0235317405 2.94146752 -0.588293493 0.0205902718]</span></span><br></pre></td></tr></table></figure>
<h3 id="xiang-liang-yun-suan">向量运算</h3>
<p>向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。<br>
许多向量运算符都以reduce开头。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#向量reduce</span></span><br><span class="line">a = tf.range(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">tf.print(tf.reduce_sum(a)) <span class="comment"># 45</span></span><br><span class="line">tf.print(tf.reduce_mean(a)) <span class="comment"># 5</span></span><br><span class="line">tf.print(tf.reduce_max(a)) <span class="comment"># 9</span></span><br><span class="line">tf.print(tf.reduce_min(a)) <span class="comment"># 1</span></span><br><span class="line">tf.print(tf.reduce_prod(a)) <span class="comment"># 362880</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#张量指定维度进行reduce</span></span><br><span class="line">b = tf.reshape(a,(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">tf.print(tf.reduce_sum(b, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line">tf.print(tf.reduce_sum(b, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">6</span>]</span><br><span class="line"> [<span class="number">15</span>]</span><br><span class="line"> [<span class="number">24</span>]]</span><br><span class="line">[[<span class="number">12</span> <span class="number">15</span> <span class="number">18</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#bool类型的reduce</span></span><br><span class="line">p = tf.constant([<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>])</span><br><span class="line">q = tf.constant([<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>])</span><br><span class="line">tf.print(tf.reduce_all(p)) <span class="comment"># 0</span></span><br><span class="line">tf.print(tf.reduce_any(q)) <span class="comment"># 1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#利用tf.foldr实现tf.reduce_sum</span></span><br><span class="line">s = tf.foldr(<span class="keyword">lambda</span> a,b:a+b,tf.range(<span class="number">10</span>)) </span><br><span class="line">tf.print(s) <span class="comment"># 45</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#cum扫描累积</span></span><br><span class="line">a = tf.range(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">tf.print(tf.math.cumsum(a)) <span class="comment"># [1 3 6 ... 28 36 45]</span></span><br><span class="line">tf.print(tf.math.cumprod(a)) <span class="comment"># [1 2 6 ... 5040 40320 362880]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#arg最大最小值索引</span></span><br><span class="line">a = tf.range(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">tf.print(tf.argmax(a)) <span class="comment"># 8</span></span><br><span class="line">tf.print(tf.argmin(a)) <span class="comment"># 0</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#tf.math.top_k可以用于对张量排序</span></span><br><span class="line">a = tf.constant([<span class="number">1</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">values,indices = tf.math.top_k(a,<span class="number">3</span>,sorted=<span class="literal">True</span>)</span><br><span class="line">tf.print(values) <span class="comment"># [8 7 5]</span></span><br><span class="line">tf.print(indices) <span class="comment"># [5 2 3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#利用tf.math.top_k可以在TensorFlow中实现KNN算法</span></span><br></pre></td></tr></table></figure>
<h3 id="ju-zhen-yun-suan">矩阵运算</h3>
<p>矩阵必须是二维的。类似tf.constant([1,2,3])这样的不是矩阵。</p>
<p>矩阵运算包括：矩阵乘法，矩阵转置，矩阵逆，矩阵求迹，矩阵范数，矩阵行列式，矩阵求特征值，矩阵分解等运算。</p>
<p>除了一些常用的运算外，大部分和矩阵有关的运算都在tf.linalg子包中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵乘法</span></span><br><span class="line">a = tf.constant([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">2</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">2</span>]])</span><br><span class="line">a@b  <span class="comment">#等价于tf.matmul(a,b)</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">2</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">6</span>, <span class="number">8</span>]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵转置</span></span><br><span class="line">a = tf.constant([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">tf.transpose(a)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">4</span>]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵逆，必须为tf.float32或tf.double类型</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]],dtype = tf.float32)</span><br><span class="line">tf.linalg.inv(a)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">-2.0000002</span> ,  <span class="number">1.0000001</span> ],</span><br><span class="line">       [ <span class="number">1.5000001</span> , <span class="number">-0.50000006</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵求trace</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]],dtype = tf.float32)</span><br><span class="line">tf.linalg.trace(a) <span class="comment"># &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵求范数</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">tf.linalg.norm(a) <span class="comment"># &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.477226&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵行列式</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">tf.linalg.det(a) <span class="comment"># &lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵特征值</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">-5</span>,<span class="number">4</span>]])</span><br><span class="line">tf.linalg.eigvals(a) </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>,), dtype=complex64, numpy=array([<span class="number">2.4999995</span>+<span class="number">2.7838817j</span>, <span class="number">2.5</span>      <span class="number">-2.783882j</span> ], dtype=complex64)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵QR分解, 将一个方阵分解为一个正交矩阵q和上三角矩阵r</span></span><br><span class="line"><span class="comment">#QR分解实际上是对矩阵a实施Schmidt正交化得到q</span></span><br><span class="line"></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]],dtype = tf.float32)</span><br><span class="line">q,r = tf.linalg.qr(a)</span><br><span class="line">tf.print(q)</span><br><span class="line">tf.print(r)</span><br><span class="line">tf.print(q@r)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">-0.316227794</span> <span class="number">-0.948683321</span>]</span><br><span class="line"> [<span class="number">-0.948683321</span> <span class="number">0.316227734</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">-3.1622777</span> <span class="number">-4.4271884</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">-0.632455349</span>]]</span><br><span class="line"></span><br><span class="line">[[<span class="number">1.00000012</span> <span class="number">1.99999976</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵svd分解</span></span><br><span class="line"><span class="comment">#svd分解可以将任意一个矩阵分解为一个正交矩阵u,一个对角阵s和一个正交矩阵v.t()的乘积</span></span><br><span class="line"><span class="comment">#svd常用于矩阵压缩和降维</span></span><br><span class="line"></span><br><span class="line">a  = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>],[<span class="number">5.0</span>,<span class="number">6.0</span>]], dtype = tf.float32)</span><br><span class="line">s,u,v = tf.linalg.svd(a)</span><br><span class="line">tf.print(u,<span class="string">"\n"</span>)</span><br><span class="line">tf.print(s,<span class="string">"\n"</span>)</span><br><span class="line">tf.print(v,<span class="string">"\n"</span>)</span><br><span class="line">tf.print(u@tf.linalg.diag(s)@tf.transpose(v))</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用svd分解可以在TensorFlow中实现主成分分析降维</span></span><br><span class="line">[[<span class="number">0.229847744</span> <span class="number">-0.88346082</span>]</span><br><span class="line"> [<span class="number">0.524744868</span> <span class="number">-0.240782902</span>]</span><br><span class="line"> [<span class="number">0.819642067</span> <span class="number">0.401896209</span>]] </span><br><span class="line"></span><br><span class="line">[<span class="number">9.52551842</span> <span class="number">0.51429987</span>] </span><br><span class="line"></span><br><span class="line">[[<span class="number">0.619629562</span> <span class="number">0.784894466</span>]</span><br><span class="line"> [<span class="number">0.784894466</span> <span class="number">-0.619629562</span>]] </span><br><span class="line"></span><br><span class="line">[[<span class="number">1.00000119</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">3.00000095</span> <span class="number">4.00000048</span>]</span><br><span class="line"> [<span class="number">5.00000143</span> <span class="number">6.00000095</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="yan-bo-ji-zhi">广播机制</h3>
<p>TensorFlow的广播规则和numpy是一样的:</p>
<ul>
<li>1、如果张量的维度不同，将维度较小的张量进行扩展，直到两个张量的维度都一样。</li>
<li>2、如果两个张量在某个维度上的长度是相同的，或者其中一个张量在该维度上的长度为1，那么我们就说这两个张量在该维度上是相容的。</li>
<li>3、如果两个张量在所有维度上都是相容的，它们就能使用广播。</li>
<li>4、广播之后，每个维度的长度将取两个张量在该维度长度的较大值。</li>
<li>5、在任何一个维度上，如果一个张量的长度为1，另一个张量长度大于1，那么在该维度上，就好像是对第一个张量进行了复制。</li>
</ul>
<p>tf.broadcast_to 以显式的方式按照广播机制扩展张量的维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = tf.constant([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]])</span><br><span class="line">b + a  <span class="comment">#等价于 b + tf.broadcast_to(a,b.shape)</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.broadcast_to(a,b.shape)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算广播后计算结果的形状，静态形状，TensorShape类型参数</span></span><br><span class="line">tf.broadcast_static_shape(a.shape,b.shape) <span class="comment"># TensorShape([3, 3])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算广播后计算结果的形状，动态形状，Tensor类型参数</span></span><br><span class="line">c = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">d = tf.constant([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line">tf.broadcast_dynamic_shape(tf.shape(c),tf.shape(d)) <span class="comment"># &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 3], dtype=int32)&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#广播效果</span></span><br><span class="line">c+d <span class="comment">#等价于 tf.broadcast_to(c,[3,3]) + tf.broadcast_to(d,[3,3])</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=int32)&gt;</span><br></pre></td></tr></table></figure>
<h2 id="auto-graph-de-shi-yong-gui-fan">AutoGraph的使用规范</h2>
<p>有三种计算图的构建方式：</p>
<ol>
<li>静态计算图</li>
<li>动态计算图</li>
<li>Autograph</li>
</ol>
<p>TensorFlow 2.0主要使用的是动态计算图和Autograph。</p>
<p>动态计算图易于调试，编码效率较高，但执行效率偏低。</p>
<p>静态计算图执行效率很高，但较难调试。</p>
<p>而Autograph机制可以将动态图转换成静态计算图，兼收执行效率和编码效率之利。</p>
<p>当然Autograph机制能够转换的代码并不是没有任何约束的，有一些编码规范需要遵循，否则可能会转换失败或者不符合预期。</p>
<p>下面着重介绍Autograph的编码规范和Autograph转换成静态图的原理，并介绍使用tf.Module来更好地构建Autograph。</p>
<h3 id="autograph-bian-ma-gui-fan-zong-jie">Autograph编码规范总结</h3>
<ol>
<li>被@tf.function修饰的函数应尽可能使用TensorFlow中的函数而不是Python中的其他函数。例如使用tf.print而不是print，使用tf.range而不是range，使用tf.constant(True)而不是True.</li>
<li>避免在@tf.function修饰的函数内部定义tf.Variable.</li>
<li>被@tf.function修饰的函数不可修改该函数外部的Python列表或字典等数据结构变量。</li>
</ol>
<h3 id="autograph-bian-ma-gui-fan-jie-xi">Autograph编码规范解析</h3>
<h4 id="bei-tf-function-xiu-shi-de-han-shu-ying-jin-liang-shi-yong-tensor-flow-zhong-de-han-shu-er-bu-shi-python-zhong-de-qi-ta-han-shu">被@tf.function修饰的函数应尽量使用TensorFlow中的函数而不是Python中的其他函数。</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">np_random</span><span class="params">()</span>:</span></span><br><span class="line">    a = np.random.randn(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">    tf.print(a)</span><br><span class="line">    </span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf_random</span><span class="params">()</span>:</span></span><br><span class="line">    a = tf.random.normal((<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">    tf.print(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#np_random每次执行都是一样的结果。</span></span><br><span class="line">np_random()</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">array([[ <span class="number">0.22619201</span>, <span class="number">-0.4550123</span> , <span class="number">-0.42587565</span>],</span><br><span class="line">       [ <span class="number">0.05429906</span>,  <span class="number">0.2312667</span> , <span class="number">-1.44819738</span>],</span><br><span class="line">       [ <span class="number">0.36571796</span>,  <span class="number">1.45578986</span>, <span class="number">-1.05348983</span>]])</span><br><span class="line">np_random()</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">array([[ <span class="number">0.22619201</span>, <span class="number">-0.4550123</span> , <span class="number">-0.42587565</span>],</span><br><span class="line">       [ <span class="number">0.05429906</span>,  <span class="number">0.2312667</span> , <span class="number">-1.44819738</span>],</span><br><span class="line">       [ <span class="number">0.36571796</span>,  <span class="number">1.45578986</span>, <span class="number">-1.05348983</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#tf_random每次执行都会有重新生成随机数。</span></span><br><span class="line">tf_random()</span><br><span class="line"><span class="comment"># output：</span></span><br><span class="line">[[<span class="number">-1.38956189</span> <span class="number">-0.394843668</span> <span class="number">0.420657277</span>]</span><br><span class="line"> [<span class="number">2.87235498</span> <span class="number">-1.33740318</span> <span class="number">-0.533843279</span>]</span><br><span class="line"> [<span class="number">0.918233037</span> <span class="number">0.118598573</span> <span class="number">-0.399486482</span>]]</span><br><span class="line">tf_random()</span><br><span class="line"><span class="comment"># output：</span></span><br><span class="line">[[<span class="number">-0.858178258</span> <span class="number">1.67509317</span> <span class="number">0.511889517</span>]</span><br><span class="line"> [<span class="number">-0.545829177</span> <span class="number">-2.20118237</span> <span class="number">-0.968222201</span>]</span><br><span class="line"> [<span class="number">0.733958483</span> <span class="number">-0.61904633</span> <span class="number">0.77440238</span>]]</span><br></pre></td></tr></table></figure>
<h4 id="bi-mian-zai-tf-function-xiu-shi-de-han-shu-nei-bu-ding-yi-tf-variable">避免在@tf.function修饰的函数内部定义tf.Variable</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 避免在@tf.function修饰的函数内部定义tf.Variable.</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">1.0</span>,dtype=tf.float32)</span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outer_var</span><span class="params">()</span>:</span></span><br><span class="line">    x.assign_add(<span class="number">1.0</span>)</span><br><span class="line">    tf.print(x)</span><br><span class="line">    <span class="keyword">return</span>(x)</span><br><span class="line"></span><br><span class="line">outer_var() </span><br><span class="line">outer_var()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inner_var</span><span class="params">()</span>:</span></span><br><span class="line">    x = tf.Variable(<span class="number">1.0</span>,dtype = tf.float32)</span><br><span class="line">    x.assign_add(<span class="number">1.0</span>)</span><br><span class="line">    tf.print(x)</span><br><span class="line">    <span class="keyword">return</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行将报错</span></span><br><span class="line"><span class="comment">#inner_var()</span></span><br><span class="line"><span class="comment">#inner_var()</span></span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">ValueError                                Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-12</span>-c95a7c3c1ddd&gt; <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">      <span class="number">7</span> </span><br><span class="line">      <span class="number">8</span> <span class="comment">#执行将报错</span></span><br><span class="line">----&gt; 9 inner_var()</span><br><span class="line">     <span class="number">10</span> inner_var()</span><br><span class="line"></span><br><span class="line">~/anaconda3/lib/python3<span class="number">.7</span>/site-packages/tensorflow_core/python/eager/def_function.py <span class="keyword">in</span> __call__(self, *args, **kwds)</span><br><span class="line">    <span class="number">566</span>         xla_context.Exit()</span><br><span class="line">    <span class="number">567</span>     <span class="keyword">else</span>:</span><br><span class="line">--&gt; 568       result = self._call(*args, **kwds)</span><br><span class="line">    <span class="number">569</span> </span><br><span class="line">    <span class="number">570</span>     <span class="keyword">if</span> tracing_count == self._get_tracing_count():</span><br><span class="line">......</span><br><span class="line">ValueError: tf.function-decorated function tried to create variables on non-first call.</span><br></pre></td></tr></table></figure>
<h4 id="bei-tf-function-xiu-shi-de-han-shu-bu-ke-xiu-gai-gai-han-shu-wai-bu-de-python-lie-biao-huo-zi-dian-deng-jie-gou-lei-xing-bian-liang">被@tf.function修饰的函数不可修改该函数外部的Python列表或字典等结构类型变量。</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tensor_list = []</span><br><span class="line"></span><br><span class="line"><span class="comment">#@tf.function #加上这一行切换成Autograph结果将不符合预期！！！</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append_tensor</span><span class="params">(x)</span>:</span></span><br><span class="line">    tensor_list.append(x)</span><br><span class="line">    <span class="keyword">return</span> tensor_list</span><br><span class="line"></span><br><span class="line">append_tensor(tf.constant(<span class="number">5.0</span>))</span><br><span class="line">append_tensor(tf.constant(<span class="number">6.0</span>))</span><br><span class="line">print(tensor_list)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[&lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">5.0</span>&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">6.0</span>&gt;]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tensor_list = []</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function #加上这一行切换成Autograph结果将不符合预期！！！</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append_tensor</span><span class="params">(x)</span>:</span></span><br><span class="line">    tensor_list.append(x)</span><br><span class="line">    <span class="keyword">return</span> tensor_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">append_tensor(tf.constant(<span class="number">5.0</span>))</span><br><span class="line">append_tensor(tf.constant(<span class="number">6.0</span>))</span><br><span class="line">print(tensor_list)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[&lt;tf.Tensor <span class="string">'x:0'</span> shape=() dtype=float32&gt;]</span><br></pre></td></tr></table></figure>
<h2 id="auto-graph-de-ji-zhi-yuan-li">AutoGraph的机制原理</h2>
<h3 id="autograph-de-ji-zhi-yuan-li">Autograph的机制原理</h3>
<p><strong>当我们使用@tf.function装饰一个函数的时候，后面到底发生了什么呢？</strong></p>
<p>例如我们写下如下代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function(autograph=True)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myadd</span><span class="params">(a,b)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tf.range(<span class="number">3</span>):</span><br><span class="line">        tf.print(i)</span><br><span class="line">    c = a+b</span><br><span class="line">    print(<span class="string">"tracing"</span>)</span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure>
<p>后面什么都没有发生。仅仅是在Python堆栈中记录了这样一个函数的签名。</p>
<p><strong>当我们第一次调用这个被@tf.function装饰的函数时，后面到底发生了什么？</strong></p>
<p>例如我们写下如下代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">myadd(tf.constant(<span class="string">"hello"</span>),tf.constant(<span class="string">"world"</span>))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tracing</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>发生了2件事情：</p>
<p>第一件事情是创建计算图，即创建一个静态计算图，跟踪执行一遍函数体中的Python代码，确定各个变量的Tensor类型，并根据执行顺序将算子添加到计算图中。在这个过程中，如果开启了autograph=True(默认开启),会将Python控制流转换成TensorFlow图内控制流。</p>
<p>主要是将if语句转换成 tf.cond算子表达，将while和for循环语句转换成tf.while_loop算子表达，并在必要的时候添加<br>
tf.control_dependencies指定执行顺序依赖关系。相当于在 tensorflow1.0执行了类似下面的语句：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    a = tf.placeholder(shape=[],dtype=tf.string)</span><br><span class="line">    b = tf.placeholder(shape=[],dtype=tf.string)</span><br><span class="line">    cond = <span class="keyword">lambda</span> i: i&lt;tf.constant(<span class="number">3</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(i)</span>:</span></span><br><span class="line">        tf.print(i)</span><br><span class="line">        <span class="keyword">return</span>(i+<span class="number">1</span>)</span><br><span class="line">    loop = tf.while_loop(cond,body,loop_vars=[<span class="number">0</span>])</span><br><span class="line">    loop</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies(loop):</span><br><span class="line">        c = tf.strings.join([a,b])</span><br><span class="line">    print(<span class="string">"tracing"</span>)</span><br></pre></td></tr></table></figure>
<p>第二件事情是执行计算图。相当于在 tensorflow1.0中执行了下面的语句：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session(graph=g) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(c,feed_dict=&#123;a:tf.constant(<span class="string">"hello"</span>),b:tf.constant(<span class="string">"world"</span>)&#125;)</span><br></pre></td></tr></table></figure>
<p>因此我们先看到的是第一个步骤的结果：即Python调用标准输出流打印&quot;tracing&quot;语句。然后看到第二个步骤的结果：TensorFlow调用标准输出流打印1,2,3。</p>
<p><strong>当我们再次用相同的输入参数类型调用这个被@tf.function装饰的函数时，后面到底发生了什么？</strong></p>
<p>例如我们写下如下代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">myadd(tf.constant(<span class="string">"good"</span>),tf.constant(<span class="string">"morning"</span>))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>只会发生一件事情，那就是上面步骤的第二步，执行计算图。所以这一次我们没有看到打印&quot;tracing&quot;的结果。</p>
<p><strong>当我们再次用不同的的输入参数类型调用这个被@tf.function装饰的函数时，后面到底发生了什么？</strong></p>
<p>例如我们写下如下代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">myadd(tf.constant(<span class="number">1</span>),tf.constant(<span class="number">2</span>))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tracing</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>由于输入参数的类型已经发生变化，已经创建的计算图不能够再次使用。需要重新做2件事情：创建新的计算图、执行计算图。</p>
<p>所以我们又会先看到的是第一个步骤的结果：即Python调用标准输出流打印&quot;tracing&quot;语句。然后再看到第二个步骤的结果：TensorFlow调用标准输出流打印1,2,3。</p>
<p><strong>需要注意的是，如果调用被@tf.function装饰的函数时输入的参数不是Tensor类型，则每次都会重新创建计算图。</strong></p>
<p>例如我们写下如下代码。两次都会重新创建计算图。因此，一般建议调用@tf.function时应传入Tensor类型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">myadd(<span class="string">"hello"</span>,<span class="string">"world"</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tracing</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">myadd(<span class="string">"good"</span>,<span class="string">"morning"</span>)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tracing</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>
<h3 id="zhong-xin-li-jie-autograph-de-bian-ma-gui-fan">重新理解Autograph的编码规范</h3>
<p>了解了以上Autograph的机制原理，我们也就能够理解Autograph编码规范的3条建议了。</p>
<ol>
<li>
<p>被@tf.function修饰的函数应尽量使用TensorFlow中的函数而不是Python中的其他函数。例如使用tf.print而不是print.</p>
<p>解释：Python中的函数仅仅会在跟踪执行函数以创建静态图的阶段使用，普通Python函数是无法嵌入到静态计算图中的，所以在计算图构建好之后再次调用的时候，这些Python函数并没有被计算，而TensorFlow中的函数则可以嵌入到计算图中。使用普通的Python函数会导致被@tf.function修饰前【eager执行】和被@tf.function修饰后【静态图执行】的输出不一致。</p>
</li>
<li>
<p>避免在@tf.function修饰的函数内部定义tf.Variable.</p>
<p>解释：如果函数内部定义了tf.Variable,那么在【eager执行】时，这种创建tf.Variable的行为在每次函数调用时候都会发生。但是在【静态图执行】时，这种创建tf.Variable的行为只会发生在第一步跟踪Python代码逻辑创建计算图时，这会导致被@tf.function修饰前【eager执行】和被@tf.function修饰后【静态图执行】的输出不一致。实际上，TensorFlow在这种情况下一般会报错。</p>
</li>
<li>
<p>被@tf.function修饰的函数不可修改该函数外部的Python列表或字典等数据结构变量。</p>
<p>解释：静态计算图是被编译成C++代码在TensorFlow内核中执行的。Python中的列表和字典等数据结构变量是无法嵌入到计算图中，它们仅仅能够在创建计算图时被读取，在执行计算图时是无法修改Python中的列表或字典这样的数据结构变量的。</p>
</li>
</ol>
<h2 id="auto-graph-he-tf-module">AutoGraph和tf.Module</h2>
<p>前面介绍了Autograph的编码规范和Autograph转换成静态图的原理。</p>
<p>本篇介绍使用tf.Module来更好地构建Autograph。</p>
<h3 id="autograph-he-tf-module-gai-shu">Autograph和tf.Module概述</h3>
<p>前面在介绍Autograph的编码规范时提到构建Autograph时应该避免在@tf.function修饰的函数内部定义tf.Variable.</p>
<p>但是如果在函数外部定义tf.Variable的话，又会显得这个函数有外部变量依赖，封装不够完美。</p>
<p>一种简单的思路是定义一个类，并将相关的tf.Variable创建放在类的初始化方法中。而将函数的逻辑放在其他方法中。</p>
<p>惊喜的是，TensorFlow提供了一个基类tf.Module，通过继承它构建子类，我们不仅可以获得以上的自然而然，而且可以非常方便地管理变量，还可以非常方便地管理它引用的其它Module，最重要的是，我们能够利用tf.saved_model保存模型并实现跨平台部署使用。</p>
<p>实际上，tf.keras.models.Model,tf.keras.layers.Layer 都是继承自tf.Module的，提供了方便的变量管理和所引用的子模块管理的功能。</p>
<p><strong>因此，利用tf.Module提供的封装，再结合TensoFlow丰富的低阶API，实际上我们能够基于TensorFlow开发任意机器学习模型(而非仅仅是神经网络模型)，并实现跨平台部署使用。</strong></p>
<h3 id="ying-yong-tf-module-feng-zhuang-autograph">应用tf.Module封装Autograph</h3>
<p>定义一个简单的function。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line">x = tf.Variable(<span class="number">1.0</span>,dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在tf.function中用input_signature限定输入张量的签名类型：shape和dtype</span></span><br><span class="line"><span class="meta">@tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])    </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_print</span><span class="params">(a)</span>:</span></span><br><span class="line">    x.assign_add(a)</span><br><span class="line">    tf.print(x)</span><br><span class="line">    <span class="keyword">return</span>(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">add_print(tf.constant(<span class="number">3.0</span>)) <span class="comment"># 4</span></span><br><span class="line"><span class="comment">#add_print(tf.constant(3)) #输入不符合张量签名的参数将报错</span></span><br></pre></td></tr></table></figure>
<p>下面利用tf.Module的子类化将其封装一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DemoModule</span><span class="params">(tf.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,init_value = tf.constant<span class="params">(<span class="number">0.0</span>)</span>,name=None)</span>:</span></span><br><span class="line">        super(DemoModule, self).__init__(name=name)</span><br><span class="line">        <span class="keyword">with</span> self.name_scope:  <span class="comment">#相当于with tf.name_scope("demo_module")</span></span><br><span class="line">            self.x = tf.Variable(init_value,dtype = tf.float32,trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">     </span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addprint</span><span class="params">(self,a)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> self.name_scope:</span><br><span class="line">            self.x.assign_add(a)</span><br><span class="line">            tf.print(self.x)</span><br><span class="line">            <span class="keyword">return</span>(self.x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#执行</span></span><br><span class="line">demo = DemoModule(init_value = tf.constant(<span class="number">1.0</span>))</span><br><span class="line">result = demo.addprint(tf.constant(<span class="number">5.0</span>)) <span class="comment"># 6</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看模块中的全部变量和全部可训练变量</span></span><br><span class="line">print(demo.variables)</span><br><span class="line">print(demo.trainable_variables)</span><br></pre></td></tr></table></figure>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(&lt;tf.Variable <span class="string">'demo_module/Variable:0'</span> shape=() <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=6.0&gt;,)</span><br><span class="line">(&lt;tf.Variable <span class="string">'demo_module/Variable:0'</span> shape=() <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=6.0&gt;,)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看模块中的全部子模块</span></span><br><span class="line">demo.submodules</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用tf.saved_model 保存模型，并指定需要跨平台部署的方法</span></span><br><span class="line">tf.saved_model.save(demo,<span class="string">"./data/demo/1"</span>,signatures = &#123;<span class="string">"serving_default"</span>:demo.addprint&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载模型</span></span><br><span class="line">demo2 = tf.saved_model.load(<span class="string">"./data/demo/1"</span>)</span><br><span class="line">demo2.addprint(tf.constant(<span class="number">5.0</span>)) <span class="comment"># 11</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看模型文件相关信息，红框标出来的输出信息在模型部署和跨平台使用时有可能会用到</span></span><br><span class="line">!saved_model_cli show --dir ./data/demo/<span class="number">1</span> --all</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/%E6%9F%A5%E7%9C%8B%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E4%BF%A1%E6%81%AF.jpg" alt></p>
<p>在tensorboard中查看计算图，模块会被添加模块名demo_module,方便层次化呈现计算图结构。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建日志</span></span><br><span class="line">stamp = datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">logdir = <span class="string">'./data/demomodule/%s'</span> % stamp</span><br><span class="line">writer = tf.summary.create_file_writer(logdir)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开启autograph跟踪</span></span><br><span class="line">tf.summary.trace_on(graph=<span class="literal">True</span>, profiler=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#执行autograph</span></span><br><span class="line">demo = DemoModule(init_value = tf.constant(<span class="number">0.0</span>))</span><br><span class="line">result = demo.addprint(tf.constant(<span class="number">5.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#将计算图信息写入日志</span></span><br><span class="line"><span class="keyword">with</span> writer.as_default():</span><br><span class="line">    tf.summary.trace_export(</span><br><span class="line">        name=<span class="string">"demomodule"</span>,</span><br><span class="line">        step=<span class="number">0</span>,</span><br><span class="line">        profiler_outdir=logdir)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#启动 tensorboard在jupyter中的魔法命令</span></span><br><span class="line">%reload_ext tensorboard</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorboard <span class="keyword">import</span> notebook</span><br><span class="line">notebook.list()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">notebook.start(<span class="string">"--logdir ./data/demomodule/"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/demomodule%E7%9A%84%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%BB%93%E6%9E%84.jpg" alt></p>
<p>除了利用tf.Module的子类化实现封装，我们也可以通过给tf.Module添加属性的方法进行封装。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mymodule = tf.Module()</span><br><span class="line">mymodule.x = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addprint</span><span class="params">(a)</span>:</span></span><br><span class="line">    mymodule.x.assign_add(a)</span><br><span class="line">    tf.print(mymodule.x)</span><br><span class="line">    <span class="keyword">return</span> (mymodule.x)</span><br><span class="line"></span><br><span class="line">mymodule.addprint = addprint</span><br><span class="line">mymodule.addprint(tf.constant(<span class="number">1.0</span>)).numpy() <span class="comment"># 1.0</span></span><br><span class="line">print(mymodule.variables) <span class="comment"># (&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0&gt;,)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用tf.saved_model 保存模型</span></span><br><span class="line">tf.saved_model.save(mymodule,<span class="string">"./data/mymodule"</span>,</span><br><span class="line">    signatures = &#123;<span class="string">"serving_default"</span>:mymodule.addprint&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载模型</span></span><br><span class="line">mymodule2 = tf.saved_model.load(<span class="string">"./data/mymodule"</span>) <span class="comment"># INFO:tensorflow:Assets written to: ./data/mymodule/assets</span></span><br><span class="line">mymodule2.addprint(tf.constant(<span class="number">5.0</span>)) <span class="comment"># 5</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-module-he-tf-keras-model-tf-keras-layers-layer">tf.Module和tf.keras.Model，tf.keras.layers.Layer</h3>
<p>tf.keras中的模型和层都是继承tf.Module实现的，也具有变量管理和子模块管理功能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,losses,metrics</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(issubclass(tf.keras.Model,tf.Module))</span><br><span class="line">print(issubclass(tf.keras.layers.Layer,tf.Module))</span><br><span class="line">print(issubclass(tf.keras.Model,tf.keras.layers.Layer))</span><br></pre></td></tr></table></figure>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session() </span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">4</span>,input_shape = (<span class="number">10</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">2</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">Model: <span class="string">"sequential"</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param <span class="comment">#   </span></span><br><span class="line">=================================================================</span><br><span class="line">dense (Dense)                (<span class="literal">None</span>, <span class="number">4</span>)                 <span class="number">44</span>        </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (<span class="literal">None</span>, <span class="number">2</span>)                 <span class="number">10</span>        </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_2 (Dense)              (<span class="literal">None</span>, <span class="number">1</span>)                 <span class="number">3</span>         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">57</span></span><br><span class="line">Trainable params: <span class="number">57</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">model.variables</span><br><span class="line"></span><br><span class="line"><span class="comment"># OUTPUT</span></span><br><span class="line">[&lt;tf.Variable <span class="string">'dense/kernel:0'</span> shape=(<span class="number">10</span>, <span class="number">4</span>) dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">-0.06741005</span>,  <span class="number">0.45534766</span>,  <span class="number">0.5190817</span> , <span class="number">-0.01806331</span>],</span><br><span class="line">        [<span class="number">-0.14258742</span>, <span class="number">-0.49711505</span>,  <span class="number">0.26030976</span>,  <span class="number">0.18607801</span>],</span><br><span class="line">        [<span class="number">-0.62806034</span>,  <span class="number">0.5327399</span> ,  <span class="number">0.42206633</span>,  <span class="number">0.29201728</span>],</span><br><span class="line">        [<span class="number">-0.16602087</span>, <span class="number">-0.18901917</span>,  <span class="number">0.55159235</span>, <span class="number">-0.01091868</span>],</span><br><span class="line">        [ <span class="number">0.04533798</span>,  <span class="number">0.326845</span>  , <span class="number">-0.582667</span>  ,  <span class="number">0.19431782</span>],</span><br><span class="line">        [ <span class="number">0.6494713</span> , <span class="number">-0.16174704</span>,  <span class="number">0.4062966</span> ,  <span class="number">0.48760796</span>],</span><br><span class="line">        [ <span class="number">0.58400524</span>, <span class="number">-0.6280886</span> , <span class="number">-0.11265379</span>, <span class="number">-0.6438277</span> ],</span><br><span class="line">        [ <span class="number">0.26642334</span>,  <span class="number">0.49275804</span>,  <span class="number">0.20793378</span>, <span class="number">-0.43889117</span>],</span><br><span class="line">        [ <span class="number">0.4092741</span> ,  <span class="number">0.09871006</span>, <span class="number">-0.2073121</span> ,  <span class="number">0.26047975</span>],</span><br><span class="line">        [ <span class="number">0.43910992</span>,  <span class="number">0.00199282</span>, <span class="number">-0.07711256</span>, <span class="number">-0.27966842</span>]],</span><br><span class="line">       dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable <span class="string">'dense/bias:0'</span> shape=(<span class="number">4</span>,) dtype=float32, numpy=array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable <span class="string">'dense_1/kernel:0'</span> shape=(<span class="number">4</span>, <span class="number">2</span>) dtype=float32, numpy=</span><br><span class="line"> array([[ <span class="number">0.5022683</span> , <span class="number">-0.0507431</span> ],</span><br><span class="line">        [<span class="number">-0.61540484</span>,  <span class="number">0.9369011</span> ],</span><br><span class="line">        [<span class="number">-0.14412141</span>, <span class="number">-0.54607415</span>],</span><br><span class="line">        [ <span class="number">0.2027781</span> , <span class="number">-0.4651153</span> ]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable <span class="string">'dense_1/bias:0'</span> shape=(<span class="number">2</span>,) dtype=float32, numpy=array([<span class="number">0.</span>, <span class="number">0.</span>], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable <span class="string">'dense_2/kernel:0'</span> shape=(<span class="number">2</span>, <span class="number">1</span>) dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">-0.244825</span> ],</span><br><span class="line">        [<span class="number">-1.2101456</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable <span class="string">'dense_2/bias:0'</span> shape=(<span class="number">1</span>,) dtype=float32, numpy=array([<span class="number">0.</span>], dtype=float32)&gt;]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model.layers[<span class="number">0</span>].trainable = <span class="literal">False</span> <span class="comment">#冻结第0层的变量,使其不可训练</span></span><br><span class="line">model.trainable_variables</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[&lt;tf.Variable <span class="string">'dense_1/kernel:0'</span> shape=(<span class="number">4</span>, <span class="number">2</span>) dtype=float32, numpy=</span><br><span class="line"> array([[ <span class="number">0.5022683</span> , <span class="number">-0.0507431</span> ],</span><br><span class="line">        [<span class="number">-0.61540484</span>,  <span class="number">0.9369011</span> ],</span><br><span class="line">        [<span class="number">-0.14412141</span>, <span class="number">-0.54607415</span>],</span><br><span class="line">        [ <span class="number">0.2027781</span> , <span class="number">-0.4651153</span> ]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable <span class="string">'dense_1/bias:0'</span> shape=(<span class="number">2</span>,) dtype=float32, numpy=array([<span class="number">0.</span>, <span class="number">0.</span>], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable <span class="string">'dense_2/kernel:0'</span> shape=(<span class="number">2</span>, <span class="number">1</span>) dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">-0.244825</span> ],</span><br><span class="line">        [<span class="number">-1.2101456</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable <span class="string">'dense_2/bias:0'</span> shape=(<span class="number">1</span>,) dtype=float32, numpy=array([<span class="number">0.</span>], dtype=float32)&gt;]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.submodules</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">(&lt;tensorflow.python.keras.engine.input_layer.InputLayer at <span class="number">0x144d8c080</span>&gt;,</span><br><span class="line"> &lt;tensorflow.python.keras.layers.core.Dense at <span class="number">0x144daada0</span>&gt;,</span><br><span class="line"> &lt;tensorflow.python.keras.layers.core.Dense at <span class="number">0x144d8c5c0</span>&gt;,</span><br><span class="line"> &lt;tensorflow.python.keras.layers.core.Dense at <span class="number">0x144d7aa20</span>&gt;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.layers</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[&lt;tensorflow.python.keras.layers.core.Dense at <span class="number">0x144daada0</span>&gt;,</span><br><span class="line"> &lt;tensorflow.python.keras.layers.core.Dense at <span class="number">0x144d8c5c0</span>&gt;,</span><br><span class="line"> &lt;tensorflow.python.keras.layers.core.Dense at <span class="number">0x144d7aa20</span>&gt;]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(model.name) <span class="comment"># sequential</span></span><br><span class="line">print(model.name_scope()) <span class="comment"># sequential</span></span><br></pre></td></tr></table></figure>
<h1 id="tensor-flow-de-zhong-jie-api">TensorFlow的中阶API</h1>
<p>TensorFlow的中阶API主要包括:</p>
<ul>
<li>
<p>数据管道(tf.data)</p>
</li>
<li>
<p>特征列(tf.feature_column)</p>
</li>
<li>
<p>激活函数(tf.nn)</p>
</li>
<li>
<p>模型层(tf.keras.layers)</p>
</li>
<li>
<p>损失函数(tf.keras.losses)</p>
</li>
<li>
<p>评估函数(tf.keras.metrics)</p>
</li>
<li>
<p>优化器(tf.keras.optimizers)</p>
</li>
<li>
<p>回调函数(tf.keras.callbacks)</p>
</li>
</ul>
<p>如果把模型比作一个房子，那么中阶API就是【模型之墙】。</p>
<h2 id="shu-ju-guan-dao-dataset">数据管道Dataset</h2>
<p>如果需要训练的数据大小不大，例如不到1G，那么可以直接全部读入内存中进行训练，这样一般效率最高。</p>
<p>但如果需要训练的数据很大，例如超过10G，无法一次载入内存，那么通常需要在训练的过程中分批逐渐读入。</p>
<p>使用 tf.data API 可以构建数据输入管道，轻松处理大量的数据，不同的数据格式，以及不同的数据转换。</p>
<h3 id="gou-jian-shu-ju-guan-dao">构建数据管道</h3>
<p>可以从 Numpy array, Pandas DataFrame, Python generator, csv文件, 文本文件, 文件路径, tfrecords文件等方式构建数据管道。其中通过Numpy array, Pandas DataFrame, 文件路径构建数据管道是最常用的方法。</p>
<p>通过tfrecords文件方式构建数据管道较为复杂，需要对样本构建tf.Example后压缩成字符串写到tfrecords文件，读取后再解析成tf.Example。但tfrecords文件的优点是压缩后文件较小，便于网络传播，加载速度较快。</p>
<h4 id="cong-numpy-array-gou-jian-shu-ju-guan-dao">从Numpy array构建数据管道</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从Numpy array构建数据管道</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets </span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ds1 = tf.data.Dataset.from_tensor_slices((iris[<span class="string">"data"</span>],iris[<span class="string">"target"</span>]))</span><br><span class="line"><span class="keyword">for</span> features,label <span class="keyword">in</span> ds1.take(<span class="number">5</span>):</span><br><span class="line">    print(features,label)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor([<span class="number">5.1</span> <span class="number">3.5</span> <span class="number">1.4</span> <span class="number">0.2</span>], shape=(<span class="number">4</span>,), dtype=float64) tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">4.9</span> <span class="number">3.</span>  <span class="number">1.4</span> <span class="number">0.2</span>], shape=(<span class="number">4</span>,), dtype=float64) tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">4.7</span> <span class="number">3.2</span> <span class="number">1.3</span> <span class="number">0.2</span>], shape=(<span class="number">4</span>,), dtype=float64) tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">4.6</span> <span class="number">3.1</span> <span class="number">1.5</span> <span class="number">0.2</span>], shape=(<span class="number">4</span>,), dtype=float64) tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">5.</span>  <span class="number">3.6</span> <span class="number">1.4</span> <span class="number">0.2</span>], shape=(<span class="number">4</span>,), dtype=float64) tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br></pre></td></tr></table></figure>
<h4 id="cong-pandas-data-frame-gou-jian-shu-ju-guan-dao">从 Pandas DataFrame构建数据管道</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从 Pandas DataFrame构建数据管道</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">dfiris = pd.DataFrame(iris[<span class="string">"data"</span>],columns = iris.feature_names)</span><br><span class="line">ds2 = tf.data.Dataset.from_tensor_slices((dfiris.to_dict(<span class="string">"list"</span>),iris[<span class="string">"target"</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> features,label <span class="keyword">in</span> ds2.take(<span class="number">3</span>):</span><br><span class="line">    print(features,label)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">&#123;<span class="string">'sepal length (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">5.1</span>&gt;, <span class="string">'sepal width (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">3.5</span>&gt;, <span class="string">'petal length (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">1.4</span>&gt;, <span class="string">'petal width (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">0.2</span>&gt;&#125; tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">&#123;<span class="string">'sepal length (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">4.9</span>&gt;, <span class="string">'sepal width (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">3.0</span>&gt;, <span class="string">'petal length (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">1.4</span>&gt;, <span class="string">'petal width (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">0.2</span>&gt;&#125; tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">&#123;<span class="string">'sepal length (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">4.7</span>&gt;, <span class="string">'sepal width (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">3.2</span>&gt;, <span class="string">'petal length (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">1.3</span>&gt;, <span class="string">'petal width (cm)'</span>: &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">0.2</span>&gt;&#125; tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br></pre></td></tr></table></figure>
<h4 id="cong-python-generator-gou-jian-shu-ju-guan-dao">从Python generator构建数据管道</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从Python generator构建数据管道</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个从文件中读取图片的generator</span></span><br><span class="line">image_generator = ImageDataGenerator(rescale=<span class="number">1.0</span>/<span class="number">255</span>).flow_from_directory(</span><br><span class="line">                    <span class="string">"./data/cifar2/test/"</span>,</span><br><span class="line">                    target_size=(<span class="number">32</span>, <span class="number">32</span>),</span><br><span class="line">                    batch_size=<span class="number">20</span>,</span><br><span class="line">                    class_mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">classdict = image_generator.class_indices</span><br><span class="line">print(classdict)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> features,label <span class="keyword">in</span> image_generator:</span><br><span class="line">        <span class="keyword">yield</span> (features,label)</span><br><span class="line"></span><br><span class="line">ds3 = tf.data.Dataset.from_generator(generator,output_types=(tf.float32,tf.int32))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">6</span>)) </span><br><span class="line"><span class="keyword">for</span> i,(img,label) <span class="keyword">in</span> enumerate(ds3.unbatch().take(<span class="number">9</span>)):</span><br><span class="line">    ax=plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    ax.imshow(img.numpy())</span><br><span class="line">    ax.set_title(<span class="string">"label = %d"</span>%label)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([]) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/5-1-cifar2%E9%A2%84%E8%A7%88.jpg" alt></p>
<h4 id="cong-csv-wen-jian-gou-jian-shu-ju-guan-dao">从csv文件构建数据管道</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从csv文件构建数据管道</span></span><br><span class="line">ds4 = tf.data.experimental.make_csv_dataset(</span><br><span class="line">      file_pattern = [<span class="string">"./data/titanic/train.csv"</span>,<span class="string">"./data/titanic/test.csv"</span>],</span><br><span class="line">      batch_size=<span class="number">3</span>, </span><br><span class="line">      label_name=<span class="string">"Survived"</span>,</span><br><span class="line">      na_value=<span class="string">""</span>,</span><br><span class="line">      num_epochs=<span class="number">1</span>,</span><br><span class="line">      ignore_errors=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data,label <span class="keyword">in</span> ds4.take(<span class="number">2</span>):</span><br><span class="line">    print(data,label)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">OrderedDict([(<span class="string">'PassengerId'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">540</span>,  <span class="number">58</span>, <span class="number">764</span>], dtype=int32)&gt;), (<span class="string">'Pclass'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>], dtype=int32)&gt;), (<span class="string">'Name'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=</span><br><span class="line">array([<span class="string">b'Frolicher, Miss. Hedwig Margaritha'</span>, <span class="string">b'Novel, Mr. Mansouer'</span>,</span><br><span class="line">       <span class="string">b'Carter, Mrs. William Ernest (Lucile Polk)'</span>], dtype=object)&gt;), (<span class="string">'Sex'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=array([<span class="string">b'female'</span>, <span class="string">b'male'</span>, <span class="string">b'female'</span>], dtype=object)&gt;), (<span class="string">'Age'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">22.</span> , <span class="number">28.5</span>, <span class="number">36.</span> ], dtype=float32)&gt;), (<span class="string">'SibSp'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], dtype=int32)&gt;), (<span class="string">'Parch'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>], dtype=int32)&gt;), (<span class="string">'Ticket'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=array([<span class="string">b'13568'</span>, <span class="string">b'2697'</span>, <span class="string">b'113760'</span>], dtype=object)&gt;), (<span class="string">'Fare'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=float32, numpy=array([ <span class="number">49.5</span>   ,   <span class="number">7.2292</span>, <span class="number">120.</span>    ], dtype=float32)&gt;), (<span class="string">'Cabin'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=array([<span class="string">b'B39'</span>, <span class="string">b''</span>, <span class="string">b'B96 B98'</span>], dtype=object)&gt;), (<span class="string">'Embarked'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=array([<span class="string">b'C'</span>, <span class="string">b'C'</span>, <span class="string">b'S'</span>], dtype=object)&gt;)]) tf.Tensor([<span class="number">1</span> <span class="number">0</span> <span class="number">1</span>], shape=(<span class="number">3</span>,), dtype=int32)</span><br><span class="line">OrderedDict([(<span class="string">'PassengerId'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">845</span>,  <span class="number">66</span>, <span class="number">390</span>], dtype=int32)&gt;), (<span class="string">'Pclass'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>], dtype=int32)&gt;), (<span class="string">'Name'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=</span><br><span class="line">array([<span class="string">b'Culumovic, Mr. Jeso'</span>, <span class="string">b'Moubarek, Master. Gerios'</span>,</span><br><span class="line">       <span class="string">b'Lehmann, Miss. Bertha'</span>], dtype=object)&gt;), (<span class="string">'Sex'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=array([<span class="string">b'male'</span>, <span class="string">b'male'</span>, <span class="string">b'female'</span>], dtype=object)&gt;), (<span class="string">'Age'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">17.</span>,  <span class="number">0.</span>, <span class="number">17.</span>], dtype=float32)&gt;), (<span class="string">'SibSp'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], dtype=int32)&gt;), (<span class="string">'Parch'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], dtype=int32)&gt;), (<span class="string">'Ticket'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=array([<span class="string">b'315090'</span>, <span class="string">b'2661'</span>, <span class="string">b'SC 1748'</span>], dtype=object)&gt;), (<span class="string">'Fare'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=float32, numpy=array([ <span class="number">8.6625</span>, <span class="number">15.2458</span>, <span class="number">12.</span>    ], dtype=float32)&gt;), (<span class="string">'Cabin'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=array([<span class="string">b''</span>, <span class="string">b''</span>, <span class="string">b''</span>], dtype=object)&gt;), (<span class="string">'Embarked'</span>, &lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=string, numpy=array([<span class="string">b'S'</span>, <span class="string">b'C'</span>, <span class="string">b'C'</span>], dtype=object)&gt;)]) tf.Tensor([<span class="number">0</span> <span class="number">1</span> <span class="number">1</span>], shape=(<span class="number">3</span>,), dtype=int32)</span><br></pre></td></tr></table></figure>
<h4 id="cong-wen-ben-wen-jian-gou-jian-shu-ju-guan-dao">从文本文件构建数据管道</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从文本文件构建数据管道</span></span><br><span class="line">ds5 = tf.data.TextLineDataset(</span><br><span class="line">    filenames = [<span class="string">"./data/titanic/train.csv"</span>,<span class="string">"./data/titanic/test.csv"</span>]</span><br><span class="line">    ).skip(<span class="number">1</span>) <span class="comment">#略去第一行header</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ds5.take(<span class="number">5</span>):</span><br><span class="line">    print(line)</span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">tf.Tensor(<span class="string">b'493,0,1,"Molson, Mr. Harry Markland",male,55.0,0,0,113787,30.5,C30,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'53,1,1,"Harper, Mrs. Henry Sleeper (Myna Haxtun)",female,49.0,1,0,PC 17572,76.7292,D33,C'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'388,1,2,"Buss, Miss. Kate",female,36.0,0,0,27849,13.0,,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'192,0,2,"Carbines, Mr. William",male,19.0,0,0,28424,13.0,,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'687,0,3,"Panula, Mr. Jaako Arnold",male,14.0,4,1,3101295,39.6875,,S'</span>, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<h4 id="cong-wen-jian-lu-jing-gou-jian-shu-ju-guan-dao">从文件路径构建数据管道</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ds6 = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/train/*/*.jpg"</span>)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> ds6.take(<span class="number">5</span>):</span><br><span class="line">    print(file)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="string">b'./data/cifar2/train/automobile/1263.jpg'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'./data/cifar2/train/airplane/2837.jpg'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'./data/cifar2/train/airplane/4264.jpg'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'./data/cifar2/train/automobile/4241.jpg'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'./data/cifar2/train/automobile/192.jpg'</span>, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(img_path,size = <span class="params">(<span class="number">32</span>,<span class="number">32</span>)</span>)</span>:</span></span><br><span class="line">    label = <span class="number">1</span> <span class="keyword">if</span> tf.strings.regex_full_match(img_path,<span class="string">".*/automobile/.*"</span>) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    img = tf.io.read_file(img_path)</span><br><span class="line">    img = tf.image.decode_jpeg(img) <span class="comment">#注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img,size)</span><br><span class="line">    <span class="keyword">return</span>(img,label)</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"><span class="keyword">for</span> i,(img,label) <span class="keyword">in</span> enumerate(ds6.map(load_image).take(<span class="number">2</span>)):</span><br><span class="line">    plt.figure(i)</span><br><span class="line">    plt.imshow((img/<span class="number">255.0</span>).numpy())</span><br><span class="line">    plt.title(<span class="string">"label = %d"</span>%label)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/5-1-car2.jpg" alt></p>
<h4 id="cong-tfrecords-wen-jian-gou-jian-shu-ju-guan-dao">从tfrecords文件构建数据管道</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># inpath：原始数据路径 outpath:TFRecord文件输出路径</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tfrecords</span><span class="params">(inpath,outpath)</span>:</span> </span><br><span class="line">    writer = tf.io.TFRecordWriter(outpath)</span><br><span class="line">    dirs = os.listdir(inpath)</span><br><span class="line">    <span class="keyword">for</span> index, name <span class="keyword">in</span> enumerate(dirs):</span><br><span class="line">        class_path = inpath +<span class="string">"/"</span>+ name+<span class="string">"/"</span></span><br><span class="line">        <span class="keyword">for</span> img_name <span class="keyword">in</span> os.listdir(class_path):</span><br><span class="line">            img_path = class_path + img_name</span><br><span class="line">            img = tf.io.read_file(img_path)</span><br><span class="line">            <span class="comment">#img = tf.image.decode_image(img)</span></span><br><span class="line">            <span class="comment">#img = tf.image.encode_jpeg(img) #统一成jpeg格式压缩</span></span><br><span class="line">            example = tf.train.Example(</span><br><span class="line">               features=tf.train.Features(feature=&#123;</span><br><span class="line">                    <span class="string">'label'</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),</span><br><span class="line">                    <span class="string">'img_raw'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.numpy()]))</span><br><span class="line">               &#125;))</span><br><span class="line">            writer.write(example.SerializeToString())</span><br><span class="line">    writer.close()</span><br><span class="line">    </span><br><span class="line">create_tfrecords(<span class="string">"./data/cifar2/test/"</span>,<span class="string">"./data/cifar2_test.tfrecords/"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_example</span><span class="params">(proto)</span>:</span></span><br><span class="line">    description =&#123; <span class="string">'img_raw'</span> : tf.io.FixedLenFeature([], tf.string),</span><br><span class="line">                   <span class="string">'label'</span>: tf.io.FixedLenFeature([], tf.int64)&#125; </span><br><span class="line">    example = tf.io.parse_single_example(proto, description)</span><br><span class="line">    img = tf.image.decode_jpeg(example[<span class="string">"img_raw"</span>])   <span class="comment">#注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img, (<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">    label = example[<span class="string">"label"</span>]</span><br><span class="line">    <span class="keyword">return</span>(img,label)</span><br><span class="line"></span><br><span class="line">ds7 = tf.data.TFRecordDataset(<span class="string">"./data/cifar2_test.tfrecords"</span>).map(parse_example).shuffle(<span class="number">3000</span>)</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">6</span>)) </span><br><span class="line"><span class="keyword">for</span> i,(img,label) <span class="keyword">in</span> enumerate(ds7.take(<span class="number">9</span>)):</span><br><span class="line">    ax=plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    ax.imshow((img/<span class="number">255.0</span>).numpy())</span><br><span class="line">    ax.set_title(<span class="string">"label = %d"</span>%label)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([]) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/5-1-car9.jpg" alt></p>
<h3 id="ying-yong-shu-ju-zhuan-huan">应用数据转换</h3>
<p>Dataset数据结构应用非常灵活，因为它本质上是一个Sequece序列，其每个元素可以是各种类型，例如可以是张量，列表，字典，也可以是Dataset。</p>
<p>Dataset包含了非常丰富的数据转换功能。</p>
<ul>
<li>map: 将转换函数映射到数据集每一个元素。</li>
<li>flat_map: 将转换函数映射到数据集的每一个元素，并将嵌套的Dataset压平。</li>
<li>interleave: 效果类似flat_map,但可以将不同来源的数据夹在一起。</li>
<li>filter: 过滤掉某些元素。</li>
<li>zip: 将两个长度相同的Dataset横向铰合。</li>
<li>concatenate: 将两个Dataset纵向连接。</li>
<li>reduce: 执行归并操作。</li>
<li>batch : 构建批次，每次放一个批次。比原始数据增加一个维度。 其逆操作为unbatch。</li>
<li>padded_batch: 构建批次，类似batch, 但可以填充到相同的形状。</li>
<li>window :构建滑动窗口，返回Dataset of Dataset.</li>
<li>shuffle: 数据顺序洗牌。</li>
<li>repeat: 重复数据若干次，不带参数时，重复无数次。</li>
<li>shard: 采样，从某个位置开始隔固定距离采样一个元素。</li>
<li>take: 采样，从开始位置取前几个元素。</li>
</ul>
<h4 id="map">map</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#map:将转换函数映射到数据集每一个元素</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="string">"hello world"</span>,<span class="string">"hello China"</span>,<span class="string">"hello Beijing"</span>])</span><br><span class="line">ds_map = ds.map(<span class="keyword">lambda</span> x:tf.strings.split(x,<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_map:</span><br><span class="line">    print(x)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">tf.Tensor([<span class="string">b'hello'</span> <span class="string">b'world'</span>], shape=(<span class="number">2</span>,), dtype=string)</span><br><span class="line">tf.Tensor([<span class="string">b'hello'</span> <span class="string">b'China'</span>], shape=(<span class="number">2</span>,), dtype=string)</span><br><span class="line">tf.Tensor([<span class="string">b'hello'</span> <span class="string">b'Beijing'</span>], shape=(<span class="number">2</span>,), dtype=string)</span><br></pre></td></tr></table></figure>
<h4 id="flat-map">flat_map</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#flat_map:将转换函数映射到数据集的每一个元素，并将嵌套的Dataset压平。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="string">"hello world"</span>,<span class="string">"hello China"</span>,<span class="string">"hello Beijing"</span>])</span><br><span class="line">ds_flatmap = ds.flat_map(<span class="keyword">lambda</span> x:tf.data.Dataset.from_tensor_slices(tf.strings.split(x,<span class="string">" "</span>)))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_flatmap:</span><br><span class="line">    print(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="string">b'hello'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'world'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'hello'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'China'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'hello'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'Beijing'</span>, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<h4 id="interleave">interleave</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># interleave: 效果类似flat_map,但可以将不同来源的数据夹在一起。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="string">"hello world"</span>,<span class="string">"hello China"</span>,<span class="string">"hello Beijing"</span>])</span><br><span class="line">ds_interleave = ds.interleave(<span class="keyword">lambda</span> x:tf.data.Dataset.from_tensor_slices(tf.strings.split(x,<span class="string">" "</span>)))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_interleave:</span><br><span class="line">    print(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output </span></span><br><span class="line">tf.Tensor(<span class="string">b'hello'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'hello'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'hello'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'world'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'China'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'Beijing'</span>, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<h4 id="filter">filter</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#filter:过滤掉某些元素。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="string">"hello world"</span>,<span class="string">"hello China"</span>,<span class="string">"hello Beijing"</span>])</span><br><span class="line"><span class="comment">#找出含有字母a或B的元素</span></span><br><span class="line">ds_filter = ds.filter(<span class="keyword">lambda</span> x: tf.strings.regex_full_match(x, <span class="string">".*[a|B].*"</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_filter:</span><br><span class="line">    print(x)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="string">b'hello China'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'hello Beijing'</span>, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<h4 id="zip">zip</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#zip:将两个长度相同的Dataset横向铰合。</span></span><br><span class="line"></span><br><span class="line">ds1 = tf.data.Dataset.range(<span class="number">0</span>,<span class="number">3</span>)</span><br><span class="line">ds2 = tf.data.Dataset.range(<span class="number">3</span>,<span class="number">6</span>)</span><br><span class="line">ds3 = tf.data.Dataset.range(<span class="number">6</span>,<span class="number">9</span>)</span><br><span class="line">ds_zip = tf.data.Dataset.zip((ds1,ds2,ds3))</span><br><span class="line"><span class="keyword">for</span> x,y,z <span class="keyword">in</span> ds_zip:</span><br><span class="line">    print(x.numpy(),y.numpy(),z.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="number">0</span> <span class="number">3</span> <span class="number">6</span></span><br><span class="line"><span class="number">1</span> <span class="number">4</span> <span class="number">7</span></span><br><span class="line"><span class="number">2</span> <span class="number">5</span> <span class="number">8</span></span><br></pre></td></tr></table></figure>
<h4 id="condatenate">condatenate</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#condatenate:将两个Dataset纵向连接。</span></span><br><span class="line"></span><br><span class="line">ds1 = tf.data.Dataset.range(<span class="number">0</span>,<span class="number">3</span>)</span><br><span class="line">ds2 = tf.data.Dataset.range(<span class="number">3</span>,<span class="number">6</span>)</span><br><span class="line">ds_concat = tf.data.Dataset.concatenate(ds1,ds2)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_concat:</span><br><span class="line">    print(x)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">1</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">2</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">3</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">4</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">5</span>, shape=(), dtype=int64)</span><br></pre></td></tr></table></figure>
<h4 id="reduce">reduce</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#reduce:执行归并操作。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5.0</span>])</span><br><span class="line">result = ds.reduce(<span class="number">0.0</span>,<span class="keyword">lambda</span> x,y:tf.add(x,y))</span><br><span class="line">result <span class="comment"># &lt;tf.Tensor: shape=(), dtype=float32, numpy=15.0&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="batch">batch</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#batch:构建批次，每次放一个批次。比原始数据增加一个维度。 其逆操作为unbatch。 </span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_batch = ds.batch(<span class="number">4</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_batch:</span><br><span class="line">    print(x)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor([<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span>], shape=(<span class="number">4</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span>], shape=(<span class="number">4</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([ <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span> <span class="number">11</span>], shape=(<span class="number">4</span>,), dtype=int64)</span><br></pre></td></tr></table></figure>
<h4 id="padden-batch">padden_batch</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#padded_batch:构建批次，类似batch, 但可以填充到相同的形状。</span></span><br><span class="line"></span><br><span class="line">elements = [[<span class="number">1</span>, <span class="number">2</span>],[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],[<span class="number">6</span>, <span class="number">7</span>],[<span class="number">8</span>]]</span><br><span class="line">ds = tf.data.Dataset.from_generator(<span class="keyword">lambda</span>: iter(elements), tf.int32)</span><br><span class="line"></span><br><span class="line">ds_padded_batch = ds.padded_batch(<span class="number">2</span>,padded_shapes = [<span class="number">4</span>,])</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_padded_batch:</span><br><span class="line">    print(x)    </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">0</span>]], shape=(<span class="number">2</span>, <span class="number">4</span>), dtype=int32)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">6</span> <span class="number">7</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">8</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]], shape=(<span class="number">2</span>, <span class="number">4</span>), dtype=int32)</span><br></pre></td></tr></table></figure>
<h4 id="window">window</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#window:构建滑动窗口，返回Dataset of Dataset.</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line"><span class="comment">#window返回的是Dataset of Dataset,可以用flat_map压平</span></span><br><span class="line">ds_window = ds.window(<span class="number">3</span>, shift=<span class="number">1</span>).flat_map(<span class="keyword">lambda</span> x: x.batch(<span class="number">3</span>,drop_remainder=<span class="literal">True</span>)) </span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_window:</span><br><span class="line">    print(x)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor([<span class="number">0</span> <span class="number">1</span> <span class="number">2</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">2</span> <span class="number">3</span> <span class="number">4</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">3</span> <span class="number">4</span> <span class="number">5</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">5</span> <span class="number">6</span> <span class="number">7</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">6</span> <span class="number">7</span> <span class="number">8</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">7</span> <span class="number">8</span> <span class="number">9</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([ <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([ <span class="number">9</span> <span class="number">10</span> <span class="number">11</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br></pre></td></tr></table></figure>
<h4 id="shuffle">shuffle</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#shuffle:数据顺序洗牌。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_shuffle = ds.shuffle(buffer_size = <span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_shuffle:</span><br><span class="line">    print(x)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="number">1</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">4</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">6</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">5</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">2</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">7</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">11</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">3</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">9</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">10</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">8</span>, shape=(), dtype=int64)</span><br></pre></td></tr></table></figure>
<h4 id="repeat">repeat</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#repeat:重复数据若干次，不带参数时，重复无数次。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">3</span>)</span><br><span class="line">ds_repeat = ds.repeat(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_repeat:</span><br><span class="line">    print(x)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">1</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">2</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">1</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">2</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">0</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">1</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">2</span>, shape=(), dtype=int64)</span><br></pre></td></tr></table></figure>
<h4 id="shard">shard</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#shard:采样，从某个位置开始隔固定距离采样一个元素。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_shard = ds.shard(<span class="number">3</span>,index = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_shard:</span><br><span class="line">    print(x)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="number">1</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">4</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">7</span>, shape=(), dtype=int64)</span><br><span class="line">tf.Tensor(<span class="number">10</span>, shape=(), dtype=int64)</span><br></pre></td></tr></table></figure>
<h4 id="take">take</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#take:采样，从开始位置取前几个元素。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_take = ds.take(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">list(ds_take.as_numpy_iterator()) <span class="comment"># [0, 1, 2]</span></span><br></pre></td></tr></table></figure>
<h3 id="ti-sheng-guan-dao-xing-neng">提升管道性能</h3>
<p>训练深度学习模型常常会非常耗时。模型训练的耗时主要来自于两个部分，一部分来自<strong>数据准备</strong>，另一部分来自<strong>参数迭代</strong>。参数迭代过程的耗时通常依赖于GPU来提升。而数据准备过程的耗时则可以通过构建高效的数据管道进行提升。</p>
<p>以下是一些构建高效数据管道的建议:</p>
<ol>
<li>使用 prefetch 方法让数据准备和参数迭代两个过程相互并行。</li>
<li>使用 interleave 方法可以让数据读取过程多进程执行,并将不同来源数据夹在一起。</li>
<li>使用 map 时设置num_parallel_calls 让数据转换过程多进程执行。</li>
<li>使用 cache 方法让数据在第一个epoch后缓存到内存中，仅限于数据集不大情形。</li>
<li>使用 map转换时，先batch, 然后采用向量化的转换方法对每个batch进行转换。</li>
</ol>
<h4 id="prefetch-rang-shu-ju-zhun-bei-he-can-shu-die-dai-bing-xing">prefetch :让数据准备和参数迭代并行</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    ts = tf.timestamp()</span><br><span class="line">    today_ts = ts%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>,end = <span class="string">""</span>)</span><br><span class="line">    tf.print(timestring)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据准备和参数迭代两个过程默认情况下是串行的。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="comment">#假设每次准备数据需要2s</span></span><br><span class="line">        time.sleep(<span class="number">2</span>) </span><br><span class="line">        <span class="keyword">yield</span> i </span><br><span class="line">ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟参数迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#假设每一步训练需要1s</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练过程预计耗时 10*2+10*1 = 30s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training..."</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds:</span><br><span class="line">    train_step()  </span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 prefetch 方法让数据准备和参数迭代两个过程相互并行。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程预计耗时 max(10*2,10*1) = 20s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training with prefetch..."</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.data.experimental.AUTOTUNE 可以让程序自动选择合适的参数</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds.prefetch(buffer_size = tf.data.experimental.AUTOTUNE):</span><br><span class="line">    train_step()  </span><br><span class="line">    </span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure>
<h4 id="interleave-shu-ju-du-qu-duo-jin-cheng-bing-jiang-bu-tong-lai-yuan-shu-ju-jia-zai-yi-qi">interleave :数据读取多进程,并将不同来源数据夹在一起</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ds_files = tf.data.Dataset.list_files(<span class="string">"./data/titanic/*.csv"</span>)</span><br><span class="line">ds = ds_files.flat_map(<span class="keyword">lambda</span> x:tf.data.TextLineDataset(x).skip(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ds.take(<span class="number">4</span>):</span><br><span class="line">    print(line)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="string">b'493,0,1,"Molson, Mr. Harry Markland",male,55.0,0,0,113787,30.5,C30,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'53,1,1,"Harper, Mrs. Henry Sleeper (Myna Haxtun)",female,49.0,1,0,PC 17572,76.7292,D33,C'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'388,1,2,"Buss, Miss. Kate",female,36.0,0,0,27849,13.0,,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'192,0,2,"Carbines, Mr. William",male,19.0,0,0,28424,13.0,,S'</span>, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ds_files = tf.data.Dataset.list_files(<span class="string">"./data/titanic/*.csv"</span>)</span><br><span class="line">ds = ds_files.interleave(<span class="keyword">lambda</span> x:tf.data.TextLineDataset(x).skip(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ds.take(<span class="number">8</span>):</span><br><span class="line">    print(line)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tf.Tensor(<span class="string">b'181,0,3,"Sage, Miss. Constance Gladys",female,,8,2,CA. 2343,69.55,,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'493,0,1,"Molson, Mr. Harry Markland",male,55.0,0,0,113787,30.5,C30,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'405,0,3,"Oreskovic, Miss. Marija",female,20.0,0,0,315096,8.6625,,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'53,1,1,"Harper, Mrs. Henry Sleeper (Myna Haxtun)",female,49.0,1,0,PC 17572,76.7292,D33,C'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'635,0,3,"Skoog, Miss. Mabel",female,9.0,3,2,347088,27.9,,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'388,1,2,"Buss, Miss. Kate",female,36.0,0,0,27849,13.0,,S'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'701,1,1,"Astor, Mrs. John Jacob (Madeleine Talmadge Force)",female,18.0,1,0,PC 17757,227.525,C62 C64,C'</span>, shape=(), dtype=string)</span><br><span class="line">tf.Tensor(<span class="string">b'192,0,2,"Carbines, Mr. William",male,19.0,0,0,28424,13.0,,S'</span>, shape=(), dtype=string)</span><br></pre></td></tr></table></figure>
<h4 id="map-she-zhi-num-parallel-calls-rang-shu-ju-zhuan-huan-guo-cheng-duo-jin-xing-zhi-xing">map : 设置num_parallel_calls 让数据转换过程多进行执行</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ds = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/train/*/*.jpg"</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(img_path,size = <span class="params">(<span class="number">32</span>,<span class="number">32</span>)</span>)</span>:</span></span><br><span class="line">    label = <span class="number">1</span> <span class="keyword">if</span> tf.strings.regex_full_match(img_path,<span class="string">".*/automobile/.*"</span>) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    img = tf.io.read_file(img_path)</span><br><span class="line">    img = tf.image.decode_jpeg(img) <span class="comment">#注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img,size)</span><br><span class="line">    <span class="keyword">return</span>(img,label)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#单进程转换</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start transformation..."</span>))</span><br><span class="line"></span><br><span class="line">ds_map = ds.map(load_image)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> ds_map:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end transformation..."</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#多进程转换</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start parallel transformation..."</span>))</span><br><span class="line"></span><br><span class="line">ds_map_parallel = ds.map(load_image,num_parallel_calls = tf.data.experimental.AUTOTUNE)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> ds_map_parallel:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end parallel transformation..."</span>))</span><br></pre></td></tr></table></figure>
<h4 id="cache-rang-shu-ju-zai-di-yi-ge-epoch-hou-huan-cun-dao-nei-cun-zhong">cache : 让数据在第一个epoch后缓存到内存中</h4>
<p>仅限于数据集不大情形</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="comment">#假设每次准备数据需要2s</span></span><br><span class="line">        time.sleep(<span class="number">2</span>) </span><br><span class="line">        <span class="keyword">yield</span> i </span><br><span class="line">ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟参数迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#假设每一步训练需要0s</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程预计耗时 (5*2+5*0)*3 = 30s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training..."</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> ds:</span><br><span class="line">        train_step()  </span><br><span class="line">    printbar()</span><br><span class="line">    tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">" ended"</span>)</span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="comment">#假设每次准备数据需要2s</span></span><br><span class="line">        time.sleep(<span class="number">2</span>) </span><br><span class="line">        <span class="keyword">yield</span> i </span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 cache 方法让数据在第一个epoch后缓存到内存中，仅限于数据集不大情形。</span></span><br><span class="line">ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32)).cache()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟参数迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#假设每一步训练需要0s</span></span><br><span class="line">    time.sleep(<span class="number">0</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程预计耗时 (5*2+5*0)+(5*0+5*0)*2 = 10s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training..."</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> ds:</span><br><span class="line">        train_step()  </span><br><span class="line">    printbar()</span><br><span class="line">    tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">" ended"</span>)</span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure>
<h4 id="map-zhuan-huan-shi-xian-batch-ran-hou-cai-yong-xiang-liang-hua-de-zhuan-huan-fang-fa-dui-mei-ge-batch-jin-xing-zhuan-huan">map转换时，先batch, 然后采用向量化的转换方法对每个batch进行转换</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先map后batch</span></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">100000</span>)</span><br><span class="line">ds_map_batch = ds.map(<span class="keyword">lambda</span> x:x**<span class="number">2</span>).batch(<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start scalar transformation..."</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_map_batch:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end scalar transformation..."</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先batch后map</span></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">100000</span>)</span><br><span class="line">ds_batch_map = ds.batch(<span class="number">20</span>).map(<span class="keyword">lambda</span> x:x**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start vector transformation..."</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_batch_map:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end vector transformation..."</span>))</span><br></pre></td></tr></table></figure>
<h2 id="te-zheng-lie-feature-column">特征列feature_column</h2>
<p>特征列通常用于对结构化数据实施特征工程时候使用，图像或者文本数据一般不会用到特征列。</p>
<h3 id="te-zheng-lie-yong-fa-gai-shu">特征列用法概述</h3>
<p>使用特征列可以将类别特征转换为one-hot编码特征，将连续特征构建分桶特征，以及对多个特征生成交叉特征等等。</p>
<p>要创建特征列，请调用 tf.feature_column 模块的函数。该模块中常用的九个函数如下图所示，所有九个函数都会返回一个 Categorical-Column 或一个<br>
Dense-Column 对象，但却不会返回 bucketized_column，后者继承自这两个类。</p>
<blockquote>
<p>注意：所有的Catogorical Column类型最终都要通过indicator_column转换成Dense Column类型才能传入模型！</p>
</blockquote>
<p><img src="/2020/05/07/tensorflow/tensorflow2/%E7%89%B9%E5%BE%81%E5%88%979%E7%A7%8D.jpg" alt></p>
<ul>
<li>numeric_column 数值列，最常用。</li>
<li>bucketized_column 分桶列，由数值列生成，可以由一个数值列出多个特征，one-hot编码。</li>
<li>categorical_column_with_identity 分类标识列，one-hot编码，相当于分桶列每个桶为1个整数的情况。</li>
<li>categorical_column_with_vocabulary_list 分类词汇列，one-hot编码，由list指定词典。</li>
<li>categorical_column_with_vocabulary_file 分类词汇列，由文件file指定词典。</li>
<li>categorical_column_with_hash_bucket 哈希列，整数或词典较大时采用。</li>
<li>indicator_column 指标列，由Categorical Column生成，one-hot编码</li>
<li>embedding_column 嵌入列，由Categorical Column生成，嵌入矢量分布参数需要学习。嵌入矢量维数建议取类别数量的 4 次方根。</li>
<li>crossed_column 交叉列，可以由除categorical_column_with_hash_bucket的任意分类列构成。</li>
</ul>
<h3 id="te-zheng-lie-shi-yong-fan-li">特征列使用范例</h3>
<p>以下是一个使用特征列解决Titanic生存问题的完整范例。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#打印日志</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printlog</span><span class="params">(info)</span>:</span></span><br><span class="line">    nowtime = datetime.datetime.now().strftime(<span class="string">'%Y-%m-%d %H:%M:%S'</span>)</span><br><span class="line">    print(<span class="string">"\n"</span>+<span class="string">"=========="</span>*<span class="number">8</span> + <span class="string">"%s"</span>%nowtime)</span><br><span class="line">    print(info+<span class="string">'...\n\n'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="gou-jian-shu-ju-guan-dao-1">构建数据管道</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line"><span class="comment"># 一，构建数据管道</span></span><br><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line">printlog(<span class="string">"step1: prepare dataset..."</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dftrain_raw = pd.read_csv(<span class="string">"./data/titanic/train.csv"</span>)</span><br><span class="line">dftest_raw = pd.read_csv(<span class="string">"./data/titanic/test.csv"</span>)</span><br><span class="line"></span><br><span class="line">dfraw = pd.concat([dftrain_raw,dftest_raw])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_dfdata</span><span class="params">(dfraw)</span>:</span></span><br><span class="line">    dfdata = dfraw.copy()</span><br><span class="line">    dfdata.columns = [x.lower() <span class="keyword">for</span> x <span class="keyword">in</span> dfdata.columns]</span><br><span class="line">    dfdata = dfdata.rename(columns=&#123;<span class="string">'survived'</span>:<span class="string">'label'</span>&#125;)</span><br><span class="line">    dfdata = dfdata.drop([<span class="string">'passengerid'</span>,<span class="string">'name'</span>],axis = <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> col,dtype <span class="keyword">in</span> dict(dfdata.dtypes).items():</span><br><span class="line">        <span class="comment"># 判断是否包含缺失值</span></span><br><span class="line">        <span class="keyword">if</span> dfdata[col].hasnans:</span><br><span class="line">            <span class="comment"># 添加标识是否缺失列</span></span><br><span class="line">            dfdata[col + <span class="string">'_nan'</span>] = pd.isna(dfdata[col]).astype(<span class="string">'int32'</span>)</span><br><span class="line">            <span class="comment"># 填充</span></span><br><span class="line">            <span class="keyword">if</span> dtype <span class="keyword">not</span> <span class="keyword">in</span> [np.object,np.str,np.unicode]:</span><br><span class="line">                dfdata[col].fillna(dfdata[col].mean(),inplace = <span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dfdata[col].fillna(<span class="string">''</span>,inplace = <span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span>(dfdata)</span><br><span class="line"></span><br><span class="line">dfdata = prepare_dfdata(dfraw)</span><br><span class="line">dftrain = dfdata.iloc[<span class="number">0</span>:len(dftrain_raw),:]</span><br><span class="line">dftest = dfdata.iloc[len(dftrain_raw):,:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从 dataframe 导入数据 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">df_to_dataset</span><span class="params">(df, shuffle=True, batch_size=<span class="number">32</span>)</span>:</span></span><br><span class="line">    dfdata = df.copy()</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'label'</span> <span class="keyword">not</span> <span class="keyword">in</span> dfdata.columns:</span><br><span class="line">        ds = tf.data.Dataset.from_tensor_slices(dfdata.to_dict(orient = <span class="string">'list'</span>))</span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        labels = dfdata.pop(<span class="string">'label'</span>)</span><br><span class="line">        ds = tf.data.Dataset.from_tensor_slices((dfdata.to_dict(orient = <span class="string">'list'</span>), labels))  </span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        ds = ds.shuffle(buffer_size=len(dfdata))</span><br><span class="line">    ds = ds.batch(batch_size)</span><br><span class="line">    <span class="keyword">return</span> ds</span><br><span class="line"></span><br><span class="line">ds_train = df_to_dataset(dftrain)</span><br><span class="line">ds_test = df_to_dataset(dftest)</span><br></pre></td></tr></table></figure>
<h4 id="ding-yi-te-zheng-lie">定义特征列</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line"><span class="comment"># 二，定义特征列</span></span><br><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line">printlog(<span class="string">"step2: make feature columns..."</span>)</span><br><span class="line"></span><br><span class="line">feature_columns = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数值列</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> [<span class="string">'age'</span>,<span class="string">'fare'</span>,<span class="string">'parch'</span>,<span class="string">'sibsp'</span>] + [</span><br><span class="line">    c <span class="keyword">for</span> c <span class="keyword">in</span> dfdata.columns <span class="keyword">if</span> c.endswith(<span class="string">'_nan'</span>)]:</span><br><span class="line">    feature_columns.append(tf.feature_column.numeric_column(col))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分桶列</span></span><br><span class="line">age = tf.feature_column.numeric_column(<span class="string">'age'</span>)</span><br><span class="line">age_buckets = tf.feature_column.bucketized_column(age, </span><br><span class="line">             boundaries=[<span class="number">18</span>, <span class="number">25</span>, <span class="number">30</span>, <span class="number">35</span>, <span class="number">40</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">55</span>, <span class="number">60</span>, <span class="number">65</span>])</span><br><span class="line">feature_columns.append(age_buckets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类别列</span></span><br><span class="line"><span class="comment"># 注意：所有的Catogorical Column类型最终都要通过indicator_column转换成Dense Column类型才能传入模型！！</span></span><br><span class="line">sex = tf.feature_column.indicator_column(</span><br><span class="line">      tf.feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">      key=<span class="string">'sex'</span>,vocabulary_list=[<span class="string">"male"</span>, <span class="string">"female"</span>]))</span><br><span class="line">feature_columns.append(sex)</span><br><span class="line"></span><br><span class="line">pclass = tf.feature_column.indicator_column(</span><br><span class="line">      tf.feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">      key=<span class="string">'pclass'</span>,vocabulary_list=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]))</span><br><span class="line">feature_columns.append(pclass)</span><br><span class="line"></span><br><span class="line">ticket = tf.feature_column.indicator_column(</span><br><span class="line">     tf.feature_column.categorical_column_with_hash_bucket(<span class="string">'ticket'</span>,<span class="number">3</span>))</span><br><span class="line">feature_columns.append(ticket)</span><br><span class="line"></span><br><span class="line">embarked = tf.feature_column.indicator_column(</span><br><span class="line">      tf.feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">      key=<span class="string">'embarked'</span>,vocabulary_list=[<span class="string">'S'</span>,<span class="string">'C'</span>,<span class="string">'B'</span>]))</span><br><span class="line">feature_columns.append(embarked)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 嵌入列</span></span><br><span class="line">cabin = tf.feature_column.embedding_column(</span><br><span class="line">    tf.feature_column.categorical_column_with_hash_bucket(<span class="string">'cabin'</span>,<span class="number">32</span>),<span class="number">2</span>)</span><br><span class="line">feature_columns.append(cabin)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉列</span></span><br><span class="line">pclass_cate = tf.feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">          key=<span class="string">'pclass'</span>,vocabulary_list=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">crossed_feature = tf.feature_column.indicator_column(</span><br><span class="line">    tf.feature_column.crossed_column([age_buckets, pclass_cate],hash_bucket_size=<span class="number">15</span>))</span><br><span class="line"></span><br><span class="line">feature_columns.append(crossed_feature)</span><br></pre></td></tr></table></figure>
<h4 id="ding-yi-mo-xing-10">定义模型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line"><span class="comment"># 三，定义模型</span></span><br><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line">printlog(<span class="string">"step3: define model..."</span>)</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">  layers.DenseFeatures(feature_columns), <span class="comment">#将特征列放入到tf.keras.layers.DenseFeatures中!!!</span></span><br><span class="line">  layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">  layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">  layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h4 id="xun-lian-mo-xing-9">训练模型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line"><span class="comment"># 四，训练模型</span></span><br><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line">printlog(<span class="string">"step4: train model..."</span>)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(ds_train,</span><br><span class="line">          validation_data=ds_test,</span><br><span class="line">          epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h4 id="ping-gu-mo-xing-4">评估模型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line"><span class="comment"># 五，评估模型</span></span><br><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line">printlog(<span class="string">"step5: eval model..."</span>)</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">'val_'</span>+metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span>+ metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span>+metric, <span class="string">'val_'</span>+metric])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_metric(history,<span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">dense_features (DenseFeature multiple                  64        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                multiple                  3008      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_1 (Dense)              multiple                  4160      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_2 (Dense)              multiple                  65        </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 7,297</span><br><span class="line">Trainable params: 7,297</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/5-2-01-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0.jpg" alt></p>
<h2 id="ji-huo-han-shu-activation">激活函数activation</h2>
<p>激活函数在深度学习中扮演着非常重要的角色，它给网络赋予了非线性，从而使得神经网络能够拟合任意复杂的函数。</p>
<p>如果没有激活函数，无论多复杂的网络，都等价于单一的线性变换，无法对非线性函数进行拟合。</p>
<p>目前，深度学习中最流行的激活函数为 relu, 但也有些新推出的激活函数，如 swish、GELU 据称效果优于relu激活函数。</p>
<h3 id="chang-yong-ji-huo-han-shu">常用激活函数</h3>
<h4 id="tf-nn-sigmoid">tf.nn.sigmoid</h4>
<p>将实数压缩到0到1之间，一般只在二分类的最后输出层使用。主要缺陷为存在梯度消失问题，计算复杂度高，输出不以0为中心。</p>
<p><img src="/2020/05/07/tensorflow/tensorflow2/sigmoid.png" alt></p>
<h4 id="tf-nn-softmax">tf.nn.softmax</h4>
<p>sigmoid的多分类扩展，一般只在多分类问题的最后输出层使用。</p>
<p><img src="/2020/05/07/tensorflow/tensorflow2/softmax%E8%AF%B4%E6%98%8E.jpg" alt></p>
<h4 id="tf-nn-tanh">tf.nn.tanh</h4>
<p>将实数压缩到-1到1之间，输出期望为0。主要缺陷为存在梯度消失问题，计算复杂度高。</p>
<p><img src="/2020/05/07/tensorflow/tensorflow2/tanh.png" alt></p>
<h4 id="tf-nn-relu">tf.nn.relu</h4>
<p>修正线性单元，最流行的激活函数。一般隐藏层使用。主要缺陷是：输出不以0为中心，输入小于0时存在梯度消失问题(死亡relu)。</p>
<p><img src="/2020/05/07/tensorflow/tensorflow2/relu.png" alt></p>
<h4 id="tf-nn-leaky-relu">tf.nn.leaky_relu</h4>
<p>对修正线性单元的改进，解决了死亡relu问题。</p>
<p><img src="/2020/05/07/tensorflow/tensorflow2/leaky_relu.png" alt></p>
<h4 id="tf-nn-elu">tf.nn.elu</h4>
<p>指数线性单元。对relu的改进，能够缓解死亡relu问题。</p>
<p><img src="/2020/05/07/tensorflow/tensorflow2/elu.png" alt></p>
<h4 id="tf-nn-selu">tf.nn.selu</h4>
<p>扩展型指数线性单元。在权重用tf.keras.initializers.lecun_normal初始化前提下能够对神经网络进行自归一化。不可能出现梯度爆炸或者梯度消失问题。需要和Dropout的变种AlphaDropout一起使用。</p>
<p><img src="/2020/05/07/tensorflow/tensorflow2/selu.png" alt></p>
<h4 id="tf-nn-swish">tf.nn.swish</h4>
<p>自门控激活函数。谷歌出品，相关研究指出用swish替代relu将获得轻微效果提升。</p>
<p><img src="/2020/05/07/tensorflow/tensorflow2/swish.png" alt></p>
<h4 id="gelu">gelu</h4>
<p>高斯误差线性单元激活函数。在Transformer中表现最好。tf.nn模块尚没有实现该函数。</p>
<p><img src="/2020/05/07/tensorflow/tensorflow2/gelu.png" alt></p>
<h3 id="zai-mo-xing-zhong-shi-yong-ji-huo-han-shu">在模型中使用激活函数</h3>
<p>在keras模型中使用激活函数一般有两种方式：</p>
<ol>
<li>一种是作为某些层的activation参数指定</li>
<li>另一种是显式添加layers.Activation激活层</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">32</span>,input_shape = (<span class="literal">None</span>,<span class="number">16</span>),activation = tf.nn.relu)) <span class="comment">#通过activation参数指定</span></span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>))</span><br><span class="line">model.add(layers.Activation(tf.nn.softmax))  <span class="comment"># 显式添加layers.Activation激活层</span></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<h2 id="mo-xing-ceng-layers">模型层layers</h2>
<p>深度学习模型一般由各种模型层组合而成。</p>
<p>tf.keras.layers内置了非常丰富的各种功能的模型层。例如，</p>
<ul>
<li>layers.Dense,layers.Flatten</li>
<li>layers.Input</li>
<li>layers.DenseFeature</li>
<li>layers.Dropout</li>
<li>layers.Conv2D</li>
<li>layers.MaxPooling2D</li>
<li>layers.Conv1D</li>
<li>layers.Embedding</li>
<li>layers.GRU</li>
<li>layers.LSTM</li>
<li>layers.Bidirectional</li>
<li>…</li>
</ul>
<p>如果这些内置模型层不能够满足需求，我们也可以通过编写tf.keras.Lambda匿名模型层或继承tf.keras.layers.Layer基类构建自定义的模型层。其中tf.keras.Lambda匿名模型层只适用于构造没有学习参数的模型层。</p>
<h3 id="nei-zhi-mo-xing-ceng">内置模型层</h3>
<p>一些常用的内置模型层简单介绍如下。</p>
<h4 id="ji-chu-ceng">基础层</h4>
<ul>
<li>
<p>Dense：密集连接层。参数个数 = 输入层特征数× 输出层特征数(weight)＋ 输出层特征数(bias)</p>
</li>
<li>
<p>Activation：激活函数层。一般放在Dense层后面，等价于在Dense层中指定activation。</p>
</li>
<li>
<p>Dropout：随机置零层。训练期间以一定几率将输入置0，一种正则化手段。</p>
</li>
<li>
<p>BatchNormalization：批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。</p>
</li>
<li>
<p>SpatialDropout2D：空间随机置零层。训练期间以一定几率将整个特征图置0，一种正则化手段，有利于避免特征图之间过高的相关性。</p>
</li>
<li>
<p>Input：输入层。通常使用Functional API方式构建模型时作为第一层。</p>
</li>
<li>
<p>DenseFeature：特征列接入层，用于接收一个特征列列表并产生一个密集连接层。</p>
</li>
<li>
<p>Flatten：压平层，用于将多维张量压成一维。</p>
</li>
<li>
<p>Reshape：形状重塑层，改变输入张量的形状。</p>
</li>
<li>
<p>Concatenate：拼接层，将多个张量在某个维度上拼接。</p>
</li>
<li>
<p>Add：加法层。</p>
</li>
<li>
<p>Subtract： 减法层。</p>
</li>
<li>
<p>Maximum：取最大值层。</p>
</li>
<li>
<p>Minimum：取最小值层。</p>
</li>
</ul>
<h4 id="juan-ji-wang-luo-xiang-guan-ceng">卷积网络相关层</h4>
<ul>
<li>
<p>Conv1D：普通一维卷积，常用于文本。参数个数 = 输入通道数×卷积核尺寸(如3)×卷积核个数</p>
</li>
<li>
<p>Conv2D：普通二维卷积，常用于图像。参数个数 = 输入通道数×卷积核尺寸(如3乘3)×卷积核个数</p>
</li>
<li>
<p>Conv3D：普通三维卷积，常用于视频。参数个数 = 输入通道数×卷积核尺寸(如3乘3乘3)×卷积核个数</p>
</li>
<li>
<p>SeparableConv2D：二维深度可分离卷积层。不同于普通卷积同时对区域和通道操作，深度可分离卷积先操作区域，再操作通道。即先对每个通道做独立卷积操作区域，再用1乘1卷积跨通道组合操作通道。参数个数 = 输入通道数×卷积核尺寸 + 输入通道数×1×1×输出通道数。深度可分离卷积的参数数量一般远小于普通卷积，效果一般也更好。</p>
</li>
<li>
<p>DepthwiseConv2D：二维深度卷积层。仅有SeparableConv2D前半部分操作，即只操作区域，不操作通道，一般输出通道数和输入通道数相同，但也可以通过设置depth_multiplier让输出通道为输入通道的若干倍数。输出通道数 = 输入通道数 × depth_multiplier。参数个数 = 输入通道数×卷积核尺寸× depth_multiplier。</p>
</li>
<li>
<p>Conv2DTranspose：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。</p>
</li>
<li>
<p>LocallyConnected2D: 二维局部连接层。类似Conv2D，唯一的差别是没有空间上的权值共享，所以其参数个数远高于二维卷积。</p>
</li>
<li>
<p>MaxPool2D: 二维最大池化层。也称作下采样层。池化层无可训练参数，主要作用是降维。</p>
</li>
<li>
<p>AveragePooling2D: 二维平均池化层。</p>
</li>
<li>
<p>GlobalMaxPool2D: 全局最大池化层。每个通道仅保留一个值。一般从卷积层过渡到全连接层时使用，是Flatten的替代方案。</p>
</li>
<li>
<p>GlobalAvgPool2D: 全局平均池化层。每个通道仅保留一个值。</p>
</li>
</ul>
<h4 id="xun-huan-wang-luo-xiang-guan-ceng">循环网络相关层</h4>
<ul>
<li>
<p>Embedding：嵌入层。一种比Onehot更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。</p>
</li>
<li>
<p>LSTM：长短记忆循环网络层。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置return_sequences = True时可以返回各个中间步骤输出，否则只返回最终输出。</p>
</li>
<li>
<p>GRU：门控循环网络层。LSTM的低配版，不具有携带轨道，参数数量少于LSTM，训练速度更快。</p>
</li>
<li>
<p>SimpleRNN：简单循环网络层。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用。</p>
</li>
<li>
<p>ConvLSTM2D：卷积长短记忆循环网络层。结构上类似LSTM，但对输入的转换操作和对状态的转换操作都是卷积运算。</p>
</li>
<li>
<p>Bidirectional：双向循环网络包装器。可以将LSTM，GRU等层包装成双向循环网络。从而增强特征提取能力。</p>
</li>
<li>
<p>RNN：RNN基本层。接受一个循环网络单元或一个循环单元列表，通过调用tf.keras.backend.rnn函数在序列上进行迭代从而转换成循环网络层。</p>
</li>
<li>
<p>LSTMCell：LSTM单元。和LSTM在整个序列上迭代相比，它仅在序列上迭代一步。可以简单理解LSTM即RNN基本层包裹LSTMCell。</p>
</li>
<li>
<p>GRUCell：GRU单元。和GRU在整个序列上迭代相比，它仅在序列上迭代一步。</p>
</li>
<li>
<p>SimpleRNNCell：SimpleRNN单元。和SimpleRNN在整个序列上迭代相比，它仅在序列上迭代一步。</p>
</li>
<li>
<p>AbstractRNNCell：抽象RNN单元。通过对它的子类化用户可以自定义RNN单元，再通过RNN基本层的包裹实现用户自定义循环网络层。</p>
</li>
<li>
<p>Attention：Dot-product类型注意力机制层。可以用于构建注意力模型。</p>
</li>
<li>
<p>AdditiveAttention：Additive类型注意力机制层。可以用于构建注意力模型。</p>
</li>
<li>
<p>TimeDistributed：时间分布包装器。包装后可以将Dense、Conv2D等作用到每一个时间片段上。</p>
</li>
</ul>
<h3 id="zi-ding-yi-mo-xing-ceng">自定义模型层</h3>
<p>如果自定义模型层没有需要被训练的参数，一般推荐使用Lamda层实现。如果自定义模型层有需要被训练的参数，则可以通过对Layer基类子类化实现。</p>
<p>Lambda层由于没有需要被训练的参数，只需要定义正向传播逻辑即可，使用比Layer基类子类化更加简单。Lambda层的正向逻辑可以使用Python的lambda函数来表达，也可以用def关键字定义函数来表达。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models,regularizers</span><br><span class="line"></span><br><span class="line">mypower = layers.Lambda(<span class="keyword">lambda</span> x:tf.math.pow(x,<span class="number">2</span>))</span><br><span class="line">mypower(tf.range(<span class="number">5</span>)) <span class="comment"># &lt;tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 0,  1,  4,  9, 16], dtype=int32)&gt;</span></span><br></pre></td></tr></table></figure>
<p>Layer的子类化一般需要重新实现初始化方法，Build方法和Call方法。下面是一个简化的线性层的范例，类似Dense.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units=<span class="number">32</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Linear, self).__init__(**kwargs)</span><br><span class="line">        self.units = units</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#build方法一般定义Layer需要被训练的参数。    </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span> </span><br><span class="line">        self.w = self.add_weight(<span class="string">"w"</span>,shape=(input_shape[<span class="number">-1</span>], self.units),</span><br><span class="line">                                 initializer=<span class="string">'random_normal'</span>,</span><br><span class="line">                                 trainable=<span class="literal">True</span>) <span class="comment">#注意必须要有参数名称"w",否则会报错</span></span><br><span class="line">        self.b = self.add_weight(<span class="string">"b"</span>,shape=(self.units,),</span><br><span class="line">                                 initializer=<span class="string">'random_normal'</span>,</span><br><span class="line">                                 trainable=<span class="literal">True</span>)</span><br><span class="line">        super(Linear,self).build(input_shape) <span class="comment"># 相当于设置self.built = True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#call方法一般定义正向传播运算逻辑，__call__方法调用了它。  </span></span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span> </span><br><span class="line">        <span class="keyword">return</span> tf.matmul(inputs, self.w) + self.b</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#如果要让自定义的Layer通过Functional API 组合成模型时可以被保存成h5模型，需要自定义get_config方法。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        config = super(Linear, self).get_config()</span><br><span class="line">        config.update(&#123;<span class="string">'units'</span>: self.units&#125;)</span><br><span class="line">        <span class="keyword">return</span> config</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">linear = Linear(units = <span class="number">8</span>)</span><br><span class="line">print(linear.built) <span class="comment"># False</span></span><br><span class="line"><span class="comment">#指定input_shape，显式调用build方法，第0维代表样本数量，用None填充</span></span><br><span class="line">linear.build(input_shape = (<span class="literal">None</span>,<span class="number">16</span>)) </span><br><span class="line">print(linear.built) <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">linear = Linear(units = <span class="number">8</span>)</span><br><span class="line">print(linear.built) <span class="comment"># False</span></span><br><span class="line">linear.build(input_shape = (<span class="literal">None</span>,<span class="number">16</span>)) </span><br><span class="line">print(linear.compute_output_shape(input_shape = (<span class="literal">None</span>,<span class="number">16</span>))) <span class="comment"># (None, 8)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">linear = Linear(units = <span class="number">16</span>)</span><br><span class="line">print(linear.built) <span class="comment"># False</span></span><br><span class="line"><span class="comment">#如果built = False，调用__call__时会先调用build方法, 再调用call方法。</span></span><br><span class="line">linear(tf.random.uniform((<span class="number">100</span>,<span class="number">64</span>))) </span><br><span class="line">print(linear.built) <span class="comment"># True</span></span><br><span class="line">config = linear.get_config()</span><br><span class="line">print(config) <span class="comment"># &#123;'name': 'linear_3', 'trainable': True, 'dtype': 'float32', 'units': 16&#125;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line"><span class="comment">#注意该处的input_shape会被模型加工，无需使用None代表样本数量维</span></span><br><span class="line">model.add(Linear(units = <span class="number">1</span>,input_shape = (<span class="number">2</span>,)))  </span><br><span class="line">print(<span class="string">"model.input_shape: "</span>,model.input_shape)</span><br><span class="line">print(<span class="string">"model.output_shape: "</span>,model.output_shape)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model.input_shape:  (None, 2)</span><br><span class="line">model.output_shape:  (None, 1)</span><br><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">linear (Linear)              (None, 1)                 3         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 3</span><br><span class="line">Trainable params: 3</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer = <span class="string">"sgd"</span>,loss = <span class="string">"mse"</span>,metrics=[<span class="string">"mae"</span>])</span><br><span class="line">print(model.predict(tf.constant([[<span class="number">3.0</span>,<span class="number">2.0</span>],[<span class="number">4.0</span>,<span class="number">5.0</span>]])))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">-0.04092304</span>]</span><br><span class="line"> [<span class="number">-0.06150477</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存成 h5模型</span></span><br><span class="line">model.save(<span class="string">"./data/linear_model.h5"</span>,save_format = <span class="string">"h5"</span>)</span><br><span class="line">model_loaded_keras = tf.keras.models.load_model(</span><br><span class="line">    <span class="string">"./data/linear_model.h5"</span>,custom_objects=&#123;<span class="string">"Linear"</span>:Linear&#125;)</span><br><span class="line">print(model_loaded_keras.predict(tf.constant([[<span class="number">3.0</span>,<span class="number">2.0</span>],[<span class="number">4.0</span>,<span class="number">5.0</span>]])))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[[<span class="number">-0.04092304</span>]</span><br><span class="line"> [<span class="number">-0.06150477</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存成 tf模型</span></span><br><span class="line">model.save(<span class="string">"./data/linear_model"</span>,save_format = <span class="string">"tf"</span>)</span><br><span class="line">model_loaded_tf = tf.keras.models.load_model(<span class="string">"./data/linear_model"</span>)</span><br><span class="line">print(model_loaded_tf.predict(tf.constant([[<span class="number">3.0</span>,<span class="number">2.0</span>],[<span class="number">4.0</span>,<span class="number">5.0</span>]])))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">INFO:tensorflow:Assets written to: ./data/linear_model/assets</span><br><span class="line">[[<span class="number">-0.04092304</span>]</span><br><span class="line"> [<span class="number">-0.06150477</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="chu-shi-hua-mo-xing-can-shu">初始化模型参数</h3>
<p>模型的默认初始化方法：权重参数元素为[-0.07, 0.07]之间均匀分布的随机数，偏差参数则全为0。但我们经常需要使用其他方法来初始化权重。比如，将权重参数初始化成均值为0、标准差为0.01的正态分布随机数，并依然将偏差参数清零。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.d1 = tf.keras.layers.Dense(</span><br><span class="line">            units=<span class="number">10</span>,</span><br><span class="line">            activation=<span class="literal">None</span>,</span><br><span class="line">            kernel_initializer=tf.random_normal_initializer(mean=<span class="number">0</span>,stddev=<span class="number">0.01</span>),</span><br><span class="line">            bias_initializer=tf.zeros_initializer()</span><br><span class="line">        )</span><br><span class="line">        self.d2 = tf.keras.layers.Dense(</span><br><span class="line">            units=<span class="number">1</span>,</span><br><span class="line">            activation=<span class="literal">None</span>,</span><br><span class="line">            kernel_initializer=tf.ones_initializer(),</span><br><span class="line">            bias_initializer=tf.ones_initializer()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        output = self.d1(input)</span><br><span class="line">        output = self.d2(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"><span class="comment"># 可以使用`tf.keras.initializers`类中的方法实现自定义初始化。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_init</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.keras.initializers.Ones()</span><br><span class="line"></span><br><span class="line">model = tf.keras.models.Sequential()</span><br><span class="line">model.add(tf.keras.layers.Dense(<span class="number">64</span>, kernel_initializer=my_init()))</span><br></pre></td></tr></table></figure>
<h2 id="sun-shi-han-shu-losses">损失函数losses</h2>
<p>一般来说，监督学习的目标函数由损失函数和正则化项组成。（Objective = Loss + Regularization）</p>
<p>对于keras模型，目标函数中的正则化项一般在各层中指定，例如使用Dense的 kernel_regularizer 和 bias_regularizer等参数指定权重使用l1或者l2正则化项，此外还可以用kernel_constraint 和 bias_constraint等参数约束权重的取值范围，这也是一种正则化手段。</p>
<p>损失函数在模型编译时候指定。对于回归模型，通常使用的损失函数是均方损失函数 mean_squared_error。对于二分类模型，通常使用的是二元交叉熵损失函数 binary_crossentropy。</p>
<p>对于多分类模型，如果label是one-hot编码的，则使用类别交叉熵损失函数 categorical_crossentropy。如果label是类别序号编码的，则需要使用稀疏类别交叉熵损失函数 sparse_categorical_crossentropy。</p>
<p>如果有需要，也可以自定义损失函数，自定义损失函数需要接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为损失函数值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models,losses,regularizers,constraints</span><br></pre></td></tr></table></figure>
<h3 id="sun-shi-han-shu-he-zheng-ze-hua-xiang">损失函数和正则化项</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, input_dim=<span class="number">64</span>,</span><br><span class="line">                kernel_regularizer=regularizers.l2(<span class="number">0.01</span>), </span><br><span class="line">                activity_regularizer=regularizers.l1(<span class="number">0.01</span>),</span><br><span class="line">                kernel_constraint = constraints.MaxNorm(max_value=<span class="number">2</span>, axis=<span class="number">0</span>))) </span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>,</span><br><span class="line">        kernel_regularizer=regularizers.l1_l2(<span class="number">0.01</span>,<span class="number">0.01</span>),activation = <span class="string">"sigmoid"</span>))</span><br><span class="line">model.compile(optimizer = <span class="string">"rmsprop"</span>,</span><br><span class="line">        loss = <span class="string">"binary_crossentropy"</span>,metrics = [<span class="string">"AUC"</span>])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">dense (Dense)                (None, 64)                4160      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_1 (Dense)              (None, 10)                650       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 4,810</span><br><span class="line">Trainable params: 4,810</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<h3 id="nei-zhi-sun-shi-han-shu">内置损失函数</h3>
<p>内置的损失函数一般有两种形式:</p>
<ul>
<li>类的实现</li>
<li>函数的实现</li>
</ul>
<p>如：CategoricalCrossentropy 和 categorical_crossentropy 都是类别交叉熵损失函数，前者是类的实现形式，后者是函数的实现形式。</p>
<p>常用的一些内置损失函数说明如下。</p>
<ul>
<li>
<p>mean_squared_error（均方误差损失，用于回归，简写为 mse, 类与函数实现形式分别为 MeanSquaredError 和 MSE）</p>
</li>
<li>
<p>mean_absolute_error (平均绝对值误差损失，用于回归，简写为 mae, 类与函数实现形式分别为 MeanAbsoluteError 和 MAE)</p>
</li>
<li>
<p>mean_absolute_percentage_error (平均百分比误差损失，用于回归，简写为 mape, 类与函数实现形式分别为 MeanAbsolutePercentageError 和 MAPE)</p>
</li>
<li>
<p>Huber(Huber损失，只有类实现形式，用于回归，介于mse和mae之间，对异常值比较鲁棒，相对mse有一定的优势)</p>
</li>
<li>
<p>binary_crossentropy(二元交叉熵，用于二分类，类实现形式为 BinaryCrossentropy)</p>
</li>
<li>
<p>categorical_crossentropy(类别交叉熵，用于多分类，要求label为onehot编码，类实现形式为 CategoricalCrossentropy)</p>
</li>
<li>
<p>sparse_categorical_crossentropy(稀疏类别交叉熵，用于多分类，要求label为序号编码形式，类实现形式为 SparseCategoricalCrossentropy)</p>
</li>
<li>
<p>hinge(合页损失函数，用于二分类，最著名的应用是作为支持向量机SVM的损失函数，类实现形式为 Hinge)</p>
</li>
<li>
<p>kld(相对熵损失，也叫KL散度，常用于最大期望算法EM的损失函数，两个概率分布差异的一种信息度量。类与函数实现形式分别为 KLDivergence 或 KLD)</p>
</li>
<li>
<p>cosine_similarity(余弦相似度，可用于多分类，类实现形式为 CosineSimilarity)</p>
</li>
</ul>
<h3 id="zi-ding-yi-sun-shi-han-shu">自定义损失函数</h3>
<p>自定义损失函数接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为损失函数值。也可以对tf.keras.losses.Loss进行子类化，重写call方法实现损失的计算逻辑，从而得到损失函数的类的实现。</p>
<p>下面是一个Focal Loss的自定义实现示范。Focal Loss是一种对binary_crossentropy的改进损失函数形式。它在样本不均衡和存在较多易分类的样本时相比binary_crossentropy具有明显的优势。它有两个可调参数，alpha参数和gamma参数。其中alpha参数主要用于衰减负样本的权重，gamma参数主要用于衰减容易训练样本的权重。从而让模型更加聚焦在正样本和困难样本上。这就是为什么这个损失函数叫做Focal Loss。</p>
<p>\[focal\_loss(y,p) = \begin{cases}
-\alpha  (1-p)^{\gamma}\log(p) &amp;
\text{if y = 1}\\
-(1-\alpha) p^{\gamma}\log(1-p) &amp;
\text{if y = 0}
\end{cases} \]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">focal_loss</span><span class="params">(gamma=<span class="number">2.</span>, alpha=<span class="number">0.75</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">focal_loss_fixed</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">        bce = tf.losses.binary_crossentropy(y_true, y_pred)</span><br><span class="line">        p_t = (y_true * y_pred) + ((<span class="number">1</span> - y_true) * (<span class="number">1</span> - y_pred))</span><br><span class="line">        alpha_factor = y_true * alpha + (<span class="number">1</span> - y_true) * (<span class="number">1</span> - alpha)</span><br><span class="line">        modulating_factor = tf.pow(<span class="number">1.0</span> - p_t, gamma)</span><br><span class="line">        loss = tf.reduce_sum(alpha_factor * modulating_factor * bce,axis = <span class="number">-1</span> )</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    <span class="keyword">return</span> focal_loss_fixed</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span><span class="params">(tf.keras.losses.Loss)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,gamma=<span class="number">2.0</span>,alpha=<span class="number">0.75</span>,name = <span class="string">"focal_loss"</span>)</span>:</span></span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.alpha = alpha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self,y_true,y_pred)</span>:</span></span><br><span class="line">        bce = tf.losses.binary_crossentropy(y_true, y_pred)</span><br><span class="line">        p_t = (y_true * y_pred) + ((<span class="number">1</span> - y_true) * (<span class="number">1</span> - y_pred))</span><br><span class="line">        alpha_factor = y_true * self.alpha + (<span class="number">1</span> - y_true) * (<span class="number">1</span> - self.alpha)</span><br><span class="line">        modulating_factor = tf.pow(<span class="number">1.0</span> - p_t, self.gamma)</span><br><span class="line">        loss = tf.reduce_sum(alpha_factor * modulating_factor * bce,axis = <span class="number">-1</span> )</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h2 id="ping-gu-zhi-biao-metrics">评估指标metrics</h2>
<p>损失函数除了作为模型训练时候的优化目标，也能够作为模型好坏的一种评价指标。但通常人们还会从其它角度评估模型的好坏。这就是评估指标。通常损失函数都可以作为评估指标，如MAE,MSE,CategoricalCrossentropy等也是常用的评估指标。但评估指标不一定可以作为损失函数，例如AUC,Accuracy,Precision。因为评估指标不要求连续可导，而损失函数通常要求连续可导。</p>
<p>编译模型时，可以通过列表形式指定多个评估指标。如果有需要，也可以自定义评估指标。自定义评估指标需要接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为评估值。</p>
<p>也可以对tf.keras.metrics.Metric进行子类化，重写初始化方法, update_state方法, result方法实现评估指标的计算逻辑，从而得到评估指标的类的实现形式。</p>
<p>由于训练的过程通常是分批次训练的，而评估指标要跑完一个epoch才能够得到整体的指标结果。因此，类形式的评估指标更为常见。即需要编写初始化方法以创建与计算指标结果相关的一些中间变量，编写update_state方法在每个batch后更新相关中间变量的状态，编写result方法输出最终指标结果。</p>
<p>如果编写函数形式的评估指标，则只能取epoch中各个batch计算的评估指标结果的平均值作为整个epoch上的评估指标结果，这个结果通常会偏离整个epoch数据一次计算的结果。</p>
<h3 id="chang-yong-de-nei-zhi-ping-gu-zhi-biao">常用的内置评估指标</h3>
<ul>
<li>
<p>MeanSquaredError（均方误差，用于回归，可以简写为MSE，函数形式为mse）</p>
</li>
<li>
<p>MeanAbsoluteError (平均绝对值误差，用于回归，可以简写为MAE，函数形式为mae)</p>
</li>
<li>
<p>MeanAbsolutePercentageError (平均百分比误差，用于回归，可以简写为MAPE，函数形式为mape)</p>
</li>
<li>
<p>RootMeanSquaredError (均方根误差，用于回归)</p>
</li>
<li>
<p>Accuracy (准确率，用于分类，可以用字符串&quot;Accuracy&quot;表示，Accuracy=(TP+TN)/(TP+TN+FP+FN)，要求y_true和y_pred都为类别序号编码)</p>
</li>
<li>
<p>Precision (精确率，用于二分类，Precision = TP/(TP+FP))</p>
</li>
<li>
<p>Recall (召回率，用于二分类，Recall = TP/(TP+FN))</p>
</li>
<li>
<p>TruePositives (真正例，用于二分类)</p>
</li>
<li>
<p>TrueNegatives (真负例，用于二分类)</p>
</li>
<li>
<p>FalsePositives (假正例，用于二分类)</p>
</li>
<li>
<p>FalseNegatives (假负例，用于二分类)</p>
</li>
<li>
<p>AUC(ROC曲线(TPR vs FPR)下的面积，用于二分类，直观解释为随机抽取一个正样本和一个负样本，正样本的预测值大于负样本的概率)</p>
</li>
<li>
<p>CategoricalAccuracy（分类准确率，与Accuracy含义相同，要求y_true(label)为onehot编码形式）</p>
</li>
<li>
<p>SparseCategoricalAccuracy (稀疏分类准确率，与Accuracy含义相同，要求y_true(label)为序号编码形式)</p>
</li>
<li>
<p>MeanIoU (Intersection-Over-Union，常用于图像分割)</p>
</li>
<li>
<p>TopKCategoricalAccuracy (多分类TopK准确率，要求y_true(label)为onehot编码形式)</p>
</li>
<li>
<p>SparseTopKCategoricalAccuracy (稀疏多分类TopK准确率，要求y_true(label)为序号编码形式)</p>
</li>
<li>
<p>Mean (平均值)</p>
</li>
<li>
<p>Sum (求和)</p>
</li>
</ul>
<h3 id="zi-ding-yi-ping-gu-zhi-biao">自定义评估指标</h3>
<p>以金融风控领域常用的KS指标为例，示范自定义评估指标。KS指标适合二分类问题，其计算方式为 KS=max(TPR-FPR).</p>
<p>其中TPR=TP/(TP+FN) , FPR = FP/(FP+TN) TPR曲线实际上就是正样本的累积分布曲线(CDF)，FPR曲线实际上就是负样本的累积分布曲线(CDF)。KS指标就是正样本和负样本累积分布曲线差值的最大值。</p>
<p><img src="/2020/05/07/tensorflow/tensorflow2/KS_curve.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models,losses,metrics</span><br><span class="line"></span><br><span class="line"><span class="comment">#函数形式的自定义评估指标</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ks</span><span class="params">(y_true,y_pred)</span>:</span></span><br><span class="line">    y_true = tf.reshape(y_true,(<span class="number">-1</span>,))</span><br><span class="line">    y_pred = tf.reshape(y_pred,(<span class="number">-1</span>,))</span><br><span class="line">    length = tf.shape(y_true)[<span class="number">0</span>]</span><br><span class="line">    t = tf.math.top_k(y_pred,k = length,sorted = <span class="literal">False</span>)</span><br><span class="line">    y_pred_sorted = tf.gather(y_pred,t.indices)</span><br><span class="line">    y_true_sorted = tf.gather(y_true,t.indices)</span><br><span class="line">    cum_positive_ratio = tf.truediv(</span><br><span class="line">        tf.cumsum(y_true_sorted),tf.reduce_sum(y_true_sorted))</span><br><span class="line">    cum_negative_ratio = tf.truediv(</span><br><span class="line">        tf.cumsum(<span class="number">1</span> - y_true_sorted),tf.reduce_sum(<span class="number">1</span> - y_true_sorted))</span><br><span class="line">    ks_value = tf.reduce_max(tf.abs(cum_positive_ratio - cum_negative_ratio)) </span><br><span class="line">    <span class="keyword">return</span> ks_value</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y_true = tf.constant([[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">0</span>]])</span><br><span class="line">y_pred = tf.constant([[<span class="number">0.6</span>],[<span class="number">0.1</span>],[<span class="number">0.4</span>],[<span class="number">0.5</span>],[<span class="number">0.7</span>],[<span class="number">0.7</span>],[<span class="number">0.7</span>],</span><br><span class="line">                      [<span class="number">0.4</span>],[<span class="number">0.4</span>],[<span class="number">0.5</span>],[<span class="number">0.8</span>],[<span class="number">0.3</span>],[<span class="number">0.5</span>],[<span class="number">0.3</span>]])</span><br><span class="line">tf.print(ks(y_true,y_pred)) <span class="comment"># 0.625</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#类形式的自定义评估指标</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KS</span><span class="params">(metrics.Metric)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name = <span class="string">"ks"</span>, **kwargs)</span>:</span></span><br><span class="line">        super(KS,self).__init__(name=name,**kwargs)</span><br><span class="line">        self.true_positives = self.add_weight(</span><br><span class="line">            name = <span class="string">"tp"</span>,shape = (<span class="number">101</span>,), initializer = <span class="string">"zeros"</span>)</span><br><span class="line">        self.false_positives = self.add_weight(</span><br><span class="line">            name = <span class="string">"fp"</span>,shape = (<span class="number">101</span>,), initializer = <span class="string">"zeros"</span>)</span><br><span class="line">   </span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_state</span><span class="params">(self,y_true,y_pred)</span>:</span></span><br><span class="line">        y_true = tf.cast(tf.reshape(y_true,(<span class="number">-1</span>,)),tf.bool)</span><br><span class="line">        y_pred = tf.cast(<span class="number">100</span>*tf.reshape(y_pred,(<span class="number">-1</span>,)),tf.int32)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tf.range(<span class="number">0</span>,tf.shape(y_true)[<span class="number">0</span>]):</span><br><span class="line">            <span class="keyword">if</span> y_true[i]:</span><br><span class="line">                self.true_positives[y_pred[i]].assign(</span><br><span class="line">                    self.true_positives[y_pred[i]]+<span class="number">1.0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.false_positives[y_pred[i]].assign(</span><br><span class="line">                    self.false_positives[y_pred[i]]+<span class="number">1.0</span>)</span><br><span class="line">        <span class="keyword">return</span> (self.true_positives,self.false_positives)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">result</span><span class="params">(self)</span>:</span></span><br><span class="line">        cum_positive_ratio = tf.truediv(</span><br><span class="line">            tf.cumsum(self.true_positives),tf.reduce_sum(self.true_positives))</span><br><span class="line">        cum_negative_ratio = tf.truediv(</span><br><span class="line">            tf.cumsum(self.false_positives),tf.reduce_sum(self.false_positives))</span><br><span class="line">        ks_value = tf.reduce_max(tf.abs(cum_positive_ratio - cum_negative_ratio)) </span><br><span class="line">        <span class="keyword">return</span> ks_value</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y_true = tf.constant([[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">0</span>]])</span><br><span class="line">y_pred = tf.constant([[<span class="number">0.6</span>],[<span class="number">0.1</span>],[<span class="number">0.4</span>],[<span class="number">0.5</span>],[<span class="number">0.7</span>],[<span class="number">0.7</span>],</span><br><span class="line">                      [<span class="number">0.7</span>],[<span class="number">0.4</span>],[<span class="number">0.4</span>],[<span class="number">0.5</span>],[<span class="number">0.8</span>],[<span class="number">0.3</span>],[<span class="number">0.5</span>],[<span class="number">0.3</span>]])</span><br><span class="line"></span><br><span class="line">myks = KS()</span><br><span class="line">myks.update_state(y_true,y_pred)</span><br><span class="line">tf.print(myks.result()) <span class="comment"># 0.625</span></span><br></pre></td></tr></table></figure>
<h2 id="you-hua-qi-optimizers">优化器optimizers</h2>
<p>模型优化算法的选择直接关系到最终模型的性能。有时候效果不好，未必是特征的问题或者模型设计的问题，很可能就是优化算法的问题。深度学习优化算法大概经历了 SGD -&gt; SGDM -&gt; NAG -&gt;Adagrad -&gt; Adadelta(RMSprop) -&gt; Adam -&gt; Nadam 这样的发展历程。</p>
<p>对于一般新手炼丹师，优化器直接使用Adam，并使用其默认参数就OK了。</p>
<p>一些爱写论文的炼丹师由于追求评估指标效果，可能会偏爱前期使用Adam优化器快速下降，后期使用SGD并精调优化器参数得到更好的结果。此外目前也有一些前沿的优化算法，据称效果比Adam更好，例如LazyAdam, Look-ahead, RAdam, Ranger等.</p>
<h3 id="you-hua-qi-de-shi-yong">优化器的使用</h3>
<p>优化器主要使用apply_gradients方法传入变量和对应梯度从而来对给定变量进行迭代，或者直接使用minimize方法对目标函数进行迭代优化。</p>
<p>当然，更常见的使用是在编译时将优化器传入keras的Model,通过调用model.fit实现对Loss的的迭代优化。</p>
<p>初始化优化器时会创建一个变量optimier.iterations用于记录迭代的次数。因此优化器和tf.Variable一样，一般需要在@tf.function外创建。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    ts = tf.timestamp()</span><br><span class="line">    today_ts = ts%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>,end = <span class="string">""</span>)</span><br><span class="line">    tf.print(timestring)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用optimizer.apply_gradients</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minimizef</span><span class="params">()</span>:</span></span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> tf.constant(<span class="literal">True</span>): </span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">        dy_dx = tape.gradient(y,x)</span><br><span class="line">        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#迭代终止条件</span></span><br><span class="line">        <span class="keyword">if</span> tf.abs(dy_dx)&lt;tf.constant(<span class="number">0.00001</span>):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> tf.math.mod(optimizer.iterations,<span class="number">100</span>)==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"step = "</span>,optimizer.iterations)</span><br><span class="line">            tf.print(<span class="string">"x = "</span>, x)</span><br><span class="line">            tf.print(<span class="string">""</span>)</span><br><span class="line">                </span><br><span class="line">    y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"y ="</span>,minimizef())</span><br><span class="line">tf.print(<span class="string">"x ="</span>,x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用optimizer.minimize</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)   </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">()</span>:</span>   </span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    y = a*tf.pow(x,<span class="number">2</span>)+b*x+c</span><br><span class="line">    <span class="keyword">return</span>(y)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epoch = <span class="number">1000</span>)</span>:</span>  </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> tf.range(epoch):  </span><br><span class="line">        optimizer.minimize(f,[x])</span><br><span class="line">    tf.print(<span class="string">"epoch = "</span>,optimizer.iterations)</span><br><span class="line">    <span class="keyword">return</span>(f())</span><br><span class="line"></span><br><span class="line">train(<span class="number">1000</span>)</span><br><span class="line">tf.print(<span class="string">"y = "</span>,f())</span><br><span class="line">tf.print(<span class="string">"x = "</span>,x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"><span class="comment"># 使用model.fit</span></span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FakeModel</span><span class="params">(tf.keras.models.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,a,b,c)</span>:</span></span><br><span class="line">        super(FakeModel,self).__init__()</span><br><span class="line">        self.a = a</span><br><span class="line">        self.b = b</span><br><span class="line">        self.c = c</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>)</span><br><span class="line">        self.built = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self,features)</span>:</span></span><br><span class="line">        loss  = self.a*(self.x)**<span class="number">2</span>+self.b*(self.x)+self.c</span><br><span class="line">        <span class="keyword">return</span>(tf.ones_like(features)*loss)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myloss</span><span class="params">(y_true,y_pred)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(y_pred)</span><br><span class="line"></span><br><span class="line">model = FakeModel(tf.constant(<span class="number">1.0</span>),tf.constant(<span class="number">-2.0</span>),tf.constant(<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line">model.build()</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.compile(optimizer = </span><br><span class="line">              tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>),loss = myloss)</span><br><span class="line">history = model.fit(tf.zeros((<span class="number">100</span>,<span class="number">2</span>)),</span><br><span class="line">                    tf.ones(<span class="number">100</span>),batch_size = <span class="number">1</span>,epochs = <span class="number">10</span>)  <span class="comment">#迭代1000次</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.print(<span class="string">"x="</span>,model.x)</span><br><span class="line">tf.print(<span class="string">"loss="</span>,model(tf.constant(<span class="number">0.0</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="nei-zhi-you-hua-qi">内置优化器</h3>
<p>深度学习优化算法大概经历了 SGD -&gt; SGDM -&gt; NAG -&gt;Adagrad -&gt; Adadelta(RMSprop) -&gt; Adam -&gt; Nadam 这样的发展历程。</p>
<p>在keras.optimizers子模块中，它们基本上都有对应的类的实现。</p>
<ul>
<li>
<p>SGD, 默认参数为纯SGD, 设置momentum参数不为0实际上变成SGDM, 考虑了一阶动量, 设置 nesterov为True后变成NAG，即 Nesterov Accelerated Gradient，在计算梯度时计算的是向前走一步所在位置的梯度。</p>
</li>
<li>
<p>Adagrad, 考虑了二阶动量，对于不同的参数有不同的学习率，即自适应学习率。缺点是学习率单调下降，可能后期学习速率过慢乃至提前停止学习。</p>
</li>
<li>
<p>RMSprop, 考虑了二阶动量，对于不同的参数有不同的学习率，即自适应学习率，对Adagrad进行了优化，通过指数平滑只考虑一定窗口内的二阶动量。</p>
</li>
<li>
<p>Adadelta, 考虑了二阶动量，与RMSprop类似，但是更加复杂一些，自适应性更强。</p>
</li>
<li>
<p>Adam, 同时考虑了一阶动量和二阶动量，可以看成RMSprop上进一步考虑了一阶动量。</p>
</li>
<li>
<p>Nadam, 在Adam基础上进一步考虑了 Nesterov Acceleration。</p>
</li>
</ul>
<h2 id="hui-diao-han-shu-callbacks">回调函数callbacks</h2>
<p>tf.keras的回调函数实际上是一个类，一般是在model.fit时作为参数指定，用于控制在训练过程开始或者在训练过程结束，在每个epoch训练开始或者训练结束，在每个batch训练开始或者训练结束时执行一些操作，例如收集一些日志信息，改变学习率等超参数，提前终止训练过程等等。</p>
<p>同样地，针对model.evaluate或者model.predict也可以指定callbacks参数，用于控制在评估或预测开始或者结束时，在每个batch开始或者结束时执行一些操作，但这种用法相对少见。</p>
<p>大部分时候，keras.callbacks子模块中定义的回调函数类已经足够使用了，如果有特定的需要，我们也可以通过对keras.callbacks.Callbacks实施子类化构造自定义的回调函数。所有回调函数都继承至 keras.callbacks.Callbacks基类，拥有params和model这两个属性。其中params 是一个dict，记录了训练相关参数 (例如 verbosity, batch size, number of epochs 等等)。model即当前关联的模型的引用。</p>
<p>此外，对于回调类中的一些方法如on_epoch_begin,on_batch_end，还会有一个输入参数logs, 提供有关当前epoch或者batch的一些信息，并能够记录计算结果，如果model.fit指定了多个回调函数类，这些logs变量将在这些回调函数类的同名函数间依顺序传递。</p>
<h3 id="nei-zhi-hui-diao-han-shu">内置回调函数</h3>
<ul>
<li>
<p>BaseLogger： 收集每个epoch上metrics在各个batch上的平均值，对stateful_metrics参数中的带中间状态的指标直接拿最终值无需对各个batch平均，指标均值结果将添加到logs变量中。该回调函数被所有模型默认添加，且是第一个被添加的。</p>
</li>
<li>
<p>History： 将BaseLogger计算的各个epoch的metrics结果记录到history这个dict变量中，并作为model.fit的返回值。该回调函数被所有模型默认添加，在BaseLogger之后被添加。</p>
</li>
<li>
<p>EarlyStopping： 当被监控指标在设定的若干个epoch后没有提升，则提前终止训练。</p>
</li>
<li>
<p>TensorBoard： 为Tensorboard可视化保存日志信息。支持评估指标，计算图，模型参数等的可视化。</p>
</li>
<li>
<p>ModelCheckpoint： 在每个epoch后保存模型。</p>
</li>
<li>
<p>ReduceLROnPlateau：如果监控指标在设定的若干个epoch后没有提升，则以一定的因子减少学习率。</p>
</li>
<li>
<p>TerminateOnNaN：如果遇到loss为NaN，提前终止训练。</p>
</li>
<li>
<p>LearningRateScheduler：学习率控制器。给定学习率lr和epoch的函数关系，根据该函数关系在每个epoch前调整学习率。</p>
</li>
<li>
<p>CSVLogger：将每个epoch后的logs结果记录到CSV文件中。</p>
</li>
<li>
<p>ProgbarLogger：将每个epoch后的logs结果打印到标准输出流中。</p>
</li>
</ul>
<h3 id="zi-ding-yi-hui-diao-han-shu">自定义回调函数</h3>
<p>可以使用callbacks.LambdaCallback编写较为简单的回调函数，也可以通过对callbacks.Callback子类化编写更加复杂的回调函数逻辑。如果需要深入学习tf.Keras中的回调函数，不要犹豫阅读内置回调函数的源代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models,losses,metrics,callbacks</span><br><span class="line"><span class="keyword">import</span> tensorflow.keras.backend <span class="keyword">as</span> K</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示范使用LambdaCallback编写较为简单的回调函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">json_log = open(<span class="string">'./data/keras_log.json'</span>, mode=<span class="string">'wt'</span>, buffering=<span class="number">1</span>)</span><br><span class="line">json_logging_callback = callbacks.LambdaCallback(</span><br><span class="line">    on_epoch_end=<span class="keyword">lambda</span> epoch, logs: json_log.write(</span><br><span class="line">        json.dumps(dict(epoch = epoch,**logs)) + <span class="string">'\n'</span>),</span><br><span class="line">    on_train_end=<span class="keyword">lambda</span> logs: json_log.close()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示范通过Callback子类化编写回调函数（LearningRateScheduler的源代码）</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LearningRateScheduler</span><span class="params">(callbacks.Callback)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, schedule, verbose=<span class="number">0</span>)</span>:</span></span><br><span class="line">        super(LearningRateScheduler, self).__init__()</span><br><span class="line">        self.schedule = schedule</span><br><span class="line">        self.verbose = verbose</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_begin</span><span class="params">(self, epoch, logs=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(self.model.optimizer, <span class="string">'lr'</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Optimizer must have a "lr" attribute.'</span>)</span><br><span class="line">        <span class="keyword">try</span>:  </span><br><span class="line">            lr = float(K.get_value(self.model.optimizer.lr))</span><br><span class="line">            lr = self.schedule(epoch, lr)</span><br><span class="line">        <span class="keyword">except</span> TypeError:  <span class="comment"># Support for old API for backward compatibility</span></span><br><span class="line">            lr = self.schedule(epoch)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(lr, (tf.Tensor, float, np.float32, np.float64)):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'The output of the "schedule" function '</span></span><br><span class="line">                             <span class="string">'should be float.'</span>)</span><br><span class="line">        <span class="keyword">if</span> isinstance(lr, ops.Tensor) <span class="keyword">and</span> <span class="keyword">not</span> lr.dtype.is_floating:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'The dtype of Tensor should be float'</span>)</span><br><span class="line">        K.set_value(self.model.optimizer.lr, K.get_value(lr))</span><br><span class="line">        <span class="keyword">if</span> self.verbose &gt; <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'\nEpoch %05d: LearningRateScheduler reducing learning '</span></span><br><span class="line">                 <span class="string">'rate to %s.'</span> % (epoch + <span class="number">1</span>, lr))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span><span class="params">(self, epoch, logs=None)</span>:</span></span><br><span class="line">        logs = logs <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">        logs[<span class="string">'lr'</span>] = K.get_value(self.model.optimizer.lr)</span><br></pre></td></tr></table></figure>
<h1 id="tensor-flow-de-gao-jie-api">TensorFlow的高阶API</h1>
<p>TensorFlow的高阶API主要是tensorflow.keras.models.</p>
<p>本章主要详细介绍tensorflow.keras.models相关的以下内容。</p>
<ul>
<li>
<p>模型的构建（Sequential、functional API、Model子类化）</p>
</li>
<li>
<p>模型的训练（内置fit方法、内置train_on_batch方法、自定义训练循环、单GPU训练模型、多GPU训练模型、TPU训练模型）</p>
</li>
<li>
<p>模型的部署（tensorflow serving部署模型、使用spark(scala)调用tensorflow模型）</p>
</li>
</ul>
<h2 id="gou-jian-mo-xing-de-3-chong-fang-fa">构建模型的3种方法</h2>
<p>可以使用以下3种方式构建模型：</p>
<ol>
<li>使用Sequential按层顺序构建模型</li>
<li>使用函数式API构建任意结构模型</li>
<li>继承Model基类构建自定义模型。</li>
</ol>
<p>对于顺序结构的模型，优先使用Sequential方法构建。</p>
<p>如果模型有多输入或者多输出，或者模型需要共享权重，或者模型具有残差连接等非顺序结构，推荐使用函数式API进行创建。</p>
<p>如果无特定必要，尽可能避免使用Model子类化的方式构建模型，这种方式提供了极大的灵活性，但也有更大的概率出错。</p>
<p>下面以IMDB电影评论的分类问题为例，演示3种创建模型的方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_token_path = <span class="string">"./data/imdb/train_token.csv"</span></span><br><span class="line">test_token_path = <span class="string">"./data/imdb/test_token.csv"</span></span><br><span class="line"></span><br><span class="line">MAX_WORDS = <span class="number">10000</span>  <span class="comment"># We will only consider the top 10,000 words in the dataset</span></span><br><span class="line">MAX_LEN = <span class="number">200</span>  <span class="comment"># We will cut reviews after 200 words</span></span><br><span class="line">BATCH_SIZE = <span class="number">20</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建管道</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_line</span><span class="params">(line)</span>:</span></span><br><span class="line">    t = tf.strings.split(line,<span class="string">"\t"</span>)</span><br><span class="line">    label = tf.reshape(tf.cast(tf.strings.to_number(t[<span class="number">0</span>]),tf.int32),(<span class="number">-1</span>,))</span><br><span class="line">    features = tf.cast(tf.strings.to_number(tf.strings.split(t[<span class="number">1</span>],<span class="string">" "</span>)),tf.int32)</span><br><span class="line">    <span class="keyword">return</span> (features,label)</span><br><span class="line"></span><br><span class="line">ds_train=  tf.data.TextLineDataset(filenames = [train_token_path]) \</span><br><span class="line">   .map(parse_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \</span><br><span class="line">   .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">   .prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">ds_test=  tf.data.TextLineDataset(filenames = [test_token_path]) \</span><br><span class="line">   .map(parse_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \</span><br><span class="line">   .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">   .prefetch(tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure>
<h3 id="sequential-an-ceng-shun-xu-chuang-jian-mo-xing">Sequential按层顺序创建模型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line"></span><br><span class="line">model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'Nadam'</span>,</span><br><span class="line">            loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>,<span class="string">"AUC"</span>])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/Sequential%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">baselogger = callbacks.BaseLogger(stateful_metrics=[<span class="string">"AUC"</span>])</span><br><span class="line">logdir = <span class="string">"./data/keras_model/"</span> + datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=<span class="number">1</span>)</span><br><span class="line">history = model.fit(ds_train,validation_data = ds_test,</span><br><span class="line">        epochs = <span class="number">6</span>,callbacks=[baselogger,tensorboard_callback])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">'val_'</span>+metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span>+ metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span>+metric, <span class="string">'val_'</span>+metric])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"AUC"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/6-1-fit%E6%A8%A1%E5%9E%8B.jpg" alt></p>
<h3 id="han-shu-shi-api-chuang-jian-ren-yi-jie-gou-mo-xing">函数式API创建任意结构模型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">inputs = layers.Input(shape=[MAX_LEN])</span><br><span class="line">x  = layers.Embedding(MAX_WORDS,<span class="number">7</span>)(inputs)</span><br><span class="line"></span><br><span class="line">branch1 = layers.SeparableConv1D(<span class="number">64</span>,<span class="number">3</span>,activation=<span class="string">"relu"</span>)(x)</span><br><span class="line">branch1 = layers.MaxPool1D(<span class="number">3</span>)(branch1)</span><br><span class="line">branch1 = layers.SeparableConv1D(<span class="number">32</span>,<span class="number">3</span>,activation=<span class="string">"relu"</span>)(branch1)</span><br><span class="line">branch1 = layers.GlobalMaxPool1D()(branch1)</span><br><span class="line"></span><br><span class="line">branch2 = layers.SeparableConv1D(<span class="number">64</span>,<span class="number">5</span>,activation=<span class="string">"relu"</span>)(x)</span><br><span class="line">branch2 = layers.MaxPool1D(<span class="number">5</span>)(branch2)</span><br><span class="line">branch2 = layers.SeparableConv1D(<span class="number">32</span>,<span class="number">5</span>,activation=<span class="string">"relu"</span>)(branch2)</span><br><span class="line">branch2 = layers.GlobalMaxPool1D()(branch2)</span><br><span class="line"></span><br><span class="line">branch3 = layers.SeparableConv1D(<span class="number">64</span>,<span class="number">7</span>,activation=<span class="string">"relu"</span>)(x)</span><br><span class="line">branch3 = layers.MaxPool1D(<span class="number">7</span>)(branch3)</span><br><span class="line">branch3 = layers.SeparableConv1D(<span class="number">32</span>,<span class="number">7</span>,activation=<span class="string">"relu"</span>)(branch3)</span><br><span class="line">branch3 = layers.GlobalMaxPool1D()(branch3)</span><br><span class="line"></span><br><span class="line">concat = layers.Concatenate()([branch1,branch2,branch3])</span><br><span class="line">outputs = layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>)(concat)</span><br><span class="line"></span><br><span class="line">model = models.Model(inputs = inputs,outputs = outputs)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'Nadam'</span>,</span><br><span class="line">            loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>,<span class="string">"AUC"</span>])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">Model: "model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">Layer (type)                    Output Shape         Param #     Connected to                     </span><br><span class="line">==================================================================================================</span><br><span class="line">input_1 (InputLayer)            [(None, 200)]        0                                            </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">embedding (Embedding)           (None, 200, 7)       70000       input_1[<span class="string">0</span>][<span class="symbol">0</span>]                    </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable_conv1d (SeparableConv (None, 198, 64)      533         embedding[<span class="string">0</span>][<span class="symbol">0</span>]                  </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable<span class="emphasis">_conv1d_</span>2 (SeparableCo (None, 196, 64)      547         embedding[<span class="string">0</span>][<span class="symbol">0</span>]                  </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable<span class="emphasis">_conv1d_</span>4 (SeparableCo (None, 194, 64)      561         embedding[<span class="string">0</span>][<span class="symbol">0</span>]                  </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">max<span class="emphasis">_pooling1d (MaxPooling1D)    (None, 66, 64)       0           separable_</span>conv1d[<span class="string">0</span>][<span class="symbol">0</span>]           </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">max<span class="emphasis">_pooling1d_</span>1 (MaxPooling1D)  (None, 39, 64)       0           separable<span class="emphasis">_conv1d_</span>2[<span class="string">0</span>][<span class="symbol">0</span>]         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">max<span class="emphasis">_pooling1d_</span>2 (MaxPooling1D)  (None, 27, 64)       0           separable<span class="emphasis">_conv1d_</span>4[<span class="string">0</span>][<span class="symbol">0</span>]         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable<span class="emphasis">_conv1d_</span>1 (SeparableCo (None, 64, 32)       2272        max_pooling1d[<span class="string">0</span>][<span class="symbol">0</span>]              </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable<span class="emphasis">_conv1d_</span>3 (SeparableCo (None, 35, 32)       2400        max<span class="emphasis">_pooling1d_</span>1[<span class="string">0</span>][<span class="symbol">0</span>]            </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable<span class="emphasis">_conv1d_</span>5 (SeparableCo (None, 21, 32)       2528        max<span class="emphasis">_pooling1d_</span>2[<span class="string">0</span>][<span class="symbol">0</span>]            </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">global<span class="emphasis">_max_</span>pooling1d (GlobalMax (None, 32)           0           separable<span class="emphasis">_conv1d_</span>1[<span class="string">0</span>][<span class="symbol">0</span>]         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">global<span class="emphasis">_max_</span>pooling1d<span class="emphasis">_1 (GlobalM (None, 32)           0           separable_</span>conv1d_3[<span class="string">0</span>][<span class="symbol">0</span>]         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">global<span class="emphasis">_max_</span>pooling1d<span class="emphasis">_2 (GlobalM (None, 32)           0           separable_</span>conv1d_5[<span class="string">0</span>][<span class="symbol">0</span>]         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">concatenate (Concatenate)       (None, 96)           0           global<span class="emphasis">_max_</span>pooling1d[<span class="string">0</span>][<span class="symbol">0</span>]       </span><br><span class="line"><span class="code">                                                                 global_max_pooling1d_1[0][0]     </span></span><br><span class="line"><span class="code">                                                                 global_max_pooling1d_2[0][0]     </span></span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">dense (Dense)                   (None, 1)            97          concatenate[<span class="string">0</span>][<span class="symbol">0</span>]                </span><br><span class="line">==================================================================================================</span><br><span class="line">Total params: 78,938</span><br><span class="line">Trainable params: 78,938</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/FunctionalAPI%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">logdir = <span class="string">"./data/keras_model/"</span> + datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=<span class="number">1</span>)</span><br><span class="line">history = model.fit(ds_train,validation_data = ds_test,epochs = <span class="number">6</span>,callbacks=[tensorboard_callback])</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">1</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">32</span>s <span class="number">32</span>ms/step - loss: <span class="number">0.5527</span> - accuracy: <span class="number">0.6758</span> - AUC: <span class="number">0.7731</span> - val_loss: <span class="number">0.3646</span> - val_accuracy: <span class="number">0.8426</span> - val_AUC: <span class="number">0.9192</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">24</span>s <span class="number">24</span>ms/step - loss: <span class="number">0.3024</span> - accuracy: <span class="number">0.8737</span> - AUC: <span class="number">0.9444</span> - val_loss: <span class="number">0.3281</span> - val_accuracy: <span class="number">0.8644</span> - val_AUC: <span class="number">0.9350</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">24</span>s <span class="number">24</span>ms/step - loss: <span class="number">0.2158</span> - accuracy: <span class="number">0.9159</span> - AUC: <span class="number">0.9715</span> - val_loss: <span class="number">0.3461</span> - val_accuracy: <span class="number">0.8666</span> - val_AUC: <span class="number">0.9363</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">24</span>s <span class="number">24</span>ms/step - loss: <span class="number">0.1492</span> - accuracy: <span class="number">0.9464</span> - AUC: <span class="number">0.9859</span> - val_loss: <span class="number">0.4017</span> - val_accuracy: <span class="number">0.8568</span> - val_AUC: <span class="number">0.9311</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">24</span>s <span class="number">24</span>ms/step - loss: <span class="number">0.0944</span> - accuracy: <span class="number">0.9696</span> - AUC: <span class="number">0.9939</span> - val_loss: <span class="number">0.4998</span> - val_accuracy: <span class="number">0.8550</span> - val_AUC: <span class="number">0.9233</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">26</span>s <span class="number">26</span>ms/step - loss: <span class="number">0.0526</span> - accuracy: <span class="number">0.9865</span> - AUC: <span class="number">0.9977</span> - val_loss: <span class="number">0.6463</span> - val_accuracy: <span class="number">0.8462</span> - val_AUC: <span class="number">0.9138</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"AUC"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/6-1-2-train.jpg" alt></p>
<h3 id="model-zi-lei-hua-chuang-jian-zi-ding-yi-mo-xing">Model子类化创建自定义模型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先自定义一个残差模块，为自定义Layer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResBlock</span><span class="params">(layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kernel_size, **kwargs)</span>:</span></span><br><span class="line">        super(ResBlock, self).__init__(**kwargs)</span><br><span class="line">        self.kernel_size = kernel_size</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self,input_shape)</span>:</span></span><br><span class="line">        self.conv1 = layers.Conv1D(filters=<span class="number">64</span>,kernel_size=self.kernel_size,</span><br><span class="line">                                   activation = <span class="string">"relu"</span>,padding=<span class="string">"same"</span>)</span><br><span class="line">        self.conv2 = layers.Conv1D(filters=<span class="number">32</span>,kernel_size=self.kernel_size,</span><br><span class="line">                                   activation = <span class="string">"relu"</span>,padding=<span class="string">"same"</span>)</span><br><span class="line">        self.conv3 = layers.Conv1D(filters=input_shape[<span class="number">-1</span>],</span><br><span class="line">                                   kernel_size=self.kernel_size,activation = <span class="string">"relu"</span>,padding=<span class="string">"same"</span>)</span><br><span class="line">        self.maxpool = layers.MaxPool1D(<span class="number">2</span>)</span><br><span class="line">        super(ResBlock,self).build(input_shape) <span class="comment"># 相当于设置self.built = True</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = self.conv1(inputs)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = layers.Add()([inputs,x])</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#如果要让自定义的Layer通过Functional API 组合成模型时可以序列化，需要自定义get_config方法。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        config = super(ResBlock, self).get_config()</span><br><span class="line">        config.update(&#123;<span class="string">'kernel_size'</span>: self.kernel_size&#125;)</span><br><span class="line">        <span class="keyword">return</span> config</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试ResBlock</span></span><br><span class="line">resblock = ResBlock(kernel_size = <span class="number">3</span>)</span><br><span class="line">resblock.build(input_shape = (<span class="literal">None</span>,<span class="number">200</span>,<span class="number">7</span>))</span><br><span class="line">resblock.compute_output_shape(input_shape=(<span class="literal">None</span>,<span class="number">200</span>,<span class="number">7</span>))</span><br><span class="line"><span class="comment"># TensorShape([None, 100, 7])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义模型，实际上也可以使用Sequential或者FunctionalAPI</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImdbModel</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(ImdbModel, self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self,input_shape)</span>:</span></span><br><span class="line">        self.embedding = layers.Embedding(MAX_WORDS,<span class="number">7</span>)</span><br><span class="line">        self.block1 = ResBlock(<span class="number">7</span>)</span><br><span class="line">        self.block2 = ResBlock(<span class="number">5</span>)</span><br><span class="line">        self.dense = layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">        super(ImdbModel,self).build(input_shape)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x = self.block1(x)</span><br><span class="line">        x = self.block2(x)</span><br><span class="line">        x = layers.Flatten()(x)</span><br><span class="line">        x = self.dense(x)</span><br><span class="line">        <span class="keyword">return</span>(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = ImdbModel()</span><br><span class="line">model.build(input_shape =(<span class="literal">None</span>,<span class="number">200</span>))</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'Nadam'</span>,</span><br><span class="line">            loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>,<span class="string">"AUC"</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Model: "imdb_model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        multiple                  70000     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">res_block (ResBlock)         multiple                  19143     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">res<span class="emphasis">_block_</span>1 (ResBlock)       multiple                  13703     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                multiple                  351       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 103,197</span><br><span class="line">Trainable params: 103,197</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/Model%E5%AD%90%E7%B1%BB%E5%8C%96%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line">logdir = <span class="string">"./tflogs/keras_model/"</span> + datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=<span class="number">1</span>)</span><br><span class="line">history = model.fit(ds_train,validation_data = ds_test,</span><br><span class="line">                    epochs = <span class="number">6</span>,callbacks=[tensorboard_callback])</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">1</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">47</span>s <span class="number">47</span>ms/step - loss: <span class="number">0.5629</span> - accuracy: <span class="number">0.6618</span> - AUC: <span class="number">0.7548</span> - val_loss: <span class="number">0.3422</span> - val_accuracy: <span class="number">0.8510</span> - val_AUC: <span class="number">0.9286</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">43</span>s <span class="number">43</span>ms/step - loss: <span class="number">0.2648</span> - accuracy: <span class="number">0.8903</span> - AUC: <span class="number">0.9576</span> - val_loss: <span class="number">0.3276</span> - val_accuracy: <span class="number">0.8650</span> - val_AUC: <span class="number">0.9410</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">42</span>s <span class="number">42</span>ms/step - loss: <span class="number">0.1573</span> - accuracy: <span class="number">0.9439</span> - AUC: <span class="number">0.9846</span> - val_loss: <span class="number">0.3861</span> - val_accuracy: <span class="number">0.8682</span> - val_AUC: <span class="number">0.9390</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">42</span>s <span class="number">42</span>ms/step - loss: <span class="number">0.0849</span> - accuracy: <span class="number">0.9706</span> - AUC: <span class="number">0.9950</span> - val_loss: <span class="number">0.5324</span> - val_accuracy: <span class="number">0.8616</span> - val_AUC: <span class="number">0.9292</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">43</span>s <span class="number">43</span>ms/step - loss: <span class="number">0.0393</span> - accuracy: <span class="number">0.9876</span> - AUC: <span class="number">0.9986</span> - val_loss: <span class="number">0.7693</span> - val_accuracy: <span class="number">0.8566</span> - val_AUC: <span class="number">0.9132</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">44</span>s <span class="number">44</span>ms/step - loss: <span class="number">0.0222</span> - accuracy: <span class="number">0.9926</span> - AUC: <span class="number">0.9994</span> - val_loss: <span class="number">0.9328</span> - val_accuracy: <span class="number">0.8584</span> - val_AUC: <span class="number">0.9052</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"AUC"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/6-1-3-fit%E6%A8%A1%E5%9E%8B.jpg" alt></p>
<h2 id="xun-lian-mo-xing-de-3-chong-fang-fa">训练模型的3种方法</h2>
<p>模型的训练主要有：</p>
<ol>
<li>内置fit方法</li>
<li>内置tran_on_batch方法</li>
<li>自定义训练循环。</li>
</ol>
<p>注：fit_generator方法在tf.keras中不推荐使用，其功能已经被fit包含。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> * </span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = <span class="number">300</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()</span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)</span><br><span class="line">x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)</span><br><span class="line"></span><br><span class="line">MAX_WORDS = x_train.max()+<span class="number">1</span></span><br><span class="line">CAT_NUM = y_train.max()+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">   </span><br><span class="line">ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<h3 id="nei-zhi-fit-fang-fa">内置fit方法</h3>
<p>该方法功能非常强大, 支持对numpy array, tf.data.Dataset以及 Python generator数据进行训练。</p>
<p>并且可以通过设置回调函数实现对训练过程的复杂控制逻辑。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    model = models.Sequential()</span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compile_model</span><span class="params">(model)</span>:</span></span><br><span class="line">    model.compile(optimizer=optimizers.Nadam(),</span><br><span class="line">                loss=losses.SparseCategoricalCrossentropy(),</span><br><span class="line">                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(<span class="number">5</span>)]) </span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"> </span><br><span class="line">model = create_model()</span><br><span class="line">model.summary()</span><br><span class="line">model = compile_model(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (None, 300, 7)            216874    </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d (Conv1D)              (None, 296, 64)           2304      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max_pooling1d (MaxPooling1D) (None, 148, 64)           0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d_1 (Conv1D)            (None, 146, 32)           6176      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max<span class="emphasis">_pooling1d_</span>1 (MaxPooling1 (None, 73, 32)            0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">flatten (Flatten)            (None, 2336)              0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 46)                107502    </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 332,856</span><br><span class="line">Trainable params: 332,856</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(ds_train,validation_data = ds_test,epochs = <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Train <span class="keyword">for</span> <span class="number">281</span> steps, validate <span class="keyword">for</span> <span class="number">71</span> steps</span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">11</span>s <span class="number">37</span>ms/step - loss: <span class="number">2.0231</span> - sparse_categorical_accuracy: <span class="number">0.4636</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7450</span> - val_loss: <span class="number">1.7346</span> - val_sparse_categorical_accuracy: <span class="number">0.5534</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7560</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">9</span>s <span class="number">31</span>ms/step - loss: <span class="number">1.5079</span> - sparse_categorical_accuracy: <span class="number">0.6091</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7901</span> - val_loss: <span class="number">1.5475</span> - val_sparse_categorical_accuracy: <span class="number">0.6109</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7792</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">9</span>s <span class="number">33</span>ms/step - loss: <span class="number">1.2204</span> - sparse_categorical_accuracy: <span class="number">0.6823</span> - sparse_top_k_categorical_accuracy: <span class="number">0.8448</span> - val_loss: <span class="number">1.5455</span> - val_sparse_categorical_accuracy: <span class="number">0.6367</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8001</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">9</span>s <span class="number">33</span>ms/step - loss: <span class="number">0.9382</span> - sparse_categorical_accuracy: <span class="number">0.7543</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9075</span> - val_loss: <span class="number">1.6780</span> - val_sparse_categorical_accuracy: <span class="number">0.6398</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8032</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">10</span>s <span class="number">34</span>ms/step - loss: <span class="number">0.6791</span> - sparse_categorical_accuracy: <span class="number">0.8255</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9513</span> - val_loss: <span class="number">1.9426</span> - val_sparse_categorical_accuracy: <span class="number">0.6376</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7956</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">9</span>s <span class="number">33</span>ms/step - loss: <span class="number">0.5063</span> - sparse_categorical_accuracy: <span class="number">0.8762</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9716</span> - val_loss: <span class="number">2.2141</span> - val_sparse_categorical_accuracy: <span class="number">0.6291</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7947</span></span><br><span class="line">Epoch <span class="number">7</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">10</span>s <span class="number">37</span>ms/step - loss: <span class="number">0.4031</span> - sparse_categorical_accuracy: <span class="number">0.9050</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9817</span> - val_loss: <span class="number">2.4126</span> - val_sparse_categorical_accuracy: <span class="number">0.6264</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7947</span></span><br><span class="line">Epoch <span class="number">8</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">10</span>s <span class="number">35</span>ms/step - loss: <span class="number">0.3380</span> - sparse_categorical_accuracy: <span class="number">0.9205</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9881</span> - val_loss: <span class="number">2.5366</span> - val_sparse_categorical_accuracy: <span class="number">0.6242</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7974</span></span><br><span class="line">Epoch <span class="number">9</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">10</span>s <span class="number">36</span>ms/step - loss: <span class="number">0.2921</span> - sparse_categorical_accuracy: <span class="number">0.9299</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9909</span> - val_loss: <span class="number">2.6564</span> - val_sparse_categorical_accuracy: <span class="number">0.6242</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7983</span></span><br><span class="line">Epoch <span class="number">10</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">9</span>s <span class="number">30</span>ms/step - loss: <span class="number">0.2613</span> - sparse_categorical_accuracy: <span class="number">0.9334</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9947</span> - val_loss: <span class="number">2.7365</span> - val_sparse_categorical_accuracy: <span class="number">0.6220</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8005</span></span><br></pre></td></tr></table></figure>
<h3 id="nei-zhi-train-on-batch-fang-fa">内置train_on_batch方法</h3>
<p>该内置方法相比较fit方法更加灵活，可以不通过回调函数而直接在批次层次上更加精细地控制训练的过程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = models.Sequential()</span><br><span class="line"></span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compile_model</span><span class="params">(model)</span>:</span></span><br><span class="line">    model.compile(optimizer=optimizers.Nadam(),</span><br><span class="line">                loss=losses.SparseCategoricalCrossentropy(),</span><br><span class="line">                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(<span class="number">5</span>)]) </span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"> </span><br><span class="line">model = create_model()</span><br><span class="line">model.summary()</span><br><span class="line">model = compile_model(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (None, 300, 7)            216874    </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d (Conv1D)              (None, 296, 64)           2304      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max_pooling1d (MaxPooling1D) (None, 148, 64)           0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d_1 (Conv1D)            (None, 146, 32)           6176      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max<span class="emphasis">_pooling1d_</span>1 (MaxPooling1 (None, 73, 32)            0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">flatten (Flatten)            (None, 2336)              0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 46)                107502    </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 332,856</span><br><span class="line">Trainable params: 332,856</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,ds_train,ds_valid,epoches)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epoches+<span class="number">1</span>):</span><br><span class="line">        model.reset_metrics()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 在后期降低学习率</span></span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">5</span>:</span><br><span class="line">            model.optimizer.lr.assign(model.optimizer.lr/<span class="number">2.0</span>)</span><br><span class="line">            tf.print(<span class="string">"Lowering optimizer Learning Rate...\n\n"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> ds_train:</span><br><span class="line">            train_result = model.train_on_batch(x, y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> ds_valid:</span><br><span class="line">            valid_result = model.test_on_batch(x, y,reset_metrics=<span class="literal">False</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">1</span> ==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch = "</span>,epoch)</span><br><span class="line">            print(<span class="string">"train:"</span>,dict(zip(model.metrics_names,train_result)))</span><br><span class="line">            print(<span class="string">"valid:"</span>,dict(zip(model.metrics_names,valid_result)))</span><br><span class="line">            print(<span class="string">""</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_model(model,ds_train,ds_test,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">================================================================================13:09:19</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">1</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.82411176</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.77272725</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.8636364</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">1.9265995</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.5743544</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.75779164</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:09:27</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">2</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.6006621</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.90909094</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.95454544</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">1.844159</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6126447</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.7920748</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:09:35</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">3</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.36935613</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.90909094</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.95454544</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">2.163433</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.63312554</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.8045414</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:09:42</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">4</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.2304088</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.90909094</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">2.8911984</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6344613</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.7978629</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">Lowering</span> <span class="string">optimizer</span> <span class="string">Learning</span> <span class="string">Rate...</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:09:51</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">5</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.111194365</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.95454544</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">3.6431572</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6295637</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.7978629</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:09:59</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">6</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.07741702</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.95454544</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">4.074161</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6255565</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.794301</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:10:07</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">7</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.056113098</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">4.4461513</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6273375</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.79652715</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:10:17</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">8</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.043448802</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">4.7687583</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6224399</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.79741764</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:10:26</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">9</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.035002146</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">5.130505</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6175423</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.794301</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:10:34</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">10</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.028303564</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">5.4559293</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6148709</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.7947462</span><span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="zi-ding-yi-xun-lian-xun-huan">自定义训练循环</h3>
<p>自定义训练循环无需编译模型，直接利用优化器根据损失函数反向传播迭代参数，拥有最高的灵活性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    model = models.Sequential()</span><br><span class="line"></span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line">model = create_model()</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optimizers.Nadam()</span><br><span class="line">loss_func = losses.SparseCategoricalCrossentropy()</span><br><span class="line"></span><br><span class="line">train_loss = metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_metric = metrics.SparseCategoricalAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">valid_loss = metrics.Mean(name=<span class="string">'valid_loss'</span>)</span><br><span class="line">valid_metric = metrics.SparseCategoricalAccuracy(name=<span class="string">'valid_accuracy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features,training = <span class="literal">True</span>)</span><br><span class="line">        loss = loss_func(labels, predictions)</span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss.update_state(loss)</span><br><span class="line">    train_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">valid_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    predictions = model(features)</span><br><span class="line">    batch_loss = loss_func(labels, predictions)</span><br><span class="line">    valid_loss.update_state(batch_loss)</span><br><span class="line">    valid_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,ds_train,ds_valid,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_train:</span><br><span class="line">            train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_valid:</span><br><span class="line">            valid_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        logs = <span class="string">'Epoch=&#123;&#125;,Loss:&#123;&#125;,Accuracy:&#123;&#125;,Valid Loss:&#123;&#125;,Valid Accuracy:&#123;&#125;'</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">1</span> ==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(tf.strings.format(logs,</span><br><span class="line">            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))</span><br><span class="line">            tf.print(<span class="string">""</span>)</span><br><span class="line">            </span><br><span class="line">        train_loss.reset_states()</span><br><span class="line">        valid_loss.reset_states()</span><br><span class="line">        train_metric.reset_states()</span><br><span class="line">        valid_metric.reset_states()</span><br><span class="line"></span><br><span class="line">train_model(model,ds_train,ds_test,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">03</span></span><br><span class="line">Epoch=<span class="number">1</span>,Loss:<span class="number">2.02051544</span>,Accuracy:<span class="number">0.460253835</span>,Valid Loss:<span class="number">1.75700927</span>,Valid Accuracy:<span class="number">0.536954582</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">09</span></span><br><span class="line">Epoch=<span class="number">2</span>,Loss:<span class="number">1.510795</span>,Accuracy:<span class="number">0.610665798</span>,Valid Loss:<span class="number">1.55349839</span>,Valid Accuracy:<span class="number">0.616206586</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">17</span></span><br><span class="line">Epoch=<span class="number">3</span>,Loss:<span class="number">1.19221532</span>,Accuracy:<span class="number">0.696170092</span>,Valid Loss:<span class="number">1.52315605</span>,Valid Accuracy:<span class="number">0.651380241</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">23</span></span><br><span class="line">Epoch=<span class="number">4</span>,Loss:<span class="number">0.90101546</span>,Accuracy:<span class="number">0.766310394</span>,Valid Loss:<span class="number">1.68327653</span>,Valid Accuracy:<span class="number">0.648263574</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">30</span></span><br><span class="line">Epoch=<span class="number">5</span>,Loss:<span class="number">0.655430496</span>,Accuracy:<span class="number">0.831329346</span>,Valid Loss:<span class="number">1.90872383</span>,Valid Accuracy:<span class="number">0.641139805</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">37</span></span><br><span class="line">Epoch=<span class="number">6</span>,Loss:<span class="number">0.492730737</span>,Accuracy:<span class="number">0.877866864</span>,Valid Loss:<span class="number">2.09966016</span>,Valid Accuracy:<span class="number">0.63223511</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">44</span></span><br><span class="line">Epoch=<span class="number">7</span>,Loss:<span class="number">0.391238362</span>,Accuracy:<span class="number">0.904030263</span>,Valid Loss:<span class="number">2.27431226</span>,Valid Accuracy:<span class="number">0.625111282</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">51</span></span><br><span class="line">Epoch=<span class="number">8</span>,Loss:<span class="number">0.327761739</span>,Accuracy:<span class="number">0.922066331</span>,Valid Loss:<span class="number">2.42568827</span>,Valid Accuracy:<span class="number">0.617542326</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">58</span></span><br><span class="line">Epoch=<span class="number">9</span>,Loss:<span class="number">0.285573095</span>,Accuracy:<span class="number">0.930527747</span>,Valid Loss:<span class="number">2.55942106</span>,Valid Accuracy:<span class="number">0.612644672</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">13</span>:<span class="number">05</span></span><br><span class="line">Epoch=<span class="number">10</span>,Loss:<span class="number">0.255482465</span>,Accuracy:<span class="number">0.936094403</span>,Valid Loss:<span class="number">2.67789412</span>,Valid Accuracy:<span class="number">0.612199485</span></span><br></pre></td></tr></table></figure>
<h2 id="shi-yong-dan-gpu-xun-lian-mo-xing">使用单GPU训练模型</h2>
<p>深度学习的训练过程常常非常耗时，一个模型训练几个小时是家常便饭，训练几天也是常有的事情，有时候甚至要训练几十天。训练过程的耗时主要来自于两个部分，一部分来自数据准备，另一部分来自参数迭代。</p>
<p>当数据准备过程还是模型训练时间的主要瓶颈时，我们可以使用更多进程来准备数据。当参数迭代过程成为训练时间的主要瓶颈时，我们通常的方法是应用GPU或者Google的TPU来进行加速。</p>
<p>无论是内置fit方法，还是自定义训练循环，从CPU切换成单GPU训练模型都是非常方便的，无需更改任何代码。当存在可用的GPU时，如果不特意指定device，tensorflow会自动优先选择使用GPU来创建张量和执行张量计算。</p>
<p>但如果是在公司或者学校实验室的服务器环境，存在多个GPU和多个使用者时，为了不让单个同学的任务占用全部GPU资源导致其他同学无法使用（tensorflow默认获取全部GPU的全部内存资源权限，但实际上只使用一个GPU的部分资源），通常会在开头增加以下几行代码以控制每个任务使用的GPU编号和显存大小，以便其他同学也能够同时训练模型。在Colab笔记本中：修改-&gt;笔记本设置-&gt;硬件加速器 中选择 GPU</p>
<p>可通过以下colab链接测试效果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%tensorflow_version <span class="number">2.</span>x</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.__version__)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> * </span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure>
<h3 id="gpu-she-zhi">GPU设置</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">gpus = tf.config.list_physical_devices(<span class="string">"GPU"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">    gpu0 = gpus[<span class="number">0</span>] <span class="comment">#如果有多个GPU，仅使用第0个GPU</span></span><br><span class="line">    tf.config.experimental.set_memory_growth(gpu0, <span class="literal">True</span>) <span class="comment">#设置GPU显存用量按需使用</span></span><br><span class="line">    <span class="comment"># 或者也可以设置GPU显存为固定使用量(例如：4G)</span></span><br><span class="line">    <span class="comment">#tf.config.experimental.set_virtual_device_configuration(gpu0,</span></span><br><span class="line">    <span class="comment">#    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) </span></span><br><span class="line">    tf.config.set_visible_devices([gpu0],<span class="string">"GPU"</span>)</span><br></pre></td></tr></table></figure>
<p>比较GPU和CPU的计算速度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">printbar()</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">"/gpu:0"</span>):</span><br><span class="line">    tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line">    a = tf.random.uniform((<span class="number">10000</span>,<span class="number">100</span>),minval = <span class="number">0</span>,maxval = <span class="number">3.0</span>)</span><br><span class="line">    b = tf.random.uniform((<span class="number">100</span>,<span class="number">100000</span>),minval = <span class="number">0</span>,maxval = <span class="number">3.0</span>)</span><br><span class="line">    c = a@b</span><br><span class="line">    tf.print(tf.reduce_sum(tf.reduce_sum(c,axis = <span class="number">0</span>),axis=<span class="number">0</span>))</span><br><span class="line">printbar()</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">01</span></span><br><span class="line"><span class="number">2.24953778e+11</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">01</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">printbar()</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">"/cpu:0"</span>):</span><br><span class="line">    tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line">    a = tf.random.uniform((<span class="number">10000</span>,<span class="number">100</span>),minval = <span class="number">0</span>,maxval = <span class="number">3.0</span>)</span><br><span class="line">    b = tf.random.uniform((<span class="number">100</span>,<span class="number">100000</span>),minval = <span class="number">0</span>,maxval = <span class="number">3.0</span>)</span><br><span class="line">    c = a@b</span><br><span class="line">    tf.print(tf.reduce_sum(tf.reduce_sum(c,axis = <span class="number">0</span>),axis=<span class="number">0</span>))</span><br><span class="line">printbar()</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">34</span></span><br><span class="line"><span class="number">2.24953795e+11</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">40</span></span><br></pre></td></tr></table></figure>
<h3 id="zhun-bei-shu-ju-10">准备数据</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = <span class="number">300</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()</span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)</span><br><span class="line">x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)</span><br><span class="line"></span><br><span class="line">MAX_WORDS = x_train.max()+<span class="number">1</span></span><br><span class="line">CAT_NUM = y_train.max()+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">   </span><br><span class="line">ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing-11">定义模型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    model = models.Sequential()</span><br><span class="line"></span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line">model = create_model()</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (None, 300, 7)            216874    </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d (Conv1D)              (None, 296, 64)           2304      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max_pooling1d (MaxPooling1D) (None, 148, 64)           0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d_1 (Conv1D)            (None, 146, 32)           6176      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max<span class="emphasis">_pooling1d_</span>1 (MaxPooling1 (None, 73, 32)            0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">flatten (Flatten)            (None, 2336)              0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 46)                107502    </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 332,856</span><br><span class="line">Trainable params: 332,856</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure>
<h3 id="xun-lian-mo-xing-10">训练模型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optimizers.Nadam()</span><br><span class="line">loss_func = losses.SparseCategoricalCrossentropy()</span><br><span class="line"></span><br><span class="line">train_loss = metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_metric = metrics.SparseCategoricalAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">valid_loss = metrics.Mean(name=<span class="string">'valid_loss'</span>)</span><br><span class="line">valid_metric = metrics.SparseCategoricalAccuracy(name=<span class="string">'valid_accuracy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features,training = <span class="literal">True</span>)</span><br><span class="line">        loss = loss_func(labels, predictions)</span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss.update_state(loss)</span><br><span class="line">    train_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">valid_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    predictions = model(features)</span><br><span class="line">    batch_loss = loss_func(labels, predictions)</span><br><span class="line">    valid_loss.update_state(batch_loss)</span><br><span class="line">    valid_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,ds_train,ds_valid,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_train:</span><br><span class="line">            train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_valid:</span><br><span class="line">            valid_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        logs = <span class="string">'Epoch=&#123;&#125;,Loss:&#123;&#125;,Accuracy:&#123;&#125;,Valid Loss:&#123;&#125;,Valid Accuracy:&#123;&#125;'</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">1</span> ==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(tf.strings.format(logs,</span><br><span class="line">            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))</span><br><span class="line">            tf.print(<span class="string">""</span>)</span><br><span class="line">            </span><br><span class="line">        train_loss.reset_states()</span><br><span class="line">        valid_loss.reset_states()</span><br><span class="line">        train_metric.reset_states()</span><br><span class="line">        valid_metric.reset_states()</span><br><span class="line"></span><br><span class="line">train_model(model,ds_train,ds_test,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">26</span></span><br><span class="line">Epoch=<span class="number">1</span>,Loss:<span class="number">1.96735072</span>,Accuracy:<span class="number">0.489200622</span>,Valid Loss:<span class="number">1.64124215</span>,Valid Accuracy:<span class="number">0.582813919</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">28</span></span><br><span class="line">Epoch=<span class="number">2</span>,Loss:<span class="number">1.4640888</span>,Accuracy:<span class="number">0.624805152</span>,Valid Loss:<span class="number">1.5559175</span>,Valid Accuracy:<span class="number">0.607747078</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">30</span></span><br><span class="line">Epoch=<span class="number">3</span>,Loss:<span class="number">1.20681274</span>,Accuracy:<span class="number">0.68581605</span>,Valid Loss:<span class="number">1.58494771</span>,Valid Accuracy:<span class="number">0.622439921</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">31</span></span><br><span class="line">Epoch=<span class="number">4</span>,Loss:<span class="number">0.937500894</span>,Accuracy:<span class="number">0.75361836</span>,Valid Loss:<span class="number">1.77466083</span>,Valid Accuracy:<span class="number">0.621994674</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">33</span></span><br><span class="line">Epoch=<span class="number">5</span>,Loss:<span class="number">0.693960547</span>,Accuracy:<span class="number">0.822199941</span>,Valid Loss:<span class="number">2.00267363</span>,Valid Accuracy:<span class="number">0.6197685</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">35</span></span><br><span class="line">Epoch=<span class="number">6</span>,Loss:<span class="number">0.519614</span>,Accuracy:<span class="number">0.870296121</span>,Valid Loss:<span class="number">2.23463202</span>,Valid Accuracy:<span class="number">0.613980412</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">37</span></span><br><span class="line">Epoch=<span class="number">7</span>,Loss:<span class="number">0.408562034</span>,Accuracy:<span class="number">0.901246965</span>,Valid Loss:<span class="number">2.46969271</span>,Valid Accuracy:<span class="number">0.612199485</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">39</span></span><br><span class="line">Epoch=<span class="number">8</span>,Loss:<span class="number">0.339028627</span>,Accuracy:<span class="number">0.920062363</span>,Valid Loss:<span class="number">2.68585229</span>,Valid Accuracy:<span class="number">0.615316093</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">41</span></span><br><span class="line">Epoch=<span class="number">9</span>,Loss:<span class="number">0.293798745</span>,Accuracy:<span class="number">0.92930305</span>,Valid Loss:<span class="number">2.88995624</span>,Valid Accuracy:<span class="number">0.613535166</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">43</span></span><br><span class="line">Epoch=<span class="number">10</span>,Loss:<span class="number">0.263130337</span>,Accuracy:<span class="number">0.936651051</span>,Valid Loss:<span class="number">3.09705234</span>,Valid Accuracy:<span class="number">0.612644672</span></span><br></pre></td></tr></table></figure>
<h2 id="shi-yong-duo-gpu-xun-lian-mo-xing">使用多GPU训练模型</h2>
<p>如果使用多GPU训练模型，推荐使用内置fit方法，较为方便，仅需添加2行代码。</p>
<p>在Colab笔记本中：修改-&gt;笔记本设置-&gt;硬件加速器 中选择 GPU</p>
<p>MirroredStrategy过程简介：</p>
<ul>
<li>训练开始前，该策略在所有 N 个计算设备上均各复制一份完整的模型；</li>
<li>每次训练传入一个批次的数据时，将数据分成 N 份，分别传入 N 个计算设备（即数据并行）；</li>
<li>N 个计算设备使用本地变量（镜像变量）分别计算自己所获得的部分数据的梯度；</li>
<li>使用分布式计算的 All-reduce 操作，在计算设备间高效交换梯度数据并进行求和，使得最终每个设备都有了所有设备的梯度之和；</li>
<li>使用梯度求和的结果更新本地变量（镜像变量）；</li>
<li>当所有设备均更新本地变量后，进行下一轮训练（即该并行策略是同步的）。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%tensorflow_version <span class="number">2.</span>x</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.__version__)</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#此处在colab上使用1个GPU模拟出两个逻辑GPU进行多GPU训练</span></span><br><span class="line">gpus = tf.config.experimental.list_physical_devices(<span class="string">'GPU'</span>)</span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">    <span class="comment"># 设置两个逻辑GPU模拟多GPU训练</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        tf.config.experimental.set_virtual_device_configuration(gpus[<span class="number">0</span>],</span><br><span class="line">            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=<span class="number">1024</span>),</span><br><span class="line">             tf.config.experimental.VirtualDeviceConfiguration(memory_limit=<span class="number">1024</span>)])</span><br><span class="line">        logical_gpus = tf.config.experimental.list_logical_devices(<span class="string">'GPU'</span>)</span><br><span class="line">        print(len(gpus), <span class="string">"Physical GPU,"</span>, len(logical_gpus), <span class="string">"Logical GPUs"</span>)</span><br><span class="line">    <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br></pre></td></tr></table></figure>
<h3 id="zhun-bei-shu-ju-11">准备数据</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = <span class="number">300</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()</span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)</span><br><span class="line">x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)</span><br><span class="line"></span><br><span class="line">MAX_WORDS = x_train.max()+<span class="number">1</span></span><br><span class="line">CAT_NUM = y_train.max()+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">   </span><br><span class="line">ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing-12">定义模型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    model = models.Sequential()</span><br><span class="line"></span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compile_model</span><span class="params">(model)</span>:</span></span><br><span class="line">    model.compile(optimizer=optimizers.Nadam(),</span><br><span class="line">                loss=losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(<span class="number">5</span>)]) </span><br><span class="line">    <span class="keyword">return</span>(model)</span><br></pre></td></tr></table></figure>
<h3 id="xun-lian-mo-xing-11">训练模型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#增加以下两行代码</span></span><br><span class="line">strategy = tf.distribute.MirroredStrategy()  </span><br><span class="line"><span class="keyword">with</span> strategy.scope(): </span><br><span class="line">    model = create_model()</span><br><span class="line">    model.summary()</span><br><span class="line">    model = compile_model(model)</span><br><span class="line">    </span><br><span class="line">history = model.fit(ds_train,validation_data = ds_test,epochs = <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">WARNING</span>:tensorflow:NCCL <span class="keyword">is</span> <span class="keyword">not</span> supported <span class="keyword">when</span> <span class="keyword">using</span> virtual GPUs, fallingback <span class="keyword">to</span> reduction <span class="keyword">to</span> one device</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:<span class="keyword">Using</span> MirroredStrategy <span class="keyword">with</span> devices (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>)</span><br><span class="line">Model: "sequential"</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="keyword">type</span>)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (<span class="keyword">None</span>, <span class="number">300</span>, <span class="number">7</span>)            <span class="number">216874</span>    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv1d (Conv1D)              (<span class="keyword">None</span>, <span class="number">296</span>, <span class="number">64</span>)           <span class="number">2304</span>      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling1d (MaxPooling1D) (<span class="keyword">None</span>, <span class="number">148</span>, <span class="number">64</span>)           <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv1d_1 (Conv1D)            (<span class="keyword">None</span>, <span class="number">146</span>, <span class="number">32</span>)           <span class="number">6176</span>      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling1d_1 (MaxPooling1 (<span class="keyword">None</span>, <span class="number">73</span>, <span class="number">32</span>)            <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten (Flatten)            (<span class="keyword">None</span>, <span class="number">2336</span>)              <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                (<span class="keyword">None</span>, <span class="number">46</span>)                <span class="number">107502</span>    </span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">332</span>,<span class="number">856</span></span><br><span class="line">Trainable params: <span class="number">332</span>,<span class="number">856</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line">Train <span class="keyword">for</span> <span class="number">281</span> steps, <span class="keyword">validate</span> <span class="keyword">for</span> <span class="number">71</span> steps</span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">10</span></span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">15</span>s <span class="number">53</span>ms/step - loss: <span class="number">2.0270</span> - sparse_categorical_accuracy: <span class="number">0.4653</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7481</span> - val_loss: <span class="number">1.7517</span> - val_sparse_categorical_accuracy: <span class="number">0.5481</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7578</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">1.5206</span> - sparse_categorical_accuracy: <span class="number">0.6045</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7938</span> - val_loss: <span class="number">1.5715</span> - val_sparse_categorical_accuracy: <span class="number">0.5993</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7983</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">1.2178</span> - sparse_categorical_accuracy: <span class="number">0.6843</span> - sparse_top_k_categorical_accuracy: <span class="number">0.8547</span> - val_loss: <span class="number">1.5232</span> - val_sparse_categorical_accuracy: <span class="number">0.6327</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8112</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">13</span>ms/step - loss: <span class="number">0.9127</span> - sparse_categorical_accuracy: <span class="number">0.7648</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9113</span> - val_loss: <span class="number">1.6527</span> - val_sparse_categorical_accuracy: <span class="number">0.6296</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8201</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.6606</span> - sparse_categorical_accuracy: <span class="number">0.8321</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9525</span> - val_loss: <span class="number">1.8791</span> - val_sparse_categorical_accuracy: <span class="number">0.6158</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8219</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.4919</span> - sparse_categorical_accuracy: <span class="number">0.8799</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9725</span> - val_loss: <span class="number">2.1282</span> - val_sparse_categorical_accuracy: <span class="number">0.6037</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8112</span></span><br><span class="line">Epoch <span class="number">7</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.3947</span> - sparse_categorical_accuracy: <span class="number">0.9051</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9814</span> - val_loss: <span class="number">2.3033</span> - val_sparse_categorical_accuracy: <span class="number">0.6046</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8094</span></span><br><span class="line">Epoch <span class="number">8</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.3335</span> - sparse_categorical_accuracy: <span class="number">0.9207</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9863</span> - val_loss: <span class="number">2.4255</span> - val_sparse_categorical_accuracy: <span class="number">0.5993</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8099</span></span><br><span class="line">Epoch <span class="number">9</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.2919</span> - sparse_categorical_accuracy: <span class="number">0.9304</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9911</span> - val_loss: <span class="number">2.5571</span> - val_sparse_categorical_accuracy: <span class="number">0.6020</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8126</span></span><br><span class="line">Epoch <span class="number">10</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.2617</span> - sparse_categorical_accuracy: <span class="number">0.9342</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9937</span> - val_loss: <span class="number">2.6700</span> - val_sparse_categorical_accuracy: <span class="number">0.6077</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8148</span></span><br><span class="line">CPU times: <span class="keyword">user</span> <span class="number">1</span>min <span class="number">2</span>s, sys: <span class="number">8.59</span> s, total: <span class="number">1</span>min <span class="number">10</span>s</span><br><span class="line">Wall <span class="type">time</span>: <span class="number">58.5</span> s</span><br></pre></td></tr></table></figure>
<h2 id="shi-yong-tpu-xun-lian-mo-xing">使用TPU训练模型</h2>
<p>如果想尝试使用Google Colab上的TPU来训练模型，也是非常方便，仅需添加6行代码。在Colab笔记本中：修改-&gt;笔记本设置-&gt;硬件加速器中选择 TPU</p>
<p>可通过以下colab链接测试效果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%tensorflow_version <span class="number">2.</span>x</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.__version__)</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<h3 id="zhun-bei-shu-ju-12">准备数据</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = <span class="number">300</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()</span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)</span><br><span class="line">x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)</span><br><span class="line"></span><br><span class="line">MAX_WORDS = x_train.max()+<span class="number">1</span></span><br><span class="line">CAT_NUM = y_train.max()+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">   </span><br><span class="line">ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure>
<h3 id="ding-yi-mo-xing-13">定义模型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    model = models.Sequential()</span><br><span class="line"></span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compile_model</span><span class="params">(model)</span>:</span></span><br><span class="line">    model.compile(optimizer=optimizers.Nadam(),</span><br><span class="line">                loss=losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(<span class="number">5</span>)]) </span><br><span class="line">    <span class="keyword">return</span>(model)</span><br></pre></td></tr></table></figure>
<h3 id="xun-lian-mo-xing-12">训练模型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#增加以下6行代码</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=<span class="string">'grpc://'</span> + os.environ[<span class="string">'COLAB_TPU_ADDR'</span>])</span><br><span class="line">tf.config.experimental_connect_to_cluster(resolver)</span><br><span class="line">tf.tpu.experimental.initialize_tpu_system(resolver)</span><br><span class="line">strategy = tf.distribute.experimental.TPUStrategy(resolver)</span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">    model = create_model()</span><br><span class="line">    model.summary()</span><br><span class="line">    model = compile_model(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">WARNING:tensorflow:TPU system <span class="number">10.26</span><span class="number">.134</span><span class="number">.242</span>:<span class="number">8470</span> has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.</span><br><span class="line">WARNING:tensorflow:TPU system <span class="number">10.26</span><span class="number">.134</span><span class="number">.242</span>:<span class="number">8470</span> has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.</span><br><span class="line">INFO:tensorflow:Initializing the TPU system: <span class="number">10.26</span><span class="number">.134</span><span class="number">.242</span>:<span class="number">8470</span></span><br><span class="line">INFO:tensorflow:Initializing the TPU system: <span class="number">10.26</span><span class="number">.134</span><span class="number">.242</span>:<span class="number">8470</span></span><br><span class="line">INFO:tensorflow:Clearing <span class="keyword">out</span> eager caches</span><br><span class="line">INFO:tensorflow:Clearing <span class="keyword">out</span> eager caches</span><br><span class="line">INFO:tensorflow:Finished initializing TPU system.</span><br><span class="line">INFO:tensorflow:Finished initializing TPU system.</span><br><span class="line">INFO:tensorflow:Found TPU system:</span><br><span class="line">INFO:tensorflow:Found TPU system:</span><br><span class="line">INFO:tensorflow:*** Num TPU Cores: <span class="number">8</span></span><br><span class="line">INFO:tensorflow:*** Num TPU Cores: <span class="number">8</span></span><br><span class="line">INFO:tensorflow:*** Num TPU Workers: <span class="number">1</span></span><br><span class="line">INFO:tensorflow:*** Num TPU Workers: <span class="number">1</span></span><br><span class="line">INFO:tensorflow:*** Num TPU Cores Per Worker: <span class="number">8</span></span><br><span class="line">INFO:tensorflow:*** Num TPU Cores Per Worker: <span class="number">8</span></span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span>, CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span>, CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:XLA_CPU:<span class="number">0</span>, XLA_CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:XLA_CPU:<span class="number">0</span>, XLA_CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span>, CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span>, CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">0</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">0</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">1</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">1</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">2</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">2</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">3</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">3</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">4</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">4</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">5</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">5</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">6</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">6</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">7</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">7</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU_SYSTEM:<span class="number">0</span>, TPU_SYSTEM, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU_SYSTEM:<span class="number">0</span>, TPU_SYSTEM, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:XLA_CPU:<span class="number">0</span>, XLA_CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:XLA_CPU:<span class="number">0</span>, XLA_CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">Model: <span class="string">"sequential"</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (None, <span class="number">300</span>, <span class="number">7</span>)            <span class="number">216874</span>    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv1d (Conv1D)              (None, <span class="number">296</span>, <span class="number">64</span>)           <span class="number">2304</span>      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling1d (MaxPooling1D) (None, <span class="number">148</span>, <span class="number">64</span>)           <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv1d_1 (Conv1D)            (None, <span class="number">146</span>, <span class="number">32</span>)           <span class="number">6176</span>      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling1d_1 (MaxPooling1 (None, <span class="number">73</span>, <span class="number">32</span>)            <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten (Flatten)            (None, <span class="number">2336</span>)              <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                (None, <span class="number">46</span>)                <span class="number">107502</span>    </span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">332</span>,<span class="number">856</span></span><br><span class="line">Trainable params: <span class="number">332</span>,<span class="number">856</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(ds_train,validation_data = ds_test,epochs = <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Train <span class="keyword">for</span> <span class="number">281</span> steps, validate <span class="keyword">for</span> <span class="number">71</span> steps</span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">12</span>s <span class="number">43</span>ms/step - loss: <span class="number">3.4466</span> - sparse_categorical_accuracy: <span class="number">0.4332</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7180</span> - val_loss: <span class="number">3.3179</span> - val_sparse_categorical_accuracy: <span class="number">0.5352</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7195</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">6</span>s <span class="number">20</span>ms/step - loss: <span class="number">3.3251</span> - sparse_categorical_accuracy: <span class="number">0.5405</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7302</span> - val_loss: <span class="number">3.3082</span> - val_sparse_categorical_accuracy: <span class="number">0.5463</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7235</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">6</span>s <span class="number">20</span>ms/step - loss: <span class="number">3.2961</span> - sparse_categorical_accuracy: <span class="number">0.5729</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7280</span> - val_loss: <span class="number">3.3026</span> - val_sparse_categorical_accuracy: <span class="number">0.5499</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7217</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">5</span>s <span class="number">19</span>ms/step - loss: <span class="number">3.2751</span> - sparse_categorical_accuracy: <span class="number">0.5924</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7276</span> - val_loss: <span class="number">3.2957</span> - val_sparse_categorical_accuracy: <span class="number">0.5543</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7217</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">5</span>s <span class="number">19</span>ms/step - loss: <span class="number">3.2655</span> - sparse_categorical_accuracy: <span class="number">0.6008</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7290</span> - val_loss: <span class="number">3.3022</span> - val_sparse_categorical_accuracy: <span class="number">0.5490</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7231</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">5</span>s <span class="number">19</span>ms/step - loss: <span class="number">3.2616</span> - sparse_categorical_accuracy: <span class="number">0.6041</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7295</span> - val_loss: <span class="number">3.3015</span> - val_sparse_categorical_accuracy: <span class="number">0.5503</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7235</span></span><br><span class="line">Epoch <span class="number">7</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">6</span>s <span class="number">21</span>ms/step - loss: <span class="number">3.2595</span> - sparse_categorical_accuracy: <span class="number">0.6059</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7322</span> - val_loss: <span class="number">3.3064</span> - val_sparse_categorical_accuracy: <span class="number">0.5454</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7266</span></span><br><span class="line">Epoch <span class="number">8</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">6</span>s <span class="number">21</span>ms/step - loss: <span class="number">3.2591</span> - sparse_categorical_accuracy: <span class="number">0.6063</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7327</span> - val_loss: <span class="number">3.3025</span> - val_sparse_categorical_accuracy: <span class="number">0.5481</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7231</span></span><br><span class="line">Epoch <span class="number">9</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">5</span>s <span class="number">19</span>ms/step - loss: <span class="number">3.2588</span> - sparse_categorical_accuracy: <span class="number">0.6062</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7332</span> - val_loss: <span class="number">3.2992</span> - val_sparse_categorical_accuracy: <span class="number">0.5521</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7257</span></span><br><span class="line">Epoch <span class="number">10</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">5</span>s <span class="number">18</span>ms/step - loss: <span class="number">3.2577</span> - sparse_categorical_accuracy: <span class="number">0.6073</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7363</span> - val_loss: <span class="number">3.2981</span> - val_sparse_categorical_accuracy: <span class="number">0.5516</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7306</span></span><br><span class="line">CPU times: user <span class="number">18.9</span> s, sys: <span class="number">3.86</span> s, total: <span class="number">22.7</span> s</span><br><span class="line">Wall time: <span class="number">1</span>min <span class="number">1</span>s</span><br></pre></td></tr></table></figure>
<h2 id="shi-yong-tensorflow-serving-bu-shu-mo-xing">使用tensorflow-serving部署模型</h2>
<p>TensorFlow训练好的模型以tensorflow原生方式保存成protobuf文件后可以用许多方式部署运行。</p>
<p>例如：通过 tensorflow-js 可以用javascrip脚本加载模型并在浏览器中运行模型。</p>
<p>通过 tensorflow-lite 可以在移动和嵌入式设备上加载并运行TensorFlow模型。</p>
<p>通过 tensorflow-serving 可以加载模型后提供网络接口API服务，通过任意编程语言发送网络请求都可以获取模型预测结果。</p>
<p>通过 tensorFlow for Java接口，可以在Java或者spark(scala)中调用tensorflow模型进行预测。</p>
<p>主要介绍tensorflow serving部署模型、使用spark(scala)调用tensorflow模型的方法。</p>
<h3 id="tensorflow-serving-mo-xing-bu-shu-gai-shu">tensorflow serving模型部署概述</h3>
<p>使用 tensorflow serving 部署模型要完成以下步骤。</p>
<ul>
<li>
<p>(1) 准备protobuf模型文件。</p>
</li>
<li>
<p>(2) 安装tensorflow serving。</p>
</li>
<li>
<p>(3) 启动tensorflow serving 服务。</p>
</li>
<li>
<p>(4) 向API服务发送请求，获取预测结果。</p>
</li>
</ul>
<p>可通过以下colab链接测试效果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%tensorflow_version <span class="number">2.</span>x</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.__version__)</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<h3 id="zhun-bei-protobuf-mo-xing-wen-jian">准备protobuf模型文件</h3>
<p>我们使用tf.keras 训练一个简单的线性回归模型，并保存成protobuf文件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,optimizers</span><br><span class="line"></span><br><span class="line"><span class="comment">## 样本数量</span></span><br><span class="line">n = <span class="number">800</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 生成测试用数据集</span></span><br><span class="line">X = tf.random.uniform([n,<span class="number">2</span>],minval=<span class="number">-10</span>,maxval=<span class="number">10</span>) </span><br><span class="line">w0 = tf.constant([[<span class="number">2.0</span>],[<span class="number">-1.0</span>]])</span><br><span class="line">b0 = tf.constant(<span class="number">3.0</span>)</span><br><span class="line"></span><br><span class="line">Y = X@w0 + b0 + tf.random.normal([n,<span class="number">1</span>],</span><br><span class="line">    mean = <span class="number">0.0</span>,stddev= <span class="number">2.0</span>) <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 建立模型</span></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line">inputs = layers.Input(shape = (<span class="number">2</span>,),name =<span class="string">"inputs"</span>) <span class="comment">#设置输入名字为inputs</span></span><br><span class="line">outputs = layers.Dense(<span class="number">1</span>, name = <span class="string">"outputs"</span>)(inputs) <span class="comment">#设置输出名字为outputs</span></span><br><span class="line">linear = models.Model(inputs = inputs,outputs = outputs)</span><br><span class="line">linear.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 使用fit方法进行训练</span></span><br><span class="line">linear.compile(optimizer=<span class="string">"rmsprop"</span>,loss=<span class="string">"mse"</span>,metrics=[<span class="string">"mae"</span>])</span><br><span class="line">linear.fit(X,Y,batch_size = <span class="number">8</span>,epochs = <span class="number">100</span>)  </span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"w = "</span>,linear.layers[<span class="number">1</span>].kernel)</span><br><span class="line">tf.print(<span class="string">"b = "</span>,linear.layers[<span class="number">1</span>].bias)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 将模型保存成pb格式文件</span></span><br><span class="line">export_path = <span class="string">"./data/linear_model/"</span></span><br><span class="line">version = <span class="string">"1"</span>       <span class="comment">#后续可以通过版本号进行模型版本迭代与管理</span></span><br><span class="line">linear.save(export_path+version, save_format=<span class="string">"tf"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看保存的模型文件</span></span><br><span class="line">!ls &#123;export_path+version&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">assets</span>	<span class="selector-tag">saved_model</span><span class="selector-class">.pb</span>	<span class="selector-tag">variables</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看模型文件相关信息</span></span><br><span class="line">!saved_model_cli show --dir &#123;export_path+str(version)&#125; --all</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">MetaGraphDef <span class="keyword">with</span> tag-<span class="keyword">set</span>: <span class="string">'serve'</span> contains the <span class="keyword">following</span> SignatureDefs:</span><br><span class="line"></span><br><span class="line">signature_def[<span class="string">'__saved_model_init_op'</span>]:</span><br><span class="line">  The given SavedModel SignatureDef contains the <span class="keyword">following</span> <span class="keyword">input</span>(s):</span><br><span class="line">  The given SavedModel SignatureDef contains the <span class="keyword">following</span> <span class="keyword">output</span>(s):</span><br><span class="line">    outputs[<span class="string">'__saved_model_init_op'</span>] tensor_info:</span><br><span class="line">        dtype: DT_INVALID</span><br><span class="line">        shape: unknown_rank</span><br><span class="line">        <span class="keyword">name</span>: NoOp</span><br><span class="line">  Method <span class="keyword">name</span> <span class="keyword">is</span>: </span><br><span class="line"></span><br><span class="line">signature_def[<span class="string">'serving_default'</span>]:</span><br><span class="line">  The given SavedModel SignatureDef contains the <span class="keyword">following</span> <span class="keyword">input</span>(s):</span><br><span class="line">    inputs[<span class="string">'inputs'</span>] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">name</span>: serving_default_inputs:<span class="number">0</span></span><br><span class="line">  The given SavedModel SignatureDef contains the <span class="keyword">following</span> <span class="keyword">output</span>(s):</span><br><span class="line">    outputs[<span class="string">'outputs'</span>] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">name</span>: StatefulPartitionedCall:<span class="number">0</span></span><br><span class="line">  Method <span class="keyword">name</span> <span class="keyword">is</span>: tensorflow/serving/predict</span><br><span class="line"><span class="keyword">WARNING</span>:tensorflow:<span class="keyword">From</span> /tensorflow<span class="number">-2.1</span><span class="number">.0</span>/python3<span class="number">.6</span>/tensorflow_core/python/ops/resource_variable_ops.py:<span class="number">1786</span>: <span class="keyword">calling</span> BaseResourceVariable.__init__ (<span class="keyword">from</span> tensorflow.python.ops.resource_variable_ops) <span class="keyword">with</span> <span class="keyword">constraint</span> <span class="keyword">is</span> deprecated <span class="keyword">and</span> will be removed <span class="keyword">in</span> a future version.</span><br><span class="line">Instructions <span class="keyword">for</span> updating:</span><br><span class="line"><span class="keyword">If</span> <span class="keyword">using</span> Keras pass *_constraint arguments <span class="keyword">to</span> layers.</span><br><span class="line"></span><br><span class="line">Defined Functions:</span><br><span class="line">  <span class="keyword">Function</span> <span class="keyword">Name</span>: <span class="string">'__call__'</span></span><br><span class="line">    <span class="keyword">Option</span> <span class="comment">#1</span></span><br><span class="line">      Callable <span class="keyword">with</span>:</span><br><span class="line">        Argument <span class="comment">#1</span></span><br><span class="line">          inputs: TensorSpec(shape=(<span class="keyword">None</span>, <span class="number">2</span>), dtype=tf.float32, <span class="keyword">name</span>=<span class="string">'inputs'</span>)</span><br><span class="line">        Argument <span class="comment">#2</span></span><br><span class="line">          DType: <span class="built_in">bool</span></span><br><span class="line">          <span class="keyword">Value</span>: <span class="literal">False</span></span><br><span class="line">        Argument <span class="comment">#3</span></span><br><span class="line">          DType: NoneType</span><br><span class="line">          <span class="keyword">Value</span>: <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">Option</span> <span class="comment">#2</span></span><br><span class="line">      Callable <span class="keyword">with</span>:</span><br><span class="line">        Argument <span class="comment">#1</span></span><br><span class="line">          inputs: TensorSpec(shape=(<span class="keyword">None</span>, <span class="number">2</span>), dtype=tf.float32, <span class="keyword">name</span>=<span class="string">'inputs'</span>)</span><br><span class="line">        Argument <span class="comment">#2</span></span><br><span class="line">          DType: <span class="built_in">bool</span></span><br><span class="line">          <span class="keyword">Value</span>: <span class="literal">True</span></span><br><span class="line">        Argument <span class="comment">#3</span></span><br><span class="line">          DType: NoneType</span><br><span class="line">          <span class="keyword">Value</span>: <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">Function</span> <span class="keyword">Name</span>: <span class="string">'_default_save_signature'</span></span><br><span class="line">    <span class="keyword">Option</span> <span class="comment">#1</span></span><br><span class="line">      Callable <span class="keyword">with</span>:</span><br><span class="line">        Argument <span class="comment">#1</span></span><br><span class="line">          inputs: TensorSpec(shape=(<span class="keyword">None</span>, <span class="number">2</span>), dtype=tf.float32, <span class="keyword">name</span>=<span class="string">'inputs'</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">Function</span> <span class="keyword">Name</span>: <span class="string">'call_and_return_all_conditional_losses'</span></span><br><span class="line">    <span class="keyword">Option</span> <span class="comment">#1</span></span><br><span class="line">      Callable <span class="keyword">with</span>:</span><br><span class="line">        Argument <span class="comment">#1</span></span><br><span class="line">          inputs: TensorSpec(shape=(<span class="keyword">None</span>, <span class="number">2</span>), dtype=tf.float32, <span class="keyword">name</span>=<span class="string">'inputs'</span>)</span><br><span class="line">        Argument <span class="comment">#2</span></span><br><span class="line">          DType: <span class="built_in">bool</span></span><br><span class="line">          <span class="keyword">Value</span>: <span class="literal">True</span></span><br><span class="line">        Argument <span class="comment">#3</span></span><br><span class="line">          DType: NoneType</span><br><span class="line">          <span class="keyword">Value</span>: <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">Option</span> <span class="comment">#2</span></span><br><span class="line">      Callable <span class="keyword">with</span>:</span><br><span class="line">        Argument <span class="comment">#1</span></span><br><span class="line">          inputs: TensorSpec(shape=(<span class="keyword">None</span>, <span class="number">2</span>), dtype=tf.float32, <span class="keyword">name</span>=<span class="string">'inputs'</span>)</span><br><span class="line">        Argument <span class="comment">#2</span></span><br><span class="line">          DType: <span class="built_in">bool</span></span><br><span class="line">          <span class="keyword">Value</span>: <span class="literal">False</span></span><br><span class="line">        Argument <span class="comment">#3</span></span><br><span class="line">          DType: NoneType</span><br><span class="line">          <span class="keyword">Value</span>: <span class="keyword">None</span></span><br></pre></td></tr></table></figure>
<h3 id="an-zhuang-tensorflow-serving">安装 tensorflow serving</h3>
<p>安装 tensorflow serving 有2种主要方法：通过Docker镜像安装，通过apt安装。通过Docker镜像安装是最简单，最直接的方法，推荐采用。</p>
<p>Docker可以理解成一种容器，其上面可以给各种不同的程序提供独立的运行环境。一般业务中用到tensorflow的企业都会有运维同学通过Docker 搭建 tensorflow serving.无需算法工程师同学动手安装，以下安装过程仅供参考。</p>
<p>不同操作系统机器上安装Docker的方法可以参照以下链接。</p>
<p>Windows: <a href="https://www.runoob.com/docker/windows-docker-install.html" target="_blank" rel="noopener">https://www.runoob.com/docker/windows-docker-install.html</a></p>
<p>MacOs: <a href="https://www.runoob.com/docker/macos-docker-install.html" target="_blank" rel="noopener">https://www.runoob.com/docker/macos-docker-install.html</a></p>
<p>CentOS: <a href="https://www.runoob.com/docker/centos-docker-install.html" target="_blank" rel="noopener">https://www.runoob.com/docker/centos-docker-install.html</a></p>
<p>安装Docker成功后，使用如下命令加载 tensorflow/serving 镜像到Docker中</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">docker</span> pull tensorflow/serving</span><br></pre></td></tr></table></figure>
<h3 id="qi-dong-tensorflow-serving-fu-wu">启动 tensorflow serving 服务</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">!docker run -t --rm -p <span class="number">8501</span>:<span class="number">8501</span> \</span><br><span class="line">    -v <span class="string">"/Users/.../data/linear_model/"</span> \</span><br><span class="line">    -e MODEL_NAME=linear_model \</span><br><span class="line">    tensorflow/serving &amp; &gt;server.log <span class="number">2</span>&gt;&amp;<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="xiang-api-fu-wu-fa-song-qing-qiu">向API服务发送请求</h3>
<p>可以使用任何编程语言的http功能发送请求，下面示范linux的 curl 命令发送请求，以及Python的requests库发送请求。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!curl -d <span class="string">'&#123;"instances": [[1.0, 2.0], [5.0,7.0]]&#125;'</span> \</span><br><span class="line">    -X POST http://localhost:<span class="number">8501</span>/v1/models/linear_model:predict</span><br></pre></td></tr></table></figure>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"predictions"</span>: [[<span class="number">3.06546211</span>], [<span class="number">6.02843142</span>]</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json,requests</span><br><span class="line"></span><br><span class="line">data = json.dumps(&#123;<span class="string">"signature_name"</span>: <span class="string">"serving_default"</span>, <span class="string">"instances"</span>: [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">5.0</span>,<span class="number">7.0</span>]]&#125;)</span><br><span class="line">headers = &#123;<span class="string">"content-type"</span>: <span class="string">"application/json"</span>&#125;</span><br><span class="line">json_response = requests.post(<span class="string">'http://localhost:8501/v1/models/linear_model:predict'</span>, </span><br><span class="line">        data=data, headers=headers)</span><br><span class="line">predictions = json.loads(json_response.text)[<span class="string">"predictions"</span>]</span><br><span class="line">print(predictions) <span class="comment"># [[3.06546211], [6.02843142]]</span></span><br></pre></td></tr></table></figure>
<h2 id="shi-yong-spark-scala-diao-yong-tensorflow-2-0-xun-lian-hao-de-mo-xing">使用spark-scala调用tensorflow2.0训练好的模型</h2>
<p>本篇文章介绍在spark中调用训练好的tensorflow模型进行预测的方法。</p>
<p>本文内容的学习需要一定的spark和scala基础。</p>
<p>如果使用pyspark的话会比较简单，只需要在每个executor上用Python加载模型分别预测就可以了。</p>
<p>但工程上为了性能考虑，通常使用的是scala版本的spark。</p>
<p>本篇文章我们通过TensorFlow for Java 在spark中调用训练好的tensorflow模型。</p>
<p>利用spark的分布式计算能力，从而可以让训练好的tensorflow模型在成百上千的机器上分布式并行执行模型推断。</p>
<h3 id="spark-scala-diao-yong-tensorflow-mo-xing-gai-shu">spark-scala调用tensorflow模型概述</h3>
<p>在spark(scala)中调用tensorflow模型进行预测需要完成以下几个步骤。</p>
<p>（1）准备protobuf模型文件</p>
<p>（2）创建spark(scala)项目，在项目中添加java版本的tensorflow对应的jar包依赖</p>
<p>（3）在spark(scala)项目中driver端加载tensorflow模型调试成功</p>
<p>（4）在spark(scala)项目中通过RDD在executor上加载tensorflow模型调试成功</p>
<p>（5） 在spark(scala)项目中通过DataFrame在executor上加载tensorflow模型调试成功</p>
<h3 id="zhun-bei-protobuf-mo-xing-wen-jian-1">准备protobuf模型文件</h3>
<p>我们使用tf.keras 训练一个简单的线性回归模型，并保存成protobuf文件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,optimizers</span><br><span class="line"></span><br><span class="line"><span class="comment">## 样本数量</span></span><br><span class="line">n = <span class="number">800</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 生成测试用数据集</span></span><br><span class="line">X = tf.random.uniform([n,<span class="number">2</span>],minval=<span class="number">-10</span>,maxval=<span class="number">10</span>) </span><br><span class="line">w0 = tf.constant([[<span class="number">2.0</span>],[<span class="number">-1.0</span>]])</span><br><span class="line">b0 = tf.constant(<span class="number">3.0</span>)</span><br><span class="line"></span><br><span class="line">Y = X@w0 + b0 + tf.random.normal([n,<span class="number">1</span>],mean = <span class="number">0.0</span>,stddev= <span class="number">2.0</span>)  <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 建立模型</span></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line">inputs = layers.Input(shape = (<span class="number">2</span>,),name =<span class="string">"inputs"</span>) <span class="comment">#设置输入名字为inputs</span></span><br><span class="line">outputs = layers.Dense(<span class="number">1</span>, name = <span class="string">"outputs"</span>)(inputs) <span class="comment">#设置输出名字为outputs</span></span><br><span class="line">linear = models.Model(inputs = inputs,outputs = outputs)</span><br><span class="line">linear.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 使用fit方法进行训练</span></span><br><span class="line">linear.compile(optimizer=<span class="string">"rmsprop"</span>,loss=<span class="string">"mse"</span>,metrics=[<span class="string">"mae"</span>])</span><br><span class="line">linear.fit(X,Y,batch_size = <span class="number">8</span>,epochs = <span class="number">100</span>)  </span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"w = "</span>,linear.layers[<span class="number">1</span>].kernel)</span><br><span class="line">tf.print(<span class="string">"b = "</span>,linear.layers[<span class="number">1</span>].bias)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 将模型保存成pb格式文件</span></span><br><span class="line">export_path = <span class="string">"./data/linear_model/"</span></span><br><span class="line">version = <span class="string">"1"</span>       <span class="comment">#后续可以通过版本号进行模型版本迭代与管理</span></span><br><span class="line">linear.save(export_path+version, save_format=<span class="string">"tf"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!ls &#123;export_path+version&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看模型文件相关信息</span></span><br><span class="line">!saved_model_cli show --dir &#123;export_path+str(version)&#125; --all</span><br></pre></td></tr></table></figure>
<p>模型文件信息中这些标红的部分都是后面有可能会用到的。</p>
<p><img src="/2020/05/07/tensorflow/tensorflow2/%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E4%BF%A1%E6%81%AF.png" alt></p>
<h3 id="chuang-jian-spark-scala-xiang-mu-zai-xiang-mu-zhong-tian-jia-java-ban-ben-de-tensorflow-dui-ying-de-jar-bao-yi-lai">创建spark(scala)项目，在项目中添加java版本的tensorflow对应的jar包依赖</h3>
<p>如果使用maven管理项目，需要添加如下 jar包依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.tensorflow/tensorflow --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.tensorflow<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>tensorflow<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>也可以从下面网址中直接下载 org.tensorflow.tensorflow的jar包</p>
<p>以及其依赖的org.tensorflow.libtensorflow 和 org.tensorflowlibtensorflow_jni的jar包 放到项目中。</p>
<p><a href="https://mvnrepository.com/artifact/org.tensorflow/tensorflow/1.15.0" target="_blank" rel="noopener">https://mvnrepository.com/artifact/org.tensorflow/tensorflow/1.15.0</a></p>
<h3 id="zai-spark-scala-xiang-mu-zhong-driver-duan-jia-zai-tensorflow-mo-xing-diao-shi-cheng-gong">在spark(scala)项目中driver端加载tensorflow模型调试成功</h3>
<p>我们的示范代码在jupyter notebook中进行演示，需要安装toree以支持spark(scala)。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">WrappedArray</span></span><br><span class="line"><span class="keyword">import</span> org.&#123;tensorflow=&gt;tf&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//注：load函数的第二个参数一般都是“serve”，可以从模型文件相关信息中找到</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> bundle = tf.<span class="type">SavedModelBundle</span> </span><br><span class="line">   .load(<span class="string">"/Users/liangyun/CodeFiles/eat_tensorflow2_in_30_days/data/linear_model/1"</span>,<span class="string">"serve"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//注：在java版本的tensorflow中还是类似tensorflow1.0中静态计算图的模式，需要建立Session, 指定feed的数据和fetch的结果, 然后 run.</span></span><br><span class="line"><span class="comment">//注：如果有多个数据需要喂入，可以连续使用多个feed方法</span></span><br><span class="line"><span class="comment">//注：输入必须是float类型</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sess = bundle.session()</span><br><span class="line"><span class="keyword">val</span> x = tf.<span class="type">Tensor</span>.create(<span class="type">Array</span>(<span class="type">Array</span>(<span class="number">1.0</span>f,<span class="number">2.0</span>f),<span class="type">Array</span>(<span class="number">2.0</span>f,<span class="number">3.0</span>f)))</span><br><span class="line"><span class="keyword">val</span> y =  sess.runner().feed(<span class="string">"serving_default_inputs:0"</span>, x)</span><br><span class="line">         .fetch(<span class="string">"StatefulPartitionedCall:0"</span>).run().get(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> result = <span class="type">Array</span>.ofDim[<span class="type">Float</span>](y.shape()(<span class="number">0</span>).toInt,y.shape()(<span class="number">1</span>).toInt)</span><br><span class="line">y.copyTo(result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(x != <span class="literal">null</span>) x.close()</span><br><span class="line"><span class="keyword">if</span>(y != <span class="literal">null</span>) y.close()</span><br><span class="line"><span class="keyword">if</span>(sess != <span class="literal">null</span>) sess.close()</span><br><span class="line"><span class="keyword">if</span>(bundle != <span class="literal">null</span>) bundle.close()  </span><br><span class="line"></span><br><span class="line">result</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Array(<span class="name">Array</span>(<span class="number">3.019596</span>), Array(<span class="number">3.9878292</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/TfDriver.png" alt></p>
<h3 id="zai-spark-scala-xiang-mu-zhong-tong-guo-rdd-zai-executor-shang-jia-zai-tensorflow-mo-xing-diao-shi-cheng-gong">在spark(scala)项目中通过RDD在executor上加载tensorflow模型调试成功</h3>
<p>下面我们通过广播机制将Driver端加载的TensorFlow模型传递到各个executor上，并在executor上分布式地调用模型进行推断。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">WrappedArray</span></span><br><span class="line"><span class="keyword">import</span> org.&#123;tensorflow=&gt;tf&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">    .builder()</span><br><span class="line">    .appName(<span class="string">"TfRDD"</span>)</span><br><span class="line">    .enableHiveSupport()</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment">//在Driver端加载模型</span></span><br><span class="line"><span class="keyword">val</span> bundle = tf.<span class="type">SavedModelBundle</span> </span><br><span class="line">   .load(<span class="string">"/Users/liangyun/CodeFiles/master_tensorflow2_in_20_hours/data/linear_model/1"</span>,<span class="string">"serve"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//利用广播将模型发送到executor上</span></span><br><span class="line"><span class="keyword">val</span> broads = sc.broadcast(bundle)</span><br><span class="line"></span><br><span class="line"><span class="comment">//构造数据集</span></span><br><span class="line"><span class="keyword">val</span> rdd_data = sc.makeRDD(<span class="type">List</span>(<span class="type">Array</span>(<span class="number">1.0</span>f,<span class="number">2.0</span>f),<span class="type">Array</span>(<span class="number">3.0</span>f,<span class="number">5.0</span>f),<span class="type">Array</span>(<span class="number">6.0</span>f,<span class="number">7.0</span>f),<span class="type">Array</span>(<span class="number">8.0</span>f,<span class="number">3.0</span>f)))</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过mapPartitions调用模型进行批量推断</span></span><br><span class="line"><span class="keyword">val</span> rdd_result = rdd_data.mapPartitions(iter =&gt; &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> arr = iter.toArray</span><br><span class="line">    <span class="keyword">val</span> model = broads.value</span><br><span class="line">    <span class="keyword">val</span> sess = model.session()</span><br><span class="line">    <span class="keyword">val</span> x = tf.<span class="type">Tensor</span>.create(arr)</span><br><span class="line">    <span class="keyword">val</span> y =  sess.runner().feed(<span class="string">"serving_default_inputs:0"</span>, x)</span><br><span class="line">             .fetch(<span class="string">"StatefulPartitionedCall:0"</span>).run().get(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将预测结果拷贝到相同shape的Float类型的Array中</span></span><br><span class="line">    <span class="keyword">val</span> result = <span class="type">Array</span>.ofDim[<span class="type">Float</span>](y.shape()(<span class="number">0</span>).toInt,y.shape()(<span class="number">1</span>).toInt)</span><br><span class="line">    y.copyTo(result)</span><br><span class="line">    result.iterator</span><br><span class="line">    </span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rdd_result.take(<span class="number">5</span>)</span><br><span class="line">bundle.close</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Array(<span class="name">Array</span>(<span class="number">3.019596</span>), Array(<span class="number">3.9264367</span>), Array(<span class="number">7.8607616</span>), Array(<span class="number">15.974984</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/07/tensorflow/tensorflow2/TfRDD.png" alt></p>
<h3 id="zai-spark-scala-xiang-mu-zhong-tong-guo-data-frame-zai-executor-shang-jia-zai-tensorflow-mo-xing-diao-shi-cheng-gong">在spark(scala)项目中通过DataFrame在executor上加载tensorflow模型调试成功</h3>
<p>除了可以在Spark的RDD数据上调用tensorflow模型进行分布式推断，</p>
<p>我们也可以在DataFrame数据上调用tensorflow模型进行分布式推断。</p>
<p>主要思路是将推断方法注册成为一个sparkSQL函数。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">WrappedArray</span></span><br><span class="line"><span class="keyword">import</span> org.&#123;tensorflow=&gt;tf&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TfDataFrame</span> <span class="keyword">extends</span> <span class="title">Serializable</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]):<span class="type">Unit</span> = &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">        .builder()</span><br><span class="line">        .appName(<span class="string">"TfDataFrame"</span>)</span><br><span class="line">        .enableHiveSupport()</span><br><span class="line">        .getOrCreate()</span><br><span class="line">        <span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> bundle = tf.<span class="type">SavedModelBundle</span> </span><br><span class="line">           .load(<span class="string">"/Users/liangyun/CodeFiles/master_tensorflow2_in_20_hours/data/linear_model/1"</span>,<span class="string">"serve"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> broads = sc.broadcast(bundle)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//构造预测函数，并将其注册成sparkSQL的udf</span></span><br><span class="line">        <span class="keyword">val</span> tfpredict = (features:<span class="type">WrappedArray</span>[<span class="type">Float</span>])  =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> bund = broads.value</span><br><span class="line">            <span class="keyword">val</span> sess = bund.session()</span><br><span class="line">            <span class="keyword">val</span> x = tf.<span class="type">Tensor</span>.create(<span class="type">Array</span>(features.toArray))</span><br><span class="line">            <span class="keyword">val</span> y =  sess.runner().feed(<span class="string">"serving_default_inputs:0"</span>, x)</span><br><span class="line">                     .fetch(<span class="string">"StatefulPartitionedCall:0"</span>).run().get(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">val</span> result = <span class="type">Array</span>.ofDim[<span class="type">Float</span>](y.shape()(<span class="number">0</span>).toInt,y.shape()(<span class="number">1</span>).toInt)</span><br><span class="line">            y.copyTo(result)</span><br><span class="line">            <span class="keyword">val</span> y_pred = result(<span class="number">0</span>)(<span class="number">0</span>)</span><br><span class="line">            y_pred</span><br><span class="line">        &#125;</span><br><span class="line">        spark.udf.register(<span class="string">"tfpredict"</span>,tfpredict)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//构造DataFrame数据集，将features放到一列中</span></span><br><span class="line">        <span class="keyword">val</span> dfdata = sc.parallelize(<span class="type">List</span>(<span class="type">Array</span>(<span class="number">1.0</span>f,<span class="number">2.0</span>f),<span class="type">Array</span>(<span class="number">3.0</span>f,<span class="number">5.0</span>f),<span class="type">Array</span>(<span class="number">7.0</span>f,<span class="number">8.0</span>f))).toDF(<span class="string">"features"</span>)</span><br><span class="line">        dfdata.show </span><br><span class="line">        </span><br><span class="line">        <span class="comment">//调用sparkSQL预测函数，增加一个新的列作为y_preds</span></span><br><span class="line">        <span class="keyword">val</span> dfresult = dfdata.selectExpr(<span class="string">"features"</span>,<span class="string">"tfpredict(features) as y_preds"</span>)</span><br><span class="line">        dfresult.show </span><br><span class="line">        bundle.close</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">TfDataFrame</span>.main(<span class="type">Array</span>())</span><br></pre></td></tr></table></figure>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">+----------+</span></span><br><span class="line">|  features|</span><br><span class="line"><span class="code">+----------+</span></span><br><span class="line">|[1.0, 2.0]|</span><br><span class="line">|[3.0, 5.0]|</span><br><span class="line">|[7.0, 8.0]|</span><br><span class="line"><span class="code">+----------+</span></span><br><span class="line"></span><br><span class="line"><span class="code">+----------+</span>---------+</span><br><span class="line">|  features|  y<span class="emphasis">_preds|</span></span><br><span class="line"><span class="emphasis">+----------+---------+</span></span><br><span class="line"><span class="emphasis">|[1.0, 2.0]| 3.019596|</span></span><br><span class="line"><span class="emphasis">|[3.0, 5.0]|3.9264367|</span></span><br><span class="line"><span class="emphasis">|[7.0, 8.0]| 8.828995|</span></span><br><span class="line"><span class="emphasis">+----------+---------+</span></span><br></pre></td></tr></table></figure>
<p>以上我们分别在spark 的RDD数据结构和DataFrame数据结构上实现了调用一个tf.keras实现的线性回归模型进行分布式模型推断。</p>
<p>在本例基础上稍作修改则可以用spark调用训练好的各种复杂的神经网络模型进行分布式模型推断。</p>
<p>但实际上tensorflow并不仅仅适合实现神经网络，其底层的计算图语言可以表达各种数值计算过程。</p>
<p>利用其丰富的低阶API，我们可以在tensorflow2.0上实现任意机器学习模型，</p>
<p>结合tf.Module提供的便捷的封装功能，我们可以将训练好的任意机器学习模型导出成模型文件并在spark上分布式调用执行。</p>
<p>这无疑为我们的工程应用提供了巨大的想象空间。</p>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://github.com/lyhue1991/eat_tensorflow2_in_30_days" target="_blank" rel="noopener">eat_tensorflow2_in_30_days</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/98472075" target="_blank" rel="noopener">《一文概览深度学习中的激活函数》</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/98863801" target="_blank" rel="noopener">《从ReLU到GELU,一文概览神经网络中的激活函数》</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/80594704" target="_blank" rel="noopener">《5分钟理解Focal Loss与GHM——解决样本不平衡利器》</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/68509398" target="_blank" rel="noopener">《用GPU加速Keras模型——Colab免费GPU使用攻略》</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32230623" target="_blank" rel="noopener">详见《一个框架看懂优化算法之异同 SGD/AdaGrad/Adam》</a></li>
<li><a href="https://colab.research.google.com/drive/1r5dLoeJq5z01sU72BX2M5UiNSkuxsEFe" target="_blank" rel="noopener">《tf_单GPU》</a></li>
<li><a href="https://colab.research.google.com/drive/1j2kp_t0S_cofExSN7IyJ4QtMscbVlXU-" target="_blank" rel="noopener">《tf_多GPU》</a></li>
<li><a href="https://colab.research.google.com/drive/1XCIhATyE1R7lq6uwFlYlRsUr5d9_-r1s" target="_blank" rel="noopener">《tf_TPU》</a></li>
<li><a href="https://colab.research.google.com/drive/1vS5LAYJTEn-H0GDb1irzIuyRB8E3eWc8" target="_blank" rel="noopener">《tf_serving》</a></li>
</ol>

    </div>

    
    
    <div>
  
    <div style="text-align:center;color:#bfbfbf;font-size:16px;">
      <span>-------- 本文结束 </span>
      <i class="fa fa-coffee"></i>
      <span> 感谢阅读 --------</span>
    </div>
  
</div>
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    鼓励一下
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat.png" alt="Li Zhen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/ali.png" alt="Li Zhen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E5%BB%BA%E6%A8%A1%E6%B5%81%E7%A8%8B/" rel="tag"><i class="fa fa-tag"></i> 建模流程</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/04/29/machine_learning/time_feature/" rel="prev" title="时间序列预测">
      <i class="fa fa-chevron-left"></i> 时间序列预测
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/05/08/machine_learning/smoothing_method/" rel="next" title="平滑方法">
      平滑方法 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#tensor-flow-jian-mo-liu-cheng"><span class="nav-number">1.</span> <span class="nav-text">TensorFlow建模流程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#jie-gou-hua-shu-ju-jian-mo-liu-cheng"><span class="nav-number">1.1.</span> <span class="nav-text">结构化数据建模流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#zhun-bei-shu-ju"><span class="nav-number">1.1.1.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ding-yi-mo-xing"><span class="nav-number">1.1.2.</span> <span class="nav-text">定义模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xun-lian-mo-xing"><span class="nav-number">1.1.3.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ping-gu-mo-xing"><span class="nav-number">1.1.4.</span> <span class="nav-text">评估模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#shi-yong-mo-xing"><span class="nav-number">1.1.5.</span> <span class="nav-text">使用模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bao-cun-mo-xing"><span class="nav-number">1.1.6.</span> <span class="nav-text">保存模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#keras-fang-shi-bao-cun"><span class="nav-number">1.1.6.1.</span> <span class="nav-text">Keras方式保存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor-flow-yuan-sheng-fang-shi-bao-cun"><span class="nav-number">1.1.6.2.</span> <span class="nav-text">TensorFlow原生方式保存</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tu-pian-shu-ju-jian-mo-liu-cheng"><span class="nav-number">1.2.</span> <span class="nav-text">图片数据建模流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#zhun-bei-shu-ju-1"><span class="nav-number">1.2.1.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ding-yi-mo-xing-1"><span class="nav-number">1.2.2.</span> <span class="nav-text">定义模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xun-lian-mo-xing-1"><span class="nav-number">1.2.3.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ping-gu-mo-xing-1"><span class="nav-number">1.2.4.</span> <span class="nav-text">评估模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#shi-yong-mo-xing-1"><span class="nav-number">1.2.5.</span> <span class="nav-text">使用模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bao-cun-mo-xing-1"><span class="nav-number">1.2.6.</span> <span class="nav-text">保存模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#wen-ben-shu-ju-jian-mo-liu-cheng"><span class="nav-number">1.3.</span> <span class="nav-text">文本数据建模流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#zhun-bei-shu-ju-2"><span class="nav-number">1.3.1.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ding-yi-mo-xing-2"><span class="nav-number">1.3.2.</span> <span class="nav-text">定义模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xun-lian-mo-xing-2"><span class="nav-number">1.3.3.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ping-gu-mo-xing-2"><span class="nav-number">1.3.4.</span> <span class="nav-text">评估模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#shi-yong-mo-xing-2"><span class="nav-number">1.3.5.</span> <span class="nav-text">使用模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bao-cun-mo-xing-2"><span class="nav-number">1.3.6.</span> <span class="nav-text">保存模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shi-jian-xu-lie-shu-ju-jian-mo-liu-cheng"><span class="nav-number">1.4.</span> <span class="nav-text">时间序列数据建模流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#zhun-bei-shu-ju-3"><span class="nav-number">1.4.1.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ding-yi-mo-xing-3"><span class="nav-number">1.4.2.</span> <span class="nav-text">定义模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xun-lian-mo-xing-3"><span class="nav-number">1.4.3.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ping-gu-mo-xing-3"><span class="nav-number">1.4.4.</span> <span class="nav-text">评估模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#shi-yong-mo-xing-3"><span class="nav-number">1.4.5.</span> <span class="nav-text">使用模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bao-cun-mo-xing-3"><span class="nav-number">1.4.6.</span> <span class="nav-text">保存模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#tensor-flow-de-he-xin-gai-nian"><span class="nav-number">2.</span> <span class="nav-text">TensorFlow的核心概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#zhang-liang-shu-ju-jie-gou"><span class="nav-number">2.1.</span> <span class="nav-text">张量数据结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#chang-liang-zhang-liang"><span class="nav-number">2.1.1.</span> <span class="nav-text">常量张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bian-liang-zhang-liang"><span class="nav-number">2.1.2.</span> <span class="nav-text">变量张量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#san-chong-ji-suan-tu"><span class="nav-number">2.2.</span> <span class="nav-text">三种计算图</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ji-suan-tu-jian-jie"><span class="nav-number">2.2.1.</span> <span class="nav-text">计算图简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#jing-tai-ji-suan-tu"><span class="nav-number">2.2.2.</span> <span class="nav-text">静态计算图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dong-tai-ji-suan-tu"><span class="nav-number">2.2.3.</span> <span class="nav-text">动态计算图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensor-flow-2-0-de-autograph"><span class="nav-number">2.2.4.</span> <span class="nav-text">TensorFlow2.0的Autograph</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#zi-dong-wei-fen-ji-zhi"><span class="nav-number">2.3.</span> <span class="nav-text">自动微分机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#li-yong-ti-du-ci-dai-qiu-dao-shu"><span class="nav-number">2.3.1.</span> <span class="nav-text">利用梯度磁带求导数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#li-yong-ti-du-ci-dai-he-you-hua-qi-qiu-zui-xiao-zhi"><span class="nav-number">2.3.2.</span> <span class="nav-text">利用梯度磁带和优化器求最小值</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#tensor-flow-de-ceng-ci-jie-gou"><span class="nav-number">3.</span> <span class="nav-text">TensorFlow的层次结构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#di-jie-api"><span class="nav-number">3.1.</span> <span class="nav-text">低阶API</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#xian-xing-hui-gui-mo-xing"><span class="nav-number">3.1.1.</span> <span class="nav-text">线性回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#zhun-bei-shu-ju-4"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ding-yi-mo-xing-4"><span class="nav-number">3.1.1.2.</span> <span class="nav-text">定义模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#xun-lian-mo-xing-4"><span class="nav-number">3.1.1.3.</span> <span class="nav-text">训练模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dnn-er-fen-lei-mo-xing"><span class="nav-number">3.1.2.</span> <span class="nav-text">DNN二分类模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#zhun-bei-shu-ju-5"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ding-yi-mo-xing-5"><span class="nav-number">3.1.2.2.</span> <span class="nav-text">定义模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#zhong-jie-api"><span class="nav-number">3.2.</span> <span class="nav-text">中阶API</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#xian-xing-hui-gui-mo-xing-1"><span class="nav-number">3.2.1.</span> <span class="nav-text">线性回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#zhun-bei-shu-ju-6"><span class="nav-number">3.2.1.1.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ding-yi-mo-xing-6"><span class="nav-number">3.2.1.2.</span> <span class="nav-text">定义模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#xun-lian-mo-xing-5"><span class="nav-number">3.2.1.3.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#jie-guo-ke-shi-hua"><span class="nav-number">3.2.1.4.</span> <span class="nav-text">结果可视化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dnn-er-fen-lei-mo-xing-1"><span class="nav-number">3.2.2.</span> <span class="nav-text">DNN二分类模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#zhun-bei-shu-ju-7"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ding-yi-mo-xing-7"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">定义模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#xun-lian-mo-xing-6"><span class="nav-number">3.2.2.3.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#jie-guo-ke-shi-hua-1"><span class="nav-number">3.2.2.4.</span> <span class="nav-text">结果可视化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gao-jie-api"><span class="nav-number">3.3.</span> <span class="nav-text">高阶API</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#xian-xing-hui-gui-mo-xing-2"><span class="nav-number">3.3.1.</span> <span class="nav-text">线性回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#zhun-bei-shu-ju-8"><span class="nav-number">3.3.1.1.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ding-yi-mo-xing-8"><span class="nav-number">3.3.1.2.</span> <span class="nav-text">定义模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#xun-lian-mo-xing-7"><span class="nav-number">3.3.1.3.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#jie-guo-ke-shi-hua-2"><span class="nav-number">3.3.1.4.</span> <span class="nav-text">结果可视化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dnn-er-fen-lei-mo-xing-2"><span class="nav-number">3.3.2.</span> <span class="nav-text">DNN二分类模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#zhun-bei-shu-ju-9"><span class="nav-number">3.3.2.1.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ding-yi-mo-xing-9"><span class="nav-number">3.3.2.2.</span> <span class="nav-text">定义模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#xun-lian-mo-xing-8"><span class="nav-number">3.3.2.3.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#jie-guo-ke-shi-hua-3"><span class="nav-number">3.3.2.4.</span> <span class="nav-text">结果可视化</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#tensor-flow-de-di-jie-api"><span class="nav-number">4.</span> <span class="nav-text">TensorFlow的低阶API</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#zhang-liang-de-jie-gou-cao-zuo"><span class="nav-number">4.1.</span> <span class="nav-text">张量的结构操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#chuang-jian-zhang-liang"><span class="nav-number">4.1.1.</span> <span class="nav-text">创建张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#suo-yin-qie-pian"><span class="nav-number">4.1.2.</span> <span class="nav-text">索引切片</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#wei-du-bian-huan"><span class="nav-number">4.1.3.</span> <span class="nav-text">维度变换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#he-bing-fen-ge"><span class="nav-number">4.1.4.</span> <span class="nav-text">合并分割</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#zhang-liang-de-shu-xue-yun-suan"><span class="nav-number">4.2.</span> <span class="nav-text">张量的数学运算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#biao-liang-yun-suan"><span class="nav-number">4.2.1.</span> <span class="nav-text">标量运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xiang-liang-yun-suan"><span class="nav-number">4.2.2.</span> <span class="nav-text">向量运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ju-zhen-yun-suan"><span class="nav-number">4.2.3.</span> <span class="nav-text">矩阵运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#yan-bo-ji-zhi"><span class="nav-number">4.2.4.</span> <span class="nav-text">广播机制</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#auto-graph-de-shi-yong-gui-fan"><span class="nav-number">4.3.</span> <span class="nav-text">AutoGraph的使用规范</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#autograph-bian-ma-gui-fan-zong-jie"><span class="nav-number">4.3.1.</span> <span class="nav-text">Autograph编码规范总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#autograph-bian-ma-gui-fan-jie-xi"><span class="nav-number">4.3.2.</span> <span class="nav-text">Autograph编码规范解析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#bei-tf-function-xiu-shi-de-han-shu-ying-jin-liang-shi-yong-tensor-flow-zhong-de-han-shu-er-bu-shi-python-zhong-de-qi-ta-han-shu"><span class="nav-number">4.3.2.1.</span> <span class="nav-text">被@tf.function修饰的函数应尽量使用TensorFlow中的函数而不是Python中的其他函数。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bi-mian-zai-tf-function-xiu-shi-de-han-shu-nei-bu-ding-yi-tf-variable"><span class="nav-number">4.3.2.2.</span> <span class="nav-text">避免在@tf.function修饰的函数内部定义tf.Variable</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bei-tf-function-xiu-shi-de-han-shu-bu-ke-xiu-gai-gai-han-shu-wai-bu-de-python-lie-biao-huo-zi-dian-deng-jie-gou-lei-xing-bian-liang"><span class="nav-number">4.3.2.3.</span> <span class="nav-text">被@tf.function修饰的函数不可修改该函数外部的Python列表或字典等结构类型变量。</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#auto-graph-de-ji-zhi-yuan-li"><span class="nav-number">4.4.</span> <span class="nav-text">AutoGraph的机制原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#autograph-de-ji-zhi-yuan-li"><span class="nav-number">4.4.1.</span> <span class="nav-text">Autograph的机制原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zhong-xin-li-jie-autograph-de-bian-ma-gui-fan"><span class="nav-number">4.4.2.</span> <span class="nav-text">重新理解Autograph的编码规范</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#auto-graph-he-tf-module"><span class="nav-number">4.5.</span> <span class="nav-text">AutoGraph和tf.Module</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#autograph-he-tf-module-gai-shu"><span class="nav-number">4.5.1.</span> <span class="nav-text">Autograph和tf.Module概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ying-yong-tf-module-feng-zhuang-autograph"><span class="nav-number">4.5.2.</span> <span class="nav-text">应用tf.Module封装Autograph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-module-he-tf-keras-model-tf-keras-layers-layer"><span class="nav-number">4.5.3.</span> <span class="nav-text">tf.Module和tf.keras.Model，tf.keras.layers.Layer</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#tensor-flow-de-zhong-jie-api"><span class="nav-number">5.</span> <span class="nav-text">TensorFlow的中阶API</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#shu-ju-guan-dao-dataset"><span class="nav-number">5.1.</span> <span class="nav-text">数据管道Dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#gou-jian-shu-ju-guan-dao"><span class="nav-number">5.1.1.</span> <span class="nav-text">构建数据管道</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#cong-numpy-array-gou-jian-shu-ju-guan-dao"><span class="nav-number">5.1.1.1.</span> <span class="nav-text">从Numpy array构建数据管道</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cong-pandas-data-frame-gou-jian-shu-ju-guan-dao"><span class="nav-number">5.1.1.2.</span> <span class="nav-text">从 Pandas DataFrame构建数据管道</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cong-python-generator-gou-jian-shu-ju-guan-dao"><span class="nav-number">5.1.1.3.</span> <span class="nav-text">从Python generator构建数据管道</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cong-csv-wen-jian-gou-jian-shu-ju-guan-dao"><span class="nav-number">5.1.1.4.</span> <span class="nav-text">从csv文件构建数据管道</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cong-wen-ben-wen-jian-gou-jian-shu-ju-guan-dao"><span class="nav-number">5.1.1.5.</span> <span class="nav-text">从文本文件构建数据管道</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cong-wen-jian-lu-jing-gou-jian-shu-ju-guan-dao"><span class="nav-number">5.1.1.6.</span> <span class="nav-text">从文件路径构建数据管道</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cong-tfrecords-wen-jian-gou-jian-shu-ju-guan-dao"><span class="nav-number">5.1.1.7.</span> <span class="nav-text">从tfrecords文件构建数据管道</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ying-yong-shu-ju-zhuan-huan"><span class="nav-number">5.1.2.</span> <span class="nav-text">应用数据转换</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#map"><span class="nav-number">5.1.2.1.</span> <span class="nav-text">map</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#flat-map"><span class="nav-number">5.1.2.2.</span> <span class="nav-text">flat_map</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#interleave"><span class="nav-number">5.1.2.3.</span> <span class="nav-text">interleave</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#filter"><span class="nav-number">5.1.2.4.</span> <span class="nav-text">filter</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#zip"><span class="nav-number">5.1.2.5.</span> <span class="nav-text">zip</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#condatenate"><span class="nav-number">5.1.2.6.</span> <span class="nav-text">condatenate</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#reduce"><span class="nav-number">5.1.2.7.</span> <span class="nav-text">reduce</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#batch"><span class="nav-number">5.1.2.8.</span> <span class="nav-text">batch</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#padden-batch"><span class="nav-number">5.1.2.9.</span> <span class="nav-text">padden_batch</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#window"><span class="nav-number">5.1.2.10.</span> <span class="nav-text">window</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#shuffle"><span class="nav-number">5.1.2.11.</span> <span class="nav-text">shuffle</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#repeat"><span class="nav-number">5.1.2.12.</span> <span class="nav-text">repeat</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#shard"><span class="nav-number">5.1.2.13.</span> <span class="nav-text">shard</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#take"><span class="nav-number">5.1.2.14.</span> <span class="nav-text">take</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ti-sheng-guan-dao-xing-neng"><span class="nav-number">5.1.3.</span> <span class="nav-text">提升管道性能</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#prefetch-rang-shu-ju-zhun-bei-he-can-shu-die-dai-bing-xing"><span class="nav-number">5.1.3.1.</span> <span class="nav-text">prefetch :让数据准备和参数迭代并行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#interleave-shu-ju-du-qu-duo-jin-cheng-bing-jiang-bu-tong-lai-yuan-shu-ju-jia-zai-yi-qi"><span class="nav-number">5.1.3.2.</span> <span class="nav-text">interleave :数据读取多进程,并将不同来源数据夹在一起</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#map-she-zhi-num-parallel-calls-rang-shu-ju-zhuan-huan-guo-cheng-duo-jin-xing-zhi-xing"><span class="nav-number">5.1.3.3.</span> <span class="nav-text">map : 设置num_parallel_calls 让数据转换过程多进行执行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cache-rang-shu-ju-zai-di-yi-ge-epoch-hou-huan-cun-dao-nei-cun-zhong"><span class="nav-number">5.1.3.4.</span> <span class="nav-text">cache : 让数据在第一个epoch后缓存到内存中</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#map-zhuan-huan-shi-xian-batch-ran-hou-cai-yong-xiang-liang-hua-de-zhuan-huan-fang-fa-dui-mei-ge-batch-jin-xing-zhuan-huan"><span class="nav-number">5.1.3.5.</span> <span class="nav-text">map转换时，先batch, 然后采用向量化的转换方法对每个batch进行转换</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#te-zheng-lie-feature-column"><span class="nav-number">5.2.</span> <span class="nav-text">特征列feature_column</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#te-zheng-lie-yong-fa-gai-shu"><span class="nav-number">5.2.1.</span> <span class="nav-text">特征列用法概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#te-zheng-lie-shi-yong-fan-li"><span class="nav-number">5.2.2.</span> <span class="nav-text">特征列使用范例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#gou-jian-shu-ju-guan-dao-1"><span class="nav-number">5.2.2.1.</span> <span class="nav-text">构建数据管道</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ding-yi-te-zheng-lie"><span class="nav-number">5.2.2.2.</span> <span class="nav-text">定义特征列</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ding-yi-mo-xing-10"><span class="nav-number">5.2.2.3.</span> <span class="nav-text">定义模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#xun-lian-mo-xing-9"><span class="nav-number">5.2.2.4.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ping-gu-mo-xing-4"><span class="nav-number">5.2.2.5.</span> <span class="nav-text">评估模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ji-huo-han-shu-activation"><span class="nav-number">5.3.</span> <span class="nav-text">激活函数activation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#chang-yong-ji-huo-han-shu"><span class="nav-number">5.3.1.</span> <span class="nav-text">常用激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-nn-sigmoid"><span class="nav-number">5.3.1.1.</span> <span class="nav-text">tf.nn.sigmoid</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-nn-softmax"><span class="nav-number">5.3.1.2.</span> <span class="nav-text">tf.nn.softmax</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-nn-tanh"><span class="nav-number">5.3.1.3.</span> <span class="nav-text">tf.nn.tanh</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-nn-relu"><span class="nav-number">5.3.1.4.</span> <span class="nav-text">tf.nn.relu</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-nn-leaky-relu"><span class="nav-number">5.3.1.5.</span> <span class="nav-text">tf.nn.leaky_relu</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-nn-elu"><span class="nav-number">5.3.1.6.</span> <span class="nav-text">tf.nn.elu</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-nn-selu"><span class="nav-number">5.3.1.7.</span> <span class="nav-text">tf.nn.selu</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-nn-swish"><span class="nav-number">5.3.1.8.</span> <span class="nav-text">tf.nn.swish</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#gelu"><span class="nav-number">5.3.1.9.</span> <span class="nav-text">gelu</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zai-mo-xing-zhong-shi-yong-ji-huo-han-shu"><span class="nav-number">5.3.2.</span> <span class="nav-text">在模型中使用激活函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mo-xing-ceng-layers"><span class="nav-number">5.4.</span> <span class="nav-text">模型层layers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#nei-zhi-mo-xing-ceng"><span class="nav-number">5.4.1.</span> <span class="nav-text">内置模型层</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ji-chu-ceng"><span class="nav-number">5.4.1.1.</span> <span class="nav-text">基础层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#juan-ji-wang-luo-xiang-guan-ceng"><span class="nav-number">5.4.1.2.</span> <span class="nav-text">卷积网络相关层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#xun-huan-wang-luo-xiang-guan-ceng"><span class="nav-number">5.4.1.3.</span> <span class="nav-text">循环网络相关层</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zi-ding-yi-mo-xing-ceng"><span class="nav-number">5.4.2.</span> <span class="nav-text">自定义模型层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#chu-shi-hua-mo-xing-can-shu"><span class="nav-number">5.4.3.</span> <span class="nav-text">初始化模型参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sun-shi-han-shu-losses"><span class="nav-number">5.5.</span> <span class="nav-text">损失函数losses</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sun-shi-han-shu-he-zheng-ze-hua-xiang"><span class="nav-number">5.5.1.</span> <span class="nav-text">损失函数和正则化项</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nei-zhi-sun-shi-han-shu"><span class="nav-number">5.5.2.</span> <span class="nav-text">内置损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zi-ding-yi-sun-shi-han-shu"><span class="nav-number">5.5.3.</span> <span class="nav-text">自定义损失函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ping-gu-zhi-biao-metrics"><span class="nav-number">5.6.</span> <span class="nav-text">评估指标metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#chang-yong-de-nei-zhi-ping-gu-zhi-biao"><span class="nav-number">5.6.1.</span> <span class="nav-text">常用的内置评估指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zi-ding-yi-ping-gu-zhi-biao"><span class="nav-number">5.6.2.</span> <span class="nav-text">自定义评估指标</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#you-hua-qi-optimizers"><span class="nav-number">5.7.</span> <span class="nav-text">优化器optimizers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#you-hua-qi-de-shi-yong"><span class="nav-number">5.7.1.</span> <span class="nav-text">优化器的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nei-zhi-you-hua-qi"><span class="nav-number">5.7.2.</span> <span class="nav-text">内置优化器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hui-diao-han-shu-callbacks"><span class="nav-number">5.8.</span> <span class="nav-text">回调函数callbacks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#nei-zhi-hui-diao-han-shu"><span class="nav-number">5.8.1.</span> <span class="nav-text">内置回调函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zi-ding-yi-hui-diao-han-shu"><span class="nav-number">5.8.2.</span> <span class="nav-text">自定义回调函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#tensor-flow-de-gao-jie-api"><span class="nav-number">6.</span> <span class="nav-text">TensorFlow的高阶API</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#gou-jian-mo-xing-de-3-chong-fang-fa"><span class="nav-number">6.1.</span> <span class="nav-text">构建模型的3种方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sequential-an-ceng-shun-xu-chuang-jian-mo-xing"><span class="nav-number">6.1.1.</span> <span class="nav-text">Sequential按层顺序创建模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#han-shu-shi-api-chuang-jian-ren-yi-jie-gou-mo-xing"><span class="nav-number">6.1.2.</span> <span class="nav-text">函数式API创建任意结构模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-zi-lei-hua-chuang-jian-zi-ding-yi-mo-xing"><span class="nav-number">6.1.3.</span> <span class="nav-text">Model子类化创建自定义模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xun-lian-mo-xing-de-3-chong-fang-fa"><span class="nav-number">6.2.</span> <span class="nav-text">训练模型的3种方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#nei-zhi-fit-fang-fa"><span class="nav-number">6.2.1.</span> <span class="nav-text">内置fit方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nei-zhi-train-on-batch-fang-fa"><span class="nav-number">6.2.2.</span> <span class="nav-text">内置train_on_batch方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zi-ding-yi-xun-lian-xun-huan"><span class="nav-number">6.2.3.</span> <span class="nav-text">自定义训练循环</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shi-yong-dan-gpu-xun-lian-mo-xing"><span class="nav-number">6.3.</span> <span class="nav-text">使用单GPU训练模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#gpu-she-zhi"><span class="nav-number">6.3.1.</span> <span class="nav-text">GPU设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zhun-bei-shu-ju-10"><span class="nav-number">6.3.2.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ding-yi-mo-xing-11"><span class="nav-number">6.3.3.</span> <span class="nav-text">定义模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xun-lian-mo-xing-10"><span class="nav-number">6.3.4.</span> <span class="nav-text">训练模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shi-yong-duo-gpu-xun-lian-mo-xing"><span class="nav-number">6.4.</span> <span class="nav-text">使用多GPU训练模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#zhun-bei-shu-ju-11"><span class="nav-number">6.4.1.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ding-yi-mo-xing-12"><span class="nav-number">6.4.2.</span> <span class="nav-text">定义模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xun-lian-mo-xing-11"><span class="nav-number">6.4.3.</span> <span class="nav-text">训练模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shi-yong-tpu-xun-lian-mo-xing"><span class="nav-number">6.5.</span> <span class="nav-text">使用TPU训练模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#zhun-bei-shu-ju-12"><span class="nav-number">6.5.1.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ding-yi-mo-xing-13"><span class="nav-number">6.5.2.</span> <span class="nav-text">定义模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xun-lian-mo-xing-12"><span class="nav-number">6.5.3.</span> <span class="nav-text">训练模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shi-yong-tensorflow-serving-bu-shu-mo-xing"><span class="nav-number">6.6.</span> <span class="nav-text">使用tensorflow-serving部署模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tensorflow-serving-mo-xing-bu-shu-gai-shu"><span class="nav-number">6.6.1.</span> <span class="nav-text">tensorflow serving模型部署概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zhun-bei-protobuf-mo-xing-wen-jian"><span class="nav-number">6.6.2.</span> <span class="nav-text">准备protobuf模型文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#an-zhuang-tensorflow-serving"><span class="nav-number">6.6.3.</span> <span class="nav-text">安装 tensorflow serving</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#qi-dong-tensorflow-serving-fu-wu"><span class="nav-number">6.6.4.</span> <span class="nav-text">启动 tensorflow serving 服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xiang-api-fu-wu-fa-song-qing-qiu"><span class="nav-number">6.6.5.</span> <span class="nav-text">向API服务发送请求</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shi-yong-spark-scala-diao-yong-tensorflow-2-0-xun-lian-hao-de-mo-xing"><span class="nav-number">6.7.</span> <span class="nav-text">使用spark-scala调用tensorflow2.0训练好的模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#spark-scala-diao-yong-tensorflow-mo-xing-gai-shu"><span class="nav-number">6.7.1.</span> <span class="nav-text">spark-scala调用tensorflow模型概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zhun-bei-protobuf-mo-xing-wen-jian-1"><span class="nav-number">6.7.2.</span> <span class="nav-text">准备protobuf模型文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#chuang-jian-spark-scala-xiang-mu-zai-xiang-mu-zhong-tian-jia-java-ban-ben-de-tensorflow-dui-ying-de-jar-bao-yi-lai"><span class="nav-number">6.7.3.</span> <span class="nav-text">创建spark(scala)项目，在项目中添加java版本的tensorflow对应的jar包依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zai-spark-scala-xiang-mu-zhong-driver-duan-jia-zai-tensorflow-mo-xing-diao-shi-cheng-gong"><span class="nav-number">6.7.4.</span> <span class="nav-text">在spark(scala)项目中driver端加载tensorflow模型调试成功</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zai-spark-scala-xiang-mu-zhong-tong-guo-rdd-zai-executor-shang-jia-zai-tensorflow-mo-xing-diao-shi-cheng-gong"><span class="nav-number">6.7.5.</span> <span class="nav-text">在spark(scala)项目中通过RDD在executor上加载tensorflow模型调试成功</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zai-spark-scala-xiang-mu-zhong-tong-guo-data-frame-zai-executor-shang-jia-zai-tensorflow-mo-xing-diao-shi-cheng-gong"><span class="nav-number">6.7.6.</span> <span class="nav-text">在spark(scala)项目中通过DataFrame在executor上加载tensorflow模型调试成功</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#can-kao"><span class="nav-number">7.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <a href='/'>
    <img class="site-author-image" itemprop="image" alt="Li Zhen"
      src="/images/snoppy.jpeg">
  </a>
  <p class="site-author-name" itemprop="name">Li Zhen</p>
  <div class="site-description" itemprop="description">他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">92</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">67</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jeffery0628" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jeffery0628" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jeffery.lee.0628@gmail.com" title="邮箱 → mailto:jeffery.lee.0628@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮箱</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Zhen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">1.3m</span>
</div>

        
<div class="busuanzi-count">
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.1.0/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  






  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>


 

<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180101,"YYYYMMDD"));
    ages = ages.replace(/years?/, "年");
    ages = ages.replace(/months?/, "月");
    ages = ages.replace(/days?/, "天");
    ages = ages.replace(/hours?/, "小时");
    ages = ages.replace(/minutes?/, "分");
    ages = ages.replace(/seconds?/, "秒");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `我已在此等候你 ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


<script src="https://cdn.jsdelivr.net/npm/sweetalert@2.1.2/dist/sweetalert.min.js"></script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'tChTbEIggDSVcdhPBUf0MFxW-gzGzoHsz',
      appKey     : 'sJ6xUiFnifEDIIfnj3Ut9zy1',
      placeholder: "留下你的小脚印吧！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '8' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
