<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/snoppy.jpeg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jeffery0628.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":true,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="ç®€ä»‹è‡ª2017å¹´æå‡ºä»¥æ¥ï¼ŒTransformeråœ¨ä¼—å¤šè‡ªç„¶è¯­è¨€å¤„ç†é—®é¢˜ä¸­å–å¾—äº†éå¸¸å¥½çš„æ•ˆæœã€‚å®ƒä¸ä½†è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œè€Œä¸”æ›´é€‚åˆå»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œå› æ­¤å¤§æœ‰å–ä»£å¾ªç¯æˆ–å·ç§¯ç¥ç»ç½‘ç»œï¼Œä¸€ç»Ÿè‡ªç„¶è¯­è¨€å¤„ç†çš„æ·±åº¦æ¨¡å‹æ±Ÿæ¹–ä¹‹åŠ¿ã€‚æœ¬æ–‡ç»“åˆã€ŠAttention is all you needã€‹è®ºæ–‡ä¸Harvardçš„ä»£ç ã€ŠAnnotated Transformerã€‹æ·±å…¥ç†è§£transformeræ¨¡å‹ã€‚">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer">
<meta property="og:url" content="https://jeffery0628.github.io/2020/05/11/transformer/index.html">
<meta property="og:site_name" content="ç«ç§2å·">
<meta property="og:description" content="ç®€ä»‹è‡ª2017å¹´æå‡ºä»¥æ¥ï¼ŒTransformeråœ¨ä¼—å¤šè‡ªç„¶è¯­è¨€å¤„ç†é—®é¢˜ä¸­å–å¾—äº†éå¸¸å¥½çš„æ•ˆæœã€‚å®ƒä¸ä½†è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œè€Œä¸”æ›´é€‚åˆå»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œå› æ­¤å¤§æœ‰å–ä»£å¾ªç¯æˆ–å·ç§¯ç¥ç»ç½‘ç»œï¼Œä¸€ç»Ÿè‡ªç„¶è¯­è¨€å¤„ç†çš„æ·±åº¦æ¨¡å‹æ±Ÿæ¹–ä¹‹åŠ¿ã€‚æœ¬æ–‡ç»“åˆã€ŠAttention is all you needã€‹è®ºæ–‡ä¸Harvardçš„ä»£ç ã€ŠAnnotated Transformerã€‹æ·±å…¥ç†è§£transformeræ¨¡å‹ã€‚">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/transformer.jpeg">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/over_all.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/position_embedding.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/encoder_layer.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/qkv.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/attention.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/attention_1.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/multi_head.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/decoder.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/decoder_layer.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/subsequent_mask.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/warm_up.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/label_smooth.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/penate.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/encoder_layer_2.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/encoder_layer_4.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/image-20200511115252714.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/image-20200511120009561.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/image-20200511120705604.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/image-20200511120927309.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/image-20200511120946538.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/image-20200511121002344.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/11/transformer/image-20200511121021648.png">
<meta property="article:published_time" content="2020-05-11T05:02:03.000Z">
<meta property="article:modified_time" content="2020-05-11T06:56:31.977Z">
<meta property="article:author" content="Li Zhen">
<meta property="article:tag" content="transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jeffery0628.github.io/2020/05/11/transformer/transformer.jpeg">

<link rel="canonical" href="https://jeffery0628.github.io/2020/05/11/transformer/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Transformer | ç«ç§2å·</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ ">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">ç«ç§2å·</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">ç«ç§è®¡åˆ’</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>å…³äº</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>æ ‡ç­¾</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>åˆ†ç±»</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>è¯»ä¹¦</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-video fa-fw"></i>ç”µå½±</a>

  </li>
        <li class="menu-item menu-item-games">

    <a href="/games/" rel="section"><i class="fa fa-gamepad fa-fw"></i>æ¸¸æˆ</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>æœç´¢
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="æœç´¢..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>



</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jeffery0628.github.io/2020/05/11/transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/snoppy.jpeg">
      <meta itemprop="name" content="Li Zhen">
      <meta itemprop="description" content="ä½†è¡Œå¥½äº‹ï¼Œè«é—®å‰ç¨‹ï¼">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ç«ç§2å·">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Transformer
        </h1>

        <div class="post-meta">
          
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2020-05-11 14:56:31" itemprop="dateModified" datetime="2020-05-11T14:56:31+08:00">2020-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">æŠ€æœ¯/è‡ªç„¶è¯­è¨€å¤„ç†</span></a>
                </span>
            </span>

          
            <span id="/2020/05/11/transformer/" class="post-meta-item leancloud_visitors" data-flag-title="Transformer" title="é˜…è¯»æ¬¡æ•°">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">é˜…è¯»æ¬¡æ•°ï¼š</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valineï¼š</span>
    
    <a title="valine" href="/2020/05/11/transformer/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/05/11/transformer/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="æœ¬æ–‡å­—æ•°">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">æœ¬æ–‡å­—æ•°ï¼š</span>
              <span>24k</span>
            </span>
            <span class="post-meta-item" title="é˜…è¯»æ—¶é•¿">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">é˜…è¯»æ—¶é•¿ &asymp;</span>
              <span>21 åˆ†é’Ÿ</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/2020/05/11/transformer/transformer.jpeg" alt></p>
<h1 id="ç®€ä»‹"><a href="#ç®€ä»‹" class="headerlink" title="ç®€ä»‹"></a>ç®€ä»‹</h1><p>è‡ª2017å¹´æå‡ºä»¥æ¥ï¼ŒTransformeråœ¨ä¼—å¤šè‡ªç„¶è¯­è¨€å¤„ç†é—®é¢˜ä¸­å–å¾—äº†éå¸¸å¥½çš„æ•ˆæœã€‚å®ƒä¸ä½†è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œè€Œä¸”æ›´é€‚åˆå»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œå› æ­¤å¤§æœ‰å–ä»£å¾ªç¯æˆ–å·ç§¯ç¥ç»ç½‘ç»œï¼Œä¸€ç»Ÿè‡ªç„¶è¯­è¨€å¤„ç†çš„æ·±åº¦æ¨¡å‹æ±Ÿæ¹–ä¹‹åŠ¿ã€‚æœ¬æ–‡ç»“åˆ<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">ã€ŠAttention is all you needã€‹</a>è®ºæ–‡ä¸Harvardçš„ä»£ç <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">ã€ŠAnnotated Transformerã€‹</a>æ·±å…¥ç†è§£transformeræ¨¡å‹ã€‚ </p>
<a id="more"></a>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>Transformerçš„æ•´ä½“ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨Encoderå’ŒDecoderä¸­éƒ½ä½¿ç”¨äº†Self-attention, Point-wiseå’Œå…¨è¿æ¥å±‚ã€‚Encoderå’Œdecoderçš„å¤§è‡´ç»“æ„åˆ†åˆ«å¦‚ä¸‹å›¾çš„å·¦åŠéƒ¨åˆ†å’Œå³åŠéƒ¨åˆ†æ‰€ç¤ºã€‚</p>
<p><img src="/2020/05/11/transformer/over_all.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Helper: Construct a model from hyperparameters.</span></span><br><span class="line"><span class="string">    src_vocab: è¾“å…¥è¯æ±‡è¡¨å¤§å°</span></span><br><span class="line"><span class="string">    tgt_vocab:è¾“å‡ºè¯æ±‡è¡¨å¤§å°</span></span><br><span class="line"><span class="string">    Nï¼šå †å ä¸ªæ•°</span></span><br><span class="line"><span class="string">    d_model:embedding_size</span></span><br><span class="line"><span class="string">    d_ff: çº¿å½¢å±‚è¾“å‡ºç»´åº¦</span></span><br><span class="line"><span class="string">    h:multi-headä¸­ head çš„ä¸ªæ•°</span></span><br><span class="line"><span class="string">    dropoutï¼š</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N), <span class="comment"># encoder</span></span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N), <span class="comment"># decoder</span></span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)), <span class="comment"># src_embed</span></span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)), <span class="comment"># tgt_embed</span></span><br><span class="line">        Generator(d_model, tgt_vocab) <span class="comment"># generator</span></span><br><span class="line">    		)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>transformerçš„è¾“å…¥æ˜¯<strong>Word Embedding + Position Embedding</strong>ã€‚</p>
<h3 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h3><p>Word embeddingåœ¨pytorchä¸­é€šå¸¸ç”¨ nn.Embedding å®ç°ï¼Œå…¶æƒé‡çŸ©é˜µé€šå¸¸æœ‰ä¸¤ç§é€‰æ‹©ï¼š</p>
<ol>
<li>ä½¿ç”¨ Pre-trainedçš„<strong>Embeddingså¹¶freeze</strong>ï¼Œè¿™ç§æƒ…å†µä¸‹å®é™…å°±æ˜¯ä¸€ä¸ª Lookup Tableã€‚</li>
<li>å¯¹å…¶è¿›è¡Œéšæœºåˆå§‹åŒ–(å½“ç„¶ä¹Ÿå¯ä»¥é€‰æ‹© Pre-trained çš„ç»“æœ)ï¼Œä½†<strong>è®¾ä¸º Trainable</strong>ã€‚è¿™æ ·åœ¨ training è¿‡ç¨‹ä¸­ä¸æ–­åœ°å¯¹ Embeddings è¿›è¡Œæ”¹è¿›ã€‚</li>
</ol>
<p>transformeré€‰æ‹©åè€…ï¼Œä»£ç å®ç°å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><figcaption><span>word_embedding</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model) <span class="comment"># vocab è¯æ±‡è¡¨å¤§å°</span></span><br><span class="line">        self.d_model = d_model  <span class="comment">#è¡¨ç¤ºembeddingçš„ç»´åº¦</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure>
<h3 id="Positional-Embedding"><a href="#Positional-Embedding" class="headerlink" title="Positional Embedding"></a>Positional Embedding</h3><p>åœ¨RNNä¸­ï¼Œå¯¹å¥å­çš„å¤„ç†æ˜¯ä¸€ä¸ªä¸ªwordæŒ‰é¡ºåºè¾“å…¥çš„ã€‚ä½†åœ¨ Transformer ä¸­ï¼Œè¾“å…¥å¥å­çš„æ‰€æœ‰wordæ˜¯åŒæ—¶å¤„ç†çš„ï¼Œæ²¡æœ‰è€ƒè™‘è¯çš„æ’åºå’Œä½ç½®ä¿¡æ¯ã€‚å› æ­¤ï¼ŒTransformer çš„ä½œè€…æå‡ºäº†åŠ å…¥ <code>positional encoding</code>çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚<code>positional encoding</code>ä½¿å¾— Transformer å¯ä»¥è¡¡é‡ word ä½ç½®æœ‰å…³çš„ä¿¡æ¯ã€‚</p>
<p><strong>å¦‚ä½•å®ç°å…·æœ‰ä½ç½®ä¿¡æ¯çš„encodingï¼Ÿ</strong></p>
<p>ä½œè€…æä¾›äº†ä¸¤ç§æ€è·¯ï¼š</p>
<ul>
<li>é€šè¿‡è®­ç»ƒå­¦ä¹  positional encoding å‘é‡ï¼›</li>
<li>ä½¿ç”¨å…¬å¼æ¥è®¡ç®— positional encodingå‘é‡ã€‚</li>
</ul>
<p>è¯•éªŒåå‘ç°ä¸¤ç§é€‰æ‹©çš„ç»“æœæ˜¯ç›¸ä¼¼çš„ï¼Œæ‰€ä»¥é‡‡ç”¨äº†ç¬¬2ç§æ–¹æ³•ï¼Œä¼˜ç‚¹æ˜¯ä¸éœ€è¦è®­ç»ƒå‚æ•°ï¼Œè€Œä¸”å³ä½¿<strong>åœ¨è®­ç»ƒé›†ä¸­æ²¡æœ‰å‡ºç°è¿‡çš„å¥å­é•¿åº¦ä¸Šä¹Ÿèƒ½ç”¨</strong>ã€‚Positional Encodingçš„å…¬å¼å¦‚ä¸‹ï¼š</p>
<script type="math/tex; mode=display">
\begin{aligned}
P E_{(p o s, 2 i)} &=\sin \left(p o s / 10000^{2 i / d_{\text {model }}}\right) \\
P E_{(p o s, 2 i+1)} &=\cos \left(p o s / 10000^{2 i / d_{\text {model }}}\right.
\end{aligned}</script><p>å…¶ä¸­ï¼Œ$pos$æŒ‡çš„æ˜¯è¿™ä¸ª word åœ¨è¿™ä¸ªå¥å­ä¸­çš„ä½ç½®ï¼›$2i$æŒ‡çš„æ˜¯ embedding è¯å‘é‡çš„å¶æ•°ç»´åº¦ï¼Œ$2i+1$æŒ‡çš„æ˜¯embedding è¯å‘é‡çš„å¥‡æ•°ç»´åº¦ã€‚å…·ä½“å®ç°å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><figcaption><span>PositionalEncoding</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>) <span class="comment"># [max_len, 1]</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * -(math.log(<span class="number">10000.0</span>) / d_model)) </span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term) <span class="comment"># å¶æ•°åˆ—</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term) <span class="comment"># å¥‡æ•°åˆ—</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)         <span class="comment"># [1, max_len, d_model]</span></span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe) <span class="comment"># ä¸Šè¿°ä»£ç åªéœ€è®¡ç®—ä¸€æ¬¡ï¼Œç„¶åæ”¾åˆ°å¯„å­˜å™¨é‡Œï¼Œéšç”¨éšå–ã€‚</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x) <span class="comment"># æ®è¯´dropout=0.1</span></span><br></pre></td></tr></table></figure>
<p><code>x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)</code> è¿™è¡Œä»£ç è¡¨ç¤ºï¼›è¾“å…¥æ¨¡å‹çš„æ•´ä¸ªEmbeddingæ˜¯Word Embeddingä¸Positional Embeddingç›´æ¥ç›¸åŠ ä¹‹åçš„ç»“æœã€‚</p>
<p>ä¸ºä»€ä¹ˆä¸Šé¢çš„ä¸¤ä¸ªå…¬å¼èƒ½ä½“ç°å•è¯çš„ç›¸å¯¹ä½ç½®ä¿¡æ¯å‘¢ï¼Ÿ</p>
<p>ä¸‹é¢ä¸€æ®µä»£ç å–è¯å‘é‡çš„4ä¸ªç»´åº¦ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># åœ¨ä½ç½®ç¼–ç ä¸‹æ–¹ï¼Œå°†åŸºäºä½ç½®æ·»åŠ æ­£å¼¦æ³¢ã€‚å¯¹äºæ¯ä¸ªç»´åº¦ï¼Œæ³¢çš„é¢‘ç‡å’Œåç§»éƒ½ä¸åŒã€‚</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line">y = pe.forward(Variable(torch.zeros(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>))) <span class="comment"># [bs=1,seq_len=100,embed_size=20]</span></span><br><span class="line">plt.plot(np.arange(<span class="number">100</span>), y[<span class="number">0</span>, :, <span class="number">4</span>:<span class="number">8</span>].data.numpy())</span><br><span class="line">plt.legend([<span class="string">"dim %d"</span> %p <span class="keyword">for</span> p <span class="keyword">in</span> [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/11/transformer/position_embedding.png" alt></p>
<p>å¯ä»¥çœ‹åˆ°æŸä¸ªåºåˆ—ä¸­ä¸åŒä½ç½®çš„å•è¯ï¼Œåœ¨æŸä¸€ç»´åº¦ä¸Šçš„ä½ç½®ç¼–ç æ•°å€¼ä¸ä¸€æ ·ï¼Œå³åŒä¸€åºåˆ—çš„ä¸åŒå•è¯åœ¨å•ä¸ªçº¬åº¦ç¬¦åˆæŸä¸ªæ­£å¼¦æˆ–è€…ä½™å¼¦ï¼Œå¯è®¤ä¸ºä»–ä»¬çš„å…·æœ‰ç›¸å¯¹å…³ç³»ã€‚</p>
<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>Encoderéƒ¨åˆ†æ˜¯ç”±ä¸ªå±‚ç›¸åŒå°Encoder Layerä¸²è”è€Œæˆã€‚å°Encoder Layerå¯ä»¥ç®€åŒ–ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼š</p>
<ol>
<li>Multi-Head Self Attention</li>
<li>Feed-Forward Network</li>
</ol>
<p><code>Multi-Head Self Attention</code> å’Œ<code>Feed-Forward Network</code>ä¹‹åéƒ½æ¥äº†ä¸€å±‚<code>Add</code> å’Œ<code>Norm</code></p>
<p>ç¤ºæ„å›¾å¦‚ä¸‹:</p>
<p><img src="/2020/05/11/transformer/encoder_layer.png" alt></p>
<h3 id="Muti-Head-Attention"><a href="#Muti-Head-Attention" class="headerlink" title="Muti-Head-Attention"></a>Muti-Head-Attention</h3><p>Multi-Head Self Attention å®é™…ä¸Šæ˜¯<strong>ç”±hä¸ªSelf Attention å±‚å¹¶è¡Œç»„æˆï¼ŒåŸæ–‡ä¸­h=8</strong>ã€‚</p>
<h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h4><p>self-attentionçš„è¾“å…¥æ˜¯åºåˆ—è¯å‘é‡<code>x</code>ã€‚<code>x</code>ç»è¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢å¾—åˆ°<code>query(Q)</code>, <code>x</code>ç»è¿‡ç¬¬äºŒä¸ªçº¿æ€§å˜æ¢å¾—åˆ°<code>key(K)</code>,<code>x</code>ç»è¿‡ç¬¬ä¸‰ä¸ªçº¿æ€§å˜æ¢å¾—åˆ°<code>value(V)</code>ã€‚ä¹Ÿå°±æ˜¯ï¼š</p>
<ul>
<li>Q = linear_q(x)</li>
<li>K = linear_k(x)</li>
<li>V = linear_v(x)</li>
</ul>
<p>å³ï¼š</p>
<p><img src="/2020/05/11/transformer/qkv.png" alt></p>
<p>linear_k, linear_q, linear_væ˜¯ç›¸äº’ç‹¬ç«‹ã€æƒé‡$ğ‘Š^ğ‘„,ğ‘Š^ğ¾,W^V$)æ˜¯ä¸åŒçš„ï¼Œé€šè¿‡è®­ç»ƒå¯å¾—åˆ°ã€‚å¾—åˆ°query(Q)ï¼Œkey(K)ï¼Œvalue(V)ä¹‹åæŒ‰ç…§ä¸‹é¢çš„å…¬å¼è®¡ç®—attention(Q, K, V)ï¼š</p>
<script type="math/tex; mode=display">
\text {Attention}(Q, K, V)=\text {Softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><p><img src="/2020/05/11/transformer/attention.png" alt></p>
<p>è¿™é‡Œ<code>Z</code>å°±æ˜¯<code>attention(Q, K, V)</code>ï¼Œ$ğ‘‘_ğ‘˜=\frac{ğ‘‘_{ğ‘šğ‘œğ‘‘ğ‘’ğ‘™}}{â„}=\frac{512}{8}=64$ã€‚</p>
<ol>
<li><p>ä¸ºä»€ä¹ˆè¦ç”¨$\sqrt{d_k}$ å¯¹ $ğ‘„ğ¾^ğ‘‡$è¿›è¡Œç¼©æ”¾å‘¢ï¼Ÿ</p>
<p>$d_k$å®é™…ä¸Šæ˜¯<code>Q/K/V</code>çš„æœ€åä¸€ä¸ªç»´åº¦ï¼Œå½“$d_k$è¶Šå¤§ï¼Œ$QK^T$å°±è¶Šå¤§ï¼Œå¯èƒ½ä¼šå°†softmaxå‡½æ•°æ¨å…¥æ¢¯åº¦æå°çš„åŒºåŸŸã€‚</p>
</li>
<li><p>softmaxä¹‹åå€¼éƒ½ä»‹äº0åˆ°1ä¹‹é—´ï¼Œå¯ä»¥ç†è§£æˆå¾—åˆ°äº† attention weightsã€‚ç„¶ååŸºäºè¿™ä¸ª attention weights å¯¹ V æ±‚ weighted sum å€¼ Attention(Q, K, V)ã€‚ </p>
</li>
</ol>
<p>Multi-Head-Attention å°±æ˜¯å°†<code>embedding</code>ä¹‹åçš„XæŒ‰ç»´åº¦$ğ‘‘_{ğ‘šğ‘œğ‘‘ğ‘’ğ‘™}=512$ åˆ‡å‰²æˆ$â„=8$ä¸ªï¼Œåˆ†åˆ«åšself-attentionä¹‹åå†åˆå¹¶åœ¨ä¸€èµ·ã€‚</p>
<p><img src="/2020/05/11/transformer/attention_1.png" alt></p>
<figure class="highlight python"><figcaption><span>Attention</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute 'Scaled Dot Product Attention</span></span><br><span class="line"><span class="string">    qkv :[batch, h, seq_len, embed_size/h(d_k)]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)  <span class="comment"># mask </span></span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)     <span class="comment"># dropout=0.1</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/11/transformer/multi_head.png" alt></p>
<figure class="highlight python"><figcaption><span>MultiHeadedAttention</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        å®ç°MultiHeadedAttentionã€‚</span></span><br><span class="line"><span class="string">           è¾“å…¥çš„qï¼Œkï¼Œvæ˜¯å½¢çŠ¶ [batch, seq_len, embed_size(d_model)]ã€‚</span></span><br><span class="line"><span class="string">           è¾“å‡ºçš„x çš„å½¢çŠ¶åŒä¸Šã€‚</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        <span class="comment">#    [batch, seq_len, embed_size] -&gt;[batch, h, seq_len, embed_size/h]</span></span><br><span class="line">        query, key, value = [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch.</span></span><br><span class="line">        <span class="comment">#    è®¡ç®—æ³¨æ„åŠ›attn å¾—åˆ°attn*v ä¸attn </span></span><br><span class="line">        <span class="comment">#    qkv :[batch, h, seq_len, embed_size/h] --&gt;</span></span><br><span class="line">        <span class="comment">#              x:[batch, h, seq_len, embed_size/h], attn[batch, h, seq_len, seq_len]</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) "Concat" using a view and apply a final linear.</span></span><br><span class="line">        <span class="comment">#     ä¸Šä¸€æ­¥çš„ç»“æœåˆå¹¶åœ¨ä¸€èµ·è¿˜åŸæˆåŸå§‹è¾“å…¥åºåˆ—çš„å½¢çŠ¶</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)  <span class="comment"># æœ€åå†è¿‡ä¸€ä¸ªçº¿æ€§å±‚</span></span><br></pre></td></tr></table></figure>
<h4 id="Self-Attentionçš„ä¼˜ç‚¹"><a href="#Self-Attentionçš„ä¼˜ç‚¹" class="headerlink" title="Self-Attentionçš„ä¼˜ç‚¹"></a>Self-Attentionçš„ä¼˜ç‚¹</h4><ol>
<li>å› ä¸ºæ¯ä¸ªè¯éƒ½å’Œå‘¨å›´æ‰€æœ‰è¯åšattentionï¼Œæ‰€ä»¥ä»»æ„ä¸¤ä¸ªä½ç½®éƒ½ç›¸å½“äºæœ‰ç›´è¿çº¿è·¯ï¼Œå¯æ•è·é•¿è·ç¦»ä¾èµ–ã€‚</li>
<li>Attentionçš„å¯è§£é‡Šæ€§æ›´å¥½ï¼Œæ ¹æ®Attention scoreå¯ä»¥çŸ¥é“ä¸€ä¸ªè¯å’Œå“ªäº›è¯çš„å…³ç³»æ¯”è¾ƒå¤§ã€‚</li>
<li>æ˜“äºå¹¶è¡ŒåŒ–ï¼Œå½“å‰å±‚çš„Attentionè®¡ç®—åªå’Œå‰ä¸€å±‚çš„å€¼æœ‰å…³ï¼Œæ‰€ä»¥ä¸€å±‚çš„æ‰€æœ‰èŠ‚ç‚¹å¯å¹¶è¡Œæ‰§è¡Œself-attentionæ“ä½œã€‚è®¡ç®—æ•ˆç‡é«˜ï¼Œä¸€æ¬¡Self-Attentionåªéœ€è¦ä¸¤æ¬¡çŸ©é˜µè¿ç®—ï¼Œé€Ÿåº¦å¾ˆå¿«ã€‚</li>
</ol>
<h3 id="Add-amp-Norm"><a href="#Add-amp-Norm" class="headerlink" title="Add &amp; Norm"></a>Add &amp; Norm</h3><p><code>x</code> åºåˆ—ç»è¿‡<code>Multi-Head-Self-Attention</code> ä¹‹åå®é™…ç»è¿‡ä¸€ä¸ª<code>Add+Norm</code>å±‚ï¼Œå†è¿›å…¥<code>feed-forward network(FFN)</code>ï¼Œåœ¨<code>FFN</code>ä¹‹ååˆç»è¿‡ä¸€ä¸ª<code>norm</code>å†è¾“å…¥ä¸‹ä¸€ä¸ªencoder layerã€‚å‡ ä¹æ¯ä¸ªsub layerä¹‹åéƒ½ä¼šç»è¿‡ä¸€ä¸ªå½’ä¸€åŒ–ï¼Œç„¶åå†åŠ åœ¨åŸæ¥çš„è¾“å…¥ä¸Šã€‚</p>
<figure class="highlight python"><figcaption><span>Norm</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features)) </span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features)) </span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Add</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="string">"Apply residual connection to any sublayer with the same size."</span></span><br><span class="line">        <span class="comment"># sublayer &lt;-- encoder layer</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure>
<h3 id="Feed-Forward-Network"><a href="#Feed-Forward-Network" class="headerlink" title="Feed-Forward Network"></a>Feed-Forward Network</h3><p>Feed-Forward Networkå¯ä»¥ç»†åˆ†ä¸ºæœ‰ä¸¤å±‚ï¼Œç¬¬ä¸€å±‚æ˜¯ä¸€ä¸ªçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œç¬¬äºŒå±‚æ˜¯æ¿€æ´»å‡½æ•°æ˜¯ReLUã€‚å¯ä»¥è¡¨ç¤ºä¸ºï¼š</p>
<script type="math/tex; mode=display">
F F N=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}</script><figure class="highlight python"><figcaption><span>PositionwiseFeedForward</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Implements FFN equation.</span></span><br><span class="line"><span class="string">    positionwiseä½“ç°æ¯ä¸ªå•è¯ä¸Šåšlinearï¼Œå•è¯ä¹‹é—´å„ä¸ç›¸åŒ</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">      <span class="string">"""</span></span><br><span class="line"><span class="string">      d_model: embedding_size</span></span><br><span class="line"><span class="string">      d_ff: çº¿å½¢å±‚è¾“å‡ºç»´åº¦</span></span><br><span class="line"><span class="string">      """</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure>
<h3 id="ç»„åˆå‡ºEncoder"><a href="#ç»„åˆå‡ºEncoder" class="headerlink" title="ç»„åˆå‡ºEncoder"></a>ç»„åˆå‡ºEncoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span><span class="params">(module, N)</span>:</span></span><br><span class="line">    <span class="string">"Produce N identical layers."</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)  <span class="comment"># sub layer</span></span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Pass the input (and mask) through each layer in turn."</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<h3 id="å°ç»“"><a href="#å°ç»“" class="headerlink" title="å°ç»“"></a>å°ç»“</h3><p>æ€»çš„æ¥è¯´Encoder æ˜¯ç”±ä¸Šè¿°å°encoder layer 6ä¸ªä¸²è¡Œå åŠ ç»„æˆã€‚encoder sub layerä¸»è¦åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼š</p>
<ul>
<li>SubLayer-1 åš Multi-Headed Attention</li>
<li>SubLayer-2 åš Feed Forward Neural Network</li>
</ul>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>Decoderä¸Encoderæœ‰æ‰€ä¸åŒï¼ŒEncoderä¸Decoderçš„å…³ç³»å¯ä»¥ç”¨ä¸‹å›¾æè¿°ï¼š</p>
<p><img src="/2020/05/11/transformer/decoder.png" alt></p>
<figure class="highlight python"><figcaption><span>Decoder</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Generic N layer decoder with masking."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span> </span><br><span class="line">      	<span class="comment"># memory Encoderæœ€åçš„è¾“å‡ºã€‚ </span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<p>Decoder å­ç»“æ„ï¼ˆsub layerï¼‰ï¼š</p>
<p><img src="/2020/05/11/transformer/decoder_layer.png" alt></p>
<p>Decoder ä¹Ÿæ˜¯N=6å±‚å †å çš„ç»“æ„ã€‚è¢«åˆ†ä¸º3ä¸ª SubLayerï¼ŒEncoderä¸Decoderæœ‰ä¸‰å¤§ä¸»è¦çš„ä¸åŒï¼š</p>
<ol>
<li>Decoder SubLayer-1 ä½¿ç”¨çš„æ˜¯ â€œMaskedâ€ Multi-Headed Attention æœºåˆ¶ï¼Œé˜²æ­¢ä¸ºäº†æ¨¡å‹çœ‹åˆ°è¦é¢„æµ‹çš„æ•°æ®ï¼Œé˜²æ­¢æ³„éœ²ã€‚</li>
<li>SubLayer-2 æ˜¯ä¸€ä¸ª Encoder-Decoder Multi-head Attentionã€‚</li>
<li>LinearLayer å’Œ SoftmaxLayer ä½œç”¨äº SubLayer-3 çš„è¾“å‡ºåé¢ï¼Œæ¥é¢„æµ‹å¯¹åº”çš„ word çš„ probabilities ã€‚</li>
</ol>
<h3 id="Mask-Multi-Head-Attention"><a href="#Mask-Multi-Head-Attention" class="headerlink" title="Mask-Multi-Head-Attention"></a>Mask-Multi-Head-Attention</h3><p>Mask çš„ç›®çš„æ˜¯é˜²æ­¢ Decoder â€œseeing the futureâ€ï¼Œé˜²æ­¢æå‰çŸ¥é“é¢„æµ‹çš„å†…å®¹ï¼ˆensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$.ï¼‰,è¿™ä¹Ÿè¯´æ˜Transformeråªæ˜¯åœ¨Encoderé˜¶æ®µå¯ä»¥å¹¶è¡ŒåŒ–ï¼ŒDecoderé˜¶æ®µä¾ç„¶è¦ä¸€ä¸ªä¸ªè¯é¡ºåºç¿»è¯‘ï¼Œä¾ç„¶æ˜¯<strong>ä¸²è¡Œ</strong>çš„ã€‚è¿™é‡Œmaskæ˜¯ä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µï¼Œå¯¹è§’çº¿ä»¥åŠå¯¹è§’çº¿å·¦ä¸‹éƒ½æ˜¯1ï¼Œå…¶ä½™éƒ½æ˜¯0ã€‚</p>
<figure class="highlight python"><figcaption><span>subsequent_mask</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="string">""""Mask out subsequent positions.</span></span><br><span class="line"><span class="string">    maskåç»­çš„ä½ç½®ï¼Œè¿”å›[size, size]å°ºå¯¸ä¸‹ä¸‰è§’Tensor</span></span><br><span class="line"><span class="string">    å¯¹è§’çº¿åŠå…¶å·¦ä¸‹è§’å…¨æ˜¯1ï¼Œå³ä¸Šè§’å…¨æ˜¯0</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    <span class="comment"># np.triu()  Upper triangle of an array: è¿”å›çŸ©é˜µä¸Šä¸‰è§’</span></span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span> <span class="comment"># ç­‰äº0ï¼Œè¿”å›çš„æ˜¯çŸ©é˜µçš„ä¸‹ä¸‰è§’</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.imshow(subsequent_mask(<span class="number">20</span>)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>ç»“æœå¦‚ä¸‹ï¼š</p>
<p><img src="/2020/05/11/transformer/subsequent_mask.png" alt></p>
<p>subsequent_mask è¿”å›ç»“æœï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]], dtype=torch.uint8)</span><br></pre></td></tr></table></figure>
<p>å½“maskä¸ä¸ºç©ºçš„æ—¶å€™ï¼Œattentionè®¡ç®—éœ€è¦å°†xåšä¸€ä¸ªæ“ä½œï¼šscores = scores.masked_fill(mask == 0, -1e9)ã€‚å³å°†mask==0çš„æ›¿æ¢ä¸º-1e9,å…¶ä½™ä¸å˜ã€‚</p>
<h3 id="Encoder-Decoder-Multi-head-Attention"><a href="#Encoder-Decoder-Multi-head-Attention" class="headerlink" title="Encoder-Decoder Multi-head Attention"></a>Encoder-Decoder Multi-head Attention</h3><p>è¿™éƒ¨åˆ†å’ŒMulti-head Attentionçš„åŒºåˆ«æ˜¯è¯¥å±‚çš„è¾“å…¥æ¥è‡ªencoderå’Œä¸Šä¸€æ¬¡decoderçš„ç»“æœã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (right) for connections."</span></span><br><span class="line">      	<span class="comment"># å°†decoderçš„ä¸‰ä¸ªSublayerä¸²è”èµ·æ¥</span></span><br><span class="line">        m = memory <span class="comment"># ä¸ºencoder æœ€åçš„è¾“å‡º</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>
<h3 id="Linear-and-Softmax-to-Produce-Output-Probabilities"><a href="#Linear-and-Softmax-to-Produce-Output-Probabilities" class="headerlink" title="Linear and Softmax to Produce Output Probabilities"></a>Linear and Softmax to Produce Output Probabilities</h3><p>Decoderçš„æœ€åä¸€ä¸ªéƒ¨åˆ†æ˜¯è¿‡ä¸€ä¸ªlinear layerå°†decoderçš„è¾“å‡ºæ‰©å±•åˆ°ä¸vocabulary sizeä¸€æ ·çš„ç»´åº¦ä¸Šã€‚ç»è¿‡softmax åï¼Œé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ä¸€ä¸ªwordä½œä¸ºé¢„æµ‹ç»“æœã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå·²ç»è®­ç»ƒå¥½çš„ç½‘ç»œï¼Œåœ¨åšé¢„æµ‹æ—¶ï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š</p>
<ol>
<li>ç»™ decoder è¾“å…¥ encoder å¯¹æ•´ä¸ªå¥å­ embedding çš„ç»“æœå’Œä¸€ä¸ªç‰¹æ®Šçš„å¼€å§‹ç¬¦å· &lt;/s&gt;ã€‚decoder å°†äº§ç”Ÿé¢„æµ‹ï¼Œåœ¨ä¾‹å­ä¸­åº”è¯¥æ˜¯ <code>I</code>ã€‚</li>
<li>ç»™ decoder è¾“å…¥ encoder çš„ embedding ç»“æœå’Œ â€œ&lt;/s&gt; Iâ€ï¼Œåœ¨è¿™ä¸€æ­¥ decoder åº”è¯¥äº§ç”Ÿé¢„æµ‹ <code>am</code>ã€‚</li>
<li>ç»™ decoder è¾“å…¥ encoder çš„ embedding ç»“æœå’Œ â€œ&lt;/s&gt; I amâ€ï¼Œåœ¨è¿™ä¸€æ­¥ decoder åº”è¯¥äº§ç”Ÿé¢„æµ‹ <code>a</code>ã€‚</li>
<li>ç»™ decoder è¾“å…¥ encoder çš„ embedding ç»“æœå’Œ â€œ&lt;/s&gt; I am aâ€ï¼Œåœ¨è¿™ä¸€æ­¥ decoder åº”è¯¥äº§ç”Ÿé¢„æµ‹ <code>student</code>ã€‚</li>
<li>ç»™ decoder è¾“å…¥ encoder çš„ embedding ç»“æœå’Œ â€œ&lt;/s&gt;I am a studentâ€, decoderåº”è¯¥ç”Ÿæˆå¥å­ç»“å°¾çš„æ ‡è®°ï¼Œdecoder åº”è¯¥è¾“å‡º <code>&lt;/eos&gt;</code>ã€‚</li>
<li>ç„¶å decoder ç”Ÿæˆäº† &lt;/eos&gt;ï¼Œç¿»è¯‘å®Œæˆã€‚</li>
</ol>
<p>è¿™éƒ¨åˆ†çš„ä»£ç å®ç°ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Define standard linear + softmax generation step."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">    		<span class="string">"""</span></span><br><span class="line"><span class="string">    		d_model:decoder è¾“å‡ºçš„embedding size</span></span><br><span class="line"><span class="string">    		vocabï¼šç›®æ ‡çš„è¯æ±‡è¡¨å¤§å°</span></span><br><span class="line"><span class="string">    		"""</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="EncoderDecoder"><a href="#EncoderDecoder" class="headerlink" title="EncoderDecoder"></a>EncoderDecoder</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many </span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed <span class="comment"># embedding</span></span><br><span class="line">        self.tgt_embed = tgt_embed <span class="comment"># embedding</span></span><br><span class="line">        self.generator = generator <span class="comment"># ç”¨äºç”Ÿæˆç¿»è¯‘ç›®æ ‡ï¼ˆLinear + softmax --&gt; probï¼‰</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Take in and process masked src and target sequences."</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure>
<h2 id="Full-Model"><a href="#Full-Model" class="headerlink" title="Full Model"></a>Full Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout) <span class="comment"># linear(relu(linear(x)))</span></span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><p>è¿™éƒ¨åˆ†ä¸»è¦æ ¹æ®è‡ªå·±çš„ç†è§£å¯¹ä»£ç åŠ æ³¨é‡Š</p>
<h2 id="Batches-and-Masking"><a href="#Batches-and-Masking" class="headerlink" title="Batches and Masking"></a>Batches and Masking</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Batch</span>:</span></span><br><span class="line">    <span class="string">"Object for holding a batch of data with mask during training."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, src, trg=None, pad=<span class="number">0</span>)</span>:</span></span><br><span class="line">        self.src = src</span><br><span class="line">        self.src_mask = (src != pad).unsqueeze(<span class="number">-2</span>) <span class="comment"># å¯¹è¶…å‡ºå¥å­é•¿åº¦éƒ¨åˆ†mask</span></span><br><span class="line">        <span class="keyword">if</span> trg <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.trg = trg[:, :<span class="number">-1</span>]</span><br><span class="line">            self.trg_y = trg[:, <span class="number">1</span>:]</span><br><span class="line">            self.trg_mask = self.make_std_mask(self.trg, pad) <span class="comment"># mask to hide padding and future words</span></span><br><span class="line">            self.ntokens = (self.trg_y != pad).data.sum()</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_std_mask</span><span class="params">(tgt, pad)</span>:</span></span><br><span class="line">        <span class="string">"Create a mask to hide padding and future words."</span></span><br><span class="line">        tgt_mask = (tgt != pad).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">        tgt_mask = tgt_mask &amp; Variable(</span><br><span class="line">            subsequent_mask(tgt.size(<span class="number">-1</span>)).type_as(tgt_mask.data))</span><br><span class="line">        <span class="keyword">return</span> tgt_mask</span><br></pre></td></tr></table></figure>
<h2 id="Training-Loop"><a href="#Training-Loop" class="headerlink" title="Training Loop"></a>Training Loop</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(data_iter, model, loss_compute)</span>:</span></span><br><span class="line">    <span class="string">"Standard Training and Logging Function"</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    total_tokens = <span class="number">0</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    tokens = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(data_iter):</span><br><span class="line">        out = model.forward(batch.src, batch.trg, </span><br><span class="line">                            batch.src_mask, batch.trg_mask)</span><br><span class="line">        loss = loss_compute(out, batch.trg_y, batch.ntokens)</span><br><span class="line">        total_loss += loss</span><br><span class="line">        total_tokens += batch.ntokens</span><br><span class="line">        tokens += batch.ntokens</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">1</span>:</span><br><span class="line">            elapsed = time.time() - start</span><br><span class="line">            print(<span class="string">"Epoch Step: %d Loss: %f Tokens per Sec: %f"</span> %</span><br><span class="line">                    (i, loss / batch.ntokens, tokens / elapsed))</span><br><span class="line">            start = time.time()</span><br><span class="line">            tokens = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> total_loss / total_tokens</span><br></pre></td></tr></table></figure>
<h2 id="Training-Data"><a href="#Training-Data" class="headerlink" title="Training Data"></a>Training Data</h2><p>ä½¿ç”¨æ ‡å‡†WMT 2014è‹±è¯­-å¾·è¯­æ•°æ®é›†è¿›è¡Œäº†è®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤§çº¦450ä¸‡ä¸ªå¥å­å¯¹ã€‚ ä½¿ç”¨å­—èŠ‚å¯¹çš„ç¼–ç æ–¹æ³•å¯¹å¥å­è¿›è¡Œç¼–ç ï¼Œè¯¥ç¼–ç å…·æœ‰å¤§çº¦37000ä¸ªè¯çš„å…±äº«æº-ç›®æ ‡è¯æ±‡è¡¨ã€‚ å¯¹äºè‹±è¯­-æ³•è¯­ï¼Œä½¿ç”¨äº†WMT 2014 è‹±è¯­-æ³•è¯­æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”±36Mä¸ªå¥å­ç»„æˆï¼Œå¹¶å°†è¯åˆ†æˆ32000ä¸ªè¯ç‰‡(Word-piece)çš„è¯æ±‡è¡¨ã€‚å¥å­å¯¹æŒ‰ç…§è¿‘ä¼¼çš„åºåˆ—é•¿åº¦è¿›è¡Œæ‰¹å¤„ç†ã€‚æ¯ä¸ªè®­ç»ƒæ‰¹åŒ…å«ä¸€ç»„å¥å­å¯¹ï¼ŒåŒ…å«å¤§çº¦25000ä¸ªæºè¯å’Œ25000ä¸ªç›®æ ‡è¯ã€‚</p>
<p>ä½¿ç”¨torch textæ¥åˆ›å»ºbatchã€‚åœ¨torchtextçš„ä¸€ä¸ªå‡½æ•°ä¸­åˆ›å»ºbatchï¼Œç¡®ä¿å¡«å……åˆ°æœ€å¤§batchè®­ç»ƒé•¿åº¦çš„å¤§å°ä¸è¶…è¿‡é˜ˆå€¼</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">global</span> max_src_in_batch, max_tgt_in_batch</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_size_fn</span><span class="params">(new, count, sofar)</span>:</span></span><br><span class="line">    <span class="string">"Keep augmenting batch and calculate total number of tokens + padding."</span></span><br><span class="line">    <span class="keyword">global</span> max_src_in_batch, max_tgt_in_batch</span><br><span class="line">    <span class="keyword">if</span> count == <span class="number">1</span>: <span class="comment"># ï¼Ÿï¼Ÿ</span></span><br><span class="line">        max_src_in_batch = <span class="number">0</span></span><br><span class="line">        max_tgt_in_batch = <span class="number">0</span></span><br><span class="line">    max_src_in_batch = max(max_src_in_batch,  len(new.src))</span><br><span class="line">    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + <span class="number">2</span>)</span><br><span class="line">    src_elements = count * max_src_in_batch</span><br><span class="line">    tgt_elements = count * max_tgt_in_batch</span><br><span class="line">    <span class="keyword">return</span> max(src_elements, tgt_elements)</span><br></pre></td></tr></table></figure>
<h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><p>é€‰æ‹©Adamä½œä¸ºä¼˜åŒ–å™¨ï¼Œå…¶å‚æ•°ä¸º$\beta_1 = 0.9, \beta_2=0.98,\epsilon=10^{-9}$ã€‚æ ¹æ®$lrate = d_{model}^{- \frac{1}{2}} * min(step_num^{- \frac{1}{2}},step_num \cdot warmup_steps^{-1.5})$ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ”¹å˜äº†å­¦ä¹ ç‡ã€‚åœ¨<code>warm_up</code>ä¸­éšæ­¥æ•°çº¿æ€§åœ°å¢åŠ å­¦ä¹ é€Ÿç‡ï¼Œéšåä¸æ­¥æ•°çš„åå¹³æ–¹æ ¹æˆæ¯”ä¾‹åœ°å‡å°å®ƒã€‚é¢„çƒ­<code>warmup_steps</code>ä¸º4000ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NoamOpt</span>:</span></span><br><span class="line">    <span class="string">"Optim wrapper that implements rate."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model_size, factor, warmup, optimizer)</span>:</span></span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self._step = <span class="number">0</span></span><br><span class="line">        self.warmup = warmup</span><br><span class="line">        self.factor = factor</span><br><span class="line">        self.model_size = model_size</span><br><span class="line">        self._rate = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"Update parameters and rate"</span></span><br><span class="line">        self._step += <span class="number">1</span></span><br><span class="line">        rate = self.rate()</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.optimizer.param_groups:</span><br><span class="line">            p[<span class="string">'lr'</span>] = rate</span><br><span class="line">        self._rate = rate</span><br><span class="line">        self.optimizer.step()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rate</span><span class="params">(self, step = None)</span>:</span></span><br><span class="line">        <span class="string">"Implement `lrate` above"</span></span><br><span class="line">        <span class="keyword">if</span> step <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            step = self._step</span><br><span class="line">        <span class="comment"># å¯¹åº”ä¸Šé¢å…¬å¼</span></span><br><span class="line">        <span class="keyword">return</span> self.factor * (self.model_size ** (<span class="number">-0.5</span>) * min(step ** (<span class="number">-0.5</span>), step * self.warmup ** (<span class="number">-1.5</span>)))</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_std_opt</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">2</span>, <span class="number">4000</span>,torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">opts = [NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="literal">None</span>), </span><br><span class="line">        NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">8000</span>, <span class="literal">None</span>),</span><br><span class="line">        NoamOpt(<span class="number">256</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="literal">None</span>)]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">20000</span>), [[opt.rate(i) <span class="keyword">for</span> opt <span class="keyword">in</span> opts] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">20000</span>)])</span><br><span class="line">plt.legend([<span class="string">"512:4000"</span>, <span class="string">"512:8000"</span>, <span class="string">"256:4000"</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/11/transformer/warm_up.png" alt></p>
<p>ä¹Ÿå°±æ˜¯è¯´ï¼š</p>
<ol>
<li>åœ¨embedding sizeç›¸åŒçš„æƒ…å†µä¸‹ï¼Œwarm upæ­¥æ•°è¶Šå°‘ï¼Œå‰æœŸçš„å­¦ä¹ ç‡æ›²çº¿è¶Šé™¡å³­ï¼Œå­¦çš„è¶Šå¿«ã€‚</li>
<li>åœ¨warm upæ­¥æ•°ç›¸åŒçš„æ—¶å€™ï¼Œembedding size è¶Šå°ï¼Œå‰æœŸçš„å­¦ä¹ ç‡æ›²çº¿è¶Šé™¡å³­ï¼Œå­¦çš„è¶Šå¿«ã€‚</li>
</ol>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><h3 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h3><h4 id="èƒŒæ™¯ä»‹ç»"><a href="#èƒŒæ™¯ä»‹ç»" class="headerlink" title="èƒŒæ™¯ä»‹ç»"></a>èƒŒæ™¯ä»‹ç»</h4><p>åœ¨å¤šåˆ†ç±»è®­ç»ƒä»»åŠ¡ä¸­ï¼Œè¾“å…¥ç»è¿‡ç¥çº§ç½‘ç»œçš„è®¡ç®—ï¼Œä¼šå¾—åˆ°å½“å‰è¾“å…¥å¯¹åº”äºå„ä¸ªç±»åˆ«çš„ç½®ä¿¡åº¦åˆ†æ•°ï¼Œè¿™äº›åˆ†æ•°ä¼šè¢«softmaxè¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œæœ€ç»ˆå¾—åˆ°å½“å‰è¾“å…¥å±äºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚</p>
<script type="math/tex; mode=display">
q_{i}=\frac{\exp \left(z_{i}\right)}{\sum_{j=1}^{K} \exp \left(z_{j}\right)}</script><p>ä¹‹ååœ¨ä½¿ç”¨äº¤å‰ç†µå‡½æ•°æ¥è®¡ç®—æŸå¤±å€¼ï¼š</p>
<script type="math/tex; mode=display">
\begin{aligned}
&L o s s=-\sum_{i=1}^{K} p_{i} \log q_{i}\\
&p_{i}=\left\{\begin{array}{l}
1, \text { if }(i=y) \\
0, i f(i \neq y)
\end{array}\right.
\end{aligned}</script><p>å…¶ä¸­ï¼Œiè¡¨ç¤ºå¤šç±»ä¸­çš„æŸä¸€ç±»ã€‚</p>
<p>æœ€ç»ˆåœ¨è®­ç»ƒç½‘ç»œæ—¶ï¼Œæœ€å°åŒ–é¢„æµ‹æ¦‚ç‡å’Œæ ‡ç­¾çœŸå®æ¦‚ç‡çš„äº¤å‰ç†µï¼Œä»è€Œå¾—åˆ°æœ€ä¼˜çš„é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œä¸ºäº†è¾¾åˆ°æœ€å¥½çš„æ‹Ÿåˆæ•ˆæœï¼Œæœ€ä¼˜çš„é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒä¸ºï¼š</p>
<script type="math/tex; mode=display">
Z_{i}=\left\{\begin{array}{l}
+\infty, i f(i=y) \\
0, i f(i \neq y)
\end{array}\right.</script><p>ä¹Ÿå°±æ˜¯è¯´ï¼Œç½‘ç»œä¼šé©±ä½¿è‡ªèº«å¾€æ­£ç¡®æ ‡ç­¾å’Œé”™è¯¯æ ‡ç­¾å·®å€¼å¤§çš„æ–¹å‘å­¦ä¹ ï¼Œåœ¨è®­ç»ƒæ•°æ®ä¸è¶³ä»¥è¡¨å¾æ‰€ä»¥çš„æ ·æœ¬ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œå°±<strong>ä¼šå¯¼è‡´ç½‘ç»œè¿‡æ‹Ÿåˆ</strong>ã€‚</p>
<h4 id="label-smoothingåŸç†"><a href="#label-smoothingåŸç†" class="headerlink" title="label smoothingåŸç†"></a>label smoothingåŸç†</h4><p>label smoothingçš„æå‡ºå°±æ˜¯ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæ˜¯ä¸€ç§æ­£åˆ™åŒ–çš„ç­–ç•¥ã€‚å…¶é€šè¿‡â€è½¯åŒ–â€ä¼ ç»Ÿçš„one-hotç±»å‹æ ‡ç­¾ï¼Œä½¿å¾—åœ¨è®¡ç®—æŸå¤±å€¼æ—¶èƒ½å¤Ÿæœ‰æ•ˆæŠ‘åˆ¶è¿‡æ‹Ÿåˆç°è±¡ã€‚label smoothingç›¸å½“äºå‡å°‘çœŸå®æ ·æœ¬æ ‡ç­¾çš„ç±»åˆ«åœ¨è®¡ç®—æŸå¤±å‡½æ•°æ—¶çš„æƒé‡ï¼Œæœ€ç»ˆèµ·åˆ°æŠ‘åˆ¶è¿‡æ‹Ÿåˆçš„æ•ˆæœã€‚</p>
<p>1.label smoothingå°†çœŸå®æ¦‚ç‡åˆ†å¸ƒä½œå¦‚ä¸‹æ”¹å˜ï¼š</p>
<script type="math/tex; mode=display">
P_{i}=\left\{\begin{array}{l}
1, i f(i=y) \\
0, i f(i \neq y)
\end{array} \Longrightarrow  P_{i}=\left\{\begin{array}{l}
(1-\varepsilon), i f(i=y) \\
\frac{\varepsilon}{K-1}, i f(i \neq y)
\end{array}\right.\right.</script><p>å…¶å®æ›´æ–°åçš„åˆ†å¸ƒå°±ç›¸å½“äºå¾€çœŸå®åˆ†å¸ƒä¸­åŠ å…¥äº†å™ªå£°ï¼Œä¸ºäº†ä¾¿äºè®¡ç®—ï¼Œè¯¥å™ªå£°æœä»ç®€å•çš„å‡åŒ€åˆ†å¸ƒã€‚</p>
<p>2.ä¸ä¹‹å¯¹åº”ï¼Œlabel smoothingå°†äº¤å‰ç†µæŸå¤±å‡½æ•°ä½œå¦‚ä¸‹æ”¹å˜ï¼š</p>
<script type="math/tex; mode=display">
L o s s=-\sum_{i=1}^{K} p_{i} \log q_{i} \Longrightarrow \operatorname{Loss}_{i}=\left\{\begin{array}{l}
(1-\varepsilon)^{*} \operatorname{Loss}, \text {if}(i=y) \\
\varepsilon^{*} \operatorname{Loss}, \text {if}(i \neq y)
\end{array}\right.</script><p>3.ä¸ä¹‹å¯¹åº”ï¼Œlabel smoothingå°†æœ€ä¼˜çš„é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒä½œå¦‚ä¸‹æ”¹å˜ï¼š</p>
<script type="math/tex; mode=display">
Z_{i}=\left\{\begin{array}{l}
+\infty, i f(i=y) \\
0, i f(i \neq y)
\end{array} \Longrightarrow Z_{i}=\left\{\begin{array}{l}
\log \frac{(k-1)(1-\varepsilon)}{\varepsilon+\alpha}, i f(i=y) \\
\alpha, i f(i \neq y)
\end{array}\right.\right.</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelSmoothing</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement label smoothing."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, padding_idx, smoothing=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(size_average=<span class="literal">False</span>)</span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, target)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>)) <span class="comment"># å‡2ï¼Œæ˜¯å‡å»ç›®æ ‡åºåˆ—çš„&lt;s&gt; &lt;e&gt;æ ‡è¯†ä½ç½®ã€‚</span></span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)</span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, Variable(true_dist, requires_grad=<span class="literal">False</span>))</span><br></pre></td></tr></table></figure>
<p>åœ¨è®­ç»ƒæœŸé—´ï¼Œé‡‡ç”¨äº†å€¼ $\epsilon_{ls}=0.1$çš„æ ‡ç­¾å¹³æ»‘ã€‚ è¿™ç§åšæ³•æé«˜äº†å›°æƒ‘åº¦ï¼Œå› ä¸ºæ¨¡å‹å˜å¾—æ›´åŠ ä¸ç¡®å®šï¼Œä½†æé«˜äº†å‡†ç¡®æ€§å’ŒBLEUåˆ†æ•°ã€‚ä½¿ç”¨KL div losså®ç°æ ‡ç­¾å¹³æ»‘ã€‚ ç›¸æ¯”ä½¿ç”¨ç‹¬çƒ­ç›®æ ‡åˆ†å¸ƒï¼Œå…¶åŒ…å«æ­£ç¡®å•è¯çš„ç½®ä¿¡åº¦å’Œæ•´ä¸ªè¯æ±‡è¡¨ä¸­åˆ†å¸ƒçš„å…¶ä½™å¹³æ»‘é¡¹ã€‚å¯ä»¥çœ‹åˆ°æ ‡ç­¾å¹³æ»‘çš„ç¤ºä¾‹:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of label smoothing.</span></span><br><span class="line"><span class="comment"># embed_size = 5, padding_idx=0,smoothing=0.4</span></span><br><span class="line">crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.4</span>)</span><br><span class="line">predict = torch.FloatTensor([[<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">                             [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>], </span><br><span class="line">                             [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>]])</span><br><span class="line">v = crit(Variable(predict.log()), </span><br><span class="line">         Variable(torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the target distributions expected by the system.</span></span><br><span class="line">plt.imshow(crit.true_dist)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/05/11/transformer/label_smooth.png" alt></p>
<p>ä»¥ç´«è‰²ä¸ºä¾‹ï¼š0å·æ ‡ç­¾åº”è¯¥ä»¥<code>-0.5</code>ä¸ºç›®æ ‡ï¼Œ<code>1,2,3,4</code>ä»¥<code>1.5</code>ä¸ºç›®æ ‡</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(x)</span>:</span></span><br><span class="line">    d = x + <span class="number">3</span> * <span class="number">1</span></span><br><span class="line">    predict = torch.FloatTensor([[<span class="number">0</span>, x / d, <span class="number">1</span> / d, <span class="number">1</span> / d, <span class="number">1</span> / d],</span><br><span class="line">                                 ])</span><br><span class="line">    <span class="comment">#print(predict)</span></span><br><span class="line">    <span class="keyword">return</span> crit(Variable(predict.log()),</span><br><span class="line">                 Variable(torch.LongTensor([<span class="number">1</span>]))).data[<span class="number">0</span>]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">100</span>), [loss(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>)])</span><br></pre></td></tr></table></figure>
<p>å¦‚æœå¯¹ç»™å®šçš„é€‰æ‹©éå¸¸æœ‰ä¿¡å¿ƒï¼Œæ ‡ç­¾å¹³æ»‘å®é™…ä¸Šä¼šå¼€å§‹æƒ©ç½šæ¨¡å‹ã€‚</p>
<p><img src="/2020/05/11/transformer/penate.png" alt></p>
<h2 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h2><h3 id="Copy-Task"><a href="#Copy-Task" class="headerlink" title="Copy Task"></a>Copy Task</h3><h4 id="Synthetic-Data"><a href="#Synthetic-Data" class="headerlink" title="Synthetic Data"></a>Synthetic Data</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_gen</span><span class="params">(V, batch, nbatches)</span>:</span></span><br><span class="line">    <span class="string">"Generate random data for a src-tgt copy task."</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nbatches):</span><br><span class="line">        data = torch.from_numpy(np.random.randint(<span class="number">1</span>, V, size=(batch, <span class="number">10</span>)))</span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        src = Variable(data, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        tgt = Variable(data, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> Batch(src, tgt, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Loss-Computation"><a href="#Loss-Computation" class="headerlink" title="Loss Computation"></a>Loss Computation</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleLossCompute</span>:</span></span><br><span class="line">    <span class="string">"A simple loss compute and train function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, generator, criterion, opt=None)</span>:</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        self.criterion = criterion</span><br><span class="line">        self.opt = opt</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x, y, norm)</span>:</span></span><br><span class="line">        x = self.generator(x)</span><br><span class="line">        loss = self.criterion(x.contiguous().view(<span class="number">-1</span>, x.size(<span class="number">-1</span>)), </span><br><span class="line">                              y.contiguous().view(<span class="number">-1</span>)) / norm</span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">if</span> self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.opt.step()</span><br><span class="line">            self.opt.optimizer.zero_grad()</span><br><span class="line">        <span class="keyword">return</span> loss.data[<span class="number">0</span>] * norm</span><br></pre></td></tr></table></figure>
<h4 id="Greedy-Decoding"><a href="#Greedy-Decoding" class="headerlink" title="Greedy Decoding"></a>Greedy Decoding</h4><p>è´ªå¿ƒè§£ç ã€‚ã€‚ã€‚å¤§ä½¬ä»¬çœŸä¼šèµ·åå­—ï¼ï¼</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the simple copy task.</span></span><br><span class="line">V = <span class="number">11</span></span><br><span class="line">criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line">model = make_model(V, V, N=<span class="number">2</span>)</span><br><span class="line">model_opt = NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">1</span>, <span class="number">400</span>,</span><br><span class="line">        torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    run_epoch(data_gen(V, <span class="number">30</span>, <span class="number">20</span>), model, </span><br><span class="line">              SimpleLossCompute(model.generator, criterion, model_opt))</span><br><span class="line">    model.eval()</span><br><span class="line">    print(run_epoch(data_gen(V, <span class="number">30</span>, <span class="number">5</span>), model, </span><br><span class="line">                    SimpleLossCompute(model.generator, criterion, <span class="literal">None</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greedy_decode</span><span class="params">(model, src, src_mask, max_len, start_symbol)</span>:</span></span><br><span class="line">    memory = model.encode(src, src_mask)</span><br><span class="line">    ys = torch.ones(<span class="number">1</span>, <span class="number">1</span>).fill_(start_symbol).type_as(src.data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_len<span class="number">-1</span>):</span><br><span class="line">        out = model.decode(memory, src_mask, </span><br><span class="line">                           Variable(ys), </span><br><span class="line">                           Variable(subsequent_mask(ys.size(<span class="number">1</span>))</span><br><span class="line">                                    .type_as(src.data)))</span><br><span class="line">        prob = model.generator(out[:, <span class="number">-1</span>])</span><br><span class="line">        _, next_word = torch.max(prob, dim = <span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat([ys, </span><br><span class="line">                        torch.ones(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ys</span><br><span class="line"></span><br><span class="line">model.eval()</span><br><span class="line">src = Variable(torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]]) )</span><br><span class="line">src_mask = Variable(torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>) )</span><br><span class="line">print(greedy_decode(model, src, src_mask, max_len=<span class="number">10</span>, start_symbol=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>result</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span>     <span class="number">4</span>     <span class="number">5</span>     <span class="number">6</span>     <span class="number">7</span>     <span class="number">8</span>     <span class="number">9</span>    <span class="number">10</span></span><br><span class="line">[torch.LongTensor of size <span class="number">1</span>x10]</span><br></pre></td></tr></table></figure>
<p>ç¿»è¯‘çš„ä¾‹å­æ¶‰åŠGPUå¹¶è¡Œæ¯”è¾ƒå¤æ‚ï¼Œä¸åšä»‹ç»ã€‚</p>
<h1 id="Attention-Visualization"><a href="#Attention-Visualization" class="headerlink" title="Attention Visualization"></a>Attention Visualization</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">tgt_sent = trans.split()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(data, x, y, ax)</span>:</span></span><br><span class="line">    seaborn.heatmap(data, </span><br><span class="line">                    xticklabels=x, square=<span class="literal">True</span>, yticklabels=y, vmin=<span class="number">0.0</span>, vmax=<span class="number">1.0</span>, </span><br><span class="line">                    cbar=<span class="literal">False</span>, ax=ax)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    print(<span class="string">"Encoder Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.encoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data, sent, sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    print(<span class="string">"Decoder Self Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.decoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data[:len(tgt_sent), :len(tgt_sent)], tgt_sent, tgt_sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"Decoder Src Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.decoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data[:len(tgt_sent), :len(sent)],sent, tgt_sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="Encoder-visualization"><a href="#Encoder-visualization" class="headerlink" title="Encoder visualization"></a>Encoder visualization</h2><div class="note info">
            <p>Encoder Layer 2</p>
          </div>
<p><img src="/2020/05/11/transformer/encoder_layer_2.png" alt></p>
<div class="note info">
            <p>Encoder Layer 2</p>
          </div>
<p><img src="/2020/05/11/transformer/encoder_layer_4.png" alt></p>
<div class="note info">
            <p>Encoder Layer 6</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511115252714.png" alt></p>
<p>åŒä¸€è¡Œï¼Œæ¯”è¾ƒä¸åŒçš„head,å¯ä»¥çœ‹å‡ºï¼Œä¸åŒçš„headï¼Œattentionåˆ°çš„å†…å®¹æ˜¯å„ä¸ç›¸åŒçš„ã€‚</p>
<h2 id="Decoder-visualization"><a href="#Decoder-visualization" class="headerlink" title="Decoder visualization"></a>Decoder visualization</h2><div class="note info">
            <p>decoder Self Layer 2 :<code>&lt;s&gt;</code>ä¼šattentionåˆ°æ‰€æœ‰å•è¯ï¼Œå•è¯å¤§å¤šä¼šattentionåˆ°è‡ªå·±ã€‚</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511120009561.png" alt></p>
<div class="note info">
            <p>decoder Src Layer 2</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511120705604.png" alt></p>
<div class="note info">
            <p>decoder Self Layer 4</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511120927309.png" alt></p>
<div class="note info">
            <p>decoder Src Layer 4</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511120946538.png" alt></p>
<div class="note info">
            <p>decoder Self Layer 6</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511121002344.png" alt></p>
<div class="note info">
            <p>decoder Src Layer 6</p>
          </div>
<p><img src="/2020/05/11/transformer/image-20200511121021648.png" alt></p>
<p>ä¸Šä¸‰è§’å…¨é»‘ï¼šè¡¨ç¤ºdecoderæ˜¯ä¸€ä¸ªä¸²è¡Œè®¡ç®—ï¼Œæ¯ä¸€ä¸ªè¯åªä¼šattentionåˆ°å…¶å‰é¢çš„è¯ã€‚</p>
<h1 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h1><h2 id="tricks"><a href="#tricks" class="headerlink" title="tricks"></a>tricks</h2><p>åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹æ²¡æœ‰æ”¶æ•›å¾—å¾ˆå¥½æ—¶ï¼ŒDecoderé¢„æµ‹äº§ç”Ÿçš„è¯å¾ˆå¯èƒ½ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚è¿™ä¸ªæ—¶å€™å¦‚æœå†æŠŠé”™è¯¯çš„æ•°æ®å†è¾“ç»™Decoderï¼Œå°±ä¼šè¶Šè·‘è¶Šåã€‚è¿™ä¸ªæ—¶å€™æ€ä¹ˆåŠï¼Ÿ</p>
<ul>
<li>åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥ä½¿ç”¨ â€œteacher forcingâ€ã€‚å› ä¸ºæˆ‘ä»¬çŸ¥é“åº”è¯¥é¢„æµ‹çš„wordæ˜¯ä»€ä¹ˆï¼Œé‚£ä¹ˆå¯ä»¥ç»™Decoderå–‚ä¸€ä¸ªæ­£ç¡®çš„ç»“æœä½œä¸ºè¾“å…¥ã€‚</li>
<li>é™¤äº†é€‰æ‹©æœ€é«˜æ¦‚ç‡çš„è¯ (greedy search)ï¼Œè¿˜å¯ä»¥é€‰æ‹©æ˜¯æ¯”å¦‚ â€œBeam Searchâ€ï¼Œå¯ä»¥ä¿ç•™topKä¸ªé¢„æµ‹çš„wordã€‚ Beam Search æ–¹æ³•ä¸å†æ˜¯åªå¾—åˆ°ä¸€ä¸ªè¾“å‡ºæ”¾åˆ°ä¸‹ä¸€æ­¥å»è®­ç»ƒäº†ï¼Œæˆ‘ä»¬å¯ä»¥è®¾å®šä¸€ä¸ªå€¼ï¼Œæ‹¿å¤šä¸ªå€¼æ”¾åˆ°ä¸‹ä¸€æ­¥å»è®­ç»ƒï¼Œè¿™æ¡è·¯å¾„çš„æ¦‚ç‡ç­‰äºæ¯ä¸€æ­¥è¾“å‡ºçš„æ¦‚ç‡çš„ä¹˜ç§¯ã€‚</li>
</ul>
<h2 id="Transformerçš„ä¼˜ç¼ºç‚¹"><a href="#Transformerçš„ä¼˜ç¼ºç‚¹" class="headerlink" title="Transformerçš„ä¼˜ç¼ºç‚¹"></a>Transformerçš„ä¼˜ç¼ºç‚¹</h2><h3 id="ä¼˜ç‚¹"><a href="#ä¼˜ç‚¹" class="headerlink" title="ä¼˜ç‚¹"></a>ä¼˜ç‚¹</h3><ol>
<li>æ¯å±‚è®¡ç®—å¤æ‚åº¦æ¯”RNNè¦ä½ã€‚</li>
<li>å¯ä»¥è¿›è¡Œ<strong>å¹¶è¡Œè®¡ç®—</strong>ã€‚</li>
<li>ä»è®¡ç®—ä¸€ä¸ªåºåˆ—é•¿åº¦ä¸ºnçš„ä¿¡æ¯è¦ç»è¿‡çš„è·¯å¾„é•¿åº¦æ¥çœ‹, CNNéœ€è¦å¢åŠ å·ç§¯å±‚æ•°æ¥æ‰©å¤§è§†é‡ï¼ŒRNNéœ€è¦ä»1åˆ°né€ä¸ªè¿›è¡Œè®¡ç®—ï¼Œè€ŒSelf-attentionåªéœ€è¦ä¸€æ­¥çŸ©é˜µè®¡ç®—å°±å¯ä»¥ã€‚Self-Attentionå¯ä»¥æ¯”RNNæ›´å¥½åœ°è§£å†³é•¿æ—¶ä¾èµ–é—®é¢˜ã€‚å½“ç„¶å¦‚æœè®¡ç®—é‡å¤ªå¤§ï¼Œæ¯”å¦‚åºåˆ—é•¿åº¦Nå¤§äºåºåˆ—ç»´åº¦Dè¿™ç§æƒ…å†µï¼Œä¹Ÿå¯ä»¥ç”¨çª—å£é™åˆ¶Self-Attentionçš„è®¡ç®—æ•°é‡ã€‚</li>
<li>ä»ä½œè€…åœ¨é™„å½•ä¸­ç»™å‡ºçš„æ —å­å¯ä»¥çœ‹å‡ºï¼ŒSelf-Attentionæ¨¡å‹æ›´å¯è§£é‡Šï¼ŒAttentionç»“æœçš„åˆ†å¸ƒè¡¨æ˜äº†è¯¥æ¨¡å‹å­¦ä¹ åˆ°äº†ä¸€äº›è¯­æ³•å’Œè¯­ä¹‰ä¿¡æ¯ã€‚</li>
</ol>
<h3 id="ç¼ºç‚¹"><a href="#ç¼ºç‚¹" class="headerlink" title="ç¼ºç‚¹"></a>ç¼ºç‚¹</h3><ol>
<li>å®è·µä¸Šï¼šæœ‰äº›RNNè½»æ˜“å¯ä»¥è§£å†³çš„é—®é¢˜transformeræ²¡åšåˆ°ï¼Œæ¯”å¦‚<strong>å¤åˆ¶string</strong>ï¼Œæˆ–è€…æ¨ç†æ—¶ç¢°åˆ°çš„sequenceé•¿åº¦æ¯”è®­ç»ƒæ—¶æ›´é•¿ï¼ˆå› ä¸º<strong>ç¢°åˆ°äº†æ²¡è§è¿‡çš„position embedding</strong>ï¼‰ã€‚</li>
<li>ç†è®ºä¸Šï¼štransformersä¸æ˜¯computationally universal(å›¾çµå®Œå¤‡)ï¼Œè¿™ç§éRNNå¼çš„æ¨¡å‹æ˜¯éå›¾çµå®Œå¤‡çš„çš„ï¼Œ<strong>æ— æ³•å•ç‹¬å®ŒæˆNLPä¸­æ¨ç†ã€å†³ç­–ç­‰è®¡ç®—é—®é¢˜</strong>ï¼ˆåŒ…æ‹¬ä½¿ç”¨transformerçš„bertæ¨¡å‹ç­‰ç­‰ï¼‰ã€‚</li>
</ol>
<h1 id="å‚è€ƒ"><a href="#å‚è€ƒ" class="headerlink" title="å‚è€ƒ"></a>å‚è€ƒ</h1><ol>
<li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">The Annotated Transformer</a></li>
<li><a href="https://www.cnblogs.com/zingp/p/11696111.html" target="_blank" rel="noopener">æ·±å…¥ç†è§£TransformeråŠå…¶æºç </a></li>
</ol>

    </div>

    
    
    <div>
  
    <div style="text-align:center;color:#bfbfbf;font-size:16px;">
      <span>-------- æœ¬æ–‡ç»“æŸ </span>
      <i class="fa fa-coffee"></i>
      <span> æ„Ÿè°¢é˜…è¯» --------</span>
    </div>
  
</div>
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    é¼“åŠ±ä¸€ä¸‹
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat.png" alt="Li Zhen å¾®ä¿¡æ”¯ä»˜">
        <p>å¾®ä¿¡æ”¯ä»˜</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/ali.png" alt="Li Zhen æ”¯ä»˜å®">
        <p>æ”¯ä»˜å®</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/transformer/" rel="tag"><i class="fa fa-tag"></i> transformer</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/05/08/%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95/" rel="prev" title="å¹³æ»‘æ–¹æ³•">
      <i class="fa fa-chevron-left"></i> å¹³æ»‘æ–¹æ³•
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#ç®€ä»‹"><span class="nav-number">1.</span> <span class="nav-text">ç®€ä»‹</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformer"><span class="nav-number">2.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Embedding"><span class="nav-number">2.1.</span> <span class="nav-text">Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Word-Embedding"><span class="nav-number">2.1.1.</span> <span class="nav-text">Word Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Positional-Embedding"><span class="nav-number">2.1.2.</span> <span class="nav-text">Positional Embedding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Encoder"><span class="nav-number">2.2.</span> <span class="nav-text">Encoder</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Muti-Head-Attention"><span class="nav-number">2.2.1.</span> <span class="nav-text">Muti-Head-Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Self-Attention"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">Self-Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Self-Attentionçš„ä¼˜ç‚¹"><span class="nav-number">2.2.1.2.</span> <span class="nav-text">Self-Attentionçš„ä¼˜ç‚¹</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Add-amp-Norm"><span class="nav-number">2.2.2.</span> <span class="nav-text">Add &amp; Norm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Feed-Forward-Network"><span class="nav-number">2.2.3.</span> <span class="nav-text">Feed-Forward Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ç»„åˆå‡ºEncoder"><span class="nav-number">2.2.4.</span> <span class="nav-text">ç»„åˆå‡ºEncoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#å°ç»“"><span class="nav-number">2.2.5.</span> <span class="nav-text">å°ç»“</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Decoder"><span class="nav-number">2.3.</span> <span class="nav-text">Decoder</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mask-Multi-Head-Attention"><span class="nav-number">2.3.1.</span> <span class="nav-text">Mask-Multi-Head-Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder-Decoder-Multi-head-Attention"><span class="nav-number">2.3.2.</span> <span class="nav-text">Encoder-Decoder Multi-head Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-and-Softmax-to-Produce-Output-Probabilities"><span class="nav-number">2.3.3.</span> <span class="nav-text">Linear and Softmax to Produce Output Probabilities</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#EncoderDecoder"><span class="nav-number">2.4.</span> <span class="nav-text">EncoderDecoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Full-Model"><span class="nav-number">2.5.</span> <span class="nav-text">Full Model</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Training"><span class="nav-number">3.</span> <span class="nav-text">Training</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Batches-and-Masking"><span class="nav-number">3.1.</span> <span class="nav-text">Batches and Masking</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-Loop"><span class="nav-number">3.2.</span> <span class="nav-text">Training Loop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-Data"><span class="nav-number">3.3.</span> <span class="nav-text">Training Data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimizer"><span class="nav-number">3.4.</span> <span class="nav-text">Optimizer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Regularization"><span class="nav-number">3.5.</span> <span class="nav-text">Regularization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Label-Smoothing"><span class="nav-number">3.5.1.</span> <span class="nav-text">Label Smoothing</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#èƒŒæ™¯ä»‹ç»"><span class="nav-number">3.5.1.1.</span> <span class="nav-text">èƒŒæ™¯ä»‹ç»</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#label-smoothingåŸç†"><span class="nav-number">3.5.1.2.</span> <span class="nav-text">label smoothingåŸç†</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Examples"><span class="nav-number">3.6.</span> <span class="nav-text">Examples</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Copy-Task"><span class="nav-number">3.6.1.</span> <span class="nav-text">Copy Task</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Synthetic-Data"><span class="nav-number">3.6.1.1.</span> <span class="nav-text">Synthetic Data</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Loss-Computation"><span class="nav-number">3.6.1.2.</span> <span class="nav-text">Loss Computation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Greedy-Decoding"><span class="nav-number">3.6.1.3.</span> <span class="nav-text">Greedy Decoding</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Attention-Visualization"><span class="nav-number">4.</span> <span class="nav-text">Attention Visualization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Encoder-visualization"><span class="nav-number">4.1.</span> <span class="nav-text">Encoder visualization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Decoder-visualization"><span class="nav-number">4.2.</span> <span class="nav-text">Decoder visualization</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#æ€»ç»“"><span class="nav-number">5.</span> <span class="nav-text">æ€»ç»“</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#tricks"><span class="nav-number">5.1.</span> <span class="nav-text">tricks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformerçš„ä¼˜ç¼ºç‚¹"><span class="nav-number">5.2.</span> <span class="nav-text">Transformerçš„ä¼˜ç¼ºç‚¹</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ä¼˜ç‚¹"><span class="nav-number">5.2.1.</span> <span class="nav-text">ä¼˜ç‚¹</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ç¼ºç‚¹"><span class="nav-number">5.2.2.</span> <span class="nav-text">ç¼ºç‚¹</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#å‚è€ƒ"><span class="nav-number">6.</span> <span class="nav-text">å‚è€ƒ</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <a href='/'>
    <img class="site-author-image" itemprop="image" alt="Li Zhen"
      src="/images/snoppy.jpeg">
  </a>
  <p class="site-author-name" itemprop="name">Li Zhen</p>
  <div class="site-description" itemprop="description">ä½†è¡Œå¥½äº‹ï¼Œè«é—®å‰ç¨‹ï¼</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">42</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jeffery0628" title="GitHub â†’ https:&#x2F;&#x2F;github.com&#x2F;jeffery0628" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jeffery.lee.0628@gmail.com" title="é‚®ç®± â†’ mailto:jeffery.lee.0628@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>é‚®ç®±</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 2018 â€“ 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Zhen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">ç«™ç‚¹æ€»å­—æ•°ï¼š</span>
    <span title="ç«™ç‚¹æ€»å­—æ•°">399k</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="site-uv">
      æˆ‘çš„ç¬¬ <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> ä½æœ‹å‹ï¼Œ
    </span>
    <span class="site-pv">
      å†ç» <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> æ¬¡å›çœ¸æ‰ä¸ä½ ç›¸é‡
    </span>
</div>






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"bu0jcrISneKfTwssc7P792xE-gzGzoHsz","app_key":"3y7nYJuTGp6zIHSfRBQlMQnB","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  

  <canvas id="evanyou"></canvas>
  <style>
    #evanyou {
      position: fixed;
      width: 100%;
      height: 100%;
      top: 0;
      left: 0;
      z-index: -1;
    }
  </style>
  <script src="/js/evan-you.js"></script>




  <canvas id="evanyou"></canvas>
  <style>
    #evanyou {
      position: fixed;
      width: 100%;
      height: 100%;
      top: 0;
      left: 0;
      z-index: -1;
    }
  </style>
  <script src="/js/evan-you.js"></script>



  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>


 

<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180101,"YYYYMMDD"));
    ages = ages.replace(/years?/, "å¹´");
    ages = ages.replace(/months?/, "æœˆ");
    ages = ages.replace(/days?/, "å¤©");
    ages = ages.replace(/hours?/, "å°æ—¶");
    ages = ages.replace(/minutes?/, "åˆ†");
    ages = ages.replace(/seconds?/, "ç§’");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `æˆ‘å·²åœ¨æ­¤ç­‰å€™ä½  ${ages}`;
  }
  var div = document.createElement("div");
  //æ’å…¥åˆ°copyrightä¹‹å
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


<script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>

<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdnjs.cloudflare.com/ajax/libs/valine/1.3.10/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'wocKAhd8E1IRPHNgYbfFVsKf-gzGzoHsz',
      appKey     : '0SDY7WADR3m02c4hBcUv3T0B',
      placeholder: "ç•™ä¸‹ä½ çš„å°è„šå°å§ï¼",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '8' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
