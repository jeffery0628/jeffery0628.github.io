<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/snoppy.jpeg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open Sans:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jeffery0628.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":15,"offset":12,"onmobile":true,"dimmer":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":true,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":true,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="bert源码">
<meta property="og:url" content="https://jeffery0628.github.io/2020/05/13/bert_code/index.html">
<meta property="og:site_name" content="火种2号">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/13/bert_code/bert.jpg">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/13/bert_code/bert-gpt-transformer-elmo.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/13/bert_code/BertModel.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/13/bert_code/bert-input-representation.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/13/bert_code/image-20200515100926049.png">
<meta property="og:image" content="https://jeffery0628.github.io/2020/05/13/bert_code/bert-specific-models.png">
<meta property="article:published_time" content="2020-05-13T12:26:31.000Z">
<meta property="article:modified_time" content="2020-05-15T13:55:59.860Z">
<meta property="article:author" content="Li Zhen">
<meta property="article:tag" content="transformer">
<meta property="article:tag" content="bert">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jeffery0628.github.io/2020/05/13/bert_code/bert.jpg">

<link rel="canonical" href="https://jeffery0628.github.io/2020/05/13/bert_code/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>bert源码 | 火种2号</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">火种2号</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">火种计划</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>简历</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>读书</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-video fa-fw"></i>电影</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>



</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jeffery0628.github.io/2020/05/13/bert_code/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/snoppy.jpeg">
      <meta itemprop="name" content="Li Zhen">
      <meta itemprop="description" content="他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="火种2号">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          bert源码
        </h1>

        <div class="post-meta">
          
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-15 21:55:59" itemprop="dateModified" datetime="2020-05-15T21:55:59+08:00">2020-05-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">技术/自然语言处理</span></a>
                </span>
            </span>

          
            <span id="/2020/05/13/bert_code/" class="post-meta-item leancloud_visitors" data-flag-title="bert源码" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2020/05/13/bert_code/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/05/13/bert_code/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>41k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>37 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/2020/05/13/bert_code/bert.jpg" alt></p>
<a id="more"></a>
<h1 id="jian-jie">简介</h1>
<p>BERT——来自<strong>Transformer的双向编码器表征</strong>。与最近的语言表征模型不同，BERT旨在基于<strong>所有层</strong>的<strong>左、右语境</strong>来预训练深度双向表征。BERT是首个在<strong>大批句子层面</strong>和<strong>token层面</strong>任务中取得当前最优性能的<strong>基于微调的表征模型</strong>，其性能超越许多使用任务特定架构的系统，刷新了11项NLP任务的最优性能记录。</p>
<h2 id="motivation">motivation</h2>
<p>作者认为现有的技术严重制约了预训练表征的能力，其主要局限在于语言模型是<strong>单向</strong>的，例如，OpenAI GPT使用的是<strong>从左到右</strong>的架构，其中<strong>每个token只能注意Transformer自注意力层中的先前token</strong>。这些局限对于<strong>句子层面的任务</strong>而言不是最佳选择，对于<strong>token级任务</strong>则可能是毁灭性的，<strong>因为在这种任务中，结合两个方向的语境至关重要</strong>BERT（Bidirectional Encoder Representations from Transformers）改进了<strong>基于微调的策略</strong>。</p>
<h2 id="solution">solution</h2>
<p>BERT提出一种新的<strong>预训练目标</strong>——<strong>遮蔽语言模型（masked language model，MLM）</strong>，来克服上文提到的单向局限。MLM<strong>随机遮蔽输入中的一些token</strong>，通过遮蔽词的语境来<strong>预测其原始词汇id</strong>。与从左到右的语言模型预训练不同，MLM目标<strong>允许表征融合左右两侧的语境</strong>，从而预训练一个深度<strong>双向Transformer</strong>。除了 MLM，还引入了一个**“下一句预测”（next sentence prediction）任务**，该任务<strong>联合预训练</strong>文本对表征。</p>
<h2 id="contribution">contribution</h2>
<ol>
<li>Bert模型的<strong>双向特性是最重要的一项新贡献</strong></li>
<li>BERT是首个在大批句子层面和token层面任务中取得当前最优性能的基于微调的表征模型，其性能超越许多使用任务特定架构的系统。证明了<strong>预训练表征</strong>可以<strong>消除对许多精心设计的任务特定架构的需求</strong>。</li>
</ol>
<p><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">论文地址</a></p>
<h1 id="mo-xing-jia-gou">模型架构</h1>
<p>BERT 旨在基于所有层的左、右语境来预训练深度双向表征。因此，预训练的 BERT 表征可以仅用一个额外的输出层进行微调，进而为很多任务创建当前最优模型，无需对任务特定架构做出大量修改。</p>
<h2 id="base-amp-large">Base &amp; Large</h2>
<blockquote>
<p>\(BERT_{Base}:L=12,H=768,A=12,Total Parameters=110M\)</p>
<p>\(BERT_{Large}:L=24,H=1024,A=16,Total Parameters=340M\)</p>
</blockquote>
<p>其中，\(L\):表示层数，\(H\):表示隐藏层的size，\(A\):表示自注意力head的个数。feed-forward的size为\(4H\),即\(H=768\)时为3072，\(H=1024\)时为4096。</p>
<h2 id="bert-amp-open-ai-gpt-amp-el-mo">BERT &amp; OpenAI GPT &amp; ELMo</h2>
<p>\(BERT_{Base}\)和OpenAI GPT的大小是一样的。BERT Transformer使用<strong>双向自注意力机制</strong>，而GPT Transformer使用受限的自注意力机制，导致每个token只能关注其左侧的语境。双向Transformer在文献中通常称为**“Transformer 编码器”<strong>，而只</strong>关注左侧语境的版本<strong>则因能用于文本生成而被称为</strong>“Transformer 解码器”**。</p>
<p><img src="/2020/05/13/bert_code/bert-gpt-transformer-elmo.png" alt></p>
<ul>
<li>BERT 使用双向Transformer</li>
<li>OpenAI GPT 使用从左到右的Transformer</li>
<li>ELMo 使用独立训练的从左到右和从右到左LSTM的级联来生成下游任务的特征。</li>
</ul>
<h2 id="pre-training-tasks">Pre-training Tasks</h2>
<h3 id="task-1-masked-lm">Task1:Masked LM</h3>
<p>BERT训练双向语言模型时以较小的概率把少量的词替成了Mask或者另一个随机的词。其目的在于使模型被迫增加对上下文的记忆。标准语言模型只能从左到右或从右到左进行训练，使得每个单词在多层上下文中<strong>间接</strong>地“see itself”。</p>
<p>为了训练一个深度双向表示，研究团队采用了一种简单的方法，即随机屏蔽（masking）部分输入token，然后只预测那些被屏蔽的token。论文将这个过程称为“masked LM”(MLM)。与masked token对应的最终隐藏向量被输入到词汇表上的输出softmax中。</p>
<p>在实验中，随机地Mask每个序列中15%的WordPiece token。</p>
<p>虽然这确实能获得双向预训练模型，但这种方法有两个缺点。</p>
<ul>
<li>
<p>缺点1：预训练和finetuning之间不匹配，因为在finetuning期间从未看到<code>[MASK]</code>token。</p>
<p>为了解决这个问题，并不总是用实际的<code>[MASK]</code>token替换被“masked”的词汇。使用训练数据生成器<strong>随机选择15％的token</strong>。例如在这个句子“my dog is hairy”中，它选择的token是“hairy”。然后，执行以下过程：</p>
<ul>
<li>80％的时间：<strong>用<code>[MASK]</code>标记替换单词</strong>，例如，<code>my dog is hairy → my dog is [MASK]</code></li>
<li>10％的时间：用一个<strong>随机的单词</strong>替换该单词，例如，<code>my dog is hairy → my dog is apple</code></li>
<li>10％的时间：<strong>保持单词不变</strong>，例如，<code>my dog is hairy → my dog is hairy</code>. 这样做的目的是将表示偏向于实际观察到的单词。</li>
</ul>
<p>由于Transformer encoder不知道它将被要求预测哪些单词或哪些单词已被随机单词替换，因此它<strong>被迫保持每个输入token的分布式上下文表示</strong>。此外，因为随机替换只发生在所有token的1.5％（即<strong>15％的10％</strong>），这不会损害模型的语言理解能力。</p>
</li>
<li>
<p>缺点2：每个batch只预测了15％的token，这表明模型可能需要更多的预训练步骤才能收敛。</p>
<p>MLM的收敛速度略慢于 left-to-right的模型（预测每个token），但MLM模型在实验上获得的提升远远超过增加的训练成本。</p>
</li>
</ul>
<h3 id="task-2-next-sentence-prediction">Task2:Next Sentence Prediction</h3>
<p>为了训练一个<strong>理解句子关系</strong>的模型，预先训练一个二分类的下一句测任务，这一任务可以从任何单语语料库中生成。具体地说，当选择句子A和B作为预训练样本时，B有50％的可能是A的下一个句子，也有50％的可能是来自语料库的随机句子。（实际bert后续的研究中，表明这个任务在理解句子关系表现的不是很理想。）例如：</p>
<figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Input = <span class="comment">[CLS]</span> the <span class="keyword">man</span> went to <span class="comment">[MASK]</span> store <span class="comment">[SEP]</span> he bought a gallon <span class="comment">[MASK]</span> milk <span class="comment">[SEP]</span></span><br><span class="line">Label = IsNext</span><br><span class="line"></span><br><span class="line">Input = <span class="comment">[CLS]</span> the <span class="keyword">man</span> <span class="comment">[MASK]</span> to the store <span class="comment">[SEP]</span> penguin <span class="comment">[MASK]</span> <span class="keyword">are</span> flight ##less birds <span class="comment">[SEP]</span></span><br><span class="line">Label = NotNext</span><br></pre></td></tr></table></figure>
<h1 id="dai-ma-shi-xian-pytorch">代码实现（pytorch）</h1>
<p>参考<a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener">transformers</a></p>
<h2 id="yu-xun-lian-mo-xing">预训练模型</h2>
<p><a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">bert中文预训练模型下载</a>，解压缩之后，会有三个文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bert_config.json <span class="comment"># 模型配置文件</span></span><br><span class="line">pytorch_model.bin <span class="comment"># 预训练好的模型</span></span><br><span class="line">vocab.txt <span class="comment">#词汇表</span></span><br></pre></td></tr></table></figure>
<p>vocab.txt是模型的词典，这个文件会经常要用到。<em>bert_config.json</em>是BERT的配置(超参数)，比如网络的层数，通常不需要修改，如果自己显存小的话，也可以调小一下bert的层数。pytorch_model是预训练好的模型的模型参数，Fine-Tuning模型的初始值就是来自这个文件，然后根据不同的任务进行Fine-Tuning。</p>
<figure class="highlight"><figcaption><span>bert_config.json</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "attention_probs_dropout_prob": 0.1,  #乘法attention时，softmax后dropout概率 </span><br><span class="line">  "directionality": "bidi", </span><br><span class="line">  "hidden_act": "gelu",  #激活函数 </span><br><span class="line">  "hidden_dropout_prob": 0.1,  #隐藏层dropout概率 </span><br><span class="line">  "hidden_size": 768,  #隐藏单元数 (embedding_size)</span><br><span class="line">  "initializer_range": 0.02,   #初始化范围 </span><br><span class="line">  "intermediate_size": 3072,  #升维维度</span><br><span class="line">  "max_position_embeddings": 512,  #用于生成position_embedding。输入序列长度（seq_len）不能超过512</span><br><span class="line">  "num_attention_heads": 12,  #每个隐藏层中的attention head数 (则，每个head的embedding_size=768/12=64)</span><br><span class="line">  "num_hidden_layers": 12, #隐藏层数 </span><br><span class="line">  "pooler_fc_size": 768, </span><br><span class="line">  "pooler_num_attention_heads": 12, </span><br><span class="line">  "pooler_num_fc_layers": 3, </span><br><span class="line">  "pooler_size_per_head": 128, </span><br><span class="line">  "pooler_type": "first_token_transform", </span><br><span class="line">  "type_vocab_size": 2,  #segment_ids类别 [0,1] </span><br><span class="line">  "vocab_size": 21128 #词典中词数</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>vocab.txt中的部分内容</p>
<figure class="highlight plain"><figcaption><span>vocab.txt</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">馬</span><br><span class="line">高</span><br><span class="line">龍</span><br><span class="line">龸</span><br><span class="line">ﬁ</span><br><span class="line">ﬂ</span><br><span class="line">！</span><br><span class="line">（</span><br><span class="line">）</span><br><span class="line">，</span><br><span class="line">－</span><br><span class="line">．</span><br><span class="line">／</span><br><span class="line">：</span><br><span class="line">？</span><br><span class="line">～</span><br><span class="line">the</span><br><span class="line">of</span><br><span class="line">and</span><br><span class="line">in</span><br><span class="line">to</span><br></pre></td></tr></table></figure>
<h2 id="data">Data</h2>
<h3 id="step-1-read-example-from-file">step 1: read example from file</h3>
<figure class="highlight python"><figcaption><span>read_examples_from_file</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InputExample</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A single training/test example for token classification.</span></span><br><span class="line"><span class="string">	用于保存当个样本实例。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        guid: 样本id.</span></span><br><span class="line"><span class="string">        words: 文本序列，list类型：[word1,word2,...wordn]. </span></span><br><span class="line"><span class="string">        labels: 序列对应的label， This should be specified for train and dev examples, but not for test examples.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 由于语法糖@dataclass,会自动对该类添加__init__()函数。</span></span><br><span class="line">    guid: str</span><br><span class="line">    words: List[str]</span><br><span class="line">    labels: Optional[List[str]]</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_examples_from_file</span><span class="params">(data_dir, mode: Union[Split, str])</span> -&gt; List[InputExample]:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    data_dir: 原始数据所在的文件夹</span></span><br><span class="line"><span class="string">    model：‘train’(‘valid’)对应的数据会有label，‘test’数据的label为0</span></span><br><span class="line"><span class="string">    return：</span></span><br><span class="line"><span class="string">    	examples：list类型，包含train or valid or test 中的所有样本。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(mode, Split):</span><br><span class="line">        mode = mode.value</span><br><span class="line">    file_path = os.path.join(data_dir, <span class="string">f"<span class="subst">&#123;mode&#125;</span>.txt"</span>)</span><br><span class="line">    guid_index = <span class="number">1</span></span><br><span class="line">    examples = []</span><br><span class="line">    <span class="keyword">with</span> open(file_path, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        words = []</span><br><span class="line">        labels = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            <span class="keyword">if</span> line.startswith(<span class="string">"-DOCSTART-"</span>) <span class="keyword">or</span> line == <span class="string">""</span> <span class="keyword">or</span> line == <span class="string">"\n"</span>:</span><br><span class="line">                <span class="keyword">if</span> words:</span><br><span class="line">                    examples.append(InputExample(guid=<span class="string">f"<span class="subst">&#123;mode&#125;</span>-<span class="subst">&#123;guid_index&#125;</span>"</span>, words=words, labels=labels))</span><br><span class="line">                    guid_index += <span class="number">1</span></span><br><span class="line">                    words = []</span><br><span class="line">                    labels = []</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                splits = line.split(<span class="string">" "</span>)</span><br><span class="line">                words.append(splits[<span class="number">0</span>])</span><br><span class="line">                <span class="keyword">if</span> len(splits) &gt; <span class="number">1</span>:</span><br><span class="line">                    labels.append(splits[<span class="number">-1</span>].replace(<span class="string">"\n"</span>, <span class="string">""</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># Examples could have no label for mode = "test"</span></span><br><span class="line">                    labels.append(<span class="string">"O"</span>)</span><br><span class="line">        <span class="keyword">if</span> words:</span><br><span class="line">            examples.append(InputExample(guid=<span class="string">f"<span class="subst">&#123;mode&#125;</span>-<span class="subst">&#123;guid_index&#125;</span>"</span>, words=words, labels=labels))</span><br><span class="line">    <span class="keyword">return</span> examples</span><br></pre></td></tr></table></figure>
<h3 id="step-2-convert-example-into-feature">step 2:convert example into feature</h3>
<h4 id="basic-tokenizer">BasicTokenizer</h4>
<p>在转化成feature之前需要先了解一下bert的分词：</p>
<ol>
<li>对于文本序列中不想被切分开的词可以放到一个列表中，通过never_split传入分词函数中，就不会对列表中的文字进行分词。</li>
<li>中文序列标注任务需要注意：<code>'\t', '\n', '\r'，' '</code>会被替换成空白字符<code>' '</code>添加到文本中，英文文本对文本进行分词的时候又会根据空白字符来进行分词，导致空白字符会被直接清洗掉，这样就会导致中文序列标注的标签中的<code>'\t', '\n', '\r'，' '</code>在分词之后找不到对应的位置。</li>
</ol>
<figure class="highlight python"><figcaption><span>BasicTokenizer</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Runs basic tokenization (punctuation splitting, lower casing, etc.)."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, do_lower_case=True, never_split=None, tokenize_chinese_chars=True)</span>:</span></span><br><span class="line">        <span class="string">""" Constructs a BasicTokenizer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            **do_lower_case**: Whether to lower case the input.</span></span><br><span class="line"><span class="string">            **never_split**: (`optional`) List of token not to split.</span></span><br><span class="line"><span class="string">            **tokenize_chinese_chars**: (`optional`),是否对中文分词，默认是true</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> never_split <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            never_split = []</span><br><span class="line">        self.do_lower_case = do_lower_case</span><br><span class="line">        self.never_split = never_split</span><br><span class="line">        self.tokenize_chinese_chars = tokenize_chinese_chars</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text, never_split=None)</span>:</span></span><br><span class="line">        <span class="string">""" Basic Tokenization of a piece of text.</span></span><br><span class="line"><span class="string">            仅仅根据 空白字符 来进行分词</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            **never_split**: (`optional`)，对哪些文字不进行分词，[person,people,..],如果正常分词的话，person 会被切成per ##son，但是person如果被放入这个列表中，就不会被切分开</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        never_split = self.never_split + (never_split <span class="keyword">if</span> never_split <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> [])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 注意： 非法中文字符会被从文本中删除，以及‘\t’, ‘\n’, and ‘\r’，‘ ’会被替换成空白字符‘ ’添加到文本中，对文本进行分词的时候又会根据空白字符来进行分词，导致空白字符会被直接清洗掉，对于序列标注任务来说，会造成文本与标签对不齐的问题。</span></span><br><span class="line">        text = self._clean_text(text)</span><br><span class="line">        <span class="keyword">if</span> self.tokenize_chinese_chars:</span><br><span class="line">            <span class="comment"># 对中文字符按 字 进行分词，对中文分词过滤一遍之后在对英文字符进行分词。</span></span><br><span class="line">            text = self._tokenize_chinese_chars(text)</span><br><span class="line">            </span><br><span class="line">        orig_tokens = whitespace_tokenize(text) <span class="comment"># 先按照空白字符进行分词。</span></span><br><span class="line">        split_tokens = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> orig_tokens:</span><br><span class="line">            <span class="keyword">if</span> self.do_lower_case <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> never_split:</span><br><span class="line">                token = token.lower()</span><br><span class="line">                token = self._run_strip_accents(token)</span><br><span class="line">            split_tokens.extend(self._run_split_on_punc(token, never_split))</span><br><span class="line">        output_tokens = whitespace_tokenize(<span class="string">" "</span>.join(split_tokens))</span><br><span class="line">        <span class="keyword">return</span> output_tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run_strip_accents</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="string">"""Strips accents from a piece of text."""</span></span><br><span class="line">        text = unicodedata.normalize(<span class="string">"NFD"</span>, text)</span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">            cat = unicodedata.category(char)</span><br><span class="line">            <span class="keyword">if</span> cat == <span class="string">"Mn"</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            output.append(char)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>.join(output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run_split_on_punc</span><span class="params">(self, text, never_split=None)</span>:</span></span><br><span class="line">        <span class="string">"""Splits punctuation on a piece of text."""</span></span><br><span class="line">        <span class="keyword">if</span> never_split <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> text <span class="keyword">in</span> never_split:</span><br><span class="line">            <span class="keyword">return</span> [text]</span><br><span class="line">        chars = list(text)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        start_new_word = <span class="literal">True</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">while</span> i &lt; len(chars):</span><br><span class="line">            char = chars[i]</span><br><span class="line">            <span class="keyword">if</span> _is_punctuation(char):</span><br><span class="line">                output.append([char])</span><br><span class="line">                start_new_word = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> start_new_word:</span><br><span class="line">                    output.append([])</span><br><span class="line">                start_new_word = <span class="literal">False</span></span><br><span class="line">                output[<span class="number">-1</span>].append(char)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [<span class="string">""</span>.join(x) <span class="keyword">for</span> x <span class="keyword">in</span> output]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_tokenize_chinese_chars</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="string">"""对中文合法字符按字进行分割。"""</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">            cp = ord(char)</span><br><span class="line">            <span class="keyword">if</span> self._is_chinese_char(cp):</span><br><span class="line">                output.append(<span class="string">" "</span>)</span><br><span class="line">                output.append(char)</span><br><span class="line">                output.append(<span class="string">" "</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output.append(char)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>.join(output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_is_chinese_char</span><span class="params">(self, cp)</span>:</span></span><br><span class="line">        <span class="string">"""判断cp是否是中文合法字符</span></span><br><span class="line"><span class="string">        0x4e00-0x9fff cjk 统一字型 常用字 共 20992个（实际只定义到0x9fc3)</span></span><br><span class="line"><span class="string">        0x3400-0x4dff cjk 统一字型扩展表a 少用字 共 6656个</span></span><br><span class="line"><span class="string">        0x20000-0x2a6df cjk 统一字型扩展表b 少用字，历史上使用 共42720个</span></span><br><span class="line"><span class="string">        0xf900-0xfaff cjk 兼容字型 重复字，可统一变体，共同字 共512个</span></span><br><span class="line"><span class="string">        0x2f800-0x2fa1f cjk 兼容字型补遗 可统一变体 共544个</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> (</span><br><span class="line">            (cp &gt;= <span class="number">0x4E00</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x9FFF</span>)</span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x3400</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x4DBF</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x20000</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2A6DF</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2A700</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2B73F</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2B740</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2B81F</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2B820</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2CEAF</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0xF900</span> <span class="keyword">and</span> cp &lt;= <span class="number">0xFAFF</span>)</span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2F800</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2FA1F</span>)  <span class="comment">#</span></span><br><span class="line">        ):  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_clean_text</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="string">"""过滤掉非法字符，把`'\t', '\n', '\r'，' '`会被替换成空白字符`' '`"""</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">            cp = ord(char)</span><br><span class="line">            <span class="keyword">if</span> cp == <span class="number">0</span> <span class="keyword">or</span> cp == <span class="number">0xFFFD</span> <span class="keyword">or</span> _is_control(char):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> _is_whitespace(char):</span><br><span class="line">                output.append(<span class="string">" "</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output.append(char)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>.join(output)</span><br></pre></td></tr></table></figure>
<h4 id="wordpiece-tokenizer">WordpieceTokenizer</h4>
<p>经过<code>BasicTokenizer</code>处理成空格隔开的单词之后，还需要在经过WordpieceTokenizer对英文单词进行更细粒度的切分：</p>
<blockquote>
<p>input = “unaffable”</p>
<p>output = [“un”, “##aff”, “##able”]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordpieceTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Runs WordPiece tokenization."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab, unk_token, max_input_chars_per_word=<span class="number">100</span>)</span>:</span></span><br><span class="line">        self.vocab = vocab</span><br><span class="line">        self.unk_token = unk_token</span><br><span class="line">        self.max_input_chars_per_word = max_input_chars_per_word</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        For example:input = "unaffable"output = ["un", "##aff", "##able"]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          text: 经过`BasicTokenizer`分词之后，</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">          A list of wordpiece tokens.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        output_tokens = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> whitespace_tokenize(text):</span><br><span class="line">            chars = list(token)</span><br><span class="line">            <span class="keyword">if</span> len(chars) &gt; self.max_input_chars_per_word:</span><br><span class="line">                output_tokens.append(self.unk_token)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            is_bad = <span class="literal">False</span></span><br><span class="line">            start = <span class="number">0</span></span><br><span class="line">            sub_tokens = []</span><br><span class="line">            <span class="keyword">while</span> start &lt; len(chars):</span><br><span class="line">                end = len(chars)</span><br><span class="line">                cur_substr = <span class="literal">None</span></span><br><span class="line">                <span class="keyword">while</span> start &lt; end:</span><br><span class="line">                    substr = <span class="string">""</span>.join(chars[start:end])</span><br><span class="line">                    <span class="keyword">if</span> start &gt; <span class="number">0</span>:</span><br><span class="line">                        substr = <span class="string">"##"</span> + substr</span><br><span class="line">                    <span class="keyword">if</span> substr <span class="keyword">in</span> self.vocab: <span class="comment"># 如果在词汇表中存在该wordpiece</span></span><br><span class="line">                        cur_substr = substr</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    end -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> cur_substr <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    is_bad = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                sub_tokens.append(cur_substr)</span><br><span class="line">                start = end</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> is_bad:</span><br><span class="line">                output_tokens.append(self.unk_token)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output_tokens.extend(sub_tokens)</span><br><span class="line">        <span class="keyword">return</span> output_tokens</span><br></pre></td></tr></table></figure>
<h4 id="bert-tokenizer">BertTokenizer</h4>
<p><code>BertTokenizer</code>基于<code>WordPiece</code>，需要注意的地方是，对于中文文本来说，中文文本是没有空格分隔的文本，所以是需要在WordPiece之前do_basic_tokenize，也就是对于中文来说，需要<code>do_basic_tokenize=True</code>,不然对于没有分隔的中文字符会由于其长度超过词的最大长度，被修改为<code>[UNK]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertTokenizer</span><span class="params">(PreTrainedTokenizer)</span>:</span></span><br><span class="line">    <span class="string">r"""</span></span><br><span class="line"><span class="string">    Constructs a BERT tokenizer. Based on WordPiece.</span></span><br><span class="line"><span class="string">    这是一个替换原文本中符号，检测元文本中的单词是否在预训练字典中，将单词替换成字典中对应的id，对文本的长度进行padding的一个类。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab_file:词汇表文件</span></span><br><span class="line"><span class="string">        do_lower_case:是否大写变小写，默认True</span></span><br><span class="line"><span class="string">        do_basic_tokenize:是否在WordPiece之前做basci tokenize,对于中文字符来说</span></span><br><span class="line"><span class="string">        never_split：哪些词不需要在进行更细粒度的切分，前提需要`do_basic_tokenize=True`，才会起作用。</span></span><br><span class="line"><span class="string">        unk_token：对于不在词汇表中的词汇被修改成unk_token，默认是 "[UNK]"，无特殊需要，使用默认就好。</span></span><br><span class="line"><span class="string">        sep_token：分隔句子的标识，默认是"[SEP]"，无特殊需要，使用默认就好。</span></span><br><span class="line"><span class="string">        pad_token:超出句子长度的内容被填补pad_token, 默认是"[PAD]"，无特殊需要，使用默认就好。</span></span><br><span class="line"><span class="string">        cls_token：句子分类，默认"[CLS]"，原用于NSP任务，可用该字符对应的输出向量做文本分类任务</span></span><br><span class="line"><span class="string">        mask_token：对文本进行mask的字符，用于MAKS LM 任务，被mask住的字符需要在训练阶段进行predict，默认是"[MASK]"，无特殊需要，使用默认就好。</span></span><br><span class="line"><span class="string">        tokenize_chinese_chars：是否对中文字符进行分词。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    vocab_files_names = VOCAB_FILES_NAMES <span class="comment"># 词汇表文件名</span></span><br><span class="line">    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP</span><br><span class="line">    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION</span><br><span class="line">    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        vocab_file,</span></span></span><br><span class="line"><span class="function"><span class="params">        do_lower_case=True,</span></span></span><br><span class="line"><span class="function"><span class="params">        do_basic_tokenize=True,</span></span></span><br><span class="line"><span class="function"><span class="params">        never_split=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        unk_token=<span class="string">"[UNK]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        sep_token=<span class="string">"[SEP]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        pad_token=<span class="string">"[PAD]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        cls_token=<span class="string">"[CLS]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        mask_token=<span class="string">"[MASK]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        tokenize_chinese_chars=True,</span></span></span><br><span class="line"><span class="function"><span class="params">        **kwargs</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">        super().__init__(</span><br><span class="line">            unk_token=unk_token,</span><br><span class="line">            sep_token=sep_token,</span><br><span class="line">            pad_token=pad_token,</span><br><span class="line">            cls_token=cls_token,</span><br><span class="line">            mask_token=mask_token,</span><br><span class="line">            **kwargs,</span><br><span class="line">        )</span><br><span class="line">        self.max_len_single_sentence = self.max_len - <span class="number">2</span>  <span class="comment"># take into account special tokens</span></span><br><span class="line">        self.max_len_sentences_pair = self.max_len - <span class="number">3</span>  <span class="comment"># roberta，take into account special tokens</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(vocab_file):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">"Can't find a vocabulary file at path '&#123;&#125;'. To load the vocabulary from a Google pretrained "</span></span><br><span class="line">                <span class="string">"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"</span>.format(vocab_file)</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 加载词汇表文件</span></span><br><span class="line">        self.vocab = load_vocab(vocab_file)</span><br><span class="line">        <span class="comment"># id 到 token 的映射</span></span><br><span class="line">        self.ids_to_tokens = collections.OrderedDict([(ids, tok) <span class="keyword">for</span> tok, ids <span class="keyword">in</span> self.vocab.items()])</span><br><span class="line">        self.do_basic_tokenize = do_basic_tokenize</span><br><span class="line">        <span class="keyword">if</span> do_basic_tokenize:</span><br><span class="line">            self.basic_tokenizer = BasicTokenizer(</span><br><span class="line">                do_lower_case=do_lower_case, never_split=never_split, tokenize_chinese_chars=tokenize_chinese_chars</span><br><span class="line">            )</span><br><span class="line">        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">vocab_size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_vocab</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> dict(self.vocab, **self.added_tokens_encoder)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        split_tokens = []</span><br><span class="line">        <span class="keyword">if</span> self.do_basic_tokenize:</span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):</span><br><span class="line">                <span class="keyword">for</span> sub_token <span class="keyword">in</span> self.wordpiece_tokenizer.tokenize(token):</span><br><span class="line">                    split_tokens.append(sub_token)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            split_tokens = self.wordpiece_tokenizer.tokenize(text)</span><br><span class="line">        <span class="keyword">return</span> split_tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_convert_token_to_id</span><span class="params">(self, token)</span>:</span></span><br><span class="line">        <span class="string">""" 把词映射到id"""</span></span><br><span class="line">        <span class="keyword">return</span> self.vocab.get(token, self.vocab.get(self.unk_token))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_convert_id_to_token</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="string">"""把token id 映射成词"""</span></span><br><span class="line">        <span class="keyword">return</span> self.ids_to_tokens.get(index, self.unk_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convert_tokens_to_string</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">        <span class="string">""" 英文：把wordpiece之后的tokens 还原成文本。没有处理中文的换行等"""</span></span><br><span class="line">        out_string = <span class="string">" "</span>.join(tokens).replace(<span class="string">" ##"</span>, <span class="string">""</span>).strip()</span><br><span class="line">        <span class="keyword">return</span> out_string</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_inputs_with_special_tokens</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span> -&gt; List[int]:</span></span><br><span class="line">        <span class="string">"""为分词后的句子添加special token：[CLS]、[SEP]</span></span><br><span class="line"><span class="string">        - single sequence: ``[CLS] X [SEP]``</span></span><br><span class="line"><span class="string">        - pair of sequences: ``[CLS] A [SEP] B [SEP]``</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> [self.cls_token_id] + token_ids_0 + [self.sep_token_id]</span><br><span class="line">        cls = [self.cls_token_id]</span><br><span class="line">        sep = [self.sep_token_id]</span><br><span class="line">        <span class="keyword">return</span> cls + token_ids_0 + sep + token_ids_1 + sep</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_special_tokens_mask</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span> -&gt; List[int]:</span></span><br><span class="line">        <span class="string">"""判断token_ids 是否已经添加过special token，用mask序列来表示：[0,1,1,1,1,0],0表示special token，返回mask 序列</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> already_has_special_tokens:</span><br><span class="line">            <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">"You should not supply a second sequence if the provided sequence of "</span></span><br><span class="line">                    <span class="string">"ids is already formated with special tokens for the model."</span></span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">return</span> list(map(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x <span class="keyword">in</span> [self.sep_token_id, self.cls_token_id] <span class="keyword">else</span> <span class="number">0</span>, token_ids_0))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> [<span class="number">1</span>] + ([<span class="number">0</span>] * len(token_ids_0)) + [<span class="number">1</span>] + ([<span class="number">0</span>] * len(token_ids_1)) + [<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> [<span class="number">1</span>] + ([<span class="number">0</span>] * len(token_ids_0)) + [<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_token_type_ids_from_sequences</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span> -&gt; List[int]:</span></span><br><span class="line">        <span class="string">"""根据序列创建 token type ids，用于表示句子分割，如果只有一个句子，返回全为0的列表，如果是两个句子，则表示成如下：</span></span><br><span class="line"><span class="string">            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1</span></span><br><span class="line"><span class="string">            | first sequence    | second sequence |</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        sep = [self.sep_token_id]</span><br><span class="line">        cls = [self.cls_token_id]</span><br><span class="line">        <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> len(cls + token_ids_0 + sep) * [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> len(cls + token_ids_0 + sep) * [<span class="number">0</span>] + len(token_ids_1 + sep) * [<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_vocabulary</span><span class="params">(self, vocab_path)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Save the sentencepiece vocabulary (copy original file) and special tokens file to a directory.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> os.path.isdir(vocab_path):</span><br><span class="line">            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[<span class="string">"vocab_file"</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            vocab_file = vocab_path</span><br><span class="line">        <span class="keyword">with</span> open(vocab_file, <span class="string">"w"</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> writer:</span><br><span class="line">            <span class="keyword">for</span> token, token_index <span class="keyword">in</span> sorted(self.vocab.items(), key=<span class="keyword">lambda</span> kv: kv[<span class="number">1</span>]):</span><br><span class="line">                <span class="keyword">if</span> index != token_index:</span><br><span class="line">                    logger.warning(</span><br><span class="line">                        <span class="string">"Saving vocabulary to &#123;&#125;: vocabulary indices are not consecutive."</span></span><br><span class="line">                        <span class="string">" Please check that the vocabulary is not corrupted!"</span>.format(vocab_file)</span><br><span class="line">                    )</span><br><span class="line">                    index = token_index</span><br><span class="line">                writer.write(token + <span class="string">"\n"</span>)</span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> (vocab_file,)</span><br></pre></td></tr></table></figure>
<p>由于BertTokenizer继承了PreTrainedTokenizer，对PreTrainedTokenizer感兴趣可以通过这个<a href="https://github.com/huggingface/transformers/blob/ef46ccb05c601f413a774d43524591816406778d/src/transformers/tokenization_utils.py#L693" target="_blank" rel="noopener">链接</a>在进行研究。</p>
<h4 id="input-feature">Input feature</h4>
<figure class="highlight python"><figcaption><span>convert_example_to_features</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InputFeatures</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    InputExample 对应的 feature，该类变量名称与model中的变量是对应的。</span></span><br><span class="line"><span class="string">    @dataclass 自动为该类添加初始化函数__init__().</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    input_ids: List[int] <span class="comment"># 输入样本的id</span></span><br><span class="line">    attention_mask: List[int] <span class="comment"># </span></span><br><span class="line">    <span class="comment"># 用来指示第几个句子，比如：</span></span><br><span class="line">    <span class="comment"># tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]</span></span><br><span class="line">    <span class="comment">#  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1</span></span><br><span class="line">    token_type_ids: Optional[List[int]] = <span class="literal">None</span> </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 由于是序列标注任务，所以label对应的也应该是一个序列。</span></span><br><span class="line">    label_ids: Optional[List[int]] = <span class="literal">None</span> </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="comment"># 实际上该函数没有处理中文分词中，换行（\n）,空格字符对齐的问题。    </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_examples_to_features</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    examples: List[InputExample], <span class="comment"># 样本集</span></span></span></span><br><span class="line"><span class="function"><span class="params">    label_list: List[str], <span class="comment"># 样本集对应的标签集</span></span></span></span><br><span class="line"><span class="function"><span class="params">    max_seq_length: int, <span class="comment"># 最大序列长度，但是不应该超过510（512应该包含[CLS],[SEP]两个字符）</span></span></span></span><br><span class="line"><span class="function"><span class="params">    tokenizer: PreTrainedTokenizer, <span class="comment"># 分词器</span></span></span></span><br><span class="line"><span class="function"><span class="params">    cls_token_at_end=False, <span class="comment"># [CLS]字符是否添加在序列最后，默认是放在序列最前面，False：[CLS] + A + [SEP] + B + [SEP]，True: A + [SEP] + B + [SEP] + [CLS]</span></span></span></span><br><span class="line"><span class="function"><span class="params">    cls_token=<span class="string">"[CLS]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    cls_token_segment_id=<span class="number">1</span>, <span class="comment"># `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)</span></span></span></span><br><span class="line"><span class="function"><span class="params">    sep_token=<span class="string">"[SEP]"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    sep_token_extra=False, <span class="comment"># roberta 中会有extra sep token</span></span></span></span><br><span class="line"><span class="function"><span class="params">    pad_on_left=False, <span class="comment"># 是否在序列的左边进行pad</span></span></span></span><br><span class="line"><span class="function"><span class="params">    pad_token=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    pad_token_segment_id=<span class="number">0</span>, <span class="comment"># padding token_ids 的值为0</span></span></span></span><br><span class="line"><span class="function"><span class="params">    pad_token_label_id=<span class="number">-100</span>, <span class="comment"># 序列标注的tag，超出序列长度的部分，pad成：-100</span></span></span></span><br><span class="line"><span class="function"><span class="params">    sequence_a_segment_id=<span class="number">0</span>, <span class="comment"># 第一句对应的seg id：0  --&gt; type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1</span></span></span></span><br><span class="line"><span class="function"><span class="params">    mask_padding_with_zero=True, <span class="comment"># 对padding的部分，mask对应的值为0</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span> -&gt; List[InputFeatures]:</span></span><br><span class="line">        </span><br><span class="line">  <span class="comment"># label 到 index 的映射 ： [B,M,E,S]--&gt;[0,1,2,3]</span></span><br><span class="line">  label_map = &#123;label: i <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(label_list)&#125; </span><br><span class="line">  </span><br><span class="line">  features = []</span><br><span class="line">  <span class="keyword">for</span> (ex_index, example) <span class="keyword">in</span> enumerate(examples):</span><br><span class="line">      <span class="keyword">if</span> ex_index % <span class="number">10</span>_000 == <span class="number">0</span>:</span><br><span class="line">          logger.info(<span class="string">"Writing example %d of %d"</span>, ex_index, len(examples))</span><br><span class="line">  </span><br><span class="line">      tokens = []  <span class="comment"># 文本序列分词之后的列表:sentence --&gt; [word1,wor2,word3,....]</span></span><br><span class="line">      label_ids = [] <span class="comment"># 由于分词之后，label 和 tokens 会产生错位现象（[CLS],[SEP],空格,换行等字符导致的问题），需要重新和分词之后的内容对齐。</span></span><br><span class="line">      <span class="keyword">for</span> word, label <span class="keyword">in</span> zip(example.words, example.labels):</span><br><span class="line">          word_tokens = tokenizer.tokenize(word)</span><br><span class="line">  </span><br><span class="line">          <span class="keyword">if</span> len(word_tokens) &gt; <span class="number">0</span>:</span><br><span class="line">              tokens.extend(word_tokens)</span><br><span class="line">              label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - <span class="number">1</span>))</span><br><span class="line">  </span><br><span class="line">      <span class="comment"># Account for [CLS] and [SEP] with "- 2" and with "- 3" for RoBERTa.</span></span><br><span class="line">      special_tokens_count = tokenizer.num_special_tokens_to_add()</span><br><span class="line">      <span class="comment"># 先取出不添加special token的序列，再在这个序列的基础上，添加special token</span></span><br><span class="line">      <span class="keyword">if</span> len(tokens) &gt; max_seq_length - special_tokens_count:</span><br><span class="line">          tokens = tokens[: (max_seq_length - special_tokens_count)]</span><br><span class="line">          label_ids = label_ids[: (max_seq_length - special_tokens_count)]</span><br><span class="line">  </span><br><span class="line">      <span class="comment"># 在序列末尾添加[SEP]</span></span><br><span class="line">      tokens += [sep_token]</span><br><span class="line">      label_ids += [pad_token_label_id]</span><br><span class="line">      </span><br><span class="line">      <span class="comment"># roberta 的extra token</span></span><br><span class="line">      <span class="keyword">if</span> sep_token_extra:</span><br><span class="line">          <span class="comment"># roberta uses an extra separator b/w pairs of sentences</span></span><br><span class="line">          tokens += [sep_token]</span><br><span class="line">          label_ids += [pad_token_label_id]</span><br><span class="line">          </span><br><span class="line">      <span class="comment"># 第一句对应的seg id：0  --&gt; type_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1</span></span><br><span class="line">      segment_ids = [sequence_a_segment_id] * len(tokens)</span><br><span class="line">    </span><br><span class="line">      <span class="comment"># [CLS] 添加到句首还是句末</span></span><br><span class="line">      <span class="keyword">if</span> cls_token_at_end:</span><br><span class="line">          tokens += [cls_token]</span><br><span class="line">          label_ids += [pad_token_label_id]</span><br><span class="line">          segment_ids += [cls_token_segment_id]</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          tokens = [cls_token] + tokens</span><br><span class="line">          label_ids = [pad_token_label_id] + label_ids</span><br><span class="line">          segment_ids = [cls_token_segment_id] + segment_ids</span><br><span class="line">    </span><br><span class="line">      <span class="comment"># 把词转化成token id</span></span><br><span class="line">      input_ids = tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line">  </span><br><span class="line">      <span class="comment"># The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.</span></span><br><span class="line">      input_mask = [<span class="number">1</span> <span class="keyword">if</span> mask_padding_with_zero <span class="keyword">else</span> <span class="number">0</span>] * len(input_ids)</span><br><span class="line">  </span><br><span class="line">      <span class="comment"># 添加0padding到句子最大长度</span></span><br><span class="line">      padding_length = max_seq_length - len(input_ids)</span><br><span class="line">      <span class="comment"># 在句子的左边padding 0 还是在句子的右边padding 0</span></span><br><span class="line">      <span class="keyword">if</span> pad_on_left:</span><br><span class="line">          input_ids = ([pad_token] * padding_length) + input_ids</span><br><span class="line">          input_mask = ([<span class="number">0</span> <span class="keyword">if</span> mask_padding_with_zero <span class="keyword">else</span> <span class="number">1</span>] * padding_length) + input_mask</span><br><span class="line">          segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids</span><br><span class="line">          label_ids = ([pad_token_label_id] * padding_length) + label_ids</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          input_ids += [pad_token] * padding_length</span><br><span class="line">          input_mask += [<span class="number">0</span> <span class="keyword">if</span> mask_padding_with_zero <span class="keyword">else</span> <span class="number">1</span>] * padding_length</span><br><span class="line">          segment_ids += [pad_token_segment_id] * padding_length</span><br><span class="line">          label_ids += [pad_token_label_id] * padding_length</span><br><span class="line">  </span><br><span class="line">      <span class="keyword">assert</span> len(input_ids) == max_seq_length</span><br><span class="line">      <span class="keyword">assert</span> len(input_mask) == max_seq_length</span><br><span class="line">      <span class="keyword">assert</span> len(segment_ids) == max_seq_length</span><br><span class="line">      <span class="keyword">assert</span> len(label_ids) == max_seq_length</span><br><span class="line">  </span><br><span class="line">      <span class="keyword">if</span> ex_index &lt; <span class="number">5</span>:</span><br><span class="line">          logger.info(<span class="string">"*** Example ***"</span>)</span><br><span class="line">          logger.info(<span class="string">"guid: %s"</span>, example.guid)</span><br><span class="line">          logger.info(<span class="string">"tokens: %s"</span>, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> tokens]))</span><br><span class="line">          logger.info(<span class="string">"input_ids: %s"</span>, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> input_ids]))</span><br><span class="line">          logger.info(<span class="string">"input_mask: %s"</span>, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> input_mask]))</span><br><span class="line">          logger.info(<span class="string">"segment_ids: %s"</span>, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> segment_ids]))</span><br><span class="line">          logger.info(<span class="string">"label_ids: %s"</span>, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> label_ids]))</span><br><span class="line">  </span><br><span class="line">      <span class="keyword">if</span> <span class="string">"token_type_ids"</span> <span class="keyword">not</span> <span class="keyword">in</span> tokenizer.model_input_names:</span><br><span class="line">          segment_ids = <span class="literal">None</span></span><br><span class="line">  </span><br><span class="line">      features.append(</span><br><span class="line">          InputFeatures(</span><br><span class="line">              input_ids=input_ids, attention_mask=input_mask, token_type_ids=segment_ids, label_ids=label_ids</span><br><span class="line">          )</span><br><span class="line">      )</span><br><span class="line">  <span class="keyword">return</span> features</span><br></pre></td></tr></table></figure>
<h3 id="step-3-dataset">step 3:Dataset</h3>
<p>有了样本和特征，构建数据集类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NerDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    features: List[InputFeatures]</span><br><span class="line">    <span class="comment"># 对于pad的部分不计算损失 </span></span><br><span class="line">    pad_token_label_id: int = nn.CrossEntropyLoss().ignore_index</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        data_dir: str,</span></span></span><br><span class="line"><span class="function"><span class="params">        tokenizer: PreTrainedTokenizer,</span></span></span><br><span class="line"><span class="function"><span class="params">        labels: List[str],</span></span></span><br><span class="line"><span class="function"><span class="params">        model_type: str,</span></span></span><br><span class="line"><span class="function"><span class="params">        max_seq_length: Optional[int] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">        overwrite_cache=False,</span></span></span><br><span class="line"><span class="function"><span class="params">        mode: Split = Split.train,</span></span></span><br><span class="line"><span class="function"><span class="params">        local_rank=<span class="number">-1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        )</span>:</span></span><br><span class="line">        <span class="comment"># 加载数据集文件</span></span><br><span class="line">        cached_features_file = os.path.join(data_dir, <span class="string">"cached_&#123;&#125;_&#123;&#125;_&#123;&#125;"</span>.format(mode.value, tokenizer.__class__.__name__, str(max_seq_length)), )</span><br><span class="line">        <span class="keyword">with</span> torch_distributed_zero_first(local_rank):</span><br><span class="line">        <span class="comment"># Make sure only the first process in distributed training processes the dataset,</span></span><br><span class="line">        <span class="comment"># and the others will use the cache.</span></span><br><span class="line">            <span class="keyword">if</span> os.path.exists(cached_features_file) <span class="keyword">and</span> <span class="keyword">not</span> overwrite_cache:</span><br><span class="line">                logger.info(<span class="string">f"Loading features from cached file <span class="subst">&#123;cached_features_file&#125;</span>"</span>)</span><br><span class="line">                self.features = torch.load(cached_features_file)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                logger.info(<span class="string">f"Creating features from dataset file at <span class="subst">&#123;data_dir&#125;</span>"</span>)</span><br><span class="line">                examples = read_examples_from_file(data_dir, mode)</span><br><span class="line">                <span class="comment"># TODO clean up all this to leverage built-in features of tokenizers</span></span><br><span class="line">                self.features = convert_examples_to_features(</span><br><span class="line">                    examples,</span><br><span class="line">                    labels,</span><br><span class="line">                    max_seq_length,</span><br><span class="line">                    tokenizer,</span><br><span class="line">                    cls_token_at_end=bool(model_type <span class="keyword">in</span> [<span class="string">"xlnet"</span>]),</span><br><span class="line">                    <span class="comment"># xlnet has a cls token at the end</span></span><br><span class="line">                    cls_token=tokenizer.cls_token,</span><br><span class="line">                    cls_token_segment_id=<span class="number">2</span> <span class="keyword">if</span> model_type <span class="keyword">in</span> [<span class="string">"xlnet"</span>] <span class="keyword">else</span> <span class="number">0</span>,</span><br><span class="line">                    sep_token=tokenizer.sep_token,</span><br><span class="line">                    sep_token_extra=bool(model_type <span class="keyword">in</span> [<span class="string">"roberta"</span>]),</span><br><span class="line">                    <span class="comment"># roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805</span></span><br><span class="line">                    pad_on_left=bool(tokenizer.padding_side == <span class="string">"left"</span>),</span><br><span class="line">                    pad_token=tokenizer.pad_token_id,</span><br><span class="line">                    pad_token_segment_id=tokenizer.pad_token_type_id,</span><br><span class="line">                    pad_token_label_id=self.pad_token_label_id,</span><br><span class="line">                )</span><br><span class="line">                <span class="comment"># 保存0卡上的数据到缓存</span></span><br><span class="line">                <span class="keyword">if</span> local_rank <span class="keyword">in</span> [<span class="number">-1</span>, <span class="number">0</span>]:</span><br><span class="line">                    logger.info(<span class="string">f"Saving features into cached file <span class="subst">&#123;cached_features_file&#125;</span>"</span>)</span><br><span class="line">                    torch.save(self.features, cached_features_file)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.features)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, i)</span> -&gt; InputFeatures:</span></span><br><span class="line">        <span class="keyword">return</span> self.features[i]</span><br></pre></td></tr></table></figure>
<h2 id="bert-model">BertModel</h2>
<p><img src="/2020/05/13/bert_code/BertModel.png" alt></p>
<p>从整体来看BertModel由三部分组成：BertEmbeddings、BertEncoder、BertPooler,需要注意<code>attention_mask和head_mask</code>的处理。</p>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>inputs，segment，mask'，position_ids'，head_mask'</code></p>
</li>
<li>
<p>输出：<code>元组 (最后一层的隐变量，最后一层第一个token的隐变量，最后一层的隐变量或每一层attentions 权重参数)</code></p>
</li>
<li>
<p>过程:<code>embedding-&gt;encoder-&gt;pooler</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertModel</span><span class="params">(BertPreTrainedModel)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__(config)</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        self.embeddings = BertEmbeddings(config)</span><br><span class="line">        self.encoder = BertEncoder(config)</span><br><span class="line">        self.pooler = BertPooler(config)</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_input_embeddings</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.embeddings.word_embeddings</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_input_embeddings</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        self.embeddings.word_embeddings = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_prune_heads</span><span class="params">(self, heads_to_prune)</span>:</span></span><br><span class="line">        <span class="string">""" E.g. &#123;1: [0, 2], 2: [2, 3]&#125; will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> layer, heads <span class="keyword">in</span> heads_to_prune.items():</span><br><span class="line">            self.encoder.layer[layer].attention.prune_heads(heads)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">        head_mask: head_mask has shape [num_heads] or [num_hidden_layers x num_heads]</span></span><br><span class="line"><span class="string">        encoder_hidden_states: encoder 的输出</span></span><br><span class="line"><span class="string">        encoder_attention_mask:</span></span><br><span class="line"><span class="string">        返回值：</span></span><br><span class="line"><span class="string">        last_hidden_state :[batch_size, sequence_length, hidden_size]，是序列在模型最后一层的输出的隐藏层</span></span><br><span class="line"><span class="string">        pooler_output :[batch_size, hidden_size]:[CLS]对应的隐状态的输出，由于这个token是用来做NSP任务的，这个输出通常不能很好的summary 整个序列的语义，如果想要获取整句话的语义通常需要对整个序列取平均或者池化。</span></span><br><span class="line"><span class="string">        hidden_states:[batch_size, sequence_length, hidden_size],embedding + output of each layer.</span></span><br><span class="line"><span class="string">        attention:[batch_size, num_heads, sequence_length, sequence_length],矩阵的权值是经过softmax的。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"You cannot specify both input_ids and inputs_embeds at the same time"</span>)</span><br><span class="line">        <span class="keyword">elif</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">elif</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"You have to specify either input_ids or inputs_embeds"</span>)</span><br><span class="line"></span><br><span class="line">        device = input_ids.device <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> inputs_embeds.device</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 全部mask成 1</span></span><br><span class="line">            attention_mask = torch.ones(input_shape, device=device)</span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 全表示成0（第一句）</span></span><br><span class="line">            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. attention_mask:[bs,seq_len] --&gt; [bs,num_heads,seq_len,seq_len]</span></span><br><span class="line">        <span class="comment"># 2. attention_value:1 --&gt; 0, 0--&gt;-10000,这样做的目的是为了和算出来的score（没有经过softmax）相加，相当于从序列中移除掉了mask的内容。</span></span><br><span class="line">        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, self.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.config.is_decoder <span class="keyword">and</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()</span><br><span class="line">            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length) <span class="comment"># [bs,seq_len]</span></span><br><span class="line">            <span class="keyword">if</span> encoder_attention_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 1. attention_mask:[bs,seq_len] --&gt; [bs,num_heads,seq_len,seq_len]</span></span><br><span class="line">            <span class="comment"># 2. attention_value:1 --&gt; 0, 0--&gt;-10000,这样做的目的是为了和算出来的score（没有经过softmax）相加，相当于从序列中移除掉了mask的内容。</span></span><br><span class="line">            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            encoder_extended_attention_mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># input head_mask： [num_heads] or [num_hidden_layers x num_heads]，1表示保留这个head。</span></span><br><span class="line">        <span class="comment"># 因为有多个head，每个head都需要mask序列中哪些是token，哪些是padding，所以需要转化head_mask为：[num_hidden_layers,bsz,num_heads,seq_length,seq_length]</span></span><br><span class="line">        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)</span><br><span class="line"></span><br><span class="line">        embedding_output = self.embeddings(</span><br><span class="line">            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds</span><br><span class="line">        )</span><br><span class="line">        encoder_outputs = self.encoder(</span><br><span class="line">            embedding_output,</span><br><span class="line">            attention_mask=extended_attention_mask,</span><br><span class="line">            head_mask=head_mask,</span><br><span class="line">            encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">            encoder_attention_mask=encoder_extended_attention_mask,</span><br><span class="line">        )</span><br><span class="line">        sequence_output = encoder_outputs[<span class="number">0</span>]</span><br><span class="line">        pooled_output = self.pooler(sequence_output)</span><br><span class="line">        <span class="comment"># add hidden_states and attentions if they are here</span></span><br><span class="line">        outputs = (sequence_output, pooled_output,) + encoder_outputs[<span class="number">1</span>:]  </span><br><span class="line">        <span class="keyword">return</span> outputs  <span class="comment"># sequence_output, pooled_output, (hidden_states), (attentions)</span></span><br></pre></td></tr></table></figure>
<h3 id="bert-embeddings">BertEmbeddings</h3>
<p>从Bert的论文中可以知道，Bert的词向量主要是由三个向量相加组合而成，分别是单词本身的向量，单词所在句子中位置的向量和句子所在单个训练文本中位置的向量。这样做的好处主要可以解决只有词向量时碰见多义词时模型预测不准的问题。</p>
<p><img src="/2020/05/13/bert_code/bert-input-representation.png" alt></p>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>inputs，segment'，position_ids'</code></p>
</li>
<li>
<p>输出：<code>words+position+segment的embedding</code></p>
</li>
<li>
<p>过程:<code>调用nn.Embedding构造words、position、segment的embedding -&gt; 三个embedding相加 -&gt; 规范化 LayerNorm（关联类BertLayerNorm）-&gt; dropout</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEmbeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Construct the embeddings from word, position and token_type embeddings.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)</span><br><span class="line">        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)</span><br><span class="line">        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)</span><br><span class="line">        </span><br><span class="line">        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">        device = input_ids.device <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> inputs_embeds.device</span><br><span class="line">        <span class="keyword">if</span> position_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 生成token 的 index 信息。[seq_len]</span></span><br><span class="line">            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)</span><br><span class="line">            position_ids = position_ids.unsqueeze(<span class="number">0</span>).expand(input_shape) <span class="comment">#[bs,seq_len]</span></span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs_embeds <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            inputs_embeds = self.word_embeddings(input_ids)</span><br><span class="line">        position_embeddings = self.position_embeddings(position_ids)</span><br><span class="line">        token_type_embeddings = self.token_type_embeddings(token_type_ids)</span><br><span class="line"></span><br><span class="line">        embeddings = inputs_embeds + position_embeddings + token_type_embeddings</span><br><span class="line">        <span class="comment"># BertLayerNorm = torch.nn.LayerNorm</span></span><br><span class="line">        embeddings = self.LayerNorm(embeddings)</span><br><span class="line">        embeddings = self.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure>
<p>从forward函数开始看，先有一个torch.arange函数。<code>torch.arange(seq_length, dtype=torch.long, device=input_ids.device)</code>来生成token 的index 信息,<span class="label danger">和transformer中的Positional Embedding不同，bert这里的positional embedding 需要通过训练进行学习，transformer中的positional embedding是使用公式计算出来的</span>(可以参考<a href="/2020/05/11/transformer/" title="Transformer">Transformer</a> 的Positional Embedding来进行比较)。此外，LayerNorm的数学表达为：\(y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\)，\(\gamma,\beta\)可以通过仿射变换来进行学习。</p>
<h3 id="bert-encoder">BertEncoder</h3>
<p>下面代码将原有的BertLayer一层一层剥开，如果想要输出每层的状态（output_hidden_states=True），则对hidden_states 进行累加，如果（output_hidden_states=False）则只与最后一层的状态相加。BertEncoder类，其实只是用来输出BertLayer类的状态的一个函数。真正模型内部的东西在BertLayer类中。</p>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states（由BetEmbeddings输出），attention_mask，head_mask</code></p>
</li>
<li>
<p>输出：<code>元组 (最后一层隐变量+每层隐变量) 或者 (最后一层attention+每一层attention)</code></p>
</li>
<li>
<p>过程:<code>调用modulelist类实例layer使得每一层输出（关联类BertLayer）-&gt; 保存所有层的attention输出 和 隐变量 -&gt; 返回元组，元组第一个是最后一层的attention或hidden，再往后是每层的。</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.output_attentions = config.output_attentions</span><br><span class="line">        self.output_hidden_states = config.output_hidden_states</span><br><span class="line">        self.layer = nn.ModuleList([BertLayer(config) <span class="keyword">for</span> _ <span class="keyword">in</span> range(config.num_hidden_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,)</span>:</span></span><br><span class="line">        all_hidden_states = ()</span><br><span class="line">        all_attentions = ()</span><br><span class="line">        <span class="comment"># 将原有的BertLayer一层一层剥开</span></span><br><span class="line">        <span class="comment"># 如果想要输出每层的状态（output_hidden_states=True），则对hidden_states 进行累加</span></span><br><span class="line">        <span class="comment"># 如果（output_hidden_states=False）则只与最后一层的状态相加。</span></span><br><span class="line">        <span class="keyword">for</span> i, layer_module <span class="keyword">in</span> enumerate(self.layer):</span><br><span class="line">            <span class="keyword">if</span> self.output_hidden_states:</span><br><span class="line">                all_hidden_states = all_hidden_states + (hidden_states,)</span><br><span class="line"></span><br><span class="line">            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask)</span><br><span class="line">            hidden_states = layer_outputs[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.output_attentions:</span><br><span class="line">                all_attentions = all_attentions + (layer_outputs[<span class="number">1</span>],)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add last layer</span></span><br><span class="line">        <span class="keyword">if</span> self.output_hidden_states:</span><br><span class="line">            all_hidden_states = all_hidden_states + (hidden_states,)</span><br><span class="line"></span><br><span class="line">        outputs = (hidden_states,)</span><br><span class="line">        <span class="keyword">if</span> self.output_hidden_states:</span><br><span class="line">            outputs = outputs + (all_hidden_states,)</span><br><span class="line">        <span class="keyword">if</span> self.output_attentions:</span><br><span class="line">            outputs = outputs + (all_attentions,)</span><br><span class="line">        <span class="keyword">return</span> outputs  <span class="comment"># last-layer hidden state, (all hidden states), (all attentions)</span></span><br></pre></td></tr></table></figure>
<h4 id="bert-layer">BertLayer</h4>
<p>BertLayer 由三部分组成：<code>BertAttention，BertIntermeidiate，BertOutput</code>。</p>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states（由上层BertLayer输出），attention_mask，head_mask</code></p>
</li>
<li>
<p>输出：<code>元组，(本层输出的隐变量，本层输出的attention)</code></p>
</li>
<li>
<p>过程:<code>调用attention得到attention_outputs -&gt; 取第一维attention_output[0]作为intermediate的参数 -&gt;调用intermediate-&gt; 调用output得到layer_output -&gt; layer_output 和 attention_outputs[1:]合并成元组返回</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.attention = BertAttention(config)</span><br><span class="line">        self.is_decoder = config.is_decoder</span><br><span class="line">        <span class="keyword">if</span> self.is_decoder:</span><br><span class="line">            self.crossattention = BertAttention(config)</span><br><span class="line">        self.intermediate = BertIntermediate(config)</span><br><span class="line">        self.output = BertOutput(config)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,)</span>:</span></span><br><span class="line">        self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)</span><br><span class="line">        attention_output = self_attention_outputs[<span class="number">0</span>]</span><br><span class="line">        outputs = self_attention_outputs[<span class="number">1</span>:]  <span class="comment"># add self attentions if we output attention weights</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.is_decoder <span class="keyword">and</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            cross_attention_outputs = self.crossattention(</span><br><span class="line">                attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask</span><br><span class="line">            )</span><br><span class="line">            attention_output = cross_attention_outputs[<span class="number">0</span>]</span><br><span class="line">            outputs = outputs + cross_attention_outputs[<span class="number">1</span>:]  <span class="comment"># add cross attentions if we output attention weights</span></span><br><span class="line"></span><br><span class="line">        intermediate_output = self.intermediate(attention_output)</span><br><span class="line">        layer_output = self.output(intermediate_output, attention_output)</span><br><span class="line">        outputs = (layer_output,) + outputs</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>从forward开始看，依次进入BertAttention，BertIntermediate和BertOutput这三个类。</p>
<h5 id="bert-attention">BertAttention</h5>
<p>这个类由两个类组成：<code>BertSelfAttention,BertSelfOutput</code>.</p>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>input_tensor(就是BertLayer的hidden_states)，mask，head_mask'</code></p>
</li>
<li>
<p>输出：<code>返回元组（attention_output，self_outputs[1:]）第一个是语义向量，第二个是概率</code></p>
</li>
<li>
<p>过程:<code>selfattention得到 self_outputs-&gt; 以self_outputs[0]作为参数调用selfoutput得到 attention_output-&gt; 返回元组（attention_output，self_outputs[1:]）第一个是语义向量，第二个是概率</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.self_att = BertSelfAttention(config)</span><br><span class="line">        self.output = BertSelfOutput(config)</span><br><span class="line">        self.pruned_heads = set()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prune_heads</span><span class="params">(self, heads)</span>:</span> <span class="comment"># </span></span><br><span class="line">        <span class="keyword">if</span> len(heads) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        mask = torch.ones(self.self_att.num_attention_heads, self.self_att.attention_head_size)</span><br><span class="line">        heads = set(heads) - self.pruned_heads  </span><br><span class="line">        <span class="keyword">for</span> head <span class="keyword">in</span> heads:</span><br><span class="line">            <span class="comment"># Compute how many pruned heads are before the head and move the index accordingly</span></span><br><span class="line">            head = head - sum(<span class="number">1</span> <span class="keyword">if</span> h &lt; head <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> h <span class="keyword">in</span> self.pruned_heads)</span><br><span class="line">            mask[head] = <span class="number">0</span></span><br><span class="line">        mask = mask.view(<span class="number">-1</span>).contiguous().eq(<span class="number">1</span>)</span><br><span class="line">        index = torch.arange(len(mask))[mask].long()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Prune linear layers</span></span><br><span class="line">        self.self_att.query = prune_linear_layer(self.self_att.query, index)</span><br><span class="line">        self.self_att.key = prune_linear_layer(self.self_att.key, index)</span><br><span class="line">        self.self_att.value = prune_linear_layer(self.self_att.value, index)</span><br><span class="line">        self.output.dense = prune_linear_layer(self.output.dense, index, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update hyper params and store pruned heads</span></span><br><span class="line">        self.self_att.num_attention_heads = self.self_att.num_attention_heads - len(heads)</span><br><span class="line">        self.self_att.all_head_size = self.self_att.attention_head_size * self.self_att.num_attention_heads</span><br><span class="line">        self.pruned_heads = self.pruned_heads.union(heads)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,)</span>:</span></span><br><span class="line">        self_outputs = self.self_att(</span><br><span class="line">            hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask</span><br><span class="line">        )</span><br><span class="line">        attention_output = self.output(self_outputs[<span class="number">0</span>], hidden_states)</span><br><span class="line">        outputs = (attention_output,) + self_outputs[<span class="number">1</span>:]  <span class="comment"># add attentions if we output them</span></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h6 id="bert-self-attention">BertSelfAttention</h6>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states(由BertLayer输出),mask，head_mask</code></p>
</li>
<li>
<p>输出：<code>返回元组（context_layer语义向量，attention_prob概率）第一个是语义向量，第二个是概率</code></p>
</li>
<li>
<p>过程:<code>self-attention 过程</code></p>
</li>
</ul>
<p><img src="/2020/05/13/bert_code/image-20200515100926049.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="keyword">if</span> config.hidden_size % config.num_attention_heads != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> hasattr(config, <span class="string">"embedding_size"</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">"The hidden size (%d) is not a multiple of the number of attention "</span></span><br><span class="line">                <span class="string">"heads (%d)"</span> % (config.hidden_size, config.num_attention_heads)</span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">        self.output_attentions = config.output_attentions</span><br><span class="line">        <span class="comment"># 12</span></span><br><span class="line">        self.num_attention_heads = config.num_attention_heads</span><br><span class="line">        <span class="comment"># 64 = 768/12 </span></span><br><span class="line">        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)</span><br><span class="line">        <span class="comment"># 768 = 12 * 64</span></span><br><span class="line">        self.all_head_size = self.num_attention_heads * self.attention_head_size</span><br><span class="line"></span><br><span class="line">        self.query = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.key = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.value = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># x:[bs,seq_len,768]--&gt; x:[bs,seq_len, 12, 64]   multi-head</span></span><br><span class="line">        new_x_shape = x.size()[:<span class="number">-1</span>] + (self.num_attention_heads, self.attention_head_size)</span><br><span class="line">        x = x.view(*new_x_shape)</span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None, )</span>:</span></span><br><span class="line">        mixed_query_layer = self.query(hidden_states)</span><br><span class="line">    <span class="comment"># q,k,v</span></span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mixed_key_layer = self.key(encoder_hidden_states)</span><br><span class="line">            mixed_value_layer = self.value(encoder_hidden_states)</span><br><span class="line">            attention_mask = encoder_attention_mask</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mixed_key_layer = self.key(hidden_states)</span><br><span class="line">            mixed_value_layer = self.value(hidden_states)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># multi-head </span></span><br><span class="line">        query_layer = self.transpose_for_scores(mixed_query_layer) <span class="comment"># [bs,seq_len, 12, 64]</span></span><br><span class="line">        key_layer = self.transpose_for_scores(mixed_key_layer) <span class="comment"># [bs,seq_len, 12, 64]</span></span><br><span class="line">        value_layer = self.transpose_for_scores(mixed_value_layer) <span class="comment"># [bs,seq_len, 12, 64]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到权值矩阵</span></span><br><span class="line">        attention_scores = torch.matmul(query_layer, key_layer.transpose(<span class="number">-1</span>, <span class="number">-2</span>))</span><br><span class="line">        attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 用求和的方式，因为在attention_mask 中：1--&gt;0,0--&gt;-10000.</span></span><br><span class="line">            attention_scores = attention_scores + attention_mask</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对最后一个维度用 softmax 计算概率</span></span><br><span class="line">        attention_probs = nn.Softmax(dim=<span class="number">-1</span>)(attention_scores)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">        <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper .</span></span><br><span class="line">        attention_probs = self.dropout(attention_probs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="keyword">if</span> head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_probs = attention_probs * head_mask</span><br><span class="line"></span><br><span class="line">        context_layer = torch.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">        context_layer = context_layer.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        <span class="comment"># [bs,seq_len,12,64]--&gt; x:[bs,seq_len, 768]   把multi-head 再拼接回去</span></span><br><span class="line">        new_context_layer_shape = context_layer.size()[:<span class="number">-2</span>] + (self.all_head_size,)</span><br><span class="line">        context_layer = context_layer.view(*new_context_layer_shape)</span><br><span class="line"></span><br><span class="line">        outputs = (context_layer, attention_probs) <span class="keyword">if</span> self.output_attentions <span class="keyword">else</span> (context_layer,)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h6 id="bert-self-output">BertSelfOutput</h6>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states（由BertSelfAttention输出）, input_tensor（就是BertAttention的input_tensor，也就是BertSelfAttention的输入）</code></p>
</li>
<li>
<p>输出：<code>hidden_states</code></p>
</li>
<li>
<p>过程:<code>对hidden_states加一层dense -&gt; dropout -&gt; 得到的hidden_states与input_tensor相加做LayerNorm #这种做法说是为了避免梯度消失，也就是曾经的残差网络解决办法：output=output+Q</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfOutput</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, hidden_states, input_tensor)</span>:</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<h5 id="bert-intermediate">BertIntermediate</h5>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states(BertSelfOutput的输出)</code></p>
</li>
<li>
<p>输出：<code>hidden_states</code></p>
</li>
<li>
<p>过程:<code>对hidden_states加一层dense，向量输出大小为intermedia_size -&gt; 调用intermediate_act_fn，这个函数是由config.hidden_act得来，是gelu、relu、swish方法中的一个 #中间层存在的意义：推测是能够使模型从低至高学习到多层级信息，从表面信息到句法到语义。还有人研究说中间层的可迁移性更好。</code></p>
</li>
</ul>
<p>ACT2FN：激活函数:<code>{&quot;gelu&quot;: gelu, &quot;relu&quot;: torch.nn.functional.relu, &quot;swish&quot;: swish}</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertIntermediate</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)</span><br><span class="line">        <span class="keyword">if</span> isinstance(config.hidden_act, str):</span><br><span class="line">            <span class="comment"># ACT2FN = &#123;"gelu": gelu, "relu": torch.nn.functional.relu, "swish": swish&#125;</span></span><br><span class="line">            self.intermediate_act_fn = ACT2FN[config.hidden_act]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.intermediate_act_fn = config.hidden_act</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, hidden_states)</span>:</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.intermediate_act_fn(hidden_states)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<h5 id="bert-output">BertOutput</h5>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>
<p>接收参数：<code>hidden_states（由BertAttention输出）, input_tensor</code></p>
</li>
<li>
<p>输出：<code>hidden_states</code></p>
</li>
<li>
<p>过程:<code>对hidden_states加一层dense ，由intermedia_size 又变回hidden_size -&gt; dropout -&gt; 得到的hidden_states与input_tensor相加做LayerNorm #这种做法说是为了避免梯度消失，也就是曾经的残差网络解决办法：output=output+Q。</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertOutput</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, hidden_states, input_tensor)</span>:</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<p>可以看到BertOutput是一个输入size是<code>config.intermediate_size</code>，输出size是<code>config.hidden_size</code>。又把size从BertIntermediate中的<code>config.intermediate_size</code>变回<code>config.hidden_size</code>。然后又接了一个Dropout和一个归一化。</p>
<h3 id="bert-pooler">BertPooler</h3>
<p>forward：(符号’是可以为None的意思)</p>
<ul>
<li>接收参数：<code>hidden_states（由BertAttention输出）, input_tensor</code></li>
<li>输出：<code>pooled_output</code></li>
<li>过程:<code>简单取第一个token -&gt; 加一层dense -&gt; Tanh激活函数输出</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertPooler</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.activation = nn.Tanh()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, hidden_states)</span>:</span></span><br><span class="line">        first_token_tensor = hidden_states[:, <span class="number">0</span>]</span><br><span class="line">        pooled_output = self.dense(first_token_tensor)</span><br><span class="line">        pooled_output = self.activation(pooled_output)</span><br><span class="line">        <span class="keyword">return</span> pooled_output</span><br></pre></td></tr></table></figure>
<h2 id="zong-jie">总结</h2>
<p>数据流：</p>
<ol>
<li>输入的数据首先经过<strong>BertEmbeddings</strong>类。在BertEmbeddings中将每个单词变为words_embeddings + position_embeddings +token_type_embeddings三项embeddings的和。</li>
<li>然后，把已经变为词向量的数据输入BertSelfAttention类中。BertSelfAttention类中是一个Multi-Head Attention（少一个Linear层）， 也就是说数据流入这个<strong>少一个Linear层的Multi-Head Attention</strong>。</li>
<li>之后，数据流入BertSelfOutput类。BertSelfOutput是一个Linear+Dropout+LayerNorm。<strong>补齐了BertSelfAttention中少的那个Linear层</strong>，并且进行一次LayerNorm。</li>
<li>再之后，数据经过BertIntermediate(Linear层+激活函数)和BertOutput(Linear+Dropout+LayerNorm)。这样整个Transformer的部分就算完成了。</li>
<li>最后，取出最后一层的<code>[CLS]</code>对应的向量，经过<code>(Linear+Tanh)</code>,得到pooled_out</li>
</ol>
<p>关于bert的训练优化参考<a href="https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py" target="_blank" rel="noopener">链接</a></p>
<p>其余类：</p>
<ol>
<li>
<p>BertConfig</p>
<p>保存BERT的各种参数配置</p>
</li>
<li>
<p>BertOnlyMLMHead
使用mask 方法训练语言模型时用的，返回预测值
过程：调用BertLMPredictionHead，返回的就是prediction_scores</p>
</li>
<li>
<p>BertLMPredictionHead
decode功能
过程：调用BertPredictionHeadTransform -&gt; linear层，输出维度是vocab_size</p>
</li>
<li>
<p>BertPredictionHeadTransform
过程：dense -&gt; 激活(gelu or relu or swish) -&gt; LayerNorm</p>
</li>
<li>
<p>BertOnlyNSPHead
NSP策略训练模型用的，返回0或1
过程：添加linear层输出size是2，返回seq_relationship_score</p>
</li>
<li>
<p>BertPreTrainingHeads
MLM和NSP策略都写在里面，输入的是Encoder的输出sequence_output, pooled_output
返回（prediction_scores, seq_relationship_score）分别是MLM和NSP下的分值</p>
</li>
<li>
<p>BertPreTrainedModel
从全局变量BERT_PRETRAINED_MODEL_ARCHIVE_MAP加载BERT模型的权重</p>
</li>
<li>
<p>BertForPreTraining
计算score和loss
通过BertPreTrainingHeads，得到prediction后计算loss，然后反向传播。</p>
</li>
<li>
<p>BertForMaskedLM
只有MLM策略的loss</p>
</li>
<li>
<p>BertForNextSentencePrediction
只有NSP策略的loss</p>
</li>
<li>
<p>BertForSequenceClassification
计算句子分类任务的loss</p>
</li>
<li>
<p>BertForMultipleChoice
计算句子选择任务的loss</p>
</li>
<li>
<p>BertForTokenClassification
计算对token分类or标注任务的loss</p>
</li>
<li>
<p>BertForQuestionAnswering
计算问答任务的loss</p>
</li>
</ol>
<h3 id="shi-yong-yu-xun-lian-mo-xing">使用预训练模型</h3>
<p>在BertModel class中有两个函数。get_pool_output表示获取每个batch第一个词的[CLS]表示结果。BERT认为这个词包含了整条语料的信息；适用于句子级别的分类问题。get_sequence_output表示BERT最终的输出结果,shape为[batch_size,seq_length,hidden_size]。可以直观理解为对每条语料的最终表示，适用于seq2seq问题。
对于其它序列标注或生成任务，也可以使用 BERT 对应的输出信息作出预测，例如每一个时间步输出一个标注或词等。下图展示了 BERT 在 11 种任务中的微调方法，它们都只添加了一个额外的输出层。在下图中，Tok 表示不同的词、E 表示输入的嵌入向量、\(T_i\)表示第\(i\) 个词在经过 BERT 处理后输出的上下文向量。</p>
<p><img src="/2020/05/13/bert_code/bert-specific-models.png" alt></p>
<h1 id="can-kao">参考</h1>
<ol start="3">
<li><a href="http://fancyerii.github.io/2019/03/09/bert-codes/" target="_blank" rel="noopener">BERT代码阅读</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/56103665" target="_blank" rel="noopener">一起读Bert文本分类代码 </a></li>
<li><a href="https://daiwk.github.io/posts/nlp-bert.html#pytorch%E7%89%88%E6%9C%AC" target="_blank" rel="noopener">bert</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/75558363" target="_blank" rel="noopener">快速掌握BERT源代码（pytorch）</a></li>
</ol>

    </div>

    
    
    <div>
  
    <div style="text-align:center;color:#bfbfbf;font-size:16px;">
      <span>-------- 本文结束 </span>
      <i class="fa fa-coffee"></i>
      <span> 感谢阅读 --------</span>
    </div>
  
</div>
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    鼓励一下
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat.png" alt="Li Zhen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/ali.png" alt="Li Zhen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/transformer/" rel="tag"><i class="fa fa-tag"></i> transformer</a>
              <a href="/tags/bert/" rel="tag"><i class="fa fa-tag"></i> bert</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/05/11/transformer/" rel="prev" title="Transformer">
      <i class="fa fa-chevron-left"></i> Transformer
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/05/15/data-structures-and-algorithms/" rel="next" title="数据结构-屠龙14式">
      数据结构-屠龙14式 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#jian-jie"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#motivation"><span class="nav-number">1.1.</span> <span class="nav-text">motivation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#solution"><span class="nav-number">1.2.</span> <span class="nav-text">solution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#contribution"><span class="nav-number">1.3.</span> <span class="nav-text">contribution</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#mo-xing-jia-gou"><span class="nav-number">2.</span> <span class="nav-text">模型架构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#base-amp-large"><span class="nav-number">2.1.</span> <span class="nav-text">Base &amp; Large</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bert-amp-open-ai-gpt-amp-el-mo"><span class="nav-number">2.2.</span> <span class="nav-text">BERT &amp; OpenAI GPT &amp; ELMo</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pre-training-tasks"><span class="nav-number">2.3.</span> <span class="nav-text">Pre-training Tasks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#task-1-masked-lm"><span class="nav-number">2.3.1.</span> <span class="nav-text">Task1:Masked LM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#task-2-next-sentence-prediction"><span class="nav-number">2.3.2.</span> <span class="nav-text">Task2:Next Sentence Prediction</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dai-ma-shi-xian-pytorch"><span class="nav-number">3.</span> <span class="nav-text">代码实现（pytorch）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#yu-xun-lian-mo-xing"><span class="nav-number">3.1.</span> <span class="nav-text">预训练模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#data"><span class="nav-number">3.2.</span> <span class="nav-text">Data</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#step-1-read-example-from-file"><span class="nav-number">3.2.1.</span> <span class="nav-text">step 1: read example from file</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#step-2-convert-example-into-feature"><span class="nav-number">3.2.2.</span> <span class="nav-text">step 2:convert example into feature</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#basic-tokenizer"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">BasicTokenizer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#wordpiece-tokenizer"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">WordpieceTokenizer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bert-tokenizer"><span class="nav-number">3.2.2.3.</span> <span class="nav-text">BertTokenizer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#input-feature"><span class="nav-number">3.2.2.4.</span> <span class="nav-text">Input feature</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#step-3-dataset"><span class="nav-number">3.2.3.</span> <span class="nav-text">step 3:Dataset</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bert-model"><span class="nav-number">3.3.</span> <span class="nav-text">BertModel</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#bert-embeddings"><span class="nav-number">3.3.1.</span> <span class="nav-text">BertEmbeddings</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bert-encoder"><span class="nav-number">3.3.2.</span> <span class="nav-text">BertEncoder</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#bert-layer"><span class="nav-number">3.3.2.1.</span> <span class="nav-text">BertLayer</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#bert-attention"><span class="nav-number">3.3.2.1.1.</span> <span class="nav-text">BertAttention</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#bert-self-attention"><span class="nav-number">3.3.2.1.1.1.</span> <span class="nav-text">BertSelfAttention</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#bert-self-output"><span class="nav-number">3.3.2.1.1.2.</span> <span class="nav-text">BertSelfOutput</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#bert-intermediate"><span class="nav-number">3.3.2.1.2.</span> <span class="nav-text">BertIntermediate</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#bert-output"><span class="nav-number">3.3.2.1.3.</span> <span class="nav-text">BertOutput</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bert-pooler"><span class="nav-number">3.3.3.</span> <span class="nav-text">BertPooler</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#zong-jie"><span class="nav-number">3.4.</span> <span class="nav-text">总结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#shi-yong-yu-xun-lian-mo-xing"><span class="nav-number">3.4.1.</span> <span class="nav-text">使用预训练模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#can-kao"><span class="nav-number">4.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <a href='/'>
    <img class="site-author-image" itemprop="image" alt="Li Zhen"
      src="/images/snoppy.jpeg">
  </a>
  <p class="site-author-name" itemprop="name">Li Zhen</p>
  <div class="site-description" itemprop="description">他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">48</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jeffery0628" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jeffery0628" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jeffery.lee.0628@gmail.com" title="邮箱 → mailto:jeffery.lee.0628@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮箱</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Zhen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">533k</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
</div>






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"bu0jcrISneKfTwssc7P792xE-gzGzoHsz","app_key":"3y7nYJuTGp6zIHSfRBQlMQnB","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  






  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>


 

<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180101,"YYYYMMDD"));
    ages = ages.replace(/years?/, "年");
    ages = ages.replace(/months?/, "月");
    ages = ages.replace(/days?/, "天");
    ages = ages.replace(/hours?/, "小时");
    ages = ages.replace(/minutes?/, "分");
    ages = ages.replace(/seconds?/, "秒");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `我已在此等候你 ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


<script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>

<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdnjs.cloudflare.com/ajax/libs/valine/1.3.10/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'tChTbEIggDSVcdhPBUf0MFxW-gzGzoHsz',
      appKey     : 'sJ6xUiFnifEDIIfnj3Ut9zy1',
      placeholder: "留下你的小脚印吧！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '8' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
