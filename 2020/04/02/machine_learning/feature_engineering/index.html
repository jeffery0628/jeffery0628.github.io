<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/snoppy.jpeg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open Sans:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jeffery.ink","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":15,"offset":12,"onmobile":true,"dimmer":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":true,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":true,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="特征工程">
<meta property="og:url" content="https://jeffery.ink/2020/04/02/machine_learning/feature_engineering/index.html">
<meta property="og:site_name" content="火种2号">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jeffery.ink/2020/04/02/machine_learning/feature_engineering/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/02/machine_learning/feature_engineering/GADWZT.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/02/machine_learning/feature_engineering/GADF54.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/02/machine_learning/feature_engineering/GA0d6P.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/02/machine_learning/feature_engineering/GA0fXV.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/02/machine_learning/feature_engineering/GArCQI.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/02/machine_learning/feature_engineering/GAsi9J.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/02/machine_learning/feature_engineering/GAsJDP.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/02/machine_learning/feature_engineering/GAyQaT.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/02/machine_learning/feature_engineering/GAyrJe.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/02/machine_learning/feature_engineering/GAyIJg.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/02/machine_learning/feature_engineering/GA69SJ.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/03/28/GAcZBq.png#shadow">
<meta property="article:published_time" content="2020-04-01T23:06:42.000Z">
<meta property="article:modified_time" content="2020-07-05T06:47:52.818Z">
<meta property="article:author" content="Li Zhen">
<meta property="article:tag" content="特征工程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jeffery.ink/2020/04/02/machine_learning/feature_engineering/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B.png">

<link rel="canonical" href="https://jeffery.ink/2020/04/02/machine_learning/feature_engineering/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>特征工程 | 火种2号</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">火种2号</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">火种计划</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>简历</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>读书</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-video fa-fw"></i>电影</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>



</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jeffery.ink/2020/04/02/machine_learning/feature_engineering/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/snoppy.jpeg">
      <meta itemprop="name" content="Li Zhen">
      <meta itemprop="description" content="他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="火种2号">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          特征工程
        </h1>

        <div class="post-meta">
          
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-07-05 14:47:52" itemprop="dateModified" datetime="2020-07-05T14:47:52+08:00">2020-07-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">技术/机器学习</span></a>
                </span>
            </span>

          
            <span id="/2020/04/02/machine_learning/feature_engineering/" class="post-meta-item leancloud_visitors" data-flag-title="特征工程" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论：</span>
    
    <a title="valine" href="/2020/04/02/machine_learning/feature_engineering/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/04/02/machine_learning/feature_engineering/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>13k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>12 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/2020/04/02/machine_learning/feature_engineering/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B.png" alt></p>
<a id="more"></a>
<p><code><a class="btn" href="特征工程.xmind">
            <i class="fa fa-download"></i>xmind文件下载
          </a></code></p>
<p><a href="https://share.weiyun.com/EDpohPCk" target="_blank" rel="noopener">数据集下载</a></p>
<p>特征工程和数据清洗转换是至关重要的一块，因为<strong>数据和特征决定了机器学习的上限，而算法和模型只是逼近这个上限而已</strong>，所以特征工程的好坏往往决定着最后的结果。</p>
<p>特征工程一般包括特征构造，特征选择，降维等步骤，但是它一般是和数据清洗转换放在一块，也有的把这两块统称为特征工程，因为两者联系实在是密切（你中有我，我中有你的景象）。</p>
<p>通过数据清洗和转换，我们能够更好地表示出潜在问题的特征，使得数据的表达清晰一些，比如处理异常值清除噪声，填充缺失值可以加入先验知识等。而特征工程又进一步增强数据的表达能力，通过构造新特征，我们可以挖掘出数据的更多信息，使得数据的表达能力进一步放大，当然如果特征过多，又往往会造成冗余，这时候我们又得根据相关性等进行特征的选择和降维操作，所以这就是特征工程的逻辑。</p>
<h1 id="te-zheng-gou-zao">特征构造</h1>
<p>特征工程这块，在特征构造的时候，需要借助一些<strong>背景知识</strong>，遵循的一般原则就是我们需要发挥想象力，尽可能多的创造特征，不用先考虑哪些特征可能好，可能不好，先弥补这个广度，而特征构造的时候数值特征，类别特征，时间特征又得分开处理：</p>
<ul>
<li>对于数值特征，我们一般会尝试一些它们之间的加减组合（当然不要乱来，根据特征表达的含义）或者提取一些统计特征。</li>
<li>对于类别特征，我们一般会尝试之间的交叉组合，embedding 也是一种思路。</li>
<li>对于时间特征，这一块又可以作为一个大专题来学习，在时间序列的预测中这一块非常重要，也会非常复杂，需要就尽可能多的挖掘时间信息，会有不同的方式技巧。</li>
</ul>
<h2 id="shu-zhi-te-zheng-gou-zao">数值特征构造</h2>
<p>数值特征这块，由于大部分都是匿名特征，处理起来不是太好处理，只能尝试一些加减组合和统计特征。</p>
<h3 id="fen-xiang">分箱</h3>
<p>一部车有效寿命 30 万公里，将其分为 5 段，每段 6 万公里，每段价值依序为新车价的 5/15、4/15、3/15、2/15、1/15。假设新车价 12 万元，已行驶 7.5 万公里（5 年左右），那么该车估值为 $12×(3+3+2+1)÷15=7.2 $万元.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分成三段</span></span><br><span class="line">bins = [<span class="number">0</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>]</span><br><span class="line">num_data[<span class="string">'kil_bin'</span>] = pd.cut(num_data[<span class="string">'kilometer'</span>], bins, labels=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h3 id="tong-ji-te-zheng">统计特征</h3>
<p>平均值， 总和和标准差</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v_features = [<span class="string">'v_'</span> + str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">15</span>)]</span><br><span class="line">num_data[<span class="string">'v_sum'</span>] = num_data[v_features].apply(<span class="keyword">lambda</span> x: x.sum(), axis=<span class="number">1</span>)</span><br><span class="line">num_data[<span class="string">'v_mean'</span>] = num_data[v_features].apply(<span class="keyword">lambda</span> x: x.mean(), axis=<span class="number">1</span>)</span><br><span class="line">num_data[<span class="string">'v_std'</span>] = num_data[v_features].apply(<span class="keyword">lambda</span> x: x.std(), axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GADWZT.png" alt></p>
<h2 id="lei-bie-te-zheng">类别特征</h2>
<p>经过分析，可以构造的类别特征如下：</p>
<ol>
<li>从邮编中提取城市信息，因为是德国的数据，所以参考德国的邮编，加入先验知识，但是感觉这个没有用，可以先试一下</li>
<li>最好是从 regioncode 中提取出是不是华东地区，因为华东地区是二手车交易的主要地区</li>
<li>私用车和商用车分开（bodyType 提取）</li>
<li>微型车单独处理</li>
<li>新能源车和燃油车分开（在 fuelType 中提取，然后进行 One-Hot）</li>
<li>地区编码还是有影响的， 不同的地区汽车的保率不同</li>
</ol>
<blockquote>
<p>注意，One-Hot 不要太早，否则有些特征就没法提取潜在信息了</p>
</blockquote>
<h3 id="you-bian-te-zheng">邮编特征</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从邮编中提取城市信息</span></span><br><span class="line">cat_data[<span class="string">'city'</span>] = cat_data[<span class="string">'regionCode'</span>].apply(<span class="keyword">lambda</span> x: str(x)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<h3 id="si-yong-che-he-shang-wu-che-fen-kai">私用车和商务车分开</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">com_car = [<span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">6.0</span>]  <span class="comment"># 商用车</span></span><br><span class="line">GL_car = [<span class="number">0.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>]   <span class="comment"># 豪华系列  </span></span><br><span class="line">self_car = [<span class="number">1.0</span>, <span class="number">7.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">class_bodyType</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x <span class="keyword">in</span> GL_car:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">elif</span> x <span class="keyword">in</span> com_car:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line">cat_data[<span class="string">'car_class'</span>] = cat_data[<span class="string">'bodyType'</span>].apply(<span class="keyword">lambda</span> x : class_bodyType(x))</span><br></pre></td></tr></table></figure>
<h3 id="xin-neng-yuan-che-he-ran-you-che-fen-kai">新能源车和燃油车分开</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 是否是新能源</span></span><br><span class="line">is_fuel = [<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">cat_data[<span class="string">'is_fuel'</span>] = cat_data[<span class="string">'fuelType'</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x <span class="keyword">in</span> is_fuel <span class="keyword">else</span> <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="gou-zao-tong-ji-te-zheng">构造统计特征</h3>
<p>可以根据 brand，燃油类型，gearbox 类型，车型等，这里只拿一个举例，其他的类似：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">train_data_gearbox = train_data.copy()   <span class="comment"># 不要动train_data</span></span><br><span class="line">train_data_gearbox[<span class="string">'gearbox'</span>] = cat_data[<span class="string">'gearbox'</span>][:train_data.shape[<span class="number">0</span>]]</span><br><span class="line">train_data_gearbox[<span class="string">'price'</span>] = train_target</span><br><span class="line"></span><br><span class="line">train_gb = train_data_gearbox.groupby(<span class="string">'gearbox'</span>)</span><br><span class="line">all_info = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> kind, kind_data <span class="keyword">in</span> train_gb:</span><br><span class="line">    info = &#123;&#125;</span><br><span class="line">    kind_data = kind_data[kind_data[<span class="string">'price'</span>] &gt; <span class="number">0</span>]</span><br><span class="line">    info[<span class="string">'gearbox_count'</span>] = len(kind_data)</span><br><span class="line">    info[<span class="string">'gearbox_price_max'</span>] = kind_data.price.max()</span><br><span class="line">    info[<span class="string">'gearbox_price_median'</span>] = kind_data.price.median()</span><br><span class="line">    info[<span class="string">'gearbox_price_min'</span>] = kind_data.price.min()</span><br><span class="line">    info[<span class="string">'gearbox_price_sum'</span>] = kind_data.price.sum()</span><br><span class="line">    info[<span class="string">'gearbox_std'</span>] = kind_data.price.std()</span><br><span class="line">    info[<span class="string">'gearbox_price_average'</span>] = round(kind_data.price.sum() / (len(kind_data) + <span class="number">1</span>), <span class="number">2</span>)</span><br><span class="line">    all_info[kind] = info</span><br><span class="line"></span><br><span class="line">gearbox_fe = pd.DataFrame(all_info).T.reset_index().rename(columns=&#123;<span class="string">"index"</span>: <span class="string">"gearbox"</span>&#125;)</span><br><span class="line"></span><br><span class="line">cat_data = cat_data.merge(gearbox_fe, how=<span class="string">'left'</span>, on=<span class="string">'gearbox'</span>)</span><br></pre></td></tr></table></figure>
<p>下面就可以把 bodyType 和 fuelType 删除，因为该提取的信息也提取完了，该独热的独热：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删掉bodyType和fuelType，然后把gearbox，car_classis_fuel独热一下，这个不能太早，构造完了统计特征之后再独热</span></span><br><span class="line"><span class="keyword">del</span> cat_data[<span class="string">'bodyType'</span>]</span><br><span class="line"><span class="keyword">del</span> cat_data[<span class="string">'fuelType'</span>]</span><br><span class="line"></span><br><span class="line">cat_data = pd.get_dummies(cat_data, columns=[<span class="string">'gearbox'</span>, <span class="string">'car_class'</span>, <span class="string">'is_fuel'</span>, <span class="string">'notRepairedDamage'</span>])</span><br></pre></td></tr></table></figure>
<p>这样，类别特征就构造完毕了。最终结果如下：</p>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GADF54.png" alt></p>
<h2 id="shi-jian-te-zheng">时间特征</h2>
<p>根据上面的分析，可以构造的时间特征如下：</p>
<ol>
<li>汽车的上线日期与汽车的注册日期之差就是汽车的使用时间，一般来说与价格成反比</li>
<li>对汽车的使用时间进行分箱，使用了 3 年以下，3-7 年，7-10 年和 10 年以上，分为四个等级，10 年之后就是报废车了，应该会影响价格</li>
<li>淡旺季也会影响价格，所以可以从汽车的上线日期上提取一下淡旺季信息</li>
</ol>
<h3 id="qi-che-de-shi-yong-shi-jian">汽车的使用时间</h3>
<p>createDate-regDate，反应汽车使用时间，一般来说与价格成反比，但是要注意这一块中的问题就是时间格式， regDateFalse 这个字段有些是 0 月，如果忽略错误计算的话，使用时间有一些会是空值，当然可以考虑删除这些空值，但是因为训练集和测试集合并了，那么就不轻易删除了，采取的办法，把错误字段都给他加 1 个月，然后计算出天数之后在加上 30 天（这个有不同的处理方式，但是一般不删除或者置为空，因为删除和空值都有潜在的副作用）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里是为了标记一下哪些字段有错误</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regDateFalse</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> str(x)[<span class="number">4</span>:<span class="number">6</span>] == <span class="string">'00'</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">time_data[<span class="string">'regDateFalse'</span>] = time_data[<span class="string">'regDate'</span>].apply(<span class="keyword">lambda</span> x: regDateFalse(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里是改正错误字段</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">changeFalse</span><span class="params">(x)</span>:</span></span><br><span class="line">    x = str(x)</span><br><span class="line">    <span class="keyword">if</span> x[<span class="number">4</span>:<span class="number">6</span>] == <span class="string">'00'</span>:</span><br><span class="line">        x = x[<span class="number">0</span>:<span class="number">4</span>] + <span class="string">'01'</span> + x[<span class="number">6</span>:]</span><br><span class="line">        x = int(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line">time_data[<span class="string">'regDate'</span>] = time_data[<span class="string">'regDate'</span>].apply(<span class="keyword">lambda</span> x: changeFalse(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用时间：data['creatDate'] - data['regDate']，反应汽车使用时间，一般来说价格与使用时间成反比</span></span><br><span class="line"><span class="comment"># 不过要注意，数据里有时间出错的格式，所以我们需要 errors='coerce'</span></span><br><span class="line">time_data[<span class="string">'used_time'</span>] = (pd.to_datetime(time_data[<span class="string">'creatDate'</span>], format=<span class="string">'%Y%m%d'</span>) - </span><br><span class="line">                            pd.to_datetime(time_data[<span class="string">'regDate'</span>], format=<span class="string">'%Y%m%d'</span>)).dt.days</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改错误</span></span><br><span class="line"><span class="comment"># 但是需要加上那一个月</span></span><br><span class="line">time_data.loc[time_data.regDateFalse==<span class="number">1</span>, <span class="string">'used_time'</span>] += <span class="number">30</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除标记列</span></span><br><span class="line"><span class="keyword">del</span> time_data[<span class="string">'regDateFalse'</span>]</span><br></pre></td></tr></table></figure>
<p>这样，一个特征构造完毕，used_time 字段，表示汽车的使用时间。</p>
<h3 id="qi-che-shi-fou-bao-fei">汽车是否报废</h3>
<p>时间特征还可以继续提取，我们假设用了 10 年的车作为报废车的话，那么我们可以根据使用天数计算出年数，然后根据年数构造出一个特征是不是报废</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用时间换成年来表示</span></span><br><span class="line">time_data[<span class="string">'used_time'</span>] = time_data[<span class="string">'used_time'</span>] / <span class="number">365.0</span></span><br><span class="line">time_data[<span class="string">'Is_scrap'</span>] = time_data[<span class="string">'used_time'</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x&gt;=<span class="number">10</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>我们还可以对 used_time 进行分箱，这个是根据背景估价的方法可以发现，汽车的使用时间 3 年，3-7 年，10 年以上的估价会有不同，所以分一下箱：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bins = [<span class="number">0</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>]</span><br><span class="line">time_data[<span class="string">'estivalue'</span>] = pd.cut(time_data[<span class="string">'used_time'</span>], bins, labels=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h3 id="dan-wang-ji">淡旺季</h3>
<p>根据汽车的上线售卖时间看，每年的 2，3 月份及 6,7,8 月份是整个汽车行业的低谷，年初和年末及 9 月份是二手车销售的黄金时期，所以根据上线时间选出淡旺季：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选出淡旺季</span></span><br><span class="line">low_seasons = [<span class="string">'3'</span>, <span class="string">'6'</span>, <span class="string">'7'</span>, <span class="string">'8'</span>]</span><br><span class="line">time_data[<span class="string">'is_low_seasons'</span>] = time_data[<span class="string">'creatDate'</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> str(x)[<span class="number">5</span>] <span class="keyword">in</span> low_seasons <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 独热一下</span></span><br><span class="line">time_data = pd.get_dummies(time_data, columns=[<span class="string">'is_low_seasons'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这样时间特征构造完毕，删除日期了</span></span><br><span class="line"><span class="keyword">del</span> time_data[<span class="string">'regDate'</span>]</span><br><span class="line"><span class="keyword">del</span> time_data[<span class="string">'creatDate'</span>]</span><br></pre></td></tr></table></figure>
<p>看一下最后的构造结果，报废特征没有构造，因为发现了一个特点就是这里的数据 10 年以上的车会偏斜，所以感觉这个用 10 年作为分界线不太合适，只是提供一种思路</p>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GA0d6P.png" alt></p>
<h3 id="gen-ju-qi-che-de-shi-yong-shi-jian-huo-zhe-dan-wang-ji-fen-tong-jin-xing-tong-ji-te-zheng-de-gou-zao">根据汽车的使用时间或者淡旺季分桶进行统计特征的构造</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造统计特征的话需要在训练集上先计算</span></span><br><span class="line">train_data_timestats = train_data.copy()   <span class="comment"># 不要动train_data</span></span><br><span class="line"></span><br><span class="line">train_data_timestats[<span class="string">'estivalue'</span>] = time_data[<span class="string">'estivalue'</span>][:train_data.shape[<span class="number">0</span>]]</span><br><span class="line">train_data_timestats[<span class="string">'price'</span>] = train_target</span><br><span class="line"></span><br><span class="line">train_gt = train_data_timestats.groupby(<span class="string">'estivalue'</span>)</span><br><span class="line">all_info = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> kind, kind_data <span class="keyword">in</span> train_gt:</span><br><span class="line">    info = &#123;&#125;</span><br><span class="line">    kind_data = kind_data[kind_data[<span class="string">'price'</span>] &gt; <span class="number">0</span>]</span><br><span class="line">    info[<span class="string">'estivalue_count'</span>] = len(kind_data)</span><br><span class="line">    info[<span class="string">'estivalue_price_max'</span>] = kind_data.price.max()</span><br><span class="line">    info[<span class="string">'estivalue_price_median'</span>] = kind_data.price.median()</span><br><span class="line">    info[<span class="string">'estivalue_price_min'</span>] = kind_data.price.min()</span><br><span class="line">    info[<span class="string">'estivalue_price_sum'</span>] = kind_data.price.sum()</span><br><span class="line">    info[<span class="string">'estivalueprice_std'</span>] = kind_data.price.std()</span><br><span class="line">    info[<span class="string">'estivalue_price_average'</span>] = round(kind_data.price.sum() / (len(kind_data) + <span class="number">1</span>), <span class="number">2</span>)</span><br><span class="line">    all_info[kind] = info</span><br><span class="line"></span><br><span class="line">estivalue_fe = pd.DataFrame(all_info).T.reset_index().rename(columns=&#123;<span class="string">"index"</span>: <span class="string">"estivalue"</span>&#125;)</span><br><span class="line">time_data = time_data.merge(estivalue_fe, how=<span class="string">'left'</span>, on=<span class="string">'estivalue'</span>)</span><br></pre></td></tr></table></figure>
<p>这样，时间特征就基本构造完毕，最后的结果如下：</p>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GA0fXV.png" alt></p>
<p>这样，时间特征这块就构造了 10 个特征出来，当然还可以更多，由于篇幅原因，其他的可以自行尝试。</p>
<h2 id="he-bing-shu-ju">合并数据</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">final_data = pd.concat([num_data, cat_data, time_data], axis=<span class="number">1</span>)</span><br><span class="line">final_data.shape  <span class="comment"># (200000, 74)</span></span><br><span class="line">final_data.head()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GArCQI.png" alt></p>
<h1 id="te-zheng-xuan-ze">特征选择</h1>
<p>好的特征选择能够提升模型的性能，更能帮助我们理解数据的特点、底层结构，这对进一步改善模型、算法都有着重要作用。但是拿到数据集，一个特征选择方法，往往很难达到目的。通常情况下，我们经常不管三七二十一，选择一种自己最熟悉或者最方便的特征选择方法（往往目的是降维，而忽略了对特征和数据理解的目的）</p>
<p>特征选择主要有两个功能：</p>
<ul>
<li>减少特征数量、降维，使模型泛化能力更强，减少过拟合</li>
<li>增强对特征和特征值之间的理解</li>
</ul>
<p>通常来说，从两个方面考虑来选择特征：</p>
<ul>
<li>特征是否发散：如果一个特征不发散，例如方差接近于 0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用</li>
<li>特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择</li>
</ul>
<p>根据特征选择的形式又可以将特征选择方法分为 3 种：</p>
<ol>
<li>Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征</li>
<li>Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征</li>
<li>Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于 Filter 方法，但是是通过训练来确定特征的优劣</li>
</ol>
<h2 id="guo-lu-shi">过滤式</h2>
<p><strong>主要思想</strong>：对每一维特征 “打分”，即给每一维的特征赋予权重，这样的权重就代表着该特征的重要性，然后依据权重排序。先进行特征选择，然后去训练学习器，所以特征选择的过程与学习器无关。相当于先对特征进行过滤操作，然后用特征子集来训练分类器</p>
<p><strong>主要方法</strong>：</p>
<ol>
<li>移除低方差的特征</li>
<li>相关系数排序，分别计算每个特征与输出值之间的相关系数，设定一个阈值，选择相关系数大于阈值的部分特征</li>
<li>利用假设检验得到特征与输出值之间的相关性，方法有比如卡方检验、t 检验、F 检验等</li>
<li>互信息，利用互信息从信息熵的角度分析相关性</li>
</ol>
<p>**Trick1：**对于数值型特征，方差很小的特征可以不要，因为太小没有什么区分度，提供不了太多的信息，对于分类特征，也是同理，取值个数高度偏斜的那种可以先去掉</p>
<p>**Trick2：**根据与目标的相关性等选出比较相关的特征（当然有时候根据字段含义也可以选）</p>
<p>**Trick3：**方检验一般是检查离散变量与离散变量的相关性，当然离散变量的相关性信息增益和信息增益比也是不错的选择（可以通过决策树模型来评估来看），person 系数一般是查看连续变量与连续变量的线性相关关系</p>
<h3 id="qu-diao-qu-zhi-bian-hua-xiao-de-te-zheng">去掉取值变化小的特征</h3>
<p>这应该是最简单的特征选择方法了：假设某特征的特征值只有 0 和 1，并且在所有输入样本中，95% 的实例的该特征取值都是 1，那就可以认为这个特征作用不大。如果 100% 都是 1，那这个特征就没意义了。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用，而且实际当中，一般不太会有 95% 以上都取某个值的特征存在，所以这种方法虽然简单但是不太好用。可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。例如 seller 和 offerType</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对方差的大小排序</span></span><br><span class="line">select_data.std().sort_values()    <span class="comment"># select_data是final_data去掉了独热的那些特征</span></span><br></pre></td></tr></table></figure>
<p>根据这个，可以把方差非常小的特征作为备选的删除特征（备选，先盲目删除）</p>
<h3 id="dan-bian-liang-te-zheng-xuan-ze">单变量特征选择</h3>
<p>单变量特征选择能够对每一个特征进行测试，衡量该特征和响应变量之间的关系，根据得分扔掉不好的特征。对于回归和分类问题可以采用卡方检验等方式对特征进行测试</p>
<p>下面重点介绍一下 pearson 相关系数，皮尔森相关系数是一种最简单的，比较常用的方式。能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为 [-1，1]，-1 表示完全的负相关(这个变量下降，那个就会上升)，+1 表示完全的正相关，0 表示没有线性相关。Pearson Correlation 速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的) 之后第一时间就执行。Scipy 的 pearsonr 方法能够同时计算相关系数和 p-value，当然 pandas 的 corr 也可以计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">corr = select_data.corr(<span class="string">'pearson'</span>)    <span class="comment"># .corr('spearman')</span></span><br><span class="line">plt.figure(figsize=(<span class="number">25</span>, <span class="number">15</span>))</span><br><span class="line">corr[<span class="string">'price'</span>].sort_values(ascending=<span class="literal">False</span>)[<span class="number">1</span>:].plot(kind=<span class="string">'bar'</span>)</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GAsi9J.png" alt></p>
<p>当然，这个数据用 pearson 系数可能不是那么合理，可以使用 spearman 系数，这个被认为是排列后的变量的 pearson 的相关系数。</p>
<blockquote>
<p>皮尔逊相关系数:下面是皮尔逊相关系数的计算公式，只需要将（X和Y的协方差）/（X的标准差*Y的标准差）<br>
\[
\rho_{X, Y}=\frac{\operatorname{cov}(X, Y)}{\sigma_{X} \sigma_{Y}}=\frac{E\left(\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right)}{\sigma_{X} \sigma_{Y}}
\]</p>
<p>spearman相关系数:Spearman秩相关系数通常被认为是排列后的变量之间的Pearson线性相关系数.</p>
<p>那么对于这两个系数，怎样的值才是好的呢，遵循下面的关系:</p>
<ul>
<li><strong>0.8-1.0</strong>：极强相关</li>
<li><strong>0.6-0.8</strong>：强相关</li>
<li><strong>0.4-0.6</strong>：中等强度相关</li>
<li><strong>0.2-0.4</strong>：弱相关</li>
<li><strong>0.0-0.2</strong>：极弱或者无相关</li>
</ul>
<p>区别:</p>
<ul>
<li>连续数据，正态分布，线性关系，用 pearson 相关系数是最恰当，当然用 spearman 相关系数也可以，效率没有 pearson 相关系数高</li>
<li>上述任一条件不满足，就用 spearman 相关系数</li>
<li>两个定序测量数据（顺序变量）之间也用 spearman 相关系数，不能用 pearson 相关系数</li>
<li>Pearson 相关系数的一个<strong>明显缺陷</strong>是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson 相关性也可能会接近 0</li>
</ul>
<p>代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = loans_2007[[<span class="string">"funded_amnt"</span>, <span class="string">"funded_amnt_inv"</span>]]</span><br><span class="line"><span class="comment">#计算皮尔逊系数</span></span><br><span class="line">print(data.corr())</span><br><span class="line"><span class="comment">#计算spearman系数</span></span><br><span class="line">print(data.corr(<span class="string">'spearman'</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                 funded_amnt  funded_amnt_inv</span><br><span class="line">funded_amnt         <span class="number">1.000000</span>         <span class="number">0.947525</span></span><br><span class="line">funded_amnt_inv     <span class="number">0.947525</span>         <span class="number">1.000000</span></span><br><span class="line">                 funded_amnt  funded_amnt_inv</span><br><span class="line">funded_amnt          <span class="number">1.00000</span>          <span class="number">0.92876</span></span><br><span class="line">funded_amnt_inv      <span class="number">0.92876</span>          <span class="number">1.00000</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>当然还可以画出热力图来，这个的目的是可以看变量之间的关系，相关性大的，可以考虑保留其中一个：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">f, ax = plt.subplots(figsize=(<span class="number">50</span>, <span class="number">30</span>))</span><br><span class="line">sns.heatmap(corr, annot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GAsJDP.png" alt></p>
<p>经过上面两个步骤就可以发现一些结论：</p>
<ul>
<li>根据与 price 的线性相关关系来看的话，可以考虑正负相关 0.6 以上的特征， v_std, v_12, v_0, v_8, estivalue_price_average, estivalue_price_median, estivalue_price_std， kil_bin, kilmoeter, estivalue_count, used_time, estivalue, v_3</li>
<li>某些变量之间有很强的的关联性，比如 v_mean 和 v_sum，这俩的相关性是 1，所以可以删掉其中一个</li>
</ul>
<p>当然，依然是备选删除选项和备选保留选项（这些都先别做），因为我们有时候不能盲目，就比如上面的相关性，我们明明知道 pearson 的缺陷是无法捕捉非线性相关，所以得出的这个结论也是片面的结论，所以这些都是备选，先做个心中有数，后面再用一些别的方式看看再说（如果现在就删除了，后面的方法就不好判断了）</p>
<h2 id="bao-guo-shi">包裹式</h2>
<p>单变量特征选择方法独立的衡量每个特征与响应变量之间的关系，另一种主流的特征选择方法是基于机器学习模型的方法。有些机器学习方法本身就具有对特征进行打分的机制，或者很容易将其运用到特征选择任务中，例如回归模型，SVM，决策树，随机森林等等。</p>
<p><strong>主要思想</strong>：包裹式从初始特征集合中不断的选择特征子集，训练学习器，根据学习器的性能来对子集进行评价，直到选择出最佳的子集。包裹式特征选择直接针对给定学习器进行优化。</p>
<p><strong>主要方法</strong>：递归特征消除算法，基于机器学习模型的特征排序。</p>
<p><strong>优缺点</strong>：</p>
<ul>
<li>优点：从最终学习器的性能来看，包裹式比过滤式更好</li>
<li>缺点：由于特征选择过程中需要多次训练学习器，因此包裹式特征选择的计算开销通常比过滤式特征选择要大得多</li>
</ul>
<p>基于学习模型的特征排序方法，这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。其实 Pearson 相关系数等价于线性回归里的标准化回归系数。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。</p>
<p>用随机森林来跑一下，看看随机森林比较喜欢特征：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score, ShuffleSplit</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"></span><br><span class="line">X = select_data.iloc[:, :<span class="number">-1</span>]</span><br><span class="line">Y = select_data[<span class="string">'price'</span>]</span><br><span class="line">names = select_data.columns</span><br><span class="line"></span><br><span class="line">rf = RandomForestRegressor(n_estimators=<span class="number">20</span>, max_depth=<span class="number">4</span>)</span><br><span class="line">kfold = KFold(n_splits=<span class="number">5</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">7</span>)</span><br><span class="line">scores = []</span><br><span class="line"><span class="keyword">for</span> column <span class="keyword">in</span> X.columns:</span><br><span class="line">    print(column)</span><br><span class="line">    tempx = X[column].values.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    score = cross_val_score(rf, tempx, Y, scoring=<span class="string">"r2"</span>,</span><br><span class="line">                              cv=kfold)</span><br><span class="line">    scores.append((round(np.mean(score), <span class="number">3</span>), column))</span><br><span class="line">print(sorted(scores, reverse=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<p>这里对喜欢的特征排序并打分，结果如下：</p>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GAyQaT.png" alt></p>
<p>这里就可以看出随机森林有用的特征排序，如果我们后面选择随机森林作为模型，就可以根据这个特征重要度选择特征。当然，如果是 xgboost，xgboost 里面有个画特征重要性的函数，可以这样做：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用xgboost跑一下</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> plot_importance</span><br><span class="line"></span><br><span class="line">xgb = XGBRegressor()</span><br><span class="line">xgb.fit(X, Y)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">plot_importance(xgb)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GAyrJe.png" alt></p>
<p>最后，把上面的这两种方式封装起来，还可以画出边际效应：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mlxtend.feature_selection <span class="keyword">import</span> SequentialFeatureSelector <span class="keyword">as</span> SFS</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># sfs = SFS(LinearRegression(), k_features=20, forward=True, floating=False, scoring='r2', cv=0)</span></span><br><span class="line">sfs = SFS(RandomForestRegressor(n_estimators=<span class="number">10</span>, max_depth=<span class="number">4</span>), k_features=<span class="number">20</span>, forward=<span class="literal">True</span>, floating=<span class="literal">False</span>, scoring=<span class="string">'r2'</span>, cv=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">X = select_data.iloc[:, :<span class="number">-1</span>]</span><br><span class="line">Y = select_data[<span class="string">'price'</span>]</span><br><span class="line"></span><br><span class="line">sfs.fit(X, Y)</span><br><span class="line">sfs.k_feature_names_    <span class="comment"># 随机森林放这里跑太慢了，所以中断了</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GAyIJg.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 画出边际效应</span></span><br><span class="line"><span class="keyword">from</span> mlxtend.plotting <span class="keyword">import</span> plot_sequential_feature_selection <span class="keyword">as</span> plot_sfs</span><br><span class="line"></span><br><span class="line">fig1 = plot_sfs(sfs.get_metric_dict(), kind=<span class="string">'std_dev'</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/04/02/machine_learning/feature_engineering/GA69SJ.png" alt></p>
<p>综合上面的这几种方式，就可以把保留和删除的特征给选出来了。</p>
<p>如果真的这样尝试一下，就会发现保留的特征里面， v_std, v_3, used_time, power, kilometer, estivalue 等这些特征都在，虽然不知道 v 系列特征的含义，但是汽车使用时间，发动机功率，行驶公里， 汽车使用时间的分箱特征其实对 price 的影响都是比较大的。</p>
<p>下面在介绍一种嵌入式的方式，当然这里我没用，因为我不打算后面的模型用线性模型来做。但这种思路得知道。</p>
<h2 id="qian-ru-shi">嵌入式</h2>
<p>在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。而嵌入式特征选择在学习器训练过程中自动地进行特征选择。嵌入式选择最常用的是 L1 正则化与 L2 正则化。在对线性回归模型加入两种正则化方法后，他们分别变成了岭回归与 Lasso 回归。</p>
<p><strong>主要思想</strong>：在模型既定的情况下学习出对提高模型准确性最好的特征。也就是在确定模型的过程中，挑选出那些对模型的训练有重要意义的特征。</p>
<p><strong>主要方法</strong>：简单易学的机器学习算法–岭回归（Ridge Regression），就是线性回归过程加入了 L2 正则项</p>
<ul>
<li>L2 正则化在拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。</li>
<li>L1 正则化有助于生成一个稀疏权值矩阵，进而可以用于特征选择。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression, Ridge,Lasso</span><br><span class="line"></span><br><span class="line">models = [LinearRegression(), Ridge(), Lasso()]</span><br><span class="line">result = dict()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">    model_name = str(model).split(<span class="string">'('</span>)[<span class="number">0</span>]</span><br><span class="line">    scores = cross_val_score(model, X=train_X, y=train_y, verbose=<span class="number">0</span>, cv=<span class="number">5</span>, scoring=<span class="string">'r2'</span>)</span><br><span class="line">    result[model_name] = scores</span><br></pre></td></tr></table></figure>
<h1 id="pca-jiang-wei">PCA 降维</h1>
<p>通过上面的特征选择部分，可以选出更好的分析特征，但是<strong>如果这些特征维度仍然很高</strong>怎么办？</p>
<p>如果数据特征维度太高，首先计算很麻烦，其次增加了问题的复杂程度，分析起来也不方便。这时候我们就会想是不是再去掉一些特征就好了呢？但是这个特征也不是凭自己的意愿去掉的，因为盲目减少数据的特征会损失掉数据包含的关键信息，容易产生错误的结论，对分析不利。所以我们想找到一个合理的方式，既可以减少我们需要分析的指标，而且尽可能多的保持原来数据的信息，PCA 就是这个合理的方式之一。</p>
<p>这里只整理如何用，但要注意一点，特征选择是从已存在的特征中选取携带信息最多的，选完之后的特征依然具有可解释性，而 PCA，将已存在的特征压缩，降维完毕后不是原来特征的任何一个，也就是 PCA 降维之后的特征我们根本不知道什么含义了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后使用</span></span><br><span class="line">pca = PCA(n_components=<span class="number">10</span>)</span><br><span class="line">X_new = pca.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="string">"""查看PCA的一些属性"""</span></span><br><span class="line">print(X_new.shape)   <span class="comment"># （200000， 10）</span></span><br><span class="line">print(pca.explained_variance_)    <span class="comment"># 属性可以查看降维后的每个特征向量上所带的信息量大小（可解释性方差的大小）</span></span><br><span class="line">print(pca.explained_variance_ratio_)  <span class="comment"># 查看降维后的每个新特征的信息量占原始数据总信息量的百分比</span></span><br><span class="line">print(pca.explained_variance_ratio_.sum())    <span class="comment"># 降维后信息保留量</span></span><br></pre></td></tr></table></figure>
<p>假设我保留了 10 个特征，然后运行代码，一下子就成了 10 维的矩阵，我们可以看一下 X_new：</p>
<p><img src="https://s1.ax1x.com/2020/03/28/GAcZBq.png#shadow" alt="img"></p>
<p>这些数已经只能说尽可能的保留原有的数据信息，但是是什么含义，我们也不知道了。</p>
<h1 id="can-kao">参考</h1>
<ol>
<li>
<p><a href="https://blog.csdn.net/wuzhongqiang/article/details/105146150" target="_blank" rel="noopener">特征选择，我们真的学会了吗？</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/wuzhongqiang/article/details/104607832" target="_blank" rel="noopener">白话机器学习算法理论 + 实战之 PCA 降维</a></p>
</li>
<li>
<p><a href="https://wmathor.com/index.php/archives/1426/" target="_blank" rel="noopener">特征工程</a></p>
</li>
</ol>

    </div>

    
    
    <div>
  
    <div style="text-align:center;color:#bfbfbf;font-size:16px;">
      <span>-------- 本文结束 </span>
      <i class="fa fa-coffee"></i>
      <span> 感谢阅读 --------</span>
    </div>
  
</div>
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    鼓励一下
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat.png" alt="Li Zhen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/ali.png" alt="Li Zhen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" rel="tag"><i class="fa fa-tag"></i> 特征工程</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/04/01/machine_learning/exploratory_data_analysis/" rel="prev" title="数据探索性分析(EDA)">
      <i class="fa fa-chevron-left"></i> 数据探索性分析(EDA)
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/04/02/machine_learning/modeling_modify_parameters/" rel="next" title="建模调参">
      建模调参 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#te-zheng-gou-zao"><span class="nav-number">1.</span> <span class="nav-text">特征构造</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#shu-zhi-te-zheng-gou-zao"><span class="nav-number">1.1.</span> <span class="nav-text">数值特征构造</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#fen-xiang"><span class="nav-number">1.1.1.</span> <span class="nav-text">分箱</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tong-ji-te-zheng"><span class="nav-number">1.1.2.</span> <span class="nav-text">统计特征</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lei-bie-te-zheng"><span class="nav-number">1.2.</span> <span class="nav-text">类别特征</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#you-bian-te-zheng"><span class="nav-number">1.2.1.</span> <span class="nav-text">邮编特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#si-yong-che-he-shang-wu-che-fen-kai"><span class="nav-number">1.2.2.</span> <span class="nav-text">私用车和商务车分开</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xin-neng-yuan-che-he-ran-you-che-fen-kai"><span class="nav-number">1.2.3.</span> <span class="nav-text">新能源车和燃油车分开</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gou-zao-tong-ji-te-zheng"><span class="nav-number">1.2.4.</span> <span class="nav-text">构造统计特征</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shi-jian-te-zheng"><span class="nav-number">1.3.</span> <span class="nav-text">时间特征</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#qi-che-de-shi-yong-shi-jian"><span class="nav-number">1.3.1.</span> <span class="nav-text">汽车的使用时间</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#qi-che-shi-fou-bao-fei"><span class="nav-number">1.3.2.</span> <span class="nav-text">汽车是否报废</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dan-wang-ji"><span class="nav-number">1.3.3.</span> <span class="nav-text">淡旺季</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gen-ju-qi-che-de-shi-yong-shi-jian-huo-zhe-dan-wang-ji-fen-tong-jin-xing-tong-ji-te-zheng-de-gou-zao"><span class="nav-number">1.3.4.</span> <span class="nav-text">根据汽车的使用时间或者淡旺季分桶进行统计特征的构造</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#he-bing-shu-ju"><span class="nav-number">1.4.</span> <span class="nav-text">合并数据</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#te-zheng-xuan-ze"><span class="nav-number">2.</span> <span class="nav-text">特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#guo-lu-shi"><span class="nav-number">2.1.</span> <span class="nav-text">过滤式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#qu-diao-qu-zhi-bian-hua-xiao-de-te-zheng"><span class="nav-number">2.1.1.</span> <span class="nav-text">去掉取值变化小的特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dan-bian-liang-te-zheng-xuan-ze"><span class="nav-number">2.1.2.</span> <span class="nav-text">单变量特征选择</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bao-guo-shi"><span class="nav-number">2.2.</span> <span class="nav-text">包裹式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#qian-ru-shi"><span class="nav-number">2.3.</span> <span class="nav-text">嵌入式</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pca-jiang-wei"><span class="nav-number">3.</span> <span class="nav-text">PCA 降维</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#can-kao"><span class="nav-number">4.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <a href='/'>
    <img class="site-author-image" itemprop="image" alt="Li Zhen"
      src="/images/snoppy.jpeg">
  </a>
  <p class="site-author-name" itemprop="name">Li Zhen</p>
  <div class="site-description" itemprop="description">他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">55</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">62</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jeffery0628" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jeffery0628" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jeffery.lee.0628@gmail.com" title="邮箱 → mailto:jeffery.lee.0628@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮箱</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Zhen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">823k</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  






  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>


 

<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180101,"YYYYMMDD"));
    ages = ages.replace(/years?/, "年");
    ages = ages.replace(/months?/, "月");
    ages = ages.replace(/days?/, "天");
    ages = ages.replace(/hours?/, "小时");
    ages = ages.replace(/minutes?/, "分");
    ages = ages.replace(/seconds?/, "秒");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `我已在此等候你 ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


<script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>

<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdnjs.cloudflare.com/ajax/libs/valine/1.3.10/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'tChTbEIggDSVcdhPBUf0MFxW-gzGzoHsz',
      appKey     : 'sJ6xUiFnifEDIIfnj3Ut9zy1',
      placeholder: "留下你的小脚印吧！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '8' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
