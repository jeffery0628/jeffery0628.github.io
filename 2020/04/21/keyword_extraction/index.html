<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/snoppy.jpeg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open Sans:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jeffery.ink","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":15,"offset":12,"onmobile":true,"dimmer":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":true,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":true,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="关键词抽取">
<meta property="og:url" content="https://jeffery.ink/2020/04/21/keyword_extraction/index.html">
<meta property="og:site_name" content="火种2号">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jeffery.ink/2020/04/21/keyword_extraction/gjc01.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/21/keyword_extraction/pagerank.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/21/keyword_extraction/rank_leak.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/21/keyword_extraction/rank_sink.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/21/keyword_extraction/textrank.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/21/keyword_extraction/lda.png">
<meta property="article:published_time" content="2020-04-21T06:40:43.000Z">
<meta property="article:modified_time" content="2020-05-12T03:58:32.125Z">
<meta property="article:author" content="Li Zhen">
<meta property="article:tag" content="nlp">
<meta property="article:tag" content="key word extraction">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jeffery.ink/2020/04/21/keyword_extraction/gjc01.png">

<link rel="canonical" href="https://jeffery.ink/2020/04/21/keyword_extraction/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>关键词抽取 | 火种2号</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">火种2号</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">火种计划</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>简历</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>读书</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-video fa-fw"></i>电影</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>



</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jeffery.ink/2020/04/21/keyword_extraction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/snoppy.jpeg">
      <meta itemprop="name" content="Li Zhen">
      <meta itemprop="description" content="他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="火种2号">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          关键词抽取
        </h1>

        <div class="post-meta">
          
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-12 11:58:32" itemprop="dateModified" datetime="2020-05-12T11:58:32+08:00">2020-05-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">技术/自然语言处理</span></a>
                </span>
            </span>

          
            <span id="/2020/04/21/keyword_extraction/" class="post-meta-item leancloud_visitors" data-flag-title="关键词抽取" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论：</span>
    
    <a title="valine" href="/2020/04/21/keyword_extraction/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/04/21/keyword_extraction/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>16k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>14 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/2020/04/21/keyword_extraction/gjc01.png" alt></p>
<a id="more"></a>
<h1 id="tf-idf-ti-qu-guan-jian-ci">TF-IDF 提取关键词</h1>
<h2 id="jian-jie">简介</h2>
<p>TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率).TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一份文件在一个语料库中的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。换句话说就是：<strong>一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章。</strong></p>
<blockquote>
<p>TF-IDF 算法主要适用于英文，中文首先要分词，分词后要解决多词一义，以及一词多义问题，这两个问题通过简单的tf-idf方法不能很好的解决。于是就有了后来的词嵌入方法，用向量来表征一个词。</p>
</blockquote>
<h2 id="tf-idf">TF-IDF</h2>
<h3 id="tf">TF</h3>
<p>表示词条（关键字）在文本中出现的次数（frequency）(一般不用)。TF 背后的隐含的假设是，查询关键字中的单词应该相对于其他单词更加重要，而文档的重要程度，也就是相关度，与单词在文档中出现的次数成正比。比如，“Car” 这个单词在文档 A 里出现了 5 次，而在文档 B 里出现了 20 次，那么 TF 计算就认为文档 B 可能更相关。</p>
<h4 id="bian-chong-yi-tong-guo-dui-shu-han-shu-bi-mian-tf-xian-xing-zeng-chang">变种一:通过对数函数避免 TF 线性增长</h4>
<p>理由：虽然我们一般认为一个文档包含查询关键词多次相对来说表达了某种相关度，但这样的关系很难说是线性的。以 “Car Insurance” 为例，文档 A 可能包含 “Car” 这个词 100 次，而文档 B 可能包含 200 次，是不是说文档 B 的相关度就是文档 A 的 2 倍呢？其实，当这种频次超过了某个阈值之后，这个 TF 也就没那么有区分度了。</p>
<p><strong>用 Log，也就是对数函数，对 TF 进行变换，就是一个不让 TF 线性增长的技巧</strong>。具体来说，人们常常用 1+Log(TF) 这个值来代替原来的 TF 取值。在这样新的计算下，假设 “Car” 出现一次，新的值是 1，出现 100 次，新的值是 \(log 100=5.6\)，而出现 200 次，新的值是\(log 200 = 6.3\)。很明显，这样的计算保持了一个平衡，既有区分度，但也不至于完全线性增长。</p>
<h4 id="bian-chong-er-biao-zhun-hua-jie-jue-chang-wen-dang-duan-wen-dang-wen-ti">变种二：标准化解决长文档、短文档问题</h4>
<p>经典的计算并没有考虑 “长文档” 和“短文档”的区别。一个文档 A 有 3,000 个单词，一个文档 B 有 250 个单词，很明显，即便 “Car” 在这两个文档中都同样出现过 20 次，也不能说这两个文档都同等相关。<strong>对 TF 进行 “标准化”（Normalization），特别是根据文档的最大 TF 值进行的标准化，成了另外一个比较常用的技巧</strong>。<br>
\[
tf_{ij} = \frac{n_{i,j}}{\sum_k n_{k,j}}
\]<br>
其中\(n_{i,j}\)是词\(w_i\)在文档\(d_j\)中出现的次数，分母是文档\(d_j\)中所有词汇出现的次数总和。</p>
<h3 id="idf">IDF</h3>
<p>仅有 TF 不能比较完整地描述文档的相关度。因为语言的因素，有一些单词可能会比较自然地在很多文档中反复出现，比如英语中的 “The”、“An”、“But” 等等。这些词大多起到了链接语句的作用，是保持语言连贯不可或缺的部分。然而，如果我们要搜索 “How to Build A Car” 这个关键词，其中的 “How”、“To” 以及 “A” 都极可能在绝大多数的文档中出现，这个时候 TF 就无法帮助我们区分文档的相关度了。</p>
<p>逆文档频率的思路很简单，就是我们需要去 “惩罚”（Penalize）那些出现在太多文档中的单词。</p>
<h4 id="bian-chong-san-dui-shu-han-shu-chu-li-idf">变种三：对数函数处理 IDF</h4>
<p>某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。<br>
\[
idf_w = log{\frac{|D|}{|j:t_i \in d_j|}}
\]</p>
<p>其中，\(|D|\)表示语料库中文档总数，分母为包含词w的文档数+1，分母加1是为了避免出现分母为零的情况。</p>
<p>样做的好处就是，第一，使用了文档总数来做标准化，很类似上面提到的标准化的思路；第二，利用对数来达到非线性增长的目的。</p>
<h4 id="bian-chong-si-cha-xun-ci-ji-wen-dang-xiang-liang-biao-zhun-hua">变种四：查询词及文档向量标准化</h4>
<p>对查询关键字向量，以及文档向量进行标准化，使得这些向量能够不受向量里有效元素多少的影响，也就是不同的文档可能有不同的长度。在线性代数里，可以把向量都标准化为一个单位向量的长度。这个时候再进行点积运算，就相当于在原来的向量上进行余弦相似度的运算。所以，另外一个角度利用这个规则就是直接在多数时候进行余弦相似度运算，以代替点积运算。</p>
<h3 id="tf-idf-1">TF-IDF</h3>
<p>某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语(关键词)。<br>
\[
TF-IDF = TF * IDF
\]</p>
<h2 id="tf-idf-de-ying-yong">TF-IDF 的应用</h2>
<ol>
<li>搜索引擎</li>
<li>关键词提取</li>
<li>文本相似性</li>
<li>文本摘要</li>
</ol>
<h2 id="dai-ma">代码</h2>
<figure class="highlight python"><figcaption><span>tf-idf</span><a href="https://github.com/jeffery0628/keyword_extraction" target="_blank" rel="noopener">github</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf_idf</span><span class="params">(document, corpus)</span>:</span>  <span class="comment"># 计算TF-IDF,并返回字典</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param document: 计算document里面每个词的tfidf值，document为文本分词后的形式，</span></span><br><span class="line"><span class="string">    如:[6 月 19 日 2012 年度 中国 爱心 城市 公益活动 新闻 发布会 在京举行]</span></span><br><span class="line"><span class="string">    如果是对一篇文档进行关键词提取，则需要对文档进行分句，把每句话看成一个document，corpus则存放的是整篇文档分词后的所有句子（句子为分词后的结果）。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param corpus:  corpus为所有问当分词后的列表：[document1,document2,document3,...]</span></span><br><span class="line"><span class="string">    :return:dict类型，按照tfidf值从大到小排序： orderdict[word] = tfidf_value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    word_tfidf = &#123;&#125;</span><br><span class="line">    <span class="comment"># 计算词频</span></span><br><span class="line">    freq_words = Counter(document)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> freq_words:</span><br><span class="line">        <span class="comment"># 计算TF：某个词在文章中出现的次数/文章总词数</span></span><br><span class="line">        tf = freq_words[word] / len(document)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算IDF：log(语料库的文档总数/(包含该词的文档数+1))</span></span><br><span class="line">        idf = math.log(len(corpus) / (wordinfilecount(word, corpus) + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算每个词的TFIDF值</span></span><br><span class="line">        tfidf = tf * idf  <span class="comment"># 计算TF-IDF</span></span><br><span class="line">        word_tfidf[word] = tfidf</span><br><span class="line"></span><br><span class="line">    orderdic = sorted(word_tfidf.items(), key=<span class="keyword">lambda</span> item: item[<span class="number">1</span>], reverse=<span class="literal">True</span>)  <span class="comment"># 给字典排序</span></span><br><span class="line">    <span class="keyword">return</span> orderdic</span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    stop_words_path = <span class="string">r'stop_words.txt'</span>  <span class="comment"># 停用词表路径</span></span><br><span class="line">    stop_words = get_stopwords(stop_words_path)  <span class="comment"># 获取停用词表列表</span></span><br><span class="line"></span><br><span class="line">    documents_dir = <span class="string">'data'</span></span><br><span class="line">    filelist = get_documents(documents_dir)  <span class="comment"># 获取文件列表</span></span><br><span class="line"></span><br><span class="line">    corpus = get_corpus(filelist, stop_words)  <span class="comment"># 建立语料库</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx,document <span class="keyword">in</span> enumerate(corpus):</span><br><span class="line">        word_tfidf = tf_idf(document, corpus)  <span class="comment"># 计算TF-IDF</span></span><br><span class="line">        <span class="comment"># 输出前十关键词</span></span><br><span class="line">        print(<span class="string">'document &#123;&#125;,top 10 key words&#123;&#125;'</span>.format(idx+<span class="number">1</span>,word_tfidf[:<span class="number">10</span>]))</span><br></pre></td></tr></table></figure>
<p>使用scikit-learn 计算tfidf</p>
<figure class="highlight python"><figcaption><span>tf-idf</span><a href="https://github.com/jeffery0628/keyword_extraction" target="_blank" rel="noopener">github</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer, TfidfVectorizer</span><br><span class="line"><span class="comment"># 对语料进行稍微处理</span></span><br><span class="line">corpus = [*map(<span class="keyword">lambda</span> x:<span class="string">" "</span>.join(x), corpus)]</span><br><span class="line">tfidf_model = TfidfVectorizer()</span><br><span class="line">tfidf_matrix = tfidf_model.fit_transform(corpus)  <span class="comment"># 计算每个词的tfidf值</span></span><br><span class="line">words = tfidf_model.get_feature_names()<span class="comment"># 所有词的集合</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(corpus)):</span><br><span class="line">  	word_tfidf = &#123;&#125;</span><br><span class="line">  	<span class="keyword">for</span> j <span class="keyword">in</span> range(len(words)):</span><br><span class="line">        word_tfidf[words[j]] = tfidf_matrix[i, j]</span><br><span class="line">        word_tfidf = sorted(word_tfidf.items(),key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>],reverse=<span class="literal">True</span>)</span><br><span class="line">        print(<span class="string">'document &#123;&#125;,top 10 key words&#123;&#125;'</span>.format(i+<span class="number">1</span>, word_tfidf[:<span class="number">10</span>]))</span><br></pre></td></tr></table></figure>
<h1 id="text-rank-ti-qu-guan-jian-ci">TextRank 提取关键词</h1>
<h2 id="jian-jie-1">简介</h2>
<p>TF-IDF 算法对有多段文本的关键词提取非常有效，但是对于单篇或者文档分割较少的文本表现得不是特别好。如果需要提取关键词的语句只有一句话，那么基于TF-IDF可以知道，所有关键词的重要度都为0(因为IDF值为0)，这种情况下使用TextRank是比较好的选择。</p>
<p>TextRank是一种基于图排序的算法，基本思想是(来源于PageRank)：通过把文本分割成若干组成单元(单词、句子)并建立图模型，利用投票机制对文本中的重要成分进行排序，<strong>仅利用单篇文档本身的信息就可以实现关键词提取、文本摘要</strong>。和 LDA、HMM 等模型不同, TextRank不需要事先对多篇文档进行学习训练, 因其简洁有效而得到广泛应用。</p>
<h2 id="page-rank">PageRank</h2>
<h3 id="page-rank-de-jian-hua-mo-xing">PageRank 的简化模型</h3>
<p>假设一共有 4 个网页 A、B、C、D。它们之间的链接信息如图所示：</p>
<p><img src="/2020/04/21/keyword_extraction/pagerank.png" alt="avatar"></p>
<p>出链指的是链接出去的链接。入链指的是链接进来的链接。比如图中 A 有 2 个入链，3 个出链。简单来说，一个网页的影响力 = 所有入链集合的页面的加权影响力之和，用公式表示为：</p>
<p>\[
P R(u)=\sum_{v \in B_{u}} \frac{P R(v)}{L(v)}
\]<br>
u 为待评估的页面， \(B_{u}\) 为页面 \(u\) 的入链集合。针对入链集合中的任意页面 \(v\)，它能给 \(u\) 带来的影响力是其自身的影响力 \(PR(v)\) 除以 \(v\) 页面的出链数量，即页面 \(v\) 把影响力 \(PR(v)\) 平均分配给了它的出链，这样统计所有能给 \(u\) 带来链接的页面 \(v\)，得到的总和就是网页 \(u\) 的影响力，即为 \(PR(u)\)。所以出链会给被链接的页面赋予影响力，当我们统计了一个网页链出去的数量，也就是统计了这个网页的跳转概率。</p>
<p>在例子中，A 有三个出链分别链接到了 B、C、D 上。那么当用户访问 A 的时候，就有跳转到 B、C 或者 D 的可能性，跳转概率均为 1/3。B 有两个出链，链接到了 A 和 D 上，跳转概率为 1/2。这样，我们可以得到 A、B、C、D 这四个网页的转移矩阵 M：</p>
<p>\[
M=\left[\begin{array}{cccc}
0 &amp; 1 / 2 &amp; 1 &amp; 0 \\
1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\
1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\
1 / 3 &amp; 1 / 2 &amp; 0 &amp; 0
\end{array}\right]
\]<br>
假设 A、B、C、D 四个页面的初始影响力都是相同的，即：<br>
\[
w_{0}=\left[\begin{array}{l}
1 / 4 \\
1 / 4 \\
1 / 4 \\
1 / 4
\end{array}\right]
\]<br>
当进行第一次转移之后，各页面的影响力 \(w_{1}\) 变为：<br>
\[
w_1=Mw_0=\left[\begin{array}{cccc}
0 &amp; 1 / 2 &amp; 1 &amp; 0 \\
1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\
1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\
1 / 3 &amp; 1 / 2 &amp; 0 &amp; 0
\end{array}\right]\left[\begin{array}{c}
1 / 4 \\
1 / 4 \\
1 / 4 \\
1 / 4
\end{array}\right]=\left[\begin{array}{c}
9 / 24 \\
5 / 24 \\
5 / 24 \\
5 / 24
\end{array}\right]
\]<br>
然后再用转移矩阵乘以 \(w_{1}\) 得到 \(w_{2}\) 结果，直到第 \(n\) 次迭代后 \(w_{n}\) 影响力不再发生变化，可以收敛到 \((0.3333,0.2222,0.2222,0.2222\)，也就是对应着 A、B、C、D 四个页面最终平衡状态下的影响力。能看出 A 页面相比于其他页面来说权重更大，也就是 PR 值更高。而 B、C、D 页面的 PR 值相等。</p>
<h3 id="deng-ji-xie-lu-rank-leak">等级泄露（Rank Leak）</h3>
<p>如果一个网页没有出链，就像是一个黑洞一样，吸收了其他网页的影响力而不释放，最终会导致其他网页的 PR 值为 0。</p>
<p><img src="/2020/04/21/keyword_extraction/rank_leak.png" alt="avatar"></p>
<h3 id="deng-ji-chen-mei-rank-sink">等级沉没（Rank Sink）</h3>
<p>如果一个网页只有出链，没有入链，计算的过程迭代下来，会导致<strong>这个网页</strong>的 PR 值为 0（也就是不存在公式中的 V）。</p>
<p><img src="/2020/04/21/keyword_extraction/rank_sink.png" alt="avatar"></p>
<h3 id="jie-jue-fang-an-sui-ji-liu-lan">解决方案：随机浏览</h3>
<p>为了解决简化模型中存在的等级泄露和等级沉没的问题，拉里·佩奇提出了 PageRank 的随机浏览模型。他假设了这样一个场景：用户并不都是按照跳转链接的方式来上网，还有一种可能是不论当前处于哪个页面，都有概率访问到其他任意的页面，比如说用户就是要直接输入网址访问其他页面，虽然这个概率比较小。</p>
<p>所以他定义了阻尼因子 d，这个因子代表了用户按照跳转链接来上网的概率，通常可以取一个固定值 0.85，而 1-d=0.15 则代表了用户不是通过跳转链接的方式来访问网页的，比如直接输入网址。<br>
\[
P R(u)=\frac{1-d}{N}+d \sum_{v \in B_{u}} \frac{P R(v)}{L(v)}
\]</p>
<p>其中 \(N\) 为网页总数，这样我们又可以重新迭代网页的权重计算了，加入了阻尼因子 \(d\)，一定程度上解决了等级泄露和等级沉没的问题。通过数学定理也可以证明，最终 PageRank 随机浏览模型是可以收敛的，也就是可以得到一个稳定正常的 PR 值。</p>
<h2 id="text-rank">TextRank</h2>
<p>TextRank通过词之间的相邻关系构建网络，然后用PageRank迭代计算每个节点的rank值，排序rank值即可得到关键词。PageRank迭代计算公式如下：</p>
<p>\[
P R\left(V_{i}\right)=(1-d)+d * \sum_{j \in \operatorname{In}\left(V_{i}\right)} \frac{1}{\left|O u t\left(V_{j}\right)\right|} P R\left(V_{j}\right)
\]<br>
其中，d:表示阻尼系数，一般为0.85。\(V_i\)：表示图中任一节点。\(In(V_i)\):表示指向顶点\(V_i\)的所有顶点的集合。\(|Out(V_j)|\)：表示由顶点\(V_j\)连接出去的所有顶点集合个数。\(PR(V_i)\)：表示顶点\(V_i\)的最终排序权重。(与pagerank公式基本一致。)</p>
<p>网页之间的链接关系可以用图表示，那么怎么把一个句子（可以看作词的序列）构建成图呢？TextRank将某一个词与其前面的N个词、以及后面的N个词均具有图相邻关系（类似于N-gram语法模型）。具体实现：设置一个长度为N的滑动窗口，所有在这个窗口之内的词都视作词结点的相邻结点；则TextRank构建的词图为无向图。下图给出了由一个文档构建的词图（去掉了停用词并按词性做了筛选）：</p>
<p><img src="/2020/04/21/keyword_extraction/textrank.png" alt="avatar"></p>
<p>考虑到不同词对可能有不同的共现（co-occurrence），TextRank将共现作为无向图边的权值。那么，TextRank的迭代计算公式如下：<br>
\[
W S\left(V_{i}\right)=(1-d)+d * \sum_{j \in \operatorname{In}\left(V_{i}\right)} \frac{w_{ij}}{\left|O u t\left(V_{j}\right)\right|} W S\left(V_{j}\right)
\]<br>
该公式仅仅比PageRank多了一个权重项\(w_{ij}\)，用来表示两个节点之间的边连接有不同的重要程度。</p>
<h3 id="text-rank-guan-jian-ci-duan-yu-ti-qu-suan-fa">TextRank 关键词(短语)提取算法</h3>
<ol>
<li>把给定的文本\(T\)按照完整句子进行分割，即\(T = [S_1,S_2,\ldots,S_n]\).</li>
<li>文本\(T\)中每个句子\(S_i\)，进行分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词，如名词、动词、形容词，即\(S_i = [t_{i,1},t_{i,2},\ldots,t_{i,\pi}]\),其中\(t_{ij}\)是保留后的候选关键词。</li>
<li>构建候选关键词图\(G = (V,E)\)，其中\(V\)为节点集，由步骤2生成的候选关键词组成，然后采用共现关系（co-occurrence）构造任两点之间的边，两个节点之间存在边仅当它们对应的词汇在长度为K的窗口中共现，K表示窗口大小，即最多共现K个单词。</li>
<li>根据上面公式，迭代传播各节点的权重，直至收敛。</li>
<li>对节点权重进行倒序排序，从而得到最重要的T个单词，作为候选关键词。</li>
<li>由步骤5得到最重要的T个单词，在原始文本中进行标记，若形成相邻词组，则组合成多词关键词。</li>
</ol>
<h3 id="dai-ma-1">代码</h3>
<figure class="highlight python"><figcaption><span>textrank</span><a href="https://github.com/jeffery0628/keyword_extraction" target="_blank" rel="noopener">github</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">textrank_graph</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.graph = defaultdict(list) <span class="comment"># key:[(),(),(),...] 如：是 [('是', '全国', 1), ('是', '调查', 1), ('是', '失业率', 1), ('是', '城镇', 1)]</span></span><br><span class="line">        self.d = <span class="number">0.85</span>  <span class="comment"># d是阻尼系数，一般设置为0.85</span></span><br><span class="line">        self.min_diff = <span class="number">1e-5</span>  <span class="comment"># 设定收敛阈值</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加节点之间的边</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addEdge</span><span class="params">(self, start, end, weight)</span>:</span></span><br><span class="line">        self.graph[start].append((start, end, weight))</span><br><span class="line">        self.graph[end].append((end, start, weight))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 节点排序</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rank</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 一共有14个节点</span></span><br><span class="line">        print(len(self.graph))</span><br><span class="line">        <span class="comment"># 默认初始化权重</span></span><br><span class="line">        weight_deault = <span class="number">1.0</span> / (len(self.graph) <span class="keyword">or</span> <span class="number">1.0</span>)</span><br><span class="line">        <span class="comment"># nodeweight_dict, 存储节点的权重</span></span><br><span class="line">        nodeweight_dict = defaultdict(float)</span><br><span class="line">        <span class="comment"># outsum，存储节点的出度权重</span></span><br><span class="line">        outsum_node_dict = defaultdict(float)</span><br><span class="line">        <span class="comment"># 根据图中的边，更新节点权重</span></span><br><span class="line">        <span class="keyword">for</span> node, out_edge <span class="keyword">in</span> self.graph.items():</span><br><span class="line">            <span class="comment"># 是 [('是', '全国', 1), ('是', '调查', 1), ('是', '失业率', 1), ('是', '城镇', 1)]</span></span><br><span class="line">            nodeweight_dict[node] = weight_deault <span class="comment"># 初始化节点权重</span></span><br><span class="line">            outsum_node_dict[node] = sum((edge[<span class="number">2</span>] <span class="keyword">for</span> edge <span class="keyword">in</span> out_edge), <span class="number">0.0</span>) <span class="comment"># 统计node节点的出度</span></span><br><span class="line">        <span class="comment"># 初始状态下的textrank重要性权重</span></span><br><span class="line">        sorted_keys = sorted(self.graph.keys())</span><br><span class="line">        <span class="comment"># 设定迭代次数，</span></span><br><span class="line">        step_dict = [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">1000</span>):</span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> sorted_keys:</span><br><span class="line">                s = <span class="number">0</span></span><br><span class="line">                <span class="comment"># 计算公式：(edge_weight/outsum_node_dict[edge_node])*node_weight[edge_node]</span></span><br><span class="line">                <span class="keyword">for</span> e <span class="keyword">in</span> self.graph[node]:</span><br><span class="line">                    s += e[<span class="number">2</span>] / outsum_node_dict[e[<span class="number">1</span>]] * nodeweight_dict[e[<span class="number">1</span>]]</span><br><span class="line">                <span class="comment"># 计算公式：(1-d) + d*s</span></span><br><span class="line">                nodeweight_dict[node] = (<span class="number">1</span> - self.d) + self.d * s</span><br><span class="line">            step_dict.append(sum(nodeweight_dict.values()))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> abs(step_dict[step] - step_dict[step - <span class="number">1</span>]) &lt;= self.min_diff:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 利用Z-score进行权重归一化，也称为离差标准化，是对原始数据的线性变换，使结果值映射到[0 - 1]之间。</span></span><br><span class="line">        <span class="comment"># 先设定最大值与最小值均为系统存储的最大值和最小值</span></span><br><span class="line">        (min_rank, max_rank) = (sys.float_info[<span class="number">0</span>], sys.float_info[<span class="number">3</span>])</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> nodeweight_dict.values():</span><br><span class="line">            <span class="keyword">if</span> w &lt; min_rank:</span><br><span class="line">                min_rank = w</span><br><span class="line">            <span class="keyword">if</span> w &gt; max_rank:</span><br><span class="line">                max_rank = w</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> n, w <span class="keyword">in</span> nodeweight_dict.items(): <span class="comment"># 归一化</span></span><br><span class="line">            nodeweight_dict[n] = (w - min_rank / <span class="number">10.0</span>) / (max_rank - min_rank / <span class="number">10.0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nodeweight_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextRank</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.candi_pos = [<span class="string">'n'</span>, <span class="string">'v'</span>, <span class="string">'a'</span>] <span class="comment"># 关键词的词性：名词，动词，形容词</span></span><br><span class="line">        self.span = <span class="number">5</span> <span class="comment"># 窗口大小</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract_keywords</span><span class="params">(self, text, num_keywords)</span>:</span></span><br><span class="line">        g = textrank_graph()</span><br><span class="line">        cm = defaultdict(int)</span><br><span class="line">        word_list = [[word.word, word.flag] <span class="keyword">for</span> word <span class="keyword">in</span> pseg.cut(text)] <span class="comment"># 使用jieba分词并且对词性进行标注</span></span><br><span class="line">        <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(word_list): <span class="comment"># 该循环用于统计在窗口范围内，词的共现次数</span></span><br><span class="line">            <span class="keyword">if</span> word[<span class="number">1</span>][<span class="number">0</span>] <span class="keyword">in</span> self.candi_pos <span class="keyword">and</span> len(word[<span class="number">0</span>]) &gt; <span class="number">1</span>: <span class="comment">#</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, i + self.span):</span><br><span class="line">                    <span class="keyword">if</span> j &gt;= len(word_list):<span class="comment"># 防止下标越界</span></span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    <span class="keyword">if</span> word_list[j][<span class="number">1</span>][<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> self.candi_pos <span class="keyword">or</span> len(word_list[j][<span class="number">0</span>]) &lt; <span class="number">2</span>: <span class="comment"># 排除词性不在关键词词性列表中的词或者词长度小于2的词</span></span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    pair = tuple((word[<span class="number">0</span>], word_list[j][<span class="number">0</span>]))</span><br><span class="line">                    cm[(pair)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> terms, w <span class="keyword">in</span> cm.items():</span><br><span class="line">            g.addEdge(terms[<span class="number">0</span>], terms[<span class="number">1</span>], w)</span><br><span class="line">        nodes_rank = g.rank()</span><br><span class="line">        nodes_rank = sorted(nodes_rank.items(), key=<span class="keyword">lambda</span> asd:asd[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nodes_rank[:num_keywords]</span><br></pre></td></tr></table></figure>
<h1 id="lda-latent-dirichlet-allocation-ti-qu-guan-jian-ci">LDA (Latent Dirichlet Allocation) 提取关键词</h1>
<h2 id="jian-jie-2">简介</h2>
<p>TF-IDF 和 TextRank 两种算法更多反映的是文本的统计信息，对于文本之间的语义关系考虑得比较少。LDA是一种能够体文本语义关系的关键词提取方法。</p>
<blockquote>
<p>二项分布（Binomial Distribution），即重复n次的伯努利试验（Bernoulli Experiment），用\(\xi\)表示随机试验的结果。如果事件发生的概率是P,则不发生的概率\(q=1-p\)，\(N\)次独立重复试验中发生K次的概率是\(P(\xi=K)= C(n,k) * p^k * (1-p)^{n-k}\)，其中\(C(n, k) =\frac{n!}{(k!(n-k)!)}\). 期望：\(E(ξ)=np\),方差：\(D(ξ)=npq\)其中\(q=1-p\)</p>
<p>多项分布（Multinomial Distribution）：多项式分布是二项式分布的推广。二项分布的典型例子是扔硬币，硬币正面朝上概率为p, 重复扔n次硬币，k次为正面的概率即为一个二项分布概率。把二项分布公式推广至多种状态，就得到了多项分布。<br>
某随机实验如果有k个可能结局\(A_1,A_2,\ldots,A_k\)，分别将他们的出现次数记为随机变量\(X_1,X_2,\ldots,X_k\)，它们的概率分布分别是\(p_1,p_2,\ldots,p_k\)那么在\(n\)次采样的总结果中，\(A_1\)出现\(n_1\)次、\(A_2\)出现\(n_2\)次,\(\dots\),\(A_k\)出现\(n_k\)次的这种事件的出现概率\(P\)有下面公式：<br>
\[
P\left(X_{1}=n_{1}, \cdots, X_{k}=n_{k}\right)=\left\{\begin{array}{ll}
\frac{n !}{n_{1} ! \cdots n_{k} !} p_{1}^{n_{1}} \cdots p_{k}^{n_{k}} &amp; , \sum_{i=1}^{k} n_{i}=n \\
0 &amp; , \text { otherwise }
\end{array}\right.
\]<br>
Beta分布与Dirichlet分布的定义域均为[0,1]，在实际使用中，通常将两者作为概率的分布，Beta分布描述的是单变量分布，Dirichlet分布描述的是多变量分布，因此，Beta分布可作为二项分布的先验概率，Dirichlet分布可作为多项分布的先验概率。</p>
</blockquote>
<p>在主题模型中，主题表示一个概念，表现为一系列相关的单词，是这些单词的条件概率。形象来说，主题就是一个桶，里面装了出现概率较高的单词，这些单词与这个主题有很强的相关性。</p>
<h2 id="li-lun">理论</h2>
<p>怎样才能生成主题？对文章的主题应该怎么分析？这是主题模型要解决的问题。</p>
<p>首先，可以用生成模型来看文档和主题这两件事。所谓生成模型，就是说，我们认为**一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”**这样一个过程得到的。那么，如果我们要生成一篇文档，它里面的每个词语出现的概率为：<br>
\[
p(W|D) = \sum_T P(W|T)P(T|D)
\]<br>
其中，\(W\)表示词，\(T\)表示主题，\(D\)表示文档。</p>
<p><img src="/2020/04/21/keyword_extraction/lda.png" alt="avatar"></p>
<p>其中”文档-词语”矩阵表示每个单词在每个文档中的词频，即出现的概率；”主题-词语”矩阵表示每个主题中每个单词的出现概率；”文档-主题”矩阵表示每个主题在每个文档中出现的概率。</p>
<p>单词对于主题的概率和主题对于文档的概率，可以通过Gibbs采样法（？？？）来进行概率的计算。</p>
<p>主题\(T_k\)下各个词\(W_i\)的权重计算公式：<br>
\[
P(W_i|T_k) = \frac{C_{ik}+\beta}{\sum_{i=1}^N{C_{ik}+N*\beta}} = \phi_i^{t=k} 
\]<br>
其中，\(w_i\)：表示单词集合中的任一单词。\(T_k\):表示主题集合中任一主题。\(P(w_i|T_k)\):表示在主题为\(k\)时，单词\(i\)出现的概率，其简记为\(\phi_i^{t=k}\)，\(C_{ik}\):表示语料库中单词\(i\)被赋予主题\(k\)的次数。\(N\):表示词汇表的大小。\(\beta\)：表示超参数。</p>
<p>文档\(D_m\)下各个词\(T_k\)的权重计算公式：<br>
\[
P(T_k|D_m) = \frac{C_{km}+\alpha}{\sum^K_{k=1}C_{km}+K*\alpha}=\theta^m_{t=k}
\]<br>
其中，\(D_m\):表示文档集合中任一文档。\(T_k\):表示主题集合中任一主题。\(P(T_k|D_m)\):表示语料库中文档m中单词被赋予主题\(k\)的次数。\(K\)：表示主题的数量。\(\alpha\)表示超参数。</p>
<p>得到了指定文档下某主题出现的概率，以及指定主题下、某单词出现的概率。那么由联合概率分布可以知道，对于指定文档某单词出现的概率：<br>
\[
P(W_i|D_m) = \sum_{k=1}^K{\phi_i^{t=k}*\theta_{t=k}^m}
\]<br>
基于上述公式，可以计算出单词\(i\)对于文档\(m\)的主题重要性。</p>
<p>但是由于在LDA主题概率模型中，所有的词汇都会以一定的概率出现在每个主题，所以这样会导致最终计算的单词对于文档的主题重要性区分度受影响。为了避免这种情况，一般会将单词相对于主题概率小于一定阈值的概率置为0.</p>
<h2 id="dai-ma-2">代码</h2>
<figure class="highlight python"><figcaption><span>LDA</span><a href="https://github.com/jeffery0628/keyword_extraction" target="_blank" rel="noopener">github</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 主题模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TopicModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 三个传入参数：处理后的数据集，关键词数量，具体模型（LSI、LDA），主题数量</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, doc_list, keyword_num, model=<span class="string">'LDA'</span>, num_topics=<span class="number">4</span>)</span>:</span></span><br><span class="line">        <span class="comment"># 使用gensim的接口，将文本转为向量化表示</span></span><br><span class="line">        <span class="comment"># 先构建词空间</span></span><br><span class="line">        self.dictionary = corpora.Dictionary(doc_list)</span><br><span class="line">        <span class="comment"># 使用BOW模型向量化 (token_id,freq)</span></span><br><span class="line">        corpus = [self.dictionary.doc2bow(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> doc_list]  <span class="comment"># (token_id,freq)</span></span><br><span class="line">        <span class="comment"># 对每个词，根据tf-idf进行加权，得到加权后的向量表示</span></span><br><span class="line">        self.tfidf_model = models.TfidfModel(corpus)</span><br><span class="line">        self.tfidf_corpus = self.tfidf_model[corpus]</span><br><span class="line"></span><br><span class="line">        self.keyword_num = keyword_num</span><br><span class="line">        self.num_topics = num_topics</span><br><span class="line">        <span class="comment"># 选择加载胡模型</span></span><br><span class="line">        <span class="keyword">if</span> model == <span class="string">'LSI'</span>:</span><br><span class="line">            self.model = self.train_lsi()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.model = self.train_lda()</span><br><span class="line">        <span class="comment"># 得到数据集的主题-词分布</span></span><br><span class="line">        word_dic = self.word_dictionary(doc_list)</span><br><span class="line">        self.wordtopic_dic = self.get_wordtopic(word_dic)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 向量化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">doc2bowvec</span><span class="params">(self, word_list)</span>:</span></span><br><span class="line">        vec_list = [<span class="number">1</span> <span class="keyword">if</span> word <span class="keyword">in</span> word_list <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> word <span class="keyword">in</span> self.dictionary]</span><br><span class="line">        print(<span class="string">"vec_list"</span>, vec_list)</span><br><span class="line">        <span class="keyword">return</span> vec_list</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 词空间构建方法和向量化方法，在没有gensim接口时的一般处理方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word_dictionary</span><span class="params">(self, doc_list)</span>:</span></span><br><span class="line">        dictionary = []</span><br><span class="line">        <span class="comment"># 2及变1及结构</span></span><br><span class="line">        <span class="keyword">for</span> doc <span class="keyword">in</span> doc_list:</span><br><span class="line">            <span class="comment"># extend he append 方法有何异同 容易出错</span></span><br><span class="line">            dictionary.extend(doc)</span><br><span class="line"></span><br><span class="line">        dictionary = list(set(dictionary))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dictionary</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到数据集的主题 - 词分布</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_wordtopic</span><span class="params">(self, word_dic)</span>:</span></span><br><span class="line">        wordtopic_dic = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> word_dic:</span><br><span class="line">            singlist = [word]</span><br><span class="line">            <span class="comment"># 计算每个词胡加权向量</span></span><br><span class="line">            word_corpus = self.tfidf_model[self.dictionary.doc2bow(singlist)]</span><br><span class="line">            <span class="comment"># 计算每个词de主题向量</span></span><br><span class="line">            word_topic = self.model[word_corpus]</span><br><span class="line">            wordtopic_dic[word] = word_topic</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> wordtopic_dic</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_lsi</span><span class="params">(self)</span>:</span></span><br><span class="line">        lsi = models.LsiModel(self.tfidf_corpus, id2word=self.dictionary, num_topics=self.num_topics)</span><br><span class="line">        <span class="keyword">return</span> lsi</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_lda</span><span class="params">(self)</span>:</span></span><br><span class="line">        lda = models.LdaModel(self.tfidf_corpus, id2word=self.dictionary, num_topics=self.num_topics)</span><br><span class="line">        <span class="keyword">return</span> lda</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算词的分布和文档的分布的相似度，取相似度最高的keyword_num个词作为关键词</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_simword</span><span class="params">(self, word_list)</span>:</span></span><br><span class="line">        <span class="comment"># 文档的加权向量</span></span><br><span class="line">        sentcorpus = self.tfidf_model[self.dictionary.doc2bow(word_list)]</span><br><span class="line">        <span class="comment"># 文档主题 向量</span></span><br><span class="line">        senttopic = self.model[sentcorpus]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># senttopic [(0, 0.03457821), (1, 0.034260772), (2, 0.8970413), (3, 0.034119748)]</span></span><br><span class="line">        <span class="comment"># 余弦相似度计算</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">calsim</span><span class="params">(l1, l2)</span>:</span></span><br><span class="line">            a, b, c = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">            <span class="keyword">for</span> t1, t2 <span class="keyword">in</span> zip(l1, l2):</span><br><span class="line">                x1 = t1[<span class="number">1</span>]</span><br><span class="line">                x2 = t2[<span class="number">1</span>]</span><br><span class="line">                a += x1 * x1</span><br><span class="line">                b += x1 * x1</span><br><span class="line">                c += x2 * x2</span><br><span class="line">            sim = a / math.sqrt(b * c) <span class="keyword">if</span> <span class="keyword">not</span> (b * c) == <span class="number">0.0</span> <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line">            <span class="keyword">return</span> sim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算输入文本和每个词的主题分布相似度</span></span><br><span class="line">        sim_dic = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> self.wordtopic_dic.items():</span><br><span class="line">            <span class="comment"># 还是计算每个再本文档中的词  和文档的相识度</span></span><br><span class="line">            <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> word_list:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment">#</span></span><br><span class="line">            sim = calsim(v, senttopic)</span><br><span class="line">            sim_dic[k] = sim</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> sorted(sim_dic.items(), key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:self.keyword_num]:</span><br><span class="line">            print(k, v)</span><br><span class="line">        print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topic_extract</span><span class="params">(word_list, model, pos=False, keyword_num=<span class="number">10</span>)</span>:</span></span><br><span class="line">    doc_list = load_data(pos)</span><br><span class="line">    topic_model = TopicModel(doc_list, keyword_num, model=model)</span><br><span class="line">    topic_model.get_simword(word_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textrank_extract</span><span class="params">(text, pos=False, keyword_num=<span class="number">10</span>)</span>:</span></span><br><span class="line">    textrank = analyse.textrank</span><br><span class="line">    keywords = textrank(text, keyword_num)</span><br><span class="line">    <span class="comment"># 输出抽取出的关键词</span></span><br><span class="line">    <span class="keyword">for</span> keyword <span class="keyword">in</span> keywords:</span><br><span class="line">        print(keyword + <span class="string">"/ "</span>, end=<span class="string">''</span>)</span><br><span class="line">    print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># test</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    text = <span class="string">'6月19日,《2012年度“中国爱心城市”公益活动新闻发布会》在京举行。'</span> + \</span><br><span class="line">           <span class="string">'中华社会救助基金会理事长许嘉璐到会讲话。基金会高级顾问朱发忠,全国老龄'</span> + \</span><br><span class="line">           <span class="string">'办副主任朱勇,民政部社会救助司助理巡视员周萍,中华社会救助基金会副理事长耿志远,'</span> + \</span><br><span class="line">           <span class="string">'重庆市民政局巡视员谭明政。晋江市人大常委会主任陈健倩,以及10余个省、市、自治区民政局'</span> + \</span><br><span class="line">           <span class="string">'领导及四十多家媒体参加了发布会。中华社会救助基金会秘书长时正新介绍本年度“中国爱心城'</span> + \</span><br><span class="line">           <span class="string">'市”公益活动将以“爱心城市宣传、孤老关爱救助项目及第二届中国爱心城市大会”为主要内容,重庆市'</span> + \</span><br><span class="line">           <span class="string">'、呼和浩特市、长沙市、太原市、蚌埠市、南昌市、汕头市、沧州市、晋江市及遵化市将会积极参加'</span> + \</span><br><span class="line">           <span class="string">'这一公益活动。中国雅虎副总编张银生和凤凰网城市频道总监赵耀分别以各自媒体优势介绍了活动'</span> + \</span><br><span class="line">           <span class="string">'的宣传方案。会上,中华社会救助基金会与“第二届中国爱心城市大会”承办方晋江市签约,许嘉璐理'</span> + \</span><br><span class="line">           <span class="string">'事长接受晋江市参与“百万孤老关爱行动”向国家重点扶贫地区捐赠的价值400万元的款物。晋江市人大'</span> + \</span><br><span class="line">           <span class="string">'常委会主任陈健倩介绍了大会的筹备情况。'</span></span><br><span class="line"></span><br><span class="line">    pos = <span class="literal">False</span></span><br><span class="line">    seg_list = seg_to_list(text, pos)</span><br><span class="line">    filter_list = word_filter(seg_list, pos)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'LDA模型结果：'</span>)</span><br><span class="line">    topic_extract(filter_list, <span class="string">'LDA'</span>, pos)</span><br></pre></td></tr></table></figure>
<h3 id="lda-bu-zou-zong-jie">LDA 步骤总结</h3>
<p>数据集处理</p>
<ol>
<li>先构建词空间  Dictionary(4064 unique tokens: [‘上将’, ‘专门’, ‘乘客’, ‘仪式’, ‘体验’]…)</li>
<li>使用BOW模型向量化   corpus [[(0, 1), (1, 1), (2, 2), (3, 1),。。</li>
<li>对每个词，根据tf-idf进行加权，得到加权后的向量表示</li>
</ol>
<p>根据数据集获得模型</p>
<ol start="4">
<li>得到数据集的主题-词分布  model (得到每个词的向量）（文档转列表 再转集合去重，再转列表）{‘白血病’: [(0, 0.1273009), (1, 0.6181468), (2, 0.12732704), (3, 0.12722531)], ‘婴儿’: [。。。</li>
<li>求文档的分布:词》向量》tf/idf加权》同第4步得到文档的分布向量 [(0, 0.033984687), (1, 0.033736005), (2, 0.8978361), (3, 0.03444325)]</li>
<li>计算余弦距离得到结果</li>
</ol>
<h2 id="zong-jie">总结</h2>
<ol>
<li>
<p>为什么要用浅层语义分析？</p>
<p>我觉着一方面是考虑文本的语义信息。还有就是用词袋模型所表示的单词-文本矩阵一方面存在数据稀疏的问题，另一方面就是词本身一词多义和多词一义现象在进行文本相似度计算的时候未必能够准确的表达两个文本的语义相似度。</p>
</li>
<li>
<p>潜在语义分析算法</p>
<ol>
<li>矩阵奇异值分解算法</li>
<li>非负矩阵分解算法</li>
</ol>
</li>
</ol>
<h1 id="can-kao">参考</h1>
<ol>
<li><a href="https://www.cnblogs.com/jpcflyer/p/11180263.html" target="_blank" rel="noopener">机器学习经典算法之PageRank</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/41091116" target="_blank" rel="noopener">通俗易懂理解——TF-IDF与TextRank</a></li>
</ol>

    </div>

    
    
    <div>
  
    <div style="text-align:center;color:#bfbfbf;font-size:16px;">
      <span>-------- 本文结束 </span>
      <i class="fa fa-coffee"></i>
      <span> 感谢阅读 --------</span>
    </div>
  
</div>
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    鼓励一下
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat.png" alt="Li Zhen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/ali.png" alt="Li Zhen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/nlp/" rel="tag"><i class="fa fa-tag"></i> nlp</a>
              <a href="/tags/key-word-extraction/" rel="tag"><i class="fa fa-tag"></i> key word extraction</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/04/13/research/bert%E7%B3%BB%E5%88%97/" rel="prev" title="bert系列">
      <i class="fa fa-chevron-left"></i> bert系列
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/04/24/text_correction/" rel="next" title="文本纠错">
      文本纠错 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#tf-idf-ti-qu-guan-jian-ci"><span class="nav-number">1.</span> <span class="nav-text">TF-IDF 提取关键词</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#jian-jie"><span class="nav-number">1.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-idf"><span class="nav-number">1.2.</span> <span class="nav-text">TF-IDF</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tf"><span class="nav-number">1.2.1.</span> <span class="nav-text">TF</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#bian-chong-yi-tong-guo-dui-shu-han-shu-bi-mian-tf-xian-xing-zeng-chang"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">变种一:通过对数函数避免 TF 线性增长</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bian-chong-er-biao-zhun-hua-jie-jue-chang-wen-dang-duan-wen-dang-wen-ti"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">变种二：标准化解决长文档、短文档问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#idf"><span class="nav-number">1.2.2.</span> <span class="nav-text">IDF</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#bian-chong-san-dui-shu-han-shu-chu-li-idf"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">变种三：对数函数处理 IDF</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bian-chong-si-cha-xun-ci-ji-wen-dang-xiang-liang-biao-zhun-hua"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">变种四：查询词及文档向量标准化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-idf-1"><span class="nav-number">1.2.3.</span> <span class="nav-text">TF-IDF</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-idf-de-ying-yong"><span class="nav-number">1.3.</span> <span class="nav-text">TF-IDF 的应用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dai-ma"><span class="nav-number">1.4.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#text-rank-ti-qu-guan-jian-ci"><span class="nav-number">2.</span> <span class="nav-text">TextRank 提取关键词</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#jian-jie-1"><span class="nav-number">2.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#page-rank"><span class="nav-number">2.2.</span> <span class="nav-text">PageRank</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#page-rank-de-jian-hua-mo-xing"><span class="nav-number">2.2.1.</span> <span class="nav-text">PageRank 的简化模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#deng-ji-xie-lu-rank-leak"><span class="nav-number">2.2.2.</span> <span class="nav-text">等级泄露（Rank Leak）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#deng-ji-chen-mei-rank-sink"><span class="nav-number">2.2.3.</span> <span class="nav-text">等级沉没（Rank Sink）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#jie-jue-fang-an-sui-ji-liu-lan"><span class="nav-number">2.2.4.</span> <span class="nav-text">解决方案：随机浏览</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#text-rank"><span class="nav-number">2.3.</span> <span class="nav-text">TextRank</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#text-rank-guan-jian-ci-duan-yu-ti-qu-suan-fa"><span class="nav-number">2.3.1.</span> <span class="nav-text">TextRank 关键词(短语)提取算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dai-ma-1"><span class="nav-number">2.3.2.</span> <span class="nav-text">代码</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lda-latent-dirichlet-allocation-ti-qu-guan-jian-ci"><span class="nav-number">3.</span> <span class="nav-text">LDA (Latent Dirichlet Allocation) 提取关键词</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#jian-jie-2"><span class="nav-number">3.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#li-lun"><span class="nav-number">3.2.</span> <span class="nav-text">理论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dai-ma-2"><span class="nav-number">3.3.</span> <span class="nav-text">代码</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#lda-bu-zou-zong-jie"><span class="nav-number">3.3.1.</span> <span class="nav-text">LDA 步骤总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#zong-jie"><span class="nav-number">3.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#can-kao"><span class="nav-number">4.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <a href='/'>
    <img class="site-author-image" itemprop="image" alt="Li Zhen"
      src="/images/snoppy.jpeg">
  </a>
  <p class="site-author-name" itemprop="name">Li Zhen</p>
  <div class="site-description" itemprop="description">他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jeffery0628" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jeffery0628" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jeffery.lee.0628@gmail.com" title="邮箱 → mailto:jeffery.lee.0628@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮箱</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Zhen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">554k</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  






  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>


 

<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180101,"YYYYMMDD"));
    ages = ages.replace(/years?/, "年");
    ages = ages.replace(/months?/, "月");
    ages = ages.replace(/days?/, "天");
    ages = ages.replace(/hours?/, "小时");
    ages = ages.replace(/minutes?/, "分");
    ages = ages.replace(/seconds?/, "秒");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `我已在此等候你 ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


<script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>

<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdnjs.cloudflare.com/ajax/libs/valine/1.3.10/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'tChTbEIggDSVcdhPBUf0MFxW-gzGzoHsz',
      appKey     : 'sJ6xUiFnifEDIIfnj3Ut9zy1',
      placeholder: "留下你的小脚印吧！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '8' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
