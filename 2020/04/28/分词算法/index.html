<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/snoppy.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/snoppy.jpeg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open Sans:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jeffery.ink","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":15,"offset":12,"onmobile":true,"dimmer":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":true,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":true,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="分词算法">
<meta property="og:url" content="https://jeffery.ink/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/index.html">
<meta property="og:site_name" content="火种2号">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jeffery.ink/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/2.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/hmm_seg.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/hmm_viterbi.png">
<meta property="og:image" content="https://jeffery.ink/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/bilstm_crf.jpeg">
<meta property="og:image" content="https://jeffery.ink/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/bilstm_cnn_crf.png">
<meta property="article:published_time" content="2020-04-28T12:53:36.000Z">
<meta property="article:modified_time" content="2020-05-12T02:00:27.015Z">
<meta property="article:author" content="Li Zhen">
<meta property="article:tag" content="自然语言处理基础">
<meta property="article:tag" content="自然语言处理">
<meta property="article:tag" content="分词算法">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jeffery.ink/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/2.png">

<link rel="canonical" href="https://jeffery.ink/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>分词算法 | 火种2号</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">火种2号</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">火种计划</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>简历</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>读书</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-video fa-fw"></i>电影</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>



</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jeffery.ink/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/snoppy.jpeg">
      <meta itemprop="name" content="Li Zhen">
      <meta itemprop="description" content="他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="火种2号">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          分词算法
        </h1>

        <div class="post-meta">
          
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-12 10:00:27" itemprop="dateModified" datetime="2020-05-12T10:00:27+08:00">2020-05-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">自然语言处理基础</span></a>
                </span>
            </span>

          
            <span id="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/" class="post-meta-item leancloud_visitors" data-flag-title="分词算法" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论：</span>
    
    <a title="valine" href="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>44k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>40 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/2.png" alt></p>
<a id="more"></a>
<h3 id="fen-ci-jian-jie">分词简介</h3>
<p>中文分词算法是指将一个汉字序列切分成一个一个单独的词，与英文以空格作为天然的分隔符不同，中文字符在语义识别时，需要把数个字符组合成词，才能表达出真正的含义。分词算法通常应用于自然语言处理、搜索引擎、智能推荐等领域。</p>
<p>分词算法根据其核心思想主要分为两种，第一种是基于词典的分词，先把句子按照字典切分成词，再寻找词的最佳组合方式；第二种是基于字的分词，即由字构词，先把句子分成一个个字，再将字组合成词，寻找最优的切分策略，同时也可以转化成序列标注问题。</p>
<p>其分类大致可分为：</p>
<ol>
<li>
<p>基于匹配规则的方法</p>
<ul>
<li>正向最大匹配法(forward maximum matching method, FMM)</li>
<li>逆向最大匹配法(backward maximum matching method, BMM)</li>
<li>最短路径分词算法</li>
</ul>
</li>
<li>
<p>基于统计以及机器学习的分词方法</p>
<ul>
<li>基于N-gram语言模型的分词方法</li>
<li>基于HMM的分词方法</li>
<li>基于CRF的分词方法</li>
<li>基于词感知机的分词方法</li>
<li>基于深度学习的端到端的分词方法</li>
</ul>
</li>
</ol>
<p>基于规则匹配的分词通常会加入一些启发式规则，比如“正向/反向最大匹配”，“长词优先”等。</p>
<p>基于统计以及机器学习的分词方法，它们基于人工标注的词性和统计特征，对中文进行建模，即根据观测到的数据(标注好的语料)对模型参数进行训练，在分词阶段再通过模型计算各种分词出现的概率，将概率最大的分词结果作为最终结果。这类分词算法能很好处理歧义和未登录词问题，效果比基于规则匹配的方法效果好，但是需要大量的人工标注数据，以及较慢的分词速度。</p>
<p><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码及数据</a></p>
<h4 id="zhong-wen-fen-ci-de-ying-yong">中文分词的应用</h4>
<p>目前在自然语言处理技术中，中文处理技术比西文处理技术要落后很大一段距离，许多西文的处理方法中文不能直接采用，就是因为中文必需有分词这道工序。中文分词是其他中文信息处理的基础，搜索引擎只是中文分词的一个应用。其他的比如机器翻译（MT）、语音合成、自动分类、自动摘要、自动校对等等，都需要用到分词。因为中文需要分词，可能会影响一些研究，但同时也为一些企业带来机会，因为国外的计算机处理技术要想进入中国市场，首先也是要解决中文分词问题。在中文研究方面，相比外国人来说，中国人有十分明显的优势。</p>
<p>分词准确性对搜索引擎来说十分重要，但如果分词速度太慢，即使准确性再高，对于搜索引擎来说也是不可用的，因为搜索引擎需要处理数以亿计的网页，如果分词耗用的时间过长，会严重影响搜索引擎内容更新的速度。因此对于搜索引擎来说，分词的准确性和速度，二者都需要达到很高的要求。目前研究中文分词的大多是科研院校，清华、北大、中科院、北京语言学院、东北大学、IBM研究院、微软中国研究院等都有自己的研究队伍，而真正专业研究中文分词的商业公司除了海量科技以外，几乎没有。</p>
<h3 id="ji-yu-pi-pei-gui-ze-de-fang-fa">基于匹配规则的方法</h3>
<p>主要是人工建立词库也叫做词典，通过词典匹配的方式对句子进行划分。其实现简单高效，但是对未登陆词很难进行处理。主要有前向最大匹配法，后向最大匹配法以及双向最大匹配法。</p>
<h4 id="qian-xiang-zui-da-pi-pei-suan-fa">前向最大匹配算法</h4>
<p>前向最大匹配算法，是从待分词句子的左边向右边搜索，寻找词的最大匹配。规定一个词的最大长度，每次扫描的时候寻找当前开始的这个长度的词来和字典中的词匹配，如果没有找到，就缩短长度继续寻找，直到找到字典中的词或者成为单字。</p>
<h5 id="suan-fa-liu-cheng">算法流程</h5>
<ol>
<li>从前向后扫描字典，测试读入的子串是否在字典中</li>
<li>如果存在，则从输入中删除掉该子串，重新按照规则取子串，重复1</li>
<li>如果不存在于字典中，则从右向左减少子串长度，重复1</li>
</ol>
<h5 id="fen-ci-shi-li">分词实例：</h5>
<p>比如说输入 “北京大学生前来应聘”，假设词典中最长的单词为 5 个（MAX_LENGTH）。</p>
<ol>
<li>第一轮：取子串 “北京大学生”，正向取词，如果匹配失败，<strong>每次去掉匹配字段最后面的一个字</strong><br>
“北京大学生”，扫描 5 字词典，没有匹配，子串长度减 1 变为“北京大学”<br>
“北京大学”，扫描 4 字词典，有匹配，输出“北京大学”，输入变为“生前来应聘”</li>
<li>第二轮：取子串“生前来应聘”<br>
“生前来应聘”，扫描 5 字词典，没有匹配，子串长度减 1 变为“生前来应”<br>
“生前来应”，扫描 4 字词典，没有匹配，子串长度减 1 变为“生前来”<br>
“生前来”，扫描 3 字词典，没有匹配，子串长度减 1 变为“生前”<br>
“生前”，扫描 2 字词典，有匹配，输出“生前”，输入变为“来应聘””</li>
<li>第三轮：取子串“来应聘”<br>
“来应聘”，扫描 3 字词典，没有匹配，子串长度减 1 变为“来应”<br>
“来应”，扫描 2 字词典，没有匹配，子串长度减 1 变为“来”<br>
颗粒度最小为 1，直接输出“来”，输入变为“应聘”</li>
<li>第四轮：取子串“应聘”<br>
“应聘”，扫描 2 字词典，有匹配，输出“应聘”，输入变为“”<br>
输入长度为0，扫描终止</li>
</ol>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>数据准备：词库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_max_match</span><span class="params">(sentence, window_size, word_dict)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    前向最大匹配算法：</span></span><br><span class="line"><span class="string">    （1）扫描字典，测试读入的子串是否在字典中</span></span><br><span class="line"><span class="string">    （2）如果存在，则从输入中删除掉该子串，重新按照规则取子串，重复（1）</span></span><br><span class="line"><span class="string">    （3）如果不存在于字典中，则从右向左减少子串长度，重复（1）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param sentence: 待分词的句子</span></span><br><span class="line"><span class="string">    :param window_size: 词的最大长度</span></span><br><span class="line"><span class="string">    :param word_dict: 词典</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    seg_words = []  <span class="comment"># 存放分词结果</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> sentence:</span><br><span class="line">        <span class="keyword">for</span> word_len <span class="keyword">in</span> range(window_size, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> sentence[:word_len] <span class="keyword">in</span> word_dict:</span><br><span class="line">                <span class="comment"># 如果该词再字典中，将该词保存到分词结果中,并将其从句中切出来</span></span><br><span class="line">                seg_words.append(sentence[:word_len])</span><br><span class="line">                sentence = sentence[word_len:]</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 如果窗口中的词不在字典中，将第一个字切分出来</span></span><br><span class="line">            seg_words.append(sentence[:word_len])</span><br><span class="line">            sentence = sentence[word_len:]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'/'</span>.join(seg_words)</span><br></pre></td></tr></table></figure>
<h4 id="hou-xiang-zui-da-pi-pei-suan-fa">后向最大匹配算法</h4>
<p>在词典中从句尾向句首进行扫描，尽可能地选择与词典中最长单词匹配的词作为目标分词，然后进行下一次匹配。</p>
<p>在实践中，逆向最大匹配算法性能优于正向最大匹配算法。</p>
<h5 id="suan-fa-liu-cheng-1">算法流程</h5>
<ol>
<li>从后向前扫描字典，测试读入的子串是否在字典中</li>
<li>如果存在，则从输入中删除掉该子串，重新按照规则取子串，重复1</li>
<li>如果不存在于字典中，则从左向右减少子串长度，重复1</li>
</ol>
<h5 id="fen-ci-shi-li-1">分词实例</h5>
<p>输入 “北京大学生前来应聘”，假设词典中最长的单词为 5 个（MAX_LENGTH）。</p>
<ol>
<li>第一轮：取子串 “生前来应聘”，逆向取词，如果匹配失败，<strong>每次去掉匹配字段最前面的一个字</strong><br>
“生前来应聘”，扫描 5 字词典，没有匹配，字串长度减 1 变为“前来应聘”<br>
“前来应聘”，扫描 4 字词典，没有匹配，字串长度减 1 变为“来应聘”<br>
“来应聘”，扫描 3 字词典，没有匹配，字串长度减 1 变为“应聘”<br>
“应聘”，扫描 2 字词典，有匹配，输出“应聘”，输入变为“大学生前来”</li>
<li>第二轮：取子串“大学生前来”<br>
“大学生前来”，扫描 5 字词典，没有匹配，字串长度减 1 变为“学生前来”<br>
“学生前来”，扫描 4 字词典，没有匹配，字串长度减 1 变为“生前来”<br>
“生前来”，扫描 3 字词典，没有匹配，字串长度减 1 变为“前来”<br>
“前来”，扫描 2 字词典，有匹配，输出“前来”，输入变为“北京大学生”</li>
<li>第三轮：取子串“北京大学生”<br>
“北京大学生”，扫描 5 字词典，没有匹配，字串长度减 1 变为“京大学生”<br>
“京大学生”，扫描 4 字词典，没有匹配，字串长度减 1 变为“大学生”<br>
“大学生”，扫描 3 字词典，有匹配，输出“大学生”，输入变为“北京”</li>
<li>第四轮：取子串“北京”<br>
“北京”，扫描 2 字词典，有匹配，输出“北京”，输入变为“”<br>
输入长度为0，扫描终止</li>
</ol>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-1"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>数据准备：词库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_max_match</span><span class="params">(sentence, window_size, word_dict)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    后向最大匹配算法：</span></span><br><span class="line"><span class="string">    （1）从后向前扫描字典，测试读入的子串是否在字典中</span></span><br><span class="line"><span class="string">    （2）如果存在，则从输入中删除掉该子串，重新按照规则取子串，重复（1）</span></span><br><span class="line"><span class="string">    （3）如果不存在于字典中，则从右向左减少子串长度，重复（1）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param sentence: 待分词的句子</span></span><br><span class="line"><span class="string">    :param window_size: 词的最大长度</span></span><br><span class="line"><span class="string">    :param word_dict: 词典</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    seg_words = []</span><br><span class="line">    <span class="keyword">while</span> sentence:</span><br><span class="line">        <span class="keyword">for</span> word_len <span class="keyword">in</span> range(window_size, <span class="number">0</span>, <span class="number">-1</span>):  <span class="comment"># 每次去掉匹配字段最前面的一个字</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> sentence[len(sentence) - word_len:] <span class="keyword">in</span> word_dict:</span><br><span class="line">                <span class="comment"># 如果该词再字典中，将该词保存到分词结果中,并将其从句中切出来</span></span><br><span class="line">                seg_words.append(sentence[len(sentence) - word_len:])</span><br><span class="line">                sentence = sentence[:len(sentence) - word_len]</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 如果窗口中的词不在字典中，将最后一个字切分出来</span></span><br><span class="line">            seg_words.append(sentence[<span class="number">-1</span>:])</span><br><span class="line">            sentence = sentence[:<span class="number">-1</span>]</span><br><span class="line">    seg_words.reverse()</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'/'</span>.join(seg_words)</span><br></pre></td></tr></table></figure>
<h4 id="shuang-xiang-zui-da-pi-pei-fa">双向最大匹配法</h4>
<p>因为同一个句子，在机械分词中经常会出现多种分词的组合，因此需要进行歧义消除，来得到最优的分词结果。</p>
<p>以很常见的MMSEG机械分词算法为例，MMSEG在搜索引擎Solr中经常使用到，是一种非常可靠高效的分词算法。MMSEG消除歧义的规则有四个，它在使用中依次用这四个规则进行过滤，直到只有一种结果或者第四个规则使用完毕。这个四个规则分别是：</p>
<ol>
<li>
<p>最大匹配。选择“词组长度最大的”那个词组，然后选择这个词组的第一个词，作为切分出的第一个词，如对于“中国人民万岁”，匹配结果分别为：</p>
<ul>
<li>中/国/人</li>
<li>中国/人/民</li>
<li>中国/人民/万岁</li>
<li>中国人/民/万岁</li>
</ul>
<p>在这个例子“词组长度最长的”词组为后两个，因此选择了“中国人/民/万岁”中的“中国人”，或者“中国/人民/万岁”中的“中国”。</p>
</li>
<li>
<p>最大平均词语长度。经过规则1过滤后，如果剩余的词组超过1个，那就选择平均词语长度最大的那个(平均词长=词组总字数/词语数量)。比如“生活水平”，可能得到如下词组：</p>
<ul>
<li>生/活水/平 (4/3=1.33)</li>
<li>生活/水/平 (4/3=1.33)</li>
<li>生活/水平 (4/2=2)</li>
</ul>
<p>根据此规则，就可以确定选择“生活/水平”这个词组</p>
</li>
<li>
<p>词语长度的最小变化率。这个变化率一般可以由标准差来决定。比如对于“中国人民万岁”这个短语，可以计算：</p>
<ul>
<li>中国/人民/万岁(标准差=sqrt(((2-2)<sup>2+(2-2)</sup>2+(2-2^2))/3)=0)</li>
<li>中国人/民/万岁(标准差=sqrt(((2-3)<sup>2+(2-1)</sup>2+(2-2)^2)/3)=0.8165)</li>
</ul>
<p>于是选择“中国/人民/万岁”这个词组。</p>
</li>
<li>
<p>计算词组中的所有单字词词频的自然对数，然后将得到的值相加，取总和最大的词组。比如：</p>
<ul>
<li>设施/和服/务</li>
<li>设施/和/服务</li>
</ul>
<p>这两个词组中分别有“务”和“和”这两个单字词，假设“务”作为单字词时候的频率是5，“和”作为单字词时候的频率是10，对5和10取自然对数，然后取最大值者，所以取“和”字所在的词组，即“设施/和/服务”。</p>
</li>
</ol>
<p>在实际中根据需求，选择或制定相应的规则来改善分词的质量。</p>
<h5 id="suan-fa-liu-cheng-2">算法流程</h5>
<ol>
<li>比较正向最大匹配和逆向最大匹配结果</li>
<li>如果分词数量结果不同，那么取分词数量较少的那个</li>
<li>如果分词数量结果相同
<ul>
<li>分词结果相同，可以返回任何一个</li>
<li>分词结果不同，返回单字数比较少的那个</li>
</ul>
</li>
</ol>
<p>这种规则的出发点来自语言学上的启发：汉语中单字词的数量要远小于非单字词。因此，算法应当尽量减少结果中的单字，保留更多的完整词语。</p>
<h5 id="fen-ci-shi-li-2">分词实例</h5>
<p>正向匹配最终切分结果为：北京大学 / 生前 / 来 / 应聘，分词数量为 4，单字数为 1<br>
逆向匹配最终切分结果为：”北京/ 大学生/ 前来 / 应聘，分词数量为 4，单字数为 0<br>
逆向匹配单字数少，因此返回逆向匹配的结果。</p>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-2"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>数据准备：词库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fb_max_match</span><span class="params">(sentence,window_size,word_dict)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    1. 比较正向最大匹配和逆向最大匹配结果</span></span><br><span class="line"><span class="string">    2. 如果分词数量结果不同，那么取分词数量较少的那个</span></span><br><span class="line"><span class="string">    3. 如果分词数量结果相同</span></span><br><span class="line"><span class="string">       * 分词结果相同，可以返回任何一个</span></span><br><span class="line"><span class="string">       * 分词结果不同，返回单字数比较少的那个</span></span><br><span class="line"><span class="string">    :param sentence: 待分词的句子</span></span><br><span class="line"><span class="string">    :param window_size: 词的最大长度</span></span><br><span class="line"><span class="string">    :param word_dict: 词典</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    forward_seg = forward_max_match(sentence,window_size,word_dict)</span><br><span class="line">    backward_seg = backward_max_match(sentence,window_size,word_dict)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果分词结果不同，返回词数较小的分词结果</span></span><br><span class="line">    <span class="keyword">if</span> len(forward_seg) != len(backward_seg):</span><br><span class="line">        <span class="keyword">return</span> forward_seg <span class="keyword">if</span> len(forward_seg) &lt; len(backward_seg) <span class="keyword">else</span> backward_seg</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果分词结果词数相同，优先考虑返回包含单个字符最少的分词结果</span></span><br><span class="line">        forward_single_word_count = len([filter(<span class="keyword">lambda</span> x:len(x) == <span class="number">1</span>,forward_seg)])</span><br><span class="line">        backward_single_word_count = len([filter(<span class="keyword">lambda</span> x:len(x) == <span class="number">1</span>,backward_seg)])</span><br><span class="line">        <span class="keyword">if</span> forward_single_word_count != backward_single_word_count:</span><br><span class="line">            <span class="keyword">return</span> forward_seg <span class="keyword">if</span> forward_single_word_count &lt; backward_single_word_count <span class="keyword">else</span> backward_seg</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 否则，返回任意结果</span></span><br><span class="line">            <span class="keyword">return</span> forward_seg</span><br></pre></td></tr></table></figure>
<h5 id="que-dian">缺点：</h5>
<p>对歧义问题的解决有点弱</p>
<h4 id="n-zui-duan-lu-jing-fen-ci-suan-fa">(N-)最短路径分词算法</h4>
<h5 id="ji-ben-si-xiang">基本思想</h5>
<p>首先基于词典对文本进行全切分(改编版最大匹配)；然后基于词语的临接关系构建一个有向图；使用模型(比如一阶马尔科夫模型)对图里的边加权；最后使用最短路径算法求，从句首到句末质量最高(比如概率最大)的路径，就得到了分词结果。</p>
<h5 id="ju-li">举例</h5>
<p>最短路径分词算法首先将一句话中的所有词匹配出来，构成词图（有向无环图DAG），之后寻找从起始点到终点的最短路径作为最佳组合方式，以“他说的确实在理”为例，给出对这句话的3-最短路：<br>
<img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95.png" alt="avatar"></p>
<ol>
<li>
<p>构建有向图：根据一个已有词典构造出的有向无环图（全切分，根据临接关系构建有向图）。它将字串分为单个的字，每个字用图中相邻的两个结点表示，故对于长度为n的字串，需要n+1个结点。两节点间若有边，则表示两节点间所包含的所有结点构成的词，如图中结点2、3、4构成词“的确”。</p>
</li>
<li>
<p>计算权重：本例子中权重为1（为了简单），实际应用中，可以使用一阶马尔科夫模型进行权重计算。</p>
<ul>
<li>
<p>p(实，在)=p(实)*p(在|实)。概率取值越大，说明一个边出现的概率越大，这条边会提升所在分句结果的概率，由于最后要计算最短路径，需要构造一个连接权重与分局结果质量成反比的指标，因此对概率取了倒：weight=1/p(实,在)</p>
</li>
<li>
<p>这个概率可能非常小，得到的权重取值非常大。而我们后面在计算路径的长度时，会将若干个边的权重加起来，这时候有上溢出的可能性。避免上溢出的常用策略是取对数。：weight=log(1/p(实，在))。</p>
</li>
<li>
<p>概率的估算：概率就用频率来估计，p(实) = （“实”字在语料中出现的次数）/（语料的总词数），</p>
<p>p(在|实) = p(实在)/p(实)=(“实在”在语料中出现的次数)/(“实”在语料中出现的次数)</p>
</li>
</ul>
</li>
</ol>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-wei-te-bi-a"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码（维特比）</a></h5>
<p>数据准备：1、词库，2、词语之间的条件概率（用来计算路径权重）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用维特比算法求词图的最短路径</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(self, word_graph)</span>:</span></span><br><span class="line">    path_length_map = &#123;&#125;  <span class="comment"># 用于存储所有的路径，后面的邻接词语所在位置，以及对应的长度</span></span><br><span class="line">    word_graph = [[[<span class="string">"&lt;start&gt;"</span>, <span class="number">1</span>]]] + word_graph + [[[<span class="string">"&lt;end&gt;"</span>, <span class="number">-1</span>]]]</span><br><span class="line">    <span class="comment"># 这是一种比较简单的数据结构</span></span><br><span class="line">    path_length_map[(<span class="string">"&lt;start&gt;"</span>,)] = [<span class="number">1</span>, <span class="number">0</span>]  <span class="comment"># start处，后面的临接词语在列表的1处，路径长度是0,。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(word_graph)):</span><br><span class="line">        distance_from_start2current = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> len(word_graph[i]) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> former_path <span class="keyword">in</span> list(path_length_map.keys()):  <span class="comment"># path_length_map内容一直在变，需要深拷贝key,也就是已经积累的所有路径</span></span><br><span class="line">            <span class="comment"># 取出已经积累的路径，后面的临接词语位置，以及路径的长度。</span></span><br><span class="line">            [next_index_4_former_path, former_distance] = path_length_map[former_path]</span><br><span class="line">            former_word = former_path[<span class="number">-1</span>]</span><br><span class="line">            later_path = list(former_path)</span><br><span class="line">            <span class="keyword">if</span> next_index_4_former_path == i:  <span class="comment"># 如果这条路径的临接词语的位置就是当前索引</span></span><br><span class="line">                <span class="keyword">for</span> current_word <span class="keyword">in</span> word_graph[i]:  <span class="comment"># 遍历词图数据中，这个位置上的所有换选词语，然后与former_path拼接新路径</span></span><br><span class="line">                    current_word, next_index = current_word</span><br><span class="line">                    new_path = tuple(later_path + [current_word])  <span class="comment"># 只有int, string, tuple这种不可修改的数据类型可以hash，</span></span><br><span class="line">                    <span class="comment"># 也就是成为dict的key</span></span><br><span class="line">                    <span class="comment"># 计算新路径的长度</span></span><br><span class="line">                    new_path_len = former_distance + self.word_distance.get((former_word, current_word), <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">                    path_length_map[new_path] = [next_index, new_path_len]  <span class="comment"># 存储新路径后面的临接词语，以及路径长度</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 维特比的部分。选择到达当前节点的路径中，最短的那一条</span></span><br><span class="line">                    <span class="keyword">if</span> current_word <span class="keyword">in</span> distance_from_start2current:  <span class="comment"># 如果已经有到达当前词语的路径，需要择优</span></span><br><span class="line">                        <span class="keyword">if</span> distance_from_start2current[current_word][<span class="number">1</span>] &gt; new_path_len:  <span class="comment"># 如果当前新路径比已有的更短</span></span><br><span class="line">                            distance_from_start2current[current_word] = [new_path, new_path_len]  <span class="comment"># 用更短的路径数据覆盖原来的</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        distance_from_start2current[current_word] = [new_path, new_path_len]  <span class="comment"># 如果还没有这条路径，就记录它</span></span><br><span class="line">    shortest_path = distance_from_start2current[<span class="string">"&lt;end&gt;"</span>][<span class="number">0</span>]</span><br><span class="line">    shortest_path = shortest_path[<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> shortest_path</span><br></pre></td></tr></table></figure>
<h4 id="que-dian-1">缺点</h4>
<ol>
<li>对歧义和新词的处理不是很好，对词典中未出现的词没法进行处理。</li>
<li>分词效果取决于词典的质量。</li>
</ol>
<h3 id="ji-yu-tong-ji-yi-ji-ji-qi-xue-xi-de-fen-ci-fang-fa">基于统计以及机器学习的分词方法</h3>
<h4 id="ji-yu-n-gram-yu-yan-mo-xing-de-fen-ci-fang-fa">基于N-gram语言模型的分词方法</h4>
<h5 id="jian-jie">简介</h5>
<p>n-gram模型，称为N元模型，可用于定义字符串中的距离，也可用于中文的分词；该模型假设第n个词的出现只与前面n-1个词相关，与其他词都不相关，整个语句的概率就是各个词出现概率的乘积；而这些概率，利用语料，统计同时出现相关词的概率次数计算得到；常用的模型是Bi-gram和Tri-gram模型。<br>
假设一个字符串s由m个词组成，因此我们需要计算出\(p(w_1,w_2,\ldots,w_m)\)的概率，根据概率论中的链式法则得到如下：<br>
\[
p(w_1,w_2,\ldots,w_m) = p(w_1)*p(w_2|w_1)*p(w_3|w_2,w_1)\ldots p(w_m|w_{m-1},\ldots,w_2,w_1)=\prod_{i}{p(w_i|w_1,w_2,\ldots,w_{i-1})}
\]<br>
那么下面的问题是如何计算上面每一个概率，比如\(p(w_1,w_2,w_3,w_4,w_5)\)，一种比较直观的计算就是计数然后用除法：<br>
\[
p(w_5|w_1,w_2,w_3,w_4) = \frac{Count(w_1,w_2,w_3,w_4,w_5)}{Count(w_1,w_2,w_3,w_4)}
\]</p>
<p>直接计算这个概率的难度有点大：</p>
<ol>
<li>
<p>直接这样计算会导致参数空间过大。</p>
<p>一个语言模型的参数就是所有的这些条件概率，试想按上面方式计算\(p(w_5|w_1,w_2,w_3,w_4)\),这里\(w_5\)有词典大小取值的可能，记词典大小：\(|V|\)，则该模型的参数个数是\(|V|^5\)，而且这还不包含\(P(w_4|w_1,w_2,w_3)\)的个数，可以看到这样去计算条件概率会使语言模型参数个数过多而无法使用。</p>
</li>
<li>
<p>数据稀疏严重。我的理解是像上面那样计数计算，比如计数分子\(w_1,w_2,w_3,w_4,w_5\),在我们所能见的文本中出现的次数是很小的，这样计算的结果是过多的条件概率会等于0，因为我们根本没有看到足够的文本来统计！假设一个语料库中单词的数量为\(|V|\)个，一个句子由\(n\)个词组成，那么每个单词都可能有\(|V|\)个取值，那么由这些词组成的\(n\)元组合的数目为\(|V|^n\)种，也就是说，组合数会随着\(n\)的增加而呈现指数级别的增长，随着\(n\)的增加，语料数据库能够提供的数据是非常有限的，除非有海量的各种类型的语料数据，否则还有大量的\(n\)元组合都没有在语料库中出现过（即由\(n\)个单词所组成的一些句子根本就没出现过，可以理解为很多的\(n\)元组所组成的句子不能被人很好地理解）也就是说依据最大似然估计得到的概率将会是0，模型可能仅仅能够计算寥寥几个句子。怎么解决呢？</p>
</li>
</ol>
<p>解决參数空间过大的问题。引入了马尔科夫假设：**随意一个词出现的概率只与它前面出现的有限的一个或者几个词有关。**假设第\(w_i\)个词语只与它前面的\(n\)个词语相关，这样我们就得到前面的条件概率计算简化如下<br>
\[
p(w_i|w_{i-1},\ldots,w_2,w_1) \approx p(w_i|w_{i-1},\ldots,w_{i-n})\\
p(w_1,w_2,\ldots,w_m) \approx \prod_{i} p(w_i|w_{i-1},\ldots,w_{i-n})
\]</p>
<p>当n=1，即一元模型（Uni-gram）,即\(w_i\)与它前面的0个词相关，即\(w_i\)不与任何词相关，每一个词都是相互独立的：<br>
\[
P\left(w_{1}, w_{2}, \cdots, w_{m}\right)=\prod_{i=1}^{m} P\left(w_{i}\right)
\]<br>
当n=2，即二元模型（Bi-gram）,此时\(w_i\)与它前面1个词相关：<br>
\[
P\left(w_{1}, w_{2}, \cdots, w_{m}\right)=\prod_{i=1}^{m} P\left(w_{i} | w_{i-1}\right)
\]<br>
当n=3时，即三元模型（Tri-gram）,此时\(w_i\)与它前面2个词相关：<br>
\[
P\left(w_{1}, w_{2}, \cdots, w_{m}\right)=\prod_{i=1}^{m} P\left(w_{i} | w_{i-2} w_{i-1}\right)
\]<br>
一般来说，N元模型就是假设当前词的出现概率只与它前面的N-1个词有关。而这些概率参数都是可以通过大规模语料库来计算。<strong>在实践中用的最多的就是bigram和trigram了，高于四元的用的非常少，由于训练它须要更庞大的语料，并且数据稀疏严重，时间复杂度高，精度却提高的不多。</strong></p>
<h5 id="can-shu-gu-ji">参数估计</h5>
<p>要计算出模型中的条件概率，这些条件概率也称为模型的参数，得到这些参数的过程称为训练。用最大似然性估计计算下面的条件概率：<br>
\[
P\left(w_{i} | w_{i-1}\right)=\frac{c\left(w_{i-1}, w_{i}\right)}{c\left(w_{i-1}\right)}
\]<br>
一元语言模型中：句子概率定义为：\(P\left(s\right)=\prod_{i=1}^{m} P\left(w_{i}\right)\),这个式子成立的条件是有一个假设，就是条件无关假设，我们认为每个词都是条件无关的。这里的参数种类是一种 \(P(w_n)\),但是参数实例有\(|V|\)个(V是词典大小),我们应该如何得到每个参数实例的值。用的是极大似然估计法。比如训练语料是:&quot;星期五早晨，我特意起了个大早，为的就是看看早晨的天空。&quot;那么我们的字典为：星 期 五 早 晨 ，我 特 意 起 了 个 大 早 为 的 就 是 看 天 空 。 22个不同词，每个词语的概率直接用极大似然估计法估计得到。如：p(星) = 1/27，p(期) = 1/27。于是需要存储学习得到的模型参数，一个向量，22维，每个维度保存着每个单词的概率值。当需要计算：p(我看看早晨的天空)=p(我)p(看)p(看)p(早)p(晨)p(的)p(天)p(空)=\(\frac{1}{27}*\frac{1}{27}*\frac{1}{27}\ldots *\frac{1}{27}\)，可以直接计算出来。于是只要将每句话拆开为每个单词然后用累积形式运算，这样就能算出每句话的概率。缺点是：不包含语序信息。</p>
<p>二元语言模型中：为了计算对应的二元模型的参数，即\(P(w_i | w_{i-1})\)，要先计数即\(c(w_{i-1},w_i)\)，然后计数\(c(w_{i-1})\),再用除法可得到这些条件概率.可以看到对于\(c(w_{i-1},w_i)\)来说，\(w_{i-1}\)有语料库词典大小（记作\(|V|\)）的可能取值，\(w_i\)也是，所以\(c(w_{i-1},w_i)\)要计算的个数有\(|V|^2\)。</p>
<p>\(c(w_{i-1},w_i)\)计数结果如下:</p>
<table>
<thead>
<tr>
<th></th>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>english</th>
</tr>
</thead>
<tbody>
<tr>
<td>i</td>
<td>5</td>
<td>827</td>
<td>0</td>
<td>9</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>want</td>
<td>2</td>
<td>0</td>
<td>608</td>
<td>1</td>
<td>6</td>
<td>6</td>
<td>5</td>
<td>1</td>
</tr>
<tr>
<td>to</td>
<td>2</td>
<td>0</td>
<td>4</td>
<td>686</td>
<td>2</td>
<td>0</td>
<td>6</td>
<td>211</td>
</tr>
<tr>
<td>eat</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>16</td>
<td>2</td>
<td>42</td>
<td>0</td>
</tr>
<tr>
<td>chinese</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>82</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>food</td>
<td>15</td>
<td>0</td>
<td>15</td>
<td>0</td>
<td>1</td>
<td>4</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>lunch</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>spend</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>\(c(w_{i-1})\)的计数如下：</p>
<table>
<thead>
<tr>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>english</th>
</tr>
</thead>
<tbody>
<tr>
<td>2533</td>
<td>927</td>
<td>2417</td>
<td>746</td>
<td>158</td>
<td>1093</td>
<td>341</td>
<td>278</td>
</tr>
</tbody>
</table>
<p>那么二元模型的参数计算结果如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>english</th>
</tr>
</thead>
<tbody>
<tr>
<td>i</td>
<td>0.002</td>
<td>0.33</td>
<td>0</td>
<td>0.0036</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.00079</td>
</tr>
<tr>
<td>want</td>
<td>0.0022</td>
<td>0</td>
<td>0.66</td>
<td>0.0011</td>
<td>0.0065</td>
<td>0.0065</td>
<td>0.0054</td>
<td>0.0011</td>
</tr>
<tr>
<td>to</td>
<td>0.00083</td>
<td>0</td>
<td>0.0017</td>
<td>0.28</td>
<td>0.00083</td>
<td>0</td>
<td>0.0025</td>
<td>0.087</td>
</tr>
<tr>
<td>eat</td>
<td>0</td>
<td>0</td>
<td>0.0027</td>
<td>0</td>
<td>0.021</td>
<td>0.0027</td>
<td>0.056</td>
<td>0</td>
</tr>
<tr>
<td>chinese</td>
<td>0.0063</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.52</td>
<td>0.0063</td>
<td>0</td>
</tr>
<tr>
<td>food</td>
<td>0.014</td>
<td>0</td>
<td>0.0014</td>
<td>0</td>
<td>0.00092</td>
<td>0.0037</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>lunch</td>
<td>0.0059</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.0029</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>spend</td>
<td>0.0036</td>
<td>0</td>
<td>0.0036</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>比如计算其中的P(want | i) = 0.33如:\(p(want|i) = \frac{c(i,want)}{c(i)}=\frac{827}{2533} \approx0.33\),针对这个语料库的二元模型建立好了后，可以计算目标，即一个句子的概率了，一个例子如下：</p>
<p>\(p(s)=p(i want english food) = p(i|&lt;start>)*p(want|i)*p(english|want)*p(food|english)*p(&lt;end>|food) \approx 0.000031\)</p>
<p>该二元模型所捕捉到的一些实际信息:</p>
<ul>
<li>\(p(english|want) = 0.0011,p(chinese|want)=0.0065\),want chinese 的概率更高，这和真实世界情况相对应，因为chinese food 比 English food 更受欢迎。</li>
<li>\(p(to|want)=0.66\), want to 的概率很高，反映了语法特性</li>
<li>\(p(food|to)=0\) ，to food 概率为0，因为这种搭配不常见</li>
<li>\(p(want|spend)=0\), spend want 概率为0，因为这样违反了语法。</li>
</ul>
<p>常常在对数空间里面计算概率，原因有两个：</p>
<ol>
<li>防止溢出，如果计算的句子很长，最后得到的结果将非常小，甚至会溢出，比如计算得到的概率是0.001，那么假设以10为底取对数的结果就是-3，这样就不会溢出。</li>
<li>对数空间里面加法可以代替乘法，因为log(p1p2) = logp1 + logp2，而在计算机内部，显然加法比乘法执行更快！</li>
</ol>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-3"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>数据准备：unigram:(word, freq),bigram:(word1,word2,freq) 语料数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="comment"># global parameter</span></span><br><span class="line">DELIMITER = <span class="string">" "</span>  <span class="comment"># 分词之后的分隔符</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DNASegment</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.word1_dict_prob = &#123;&#125;  <span class="comment"># 记录概率,1-gram</span></span><br><span class="line">        self.word1_dict_count = &#123;&#125;  <span class="comment"># 记录词频,1-gram</span></span><br><span class="line">        self.word1_dict_count[<span class="string">"&lt;S&gt;"</span>] = <span class="number">8310575403</span>  <span class="comment"># 开始的&lt;S&gt;的个数</span></span><br><span class="line"></span><br><span class="line">        self.word2_dict_prob = &#123;&#125;  <span class="comment"># 记录概率,2-gram</span></span><br><span class="line">        self.word2_dict_count = &#123;&#125;  <span class="comment"># 记录词频,2-gram</span></span><br><span class="line"></span><br><span class="line">        self.gmax_word_length = <span class="number">0</span></span><br><span class="line">        self.all_freq = <span class="number">0</span>  <span class="comment"># 所有词的词频总和,1-gram的</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 估算未出现的词的概率,根据beautiful data里面的方法估算</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_unkonw_word_prob</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> math.log(<span class="number">10.</span> / (self.all_freq * <span class="number">10</span> ** len(word)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得片段的概率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_prob</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> self.word1_dict_prob:  <span class="comment"># 如果字典包含这个词</span></span><br><span class="line">            prob = self.word1_dict_prob[word]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            prob = self.get_unkonw_word_prob(word)</span><br><span class="line">        <span class="keyword">return</span> prob</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得两个词的转移概率(bigram)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_trans_prob</span><span class="params">(self, first_word, second_word)</span>:</span></span><br><span class="line">        trans_word = first_word + <span class="string">" "</span> + second_word</span><br><span class="line">        <span class="comment"># print trans_word</span></span><br><span class="line">        <span class="keyword">if</span> trans_word <span class="keyword">in</span> self.word2_dict_count:</span><br><span class="line">            trans_prob = math.log(self.word2_dict_count[trans_word] / self.word1_dict_count[first_word])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            trans_prob = self.get_word_prob(second_word)</span><br><span class="line">        <span class="keyword">return</span> trans_prob</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 寻找node的最佳前驱节点</span></span><br><span class="line">    <span class="comment"># 方法为寻找所有可能的前驱片段</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_best_pre_node</span><span class="params">(self, sequence, node, node_state_list)</span>:</span></span><br><span class="line">        <span class="comment"># 如果node比最大词长小，取的片段长度以node的长度为限</span></span><br><span class="line">        max_seg_length = min([node, self.gmax_word_length])</span><br><span class="line">        pre_node_list = []  <span class="comment"># 前驱节点列表</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得所有的前驱片段，并记录累加概率</span></span><br><span class="line">        <span class="keyword">for</span> segment_length <span class="keyword">in</span> range(<span class="number">1</span>, max_seg_length + <span class="number">1</span>):</span><br><span class="line">            segment_start_node = node - segment_length</span><br><span class="line">            segment = sequence[segment_start_node:node]  <span class="comment"># 获取片段</span></span><br><span class="line"></span><br><span class="line">            pre_node = segment_start_node  <span class="comment"># 取该片段，则记录对应的前驱节点</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> pre_node == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 如果前驱片段开始节点是序列的开始节点，</span></span><br><span class="line">                <span class="comment"># 则概率为&lt;S&gt;转移到当前词的概率</span></span><br><span class="line">                <span class="comment"># segment_prob = self.get_word_prob(segment)</span></span><br><span class="line">                segment_prob = self.get_word_trans_prob(<span class="string">"&lt;S&gt;"</span>, segment)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 如果不是序列开始节点，按照二元概率计算</span></span><br><span class="line">                <span class="comment"># 获得前驱片段的前一个词</span></span><br><span class="line">                pre_pre_node = node_state_list[pre_node][<span class="string">"pre_node"</span>]</span><br><span class="line">                pre_pre_word = sequence[pre_pre_node:pre_node]</span><br><span class="line">                segment_prob = self.get_word_trans_prob(pre_pre_word, segment)</span><br><span class="line"></span><br><span class="line">            pre_node_prob_sum = node_state_list[pre_node][<span class="string">"prob_sum"</span>]  <span class="comment"># 前驱节点的概率的累加值</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 当前node一个候选的累加概率值</span></span><br><span class="line">            candidate_prob_sum = pre_node_prob_sum + segment_prob</span><br><span class="line"></span><br><span class="line">            pre_node_list.append((pre_node, candidate_prob_sum))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 找到最大的候选概率值</span></span><br><span class="line">        (best_pre_node, best_prob_sum) = max(pre_node_list, key=<span class="keyword">lambda</span> d: d[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> (best_pre_node, best_prob_sum)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最大概率分词</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mp_seg</span><span class="params">(self, sequence)</span>:</span></span><br><span class="line">        sequence = sequence.strip()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化</span></span><br><span class="line">        node_state_list = []  <span class="comment"># 记录节点的最佳前驱，index就是位置信息</span></span><br><span class="line">        <span class="comment"># 初始节点，也就是0节点信息</span></span><br><span class="line">        ini_state = &#123;&#125;</span><br><span class="line">        ini_state[<span class="string">"pre_node"</span>] = <span class="number">-1</span>  <span class="comment"># 前一个节点</span></span><br><span class="line">        ini_state[<span class="string">"prob_sum"</span>] = <span class="number">0</span>  <span class="comment"># 当前的概率总和</span></span><br><span class="line">        node_state_list.append(ini_state)</span><br><span class="line">        <span class="comment"># 字符串概率为2元概率</span></span><br><span class="line">        <span class="comment"># P(a b c) = P(a|&lt;S&gt;)P(b|a)P(c|b)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 逐个节点寻找最佳前驱节点</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> range(<span class="number">1</span>, len(sequence) + <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 寻找最佳前驱，并记录当前最大的概率累加值</span></span><br><span class="line">            (best_pre_node, best_prob_sum) = self.get_best_pre_node(sequence, node, node_state_list)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 添加到队列</span></span><br><span class="line">            cur_node = &#123;&#125;</span><br><span class="line">            cur_node[<span class="string">"pre_node"</span>] = best_pre_node</span><br><span class="line">            cur_node[<span class="string">"prob_sum"</span>] = best_prob_sum</span><br><span class="line">            node_state_list.append(cur_node)</span><br><span class="line">            <span class="comment"># print "cur node list",node_state_list</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># step 2, 获得最优路径,从后到前</span></span><br><span class="line">        best_path = []</span><br><span class="line">        node = len(sequence)  <span class="comment"># 最后一个点</span></span><br><span class="line">        best_path.append(node)</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            pre_node = node_state_list[node][<span class="string">"pre_node"</span>]</span><br><span class="line">            <span class="keyword">if</span> pre_node == <span class="number">-1</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            node = pre_node</span><br><span class="line">            best_path.append(node)</span><br><span class="line">        best_path.reverse()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># step 3, 构建切分</span></span><br><span class="line">        word_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(best_path) - <span class="number">1</span>):</span><br><span class="line">            left = best_path[i]</span><br><span class="line">            right = best_path[i + <span class="number">1</span>]</span><br><span class="line">            word = sequence[left:right]</span><br><span class="line">            word_list.append(word)</span><br><span class="line"></span><br><span class="line">        seg_sequence = DELIMITER.join(word_list)</span><br><span class="line">        <span class="keyword">return</span> seg_sequence</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载词典，为词\t词频的格式</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initial_dict</span><span class="params">(self, gram1_file, gram2_file)</span>:</span></span><br><span class="line">        <span class="comment"># 读取unigram文件</span></span><br><span class="line">        dict_file = open(gram1_file, <span class="string">"r"</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> dict_file:</span><br><span class="line">            sequence = line.strip()</span><br><span class="line">            key = sequence.split(<span class="string">'\t'</span>)[<span class="number">0</span>]</span><br><span class="line">            value = float(sequence.split(<span class="string">'\t'</span>)[<span class="number">1</span>])</span><br><span class="line">            self.word1_dict_count[key] = value</span><br><span class="line">        <span class="comment"># 计算频率</span></span><br><span class="line">        self.all_freq = sum(self.word1_dict_count.values())  <span class="comment"># 所有词的词频</span></span><br><span class="line">        self.gmax_word_length = max(len(key) <span class="keyword">for</span> key <span class="keyword">in</span> self.word1_dict_count.keys())</span><br><span class="line">        self.gmax_word_length = <span class="number">20</span></span><br><span class="line">        self.all_freq = <span class="number">1024908267229.0</span></span><br><span class="line">        <span class="comment"># 计算1gram词的概率</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> self.word1_dict_count:</span><br><span class="line">            self.word1_dict_prob[key] = math.log(self.word1_dict_count[key] / self.all_freq)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取2_gram_file，同时计算转移概率</span></span><br><span class="line">        dict_file = open(gram2_file, <span class="string">"r"</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> dict_file:</span><br><span class="line">            sequence = line.strip()</span><br><span class="line">            key = sequence.split(<span class="string">'\t'</span>)[<span class="number">0</span>]</span><br><span class="line">            value = float(sequence.split(<span class="string">'\t'</span>)[<span class="number">1</span>])</span><br><span class="line">            first_word = key.split(<span class="string">" "</span>)[<span class="number">0</span>]</span><br><span class="line">            second_word = key.split(<span class="string">" "</span>)[<span class="number">1</span>]</span><br><span class="line">            self.word2_dict_count[key] = float(value)</span><br><span class="line">            <span class="keyword">if</span> first_word <span class="keyword">in</span> self.word1_dict_count:</span><br><span class="line">                <span class="comment"># 取自然对数</span></span><br><span class="line">                self.word2_dict_prob[key] = math.log(value / self.word1_dict_count[first_word])  </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.word2_dict_prob[key] = self.word1_dict_prob[second_word]</span><br></pre></td></tr></table></figure>
<h5 id="zong-jie">总结</h5>
<p><strong>列举出所有可能的分词方式，再分别计算句子概率，选择概率最大的作为最终分词结果。穷举法，速度慢。</strong></p>
<p>缺点：</p>
<ol>
<li>N-grams有一些不足，因为语言存在一个长距离依赖关系，比如：“The computer which I had just put into the machine room on the fifth floor crashed.”假如要预测最后一个词语crashed出现的概率，如果采用二元模型，那么crashed与floor实际关联可能性应该非常小，相反的，这句子的主语computer与crashed的相关性很大，但是n-grams并没有捕捉到这个信息。</li>
<li>一个词是由前一个或者几个词决定的，这样可以去除一部分歧义问题，但是n-gram模型还是基于马尔科夫模型的，其基本原理就是无后效性，就是后续的节点的状态不影响前面的状态，就是先前的分词形式一旦确定，无论后续跟的是什么词，都不会再有变化，这在现实中显然是不成立的。。因此就有一些可以考虑到后续词的算法，如crf等方法。</li>
</ol>
<h4 id="ji-yu-hmm-de-fen-ci-suan-fa">基于HMM的分词算法</h4>
<h5 id="yin-ma-er-ke-fu-mo-xing-hmm">隐马尔科夫模型(HMM)</h5>
<p>隐马尔可夫模型是关于<strong>时序</strong>的概率模型,描述由一个隐藏的马尔可夫链随机生成不可观测的<strong>状态序列</strong>,再由各个状态生成一个观测而产生<strong>观测随机序列</strong>的过程.HMM是一种生成式模型，它的理论基础是朴素贝叶斯，本质上就类似于我们将朴素贝叶斯在单样本分类问题上的应用推广到序列样本分类问题上。</p>
<h6 id="mo-xing-biao-shi">模型表示</h6>
<p>设Q是所有可能的状态的集合\(Q=\left\{q_{1}, q_{2}, \cdots, q_{N}\right\}\),V是所有可能的观测的集合\(V=\left\{v_{1}, v_{2}, \cdots, v_{M}\right\}\),I是长度为T的状态序列\(I=\left(i_{1}, i_{2}, \cdots, i_{T}\right)\), O是对应的观测序列\(O=\left(o_{1}, o_{2}, \cdots, o_{T}\right)\),</p>
<ol>
<li>A是<strong>状态转移概率矩阵</strong>\(A=\left[a_{i j}\right]_{N \times N}\),\(a_{ij}\)表示在时刻t处于状态\(q_i\)的条件下在时刻t+1转移到状态\(q_j\)的概率.</li>
<li>B是<strong>观测概率矩阵</strong> \(B=\left[b_{j}(k)\right]_{N \times M}\),\(b_{ij}\)是在时刻t处于状态\(q_j\)的条件下生成观测\(v_k\)的概率.</li>
<li>\(\pi\)是<strong>初始状态概率向量</strong>\(\pi=\pi(x)\),\(\pi_i\)表示时刻t=1处于状态qi的概率.</li>
</ol>
<p>隐马尔可夫模型由初始状态概率向量\(pi\),状态转移概率矩阵A以及观测概率矩阵B确定.\(\pi\)和A决定即隐藏的<strong>马尔可夫链</strong>,生成不可观测的<strong>状态序列</strong>.B决定如何从状态生成观测,与状态序列综合确定了<strong>观测序列</strong>.因此,隐马尔可夫模型可以用<strong>三元符号</strong>表示\(\lambda = (A,B,\pi)\)</p>
<h6 id="liang-ge-ji-ben-jia-she">两个基本假设</h6>
<ol>
<li><strong>齐次马尔可夫性假设</strong>:假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态.</li>
<li><strong>观测独立性假设</strong>:假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态.</li>
</ol>
<h6 id="san-ge-ji-ben-wen-ti">三个基本问题</h6>
<p><strong>1. 概率计算问题</strong></p>
<p>给定模型\(\lambda = (A,B,\pi)\)和观测序列,\(O=\left(o_{1}, o_{2}, \cdots, o_{T}\right)\)计算在模型\(\lambda\)下观测序列O出现的概率\(P(O|λ)\).</p>
<ol>
<li>直接计算：列举所有可能长度为T的状态序列,求各个状态序列I与观测序列O的联合概率,但计算量太大,实际操作不可行.</li>
<li><strong>前向算法</strong>：定义到时刻t部分观测序列为\(o_1\)~\(o_t\)且状态为\(q_i\)的概率为<strong>前向概率</strong>,记作\(\alpha_{t}(i)=P\left(o_{1}, o_{2}, \cdots, o_{t}, i_{t}=q_{i} | \lambda\right)\).初始化前向概率\(\alpha_{1}(i)=\pi_{i} b_{i}\left(o_{1}\right), \quad i=1,2, \cdots, N\)，递推，对\(t=1\) ~ \(T-1\),\(\alpha_{t+1}(i)=\left[\sum_{j=1}^{N} \alpha_{t}(j) a_{j i}\right] b_{i}\left(o_{t+1}\right), \quad i=1,2, \cdots, N\),得到\(P(O | \lambda)=\sum_{i=1}^{N} \alpha_{T}(i)\)减少计算量的原因在于每一次计算直接引用前一个时刻的计算结果,避免重复计算.</li>
<li><strong>后向算法</strong>:定义在时刻t状态为\(q_i\)的条件下,从t+1到T的部分观测序列为\(o_{i+1}\)~\(o_T\)的概率为<strong>后向概率</strong>,记作\(\beta_{t}(i)=P\left(o_{t+1}, o_{t+2}, \cdots, o_{r} | i_{t}=q_{i}, \lambda\right)\).初始化后向概率\(\beta_{r}(i)=1, \quad i=1,2, \cdots, N\),递推,对\(t=T-1\)~\(1\)\(\beta_{t}(i)=\sum_{j=1}^{N} a_{i j} b_{j}\left(o_{i+1}\right) \beta_{i+1}(j), \quad i=1,2, \cdots, N\),得到\(P(O | \lambda)=\sum_{i=1}^{N} \pi_{i} b_{i}\left(o_{1}\right) \beta_{1}(i)\)</li>
</ol>
<p><strong>2. 学习算法</strong></p>
<p>已知观测序列\(O=(o_1,o_2, \cdots,o_r)\),估计模型\(\lambda = (A,B,\pi)\),的参数,使得在该模型下观测序列概率\(p(O|\lambda)\)最大.根据训练数据是否包括观察序列对应的状态序列分别由监督学习与非监督学习实现.</p>
<ol>
<li>
<p>监督学习：估计转移概率\(\hat{a}_{i j}=\frac{A_{i j}}{\sum_{j=1}^{N} A_{i j}}, \quad i=1,2, \cdots, N ; \quad j=1,2, \cdots, N\) 和观测概率\(\hat{b}_{j}(k)=\frac{B_{j k}}{\sum_{k=1}^{M} B_{j k}}, \quad j=1,2, \cdots, N, k=1,2, \cdots, M\).初始状态概率\(\pi_i\)的估计为S个样本中初始状态为\(q_i\)的频率.</p>
</li>
<li>
<p><strong>非监督学习(Baum-Welch算法)</strong>:将观测序列数据看作观测数据O,状态序列数据看作不可观测的<strong>隐数据</strong>I.首先确定完全数据的对数似然函数\(log p(O,I|\lambda)\),求Q函数</p>
</li>
</ol>
<p>\[
   \begin{aligned}
   Q(\lambda, \bar{\lambda})=&amp; \sum_{t} \log \pi_{i_1} P(O, I | \bar{\lambda}) \\
   &amp;+\sum_{I}\left(\sum_{i=1}^{I-1} \log a_{i_t,i_{t+1}}\right) P(O, I | \bar{\lambda}) \\
   &amp;+\sum_{I}\left(\sum_{i=1}^{T} \log b_{i_t}\left(o_{t}\right)\right) P(O, I | \bar{\lambda})
   \end{aligned}
\]</p>
<p>,用拉格朗日乘子法极大化Q函数求模型参数\(\pi_{i}=\frac{P\left(O, i_{1}=i | \bar{\lambda}\right)}{P(O | \bar{\lambda})}\),\(a_{i j}=\frac{\sum_{i=1}^{T-1} P\left(O, i_{t}=i, i_{t+1}=j | \bar{\lambda}\right)}{\sum_{t=1}^{T-1} P\left(O, i_{t}=i | \bar{\lambda}\right)}\),\(b_{j}(k)=\frac{\sum_{i=1}^{T} P\left(O, i_{t}=j | \bar{\lambda}\right) I\left(o_{i}=v_{k}\right)}{\sum_{i=1}^{T} P\left(O, i_{t}=j | \bar{\lambda}\right)}\),</p>
<p><strong>3. 预测问题</strong></p>
<p>也称为解码问题.已知模型\(\lambda = (A,B,\pi)\)和观测序列\(O=(O_1,O_2,\cdots,O_T)\),求对给定观测序列条件概率\(P(I|O)\)最大的状态序列\(I=(i_1,i_2,\cdots,i_T)\)</p>
<ol>
<li>
<p><strong>近似算法</strong>: 在每个时刻t选择在该时刻最有可能出现的状态\(i_t^*\),从而得到一个状态序列作为预测的结果.优点是<strong>计算简单</strong>,缺点是不能保证状态序列整体是最有可能的状态序列</p>
</li>
<li>
<p><strong>维特比算法</strong>:用<strong>动态规划</strong>求概率最大路径,这一条路径对应着一个状态序列.从t=1开始,递推地计算在时刻t状态为i的各条部分路径的最大概率,直至得到时刻t=T状态为i的各条路径的最大概率.时刻t=T的最大概率即为<strong>最优路径</strong>的概率\(P^\star\),最优路径的<strong>终结点</strong>\(i_t^\star\)也同时得到,之后从终结点开始由后向前逐步求得<strong>结点</strong>,得到最优路径.</p>
</li>
</ol>
<h5 id="jian-jie-1">简介</h5>
<p>在分词算法中，分词对应着三大问题中的预测问题（解码问题），隐马尔可夫经常用作能够发现新词的算法，通过海量的数据学习，能够将人名、地名、互联网上的新词等一一识别出来，具有广泛的应用场景。</p>
<p>其分词过程:</p>
<p><img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/hmm_seg.png" alt="avatar"></p>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-4"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>数据准备：分好词的词库</p>
<p>step 1:通过统计语料库中词的频次，计算三个概率：初始状态概率start，状态转移概率矩阵trans，发射概率emit。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HmmTrain</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.line_index = <span class="number">-1</span></span><br><span class="line">        self.char_set = set()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init</span><span class="params">(self)</span>:</span>  <span class="comment"># 初始化字典</span></span><br><span class="line">        trans_dict = &#123;&#125;  <span class="comment"># 存储状态转移概率</span></span><br><span class="line">        emit_dict = &#123;&#125;  <span class="comment"># 发射概率(状态-&gt;词语的条件概率)</span></span><br><span class="line">        Count_dict = &#123;&#125;  <span class="comment"># 存储所有状态序列 ，用于归一化分母</span></span><br><span class="line">        start_dict = &#123;&#125;  <span class="comment"># 存储状态的初始概率</span></span><br><span class="line">        state_list = [<span class="string">'B'</span>, <span class="string">'M'</span>, <span class="string">'E'</span>, <span class="string">'S'</span>]  <span class="comment"># 状态序列</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> state <span class="keyword">in</span> state_list:</span><br><span class="line">            trans_dict[state] = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> state1 <span class="keyword">in</span> state_list:</span><br><span class="line">                trans_dict[state][state1] = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> state <span class="keyword">in</span> state_list:</span><br><span class="line">            start_dict[state] = <span class="number">0.0</span></span><br><span class="line">            emit_dict[state] = &#123;&#125;</span><br><span class="line">            Count_dict[state] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(trans_dict) #&#123;'B': &#123;'B': 0.0, 'S': 0.0, 'M': 0.0, 'E': 0.0&#125;, 'S': &#123;'B': 0.0, 'S': 0.0, 'M': 0.0, 'E': 0.0&#125;,。。。&#125;</span></span><br><span class="line">        <span class="comment"># print(emit_dict) # &#123;'B': &#123;&#125;, 'S': &#123;&#125;, 'M': &#123;&#125;, 'E': &#123;&#125;&#125;</span></span><br><span class="line">        <span class="comment"># print(start_dict) # &#123;'B': 0.0, 'S': 0.0, 'M': 0.0, 'E': 0.0&#125;</span></span><br><span class="line">        <span class="comment"># print(Count_dict) # &#123;'B': 0, 'S': 0, 'M': 0, 'E': 0&#125;</span></span><br><span class="line">        <span class="keyword">return</span> trans_dict, emit_dict, start_dict, Count_dict</span><br><span class="line"></span><br><span class="line">    <span class="string">'''保存模型'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_model</span><span class="params">(self, word_dict, model_path)</span>:</span></span><br><span class="line">        f = open(model_path, <span class="string">'w'</span>)</span><br><span class="line">        f.write(str(word_dict))</span><br><span class="line">        f.close()</span><br><span class="line"></span><br><span class="line">    <span class="string">'''词语状态转换'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_status</span><span class="params">(self, word)</span>:</span>  <span class="comment"># 根据词语，输出词语对应的SBME状态</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        S:单字词</span></span><br><span class="line"><span class="string">        B:词的开头</span></span><br><span class="line"><span class="string">        M:词的中间</span></span><br><span class="line"><span class="string">        E:词的末尾</span></span><br><span class="line"><span class="string">        能 ['S']</span></span><br><span class="line"><span class="string">        前往 ['B', 'E']</span></span><br><span class="line"><span class="string">        科威特 ['B', 'M', 'E']</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        word_status = []</span><br><span class="line">        <span class="keyword">if</span> len(word) == <span class="number">1</span>:</span><br><span class="line">            word_status.append(<span class="string">'S'</span>)</span><br><span class="line">        <span class="keyword">elif</span> len(word) == <span class="number">2</span>:</span><br><span class="line">            word_status = [<span class="string">'B'</span>, <span class="string">'E'</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            M_num = len(word) - <span class="number">2</span></span><br><span class="line">            M_list = [<span class="string">'M'</span>] * M_num</span><br><span class="line">            word_status.append(<span class="string">'B'</span>)</span><br><span class="line">            word_status.extend(M_list)</span><br><span class="line">            word_status.append(<span class="string">'E'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> word_status</span><br><span class="line"></span><br><span class="line">    <span class="string">'''基于人工标注语料库，训练发射概率，初始状态， 转移概率'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, train_filepath, trans_path, emit_path, start_path)</span>:</span></span><br><span class="line">        trans_dict, emit_dict, start_dict, Count_dict = self.init()</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> open(train_filepath):</span><br><span class="line">            self.line_index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            line = line.strip()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            char_list = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(line)):</span><br><span class="line">                <span class="keyword">if</span> line[i] == <span class="string">" "</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                char_list.append(line[i])</span><br><span class="line"></span><br><span class="line">            self.char_set = set(char_list)  <span class="comment"># 训练预料库中所有字的集合</span></span><br><span class="line"></span><br><span class="line">            word_list = line.split(<span class="string">" "</span>)</span><br><span class="line">            line_status = []  <span class="comment"># 统计状态序列</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> word_list:</span><br><span class="line">                line_status.extend(self.get_word_status(word))  <span class="comment"># 一句话对应一行连续的状态</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> len(char_list) == len(line_status):</span><br><span class="line">                <span class="comment"># print(word_list) # ['但', '从', '生物学', '眼光', '看', '就', '并非', '如此', '了', '。']</span></span><br><span class="line">                <span class="comment"># print(line_status) # ['S', 'S', 'B', 'M', 'E', 'B', 'E', 'S', 'S', 'B', 'E', 'B', 'E', 'S', 'S']</span></span><br><span class="line">                <span class="comment"># print('******')</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(len(line_status)):</span><br><span class="line">                    <span class="keyword">if</span> i == <span class="number">0</span>:  <span class="comment"># 如果只有一个词，则直接算作是初始概率</span></span><br><span class="line">                        start_dict[line_status[<span class="number">0</span>]] += <span class="number">1</span>  <span class="comment"># start_dict记录句子第一个字的状态，用于计算初始状态概率</span></span><br><span class="line">                        Count_dict[line_status[<span class="number">0</span>]] += <span class="number">1</span>  <span class="comment"># 记录每一个状态的出现次数</span></span><br><span class="line">                    <span class="keyword">else</span>:  <span class="comment"># 统计上一个状态到下一个状态，两个状态之间的转移概率</span></span><br><span class="line">                        trans_dict[line_status[i - <span class="number">1</span>]][line_status[i]] += <span class="number">1</span>  <span class="comment"># 用于计算转移概率</span></span><br><span class="line">                        Count_dict[line_status[i]] += <span class="number">1</span></span><br><span class="line">                        <span class="comment"># 统计发射概率</span></span><br><span class="line">                        <span class="keyword">if</span> char_list[i] <span class="keyword">not</span> <span class="keyword">in</span> emit_dict[line_status[i]]:</span><br><span class="line">                            emit_dict[line_status[i]][char_list[i]] = <span class="number">0.0</span></span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            emit_dict[line_status[i]][char_list[i]] += <span class="number">1</span>  <span class="comment"># 用于计算发射概率</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(emit_dict)#&#123;'S': &#123;'否': 10.0, '昔': 25.0, '直': 238.0, '六': 1004.0, '殖': 17.0, '仗': 36.0, '挪': 15.0, '朗': 151.0</span></span><br><span class="line">        <span class="comment"># print(trans_dict)#&#123;'S': &#123;'S': 747969.0, 'E': 0.0, 'M': 0.0, 'B': 563988.0&#125;, 'E': &#123;'S': 737404.0, 'E': 0.0, 'M': 0.0, 'B': 651128.0&#125;,</span></span><br><span class="line">        <span class="comment"># print(start_dict) #&#123;'S': 124543.0, 'E': 0.0, 'M': 0.0, 'B': 173416.0&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 进行归一化</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> start_dict:  <span class="comment"># 状态的初始概率</span></span><br><span class="line">            start_dict[key] = start_dict[key] * <span class="number">1.0</span> / self.line_index</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> trans_dict:  <span class="comment"># 状态转移概率</span></span><br><span class="line">            <span class="keyword">for</span> key1 <span class="keyword">in</span> trans_dict[key]:</span><br><span class="line">                trans_dict[key][key1] = trans_dict[key][key1] / Count_dict[key]</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> emit_dict:  <span class="comment"># 发射概率(状态-&gt;词语的条件概率)</span></span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> emit_dict[key]:</span><br><span class="line">                emit_dict[key][word] = emit_dict[key][word] / Count_dict[key]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(emit_dict)#&#123;'S': &#123;'否': 6.211504202703743e-06, '昔': 1.5528760506759358e-05, '直': 0.0001478338000243491,</span></span><br><span class="line">        <span class="comment"># print(trans_dict)#&#123;'S': &#123;'S': 0.46460125869921165, 'E': 0.0, 'M': 0.0, 'B': 0.3503213832274479&#125;,</span></span><br><span class="line">        <span class="comment"># print(start_dict)#&#123;'S': 0.41798844132394497, 'E': 0.0, 'M': 0.0, 'B': 0.5820149148537713&#125;</span></span><br><span class="line">        self.save_model(trans_dict, trans_path)</span><br><span class="line">        self.save_model(emit_dict, emit_path)</span><br><span class="line">        self.save_model(start_dict, start_path)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> trans_dict, emit_dict, start_dict</span><br></pre></td></tr></table></figure>
<p>Step 2: 使用维特比算法，求解概率最大路径</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HmmCut</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,start_model_path,trans_model_path,emit_model_path)</span>:</span></span><br><span class="line">        self.prob_trans = self.load_model(trans_model_path)</span><br><span class="line">        self.prob_emit = self.load_model(emit_model_path)</span><br><span class="line">        self.prob_start = self.load_model(start_model_path)</span><br><span class="line"></span><br><span class="line">    <span class="string">'''加载模型'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_model</span><span class="params">(self, model_path)</span>:</span></span><br><span class="line">        f = open(model_path, <span class="string">'r'</span>)</span><br><span class="line">        a = f.read()</span><br><span class="line">        word_dict = eval(a)</span><br><span class="line">        f.close()</span><br><span class="line">        <span class="keyword">return</span> word_dict</span><br><span class="line"></span><br><span class="line">    <span class="string">'''verterbi算法求解'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(self, obs, states, start_p, trans_p, emit_p)</span>:</span>  <span class="comment"># 维特比算法（一种递归算法）</span></span><br><span class="line">        <span class="comment"># 算法的局限在于训练语料要足够大，需要给每个词一个发射概率,.get(obs[0], 0)的用法是如果dict中不存在这个key,则返回0值</span></span><br><span class="line">        V = [&#123;&#125;]</span><br><span class="line">        path = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">            V[<span class="number">0</span>][y] = start_p[y] * emit_p[y].get(obs[<span class="number">0</span>], <span class="number">0</span>)  <span class="comment"># 在位置0，以y状态为末尾的状态序列的最大概率</span></span><br><span class="line">            path[y] = [y]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, len(obs)):</span><br><span class="line">            V.append(&#123;&#125;)</span><br><span class="line">            newpath = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">                state_path = ([(V[t - <span class="number">1</span>][y0] * trans_p[y0].get(y, <span class="number">0</span>) * emit_p[y].get(obs[t], <span class="number">0</span>), y0) <span class="keyword">for</span> y0 <span class="keyword">in</span> states <span class="keyword">if</span> V[t - <span class="number">1</span>][y0] &gt; <span class="number">0</span>])</span><br><span class="line">                <span class="keyword">if</span> state_path == []:</span><br><span class="line">                    (prob, state) = (<span class="number">0.0</span>, <span class="string">'S'</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    (prob, state) = max(state_path)</span><br><span class="line">                V[t][y] = prob</span><br><span class="line">                newpath[y] = path[state] + [y]</span><br><span class="line"></span><br><span class="line">            path = newpath  <span class="comment"># 记录状态序列</span></span><br><span class="line">        (prob, state) = max([(V[len(obs) - <span class="number">1</span>][y], y) <span class="keyword">for</span> y <span class="keyword">in</span> states])  <span class="comment"># 在最后一个位置，以y状态为末尾的状态序列的最大概率</span></span><br><span class="line">        <span class="keyword">return</span> (prob, path[state])  <span class="comment"># 返回概率和状态序列</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分词主控函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cut</span><span class="params">(self, sent)</span>:</span></span><br><span class="line">        prob, pos_list = self.viterbi(sent, (<span class="string">'B'</span>, <span class="string">'M'</span>, <span class="string">'E'</span>, <span class="string">'S'</span>), self.prob_start, self.prob_trans, self.prob_emit)</span><br><span class="line">        seglist = list()</span><br><span class="line">        word = list()</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(len(pos_list)):</span><br><span class="line">            <span class="keyword">if</span> pos_list[index] == <span class="string">'S'</span>:</span><br><span class="line">                word.append(sent[index])</span><br><span class="line">                seglist.append(word)</span><br><span class="line">                word = []</span><br><span class="line">            <span class="keyword">elif</span> pos_list[index] <span class="keyword">in</span> [<span class="string">'B'</span>, <span class="string">'M'</span>]:</span><br><span class="line">                word.append(sent[index])</span><br><span class="line">            <span class="keyword">elif</span> pos_list[index] == <span class="string">'E'</span>:</span><br><span class="line">                word.append(sent[index])</span><br><span class="line">                seglist.append(word)</span><br><span class="line">                word = []</span><br><span class="line">        seglist = [<span class="string">''</span>.join(tmp) <span class="keyword">for</span> tmp <span class="keyword">in</span> seglist]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> seglist</span><br></pre></td></tr></table></figure>
<p>如果画出状态转移图像，发现状态只能在层间点转移，转移路径上的分数为概率，求一条概率最大的路径。这可以用维特比算法计算，本质上就是一个动态规划</p>
<p><img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/hmm_viterbi.png" alt="avatar"></p>
<h5 id="zong-jie-1">总结</h5>
<p><strong>从三方面比较HMM和N-gram。</strong></p>
<ol>
<li>HMM模型是一个生成模型，区别于N-gram语言模型，HMM没有直接对给定观测值后状态的分布 \(P(S|O)\)（<em>O</em> 代表观测序列）进行建模，而是对状态序列本身的分布\(P(S)\)和给定状态后观测值的分布 \(p(O|S)\)建模 ；</li>
<li>学习过程与N-gram相同，HMM在有监督学习的情况下，使用极大似然估计参数；</li>
<li>预测时，HMM采用维特比算法。</li>
</ol>
<h4 id="crf-fen-ci">CRF分词</h4>
<h5 id="tiao-jian-sui-ji-chang-crf">条件随机场CRF</h5>
<p>条件随机场基于概率无向图模型，利用最大团理论，对随机变量的联合分布\(P(Y)\)进行建模。</p>
<p>在序列标注任务中的条件随机场，往往是指<strong>线性链条件随机场</strong>，这时，在条件概率模型\(P(Y|X)\)中，\(Y\)是输出变量，表示标记序列（或状态序列），\(X\)是输入变量，表示需要标注的观测序列。学习时，利用训练数据集通过极大似然估计或正则化的极大似然估计得到条件概率模型\(p(Y|X)\)；预测时，对于给定的输入序列x，求出条件概率\(p(y|x)\)最大的输出序列y.</p>
<h6 id="tiao-jian-sui-ji-chang-de-can-shu-hua-xing-shi">条件随机场的参数化形式</h6>
<p>\[
P(y|x)=\frac{1}{Z(x)} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, j} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right) 
\\
Z(x)=\sum_{y} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, j} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right)
\]</p>
<h5 id="tiao-jian-sui-ji-chang-fen-ci-fang-fa">条件随机场分词方法</h5>
<p>条件随机场和隐马尔可夫一样，也是使用BMES四个状态位来进行分词。以如下句子为例：</p>
<p>中 国 是 泱 泱 大 国<br>
B  B  B  B  B  B  B<br>
M M M M M M M<br>
E  E  E  E  E  E  E<br>
S  S  S  S  S  S  S</p>
<p>条件随机场解码就是在以上由标记组成的数组中搜索一条最优的路径。</p>
<p>要把每一个字(即观察变量)对应的每一个状态BMES(即标记变量)的概率都求出来。例如对于观察变量“国”，当前标记变量为E，前一个观察变量为“中”，前一个标记变量为B，则：t(B, E, ‘国’) 对应到条件随机场里相邻标记变量\((y_{i-1},y_i)\)的势函数。s(E, ‘国’) 对应到条件随机场里单个标记变量\(y_i\)对应的势函数\(s_l(y_i,x,i)\)。t(B, E, ‘国’), s(E, ‘国’)相应的权值\(λ_k,\mu_l\)， 都是由条件随机场用大量的标注语料训练出来。因此分词的标记识别就是求对于各个观察变量，它们的标记变量(BMES)状态序列的概率最大值，即求：的概率组合最大值。这个解法与隐马尔可夫类似，可以用viterbi算法求解。</p>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-5"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<p>使用crf<ins>实现对模型的训练，crf</ins> 安装、数据格式及模板参数请参考：<a href="https://jeffery0628.github.io/2020/04/09/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/" target="_blank" rel="noopener">序列标注</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> CRFPP</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRFModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model=<span class="string">'model_name'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 类初始化</span></span><br><span class="line"><span class="string">        :param model: 模型名称</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_tagger</span><span class="params">(self, tag_data)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 添加语料</span></span><br><span class="line"><span class="string">        :param tag_data: 数据</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        word_str = tag_data.strip()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(self.model):</span><br><span class="line">            print(<span class="string">'模型不存在,请确认模型路径是否正确!'</span>)</span><br><span class="line">            exit()</span><br><span class="line">        tagger = CRFPP.Tagger(<span class="string">"-m &#123;&#125; -v 3 -n2"</span>.format(self.model))</span><br><span class="line">        tagger.clear()</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> word_str:</span><br><span class="line">            tagger.add(word)</span><br><span class="line">        tagger.parse()</span><br><span class="line">        <span class="keyword">return</span> tagger</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">text_mark</span><span class="params">(self, tag_data, begin=<span class="string">'B'</span>, middle=<span class="string">'I'</span>, end=<span class="string">'E'</span>, single=<span class="string">'S'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        文本标记</span></span><br><span class="line"><span class="string">        :param tag_data: 数据</span></span><br><span class="line"><span class="string">        :param begin: 开始标记</span></span><br><span class="line"><span class="string">        :param middle: 中间标记</span></span><br><span class="line"><span class="string">        :param end: 结束标记</span></span><br><span class="line"><span class="string">        :param single: 单字结束标记</span></span><br><span class="line"><span class="string">        :return result: 标记列表</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        tagger = self.add_tagger(tag_data)</span><br><span class="line">        size = tagger.size()</span><br><span class="line">        tag_text = <span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, size):</span><br><span class="line">            word, tag = tagger.x(i, <span class="number">0</span>), tagger.y2(i)</span><br><span class="line">            <span class="keyword">if</span> tag <span class="keyword">in</span> [begin, middle]:</span><br><span class="line">                tag_text += word</span><br><span class="line">            <span class="keyword">elif</span> tag <span class="keyword">in</span> [end, single]:</span><br><span class="line">                tag_text += word + <span class="string">"*&amp;*"</span></span><br><span class="line">        result = tag_text.split(<span class="string">'*&amp;*'</span>)</span><br><span class="line">        result.pop()</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crf_test</span><span class="params">(self, tag_data, separator=<span class="string">'/'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: crf测试</span></span><br><span class="line"><span class="string">        :param tag_data:</span></span><br><span class="line"><span class="string">        :param separator:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        result = self.text_mark(tag_data)</span><br><span class="line">        data = separator.join(result)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crf_learn</span><span class="params">(self, filename)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数说明: 训练模型</span></span><br><span class="line"><span class="string">        :param filename: 已标注数据源</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        crf_bash = <span class="string">"crf_learn -f 3 -c 4.0 data/template.txt &#123;&#125; &#123;&#125;"</span>.format(filename, self.model)</span><br><span class="line">        process = subprocess.Popen(crf_bash.split(), stdout=subprocess.PIPE)</span><br><span class="line">        output = process.communicate()[<span class="number">0</span>]</span><br><span class="line">        print(output.decode(encoding=<span class="string">'utf-8'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    crf_model = CRFModel(model=<span class="string">'data/model_crf'</span>)</span><br><span class="line">    crf_model.crf_learn(filename=<span class="string">'data/train_file_crf.txt'</span>)</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    print(crf_model.crf_test(tag_data=<span class="string">'我们一定要战胜敌人，我们认为它们都是纸老虎。'</span>))</span><br></pre></td></tr></table></figure>
<h5 id="tiao-jian-sui-ji-chang-fen-ci-de-you-que-dian">条件随机场分词的优缺点</h5>
<p>条件随机场分词是一种精度很高的分词方法，它比隐马尔可夫的精度要高，是因为隐马尔可夫假设观察变量\(x_i\)只与当前状态\(y_i\)有关，而与其它状态\(y_{i-1}\)，\(y_{i+1}\)无关;而条件随机场假设了当前观察变量\(x_i\)与上下文相关，如 ，就是考虑到上一个字标记状态为B时，当前标记状态为E并且输出“国”字的概率。因此通过上下文的分析，条件随机场分词会提升到更高的精度。但因为复杂度比较高，条件随机场一般训练代价都比较大。</p>
<h4 id="jie-gou-hua-gan-zhi-ji-fen-ci-suan-fa">结构化感知机分词算法</h4>
<p>利用隐马尔科夫模型实现基于序列标注的中文分词器，效果并不理想。事实上，隐马尔可夫模型假设人们说的话仅仅取决于一个隐藏的BMES序列，这个假设太单纯了，不符合语言规律。语言不是由这么简单的标签序列生成，语言含有更多特征，而隐马尔科夫模型没有捕捉到。<strong>隐马弥可夫模型能捕捉的特征仅限于两种: 其一，前一个标签是什么；其二，当前字符是什么</strong>。为了利用更多的特征，线性模型( linear model )应运而生。线性模型由两部分构成: 一系列用来提取特征的特征函数\(\phi\)，以及相应的权重向量\(w\)。</p>
<h5 id="gan-zhi-ji-suan-fa">感知机算法</h5>
<p>感知机算法是一种迭代式的算法：在训练集上运行多个迭代，每次读入一个样本，执行预测，将预测结果与正确答案进行对比，计算误差，根据误差更新模型参数，再次进行训练，直到误差最小为止。</p>
<ul>
<li><strong>损失函数</strong>: 从数值优化的角度来讲，迭代式机器学习算法都在优化(减小)一个损失函数。损失函数 J(w) 用来衡量模型在训练集上的错误程度，自变量是模型参数 \(w\)，因变量是一个标量，表示模型在训练集上的损失的大小。</li>
<li><strong>梯度下降</strong>: 给定样本，其特征向量 \(x\) 只是常数，对 \(J(w)\) 求导，得到一个梯度向量 \(\Delta w\)，它的反方向一定是当前位置损失函数减小速度最快的方向。如果参数点 \(w\) 反方向移动就会使损失函数减小，叫梯度下降。</li>
<li><strong>学习率</strong>: 梯度下降的步长叫做学习率。</li>
<li><strong>随机梯度下降</strong>(SGD): 如果算法每次迭代随机选取部分样本计算损失函数的梯度，则称为随机梯度下降。</li>
</ul>
<p>假如数据本身线性不可分，感知机损失函数不会收敛，每次迭代分离超平面都会剧烈振荡。这时可以对感知机算法打补丁，使用投票感知机或平均感知机。</p>
<ol>
<li>
<p><strong>投票感知机</strong>：每次迭代的模型都保留，准确率也保留，预测时，每个模型都给出自己的结果，乘以它的准确率加权平均值作为最终结果。</p>
</li>
<li>
<p>投票感知机要求存储多个模型及加权，计算开销较大，更实际的做法是取多个模型的权重的平均，这就是<strong>平均感知机</strong>。</p>
</li>
</ol>
<h5 id="jie-gou-hua-yu-ce-wen-ti">结构化预测问题</h5>
<p>自然语言处理问题大致可分为两类，一种是分类问题，另一种就是结构化预测问题，序列标注只是结构化预测的一个特例，对感知机稍作拓展，分类器就能支持结构化预测。<br>
信息的层次结构特点称作结构化。<strong>那么结构化预测</strong>(structhre，prediction)则是预测对象结构的一类监督学习问题。相应的模型训练过程称作<strong>结构化学习</strong>(stutured laming )。分类问题的预测结果是一个决策边界， 回归问题的预测结果是一个实数标量，而结构化预测的结果则是一个完整的结构。<br>
自然语言处理中有许多任务是结构化预测，比如序列标注预测结构是一整个序列，句法分析预测结构是一棵句法树，机器翻译预测结构是一段完整的译文。这些结构由许多部分构成，最小的部分虽然也是分类问题(比如中文分词时每个字符分类为{B,M,E,S} ),但必须考虑结构整体的合理程度。</p>
<h6 id="jie-gou-hua-yu-ce-yu-xue-xi-liu-cheng">结构化预测与学习流程</h6>
<p>结构化预测的过程就是给定一个模型 λ 及打分函数 score，利用打分函数给一些备选结构打分，选择分数最高的结构作为预测输出，公式如下:<br>
\[
\hat{y}=\arg \max _{y \in Y} \operatorname{score}({\lambda}(x, y))
\]<br>
其中，Y 是备选结构的集合。既然结构化预测就是搜索得分最高的结构 y，那么结构化学习的目标就是想方设法让正确答案 y 的得分最高。不同的模型有不同的算法，对于线性模型，训练算法为结构化感知机。</p>
<h5 id="jie-gou-hua-gan-zhi-ji-suan-fa">结构化感知机算法</h5>
<p>要让线性模型支持结构化预测，必须先设计打分函数。打分函数的输入有两个缺一不可的参数: 特征 \(x\) 和结构\(y\)。但之前的线性模型的“打分函数”只接受一个自变量 \(x\)。做法是定义新的特征函数\(\phi (x,y)\)，把结构 \(y\) 也作为一种特征，输出新的“结构化特征向量”。新特征向量与权重向量做点积后，就得到一个标量，将其作为分数:<br>
\[
\operatorname{score}(x, y)=w \cdot \phi(x, y)
\]<br>
打分函数有了，取分值最大的结构作为预测结果，得到结构化预测函数:<br>
\[
\hat{y}=\arg \max _{y \in Y}(w \cdot \phi(x, y))
\]<br>
预测函数与线性分类器的决策函数很像，都是权重向量点积特征向量。那么感知机算法也可以拓展复用，得到线性模型的结构化学习算法:</p>
<ol>
<li>读入样本 \((x,y)\)，进行结构化预测\(\hat{y}=\arg \max _{y \in Y}(w \cdot \phi(x, y))\)</li>
<li>与正确答案相比，若不相等，则更新参数: 奖励正确答案触发的特征函数的权重，否则进行惩罚:\(w \leftarrow w+\phi\left(x^{(i)}, y\right)-\phi\left(x^{(i)}, \hat{y}\right)\)</li>
<li>调整学习率:\(\boldsymbol{w} \leftarrow \boldsymbol{w}+\alpha\left(\phi\left(\boldsymbol{x}^{(i)}, \boldsymbol{y}\right)-\phi\left(\boldsymbol{x}^{(i)}, \hat{\boldsymbol{y}}\right)\right)\)</li>
</ol>
<h6 id="jie-gou-hua-gan-zhi-ji-yu-gan-zhi-ji-suan-fa-bi-jiao">结构化感知机与感知机算法比较</h6>
<ul>
<li>结构化感知机修改了特征向量。</li>
<li>结构化感知机的参数更新赏罚分明。</li>
</ul>
<h6 id="jie-gou-hua-gan-zhi-ji-yu-xu-lie-biao-zhu">结构化感知机与序列标注</h6>
<p>序列标注最大的结构特点就是标签相互之间的依赖性，这种依赖性利用初始状态概率想俩狗和状态转移概率矩阵体系那，那么对于结构化感知机，就可以使用<strong>转移特征</strong>来表示:<br>
\[
\phi_{k}\left(y_{t-1}, y_{t}\right)=\left\{\begin{array}{ll}
1, &amp; y_{t-1}=s_{i}, and , y_{t}=s_{j} \\
0
\end{array} \quad i=0, \cdots, N ; j=1, \cdots, N\right.
\]<br>
其中，\(y_t\)为序列第 t 个标签，\(s_i\)为标注集第 i 种标签，N 为标注集大小。</p>
<p><strong>状态特征</strong>，类似于隐马尔可夫模型的发射概率矩阵，状态特征只与当前的状态有关，与之前的状态无关:<br>
\[
\phi_{i}\left(x_{i}, y_{i}\right)=\left\{\begin{array}{l}
1 \\
0
\end{array}\right.
\]<br>
于是，结构化感知机的特征函数就是转移特征和状态特征的合集:<br>
\[
\phi=\left[\phi_{k} ; \phi_{l}\right] \quad k=1, \cdots, N^{2}+N ; l=N^{2}+N+1, \cdots
\]<br>
基于以上公式，统一用打分函数来表示:<br>
\[
\operatorname{score}(\boldsymbol{x}, \boldsymbol{y})=\sum_{t=1}^{T} \boldsymbol{w} \cdot \phi\left(y_{t-1}, y_{t}, \boldsymbol{x}_{t}\right)
\]<br>
有了打分公式，就可以利用维特比算法求解得分最高的序列。</p>
<h5 id="a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-6"><a href="https://github.com/jeffery0628/chinese_word_seg" target="_blank" rel="noopener">代码</a></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CPTTrain</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, segment, train)</span>:</span></span><br><span class="line">        self.__char_type = &#123;&#125;</span><br><span class="line">        data_path = <span class="string">"data"</span></span><br><span class="line">        <span class="keyword">for</span> ind, name <span class="keyword">in</span> enumerate([<span class="string">"punc"</span>, <span class="string">"alph"</span>, <span class="string">"date"</span>, <span class="string">"num"</span>]):</span><br><span class="line">            fn = data_path + <span class="string">"/"</span> + name</span><br><span class="line">            <span class="keyword">if</span> os.path.isfile(fn):</span><br><span class="line">                <span class="keyword">for</span> line <span class="keyword">in</span> open(fn, <span class="string">"r"</span>):</span><br><span class="line">                    self.__char_type[line.strip()] = ind</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">"can't open"</span>, fn)</span><br><span class="line">                exit()</span><br><span class="line"></span><br><span class="line">        self.__train_insts = <span class="literal">None</span>  <span class="comment"># all instances for training.</span></span><br><span class="line">        self.__feats_weight = <span class="literal">None</span>  <span class="comment"># ["b", "m", "e", "s"][all the features] --&gt; weight.</span></span><br><span class="line">        self.__words_num = <span class="literal">None</span>  <span class="comment"># total words num in all the instances.</span></span><br><span class="line">        self.__insts_num = <span class="literal">None</span>  <span class="comment"># namley the sentences' num.</span></span><br><span class="line">        self.__cur_ite_ID = <span class="literal">None</span>  <span class="comment"># current iteration index.</span></span><br><span class="line">        self.__cur_inst_ID = <span class="literal">None</span>  <span class="comment"># current index_th instance.</span></span><br><span class="line">        self.__real_inst_ID = <span class="literal">None</span>  <span class="comment"># the accurate index in training instances after randimizing.</span></span><br><span class="line">        self.__last_update = <span class="literal">None</span>  <span class="comment"># ["b".."s"][feature] --&gt; [last_update_ite_ID, last_update_inst_ID]</span></span><br><span class="line">        self.__feats_weight_sum = <span class="literal">None</span>  <span class="comment"># sum of ["b".."s"][feature] from begin to end.</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> segment <span class="keyword">and</span> train <span class="keyword">or</span> <span class="keyword">not</span> segment <span class="keyword">and</span> <span class="keyword">not</span> train:</span><br><span class="line">            print(<span class="string">"there is only a True and False in segment and train"</span>)</span><br><span class="line">            exit()</span><br><span class="line">        <span class="keyword">elif</span> train:</span><br><span class="line">            self.Train = self.__Train</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.__LoadModel()</span><br><span class="line">            self.Segment = self.__Segment</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__LoadModel</span><span class="params">(self)</span>:</span></span><br><span class="line">        model = <span class="string">"data/avgmodel"</span></span><br><span class="line">        print(<span class="string">"load"</span>, model, <span class="string">"..."</span>)</span><br><span class="line">        self.__feats_weight = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> os.path.isfile(model):</span><br><span class="line">            start = time.clock()</span><br><span class="line">            self.__feats_weight = pickle.load(open(model, <span class="string">"rb"</span>))</span><br><span class="line">            end = time.clock()</span><br><span class="line">            print(<span class="string">"It takes %d seconds"</span> % (end - start))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"can't open"</span>, model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Train</span><span class="params">(self, corp_file_name, max_train_num, max_ite_num)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.__LoadCorp(corp_file_name, max_train_num):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        starttime = time.clock()</span><br><span class="line"></span><br><span class="line">        self.__feats_weight = &#123;&#125;</span><br><span class="line">        self.__last_update = &#123;&#125;</span><br><span class="line">        self.__feats_weight_sum = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> self.__cur_ite_ID <span class="keyword">in</span> range(max_ite_num):</span><br><span class="line">            <span class="keyword">if</span> self.__Iterate():</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        self.__SaveModel()</span><br><span class="line">        endtime = time.clock()</span><br><span class="line">        print(<span class="string">"total iteration times is %d seconds"</span> % (endtime - starttime))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__GenerateFeats</span><span class="params">(self, inst)</span>:</span></span><br><span class="line">        inst_feat = []</span><br><span class="line">        <span class="keyword">for</span> ind, [c, tag, t] <span class="keyword">in</span> enumerate(inst):</span><br><span class="line">            inst_feat.append([])</span><br><span class="line">            <span class="keyword">if</span> t == <span class="number">-1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># Cn</span></span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">-2</span>, <span class="number">3</span>):</span><br><span class="line">                inst_feat[<span class="number">-1</span>].append(<span class="string">"C%d==%s"</span> % (n, inst[ind + n][<span class="number">0</span>]))</span><br><span class="line">            <span class="comment"># CnCn+1</span></span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">-2</span>, <span class="number">2</span>):</span><br><span class="line">                inst_feat[<span class="number">-1</span>].append(<span class="string">"C%dC%d==%s%s"</span> % (n, n + <span class="number">1</span>, inst[ind + n][<span class="number">0</span>], inst[ind + n + <span class="number">1</span>][<span class="number">0</span>]))</span><br><span class="line">            <span class="comment"># C-1C1</span></span><br><span class="line">            inst_feat[<span class="number">-1</span>].append(<span class="string">"C-1C1==%s%s"</span> % (inst[ind - <span class="number">1</span>][<span class="number">0</span>], inst[ind + <span class="number">1</span>][<span class="number">0</span>]))</span><br><span class="line">            <span class="comment"># Pu(C0)</span></span><br><span class="line">            inst_feat[<span class="number">-1</span>].append(<span class="string">"Pu(%s)==%d"</span> % (c, int(t == <span class="number">0</span>)))</span><br><span class="line">            <span class="comment"># T(C-2)T(C-1)T(C0)T(C1)T(C2)</span></span><br><span class="line">            inst_feat[<span class="number">-1</span>].append(<span class="string">"T-2...2=%d%d%d%d%d"</span> % (</span><br><span class="line">            inst[ind - <span class="number">2</span>][<span class="number">2</span>], inst[ind - <span class="number">1</span>][<span class="number">2</span>], inst[ind][<span class="number">2</span>], inst[ind + <span class="number">1</span>][<span class="number">2</span>], inst[ind + <span class="number">2</span>][<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> inst_feat</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__SaveModel</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># the last time to sum all the features.</span></span><br><span class="line">        norm = float(self.__cur_ite_ID + <span class="number">1</span>) * self.__insts_num</span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> self.__feats_weight_sum:</span><br><span class="line">            last_ite_ID = self.__last_update[feat][<span class="number">0</span>]</span><br><span class="line">            last_inst_ID = self.__last_update[feat][<span class="number">1</span>]</span><br><span class="line">            c = (self.__cur_ite_ID - last_ite_ID) * self.__insts_num + self.__cur_inst_ID - last_inst_ID</span><br><span class="line">            self.__feats_weight_sum[feat] += self.__feats_weight[feat] * c</span><br><span class="line">            self.__feats_weight_sum[feat] = self.__feats_weight_sum[feat] / norm</span><br><span class="line"></span><br><span class="line">        pickle.dump(self.__feats_weight_sum, open(<span class="string">"data/avgmodel"</span>, <span class="string">"wb"</span>))</span><br><span class="line">        self.__train_insts = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__LoadCorp</span><span class="params">(self, corp_file_name, max_train_num)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(corp_file_name):</span><br><span class="line">            print(<span class="string">"can't open"</span>, corp_file_name)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        self.__train_insts = []</span><br><span class="line">        self.__words_num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> ind, line <span class="keyword">in</span> enumerate(open(corp_file_name, <span class="string">"r"</span>)):</span><br><span class="line">            <span class="keyword">if</span> max_train_num &gt; <span class="number">0</span> <span class="keyword">and</span> ind &gt;= max_train_num:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            self.__train_insts.append(self.__PreProcess(line.strip()))</span><br><span class="line">            self.__words_num += len(self.__train_insts[<span class="number">-1</span>]) - <span class="number">4</span></span><br><span class="line">        self.__insts_num = len(self.__train_insts)</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"number of total insts is"</span>, self.__insts_num)</span><br><span class="line">        print(<span class="string">"number of total characters is"</span>, self.__words_num)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__PreProcess</span><span class="params">(self, sent)</span>:</span></span><br><span class="line">        inst = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            inst.append([<span class="string">"&lt;s&gt;"</span>, <span class="string">"s"</span>, <span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sent.split():</span><br><span class="line">            rt = word.rpartition(<span class="string">"/"</span>)</span><br><span class="line">            t = self.__char_type.get(rt[<span class="number">0</span>], <span class="number">4</span>)</span><br><span class="line">            inst.append([rt[<span class="number">0</span>], rt[<span class="number">2</span>], t])  <span class="comment"># [c, tag, t]</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            inst.append([<span class="string">"&lt;s&gt;"</span>, <span class="string">"s"</span>, <span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> inst</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Segment</span><span class="params">(self, src)</span>:</span></span><br><span class="line">        <span class="string">"""suppose there is one sentence once."""</span></span><br><span class="line">        inst = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            inst.append([<span class="string">"&lt;s&gt;"</span>, <span class="string">"s"</span>, <span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> src:</span><br><span class="line">            inst.append([c, <span class="string">""</span>, self.__char_type.get(c, <span class="number">4</span>)])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            inst.append([<span class="string">"&lt;s&gt;"</span>, <span class="string">"s"</span>, <span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        feats = self.__GenerateFeats(inst)</span><br><span class="line">        tags = self.__DPSegment(inst, feats)</span><br><span class="line"></span><br><span class="line">        rst = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(tags) - <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">if</span> tags[i] <span class="keyword">in</span> [<span class="string">"s"</span>, <span class="string">"b"</span>]:</span><br><span class="line">                rst.append(inst[i][<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                rst[<span class="number">-1</span>] += inst[i][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">" "</span>.join(rst)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Iterate</span><span class="params">(self)</span>:</span></span><br><span class="line">        start = time.clock()</span><br><span class="line">        print(<span class="string">"%d th iteration"</span> % self.__cur_ite_ID)</span><br><span class="line"></span><br><span class="line">        train_list = random.sample(range(self.__insts_num), self.__insts_num)</span><br><span class="line">        error_sents_num = <span class="number">0</span></span><br><span class="line">        error_words_num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> self.__cur_inst_ID, self.__real_inst_ID <span class="keyword">in</span> enumerate(train_list):</span><br><span class="line">            num = self.__TrainInstance()</span><br><span class="line">            error_sents_num += <span class="number">1</span> <span class="keyword">if</span> num &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            error_words_num += num</span><br><span class="line"></span><br><span class="line">        st = <span class="number">1</span> - float(error_sents_num) / self.__insts_num</span><br><span class="line">        wt = <span class="number">1</span> - float(error_words_num) / self.__words_num</span><br><span class="line"></span><br><span class="line">        end = time.clock()</span><br><span class="line">        print(<span class="string">"sents accuracy = %f%%, words accuracy = %f%%, it takes %d seconds"</span> % (st * <span class="number">100</span>, wt * <span class="number">100</span>, end - start))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> error_sents_num == <span class="number">0</span> <span class="keyword">and</span> error_words_num == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__TrainInstance</span><span class="params">(self)</span>:</span></span><br><span class="line">        cur_inst = self.__train_insts[self.__real_inst_ID]</span><br><span class="line">        feats = self.__GenerateFeats(cur_inst)</span><br><span class="line"></span><br><span class="line">        seg = self.__DPSegment(cur_inst, feats)</span><br><span class="line">        <span class="keyword">return</span> self.__Correct(seg, feats)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__DPSegment</span><span class="params">(self, inst, feats)</span>:</span></span><br><span class="line">        num = len(inst)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get all position's score.</span></span><br><span class="line">        value = [&#123;&#125; <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, num - <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> [<span class="string">"b"</span>, <span class="string">"m"</span>, <span class="string">"e"</span>, <span class="string">"s"</span>]:</span><br><span class="line">                value[i][t] = self.__GetScore(i, t, feats)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># find optimal path.</span></span><br><span class="line">        tags = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]</span><br><span class="line">        best = [<span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]  <span class="comment"># best[i]: [i, i + length(i)) is optimal segment.</span></span><br><span class="line">        length = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(num)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num - <span class="number">2</span> - <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">for</span> dis <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">                <span class="keyword">if</span> i + dis &gt; num - <span class="number">2</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                cur_score = best[i + dis]</span><br><span class="line">                self.__Tag(i, i + dis, tags)</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(i, i + dis):</span><br><span class="line">                    cur_score += value[k][tags[k]]</span><br><span class="line">                <span class="keyword">if</span> length[i] <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> cur_score &gt; best[i]:</span><br><span class="line">                    best[i] = cur_score</span><br><span class="line">                    length[i] = dis</span><br><span class="line"></span><br><span class="line">        i = <span class="number">2</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; num - <span class="number">2</span>:</span><br><span class="line">            self.__Tag(i, i + length[i], tags)</span><br><span class="line">            i += length[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tags</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__GetScore</span><span class="params">(self, pos, t, feats)</span>:</span></span><br><span class="line">        pos_feats = feats[pos]</span><br><span class="line">        score = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> pos_feats:</span><br><span class="line">            score += self.__feats_weight.get(feat + <span class="string">"=&gt;"</span> + t, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Tag</span><span class="params">(self, f, t, tags)</span>:</span></span><br><span class="line">        <span class="string">"""tag the sequence tags in the xrange of [f, t)"""</span></span><br><span class="line">        <span class="keyword">if</span> t - f == <span class="number">1</span>:</span><br><span class="line">            tags[f] = <span class="string">"s"</span></span><br><span class="line">        <span class="keyword">elif</span> t - f &gt;= <span class="number">2</span>:</span><br><span class="line">            tags[f], tags[t - <span class="number">1</span>] = <span class="string">"b"</span>, <span class="string">"e"</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(f + <span class="number">1</span>, t - <span class="number">1</span>):</span><br><span class="line">                tags[i] = <span class="string">"m"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Correct</span><span class="params">(self, tags, feats)</span>:</span></span><br><span class="line">        updates = &#123;&#125;</span><br><span class="line">        cur_inst = self.__train_insts[self.__real_inst_ID]</span><br><span class="line">        error_words_num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(cur_inst) - <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">if</span> tags[i] == cur_inst[i][<span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            error_words_num += <span class="number">1</span></span><br><span class="line">            pos_feats = feats[i]</span><br><span class="line">            target = cur_inst[i][<span class="number">1</span>]</span><br><span class="line">            mine = tags[i]</span><br><span class="line">            <span class="keyword">for</span> feat <span class="keyword">in</span> pos_feats:</span><br><span class="line">                updates[feat + <span class="string">"=&gt;"</span> + target] = updates.get(feat + <span class="string">"=&gt;"</span> + target, <span class="number">0.0</span>) + <span class="number">1</span></span><br><span class="line">                updates[feat + <span class="string">"=&gt;"</span> + mine] = updates.get(feat + <span class="string">"=&gt;"</span> + mine, <span class="number">0.0</span>) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        self.__Update(updates)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> error_words_num</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__Update</span><span class="params">(self, updates)</span>:</span></span><br><span class="line">        <span class="comment"># update the features weight.</span></span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> updates:</span><br><span class="line">            pair = self.__last_update.get(feat, [<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">            last_ite_ID = pair[<span class="number">0</span>]</span><br><span class="line">            last_inst_ID = pair[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            c = (self.__cur_ite_ID - last_ite_ID) * self.__insts_num + self.__cur_inst_ID - last_inst_ID</span><br><span class="line">            self.__feats_weight_sum[feat] = self.__feats_weight_sum.get(feat, <span class="number">0</span>) + c * self.__feats_weight.get(feat, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            self.__feats_weight[feat] = self.__feats_weight.get(feat, <span class="number">0</span>) + updates[feat]</span><br><span class="line">            self.__last_update[feat] = [self.__cur_ite_ID, self.__cur_inst_ID]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"></span><br><span class="line">    train = CPTTrain(train=<span class="literal">True</span>, segment=<span class="literal">False</span>)</span><br><span class="line">    train.Train(<span class="string">"data/msr_train.txt"</span>, max_train_num=<span class="number">1000000</span>, max_ite_num=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    srcs = [<span class="string">"夏天的清晨"</span>,</span><br><span class="line">            <span class="string">"“人们常说生活是一部教科书，而血与火的战争更是不可多得的教科书，她确实是名副其实的‘我的大学’。"</span>,</span><br><span class="line">            <span class="string">"夏天的清晨夏天看见猪八戒和嫦娥了。"</span>,</span><br><span class="line">            <span class="string">"海运业雄踞全球之首，按吨位计占世界总数的１７％。"</span>]</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"avg"</span>)</span><br><span class="line">    seg = CPTTrain(train=<span class="literal">False</span>, segment=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> src <span class="keyword">in</span> srcs:</span><br><span class="line">        print(seg.Segment(src))</span><br></pre></td></tr></table></figure>
<h4 id="ji-yu-shen-du-xue-xi-de-duan-dao-duan-de-fen-ci-fang-fa">基于深度学习的端到端的分词方法</h4>
<p>在中文分词上，基于神经网络的方法，往往使用「字向量 + BiLSTM + CRF」模型，利用神经网络来学习特征，将传统 CRF 中的人工特征工程量降到最低</p>
<p>BiLSTM、BiLSTM+CRF、BiLSTM+CNN+CRF、BERT、BERT+CRF、BERT+BiLSTM、BERT+BiLSTM+CRF，由于以上模型均可对序列标注任务进行建模求解，所以均可拿来做中文分词。以比较典型的「字向量 + BiLSTM （+CNN）+ CRF」模型为例：</p>
<p>BiLSTM融合两组学习方向相反（一个按句子顺序，一个按句子逆序）的LSTM层，能够在理论上实现当前词即包含历史信息、又包含未来信息，更有利于对当前词进行标注。虽然依赖于神经网络强大的非线性拟合能力，理论上我们已经能够学习出不错的模型。但是，BiLSTM只考虑了标签上的上下文信息。对于序列标注任务来说，当前位置的标签\(y_t\)与前一个位置\(y_{t-1}\)、后一个位置\(y_{t+1}\)都有潜在的关系。例如，“东南大学欢迎您”被标注为“东/S 南/M 大/M 学/E 欢/B 迎/E 您/S”，由分词的标注规则可知，B标签后只能接M和E，BiLSTM没有利用这种标签之间的上下文信息。因此，就有人提出了在模型后接一层CRF层，用于在整个序列上学习最优的标签序列：</p>
<p><img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/bilstm_crf.jpeg" alt="avatar"></p>
<p>BiLSTM+CNN+CRF：对于分词任务，当前词的标签基本上只与前几个和和几个词有关联。BiLSTM在学习较长句子时，可能因为模型容量问题丢弃一些重要信息，因此在模型中加了一个CNN层，用于提取当前词的局部特征。（不知效果怎样？？！！）</p>
<p><img src="/2020/04/28/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/bilstm_cnn_crf.png" alt="avatar"></p>
<h5 id="mo-xing-bi-jiao">模型比较</h5>
<p>Trained and tested with pku dataset</p>
<p>CRF</p>
<table>
<thead>
<tr>
<th style="text-align:left">Template</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">crf_template</td>
<td style="text-align:center">0.938</td>
<td style="text-align:center">0.923</td>
<td style="text-align:center">0.931</td>
</tr>
</tbody>
</table>
<p>Bi-LSTM</p>
<table>
<thead>
<tr>
<th style="text-align:left">Structure</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">emb256_hid256_l3</td>
<td style="text-align:center">0.9252</td>
<td style="text-align:center">0.9237</td>
<td style="text-align:center">0.9243</td>
</tr>
</tbody>
</table>
<p>Bi-LSTM + CRF</p>
<table>
<thead>
<tr>
<th style="text-align:left">Structure</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">emb256_hid256_l3</td>
<td style="text-align:center">0.9343</td>
<td style="text-align:center">0.9336</td>
<td style="text-align:center">0.9339</td>
</tr>
</tbody>
</table>
<p>BERT + Bi-LSTM</p>
<table>
<thead>
<tr>
<th style="text-align:left">Structure</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
<th style="text-align:center">F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">emb768_hid512_l2</td>
<td style="text-align:center">0.9698</td>
<td style="text-align:center">0.9650</td>
<td style="text-align:center">0.9646</td>
</tr>
</tbody>
</table>
<h5 id="dai-ma">代码</h5>
<p>以BERT+RNN+CRF为例，可根据需求进行改动。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> TorchCRF <span class="keyword">import</span> CRF </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertRNNCRF</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_tags, rnn_type, bert_path, bert_train, seg_vocab_size,hidden_dim, n_layers, bidirectional, batch_first,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout, restrain)</span>:</span></span><br><span class="line">        super(BertRNNCRF, self).__init__()</span><br><span class="line">        self.rnn_type = rnn_type.lower()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.bert = BertModel.from_pretrained(bert_path)</span><br><span class="line">        <span class="comment"># 对bert进行训练</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.bert.named_parameters():</span><br><span class="line">            param.requires_grad = bert_train</span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                               hidden_size=hidden_dim,</span><br><span class="line">                               num_layers=n_layers,</span><br><span class="line">                               bidirectional=bidirectional,</span><br><span class="line">                               batch_first=batch_first,</span><br><span class="line">                               dropout=dropout)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">'gru'</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.bert.config.to_dict()[<span class="string">'hidden_size'</span>],</span><br><span class="line">                              hidden_size=hidden_dim,</span><br><span class="line">                              num_layers=n_layers,</span><br><span class="line">                              bidirectional=bidirectional,</span><br><span class="line">                              batch_first=batch_first,</span><br><span class="line">                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">        self.crf = CRF(num_tags, batch_first=<span class="literal">True</span>, restrain_matrix=restrain, loss_side=<span class="number">2.5</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            self.fc_tags = nn.Linear(hidden_dim*<span class="number">2</span>, num_tags)</span><br><span class="line">            <span class="comment"># self.fc_tags = nn.Linear(768, num_tags)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.fc_tags = nn.Linear(hidden_dim, num_tags)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, context, seq_len,max_seq_len, mask_bert)</span>:</span></span><br><span class="line">        <span class="comment"># context    输入的句子</span></span><br><span class="line">        <span class="comment"># mask_bert  对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        bert_sentence, bert_cls = self.bert(context, attention_mask=mask_bert)</span><br><span class="line">        <span class="comment"># sentence_len = bert_sentence.shape[1]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># bert_cls = bert_cls.unsqueeze(dim=1).repeat(1, sentence_len, 1)</span></span><br><span class="line">        <span class="comment"># bert_sentence = bert_sentence + bert_cls</span></span><br><span class="line">        encoder_out, sorted_seq_lengths, desorted_indices = self.prepare_pack_padded_sequence(bert_sentence, seq_len)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(encoder_out, sorted_seq_lengths,</span><br><span class="line">                                                            batch_first=self.batch_first)</span><br><span class="line">        <span class="comment"># self.rnn.flatten_parameters()</span></span><br><span class="line">        <span class="keyword">if</span> self.rnn_type <span class="keyword">in</span> [<span class="string">'rnn'</span>, <span class="string">'gru'</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        <span class="comment"># output = [ batch size,sent len, hidden_dim * bidirectional]</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first,total_length=max_seq_len)</span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        out = self.fc_tags(self.dropout(output.contiguous()))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out[:, <span class="number">1</span>:, :]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_pack_padded_sequence</span><span class="params">(self, inputs_words, seq_lengths, descending=True)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param device:</span></span><br><span class="line"><span class="string">        :param inputs_words:</span></span><br><span class="line"><span class="string">        :param seq_lengths:</span></span><br><span class="line"><span class="string">        :param descending:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        sorted_seq_lengths, indices = torch.sort(seq_lengths, descending=descending)</span><br><span class="line">        _, desorted_indices = torch.sort(indices, descending=<span class="literal">False</span>)</span><br><span class="line">        sorted_inputs_words = inputs_words[indices]</span><br><span class="line">        <span class="keyword">return</span> sorted_inputs_words, sorted_seq_lengths, desorted_indices</span><br></pre></td></tr></table></figure>
<h3 id="fen-ci-zhong-de-nan-ti">分词中的难题</h3>
<p>中文是一种十分复杂的语言，让计算机理解中文语言更是困难。在中文分词过程中，有两大难题一直没有完全突破。</p>
<h4 id="1-qi-yi-qie-fen-wen-ti">1、歧义切分问题</h4>
<p>歧义是指同样的一句话，可能有两种或者更多的切分方法。</p>
<ul>
<li>交集歧义：“结合成”，结合/成，结/合成</li>
<li>组合歧义：“起身”，他站起/身/来。 他明天/起身/去北京。</li>
<li>混合型歧义：同时具备交集歧义和组合歧义的特点。1，这篇文章写得太平淡了。2，这墙抹的太平了。3，即使太平时期也不应该放松警惕。“太平淡”是交集型，“太平”是组合型。</li>
</ul>
<p>交集歧义相对组合歧义来说是还算比较容易处理，组合歧义需要根据整个句子来判断。（特征：上下文语义分析，韵律分析，语气，重音，停顿）</p>
<h4 id="2-wei-deng-lu-ci">2、未登录词</h4>
<p>未登录词（生词，新词）：1.已有的词表中没有收录的词。2.已有的训练语料中未曾出现的词（集外词OOV）。</p>
<p>类型：</p>
<ol>
<li>新出现的普通词汇：博客，超女，给力。</li>
<li>专有名词：人名、地名、组织名、时间、数字表达、</li>
<li>专业名词和研究领域名词：苏丹红，禽流感</li>
<li>其他专用名词：新出现的产品名，电影，书记。</li>
</ol>
<p>对于真实数据来说，未登录词对分词精度的影响远远超过了歧义切分。未登录词是很难处理的问题，而且这些又正好是人们经常使用的词，因此对于搜索引擎来说，分词系统中的未登录词识别十分重要。目前未登录词识别准确率已经成为评价一个分词系统好坏的重要标志之一。</p>
<h3 id="can-kao">参考</h3>
<ol>
<li>
<p><a href="https://blog.csdn.net/selinda001/article/details/79345072" target="_blank" rel="noopener">中文分词引擎 java 实现 — 正向最大、逆向最大、双向最大匹配法</a></p>
</li>
<li>
<p><a href="https://www.cnblogs.com/sxron/articles/6391926.html" target="_blank" rel="noopener">中文分词算法总结</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/Zh823275484/article/details/87878512" target="_blank" rel="noopener">基于n-gram模型的中文分词</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/qq_27825451/article/details/102457058" target="_blank" rel="noopener">详解语言模型NGram及困惑度Perplexity</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/wangliang_f/article/details/17532633" target="_blank" rel="noopener">分词学习(3)，基于ngram语言模型的n元分词</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/33261835" target="_blank" rel="noopener">中文分词算法简介</a></p>
</li>
<li>
<p><a href="https://www.jianshu.com/p/715fa597c6bc" target="_blank" rel="noopener">NLP：分词算法综述</a></p>
</li>
<li>
<p><a href="https://github.com/NLP-LOVE/Introduction-NLP/blob/master/chapter/5.%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%88%86%E7%B1%BB%E4%B8%8E%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8.md" target="_blank" rel="noopener">感知机分类与序列标注</a></p>
</li>
</ol>

    </div>

    
    
    <div>
  
    <div style="text-align:center;color:#bfbfbf;font-size:16px;">
      <span>-------- 本文结束 </span>
      <i class="fa fa-coffee"></i>
      <span> 感谢阅读 --------</span>
    </div>
  
</div>
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    鼓励一下
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat.png" alt="Li Zhen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/ali.png" alt="Li Zhen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80/" rel="tag"><i class="fa fa-tag"></i> 自然语言处理基础</a>
              <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" rel="tag"><i class="fa fa-tag"></i> 自然语言处理</a>
              <a href="/tags/%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/" rel="tag"><i class="fa fa-tag"></i> 分词算法</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/04/24/text_correction/" rel="prev" title="文本纠错">
      <i class="fa fa-chevron-left"></i> 文本纠错
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/05/08/%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95/" rel="next" title="平滑方法">
      平滑方法 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#fen-ci-jian-jie"><span class="nav-number">1.</span> <span class="nav-text">分词简介</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#zhong-wen-fen-ci-de-ying-yong"><span class="nav-number">1.1.</span> <span class="nav-text">中文分词的应用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ji-yu-pi-pei-gui-ze-de-fang-fa"><span class="nav-number">2.</span> <span class="nav-text">基于匹配规则的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#qian-xiang-zui-da-pi-pei-suan-fa"><span class="nav-number">2.1.</span> <span class="nav-text">前向最大匹配算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#suan-fa-liu-cheng"><span class="nav-number">2.1.1.</span> <span class="nav-text">算法流程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#fen-ci-shi-li"><span class="nav-number">2.1.2.</span> <span class="nav-text">分词实例：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a"><span class="nav-number">2.1.3.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#hou-xiang-zui-da-pi-pei-suan-fa"><span class="nav-number">2.2.</span> <span class="nav-text">后向最大匹配算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#suan-fa-liu-cheng-1"><span class="nav-number">2.2.1.</span> <span class="nav-text">算法流程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#fen-ci-shi-li-1"><span class="nav-number">2.2.2.</span> <span class="nav-text">分词实例</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-1"><span class="nav-number">2.2.3.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#shuang-xiang-zui-da-pi-pei-fa"><span class="nav-number">2.3.</span> <span class="nav-text">双向最大匹配法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#suan-fa-liu-cheng-2"><span class="nav-number">2.3.1.</span> <span class="nav-text">算法流程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#fen-ci-shi-li-2"><span class="nav-number">2.3.2.</span> <span class="nav-text">分词实例</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-2"><span class="nav-number">2.3.3.</span> <span class="nav-text">代码</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#que-dian"><span class="nav-number">2.3.4.</span> <span class="nav-text">缺点：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#n-zui-duan-lu-jing-fen-ci-suan-fa"><span class="nav-number">2.4.</span> <span class="nav-text">(N-)最短路径分词算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ji-ben-si-xiang"><span class="nav-number">2.4.1.</span> <span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ju-li"><span class="nav-number">2.4.2.</span> <span class="nav-text">举例</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-wei-te-bi-a"><span class="nav-number">2.4.3.</span> <span class="nav-text">代码（维特比）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#que-dian-1"><span class="nav-number">2.5.</span> <span class="nav-text">缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ji-yu-tong-ji-yi-ji-ji-qi-xue-xi-de-fen-ci-fang-fa"><span class="nav-number">3.</span> <span class="nav-text">基于统计以及机器学习的分词方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ji-yu-n-gram-yu-yan-mo-xing-de-fen-ci-fang-fa"><span class="nav-number">3.1.</span> <span class="nav-text">基于N-gram语言模型的分词方法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#jian-jie"><span class="nav-number">3.1.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#can-shu-gu-ji"><span class="nav-number">3.1.2.</span> <span class="nav-text">参数估计</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-3"><span class="nav-number">3.1.3.</span> <span class="nav-text">代码</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#zong-jie"><span class="nav-number">3.1.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ji-yu-hmm-de-fen-ci-suan-fa"><span class="nav-number">3.2.</span> <span class="nav-text">基于HMM的分词算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#yin-ma-er-ke-fu-mo-xing-hmm"><span class="nav-number">3.2.1.</span> <span class="nav-text">隐马尔科夫模型(HMM)</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#mo-xing-biao-shi"><span class="nav-number">3.2.1.1.</span> <span class="nav-text">模型表示</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#liang-ge-ji-ben-jia-she"><span class="nav-number">3.2.1.2.</span> <span class="nav-text">两个基本假设</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#san-ge-ji-ben-wen-ti"><span class="nav-number">3.2.1.3.</span> <span class="nav-text">三个基本问题</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#jian-jie-1"><span class="nav-number">3.2.2.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-4"><span class="nav-number">3.2.3.</span> <span class="nav-text">代码</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#zong-jie-1"><span class="nav-number">3.2.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#crf-fen-ci"><span class="nav-number">3.3.</span> <span class="nav-text">CRF分词</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#tiao-jian-sui-ji-chang-crf"><span class="nav-number">3.3.1.</span> <span class="nav-text">条件随机场CRF</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#tiao-jian-sui-ji-chang-de-can-shu-hua-xing-shi"><span class="nav-number">3.3.1.1.</span> <span class="nav-text">条件随机场的参数化形式</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#tiao-jian-sui-ji-chang-fen-ci-fang-fa"><span class="nav-number">3.3.2.</span> <span class="nav-text">条件随机场分词方法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-5"><span class="nav-number">3.3.3.</span> <span class="nav-text">代码</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#tiao-jian-sui-ji-chang-fen-ci-de-you-que-dian"><span class="nav-number">3.3.4.</span> <span class="nav-text">条件随机场分词的优缺点</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#jie-gou-hua-gan-zhi-ji-fen-ci-suan-fa"><span class="nav-number">3.4.</span> <span class="nav-text">结构化感知机分词算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#gan-zhi-ji-suan-fa"><span class="nav-number">3.4.1.</span> <span class="nav-text">感知机算法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#jie-gou-hua-yu-ce-wen-ti"><span class="nav-number">3.4.2.</span> <span class="nav-text">结构化预测问题</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#jie-gou-hua-yu-ce-yu-xue-xi-liu-cheng"><span class="nav-number">3.4.2.1.</span> <span class="nav-text">结构化预测与学习流程</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#jie-gou-hua-gan-zhi-ji-suan-fa"><span class="nav-number">3.4.3.</span> <span class="nav-text">结构化感知机算法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#jie-gou-hua-gan-zhi-ji-yu-gan-zhi-ji-suan-fa-bi-jiao"><span class="nav-number">3.4.3.1.</span> <span class="nav-text">结构化感知机与感知机算法比较</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#jie-gou-hua-gan-zhi-ji-yu-xu-lie-biao-zhu"><span class="nav-number">3.4.3.2.</span> <span class="nav-text">结构化感知机与序列标注</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#a-href-https-github-com-jeffery-0628-chinese-word-seg-dai-ma-a-6"><span class="nav-number">3.4.4.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ji-yu-shen-du-xue-xi-de-duan-dao-duan-de-fen-ci-fang-fa"><span class="nav-number">3.5.</span> <span class="nav-text">基于深度学习的端到端的分词方法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#mo-xing-bi-jiao"><span class="nav-number">3.5.1.</span> <span class="nav-text">模型比较</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#dai-ma"><span class="nav-number">3.5.2.</span> <span class="nav-text">代码</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fen-ci-zhong-de-nan-ti"><span class="nav-number">4.</span> <span class="nav-text">分词中的难题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-qi-yi-qie-fen-wen-ti"><span class="nav-number">4.1.</span> <span class="nav-text">1、歧义切分问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-wei-deng-lu-ci"><span class="nav-number">4.2.</span> <span class="nav-text">2、未登录词</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#can-kao"><span class="nav-number">5.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <a href='/'>
    <img class="site-author-image" itemprop="image" alt="Li Zhen"
      src="/images/snoppy.jpeg">
  </a>
  <p class="site-author-name" itemprop="name">Li Zhen</p>
  <div class="site-description" itemprop="description">他，不论在成年还是在小时候，必须踏上一条极为艰苦的道路，不过这是一条最可靠的道路。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">49</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jeffery0628" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jeffery0628" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jeffery.lee.0628@gmail.com" title="邮箱 → mailto:jeffery.lee.0628@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>邮箱</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Zhen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">538k</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="site-uv">
      我的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位朋友，
    </span>
    <span class="site-pv">
      历经 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次回眸才与你相遇
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  






  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>


 

<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180101,"YYYYMMDD"));
    ages = ages.replace(/years?/, "年");
    ages = ages.replace(/months?/, "月");
    ages = ages.replace(/days?/, "天");
    ages = ages.replace(/hours?/, "小时");
    ages = ages.replace(/minutes?/, "分");
    ages = ages.replace(/seconds?/, "秒");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `我已在此等候你 ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


<script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>

<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdnjs.cloudflare.com/ajax/libs/valine/1.3.10/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'tChTbEIggDSVcdhPBUf0MFxW-gzGzoHsz',
      appKey     : 'sJ6xUiFnifEDIIfnj3Ut9zy1',
      placeholder: "留下你的小脚印吧！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '8' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
